<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp", "content": "//===---- CGBuiltin.cpp - Emit LLVM Code for builtins ---------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This contains code to emit Builtin calls as LLVM code.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"CGCXXABI.h\"\n#include \"CGObjCRuntime.h\"\n#include \"CGOpenCLRuntime.h\"\n#include \"CGRecordLayout.h\"\n#include \"CodeGenFunction.h\"\n#include \"CodeGenModule.h\"\n#include \"ConstantEmitter.h\"\n#include \"PatternInit.h\"\n#include \"TargetInfo.h\"\n#include \"clang/AST/ASTContext.h\"\n#include \"clang/AST/Attr.h\"\n#include \"clang/AST/Decl.h\"\n#include \"clang/AST/OSLog.h\"\n#include \"clang/Basic/TargetBuiltins.h\"\n#include \"clang/Basic/TargetInfo.h\"\n#include \"clang/CodeGen/CGFunctionInfo.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/StringExtras.h\"\n#include \"llvm/Analysis/ValueTracking.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/InlineAsm.h\"\n#include \"llvm/IR/Intrinsics.h\"\n#include \"llvm/IR/IntrinsicsAArch64.h\"\n#include \"llvm/IR/IntrinsicsAMDGPU.h\"\n#include \"llvm/IR/IntrinsicsARM.h\"\n#include \"llvm/IR/IntrinsicsBPF.h\"\n#include \"llvm/IR/IntrinsicsHexagon.h\"\n#include \"llvm/IR/IntrinsicsNVPTX.h\"\n#include \"llvm/IR/IntrinsicsPowerPC.h\"\n#include \"llvm/IR/IntrinsicsR600.h\"\n#include \"llvm/IR/IntrinsicsS390.h\"\n#include \"llvm/IR/IntrinsicsWebAssembly.h\"\n#include \"llvm/IR/IntrinsicsX86.h\"\n#include \"llvm/IR/MDBuilder.h\"\n#include \"llvm/IR/MatrixBuilder.h\"\n#include \"llvm/Support/ConvertUTF.h\"\n#include \"llvm/Support/ScopedPrinter.h\"\n#include \"llvm/Support/X86TargetParser.h\"\n#include <sstream>\n\nusing namespace clang;\nusing namespace CodeGen;\nusing namespace llvm;\n\nstatic\nint64_t clamp(int64_t Value, int64_t Low, int64_t High) {\n  return std::min(High, std::max(Low, Value));\n}\n\nstatic void initializeAlloca(CodeGenFunction &CGF, AllocaInst *AI, Value *Size,\n                             Align AlignmentInBytes) {\n  ConstantInt *Byte;\n  switch (CGF.getLangOpts().getTrivialAutoVarInit()) {\n  case LangOptions::TrivialAutoVarInitKind::Uninitialized:\n    // Nothing to initialize.\n    return;\n  case LangOptions::TrivialAutoVarInitKind::Zero:\n    Byte = CGF.Builder.getInt8(0x00);\n    break;\n  case LangOptions::TrivialAutoVarInitKind::Pattern: {\n    llvm::Type *Int8 = llvm::IntegerType::getInt8Ty(CGF.CGM.getLLVMContext());\n    Byte = llvm::dyn_cast<llvm::ConstantInt>(\n        initializationPatternFor(CGF.CGM, Int8));\n    break;\n  }\n  }\n  if (CGF.CGM.stopAutoInit())\n    return;\n  auto *I = CGF.Builder.CreateMemSet(AI, Byte, Size, AlignmentInBytes);\n  I->addAnnotationMetadata(\"auto-init\");\n}\n\n/// getBuiltinLibFunction - Given a builtin id for a function like\n/// \"__builtin_fabsf\", return a Function* for \"fabsf\".\nllvm::Constant *CodeGenModule::getBuiltinLibFunction(const FunctionDecl *FD,\n                                                     unsigned BuiltinID) {\n  assert(Context.BuiltinInfo.isLibFunction(BuiltinID));\n\n  // Get the name, skip over the __builtin_ prefix (if necessary).\n  StringRef Name;\n  GlobalDecl D(FD);\n\n  // If the builtin has been declared explicitly with an assembler label,\n  // use the mangled name. This differs from the plain label on platforms\n  // that prefix labels.\n  if (FD->hasAttr<AsmLabelAttr>())\n    Name = getMangledName(D);\n  else\n    Name = Context.BuiltinInfo.getName(BuiltinID) + 10;\n\n  llvm::FunctionType *Ty =\n    cast<llvm::FunctionType>(getTypes().ConvertType(FD->getType()));\n\n  return GetOrCreateLLVMFunction(Name, Ty, D, /*ForVTable=*/false);\n}\n\n/// Emit the conversions required to turn the given value into an\n/// integer of the given size.\nstatic Value *EmitToInt(CodeGenFunction &CGF, llvm::Value *V,\n                        QualType T, llvm::IntegerType *IntType) {\n  V = CGF.EmitToMemory(V, T);\n\n  if (V->getType()->isPointerTy())\n    return CGF.Builder.CreatePtrToInt(V, IntType);\n\n  assert(V->getType() == IntType);\n  return V;\n}\n\nstatic Value *EmitFromInt(CodeGenFunction &CGF, llvm::Value *V,\n                          QualType T, llvm::Type *ResultType) {\n  V = CGF.EmitFromMemory(V, T);\n\n  if (ResultType->isPointerTy())\n    return CGF.Builder.CreateIntToPtr(V, ResultType);\n\n  assert(V->getType() == ResultType);\n  return V;\n}\n\n/// Utility to insert an atomic instruction based on Intrinsic::ID\n/// and the expression node.\nstatic Value *MakeBinaryAtomicValue(\n    CodeGenFunction &CGF, llvm::AtomicRMWInst::BinOp Kind, const CallExpr *E,\n    AtomicOrdering Ordering = AtomicOrdering::SequentiallyConsistent) {\n  QualType T = E->getType();\n  assert(E->getArg(0)->getType()->isPointerType());\n  assert(CGF.getContext().hasSameUnqualifiedType(T,\n                                  E->getArg(0)->getType()->getPointeeType()));\n  assert(CGF.getContext().hasSameUnqualifiedType(T, E->getArg(1)->getType()));\n\n  llvm::Value *DestPtr = CGF.EmitScalarExpr(E->getArg(0));\n  unsigned AddrSpace = DestPtr->getType()->getPointerAddressSpace();\n\n  llvm::IntegerType *IntType =\n    llvm::IntegerType::get(CGF.getLLVMContext(),\n                           CGF.getContext().getTypeSize(T));\n  llvm::Type *IntPtrType = IntType->getPointerTo(AddrSpace);\n\n  llvm::Value *Args[2];\n  Args[0] = CGF.Builder.CreateBitCast(DestPtr, IntPtrType);\n  Args[1] = CGF.EmitScalarExpr(E->getArg(1));\n  llvm::Type *ValueType = Args[1]->getType();\n  Args[1] = EmitToInt(CGF, Args[1], T, IntType);\n\n  llvm::Value *Result = CGF.Builder.CreateAtomicRMW(\n      Kind, Args[0], Args[1], Ordering);\n  return EmitFromInt(CGF, Result, T, ValueType);\n}\n\nstatic Value *EmitNontemporalStore(CodeGenFunction &CGF, const CallExpr *E) {\n  Value *Val = CGF.EmitScalarExpr(E->getArg(0));\n  Value *Address = CGF.EmitScalarExpr(E->getArg(1));\n\n  // Convert the type of the pointer to a pointer to the stored type.\n  Val = CGF.EmitToMemory(Val, E->getArg(0)->getType());\n  Value *BC = CGF.Builder.CreateBitCast(\n      Address, llvm::PointerType::getUnqual(Val->getType()), \"cast\");\n  LValue LV = CGF.MakeNaturalAlignAddrLValue(BC, E->getArg(0)->getType());\n  LV.setNontemporal(true);\n  CGF.EmitStoreOfScalar(Val, LV, false);\n  return nullptr;\n}\n\nstatic Value *EmitNontemporalLoad(CodeGenFunction &CGF, const CallExpr *E) {\n  Value *Address = CGF.EmitScalarExpr(E->getArg(0));\n\n  LValue LV = CGF.MakeNaturalAlignAddrLValue(Address, E->getType());\n  LV.setNontemporal(true);\n  return CGF.EmitLoadOfScalar(LV, E->getExprLoc());\n}\n\nstatic RValue EmitBinaryAtomic(CodeGenFunction &CGF,\n                               llvm::AtomicRMWInst::BinOp Kind,\n                               const CallExpr *E) {\n  return RValue::get(MakeBinaryAtomicValue(CGF, Kind, E));\n}\n\n/// Utility to insert an atomic instruction based Intrinsic::ID and\n/// the expression node, where the return value is the result of the\n/// operation.\nstatic RValue EmitBinaryAtomicPost(CodeGenFunction &CGF,\n                                   llvm::AtomicRMWInst::BinOp Kind,\n                                   const CallExpr *E,\n                                   Instruction::BinaryOps Op,\n                                   bool Invert = false) {\n  QualType T = E->getType();\n  assert(E->getArg(0)->getType()->isPointerType());\n  assert(CGF.getContext().hasSameUnqualifiedType(T,\n                                  E->getArg(0)->getType()->getPointeeType()));\n  assert(CGF.getContext().hasSameUnqualifiedType(T, E->getArg(1)->getType()));\n\n  llvm::Value *DestPtr = CGF.EmitScalarExpr(E->getArg(0));\n  unsigned AddrSpace = DestPtr->getType()->getPointerAddressSpace();\n\n  llvm::IntegerType *IntType =\n    llvm::IntegerType::get(CGF.getLLVMContext(),\n                           CGF.getContext().getTypeSize(T));\n  llvm::Type *IntPtrType = IntType->getPointerTo(AddrSpace);\n\n  llvm::Value *Args[2];\n  Args[1] = CGF.EmitScalarExpr(E->getArg(1));\n  llvm::Type *ValueType = Args[1]->getType();\n  Args[1] = EmitToInt(CGF, Args[1], T, IntType);\n  Args[0] = CGF.Builder.CreateBitCast(DestPtr, IntPtrType);\n\n  llvm::Value *Result = CGF.Builder.CreateAtomicRMW(\n      Kind, Args[0], Args[1], llvm::AtomicOrdering::SequentiallyConsistent);\n  Result = CGF.Builder.CreateBinOp(Op, Result, Args[1]);\n  if (Invert)\n    Result =\n        CGF.Builder.CreateBinOp(llvm::Instruction::Xor, Result,\n                                llvm::ConstantInt::getAllOnesValue(IntType));\n  Result = EmitFromInt(CGF, Result, T, ValueType);\n  return RValue::get(Result);\n}\n\n/// Utility to insert an atomic cmpxchg instruction.\n///\n/// @param CGF The current codegen function.\n/// @param E   Builtin call expression to convert to cmpxchg.\n///            arg0 - address to operate on\n///            arg1 - value to compare with\n///            arg2 - new value\n/// @param ReturnBool Specifies whether to return success flag of\n///                   cmpxchg result or the old value.\n///\n/// @returns result of cmpxchg, according to ReturnBool\n///\n/// Note: In order to lower Microsoft's _InterlockedCompareExchange* intrinsics\n/// invoke the function EmitAtomicCmpXchgForMSIntrin.\nstatic Value *MakeAtomicCmpXchgValue(CodeGenFunction &CGF, const CallExpr *E,\n                                     bool ReturnBool) {\n  QualType T = ReturnBool ? E->getArg(1)->getType() : E->getType();\n  llvm::Value *DestPtr = CGF.EmitScalarExpr(E->getArg(0));\n  unsigned AddrSpace = DestPtr->getType()->getPointerAddressSpace();\n\n  llvm::IntegerType *IntType = llvm::IntegerType::get(\n      CGF.getLLVMContext(), CGF.getContext().getTypeSize(T));\n  llvm::Type *IntPtrType = IntType->getPointerTo(AddrSpace);\n\n  Value *Args[3];\n  Args[0] = CGF.Builder.CreateBitCast(DestPtr, IntPtrType);\n  Args[1] = CGF.EmitScalarExpr(E->getArg(1));\n  llvm::Type *ValueType = Args[1]->getType();\n  Args[1] = EmitToInt(CGF, Args[1], T, IntType);\n  Args[2] = EmitToInt(CGF, CGF.EmitScalarExpr(E->getArg(2)), T, IntType);\n\n  Value *Pair = CGF.Builder.CreateAtomicCmpXchg(\n      Args[0], Args[1], Args[2], llvm::AtomicOrdering::SequentiallyConsistent,\n      llvm::AtomicOrdering::SequentiallyConsistent);\n  if (ReturnBool)\n    // Extract boolean success flag and zext it to int.\n    return CGF.Builder.CreateZExt(CGF.Builder.CreateExtractValue(Pair, 1),\n                                  CGF.ConvertType(E->getType()));\n  else\n    // Extract old value and emit it using the same type as compare value.\n    return EmitFromInt(CGF, CGF.Builder.CreateExtractValue(Pair, 0), T,\n                       ValueType);\n}\n\n/// This function should be invoked to emit atomic cmpxchg for Microsoft's\n/// _InterlockedCompareExchange* intrinsics which have the following signature:\n/// T _InterlockedCompareExchange(T volatile *Destination,\n///                               T Exchange,\n///                               T Comparand);\n///\n/// Whereas the llvm 'cmpxchg' instruction has the following syntax:\n/// cmpxchg *Destination, Comparand, Exchange.\n/// So we need to swap Comparand and Exchange when invoking\n/// CreateAtomicCmpXchg. That is the reason we could not use the above utility\n/// function MakeAtomicCmpXchgValue since it expects the arguments to be\n/// already swapped.\n\nstatic\nValue *EmitAtomicCmpXchgForMSIntrin(CodeGenFunction &CGF, const CallExpr *E,\n    AtomicOrdering SuccessOrdering = AtomicOrdering::SequentiallyConsistent) {\n  assert(E->getArg(0)->getType()->isPointerType());\n  assert(CGF.getContext().hasSameUnqualifiedType(\n      E->getType(), E->getArg(0)->getType()->getPointeeType()));\n  assert(CGF.getContext().hasSameUnqualifiedType(E->getType(),\n                                                 E->getArg(1)->getType()));\n  assert(CGF.getContext().hasSameUnqualifiedType(E->getType(),\n                                                 E->getArg(2)->getType()));\n\n  auto *Destination = CGF.EmitScalarExpr(E->getArg(0));\n  auto *Comparand = CGF.EmitScalarExpr(E->getArg(2));\n  auto *Exchange = CGF.EmitScalarExpr(E->getArg(1));\n\n  // For Release ordering, the failure ordering should be Monotonic.\n  auto FailureOrdering = SuccessOrdering == AtomicOrdering::Release ?\n                         AtomicOrdering::Monotonic :\n                         SuccessOrdering;\n\n  // The atomic instruction is marked volatile for consistency with MSVC. This\n  // blocks the few atomics optimizations that LLVM has. If we want to optimize\n  // _Interlocked* operations in the future, we will have to remove the volatile\n  // marker.\n  auto *Result = CGF.Builder.CreateAtomicCmpXchg(\n                   Destination, Comparand, Exchange,\n                   SuccessOrdering, FailureOrdering);\n  Result->setVolatile(true);\n  return CGF.Builder.CreateExtractValue(Result, 0);\n}\n\n// 64-bit Microsoft platforms support 128 bit cmpxchg operations. They are\n// prototyped like this:\n//\n// unsigned char _InterlockedCompareExchange128...(\n//     __int64 volatile * _Destination,\n//     __int64 _ExchangeHigh,\n//     __int64 _ExchangeLow,\n//     __int64 * _ComparandResult);\nstatic Value *EmitAtomicCmpXchg128ForMSIntrin(CodeGenFunction &CGF,\n                                              const CallExpr *E,\n                                              AtomicOrdering SuccessOrdering) {\n  assert(E->getNumArgs() == 4);\n  llvm::Value *Destination = CGF.EmitScalarExpr(E->getArg(0));\n  llvm::Value *ExchangeHigh = CGF.EmitScalarExpr(E->getArg(1));\n  llvm::Value *ExchangeLow = CGF.EmitScalarExpr(E->getArg(2));\n  llvm::Value *ComparandPtr = CGF.EmitScalarExpr(E->getArg(3));\n\n  assert(Destination->getType()->isPointerTy());\n  assert(!ExchangeHigh->getType()->isPointerTy());\n  assert(!ExchangeLow->getType()->isPointerTy());\n  assert(ComparandPtr->getType()->isPointerTy());\n\n  // For Release ordering, the failure ordering should be Monotonic.\n  auto FailureOrdering = SuccessOrdering == AtomicOrdering::Release\n                             ? AtomicOrdering::Monotonic\n                             : SuccessOrdering;\n\n  // Convert to i128 pointers and values.\n  llvm::Type *Int128Ty = llvm::IntegerType::get(CGF.getLLVMContext(), 128);\n  llvm::Type *Int128PtrTy = Int128Ty->getPointerTo();\n  Destination = CGF.Builder.CreateBitCast(Destination, Int128PtrTy);\n  Address ComparandResult(CGF.Builder.CreateBitCast(ComparandPtr, Int128PtrTy),\n                          CGF.getContext().toCharUnitsFromBits(128));\n\n  // (((i128)hi) << 64) | ((i128)lo)\n  ExchangeHigh = CGF.Builder.CreateZExt(ExchangeHigh, Int128Ty);\n  ExchangeLow = CGF.Builder.CreateZExt(ExchangeLow, Int128Ty);\n  ExchangeHigh =\n      CGF.Builder.CreateShl(ExchangeHigh, llvm::ConstantInt::get(Int128Ty, 64));\n  llvm::Value *Exchange = CGF.Builder.CreateOr(ExchangeHigh, ExchangeLow);\n\n  // Load the comparand for the instruction.\n  llvm::Value *Comparand = CGF.Builder.CreateLoad(ComparandResult);\n\n  auto *CXI = CGF.Builder.CreateAtomicCmpXchg(Destination, Comparand, Exchange,\n                                              SuccessOrdering, FailureOrdering);\n\n  // The atomic instruction is marked volatile for consistency with MSVC. This\n  // blocks the few atomics optimizations that LLVM has. If we want to optimize\n  // _Interlocked* operations in the future, we will have to remove the volatile\n  // marker.\n  CXI->setVolatile(true);\n\n  // Store the result as an outparameter.\n  CGF.Builder.CreateStore(CGF.Builder.CreateExtractValue(CXI, 0),\n                          ComparandResult);\n\n  // Get the success boolean and zero extend it to i8.\n  Value *Success = CGF.Builder.CreateExtractValue(CXI, 1);\n  return CGF.Builder.CreateZExt(Success, CGF.Int8Ty);\n}\n\nstatic Value *EmitAtomicIncrementValue(CodeGenFunction &CGF, const CallExpr *E,\n    AtomicOrdering Ordering = AtomicOrdering::SequentiallyConsistent) {\n  assert(E->getArg(0)->getType()->isPointerType());\n\n  auto *IntTy = CGF.ConvertType(E->getType());\n  auto *Result = CGF.Builder.CreateAtomicRMW(\n                   AtomicRMWInst::Add,\n                   CGF.EmitScalarExpr(E->getArg(0)),\n                   ConstantInt::get(IntTy, 1),\n                   Ordering);\n  return CGF.Builder.CreateAdd(Result, ConstantInt::get(IntTy, 1));\n}\n\nstatic Value *EmitAtomicDecrementValue(CodeGenFunction &CGF, const CallExpr *E,\n    AtomicOrdering Ordering = AtomicOrdering::SequentiallyConsistent) {\n  assert(E->getArg(0)->getType()->isPointerType());\n\n  auto *IntTy = CGF.ConvertType(E->getType());\n  auto *Result = CGF.Builder.CreateAtomicRMW(\n                   AtomicRMWInst::Sub,\n                   CGF.EmitScalarExpr(E->getArg(0)),\n                   ConstantInt::get(IntTy, 1),\n                   Ordering);\n  return CGF.Builder.CreateSub(Result, ConstantInt::get(IntTy, 1));\n}\n\n// Build a plain volatile load.\nstatic Value *EmitISOVolatileLoad(CodeGenFunction &CGF, const CallExpr *E) {\n  Value *Ptr = CGF.EmitScalarExpr(E->getArg(0));\n  QualType ElTy = E->getArg(0)->getType()->getPointeeType();\n  CharUnits LoadSize = CGF.getContext().getTypeSizeInChars(ElTy);\n  llvm::Type *ITy =\n      llvm::IntegerType::get(CGF.getLLVMContext(), LoadSize.getQuantity() * 8);\n  Ptr = CGF.Builder.CreateBitCast(Ptr, ITy->getPointerTo());\n  llvm::LoadInst *Load = CGF.Builder.CreateAlignedLoad(Ptr, LoadSize);\n  Load->setVolatile(true);\n  return Load;\n}\n\n// Build a plain volatile store.\nstatic Value *EmitISOVolatileStore(CodeGenFunction &CGF, const CallExpr *E) {\n  Value *Ptr = CGF.EmitScalarExpr(E->getArg(0));\n  Value *Value = CGF.EmitScalarExpr(E->getArg(1));\n  QualType ElTy = E->getArg(0)->getType()->getPointeeType();\n  CharUnits StoreSize = CGF.getContext().getTypeSizeInChars(ElTy);\n  llvm::Type *ITy =\n      llvm::IntegerType::get(CGF.getLLVMContext(), StoreSize.getQuantity() * 8);\n  Ptr = CGF.Builder.CreateBitCast(Ptr, ITy->getPointerTo());\n  llvm::StoreInst *Store =\n      CGF.Builder.CreateAlignedStore(Value, Ptr, StoreSize);\n  Store->setVolatile(true);\n  return Store;\n}\n\n// Emit a simple mangled intrinsic that has 1 argument and a return type\n// matching the argument type. Depending on mode, this may be a constrained\n// floating-point intrinsic.\nstatic Value *emitUnaryMaybeConstrainedFPBuiltin(CodeGenFunction &CGF,\n                                const CallExpr *E, unsigned IntrinsicID,\n                                unsigned ConstrainedIntrinsicID) {\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n\n  if (CGF.Builder.getIsFPConstrained()) {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, E);\n    Function *F = CGF.CGM.getIntrinsic(ConstrainedIntrinsicID, Src0->getType());\n    return CGF.Builder.CreateConstrainedFPCall(F, { Src0 });\n  } else {\n    Function *F = CGF.CGM.getIntrinsic(IntrinsicID, Src0->getType());\n    return CGF.Builder.CreateCall(F, Src0);\n  }\n}\n\n// Emit an intrinsic that has 2 operands of the same type as its result.\n// Depending on mode, this may be a constrained floating-point intrinsic.\nstatic Value *emitBinaryMaybeConstrainedFPBuiltin(CodeGenFunction &CGF,\n                                const CallExpr *E, unsigned IntrinsicID,\n                                unsigned ConstrainedIntrinsicID) {\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n  llvm::Value *Src1 = CGF.EmitScalarExpr(E->getArg(1));\n\n  if (CGF.Builder.getIsFPConstrained()) {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, E);\n    Function *F = CGF.CGM.getIntrinsic(ConstrainedIntrinsicID, Src0->getType());\n    return CGF.Builder.CreateConstrainedFPCall(F, { Src0, Src1 });\n  } else {\n    Function *F = CGF.CGM.getIntrinsic(IntrinsicID, Src0->getType());\n    return CGF.Builder.CreateCall(F, { Src0, Src1 });\n  }\n}\n\n// Emit an intrinsic that has 3 operands of the same type as its result.\n// Depending on mode, this may be a constrained floating-point intrinsic.\nstatic Value *emitTernaryMaybeConstrainedFPBuiltin(CodeGenFunction &CGF,\n                                 const CallExpr *E, unsigned IntrinsicID,\n                                 unsigned ConstrainedIntrinsicID) {\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n  llvm::Value *Src1 = CGF.EmitScalarExpr(E->getArg(1));\n  llvm::Value *Src2 = CGF.EmitScalarExpr(E->getArg(2));\n\n  if (CGF.Builder.getIsFPConstrained()) {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, E);\n    Function *F = CGF.CGM.getIntrinsic(ConstrainedIntrinsicID, Src0->getType());\n    return CGF.Builder.CreateConstrainedFPCall(F, { Src0, Src1, Src2 });\n  } else {\n    Function *F = CGF.CGM.getIntrinsic(IntrinsicID, Src0->getType());\n    return CGF.Builder.CreateCall(F, { Src0, Src1, Src2 });\n  }\n}\n\n// Emit an intrinsic where all operands are of the same type as the result.\n// Depending on mode, this may be a constrained floating-point intrinsic.\nstatic Value *emitCallMaybeConstrainedFPBuiltin(CodeGenFunction &CGF,\n                                                unsigned IntrinsicID,\n                                                unsigned ConstrainedIntrinsicID,\n                                                llvm::Type *Ty,\n                                                ArrayRef<Value *> Args) {\n  Function *F;\n  if (CGF.Builder.getIsFPConstrained())\n    F = CGF.CGM.getIntrinsic(ConstrainedIntrinsicID, Ty);\n  else\n    F = CGF.CGM.getIntrinsic(IntrinsicID, Ty);\n\n  if (CGF.Builder.getIsFPConstrained())\n    return CGF.Builder.CreateConstrainedFPCall(F, Args);\n  else\n    return CGF.Builder.CreateCall(F, Args);\n}\n\n// Emit a simple mangled intrinsic that has 1 argument and a return type\n// matching the argument type.\nstatic Value *emitUnaryBuiltin(CodeGenFunction &CGF,\n                               const CallExpr *E,\n                               unsigned IntrinsicID) {\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n\n  Function *F = CGF.CGM.getIntrinsic(IntrinsicID, Src0->getType());\n  return CGF.Builder.CreateCall(F, Src0);\n}\n\n// Emit an intrinsic that has 2 operands of the same type as its result.\nstatic Value *emitBinaryBuiltin(CodeGenFunction &CGF,\n                                const CallExpr *E,\n                                unsigned IntrinsicID) {\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n  llvm::Value *Src1 = CGF.EmitScalarExpr(E->getArg(1));\n\n  Function *F = CGF.CGM.getIntrinsic(IntrinsicID, Src0->getType());\n  return CGF.Builder.CreateCall(F, { Src0, Src1 });\n}\n\n// Emit an intrinsic that has 3 operands of the same type as its result.\nstatic Value *emitTernaryBuiltin(CodeGenFunction &CGF,\n                                 const CallExpr *E,\n                                 unsigned IntrinsicID) {\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n  llvm::Value *Src1 = CGF.EmitScalarExpr(E->getArg(1));\n  llvm::Value *Src2 = CGF.EmitScalarExpr(E->getArg(2));\n\n  Function *F = CGF.CGM.getIntrinsic(IntrinsicID, Src0->getType());\n  return CGF.Builder.CreateCall(F, { Src0, Src1, Src2 });\n}\n\n// Emit an intrinsic that has 1 float or double operand, and 1 integer.\nstatic Value *emitFPIntBuiltin(CodeGenFunction &CGF,\n                               const CallExpr *E,\n                               unsigned IntrinsicID) {\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n  llvm::Value *Src1 = CGF.EmitScalarExpr(E->getArg(1));\n\n  Function *F = CGF.CGM.getIntrinsic(IntrinsicID, Src0->getType());\n  return CGF.Builder.CreateCall(F, {Src0, Src1});\n}\n\n// Emit an intrinsic that has overloaded integer result and fp operand.\nstatic Value *\nemitMaybeConstrainedFPToIntRoundBuiltin(CodeGenFunction &CGF, const CallExpr *E,\n                                        unsigned IntrinsicID,\n                                        unsigned ConstrainedIntrinsicID) {\n  llvm::Type *ResultType = CGF.ConvertType(E->getType());\n  llvm::Value *Src0 = CGF.EmitScalarExpr(E->getArg(0));\n\n  if (CGF.Builder.getIsFPConstrained()) {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, E);\n    Function *F = CGF.CGM.getIntrinsic(ConstrainedIntrinsicID,\n                                       {ResultType, Src0->getType()});\n    return CGF.Builder.CreateConstrainedFPCall(F, {Src0});\n  } else {\n    Function *F =\n        CGF.CGM.getIntrinsic(IntrinsicID, {ResultType, Src0->getType()});\n    return CGF.Builder.CreateCall(F, Src0);\n  }\n}\n\n/// EmitFAbs - Emit a call to @llvm.fabs().\nstatic Value *EmitFAbs(CodeGenFunction &CGF, Value *V) {\n  Function *F = CGF.CGM.getIntrinsic(Intrinsic::fabs, V->getType());\n  llvm::CallInst *Call = CGF.Builder.CreateCall(F, V);\n  Call->setDoesNotAccessMemory();\n  return Call;\n}\n\n/// Emit the computation of the sign bit for a floating point value. Returns\n/// the i1 sign bit value.\nstatic Value *EmitSignBit(CodeGenFunction &CGF, Value *V) {\n  LLVMContext &C = CGF.CGM.getLLVMContext();\n\n  llvm::Type *Ty = V->getType();\n  int Width = Ty->getPrimitiveSizeInBits();\n  llvm::Type *IntTy = llvm::IntegerType::get(C, Width);\n  V = CGF.Builder.CreateBitCast(V, IntTy);\n  if (Ty->isPPC_FP128Ty()) {\n    // We want the sign bit of the higher-order double. The bitcast we just\n    // did works as if the double-double was stored to memory and then\n    // read as an i128. The \"store\" will put the higher-order double in the\n    // lower address in both little- and big-Endian modes, but the \"load\"\n    // will treat those bits as a different part of the i128: the low bits in\n    // little-Endian, the high bits in big-Endian. Therefore, on big-Endian\n    // we need to shift the high bits down to the low before truncating.\n    Width >>= 1;\n    if (CGF.getTarget().isBigEndian()) {\n      Value *ShiftCst = llvm::ConstantInt::get(IntTy, Width);\n      V = CGF.Builder.CreateLShr(V, ShiftCst);\n    }\n    // We are truncating value in order to extract the higher-order\n    // double, which we will be using to extract the sign from.\n    IntTy = llvm::IntegerType::get(C, Width);\n    V = CGF.Builder.CreateTrunc(V, IntTy);\n  }\n  Value *Zero = llvm::Constant::getNullValue(IntTy);\n  return CGF.Builder.CreateICmpSLT(V, Zero);\n}\n\nstatic RValue emitLibraryCall(CodeGenFunction &CGF, const FunctionDecl *FD,\n                              const CallExpr *E, llvm::Constant *calleeValue) {\n  CGCallee callee = CGCallee::forDirect(calleeValue, GlobalDecl(FD));\n  return CGF.EmitCall(E->getCallee()->getType(), callee, E, ReturnValueSlot());\n}\n\n/// Emit a call to llvm.{sadd,uadd,ssub,usub,smul,umul}.with.overflow.*\n/// depending on IntrinsicID.\n///\n/// \\arg CGF The current codegen function.\n/// \\arg IntrinsicID The ID for the Intrinsic we wish to generate.\n/// \\arg X The first argument to the llvm.*.with.overflow.*.\n/// \\arg Y The second argument to the llvm.*.with.overflow.*.\n/// \\arg Carry The carry returned by the llvm.*.with.overflow.*.\n/// \\returns The result (i.e. sum/product) returned by the intrinsic.\nstatic llvm::Value *EmitOverflowIntrinsic(CodeGenFunction &CGF,\n                                          const llvm::Intrinsic::ID IntrinsicID,\n                                          llvm::Value *X, llvm::Value *Y,\n                                          llvm::Value *&Carry) {\n  // Make sure we have integers of the same width.\n  assert(X->getType() == Y->getType() &&\n         \"Arguments must be the same type. (Did you forget to make sure both \"\n         \"arguments have the same integer width?)\");\n\n  Function *Callee = CGF.CGM.getIntrinsic(IntrinsicID, X->getType());\n  llvm::Value *Tmp = CGF.Builder.CreateCall(Callee, {X, Y});\n  Carry = CGF.Builder.CreateExtractValue(Tmp, 1);\n  return CGF.Builder.CreateExtractValue(Tmp, 0);\n}\n\nstatic Value *emitRangedBuiltin(CodeGenFunction &CGF,\n                                unsigned IntrinsicID,\n                                int low, int high) {\n    llvm::MDBuilder MDHelper(CGF.getLLVMContext());\n    llvm::MDNode *RNode = MDHelper.createRange(APInt(32, low), APInt(32, high));\n    Function *F = CGF.CGM.getIntrinsic(IntrinsicID, {});\n    llvm::Instruction *Call = CGF.Builder.CreateCall(F);\n    Call->setMetadata(llvm::LLVMContext::MD_range, RNode);\n    return Call;\n}\n\nnamespace {\n  struct WidthAndSignedness {\n    unsigned Width;\n    bool Signed;\n  };\n}\n\nstatic WidthAndSignedness\ngetIntegerWidthAndSignedness(const clang::ASTContext &context,\n                             const clang::QualType Type) {\n  assert(Type->isIntegerType() && \"Given type is not an integer.\");\n  unsigned Width = Type->isBooleanType()  ? 1\n                   : Type->isExtIntType() ? context.getIntWidth(Type)\n                                          : context.getTypeInfo(Type).Width;\n  bool Signed = Type->isSignedIntegerType();\n  return {Width, Signed};\n}\n\n// Given one or more integer types, this function produces an integer type that\n// encompasses them: any value in one of the given types could be expressed in\n// the encompassing type.\nstatic struct WidthAndSignedness\nEncompassingIntegerType(ArrayRef<struct WidthAndSignedness> Types) {\n  assert(Types.size() > 0 && \"Empty list of types.\");\n\n  // If any of the given types is signed, we must return a signed type.\n  bool Signed = false;\n  for (const auto &Type : Types) {\n    Signed |= Type.Signed;\n  }\n\n  // The encompassing type must have a width greater than or equal to the width\n  // of the specified types.  Additionally, if the encompassing type is signed,\n  // its width must be strictly greater than the width of any unsigned types\n  // given.\n  unsigned Width = 0;\n  for (const auto &Type : Types) {\n    unsigned MinWidth = Type.Width + (Signed && !Type.Signed);\n    if (Width < MinWidth) {\n      Width = MinWidth;\n    }\n  }\n\n  return {Width, Signed};\n}\n\nValue *CodeGenFunction::EmitVAStartEnd(Value *ArgValue, bool IsStart) {\n  llvm::Type *DestType = Int8PtrTy;\n  if (ArgValue->getType() != DestType)\n    ArgValue =\n        Builder.CreateBitCast(ArgValue, DestType, ArgValue->getName().data());\n\n  Intrinsic::ID inst = IsStart ? Intrinsic::vastart : Intrinsic::vaend;\n  return Builder.CreateCall(CGM.getIntrinsic(inst), ArgValue);\n}\n\n/// Checks if using the result of __builtin_object_size(p, @p From) in place of\n/// __builtin_object_size(p, @p To) is correct\nstatic bool areBOSTypesCompatible(int From, int To) {\n  // Note: Our __builtin_object_size implementation currently treats Type=0 and\n  // Type=2 identically. Encoding this implementation detail here may make\n  // improving __builtin_object_size difficult in the future, so it's omitted.\n  return From == To || (From == 0 && To == 1) || (From == 3 && To == 2);\n}\n\nstatic llvm::Value *\ngetDefaultBuiltinObjectSizeResult(unsigned Type, llvm::IntegerType *ResType) {\n  return ConstantInt::get(ResType, (Type & 2) ? 0 : -1, /*isSigned=*/true);\n}\n\nllvm::Value *\nCodeGenFunction::evaluateOrEmitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                                 llvm::IntegerType *ResType,\n                                                 llvm::Value *EmittedE,\n                                                 bool IsDynamic) {\n  uint64_t ObjectSize;\n  if (!E->tryEvaluateObjectSize(ObjectSize, getContext(), Type))\n    return emitBuiltinObjectSize(E, Type, ResType, EmittedE, IsDynamic);\n  return ConstantInt::get(ResType, ObjectSize, /*isSigned=*/true);\n}\n\n/// Returns a Value corresponding to the size of the given expression.\n/// This Value may be either of the following:\n///   - A llvm::Argument (if E is a param with the pass_object_size attribute on\n///     it)\n///   - A call to the @llvm.objectsize intrinsic\n///\n/// EmittedE is the result of emitting `E` as a scalar expr. If it's non-null\n/// and we wouldn't otherwise try to reference a pass_object_size parameter,\n/// we'll call @llvm.objectsize on EmittedE, rather than emitting E.\nllvm::Value *\nCodeGenFunction::emitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                       llvm::IntegerType *ResType,\n                                       llvm::Value *EmittedE, bool IsDynamic) {\n  // We need to reference an argument if the pointer is a parameter with the\n  // pass_object_size attribute.\n  if (auto *D = dyn_cast<DeclRefExpr>(E->IgnoreParenImpCasts())) {\n    auto *Param = dyn_cast<ParmVarDecl>(D->getDecl());\n    auto *PS = D->getDecl()->getAttr<PassObjectSizeAttr>();\n    if (Param != nullptr && PS != nullptr &&\n        areBOSTypesCompatible(PS->getType(), Type)) {\n      auto Iter = SizeArguments.find(Param);\n      assert(Iter != SizeArguments.end());\n\n      const ImplicitParamDecl *D = Iter->second;\n      auto DIter = LocalDeclMap.find(D);\n      assert(DIter != LocalDeclMap.end());\n\n      return EmitLoadOfScalar(DIter->second, /*Volatile=*/false,\n                              getContext().getSizeType(), E->getBeginLoc());\n    }\n  }\n\n  // LLVM can't handle Type=3 appropriately, and __builtin_object_size shouldn't\n  // evaluate E for side-effects. In either case, we shouldn't lower to\n  // @llvm.objectsize.\n  if (Type == 3 || (!EmittedE && E->HasSideEffects(getContext())))\n    return getDefaultBuiltinObjectSizeResult(Type, ResType);\n\n  Value *Ptr = EmittedE ? EmittedE : EmitScalarExpr(E);\n  assert(Ptr->getType()->isPointerTy() &&\n         \"Non-pointer passed to __builtin_object_size?\");\n\n  Function *F =\n      CGM.getIntrinsic(Intrinsic::objectsize, {ResType, Ptr->getType()});\n\n  // LLVM only supports 0 and 2, make sure that we pass along that as a boolean.\n  Value *Min = Builder.getInt1((Type & 2) != 0);\n  // For GCC compatibility, __builtin_object_size treat NULL as unknown size.\n  Value *NullIsUnknown = Builder.getTrue();\n  Value *Dynamic = Builder.getInt1(IsDynamic);\n  return Builder.CreateCall(F, {Ptr, Min, NullIsUnknown, Dynamic});\n}\n\nnamespace {\n/// A struct to generically describe a bit test intrinsic.\nstruct BitTest {\n  enum ActionKind : uint8_t { TestOnly, Complement, Reset, Set };\n  enum InterlockingKind : uint8_t {\n    Unlocked,\n    Sequential,\n    Acquire,\n    Release,\n    NoFence\n  };\n\n  ActionKind Action;\n  InterlockingKind Interlocking;\n  bool Is64Bit;\n\n  static BitTest decodeBitTestBuiltin(unsigned BuiltinID);\n};\n} // namespace\n\nBitTest BitTest::decodeBitTestBuiltin(unsigned BuiltinID) {\n  switch (BuiltinID) {\n    // Main portable variants.\n  case Builtin::BI_bittest:\n    return {TestOnly, Unlocked, false};\n  case Builtin::BI_bittestandcomplement:\n    return {Complement, Unlocked, false};\n  case Builtin::BI_bittestandreset:\n    return {Reset, Unlocked, false};\n  case Builtin::BI_bittestandset:\n    return {Set, Unlocked, false};\n  case Builtin::BI_interlockedbittestandreset:\n    return {Reset, Sequential, false};\n  case Builtin::BI_interlockedbittestandset:\n    return {Set, Sequential, false};\n\n    // X86-specific 64-bit variants.\n  case Builtin::BI_bittest64:\n    return {TestOnly, Unlocked, true};\n  case Builtin::BI_bittestandcomplement64:\n    return {Complement, Unlocked, true};\n  case Builtin::BI_bittestandreset64:\n    return {Reset, Unlocked, true};\n  case Builtin::BI_bittestandset64:\n    return {Set, Unlocked, true};\n  case Builtin::BI_interlockedbittestandreset64:\n    return {Reset, Sequential, true};\n  case Builtin::BI_interlockedbittestandset64:\n    return {Set, Sequential, true};\n\n    // ARM/AArch64-specific ordering variants.\n  case Builtin::BI_interlockedbittestandset_acq:\n    return {Set, Acquire, false};\n  case Builtin::BI_interlockedbittestandset_rel:\n    return {Set, Release, false};\n  case Builtin::BI_interlockedbittestandset_nf:\n    return {Set, NoFence, false};\n  case Builtin::BI_interlockedbittestandreset_acq:\n    return {Reset, Acquire, false};\n  case Builtin::BI_interlockedbittestandreset_rel:\n    return {Reset, Release, false};\n  case Builtin::BI_interlockedbittestandreset_nf:\n    return {Reset, NoFence, false};\n  }\n  llvm_unreachable(\"expected only bittest intrinsics\");\n}\n\nstatic char bitActionToX86BTCode(BitTest::ActionKind A) {\n  switch (A) {\n  case BitTest::TestOnly:   return '\\0';\n  case BitTest::Complement: return 'c';\n  case BitTest::Reset:      return 'r';\n  case BitTest::Set:        return 's';\n  }\n  llvm_unreachable(\"invalid action\");\n}\n\nstatic llvm::Value *EmitX86BitTestIntrinsic(CodeGenFunction &CGF,\n                                            BitTest BT,\n                                            const CallExpr *E, Value *BitBase,\n                                            Value *BitPos) {\n  char Action = bitActionToX86BTCode(BT.Action);\n  char SizeSuffix = BT.Is64Bit ? 'q' : 'l';\n\n  // Build the assembly.\n  SmallString<64> Asm;\n  raw_svector_ostream AsmOS(Asm);\n  if (BT.Interlocking != BitTest::Unlocked)\n    AsmOS << \"lock \";\n  AsmOS << \"bt\";\n  if (Action)\n    AsmOS << Action;\n  AsmOS << SizeSuffix << \" $2, ($1)\";\n\n  // Build the constraints. FIXME: We should support immediates when possible.\n  std::string Constraints = \"={@ccc},r,r,~{cc},~{memory}\";\n  std::string MachineClobbers = CGF.getTarget().getClobbers();\n  if (!MachineClobbers.empty()) {\n    Constraints += ',';\n    Constraints += MachineClobbers;\n  }\n  llvm::IntegerType *IntType = llvm::IntegerType::get(\n      CGF.getLLVMContext(),\n      CGF.getContext().getTypeSize(E->getArg(1)->getType()));\n  llvm::Type *IntPtrType = IntType->getPointerTo();\n  llvm::FunctionType *FTy =\n      llvm::FunctionType::get(CGF.Int8Ty, {IntPtrType, IntType}, false);\n\n  llvm::InlineAsm *IA =\n      llvm::InlineAsm::get(FTy, Asm, Constraints, /*hasSideEffects=*/true);\n  return CGF.Builder.CreateCall(IA, {BitBase, BitPos});\n}\n\nstatic llvm::AtomicOrdering\ngetBitTestAtomicOrdering(BitTest::InterlockingKind I) {\n  switch (I) {\n  case BitTest::Unlocked:   return llvm::AtomicOrdering::NotAtomic;\n  case BitTest::Sequential: return llvm::AtomicOrdering::SequentiallyConsistent;\n  case BitTest::Acquire:    return llvm::AtomicOrdering::Acquire;\n  case BitTest::Release:    return llvm::AtomicOrdering::Release;\n  case BitTest::NoFence:    return llvm::AtomicOrdering::Monotonic;\n  }\n  llvm_unreachable(\"invalid interlocking\");\n}\n\n/// Emit a _bittest* intrinsic. These intrinsics take a pointer to an array of\n/// bits and a bit position and read and optionally modify the bit at that\n/// position. The position index can be arbitrarily large, i.e. it can be larger\n/// than 31 or 63, so we need an indexed load in the general case.\nstatic llvm::Value *EmitBitTestIntrinsic(CodeGenFunction &CGF,\n                                         unsigned BuiltinID,\n                                         const CallExpr *E) {\n  Value *BitBase = CGF.EmitScalarExpr(E->getArg(0));\n  Value *BitPos = CGF.EmitScalarExpr(E->getArg(1));\n\n  BitTest BT = BitTest::decodeBitTestBuiltin(BuiltinID);\n\n  // X86 has special BT, BTC, BTR, and BTS instructions that handle the array\n  // indexing operation internally. Use them if possible.\n  if (CGF.getTarget().getTriple().isX86())\n    return EmitX86BitTestIntrinsic(CGF, BT, E, BitBase, BitPos);\n\n  // Otherwise, use generic code to load one byte and test the bit. Use all but\n  // the bottom three bits as the array index, and the bottom three bits to form\n  // a mask.\n  // Bit = BitBaseI8[BitPos >> 3] & (1 << (BitPos & 0x7)) != 0;\n  Value *ByteIndex = CGF.Builder.CreateAShr(\n      BitPos, llvm::ConstantInt::get(BitPos->getType(), 3), \"bittest.byteidx\");\n  Value *BitBaseI8 = CGF.Builder.CreatePointerCast(BitBase, CGF.Int8PtrTy);\n  Address ByteAddr(CGF.Builder.CreateInBoundsGEP(CGF.Int8Ty, BitBaseI8,\n                                                 ByteIndex, \"bittest.byteaddr\"),\n                   CharUnits::One());\n  Value *PosLow =\n      CGF.Builder.CreateAnd(CGF.Builder.CreateTrunc(BitPos, CGF.Int8Ty),\n                            llvm::ConstantInt::get(CGF.Int8Ty, 0x7));\n\n  // The updating instructions will need a mask.\n  Value *Mask = nullptr;\n  if (BT.Action != BitTest::TestOnly) {\n    Mask = CGF.Builder.CreateShl(llvm::ConstantInt::get(CGF.Int8Ty, 1), PosLow,\n                                 \"bittest.mask\");\n  }\n\n  // Check the action and ordering of the interlocked intrinsics.\n  llvm::AtomicOrdering Ordering = getBitTestAtomicOrdering(BT.Interlocking);\n\n  Value *OldByte = nullptr;\n  if (Ordering != llvm::AtomicOrdering::NotAtomic) {\n    // Emit a combined atomicrmw load/store operation for the interlocked\n    // intrinsics.\n    llvm::AtomicRMWInst::BinOp RMWOp = llvm::AtomicRMWInst::Or;\n    if (BT.Action == BitTest::Reset) {\n      Mask = CGF.Builder.CreateNot(Mask);\n      RMWOp = llvm::AtomicRMWInst::And;\n    }\n    OldByte = CGF.Builder.CreateAtomicRMW(RMWOp, ByteAddr.getPointer(), Mask,\n                                          Ordering);\n  } else {\n    // Emit a plain load for the non-interlocked intrinsics.\n    OldByte = CGF.Builder.CreateLoad(ByteAddr, \"bittest.byte\");\n    Value *NewByte = nullptr;\n    switch (BT.Action) {\n    case BitTest::TestOnly:\n      // Don't store anything.\n      break;\n    case BitTest::Complement:\n      NewByte = CGF.Builder.CreateXor(OldByte, Mask);\n      break;\n    case BitTest::Reset:\n      NewByte = CGF.Builder.CreateAnd(OldByte, CGF.Builder.CreateNot(Mask));\n      break;\n    case BitTest::Set:\n      NewByte = CGF.Builder.CreateOr(OldByte, Mask);\n      break;\n    }\n    if (NewByte)\n      CGF.Builder.CreateStore(NewByte, ByteAddr);\n  }\n\n  // However we loaded the old byte, either by plain load or atomicrmw, shift\n  // the bit into the low position and mask it to 0 or 1.\n  Value *ShiftedByte = CGF.Builder.CreateLShr(OldByte, PosLow, \"bittest.shr\");\n  return CGF.Builder.CreateAnd(\n      ShiftedByte, llvm::ConstantInt::get(CGF.Int8Ty, 1), \"bittest.res\");\n}\n\nnamespace {\nenum class MSVCSetJmpKind {\n  _setjmpex,\n  _setjmp3,\n  _setjmp\n};\n}\n\n/// MSVC handles setjmp a bit differently on different platforms. On every\n/// architecture except 32-bit x86, the frame address is passed. On x86, extra\n/// parameters can be passed as variadic arguments, but we always pass none.\nstatic RValue EmitMSVCRTSetJmp(CodeGenFunction &CGF, MSVCSetJmpKind SJKind,\n                               const CallExpr *E) {\n  llvm::Value *Arg1 = nullptr;\n  llvm::Type *Arg1Ty = nullptr;\n  StringRef Name;\n  bool IsVarArg = false;\n  if (SJKind == MSVCSetJmpKind::_setjmp3) {\n    Name = \"_setjmp3\";\n    Arg1Ty = CGF.Int32Ty;\n    Arg1 = llvm::ConstantInt::get(CGF.IntTy, 0);\n    IsVarArg = true;\n  } else {\n    Name = SJKind == MSVCSetJmpKind::_setjmp ? \"_setjmp\" : \"_setjmpex\";\n    Arg1Ty = CGF.Int8PtrTy;\n    if (CGF.getTarget().getTriple().getArch() == llvm::Triple::aarch64) {\n      Arg1 = CGF.Builder.CreateCall(\n          CGF.CGM.getIntrinsic(Intrinsic::sponentry, CGF.AllocaInt8PtrTy));\n    } else\n      Arg1 = CGF.Builder.CreateCall(\n          CGF.CGM.getIntrinsic(Intrinsic::frameaddress, CGF.AllocaInt8PtrTy),\n          llvm::ConstantInt::get(CGF.Int32Ty, 0));\n  }\n\n  // Mark the call site and declaration with ReturnsTwice.\n  llvm::Type *ArgTypes[2] = {CGF.Int8PtrTy, Arg1Ty};\n  llvm::AttributeList ReturnsTwiceAttr = llvm::AttributeList::get(\n      CGF.getLLVMContext(), llvm::AttributeList::FunctionIndex,\n      llvm::Attribute::ReturnsTwice);\n  llvm::FunctionCallee SetJmpFn = CGF.CGM.CreateRuntimeFunction(\n      llvm::FunctionType::get(CGF.IntTy, ArgTypes, IsVarArg), Name,\n      ReturnsTwiceAttr, /*Local=*/true);\n\n  llvm::Value *Buf = CGF.Builder.CreateBitOrPointerCast(\n      CGF.EmitScalarExpr(E->getArg(0)), CGF.Int8PtrTy);\n  llvm::Value *Args[] = {Buf, Arg1};\n  llvm::CallBase *CB = CGF.EmitRuntimeCallOrInvoke(SetJmpFn, Args);\n  CB->setAttributes(ReturnsTwiceAttr);\n  return RValue::get(CB);\n}\n\n// Many of MSVC builtins are on x64, ARM and AArch64; to avoid repeating code,\n// we handle them here.\nenum class CodeGenFunction::MSVCIntrin {\n  _BitScanForward,\n  _BitScanReverse,\n  _InterlockedAnd,\n  _InterlockedDecrement,\n  _InterlockedExchange,\n  _InterlockedExchangeAdd,\n  _InterlockedExchangeSub,\n  _InterlockedIncrement,\n  _InterlockedOr,\n  _InterlockedXor,\n  _InterlockedExchangeAdd_acq,\n  _InterlockedExchangeAdd_rel,\n  _InterlockedExchangeAdd_nf,\n  _InterlockedExchange_acq,\n  _InterlockedExchange_rel,\n  _InterlockedExchange_nf,\n  _InterlockedCompareExchange_acq,\n  _InterlockedCompareExchange_rel,\n  _InterlockedCompareExchange_nf,\n  _InterlockedCompareExchange128,\n  _InterlockedCompareExchange128_acq,\n  _InterlockedCompareExchange128_rel,\n  _InterlockedCompareExchange128_nf,\n  _InterlockedOr_acq,\n  _InterlockedOr_rel,\n  _InterlockedOr_nf,\n  _InterlockedXor_acq,\n  _InterlockedXor_rel,\n  _InterlockedXor_nf,\n  _InterlockedAnd_acq,\n  _InterlockedAnd_rel,\n  _InterlockedAnd_nf,\n  _InterlockedIncrement_acq,\n  _InterlockedIncrement_rel,\n  _InterlockedIncrement_nf,\n  _InterlockedDecrement_acq,\n  _InterlockedDecrement_rel,\n  _InterlockedDecrement_nf,\n  __fastfail,\n};\n\nstatic Optional<CodeGenFunction::MSVCIntrin>\ntranslateArmToMsvcIntrin(unsigned BuiltinID) {\n  using MSVCIntrin = CodeGenFunction::MSVCIntrin;\n  switch (BuiltinID) {\n  default:\n    return None;\n  case ARM::BI_BitScanForward:\n  case ARM::BI_BitScanForward64:\n    return MSVCIntrin::_BitScanForward;\n  case ARM::BI_BitScanReverse:\n  case ARM::BI_BitScanReverse64:\n    return MSVCIntrin::_BitScanReverse;\n  case ARM::BI_InterlockedAnd64:\n    return MSVCIntrin::_InterlockedAnd;\n  case ARM::BI_InterlockedExchange64:\n    return MSVCIntrin::_InterlockedExchange;\n  case ARM::BI_InterlockedExchangeAdd64:\n    return MSVCIntrin::_InterlockedExchangeAdd;\n  case ARM::BI_InterlockedExchangeSub64:\n    return MSVCIntrin::_InterlockedExchangeSub;\n  case ARM::BI_InterlockedOr64:\n    return MSVCIntrin::_InterlockedOr;\n  case ARM::BI_InterlockedXor64:\n    return MSVCIntrin::_InterlockedXor;\n  case ARM::BI_InterlockedDecrement64:\n    return MSVCIntrin::_InterlockedDecrement;\n  case ARM::BI_InterlockedIncrement64:\n    return MSVCIntrin::_InterlockedIncrement;\n  case ARM::BI_InterlockedExchangeAdd8_acq:\n  case ARM::BI_InterlockedExchangeAdd16_acq:\n  case ARM::BI_InterlockedExchangeAdd_acq:\n  case ARM::BI_InterlockedExchangeAdd64_acq:\n    return MSVCIntrin::_InterlockedExchangeAdd_acq;\n  case ARM::BI_InterlockedExchangeAdd8_rel:\n  case ARM::BI_InterlockedExchangeAdd16_rel:\n  case ARM::BI_InterlockedExchangeAdd_rel:\n  case ARM::BI_InterlockedExchangeAdd64_rel:\n    return MSVCIntrin::_InterlockedExchangeAdd_rel;\n  case ARM::BI_InterlockedExchangeAdd8_nf:\n  case ARM::BI_InterlockedExchangeAdd16_nf:\n  case ARM::BI_InterlockedExchangeAdd_nf:\n  case ARM::BI_InterlockedExchangeAdd64_nf:\n    return MSVCIntrin::_InterlockedExchangeAdd_nf;\n  case ARM::BI_InterlockedExchange8_acq:\n  case ARM::BI_InterlockedExchange16_acq:\n  case ARM::BI_InterlockedExchange_acq:\n  case ARM::BI_InterlockedExchange64_acq:\n    return MSVCIntrin::_InterlockedExchange_acq;\n  case ARM::BI_InterlockedExchange8_rel:\n  case ARM::BI_InterlockedExchange16_rel:\n  case ARM::BI_InterlockedExchange_rel:\n  case ARM::BI_InterlockedExchange64_rel:\n    return MSVCIntrin::_InterlockedExchange_rel;\n  case ARM::BI_InterlockedExchange8_nf:\n  case ARM::BI_InterlockedExchange16_nf:\n  case ARM::BI_InterlockedExchange_nf:\n  case ARM::BI_InterlockedExchange64_nf:\n    return MSVCIntrin::_InterlockedExchange_nf;\n  case ARM::BI_InterlockedCompareExchange8_acq:\n  case ARM::BI_InterlockedCompareExchange16_acq:\n  case ARM::BI_InterlockedCompareExchange_acq:\n  case ARM::BI_InterlockedCompareExchange64_acq:\n    return MSVCIntrin::_InterlockedCompareExchange_acq;\n  case ARM::BI_InterlockedCompareExchange8_rel:\n  case ARM::BI_InterlockedCompareExchange16_rel:\n  case ARM::BI_InterlockedCompareExchange_rel:\n  case ARM::BI_InterlockedCompareExchange64_rel:\n    return MSVCIntrin::_InterlockedCompareExchange_rel;\n  case ARM::BI_InterlockedCompareExchange8_nf:\n  case ARM::BI_InterlockedCompareExchange16_nf:\n  case ARM::BI_InterlockedCompareExchange_nf:\n  case ARM::BI_InterlockedCompareExchange64_nf:\n    return MSVCIntrin::_InterlockedCompareExchange_nf;\n  case ARM::BI_InterlockedOr8_acq:\n  case ARM::BI_InterlockedOr16_acq:\n  case ARM::BI_InterlockedOr_acq:\n  case ARM::BI_InterlockedOr64_acq:\n    return MSVCIntrin::_InterlockedOr_acq;\n  case ARM::BI_InterlockedOr8_rel:\n  case ARM::BI_InterlockedOr16_rel:\n  case ARM::BI_InterlockedOr_rel:\n  case ARM::BI_InterlockedOr64_rel:\n    return MSVCIntrin::_InterlockedOr_rel;\n  case ARM::BI_InterlockedOr8_nf:\n  case ARM::BI_InterlockedOr16_nf:\n  case ARM::BI_InterlockedOr_nf:\n  case ARM::BI_InterlockedOr64_nf:\n    return MSVCIntrin::_InterlockedOr_nf;\n  case ARM::BI_InterlockedXor8_acq:\n  case ARM::BI_InterlockedXor16_acq:\n  case ARM::BI_InterlockedXor_acq:\n  case ARM::BI_InterlockedXor64_acq:\n    return MSVCIntrin::_InterlockedXor_acq;\n  case ARM::BI_InterlockedXor8_rel:\n  case ARM::BI_InterlockedXor16_rel:\n  case ARM::BI_InterlockedXor_rel:\n  case ARM::BI_InterlockedXor64_rel:\n    return MSVCIntrin::_InterlockedXor_rel;\n  case ARM::BI_InterlockedXor8_nf:\n  case ARM::BI_InterlockedXor16_nf:\n  case ARM::BI_InterlockedXor_nf:\n  case ARM::BI_InterlockedXor64_nf:\n    return MSVCIntrin::_InterlockedXor_nf;\n  case ARM::BI_InterlockedAnd8_acq:\n  case ARM::BI_InterlockedAnd16_acq:\n  case ARM::BI_InterlockedAnd_acq:\n  case ARM::BI_InterlockedAnd64_acq:\n    return MSVCIntrin::_InterlockedAnd_acq;\n  case ARM::BI_InterlockedAnd8_rel:\n  case ARM::BI_InterlockedAnd16_rel:\n  case ARM::BI_InterlockedAnd_rel:\n  case ARM::BI_InterlockedAnd64_rel:\n    return MSVCIntrin::_InterlockedAnd_rel;\n  case ARM::BI_InterlockedAnd8_nf:\n  case ARM::BI_InterlockedAnd16_nf:\n  case ARM::BI_InterlockedAnd_nf:\n  case ARM::BI_InterlockedAnd64_nf:\n    return MSVCIntrin::_InterlockedAnd_nf;\n  case ARM::BI_InterlockedIncrement16_acq:\n  case ARM::BI_InterlockedIncrement_acq:\n  case ARM::BI_InterlockedIncrement64_acq:\n    return MSVCIntrin::_InterlockedIncrement_acq;\n  case ARM::BI_InterlockedIncrement16_rel:\n  case ARM::BI_InterlockedIncrement_rel:\n  case ARM::BI_InterlockedIncrement64_rel:\n    return MSVCIntrin::_InterlockedIncrement_rel;\n  case ARM::BI_InterlockedIncrement16_nf:\n  case ARM::BI_InterlockedIncrement_nf:\n  case ARM::BI_InterlockedIncrement64_nf:\n    return MSVCIntrin::_InterlockedIncrement_nf;\n  case ARM::BI_InterlockedDecrement16_acq:\n  case ARM::BI_InterlockedDecrement_acq:\n  case ARM::BI_InterlockedDecrement64_acq:\n    return MSVCIntrin::_InterlockedDecrement_acq;\n  case ARM::BI_InterlockedDecrement16_rel:\n  case ARM::BI_InterlockedDecrement_rel:\n  case ARM::BI_InterlockedDecrement64_rel:\n    return MSVCIntrin::_InterlockedDecrement_rel;\n  case ARM::BI_InterlockedDecrement16_nf:\n  case ARM::BI_InterlockedDecrement_nf:\n  case ARM::BI_InterlockedDecrement64_nf:\n    return MSVCIntrin::_InterlockedDecrement_nf;\n  }\n  llvm_unreachable(\"must return from switch\");\n}\n\nstatic Optional<CodeGenFunction::MSVCIntrin>\ntranslateAarch64ToMsvcIntrin(unsigned BuiltinID) {\n  using MSVCIntrin = CodeGenFunction::MSVCIntrin;\n  switch (BuiltinID) {\n  default:\n    return None;\n  case AArch64::BI_BitScanForward:\n  case AArch64::BI_BitScanForward64:\n    return MSVCIntrin::_BitScanForward;\n  case AArch64::BI_BitScanReverse:\n  case AArch64::BI_BitScanReverse64:\n    return MSVCIntrin::_BitScanReverse;\n  case AArch64::BI_InterlockedAnd64:\n    return MSVCIntrin::_InterlockedAnd;\n  case AArch64::BI_InterlockedExchange64:\n    return MSVCIntrin::_InterlockedExchange;\n  case AArch64::BI_InterlockedExchangeAdd64:\n    return MSVCIntrin::_InterlockedExchangeAdd;\n  case AArch64::BI_InterlockedExchangeSub64:\n    return MSVCIntrin::_InterlockedExchangeSub;\n  case AArch64::BI_InterlockedOr64:\n    return MSVCIntrin::_InterlockedOr;\n  case AArch64::BI_InterlockedXor64:\n    return MSVCIntrin::_InterlockedXor;\n  case AArch64::BI_InterlockedDecrement64:\n    return MSVCIntrin::_InterlockedDecrement;\n  case AArch64::BI_InterlockedIncrement64:\n    return MSVCIntrin::_InterlockedIncrement;\n  case AArch64::BI_InterlockedExchangeAdd8_acq:\n  case AArch64::BI_InterlockedExchangeAdd16_acq:\n  case AArch64::BI_InterlockedExchangeAdd_acq:\n  case AArch64::BI_InterlockedExchangeAdd64_acq:\n    return MSVCIntrin::_InterlockedExchangeAdd_acq;\n  case AArch64::BI_InterlockedExchangeAdd8_rel:\n  case AArch64::BI_InterlockedExchangeAdd16_rel:\n  case AArch64::BI_InterlockedExchangeAdd_rel:\n  case AArch64::BI_InterlockedExchangeAdd64_rel:\n    return MSVCIntrin::_InterlockedExchangeAdd_rel;\n  case AArch64::BI_InterlockedExchangeAdd8_nf:\n  case AArch64::BI_InterlockedExchangeAdd16_nf:\n  case AArch64::BI_InterlockedExchangeAdd_nf:\n  case AArch64::BI_InterlockedExchangeAdd64_nf:\n    return MSVCIntrin::_InterlockedExchangeAdd_nf;\n  case AArch64::BI_InterlockedExchange8_acq:\n  case AArch64::BI_InterlockedExchange16_acq:\n  case AArch64::BI_InterlockedExchange_acq:\n  case AArch64::BI_InterlockedExchange64_acq:\n    return MSVCIntrin::_InterlockedExchange_acq;\n  case AArch64::BI_InterlockedExchange8_rel:\n  case AArch64::BI_InterlockedExchange16_rel:\n  case AArch64::BI_InterlockedExchange_rel:\n  case AArch64::BI_InterlockedExchange64_rel:\n    return MSVCIntrin::_InterlockedExchange_rel;\n  case AArch64::BI_InterlockedExchange8_nf:\n  case AArch64::BI_InterlockedExchange16_nf:\n  case AArch64::BI_InterlockedExchange_nf:\n  case AArch64::BI_InterlockedExchange64_nf:\n    return MSVCIntrin::_InterlockedExchange_nf;\n  case AArch64::BI_InterlockedCompareExchange8_acq:\n  case AArch64::BI_InterlockedCompareExchange16_acq:\n  case AArch64::BI_InterlockedCompareExchange_acq:\n  case AArch64::BI_InterlockedCompareExchange64_acq:\n    return MSVCIntrin::_InterlockedCompareExchange_acq;\n  case AArch64::BI_InterlockedCompareExchange8_rel:\n  case AArch64::BI_InterlockedCompareExchange16_rel:\n  case AArch64::BI_InterlockedCompareExchange_rel:\n  case AArch64::BI_InterlockedCompareExchange64_rel:\n    return MSVCIntrin::_InterlockedCompareExchange_rel;\n  case AArch64::BI_InterlockedCompareExchange8_nf:\n  case AArch64::BI_InterlockedCompareExchange16_nf:\n  case AArch64::BI_InterlockedCompareExchange_nf:\n  case AArch64::BI_InterlockedCompareExchange64_nf:\n    return MSVCIntrin::_InterlockedCompareExchange_nf;\n  case AArch64::BI_InterlockedCompareExchange128:\n    return MSVCIntrin::_InterlockedCompareExchange128;\n  case AArch64::BI_InterlockedCompareExchange128_acq:\n    return MSVCIntrin::_InterlockedCompareExchange128_acq;\n  case AArch64::BI_InterlockedCompareExchange128_nf:\n    return MSVCIntrin::_InterlockedCompareExchange128_nf;\n  case AArch64::BI_InterlockedCompareExchange128_rel:\n    return MSVCIntrin::_InterlockedCompareExchange128_rel;\n  case AArch64::BI_InterlockedOr8_acq:\n  case AArch64::BI_InterlockedOr16_acq:\n  case AArch64::BI_InterlockedOr_acq:\n  case AArch64::BI_InterlockedOr64_acq:\n    return MSVCIntrin::_InterlockedOr_acq;\n  case AArch64::BI_InterlockedOr8_rel:\n  case AArch64::BI_InterlockedOr16_rel:\n  case AArch64::BI_InterlockedOr_rel:\n  case AArch64::BI_InterlockedOr64_rel:\n    return MSVCIntrin::_InterlockedOr_rel;\n  case AArch64::BI_InterlockedOr8_nf:\n  case AArch64::BI_InterlockedOr16_nf:\n  case AArch64::BI_InterlockedOr_nf:\n  case AArch64::BI_InterlockedOr64_nf:\n    return MSVCIntrin::_InterlockedOr_nf;\n  case AArch64::BI_InterlockedXor8_acq:\n  case AArch64::BI_InterlockedXor16_acq:\n  case AArch64::BI_InterlockedXor_acq:\n  case AArch64::BI_InterlockedXor64_acq:\n    return MSVCIntrin::_InterlockedXor_acq;\n  case AArch64::BI_InterlockedXor8_rel:\n  case AArch64::BI_InterlockedXor16_rel:\n  case AArch64::BI_InterlockedXor_rel:\n  case AArch64::BI_InterlockedXor64_rel:\n    return MSVCIntrin::_InterlockedXor_rel;\n  case AArch64::BI_InterlockedXor8_nf:\n  case AArch64::BI_InterlockedXor16_nf:\n  case AArch64::BI_InterlockedXor_nf:\n  case AArch64::BI_InterlockedXor64_nf:\n    return MSVCIntrin::_InterlockedXor_nf;\n  case AArch64::BI_InterlockedAnd8_acq:\n  case AArch64::BI_InterlockedAnd16_acq:\n  case AArch64::BI_InterlockedAnd_acq:\n  case AArch64::BI_InterlockedAnd64_acq:\n    return MSVCIntrin::_InterlockedAnd_acq;\n  case AArch64::BI_InterlockedAnd8_rel:\n  case AArch64::BI_InterlockedAnd16_rel:\n  case AArch64::BI_InterlockedAnd_rel:\n  case AArch64::BI_InterlockedAnd64_rel:\n    return MSVCIntrin::_InterlockedAnd_rel;\n  case AArch64::BI_InterlockedAnd8_nf:\n  case AArch64::BI_InterlockedAnd16_nf:\n  case AArch64::BI_InterlockedAnd_nf:\n  case AArch64::BI_InterlockedAnd64_nf:\n    return MSVCIntrin::_InterlockedAnd_nf;\n  case AArch64::BI_InterlockedIncrement16_acq:\n  case AArch64::BI_InterlockedIncrement_acq:\n  case AArch64::BI_InterlockedIncrement64_acq:\n    return MSVCIntrin::_InterlockedIncrement_acq;\n  case AArch64::BI_InterlockedIncrement16_rel:\n  case AArch64::BI_InterlockedIncrement_rel:\n  case AArch64::BI_InterlockedIncrement64_rel:\n    return MSVCIntrin::_InterlockedIncrement_rel;\n  case AArch64::BI_InterlockedIncrement16_nf:\n  case AArch64::BI_InterlockedIncrement_nf:\n  case AArch64::BI_InterlockedIncrement64_nf:\n    return MSVCIntrin::_InterlockedIncrement_nf;\n  case AArch64::BI_InterlockedDecrement16_acq:\n  case AArch64::BI_InterlockedDecrement_acq:\n  case AArch64::BI_InterlockedDecrement64_acq:\n    return MSVCIntrin::_InterlockedDecrement_acq;\n  case AArch64::BI_InterlockedDecrement16_rel:\n  case AArch64::BI_InterlockedDecrement_rel:\n  case AArch64::BI_InterlockedDecrement64_rel:\n    return MSVCIntrin::_InterlockedDecrement_rel;\n  case AArch64::BI_InterlockedDecrement16_nf:\n  case AArch64::BI_InterlockedDecrement_nf:\n  case AArch64::BI_InterlockedDecrement64_nf:\n    return MSVCIntrin::_InterlockedDecrement_nf;\n  }\n  llvm_unreachable(\"must return from switch\");\n}\n\nstatic Optional<CodeGenFunction::MSVCIntrin>\ntranslateX86ToMsvcIntrin(unsigned BuiltinID) {\n  using MSVCIntrin = CodeGenFunction::MSVCIntrin;\n  switch (BuiltinID) {\n  default:\n    return None;\n  case clang::X86::BI_BitScanForward:\n  case clang::X86::BI_BitScanForward64:\n    return MSVCIntrin::_BitScanForward;\n  case clang::X86::BI_BitScanReverse:\n  case clang::X86::BI_BitScanReverse64:\n    return MSVCIntrin::_BitScanReverse;\n  case clang::X86::BI_InterlockedAnd64:\n    return MSVCIntrin::_InterlockedAnd;\n  case clang::X86::BI_InterlockedCompareExchange128:\n    return MSVCIntrin::_InterlockedCompareExchange128;\n  case clang::X86::BI_InterlockedExchange64:\n    return MSVCIntrin::_InterlockedExchange;\n  case clang::X86::BI_InterlockedExchangeAdd64:\n    return MSVCIntrin::_InterlockedExchangeAdd;\n  case clang::X86::BI_InterlockedExchangeSub64:\n    return MSVCIntrin::_InterlockedExchangeSub;\n  case clang::X86::BI_InterlockedOr64:\n    return MSVCIntrin::_InterlockedOr;\n  case clang::X86::BI_InterlockedXor64:\n    return MSVCIntrin::_InterlockedXor;\n  case clang::X86::BI_InterlockedDecrement64:\n    return MSVCIntrin::_InterlockedDecrement;\n  case clang::X86::BI_InterlockedIncrement64:\n    return MSVCIntrin::_InterlockedIncrement;\n  }\n  llvm_unreachable(\"must return from switch\");\n}\n\n// Emit an MSVC intrinsic. Assumes that arguments have *not* been evaluated.\nValue *CodeGenFunction::EmitMSVCBuiltinExpr(MSVCIntrin BuiltinID,\n                                            const CallExpr *E) {\n  switch (BuiltinID) {\n  case MSVCIntrin::_BitScanForward:\n  case MSVCIntrin::_BitScanReverse: {\n    Address IndexAddress(EmitPointerWithAlignment(E->getArg(0)));\n    Value *ArgValue = EmitScalarExpr(E->getArg(1));\n\n    llvm::Type *ArgType = ArgValue->getType();\n    llvm::Type *IndexType =\n        IndexAddress.getPointer()->getType()->getPointerElementType();\n    llvm::Type *ResultType = ConvertType(E->getType());\n\n    Value *ArgZero = llvm::Constant::getNullValue(ArgType);\n    Value *ResZero = llvm::Constant::getNullValue(ResultType);\n    Value *ResOne = llvm::ConstantInt::get(ResultType, 1);\n\n    BasicBlock *Begin = Builder.GetInsertBlock();\n    BasicBlock *End = createBasicBlock(\"bitscan_end\", this->CurFn);\n    Builder.SetInsertPoint(End);\n    PHINode *Result = Builder.CreatePHI(ResultType, 2, \"bitscan_result\");\n\n    Builder.SetInsertPoint(Begin);\n    Value *IsZero = Builder.CreateICmpEQ(ArgValue, ArgZero);\n    BasicBlock *NotZero = createBasicBlock(\"bitscan_not_zero\", this->CurFn);\n    Builder.CreateCondBr(IsZero, End, NotZero);\n    Result->addIncoming(ResZero, Begin);\n\n    Builder.SetInsertPoint(NotZero);\n\n    if (BuiltinID == MSVCIntrin::_BitScanForward) {\n      Function *F = CGM.getIntrinsic(Intrinsic::cttz, ArgType);\n      Value *ZeroCount = Builder.CreateCall(F, {ArgValue, Builder.getTrue()});\n      ZeroCount = Builder.CreateIntCast(ZeroCount, IndexType, false);\n      Builder.CreateStore(ZeroCount, IndexAddress, false);\n    } else {\n      unsigned ArgWidth = cast<llvm::IntegerType>(ArgType)->getBitWidth();\n      Value *ArgTypeLastIndex = llvm::ConstantInt::get(IndexType, ArgWidth - 1);\n\n      Function *F = CGM.getIntrinsic(Intrinsic::ctlz, ArgType);\n      Value *ZeroCount = Builder.CreateCall(F, {ArgValue, Builder.getTrue()});\n      ZeroCount = Builder.CreateIntCast(ZeroCount, IndexType, false);\n      Value *Index = Builder.CreateNSWSub(ArgTypeLastIndex, ZeroCount);\n      Builder.CreateStore(Index, IndexAddress, false);\n    }\n    Builder.CreateBr(End);\n    Result->addIncoming(ResOne, NotZero);\n\n    Builder.SetInsertPoint(End);\n    return Result;\n  }\n  case MSVCIntrin::_InterlockedAnd:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::And, E);\n  case MSVCIntrin::_InterlockedExchange:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xchg, E);\n  case MSVCIntrin::_InterlockedExchangeAdd:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Add, E);\n  case MSVCIntrin::_InterlockedExchangeSub:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Sub, E);\n  case MSVCIntrin::_InterlockedOr:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Or, E);\n  case MSVCIntrin::_InterlockedXor:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xor, E);\n  case MSVCIntrin::_InterlockedExchangeAdd_acq:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Add, E,\n                                 AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedExchangeAdd_rel:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Add, E,\n                                 AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedExchangeAdd_nf:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Add, E,\n                                 AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedExchange_acq:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xchg, E,\n                                 AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedExchange_rel:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xchg, E,\n                                 AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedExchange_nf:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xchg, E,\n                                 AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedCompareExchange_acq:\n    return EmitAtomicCmpXchgForMSIntrin(*this, E, AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedCompareExchange_rel:\n    return EmitAtomicCmpXchgForMSIntrin(*this, E, AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedCompareExchange_nf:\n    return EmitAtomicCmpXchgForMSIntrin(*this, E, AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedCompareExchange128:\n    return EmitAtomicCmpXchg128ForMSIntrin(\n        *this, E, AtomicOrdering::SequentiallyConsistent);\n  case MSVCIntrin::_InterlockedCompareExchange128_acq:\n    return EmitAtomicCmpXchg128ForMSIntrin(*this, E, AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedCompareExchange128_rel:\n    return EmitAtomicCmpXchg128ForMSIntrin(*this, E, AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedCompareExchange128_nf:\n    return EmitAtomicCmpXchg128ForMSIntrin(*this, E, AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedOr_acq:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Or, E,\n                                 AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedOr_rel:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Or, E,\n                                 AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedOr_nf:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Or, E,\n                                 AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedXor_acq:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xor, E,\n                                 AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedXor_rel:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xor, E,\n                                 AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedXor_nf:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::Xor, E,\n                                 AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedAnd_acq:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::And, E,\n                                 AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedAnd_rel:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::And, E,\n                                 AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedAnd_nf:\n    return MakeBinaryAtomicValue(*this, AtomicRMWInst::And, E,\n                                 AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedIncrement_acq:\n    return EmitAtomicIncrementValue(*this, E, AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedIncrement_rel:\n    return EmitAtomicIncrementValue(*this, E, AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedIncrement_nf:\n    return EmitAtomicIncrementValue(*this, E, AtomicOrdering::Monotonic);\n  case MSVCIntrin::_InterlockedDecrement_acq:\n    return EmitAtomicDecrementValue(*this, E, AtomicOrdering::Acquire);\n  case MSVCIntrin::_InterlockedDecrement_rel:\n    return EmitAtomicDecrementValue(*this, E, AtomicOrdering::Release);\n  case MSVCIntrin::_InterlockedDecrement_nf:\n    return EmitAtomicDecrementValue(*this, E, AtomicOrdering::Monotonic);\n\n  case MSVCIntrin::_InterlockedDecrement:\n    return EmitAtomicDecrementValue(*this, E);\n  case MSVCIntrin::_InterlockedIncrement:\n    return EmitAtomicIncrementValue(*this, E);\n\n  case MSVCIntrin::__fastfail: {\n    // Request immediate process termination from the kernel. The instruction\n    // sequences to do this are documented on MSDN:\n    // https://msdn.microsoft.com/en-us/library/dn774154.aspx\n    llvm::Triple::ArchType ISA = getTarget().getTriple().getArch();\n    StringRef Asm, Constraints;\n    switch (ISA) {\n    default:\n      ErrorUnsupported(E, \"__fastfail call for this architecture\");\n      break;\n    case llvm::Triple::x86:\n    case llvm::Triple::x86_64:\n      Asm = \"int $$0x29\";\n      Constraints = \"{cx}\";\n      break;\n    case llvm::Triple::thumb:\n      Asm = \"udf #251\";\n      Constraints = \"{r0}\";\n      break;\n    case llvm::Triple::aarch64:\n      Asm = \"brk #0xF003\";\n      Constraints = \"{w0}\";\n    }\n    llvm::FunctionType *FTy = llvm::FunctionType::get(VoidTy, {Int32Ty}, false);\n    llvm::InlineAsm *IA =\n        llvm::InlineAsm::get(FTy, Asm, Constraints, /*hasSideEffects=*/true);\n    llvm::AttributeList NoReturnAttr = llvm::AttributeList::get(\n        getLLVMContext(), llvm::AttributeList::FunctionIndex,\n        llvm::Attribute::NoReturn);\n    llvm::CallInst *CI = Builder.CreateCall(IA, EmitScalarExpr(E->getArg(0)));\n    CI->setAttributes(NoReturnAttr);\n    return CI;\n  }\n  }\n  llvm_unreachable(\"Incorrect MSVC intrinsic!\");\n}\n\nnamespace {\n// ARC cleanup for __builtin_os_log_format\nstruct CallObjCArcUse final : EHScopeStack::Cleanup {\n  CallObjCArcUse(llvm::Value *object) : object(object) {}\n  llvm::Value *object;\n\n  void Emit(CodeGenFunction &CGF, Flags flags) override {\n    CGF.EmitARCIntrinsicUse(object);\n  }\n};\n}\n\nValue *CodeGenFunction::EmitCheckedArgForBuiltin(const Expr *E,\n                                                 BuiltinCheckKind Kind) {\n  assert((Kind == BCK_CLZPassedZero || Kind == BCK_CTZPassedZero)\n          && \"Unsupported builtin check kind\");\n\n  Value *ArgValue = EmitScalarExpr(E);\n  if (!SanOpts.has(SanitizerKind::Builtin) || !getTarget().isCLZForZeroUndef())\n    return ArgValue;\n\n  SanitizerScope SanScope(this);\n  Value *Cond = Builder.CreateICmpNE(\n      ArgValue, llvm::Constant::getNullValue(ArgValue->getType()));\n  EmitCheck(std::make_pair(Cond, SanitizerKind::Builtin),\n            SanitizerHandler::InvalidBuiltin,\n            {EmitCheckSourceLocation(E->getExprLoc()),\n             llvm::ConstantInt::get(Builder.getInt8Ty(), Kind)},\n            None);\n  return ArgValue;\n}\n\n/// Get the argument type for arguments to os_log_helper.\nstatic CanQualType getOSLogArgType(ASTContext &C, int Size) {\n  QualType UnsignedTy = C.getIntTypeForBitwidth(Size * 8, /*Signed=*/false);\n  return C.getCanonicalType(UnsignedTy);\n}\n\nllvm::Function *CodeGenFunction::generateBuiltinOSLogHelperFunction(\n    const analyze_os_log::OSLogBufferLayout &Layout,\n    CharUnits BufferAlignment) {\n  ASTContext &Ctx = getContext();\n\n  llvm::SmallString<64> Name;\n  {\n    raw_svector_ostream OS(Name);\n    OS << \"__os_log_helper\";\n    OS << \"_\" << BufferAlignment.getQuantity();\n    OS << \"_\" << int(Layout.getSummaryByte());\n    OS << \"_\" << int(Layout.getNumArgsByte());\n    for (const auto &Item : Layout.Items)\n      OS << \"_\" << int(Item.getSizeByte()) << \"_\"\n         << int(Item.getDescriptorByte());\n  }\n\n  if (llvm::Function *F = CGM.getModule().getFunction(Name))\n    return F;\n\n  llvm::SmallVector<QualType, 4> ArgTys;\n  FunctionArgList Args;\n  Args.push_back(ImplicitParamDecl::Create(\n      Ctx, nullptr, SourceLocation(), &Ctx.Idents.get(\"buffer\"), Ctx.VoidPtrTy,\n      ImplicitParamDecl::Other));\n  ArgTys.emplace_back(Ctx.VoidPtrTy);\n\n  for (unsigned int I = 0, E = Layout.Items.size(); I < E; ++I) {\n    char Size = Layout.Items[I].getSizeByte();\n    if (!Size)\n      continue;\n\n    QualType ArgTy = getOSLogArgType(Ctx, Size);\n    Args.push_back(ImplicitParamDecl::Create(\n        Ctx, nullptr, SourceLocation(),\n        &Ctx.Idents.get(std::string(\"arg\") + llvm::to_string(I)), ArgTy,\n        ImplicitParamDecl::Other));\n    ArgTys.emplace_back(ArgTy);\n  }\n\n  QualType ReturnTy = Ctx.VoidTy;\n  QualType FuncionTy = Ctx.getFunctionType(ReturnTy, ArgTys, {});\n\n  // The helper function has linkonce_odr linkage to enable the linker to merge\n  // identical functions. To ensure the merging always happens, 'noinline' is\n  // attached to the function when compiling with -Oz.\n  const CGFunctionInfo &FI =\n      CGM.getTypes().arrangeBuiltinFunctionDeclaration(ReturnTy, Args);\n  llvm::FunctionType *FuncTy = CGM.getTypes().GetFunctionType(FI);\n  llvm::Function *Fn = llvm::Function::Create(\n      FuncTy, llvm::GlobalValue::LinkOnceODRLinkage, Name, &CGM.getModule());\n  Fn->setVisibility(llvm::GlobalValue::HiddenVisibility);\n  CGM.SetLLVMFunctionAttributes(GlobalDecl(), FI, Fn);\n  CGM.SetLLVMFunctionAttributesForDefinition(nullptr, Fn);\n  Fn->setDoesNotThrow();\n\n  // Attach 'noinline' at -Oz.\n  if (CGM.getCodeGenOpts().OptimizeSize == 2)\n    Fn->addFnAttr(llvm::Attribute::NoInline);\n\n  auto NL = ApplyDebugLocation::CreateEmpty(*this);\n  IdentifierInfo *II = &Ctx.Idents.get(Name);\n  FunctionDecl *FD = FunctionDecl::Create(\n      Ctx, Ctx.getTranslationUnitDecl(), SourceLocation(), SourceLocation(), II,\n      FuncionTy, nullptr, SC_PrivateExtern, false, false);\n  // Avoid generating debug location info for the function.\n  FD->setImplicit();\n\n  StartFunction(FD, ReturnTy, Fn, FI, Args);\n\n  // Create a scope with an artificial location for the body of this function.\n  auto AL = ApplyDebugLocation::CreateArtificial(*this);\n\n  CharUnits Offset;\n  Address BufAddr(Builder.CreateLoad(GetAddrOfLocalVar(Args[0]), \"buf\"),\n                  BufferAlignment);\n  Builder.CreateStore(Builder.getInt8(Layout.getSummaryByte()),\n                      Builder.CreateConstByteGEP(BufAddr, Offset++, \"summary\"));\n  Builder.CreateStore(Builder.getInt8(Layout.getNumArgsByte()),\n                      Builder.CreateConstByteGEP(BufAddr, Offset++, \"numArgs\"));\n\n  unsigned I = 1;\n  for (const auto &Item : Layout.Items) {\n    Builder.CreateStore(\n        Builder.getInt8(Item.getDescriptorByte()),\n        Builder.CreateConstByteGEP(BufAddr, Offset++, \"argDescriptor\"));\n    Builder.CreateStore(\n        Builder.getInt8(Item.getSizeByte()),\n        Builder.CreateConstByteGEP(BufAddr, Offset++, \"argSize\"));\n\n    CharUnits Size = Item.size();\n    if (!Size.getQuantity())\n      continue;\n\n    Address Arg = GetAddrOfLocalVar(Args[I]);\n    Address Addr = Builder.CreateConstByteGEP(BufAddr, Offset, \"argData\");\n    Addr = Builder.CreateBitCast(Addr, Arg.getPointer()->getType(),\n                                 \"argDataCast\");\n    Builder.CreateStore(Builder.CreateLoad(Arg), Addr);\n    Offset += Size;\n    ++I;\n  }\n\n  FinishFunction();\n\n  return Fn;\n}\n\nRValue CodeGenFunction::emitBuiltinOSLogFormat(const CallExpr &E) {\n  assert(E.getNumArgs() >= 2 &&\n         \"__builtin_os_log_format takes at least 2 arguments\");\n  ASTContext &Ctx = getContext();\n  analyze_os_log::OSLogBufferLayout Layout;\n  analyze_os_log::computeOSLogBufferLayout(Ctx, &E, Layout);\n  Address BufAddr = EmitPointerWithAlignment(E.getArg(0));\n  llvm::SmallVector<llvm::Value *, 4> RetainableOperands;\n\n  // Ignore argument 1, the format string. It is not currently used.\n  CallArgList Args;\n  Args.add(RValue::get(BufAddr.getPointer()), Ctx.VoidPtrTy);\n\n  for (const auto &Item : Layout.Items) {\n    int Size = Item.getSizeByte();\n    if (!Size)\n      continue;\n\n    llvm::Value *ArgVal;\n\n    if (Item.getKind() == analyze_os_log::OSLogBufferItem::MaskKind) {\n      uint64_t Val = 0;\n      for (unsigned I = 0, E = Item.getMaskType().size(); I < E; ++I)\n        Val |= ((uint64_t)Item.getMaskType()[I]) << I * 8;\n      ArgVal = llvm::Constant::getIntegerValue(Int64Ty, llvm::APInt(64, Val));\n    } else if (const Expr *TheExpr = Item.getExpr()) {\n      ArgVal = EmitScalarExpr(TheExpr, /*Ignore*/ false);\n\n      // If a temporary object that requires destruction after the full\n      // expression is passed, push a lifetime-extended cleanup to extend its\n      // lifetime to the end of the enclosing block scope.\n      auto LifetimeExtendObject = [&](const Expr *E) {\n        E = E->IgnoreParenCasts();\n        // Extend lifetimes of objects returned by function calls and message\n        // sends.\n\n        // FIXME: We should do this in other cases in which temporaries are\n        //        created including arguments of non-ARC types (e.g., C++\n        //        temporaries).\n        if (isa<CallExpr>(E) || isa<ObjCMessageExpr>(E))\n          return true;\n        return false;\n      };\n\n      if (TheExpr->getType()->isObjCRetainableType() &&\n          getLangOpts().ObjCAutoRefCount && LifetimeExtendObject(TheExpr)) {\n        assert(getEvaluationKind(TheExpr->getType()) == TEK_Scalar &&\n               \"Only scalar can be a ObjC retainable type\");\n        if (!isa<Constant>(ArgVal)) {\n          CleanupKind Cleanup = getARCCleanupKind();\n          QualType Ty = TheExpr->getType();\n          Address Alloca = Address::invalid();\n          Address Addr = CreateMemTemp(Ty, \"os.log.arg\", &Alloca);\n          ArgVal = EmitARCRetain(Ty, ArgVal);\n          Builder.CreateStore(ArgVal, Addr);\n          pushLifetimeExtendedDestroy(Cleanup, Alloca, Ty,\n                                      CodeGenFunction::destroyARCStrongPrecise,\n                                      Cleanup & EHCleanup);\n\n          // Push a clang.arc.use call to ensure ARC optimizer knows that the\n          // argument has to be alive.\n          if (CGM.getCodeGenOpts().OptimizationLevel != 0)\n            pushCleanupAfterFullExpr<CallObjCArcUse>(Cleanup, ArgVal);\n        }\n      }\n    } else {\n      ArgVal = Builder.getInt32(Item.getConstValue().getQuantity());\n    }\n\n    unsigned ArgValSize =\n        CGM.getDataLayout().getTypeSizeInBits(ArgVal->getType());\n    llvm::IntegerType *IntTy = llvm::Type::getIntNTy(getLLVMContext(),\n                                                     ArgValSize);\n    ArgVal = Builder.CreateBitOrPointerCast(ArgVal, IntTy);\n    CanQualType ArgTy = getOSLogArgType(Ctx, Size);\n    // If ArgVal has type x86_fp80, zero-extend ArgVal.\n    ArgVal = Builder.CreateZExtOrBitCast(ArgVal, ConvertType(ArgTy));\n    Args.add(RValue::get(ArgVal), ArgTy);\n  }\n\n  const CGFunctionInfo &FI =\n      CGM.getTypes().arrangeBuiltinFunctionCall(Ctx.VoidTy, Args);\n  llvm::Function *F = CodeGenFunction(CGM).generateBuiltinOSLogHelperFunction(\n      Layout, BufAddr.getAlignment());\n  EmitCall(FI, CGCallee::forDirect(F), ReturnValueSlot(), Args);\n  return RValue::get(BufAddr.getPointer());\n}\n\nstatic bool isSpecialUnsignedMultiplySignedResult(\n    unsigned BuiltinID, WidthAndSignedness Op1Info, WidthAndSignedness Op2Info,\n    WidthAndSignedness ResultInfo) {\n  return BuiltinID == Builtin::BI__builtin_mul_overflow &&\n         Op1Info.Width == Op2Info.Width && Op2Info.Width == ResultInfo.Width &&\n         !Op1Info.Signed && !Op2Info.Signed && ResultInfo.Signed;\n}\n\nstatic RValue EmitCheckedUnsignedMultiplySignedResult(\n    CodeGenFunction &CGF, const clang::Expr *Op1, WidthAndSignedness Op1Info,\n    const clang::Expr *Op2, WidthAndSignedness Op2Info,\n    const clang::Expr *ResultArg, QualType ResultQTy,\n    WidthAndSignedness ResultInfo) {\n  assert(isSpecialUnsignedMultiplySignedResult(\n             Builtin::BI__builtin_mul_overflow, Op1Info, Op2Info, ResultInfo) &&\n         \"Cannot specialize this multiply\");\n\n  llvm::Value *V1 = CGF.EmitScalarExpr(Op1);\n  llvm::Value *V2 = CGF.EmitScalarExpr(Op2);\n\n  llvm::Value *HasOverflow;\n  llvm::Value *Result = EmitOverflowIntrinsic(\n      CGF, llvm::Intrinsic::umul_with_overflow, V1, V2, HasOverflow);\n\n  // The intrinsic call will detect overflow when the value is > UINT_MAX,\n  // however, since the original builtin had a signed result, we need to report\n  // an overflow when the result is greater than INT_MAX.\n  auto IntMax = llvm::APInt::getSignedMaxValue(ResultInfo.Width);\n  llvm::Value *IntMaxValue = llvm::ConstantInt::get(Result->getType(), IntMax);\n\n  llvm::Value *IntMaxOverflow = CGF.Builder.CreateICmpUGT(Result, IntMaxValue);\n  HasOverflow = CGF.Builder.CreateOr(HasOverflow, IntMaxOverflow);\n\n  bool isVolatile =\n      ResultArg->getType()->getPointeeType().isVolatileQualified();\n  Address ResultPtr = CGF.EmitPointerWithAlignment(ResultArg);\n  CGF.Builder.CreateStore(CGF.EmitToMemory(Result, ResultQTy), ResultPtr,\n                          isVolatile);\n  return RValue::get(HasOverflow);\n}\n\n/// Determine if a binop is a checked mixed-sign multiply we can specialize.\nstatic bool isSpecialMixedSignMultiply(unsigned BuiltinID,\n                                       WidthAndSignedness Op1Info,\n                                       WidthAndSignedness Op2Info,\n                                       WidthAndSignedness ResultInfo) {\n  return BuiltinID == Builtin::BI__builtin_mul_overflow &&\n         std::max(Op1Info.Width, Op2Info.Width) >= ResultInfo.Width &&\n         Op1Info.Signed != Op2Info.Signed;\n}\n\n/// Emit a checked mixed-sign multiply. This is a cheaper specialization of\n/// the generic checked-binop irgen.\nstatic RValue\nEmitCheckedMixedSignMultiply(CodeGenFunction &CGF, const clang::Expr *Op1,\n                             WidthAndSignedness Op1Info, const clang::Expr *Op2,\n                             WidthAndSignedness Op2Info,\n                             const clang::Expr *ResultArg, QualType ResultQTy,\n                             WidthAndSignedness ResultInfo) {\n  assert(isSpecialMixedSignMultiply(Builtin::BI__builtin_mul_overflow, Op1Info,\n                                    Op2Info, ResultInfo) &&\n         \"Not a mixed-sign multipliction we can specialize\");\n\n  // Emit the signed and unsigned operands.\n  const clang::Expr *SignedOp = Op1Info.Signed ? Op1 : Op2;\n  const clang::Expr *UnsignedOp = Op1Info.Signed ? Op2 : Op1;\n  llvm::Value *Signed = CGF.EmitScalarExpr(SignedOp);\n  llvm::Value *Unsigned = CGF.EmitScalarExpr(UnsignedOp);\n  unsigned SignedOpWidth = Op1Info.Signed ? Op1Info.Width : Op2Info.Width;\n  unsigned UnsignedOpWidth = Op1Info.Signed ? Op2Info.Width : Op1Info.Width;\n\n  // One of the operands may be smaller than the other. If so, [s|z]ext it.\n  if (SignedOpWidth < UnsignedOpWidth)\n    Signed = CGF.Builder.CreateSExt(Signed, Unsigned->getType(), \"op.sext\");\n  if (UnsignedOpWidth < SignedOpWidth)\n    Unsigned = CGF.Builder.CreateZExt(Unsigned, Signed->getType(), \"op.zext\");\n\n  llvm::Type *OpTy = Signed->getType();\n  llvm::Value *Zero = llvm::Constant::getNullValue(OpTy);\n  Address ResultPtr = CGF.EmitPointerWithAlignment(ResultArg);\n  llvm::Type *ResTy = ResultPtr.getElementType();\n  unsigned OpWidth = std::max(Op1Info.Width, Op2Info.Width);\n\n  // Take the absolute value of the signed operand.\n  llvm::Value *IsNegative = CGF.Builder.CreateICmpSLT(Signed, Zero);\n  llvm::Value *AbsOfNegative = CGF.Builder.CreateSub(Zero, Signed);\n  llvm::Value *AbsSigned =\n      CGF.Builder.CreateSelect(IsNegative, AbsOfNegative, Signed);\n\n  // Perform a checked unsigned multiplication.\n  llvm::Value *UnsignedOverflow;\n  llvm::Value *UnsignedResult =\n      EmitOverflowIntrinsic(CGF, llvm::Intrinsic::umul_with_overflow, AbsSigned,\n                            Unsigned, UnsignedOverflow);\n\n  llvm::Value *Overflow, *Result;\n  if (ResultInfo.Signed) {\n    // Signed overflow occurs if the result is greater than INT_MAX or lesser\n    // than INT_MIN, i.e when |Result| > (INT_MAX + IsNegative).\n    auto IntMax =\n        llvm::APInt::getSignedMaxValue(ResultInfo.Width).zextOrSelf(OpWidth);\n    llvm::Value *MaxResult =\n        CGF.Builder.CreateAdd(llvm::ConstantInt::get(OpTy, IntMax),\n                              CGF.Builder.CreateZExt(IsNegative, OpTy));\n    llvm::Value *SignedOverflow =\n        CGF.Builder.CreateICmpUGT(UnsignedResult, MaxResult);\n    Overflow = CGF.Builder.CreateOr(UnsignedOverflow, SignedOverflow);\n\n    // Prepare the signed result (possibly by negating it).\n    llvm::Value *NegativeResult = CGF.Builder.CreateNeg(UnsignedResult);\n    llvm::Value *SignedResult =\n        CGF.Builder.CreateSelect(IsNegative, NegativeResult, UnsignedResult);\n    Result = CGF.Builder.CreateTrunc(SignedResult, ResTy);\n  } else {\n    // Unsigned overflow occurs if the result is < 0 or greater than UINT_MAX.\n    llvm::Value *Underflow = CGF.Builder.CreateAnd(\n        IsNegative, CGF.Builder.CreateIsNotNull(UnsignedResult));\n    Overflow = CGF.Builder.CreateOr(UnsignedOverflow, Underflow);\n    if (ResultInfo.Width < OpWidth) {\n      auto IntMax =\n          llvm::APInt::getMaxValue(ResultInfo.Width).zext(OpWidth);\n      llvm::Value *TruncOverflow = CGF.Builder.CreateICmpUGT(\n          UnsignedResult, llvm::ConstantInt::get(OpTy, IntMax));\n      Overflow = CGF.Builder.CreateOr(Overflow, TruncOverflow);\n    }\n\n    // Negate the product if it would be negative in infinite precision.\n    Result = CGF.Builder.CreateSelect(\n        IsNegative, CGF.Builder.CreateNeg(UnsignedResult), UnsignedResult);\n\n    Result = CGF.Builder.CreateTrunc(Result, ResTy);\n  }\n  assert(Overflow && Result && \"Missing overflow or result\");\n\n  bool isVolatile =\n      ResultArg->getType()->getPointeeType().isVolatileQualified();\n  CGF.Builder.CreateStore(CGF.EmitToMemory(Result, ResultQTy), ResultPtr,\n                          isVolatile);\n  return RValue::get(Overflow);\n}\n\nstatic llvm::Value *dumpRecord(CodeGenFunction &CGF, QualType RType,\n                               Value *&RecordPtr, CharUnits Align,\n                               llvm::FunctionCallee Func, int Lvl) {\n  ASTContext &Context = CGF.getContext();\n  RecordDecl *RD = RType->castAs<RecordType>()->getDecl()->getDefinition();\n  std::string Pad = std::string(Lvl * 4, ' ');\n\n  Value *GString =\n      CGF.Builder.CreateGlobalStringPtr(RType.getAsString() + \" {\\n\");\n  Value *Res = CGF.Builder.CreateCall(Func, {GString});\n\n  static llvm::DenseMap<QualType, const char *> Types;\n  if (Types.empty()) {\n    Types[Context.CharTy] = \"%c\";\n    Types[Context.BoolTy] = \"%d\";\n    Types[Context.SignedCharTy] = \"%hhd\";\n    Types[Context.UnsignedCharTy] = \"%hhu\";\n    Types[Context.IntTy] = \"%d\";\n    Types[Context.UnsignedIntTy] = \"%u\";\n    Types[Context.LongTy] = \"%ld\";\n    Types[Context.UnsignedLongTy] = \"%lu\";\n    Types[Context.LongLongTy] = \"%lld\";\n    Types[Context.UnsignedLongLongTy] = \"%llu\";\n    Types[Context.ShortTy] = \"%hd\";\n    Types[Context.UnsignedShortTy] = \"%hu\";\n    Types[Context.VoidPtrTy] = \"%p\";\n    Types[Context.FloatTy] = \"%f\";\n    Types[Context.DoubleTy] = \"%f\";\n    Types[Context.LongDoubleTy] = \"%Lf\";\n    Types[Context.getPointerType(Context.CharTy)] = \"%s\";\n    Types[Context.getPointerType(Context.getConstType(Context.CharTy))] = \"%s\";\n  }\n\n  for (const auto *FD : RD->fields()) {\n    Value *FieldPtr = RecordPtr;\n    if (RD->isUnion())\n      FieldPtr = CGF.Builder.CreatePointerCast(\n          FieldPtr, CGF.ConvertType(Context.getPointerType(FD->getType())));\n    else\n      FieldPtr = CGF.Builder.CreateStructGEP(CGF.ConvertType(RType), FieldPtr,\n                                             FD->getFieldIndex());\n\n    GString = CGF.Builder.CreateGlobalStringPtr(\n        llvm::Twine(Pad)\n            .concat(FD->getType().getAsString())\n            .concat(llvm::Twine(' '))\n            .concat(FD->getNameAsString())\n            .concat(\" : \")\n            .str());\n    Value *TmpRes = CGF.Builder.CreateCall(Func, {GString});\n    Res = CGF.Builder.CreateAdd(Res, TmpRes);\n\n    QualType CanonicalType =\n        FD->getType().getUnqualifiedType().getCanonicalType();\n\n    // We check whether we are in a recursive type\n    if (CanonicalType->isRecordType()) {\n      TmpRes = dumpRecord(CGF, CanonicalType, FieldPtr, Align, Func, Lvl + 1);\n      Res = CGF.Builder.CreateAdd(TmpRes, Res);\n      continue;\n    }\n\n    // We try to determine the best format to print the current field\n    llvm::Twine Format = Types.find(CanonicalType) == Types.end()\n                             ? Types[Context.VoidPtrTy]\n                             : Types[CanonicalType];\n\n    Address FieldAddress = Address(FieldPtr, Align);\n    FieldPtr = CGF.Builder.CreateLoad(FieldAddress);\n\n    // FIXME Need to handle bitfield here\n    GString = CGF.Builder.CreateGlobalStringPtr(\n        Format.concat(llvm::Twine('\\n')).str());\n    TmpRes = CGF.Builder.CreateCall(Func, {GString, FieldPtr});\n    Res = CGF.Builder.CreateAdd(Res, TmpRes);\n  }\n\n  GString = CGF.Builder.CreateGlobalStringPtr(Pad + \"}\\n\");\n  Value *TmpRes = CGF.Builder.CreateCall(Func, {GString});\n  Res = CGF.Builder.CreateAdd(Res, TmpRes);\n  return Res;\n}\n\nstatic bool\nTypeRequiresBuiltinLaunderImp(const ASTContext &Ctx, QualType Ty,\n                              llvm::SmallPtrSetImpl<const Decl *> &Seen) {\n  if (const auto *Arr = Ctx.getAsArrayType(Ty))\n    Ty = Ctx.getBaseElementType(Arr);\n\n  const auto *Record = Ty->getAsCXXRecordDecl();\n  if (!Record)\n    return false;\n\n  // We've already checked this type, or are in the process of checking it.\n  if (!Seen.insert(Record).second)\n    return false;\n\n  assert(Record->hasDefinition() &&\n         \"Incomplete types should already be diagnosed\");\n\n  if (Record->isDynamicClass())\n    return true;\n\n  for (FieldDecl *F : Record->fields()) {\n    if (TypeRequiresBuiltinLaunderImp(Ctx, F->getType(), Seen))\n      return true;\n  }\n  return false;\n}\n\n/// Determine if the specified type requires laundering by checking if it is a\n/// dynamic class type or contains a subobject which is a dynamic class type.\nstatic bool TypeRequiresBuiltinLaunder(CodeGenModule &CGM, QualType Ty) {\n  if (!CGM.getCodeGenOpts().StrictVTablePointers)\n    return false;\n  llvm::SmallPtrSet<const Decl *, 16> Seen;\n  return TypeRequiresBuiltinLaunderImp(CGM.getContext(), Ty, Seen);\n}\n\nRValue CodeGenFunction::emitRotate(const CallExpr *E, bool IsRotateRight) {\n  llvm::Value *Src = EmitScalarExpr(E->getArg(0));\n  llvm::Value *ShiftAmt = EmitScalarExpr(E->getArg(1));\n\n  // The builtin's shift arg may have a different type than the source arg and\n  // result, but the LLVM intrinsic uses the same type for all values.\n  llvm::Type *Ty = Src->getType();\n  ShiftAmt = Builder.CreateIntCast(ShiftAmt, Ty, false);\n\n  // Rotate is a special case of LLVM funnel shift - 1st 2 args are the same.\n  unsigned IID = IsRotateRight ? Intrinsic::fshr : Intrinsic::fshl;\n  Function *F = CGM.getIntrinsic(IID, Ty);\n  return RValue::get(Builder.CreateCall(F, { Src, Src, ShiftAmt }));\n}\n\n// Map math builtins for long-double to f128 version.\nstatic unsigned mutateLongDoubleBuiltin(unsigned BuiltinID) {\n  switch (BuiltinID) {\n#define MUTATE_LDBL(func) \\\n  case Builtin::BI__builtin_##func##l: \\\n    return Builtin::BI__builtin_##func##f128;\n  MUTATE_LDBL(sqrt)\n  MUTATE_LDBL(cbrt)\n  MUTATE_LDBL(fabs)\n  MUTATE_LDBL(log)\n  MUTATE_LDBL(log2)\n  MUTATE_LDBL(log10)\n  MUTATE_LDBL(log1p)\n  MUTATE_LDBL(logb)\n  MUTATE_LDBL(exp)\n  MUTATE_LDBL(exp2)\n  MUTATE_LDBL(expm1)\n  MUTATE_LDBL(fdim)\n  MUTATE_LDBL(hypot)\n  MUTATE_LDBL(ilogb)\n  MUTATE_LDBL(pow)\n  MUTATE_LDBL(fmin)\n  MUTATE_LDBL(fmax)\n  MUTATE_LDBL(ceil)\n  MUTATE_LDBL(trunc)\n  MUTATE_LDBL(rint)\n  MUTATE_LDBL(nearbyint)\n  MUTATE_LDBL(round)\n  MUTATE_LDBL(floor)\n  MUTATE_LDBL(lround)\n  MUTATE_LDBL(llround)\n  MUTATE_LDBL(lrint)\n  MUTATE_LDBL(llrint)\n  MUTATE_LDBL(fmod)\n  MUTATE_LDBL(modf)\n  MUTATE_LDBL(nan)\n  MUTATE_LDBL(nans)\n  MUTATE_LDBL(inf)\n  MUTATE_LDBL(fma)\n  MUTATE_LDBL(sin)\n  MUTATE_LDBL(cos)\n  MUTATE_LDBL(tan)\n  MUTATE_LDBL(sinh)\n  MUTATE_LDBL(cosh)\n  MUTATE_LDBL(tanh)\n  MUTATE_LDBL(asin)\n  MUTATE_LDBL(acos)\n  MUTATE_LDBL(atan)\n  MUTATE_LDBL(asinh)\n  MUTATE_LDBL(acosh)\n  MUTATE_LDBL(atanh)\n  MUTATE_LDBL(atan2)\n  MUTATE_LDBL(erf)\n  MUTATE_LDBL(erfc)\n  MUTATE_LDBL(ldexp)\n  MUTATE_LDBL(frexp)\n  MUTATE_LDBL(huge_val)\n  MUTATE_LDBL(copysign)\n  MUTATE_LDBL(nextafter)\n  MUTATE_LDBL(nexttoward)\n  MUTATE_LDBL(remainder)\n  MUTATE_LDBL(remquo)\n  MUTATE_LDBL(scalbln)\n  MUTATE_LDBL(scalbn)\n  MUTATE_LDBL(tgamma)\n  MUTATE_LDBL(lgamma)\n#undef MUTATE_LDBL\n  default:\n    return BuiltinID;\n  }\n}\n\nRValue CodeGenFunction::EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,\n                                        const CallExpr *E,\n                                        ReturnValueSlot ReturnValue) {\n  const FunctionDecl *FD = GD.getDecl()->getAsFunction();\n  // See if we can constant fold this builtin.  If so, don't emit it at all.\n  Expr::EvalResult Result;\n  if (E->EvaluateAsRValue(Result, CGM.getContext()) &&\n      !Result.hasSideEffects()) {\n    if (Result.Val.isInt())\n      return RValue::get(llvm::ConstantInt::get(getLLVMContext(),\n                                                Result.Val.getInt()));\n    if (Result.Val.isFloat())\n      return RValue::get(llvm::ConstantFP::get(getLLVMContext(),\n                                               Result.Val.getFloat()));\n  }\n\n  // If current long-double semantics is IEEE 128-bit, replace math builtins\n  // of long-double with f128 equivalent.\n  // TODO: This mutation should also be applied to other targets other than PPC,\n  // after backend supports IEEE 128-bit style libcalls.\n  if (getTarget().getTriple().isPPC64() &&\n      &getTarget().getLongDoubleFormat() == &llvm::APFloat::IEEEquad())\n    BuiltinID = mutateLongDoubleBuiltin(BuiltinID);\n\n  // If the builtin has been declared explicitly with an assembler label,\n  // disable the specialized emitting below. Ideally we should communicate the\n  // rename in IR, or at least avoid generating the intrinsic calls that are\n  // likely to get lowered to the renamed library functions.\n  const unsigned BuiltinIDIfNoAsmLabel =\n      FD->hasAttr<AsmLabelAttr>() ? 0 : BuiltinID;\n\n  // There are LLVM math intrinsics/instructions corresponding to math library\n  // functions except the LLVM op will never set errno while the math library\n  // might. Also, math builtins have the same semantics as their math library\n  // twins. Thus, we can transform math library and builtin calls to their\n  // LLVM counterparts if the call is marked 'const' (known to never set errno).\n  if (FD->hasAttr<ConstAttr>()) {\n    switch (BuiltinIDIfNoAsmLabel) {\n    case Builtin::BIceil:\n    case Builtin::BIceilf:\n    case Builtin::BIceill:\n    case Builtin::BI__builtin_ceil:\n    case Builtin::BI__builtin_ceilf:\n    case Builtin::BI__builtin_ceilf16:\n    case Builtin::BI__builtin_ceill:\n    case Builtin::BI__builtin_ceilf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::ceil,\n                                   Intrinsic::experimental_constrained_ceil));\n\n    case Builtin::BIcopysign:\n    case Builtin::BIcopysignf:\n    case Builtin::BIcopysignl:\n    case Builtin::BI__builtin_copysign:\n    case Builtin::BI__builtin_copysignf:\n    case Builtin::BI__builtin_copysignf16:\n    case Builtin::BI__builtin_copysignl:\n    case Builtin::BI__builtin_copysignf128:\n      return RValue::get(emitBinaryBuiltin(*this, E, Intrinsic::copysign));\n\n    case Builtin::BIcos:\n    case Builtin::BIcosf:\n    case Builtin::BIcosl:\n    case Builtin::BI__builtin_cos:\n    case Builtin::BI__builtin_cosf:\n    case Builtin::BI__builtin_cosf16:\n    case Builtin::BI__builtin_cosl:\n    case Builtin::BI__builtin_cosf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::cos,\n                                   Intrinsic::experimental_constrained_cos));\n\n    case Builtin::BIexp:\n    case Builtin::BIexpf:\n    case Builtin::BIexpl:\n    case Builtin::BI__builtin_exp:\n    case Builtin::BI__builtin_expf:\n    case Builtin::BI__builtin_expf16:\n    case Builtin::BI__builtin_expl:\n    case Builtin::BI__builtin_expf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::exp,\n                                   Intrinsic::experimental_constrained_exp));\n\n    case Builtin::BIexp2:\n    case Builtin::BIexp2f:\n    case Builtin::BIexp2l:\n    case Builtin::BI__builtin_exp2:\n    case Builtin::BI__builtin_exp2f:\n    case Builtin::BI__builtin_exp2f16:\n    case Builtin::BI__builtin_exp2l:\n    case Builtin::BI__builtin_exp2f128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::exp2,\n                                   Intrinsic::experimental_constrained_exp2));\n\n    case Builtin::BIfabs:\n    case Builtin::BIfabsf:\n    case Builtin::BIfabsl:\n    case Builtin::BI__builtin_fabs:\n    case Builtin::BI__builtin_fabsf:\n    case Builtin::BI__builtin_fabsf16:\n    case Builtin::BI__builtin_fabsl:\n    case Builtin::BI__builtin_fabsf128:\n      return RValue::get(emitUnaryBuiltin(*this, E, Intrinsic::fabs));\n\n    case Builtin::BIfloor:\n    case Builtin::BIfloorf:\n    case Builtin::BIfloorl:\n    case Builtin::BI__builtin_floor:\n    case Builtin::BI__builtin_floorf:\n    case Builtin::BI__builtin_floorf16:\n    case Builtin::BI__builtin_floorl:\n    case Builtin::BI__builtin_floorf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::floor,\n                                   Intrinsic::experimental_constrained_floor));\n\n    case Builtin::BIfma:\n    case Builtin::BIfmaf:\n    case Builtin::BIfmal:\n    case Builtin::BI__builtin_fma:\n    case Builtin::BI__builtin_fmaf:\n    case Builtin::BI__builtin_fmaf16:\n    case Builtin::BI__builtin_fmal:\n    case Builtin::BI__builtin_fmaf128:\n      return RValue::get(emitTernaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::fma,\n                                   Intrinsic::experimental_constrained_fma));\n\n    case Builtin::BIfmax:\n    case Builtin::BIfmaxf:\n    case Builtin::BIfmaxl:\n    case Builtin::BI__builtin_fmax:\n    case Builtin::BI__builtin_fmaxf:\n    case Builtin::BI__builtin_fmaxf16:\n    case Builtin::BI__builtin_fmaxl:\n    case Builtin::BI__builtin_fmaxf128:\n      return RValue::get(emitBinaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::maxnum,\n                                   Intrinsic::experimental_constrained_maxnum));\n\n    case Builtin::BIfmin:\n    case Builtin::BIfminf:\n    case Builtin::BIfminl:\n    case Builtin::BI__builtin_fmin:\n    case Builtin::BI__builtin_fminf:\n    case Builtin::BI__builtin_fminf16:\n    case Builtin::BI__builtin_fminl:\n    case Builtin::BI__builtin_fminf128:\n      return RValue::get(emitBinaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::minnum,\n                                   Intrinsic::experimental_constrained_minnum));\n\n    // fmod() is a special-case. It maps to the frem instruction rather than an\n    // LLVM intrinsic.\n    case Builtin::BIfmod:\n    case Builtin::BIfmodf:\n    case Builtin::BIfmodl:\n    case Builtin::BI__builtin_fmod:\n    case Builtin::BI__builtin_fmodf:\n    case Builtin::BI__builtin_fmodf16:\n    case Builtin::BI__builtin_fmodl:\n    case Builtin::BI__builtin_fmodf128: {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(*this, E);\n      Value *Arg1 = EmitScalarExpr(E->getArg(0));\n      Value *Arg2 = EmitScalarExpr(E->getArg(1));\n      return RValue::get(Builder.CreateFRem(Arg1, Arg2, \"fmod\"));\n    }\n\n    case Builtin::BIlog:\n    case Builtin::BIlogf:\n    case Builtin::BIlogl:\n    case Builtin::BI__builtin_log:\n    case Builtin::BI__builtin_logf:\n    case Builtin::BI__builtin_logf16:\n    case Builtin::BI__builtin_logl:\n    case Builtin::BI__builtin_logf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::log,\n                                   Intrinsic::experimental_constrained_log));\n\n    case Builtin::BIlog10:\n    case Builtin::BIlog10f:\n    case Builtin::BIlog10l:\n    case Builtin::BI__builtin_log10:\n    case Builtin::BI__builtin_log10f:\n    case Builtin::BI__builtin_log10f16:\n    case Builtin::BI__builtin_log10l:\n    case Builtin::BI__builtin_log10f128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::log10,\n                                   Intrinsic::experimental_constrained_log10));\n\n    case Builtin::BIlog2:\n    case Builtin::BIlog2f:\n    case Builtin::BIlog2l:\n    case Builtin::BI__builtin_log2:\n    case Builtin::BI__builtin_log2f:\n    case Builtin::BI__builtin_log2f16:\n    case Builtin::BI__builtin_log2l:\n    case Builtin::BI__builtin_log2f128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::log2,\n                                   Intrinsic::experimental_constrained_log2));\n\n    case Builtin::BInearbyint:\n    case Builtin::BInearbyintf:\n    case Builtin::BInearbyintl:\n    case Builtin::BI__builtin_nearbyint:\n    case Builtin::BI__builtin_nearbyintf:\n    case Builtin::BI__builtin_nearbyintl:\n    case Builtin::BI__builtin_nearbyintf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                Intrinsic::nearbyint,\n                                Intrinsic::experimental_constrained_nearbyint));\n\n    case Builtin::BIpow:\n    case Builtin::BIpowf:\n    case Builtin::BIpowl:\n    case Builtin::BI__builtin_pow:\n    case Builtin::BI__builtin_powf:\n    case Builtin::BI__builtin_powf16:\n    case Builtin::BI__builtin_powl:\n    case Builtin::BI__builtin_powf128:\n      return RValue::get(emitBinaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::pow,\n                                   Intrinsic::experimental_constrained_pow));\n\n    case Builtin::BIrint:\n    case Builtin::BIrintf:\n    case Builtin::BIrintl:\n    case Builtin::BI__builtin_rint:\n    case Builtin::BI__builtin_rintf:\n    case Builtin::BI__builtin_rintf16:\n    case Builtin::BI__builtin_rintl:\n    case Builtin::BI__builtin_rintf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::rint,\n                                   Intrinsic::experimental_constrained_rint));\n\n    case Builtin::BIround:\n    case Builtin::BIroundf:\n    case Builtin::BIroundl:\n    case Builtin::BI__builtin_round:\n    case Builtin::BI__builtin_roundf:\n    case Builtin::BI__builtin_roundf16:\n    case Builtin::BI__builtin_roundl:\n    case Builtin::BI__builtin_roundf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::round,\n                                   Intrinsic::experimental_constrained_round));\n\n    case Builtin::BIsin:\n    case Builtin::BIsinf:\n    case Builtin::BIsinl:\n    case Builtin::BI__builtin_sin:\n    case Builtin::BI__builtin_sinf:\n    case Builtin::BI__builtin_sinf16:\n    case Builtin::BI__builtin_sinl:\n    case Builtin::BI__builtin_sinf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::sin,\n                                   Intrinsic::experimental_constrained_sin));\n\n    case Builtin::BIsqrt:\n    case Builtin::BIsqrtf:\n    case Builtin::BIsqrtl:\n    case Builtin::BI__builtin_sqrt:\n    case Builtin::BI__builtin_sqrtf:\n    case Builtin::BI__builtin_sqrtf16:\n    case Builtin::BI__builtin_sqrtl:\n    case Builtin::BI__builtin_sqrtf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::sqrt,\n                                   Intrinsic::experimental_constrained_sqrt));\n\n    case Builtin::BItrunc:\n    case Builtin::BItruncf:\n    case Builtin::BItruncl:\n    case Builtin::BI__builtin_trunc:\n    case Builtin::BI__builtin_truncf:\n    case Builtin::BI__builtin_truncf16:\n    case Builtin::BI__builtin_truncl:\n    case Builtin::BI__builtin_truncf128:\n      return RValue::get(emitUnaryMaybeConstrainedFPBuiltin(*this, E,\n                                   Intrinsic::trunc,\n                                   Intrinsic::experimental_constrained_trunc));\n\n    case Builtin::BIlround:\n    case Builtin::BIlroundf:\n    case Builtin::BIlroundl:\n    case Builtin::BI__builtin_lround:\n    case Builtin::BI__builtin_lroundf:\n    case Builtin::BI__builtin_lroundl:\n    case Builtin::BI__builtin_lroundf128:\n      return RValue::get(emitMaybeConstrainedFPToIntRoundBuiltin(\n          *this, E, Intrinsic::lround,\n          Intrinsic::experimental_constrained_lround));\n\n    case Builtin::BIllround:\n    case Builtin::BIllroundf:\n    case Builtin::BIllroundl:\n    case Builtin::BI__builtin_llround:\n    case Builtin::BI__builtin_llroundf:\n    case Builtin::BI__builtin_llroundl:\n    case Builtin::BI__builtin_llroundf128:\n      return RValue::get(emitMaybeConstrainedFPToIntRoundBuiltin(\n          *this, E, Intrinsic::llround,\n          Intrinsic::experimental_constrained_llround));\n\n    case Builtin::BIlrint:\n    case Builtin::BIlrintf:\n    case Builtin::BIlrintl:\n    case Builtin::BI__builtin_lrint:\n    case Builtin::BI__builtin_lrintf:\n    case Builtin::BI__builtin_lrintl:\n    case Builtin::BI__builtin_lrintf128:\n      return RValue::get(emitMaybeConstrainedFPToIntRoundBuiltin(\n          *this, E, Intrinsic::lrint,\n          Intrinsic::experimental_constrained_lrint));\n\n    case Builtin::BIllrint:\n    case Builtin::BIllrintf:\n    case Builtin::BIllrintl:\n    case Builtin::BI__builtin_llrint:\n    case Builtin::BI__builtin_llrintf:\n    case Builtin::BI__builtin_llrintl:\n    case Builtin::BI__builtin_llrintf128:\n      return RValue::get(emitMaybeConstrainedFPToIntRoundBuiltin(\n          *this, E, Intrinsic::llrint,\n          Intrinsic::experimental_constrained_llrint));\n\n    default:\n      break;\n    }\n  }\n\n  switch (BuiltinIDIfNoAsmLabel) {\n  default: break;\n  case Builtin::BI__builtin___CFStringMakeConstantString:\n  case Builtin::BI__builtin___NSStringMakeConstantString:\n    return RValue::get(ConstantEmitter(*this).emitAbstract(E, E->getType()));\n  case Builtin::BI__builtin_stdarg_start:\n  case Builtin::BI__builtin_va_start:\n  case Builtin::BI__va_start:\n  case Builtin::BI__builtin_va_end:\n    return RValue::get(\n        EmitVAStartEnd(BuiltinID == Builtin::BI__va_start\n                           ? EmitScalarExpr(E->getArg(0))\n                           : EmitVAListRef(E->getArg(0)).getPointer(),\n                       BuiltinID != Builtin::BI__builtin_va_end));\n  case Builtin::BI__builtin_va_copy: {\n    Value *DstPtr = EmitVAListRef(E->getArg(0)).getPointer();\n    Value *SrcPtr = EmitVAListRef(E->getArg(1)).getPointer();\n\n    llvm::Type *Type = Int8PtrTy;\n\n    DstPtr = Builder.CreateBitCast(DstPtr, Type);\n    SrcPtr = Builder.CreateBitCast(SrcPtr, Type);\n    return RValue::get(Builder.CreateCall(CGM.getIntrinsic(Intrinsic::vacopy),\n                                          {DstPtr, SrcPtr}));\n  }\n  case Builtin::BI__builtin_abs:\n  case Builtin::BI__builtin_labs:\n  case Builtin::BI__builtin_llabs: {\n    // X < 0 ? -X : X\n    // The negation has 'nsw' because abs of INT_MIN is undefined.\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n    Value *NegOp = Builder.CreateNSWNeg(ArgValue, \"neg\");\n    Constant *Zero = llvm::Constant::getNullValue(ArgValue->getType());\n    Value *CmpResult = Builder.CreateICmpSLT(ArgValue, Zero, \"abscond\");\n    Value *Result = Builder.CreateSelect(CmpResult, NegOp, ArgValue, \"abs\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_complex: {\n    Value *Real = EmitScalarExpr(E->getArg(0));\n    Value *Imag = EmitScalarExpr(E->getArg(1));\n    return RValue::getComplex({Real, Imag});\n  }\n  case Builtin::BI__builtin_conj:\n  case Builtin::BI__builtin_conjf:\n  case Builtin::BI__builtin_conjl:\n  case Builtin::BIconj:\n  case Builtin::BIconjf:\n  case Builtin::BIconjl: {\n    ComplexPairTy ComplexVal = EmitComplexExpr(E->getArg(0));\n    Value *Real = ComplexVal.first;\n    Value *Imag = ComplexVal.second;\n    Imag = Builder.CreateFNeg(Imag, \"neg\");\n    return RValue::getComplex(std::make_pair(Real, Imag));\n  }\n  case Builtin::BI__builtin_creal:\n  case Builtin::BI__builtin_crealf:\n  case Builtin::BI__builtin_creall:\n  case Builtin::BIcreal:\n  case Builtin::BIcrealf:\n  case Builtin::BIcreall: {\n    ComplexPairTy ComplexVal = EmitComplexExpr(E->getArg(0));\n    return RValue::get(ComplexVal.first);\n  }\n\n  case Builtin::BI__builtin_dump_struct: {\n    llvm::Type *LLVMIntTy = getTypes().ConvertType(getContext().IntTy);\n    llvm::FunctionType *LLVMFuncType = llvm::FunctionType::get(\n        LLVMIntTy, {llvm::Type::getInt8PtrTy(getLLVMContext())}, true);\n\n    Value *Func = EmitScalarExpr(E->getArg(1)->IgnoreImpCasts());\n    CharUnits Arg0Align = EmitPointerWithAlignment(E->getArg(0)).getAlignment();\n\n    const Expr *Arg0 = E->getArg(0)->IgnoreImpCasts();\n    QualType Arg0Type = Arg0->getType()->getPointeeType();\n\n    Value *RecordPtr = EmitScalarExpr(Arg0);\n    Value *Res = dumpRecord(*this, Arg0Type, RecordPtr, Arg0Align,\n                            {LLVMFuncType, Func}, 0);\n    return RValue::get(Res);\n  }\n\n  case Builtin::BI__builtin_preserve_access_index: {\n    // Only enabled preserved access index region when debuginfo\n    // is available as debuginfo is needed to preserve user-level\n    // access pattern.\n    if (!getDebugInfo()) {\n      CGM.Error(E->getExprLoc(), \"using builtin_preserve_access_index() without -g\");\n      return RValue::get(EmitScalarExpr(E->getArg(0)));\n    }\n\n    // Nested builtin_preserve_access_index() not supported\n    if (IsInPreservedAIRegion) {\n      CGM.Error(E->getExprLoc(), \"nested builtin_preserve_access_index() not supported\");\n      return RValue::get(EmitScalarExpr(E->getArg(0)));\n    }\n\n    IsInPreservedAIRegion = true;\n    Value *Res = EmitScalarExpr(E->getArg(0));\n    IsInPreservedAIRegion = false;\n    return RValue::get(Res);\n  }\n\n  case Builtin::BI__builtin_cimag:\n  case Builtin::BI__builtin_cimagf:\n  case Builtin::BI__builtin_cimagl:\n  case Builtin::BIcimag:\n  case Builtin::BIcimagf:\n  case Builtin::BIcimagl: {\n    ComplexPairTy ComplexVal = EmitComplexExpr(E->getArg(0));\n    return RValue::get(ComplexVal.second);\n  }\n\n  case Builtin::BI__builtin_clrsb:\n  case Builtin::BI__builtin_clrsbl:\n  case Builtin::BI__builtin_clrsbll: {\n    // clrsb(x) -> clz(x < 0 ? ~x : x) - 1 or\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n\n    llvm::Type *ArgType = ArgValue->getType();\n    Function *F = CGM.getIntrinsic(Intrinsic::ctlz, ArgType);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *Zero = llvm::Constant::getNullValue(ArgType);\n    Value *IsNeg = Builder.CreateICmpSLT(ArgValue, Zero, \"isneg\");\n    Value *Inverse = Builder.CreateNot(ArgValue, \"not\");\n    Value *Tmp = Builder.CreateSelect(IsNeg, Inverse, ArgValue);\n    Value *Ctlz = Builder.CreateCall(F, {Tmp, Builder.getFalse()});\n    Value *Result = Builder.CreateSub(Ctlz, llvm::ConstantInt::get(ArgType, 1));\n    Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                   \"cast\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_ctzs:\n  case Builtin::BI__builtin_ctz:\n  case Builtin::BI__builtin_ctzl:\n  case Builtin::BI__builtin_ctzll: {\n    Value *ArgValue = EmitCheckedArgForBuiltin(E->getArg(0), BCK_CTZPassedZero);\n\n    llvm::Type *ArgType = ArgValue->getType();\n    Function *F = CGM.getIntrinsic(Intrinsic::cttz, ArgType);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *ZeroUndef = Builder.getInt1(getTarget().isCLZForZeroUndef());\n    Value *Result = Builder.CreateCall(F, {ArgValue, ZeroUndef});\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                     \"cast\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_clzs:\n  case Builtin::BI__builtin_clz:\n  case Builtin::BI__builtin_clzl:\n  case Builtin::BI__builtin_clzll: {\n    Value *ArgValue = EmitCheckedArgForBuiltin(E->getArg(0), BCK_CLZPassedZero);\n\n    llvm::Type *ArgType = ArgValue->getType();\n    Function *F = CGM.getIntrinsic(Intrinsic::ctlz, ArgType);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *ZeroUndef = Builder.getInt1(getTarget().isCLZForZeroUndef());\n    Value *Result = Builder.CreateCall(F, {ArgValue, ZeroUndef});\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                     \"cast\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_ffs:\n  case Builtin::BI__builtin_ffsl:\n  case Builtin::BI__builtin_ffsll: {\n    // ffs(x) -> x ? cttz(x) + 1 : 0\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n\n    llvm::Type *ArgType = ArgValue->getType();\n    Function *F = CGM.getIntrinsic(Intrinsic::cttz, ArgType);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *Tmp =\n        Builder.CreateAdd(Builder.CreateCall(F, {ArgValue, Builder.getTrue()}),\n                          llvm::ConstantInt::get(ArgType, 1));\n    Value *Zero = llvm::Constant::getNullValue(ArgType);\n    Value *IsZero = Builder.CreateICmpEQ(ArgValue, Zero, \"iszero\");\n    Value *Result = Builder.CreateSelect(IsZero, Zero, Tmp, \"ffs\");\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                     \"cast\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_parity:\n  case Builtin::BI__builtin_parityl:\n  case Builtin::BI__builtin_parityll: {\n    // parity(x) -> ctpop(x) & 1\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n\n    llvm::Type *ArgType = ArgValue->getType();\n    Function *F = CGM.getIntrinsic(Intrinsic::ctpop, ArgType);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *Tmp = Builder.CreateCall(F, ArgValue);\n    Value *Result = Builder.CreateAnd(Tmp, llvm::ConstantInt::get(ArgType, 1));\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                     \"cast\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__lzcnt16:\n  case Builtin::BI__lzcnt:\n  case Builtin::BI__lzcnt64: {\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n\n    llvm::Type *ArgType = ArgValue->getType();\n    Function *F = CGM.getIntrinsic(Intrinsic::ctlz, ArgType);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *Result = Builder.CreateCall(F, {ArgValue, Builder.getFalse()});\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                     \"cast\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__popcnt16:\n  case Builtin::BI__popcnt:\n  case Builtin::BI__popcnt64:\n  case Builtin::BI__builtin_popcount:\n  case Builtin::BI__builtin_popcountl:\n  case Builtin::BI__builtin_popcountll: {\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n\n    llvm::Type *ArgType = ArgValue->getType();\n    Function *F = CGM.getIntrinsic(Intrinsic::ctpop, ArgType);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *Result = Builder.CreateCall(F, ArgValue);\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                     \"cast\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_unpredictable: {\n    // Always return the argument of __builtin_unpredictable. LLVM does not\n    // handle this builtin. Metadata for this builtin should be added directly\n    // to instructions such as branches or switches that use it.\n    return RValue::get(EmitScalarExpr(E->getArg(0)));\n  }\n  case Builtin::BI__builtin_expect: {\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n    llvm::Type *ArgType = ArgValue->getType();\n\n    Value *ExpectedValue = EmitScalarExpr(E->getArg(1));\n    // Don't generate llvm.expect on -O0 as the backend won't use it for\n    // anything.\n    // Note, we still IRGen ExpectedValue because it could have side-effects.\n    if (CGM.getCodeGenOpts().OptimizationLevel == 0)\n      return RValue::get(ArgValue);\n\n    Function *FnExpect = CGM.getIntrinsic(Intrinsic::expect, ArgType);\n    Value *Result =\n        Builder.CreateCall(FnExpect, {ArgValue, ExpectedValue}, \"expval\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_expect_with_probability: {\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n    llvm::Type *ArgType = ArgValue->getType();\n\n    Value *ExpectedValue = EmitScalarExpr(E->getArg(1));\n    llvm::APFloat Probability(0.0);\n    const Expr *ProbArg = E->getArg(2);\n    bool EvalSucceed = ProbArg->EvaluateAsFloat(Probability, CGM.getContext());\n    assert(EvalSucceed && \"probability should be able to evaluate as float\");\n    (void)EvalSucceed;\n    bool LoseInfo = false;\n    Probability.convert(llvm::APFloat::IEEEdouble(),\n                        llvm::RoundingMode::Dynamic, &LoseInfo);\n    llvm::Type *Ty = ConvertType(ProbArg->getType());\n    Constant *Confidence = ConstantFP::get(Ty, Probability);\n    // Don't generate llvm.expect.with.probability on -O0 as the backend\n    // won't use it for anything.\n    // Note, we still IRGen ExpectedValue because it could have side-effects.\n    if (CGM.getCodeGenOpts().OptimizationLevel == 0)\n      return RValue::get(ArgValue);\n\n    Function *FnExpect =\n        CGM.getIntrinsic(Intrinsic::expect_with_probability, ArgType);\n    Value *Result = Builder.CreateCall(\n        FnExpect, {ArgValue, ExpectedValue, Confidence}, \"expval\");\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_assume_aligned: {\n    const Expr *Ptr = E->getArg(0);\n    Value *PtrValue = EmitScalarExpr(Ptr);\n    Value *OffsetValue =\n      (E->getNumArgs() > 2) ? EmitScalarExpr(E->getArg(2)) : nullptr;\n\n    Value *AlignmentValue = EmitScalarExpr(E->getArg(1));\n    ConstantInt *AlignmentCI = cast<ConstantInt>(AlignmentValue);\n    if (AlignmentCI->getValue().ugt(llvm::Value::MaximumAlignment))\n      AlignmentCI = ConstantInt::get(AlignmentCI->getType(),\n                                     llvm::Value::MaximumAlignment);\n\n    emitAlignmentAssumption(PtrValue, Ptr,\n                            /*The expr loc is sufficient.*/ SourceLocation(),\n                            AlignmentCI, OffsetValue);\n    return RValue::get(PtrValue);\n  }\n  case Builtin::BI__assume:\n  case Builtin::BI__builtin_assume: {\n    if (E->getArg(0)->HasSideEffects(getContext()))\n      return RValue::get(nullptr);\n\n    Value *ArgValue = EmitScalarExpr(E->getArg(0));\n    Function *FnAssume = CGM.getIntrinsic(Intrinsic::assume);\n    return RValue::get(Builder.CreateCall(FnAssume, ArgValue));\n  }\n  case Builtin::BI__builtin_bswap16:\n  case Builtin::BI__builtin_bswap32:\n  case Builtin::BI__builtin_bswap64: {\n    return RValue::get(emitUnaryBuiltin(*this, E, Intrinsic::bswap));\n  }\n  case Builtin::BI__builtin_bitreverse8:\n  case Builtin::BI__builtin_bitreverse16:\n  case Builtin::BI__builtin_bitreverse32:\n  case Builtin::BI__builtin_bitreverse64: {\n    return RValue::get(emitUnaryBuiltin(*this, E, Intrinsic::bitreverse));\n  }\n  case Builtin::BI__builtin_rotateleft8:\n  case Builtin::BI__builtin_rotateleft16:\n  case Builtin::BI__builtin_rotateleft32:\n  case Builtin::BI__builtin_rotateleft64:\n  case Builtin::BI_rotl8: // Microsoft variants of rotate left\n  case Builtin::BI_rotl16:\n  case Builtin::BI_rotl:\n  case Builtin::BI_lrotl:\n  case Builtin::BI_rotl64:\n    return emitRotate(E, false);\n\n  case Builtin::BI__builtin_rotateright8:\n  case Builtin::BI__builtin_rotateright16:\n  case Builtin::BI__builtin_rotateright32:\n  case Builtin::BI__builtin_rotateright64:\n  case Builtin::BI_rotr8: // Microsoft variants of rotate right\n  case Builtin::BI_rotr16:\n  case Builtin::BI_rotr:\n  case Builtin::BI_lrotr:\n  case Builtin::BI_rotr64:\n    return emitRotate(E, true);\n\n  case Builtin::BI__builtin_constant_p: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n\n    const Expr *Arg = E->getArg(0);\n    QualType ArgType = Arg->getType();\n    // FIXME: The allowance for Obj-C pointers and block pointers is historical\n    // and likely a mistake.\n    if (!ArgType->isIntegralOrEnumerationType() && !ArgType->isFloatingType() &&\n        !ArgType->isObjCObjectPointerType() && !ArgType->isBlockPointerType())\n      // Per the GCC documentation, only numeric constants are recognized after\n      // inlining.\n      return RValue::get(ConstantInt::get(ResultType, 0));\n\n    if (Arg->HasSideEffects(getContext()))\n      // The argument is unevaluated, so be conservative if it might have\n      // side-effects.\n      return RValue::get(ConstantInt::get(ResultType, 0));\n\n    Value *ArgValue = EmitScalarExpr(Arg);\n    if (ArgType->isObjCObjectPointerType()) {\n      // Convert Objective-C objects to id because we cannot distinguish between\n      // LLVM types for Obj-C classes as they are opaque.\n      ArgType = CGM.getContext().getObjCIdType();\n      ArgValue = Builder.CreateBitCast(ArgValue, ConvertType(ArgType));\n    }\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::is_constant, ConvertType(ArgType));\n    Value *Result = Builder.CreateCall(F, ArgValue);\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/false);\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_dynamic_object_size:\n  case Builtin::BI__builtin_object_size: {\n    unsigned Type =\n        E->getArg(1)->EvaluateKnownConstInt(getContext()).getZExtValue();\n    auto *ResType = cast<llvm::IntegerType>(ConvertType(E->getType()));\n\n    // We pass this builtin onto the optimizer so that it can figure out the\n    // object size in more complex cases.\n    bool IsDynamic = BuiltinID == Builtin::BI__builtin_dynamic_object_size;\n    return RValue::get(emitBuiltinObjectSize(E->getArg(0), Type, ResType,\n                                             /*EmittedE=*/nullptr, IsDynamic));\n  }\n  case Builtin::BI__builtin_prefetch: {\n    Value *Locality, *RW, *Address = EmitScalarExpr(E->getArg(0));\n    // FIXME: Technically these constants should of type 'int', yes?\n    RW = (E->getNumArgs() > 1) ? EmitScalarExpr(E->getArg(1)) :\n      llvm::ConstantInt::get(Int32Ty, 0);\n    Locality = (E->getNumArgs() > 2) ? EmitScalarExpr(E->getArg(2)) :\n      llvm::ConstantInt::get(Int32Ty, 3);\n    Value *Data = llvm::ConstantInt::get(Int32Ty, 1);\n    Function *F = CGM.getIntrinsic(Intrinsic::prefetch, Address->getType());\n    return RValue::get(Builder.CreateCall(F, {Address, RW, Locality, Data}));\n  }\n  case Builtin::BI__builtin_readcyclecounter: {\n    Function *F = CGM.getIntrinsic(Intrinsic::readcyclecounter);\n    return RValue::get(Builder.CreateCall(F));\n  }\n  case Builtin::BI__builtin___clear_cache: {\n    Value *Begin = EmitScalarExpr(E->getArg(0));\n    Value *End = EmitScalarExpr(E->getArg(1));\n    Function *F = CGM.getIntrinsic(Intrinsic::clear_cache);\n    return RValue::get(Builder.CreateCall(F, {Begin, End}));\n  }\n  case Builtin::BI__builtin_trap:\n    return RValue::get(EmitTrapCall(Intrinsic::trap));\n  case Builtin::BI__debugbreak:\n    return RValue::get(EmitTrapCall(Intrinsic::debugtrap));\n  case Builtin::BI__builtin_unreachable: {\n    EmitUnreachable(E->getExprLoc());\n\n    // We do need to preserve an insertion point.\n    EmitBlock(createBasicBlock(\"unreachable.cont\"));\n\n    return RValue::get(nullptr);\n  }\n\n  case Builtin::BI__builtin_powi:\n  case Builtin::BI__builtin_powif:\n  case Builtin::BI__builtin_powil:\n    return RValue::get(emitBinaryMaybeConstrainedFPBuiltin(\n        *this, E, Intrinsic::powi, Intrinsic::experimental_constrained_powi));\n\n  case Builtin::BI__builtin_isgreater:\n  case Builtin::BI__builtin_isgreaterequal:\n  case Builtin::BI__builtin_isless:\n  case Builtin::BI__builtin_islessequal:\n  case Builtin::BI__builtin_islessgreater:\n  case Builtin::BI__builtin_isunordered: {\n    // Ordered comparisons: we know the arguments to these are matching scalar\n    // floating point values.\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(*this, E);\n    // FIXME: for strictfp/IEEE-754 we need to not trap on SNaN here.\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unknown ordered comparison\");\n    case Builtin::BI__builtin_isgreater:\n      LHS = Builder.CreateFCmpOGT(LHS, RHS, \"cmp\");\n      break;\n    case Builtin::BI__builtin_isgreaterequal:\n      LHS = Builder.CreateFCmpOGE(LHS, RHS, \"cmp\");\n      break;\n    case Builtin::BI__builtin_isless:\n      LHS = Builder.CreateFCmpOLT(LHS, RHS, \"cmp\");\n      break;\n    case Builtin::BI__builtin_islessequal:\n      LHS = Builder.CreateFCmpOLE(LHS, RHS, \"cmp\");\n      break;\n    case Builtin::BI__builtin_islessgreater:\n      LHS = Builder.CreateFCmpONE(LHS, RHS, \"cmp\");\n      break;\n    case Builtin::BI__builtin_isunordered:\n      LHS = Builder.CreateFCmpUNO(LHS, RHS, \"cmp\");\n      break;\n    }\n    // ZExt bool to int type.\n    return RValue::get(Builder.CreateZExt(LHS, ConvertType(E->getType())));\n  }\n  case Builtin::BI__builtin_isnan: {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(*this, E);\n    // FIXME: for strictfp/IEEE-754 we need to not trap on SNaN here.\n    Value *V = EmitScalarExpr(E->getArg(0));\n    V = Builder.CreateFCmpUNO(V, V, \"cmp\");\n    return RValue::get(Builder.CreateZExt(V, ConvertType(E->getType())));\n  }\n\n  case Builtin::BI__builtin_matrix_transpose: {\n    const auto *MatrixTy = E->getArg(0)->getType()->getAs<ConstantMatrixType>();\n    Value *MatValue = EmitScalarExpr(E->getArg(0));\n    MatrixBuilder<CGBuilderTy> MB(Builder);\n    Value *Result = MB.CreateMatrixTranspose(MatValue, MatrixTy->getNumRows(),\n                                             MatrixTy->getNumColumns());\n    return RValue::get(Result);\n  }\n\n  case Builtin::BI__builtin_matrix_column_major_load: {\n    MatrixBuilder<CGBuilderTy> MB(Builder);\n    // Emit everything that isn't dependent on the first parameter type\n    Value *Stride = EmitScalarExpr(E->getArg(3));\n    const auto *ResultTy = E->getType()->getAs<ConstantMatrixType>();\n    auto *PtrTy = E->getArg(0)->getType()->getAs<PointerType>();\n    assert(PtrTy && \"arg0 must be of pointer type\");\n    bool IsVolatile = PtrTy->getPointeeType().isVolatileQualified();\n\n    Address Src = EmitPointerWithAlignment(E->getArg(0));\n    EmitNonNullArgCheck(RValue::get(Src.getPointer()), E->getArg(0)->getType(),\n                        E->getArg(0)->getExprLoc(), FD, 0);\n    Value *Result = MB.CreateColumnMajorLoad(\n        Src.getPointer(), Align(Src.getAlignment().getQuantity()), Stride,\n        IsVolatile, ResultTy->getNumRows(), ResultTy->getNumColumns(),\n        \"matrix\");\n    return RValue::get(Result);\n  }\n\n  case Builtin::BI__builtin_matrix_column_major_store: {\n    MatrixBuilder<CGBuilderTy> MB(Builder);\n    Value *Matrix = EmitScalarExpr(E->getArg(0));\n    Address Dst = EmitPointerWithAlignment(E->getArg(1));\n    Value *Stride = EmitScalarExpr(E->getArg(2));\n\n    const auto *MatrixTy = E->getArg(0)->getType()->getAs<ConstantMatrixType>();\n    auto *PtrTy = E->getArg(1)->getType()->getAs<PointerType>();\n    assert(PtrTy && \"arg1 must be of pointer type\");\n    bool IsVolatile = PtrTy->getPointeeType().isVolatileQualified();\n\n    EmitNonNullArgCheck(RValue::get(Dst.getPointer()), E->getArg(1)->getType(),\n                        E->getArg(1)->getExprLoc(), FD, 0);\n    Value *Result = MB.CreateColumnMajorStore(\n        Matrix, Dst.getPointer(), Align(Dst.getAlignment().getQuantity()),\n        Stride, IsVolatile, MatrixTy->getNumRows(), MatrixTy->getNumColumns());\n    return RValue::get(Result);\n  }\n\n  case Builtin::BIfinite:\n  case Builtin::BI__finite:\n  case Builtin::BIfinitef:\n  case Builtin::BI__finitef:\n  case Builtin::BIfinitel:\n  case Builtin::BI__finitel:\n  case Builtin::BI__builtin_isinf:\n  case Builtin::BI__builtin_isfinite: {\n    // isinf(x)    --> fabs(x) == infinity\n    // isfinite(x) --> fabs(x) != infinity\n    // x != NaN via the ordered compare in either case.\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(*this, E);\n    // FIXME: for strictfp/IEEE-754 we need to not trap on SNaN here.\n    Value *V = EmitScalarExpr(E->getArg(0));\n    Value *Fabs = EmitFAbs(*this, V);\n    Constant *Infinity = ConstantFP::getInfinity(V->getType());\n    CmpInst::Predicate Pred = (BuiltinID == Builtin::BI__builtin_isinf)\n                                  ? CmpInst::FCMP_OEQ\n                                  : CmpInst::FCMP_ONE;\n    Value *FCmp = Builder.CreateFCmp(Pred, Fabs, Infinity, \"cmpinf\");\n    return RValue::get(Builder.CreateZExt(FCmp, ConvertType(E->getType())));\n  }\n\n  case Builtin::BI__builtin_isinf_sign: {\n    // isinf_sign(x) -> fabs(x) == infinity ? (signbit(x) ? -1 : 1) : 0\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(*this, E);\n    // FIXME: for strictfp/IEEE-754 we need to not trap on SNaN here.\n    Value *Arg = EmitScalarExpr(E->getArg(0));\n    Value *AbsArg = EmitFAbs(*this, Arg);\n    Value *IsInf = Builder.CreateFCmpOEQ(\n        AbsArg, ConstantFP::getInfinity(Arg->getType()), \"isinf\");\n    Value *IsNeg = EmitSignBit(*this, Arg);\n\n    llvm::Type *IntTy = ConvertType(E->getType());\n    Value *Zero = Constant::getNullValue(IntTy);\n    Value *One = ConstantInt::get(IntTy, 1);\n    Value *NegativeOne = ConstantInt::get(IntTy, -1);\n    Value *SignResult = Builder.CreateSelect(IsNeg, NegativeOne, One);\n    Value *Result = Builder.CreateSelect(IsInf, SignResult, Zero);\n    return RValue::get(Result);\n  }\n\n  case Builtin::BI__builtin_isnormal: {\n    // isnormal(x) --> x == x && fabsf(x) < infinity && fabsf(x) >= float_min\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(*this, E);\n    // FIXME: for strictfp/IEEE-754 we need to not trap on SNaN here.\n    Value *V = EmitScalarExpr(E->getArg(0));\n    Value *Eq = Builder.CreateFCmpOEQ(V, V, \"iseq\");\n\n    Value *Abs = EmitFAbs(*this, V);\n    Value *IsLessThanInf =\n      Builder.CreateFCmpULT(Abs, ConstantFP::getInfinity(V->getType()),\"isinf\");\n    APFloat Smallest = APFloat::getSmallestNormalized(\n                   getContext().getFloatTypeSemantics(E->getArg(0)->getType()));\n    Value *IsNormal =\n      Builder.CreateFCmpUGE(Abs, ConstantFP::get(V->getContext(), Smallest),\n                            \"isnormal\");\n    V = Builder.CreateAnd(Eq, IsLessThanInf, \"and\");\n    V = Builder.CreateAnd(V, IsNormal, \"and\");\n    return RValue::get(Builder.CreateZExt(V, ConvertType(E->getType())));\n  }\n\n  case Builtin::BI__builtin_flt_rounds: {\n    Function *F = CGM.getIntrinsic(Intrinsic::flt_rounds);\n\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *Result = Builder.CreateCall(F);\n    if (Result->getType() != ResultType)\n      Result = Builder.CreateIntCast(Result, ResultType, /*isSigned*/true,\n                                     \"cast\");\n    return RValue::get(Result);\n  }\n\n  case Builtin::BI__builtin_fpclassify: {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(*this, E);\n    // FIXME: for strictfp/IEEE-754 we need to not trap on SNaN here.\n    Value *V = EmitScalarExpr(E->getArg(5));\n    llvm::Type *Ty = ConvertType(E->getArg(5)->getType());\n\n    // Create Result\n    BasicBlock *Begin = Builder.GetInsertBlock();\n    BasicBlock *End = createBasicBlock(\"fpclassify_end\", this->CurFn);\n    Builder.SetInsertPoint(End);\n    PHINode *Result =\n      Builder.CreatePHI(ConvertType(E->getArg(0)->getType()), 4,\n                        \"fpclassify_result\");\n\n    // if (V==0) return FP_ZERO\n    Builder.SetInsertPoint(Begin);\n    Value *IsZero = Builder.CreateFCmpOEQ(V, Constant::getNullValue(Ty),\n                                          \"iszero\");\n    Value *ZeroLiteral = EmitScalarExpr(E->getArg(4));\n    BasicBlock *NotZero = createBasicBlock(\"fpclassify_not_zero\", this->CurFn);\n    Builder.CreateCondBr(IsZero, End, NotZero);\n    Result->addIncoming(ZeroLiteral, Begin);\n\n    // if (V != V) return FP_NAN\n    Builder.SetInsertPoint(NotZero);\n    Value *IsNan = Builder.CreateFCmpUNO(V, V, \"cmp\");\n    Value *NanLiteral = EmitScalarExpr(E->getArg(0));\n    BasicBlock *NotNan = createBasicBlock(\"fpclassify_not_nan\", this->CurFn);\n    Builder.CreateCondBr(IsNan, End, NotNan);\n    Result->addIncoming(NanLiteral, NotZero);\n\n    // if (fabs(V) == infinity) return FP_INFINITY\n    Builder.SetInsertPoint(NotNan);\n    Value *VAbs = EmitFAbs(*this, V);\n    Value *IsInf =\n      Builder.CreateFCmpOEQ(VAbs, ConstantFP::getInfinity(V->getType()),\n                            \"isinf\");\n    Value *InfLiteral = EmitScalarExpr(E->getArg(1));\n    BasicBlock *NotInf = createBasicBlock(\"fpclassify_not_inf\", this->CurFn);\n    Builder.CreateCondBr(IsInf, End, NotInf);\n    Result->addIncoming(InfLiteral, NotNan);\n\n    // if (fabs(V) >= MIN_NORMAL) return FP_NORMAL else FP_SUBNORMAL\n    Builder.SetInsertPoint(NotInf);\n    APFloat Smallest = APFloat::getSmallestNormalized(\n        getContext().getFloatTypeSemantics(E->getArg(5)->getType()));\n    Value *IsNormal =\n      Builder.CreateFCmpUGE(VAbs, ConstantFP::get(V->getContext(), Smallest),\n                            \"isnormal\");\n    Value *NormalResult =\n      Builder.CreateSelect(IsNormal, EmitScalarExpr(E->getArg(2)),\n                           EmitScalarExpr(E->getArg(3)));\n    Builder.CreateBr(End);\n    Result->addIncoming(NormalResult, NotInf);\n\n    // return Result\n    Builder.SetInsertPoint(End);\n    return RValue::get(Result);\n  }\n\n  case Builtin::BIalloca:\n  case Builtin::BI_alloca:\n  case Builtin::BI__builtin_alloca: {\n    Value *Size = EmitScalarExpr(E->getArg(0));\n    const TargetInfo &TI = getContext().getTargetInfo();\n    // The alignment of the alloca should correspond to __BIGGEST_ALIGNMENT__.\n    const Align SuitableAlignmentInBytes =\n        CGM.getContext()\n            .toCharUnitsFromBits(TI.getSuitableAlign())\n            .getAsAlign();\n    AllocaInst *AI = Builder.CreateAlloca(Builder.getInt8Ty(), Size);\n    AI->setAlignment(SuitableAlignmentInBytes);\n    initializeAlloca(*this, AI, Size, SuitableAlignmentInBytes);\n    return RValue::get(AI);\n  }\n\n  case Builtin::BI__builtin_alloca_with_align: {\n    Value *Size = EmitScalarExpr(E->getArg(0));\n    Value *AlignmentInBitsValue = EmitScalarExpr(E->getArg(1));\n    auto *AlignmentInBitsCI = cast<ConstantInt>(AlignmentInBitsValue);\n    unsigned AlignmentInBits = AlignmentInBitsCI->getZExtValue();\n    const Align AlignmentInBytes =\n        CGM.getContext().toCharUnitsFromBits(AlignmentInBits).getAsAlign();\n    AllocaInst *AI = Builder.CreateAlloca(Builder.getInt8Ty(), Size);\n    AI->setAlignment(AlignmentInBytes);\n    initializeAlloca(*this, AI, Size, AlignmentInBytes);\n    return RValue::get(AI);\n  }\n\n  case Builtin::BIbzero:\n  case Builtin::BI__builtin_bzero: {\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Value *SizeVal = EmitScalarExpr(E->getArg(1));\n    EmitNonNullArgCheck(RValue::get(Dest.getPointer()), E->getArg(0)->getType(),\n                        E->getArg(0)->getExprLoc(), FD, 0);\n    Builder.CreateMemSet(Dest, Builder.getInt8(0), SizeVal, false);\n    return RValue::get(nullptr);\n  }\n  case Builtin::BImemcpy:\n  case Builtin::BI__builtin_memcpy:\n  case Builtin::BImempcpy:\n  case Builtin::BI__builtin_mempcpy: {\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Address Src = EmitPointerWithAlignment(E->getArg(1));\n    Value *SizeVal = EmitScalarExpr(E->getArg(2));\n    EmitNonNullArgCheck(RValue::get(Dest.getPointer()), E->getArg(0)->getType(),\n                        E->getArg(0)->getExprLoc(), FD, 0);\n    EmitNonNullArgCheck(RValue::get(Src.getPointer()), E->getArg(1)->getType(),\n                        E->getArg(1)->getExprLoc(), FD, 1);\n    Builder.CreateMemCpy(Dest, Src, SizeVal, false);\n    if (BuiltinID == Builtin::BImempcpy ||\n        BuiltinID == Builtin::BI__builtin_mempcpy)\n      return RValue::get(Builder.CreateInBoundsGEP(Dest.getPointer(), SizeVal));\n    else\n      return RValue::get(Dest.getPointer());\n  }\n\n  case Builtin::BI__builtin_memcpy_inline: {\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Address Src = EmitPointerWithAlignment(E->getArg(1));\n    uint64_t Size =\n        E->getArg(2)->EvaluateKnownConstInt(getContext()).getZExtValue();\n    EmitNonNullArgCheck(RValue::get(Dest.getPointer()), E->getArg(0)->getType(),\n                        E->getArg(0)->getExprLoc(), FD, 0);\n    EmitNonNullArgCheck(RValue::get(Src.getPointer()), E->getArg(1)->getType(),\n                        E->getArg(1)->getExprLoc(), FD, 1);\n    Builder.CreateMemCpyInline(Dest, Src, Size);\n    return RValue::get(nullptr);\n  }\n\n  case Builtin::BI__builtin_char_memchr:\n    BuiltinID = Builtin::BI__builtin_memchr;\n    break;\n\n  case Builtin::BI__builtin___memcpy_chk: {\n    // fold __builtin_memcpy_chk(x, y, cst1, cst2) to memcpy iff cst1<=cst2.\n    Expr::EvalResult SizeResult, DstSizeResult;\n    if (!E->getArg(2)->EvaluateAsInt(SizeResult, CGM.getContext()) ||\n        !E->getArg(3)->EvaluateAsInt(DstSizeResult, CGM.getContext()))\n      break;\n    llvm::APSInt Size = SizeResult.Val.getInt();\n    llvm::APSInt DstSize = DstSizeResult.Val.getInt();\n    if (Size.ugt(DstSize))\n      break;\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Address Src = EmitPointerWithAlignment(E->getArg(1));\n    Value *SizeVal = llvm::ConstantInt::get(Builder.getContext(), Size);\n    Builder.CreateMemCpy(Dest, Src, SizeVal, false);\n    return RValue::get(Dest.getPointer());\n  }\n\n  case Builtin::BI__builtin_objc_memmove_collectable: {\n    Address DestAddr = EmitPointerWithAlignment(E->getArg(0));\n    Address SrcAddr = EmitPointerWithAlignment(E->getArg(1));\n    Value *SizeVal = EmitScalarExpr(E->getArg(2));\n    CGM.getObjCRuntime().EmitGCMemmoveCollectable(*this,\n                                                  DestAddr, SrcAddr, SizeVal);\n    return RValue::get(DestAddr.getPointer());\n  }\n\n  case Builtin::BI__builtin___memmove_chk: {\n    // fold __builtin_memmove_chk(x, y, cst1, cst2) to memmove iff cst1<=cst2.\n    Expr::EvalResult SizeResult, DstSizeResult;\n    if (!E->getArg(2)->EvaluateAsInt(SizeResult, CGM.getContext()) ||\n        !E->getArg(3)->EvaluateAsInt(DstSizeResult, CGM.getContext()))\n      break;\n    llvm::APSInt Size = SizeResult.Val.getInt();\n    llvm::APSInt DstSize = DstSizeResult.Val.getInt();\n    if (Size.ugt(DstSize))\n      break;\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Address Src = EmitPointerWithAlignment(E->getArg(1));\n    Value *SizeVal = llvm::ConstantInt::get(Builder.getContext(), Size);\n    Builder.CreateMemMove(Dest, Src, SizeVal, false);\n    return RValue::get(Dest.getPointer());\n  }\n\n  case Builtin::BImemmove:\n  case Builtin::BI__builtin_memmove: {\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Address Src = EmitPointerWithAlignment(E->getArg(1));\n    Value *SizeVal = EmitScalarExpr(E->getArg(2));\n    EmitNonNullArgCheck(RValue::get(Dest.getPointer()), E->getArg(0)->getType(),\n                        E->getArg(0)->getExprLoc(), FD, 0);\n    EmitNonNullArgCheck(RValue::get(Src.getPointer()), E->getArg(1)->getType(),\n                        E->getArg(1)->getExprLoc(), FD, 1);\n    Builder.CreateMemMove(Dest, Src, SizeVal, false);\n    return RValue::get(Dest.getPointer());\n  }\n  case Builtin::BImemset:\n  case Builtin::BI__builtin_memset: {\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Value *ByteVal = Builder.CreateTrunc(EmitScalarExpr(E->getArg(1)),\n                                         Builder.getInt8Ty());\n    Value *SizeVal = EmitScalarExpr(E->getArg(2));\n    EmitNonNullArgCheck(RValue::get(Dest.getPointer()), E->getArg(0)->getType(),\n                        E->getArg(0)->getExprLoc(), FD, 0);\n    Builder.CreateMemSet(Dest, ByteVal, SizeVal, false);\n    return RValue::get(Dest.getPointer());\n  }\n  case Builtin::BI__builtin___memset_chk: {\n    // fold __builtin_memset_chk(x, y, cst1, cst2) to memset iff cst1<=cst2.\n    Expr::EvalResult SizeResult, DstSizeResult;\n    if (!E->getArg(2)->EvaluateAsInt(SizeResult, CGM.getContext()) ||\n        !E->getArg(3)->EvaluateAsInt(DstSizeResult, CGM.getContext()))\n      break;\n    llvm::APSInt Size = SizeResult.Val.getInt();\n    llvm::APSInt DstSize = DstSizeResult.Val.getInt();\n    if (Size.ugt(DstSize))\n      break;\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    Value *ByteVal = Builder.CreateTrunc(EmitScalarExpr(E->getArg(1)),\n                                         Builder.getInt8Ty());\n    Value *SizeVal = llvm::ConstantInt::get(Builder.getContext(), Size);\n    Builder.CreateMemSet(Dest, ByteVal, SizeVal, false);\n    return RValue::get(Dest.getPointer());\n  }\n  case Builtin::BI__builtin_wmemcmp: {\n    // The MSVC runtime library does not provide a definition of wmemcmp, so we\n    // need an inline implementation.\n    if (!getTarget().getTriple().isOSMSVCRT())\n      break;\n\n    llvm::Type *WCharTy = ConvertType(getContext().WCharTy);\n\n    Value *Dst = EmitScalarExpr(E->getArg(0));\n    Value *Src = EmitScalarExpr(E->getArg(1));\n    Value *Size = EmitScalarExpr(E->getArg(2));\n\n    BasicBlock *Entry = Builder.GetInsertBlock();\n    BasicBlock *CmpGT = createBasicBlock(\"wmemcmp.gt\");\n    BasicBlock *CmpLT = createBasicBlock(\"wmemcmp.lt\");\n    BasicBlock *Next = createBasicBlock(\"wmemcmp.next\");\n    BasicBlock *Exit = createBasicBlock(\"wmemcmp.exit\");\n    Value *SizeEq0 = Builder.CreateICmpEQ(Size, ConstantInt::get(SizeTy, 0));\n    Builder.CreateCondBr(SizeEq0, Exit, CmpGT);\n\n    EmitBlock(CmpGT);\n    PHINode *DstPhi = Builder.CreatePHI(Dst->getType(), 2);\n    DstPhi->addIncoming(Dst, Entry);\n    PHINode *SrcPhi = Builder.CreatePHI(Src->getType(), 2);\n    SrcPhi->addIncoming(Src, Entry);\n    PHINode *SizePhi = Builder.CreatePHI(SizeTy, 2);\n    SizePhi->addIncoming(Size, Entry);\n    CharUnits WCharAlign =\n        getContext().getTypeAlignInChars(getContext().WCharTy);\n    Value *DstCh = Builder.CreateAlignedLoad(WCharTy, DstPhi, WCharAlign);\n    Value *SrcCh = Builder.CreateAlignedLoad(WCharTy, SrcPhi, WCharAlign);\n    Value *DstGtSrc = Builder.CreateICmpUGT(DstCh, SrcCh);\n    Builder.CreateCondBr(DstGtSrc, Exit, CmpLT);\n\n    EmitBlock(CmpLT);\n    Value *DstLtSrc = Builder.CreateICmpULT(DstCh, SrcCh);\n    Builder.CreateCondBr(DstLtSrc, Exit, Next);\n\n    EmitBlock(Next);\n    Value *NextDst = Builder.CreateConstInBoundsGEP1_32(WCharTy, DstPhi, 1);\n    Value *NextSrc = Builder.CreateConstInBoundsGEP1_32(WCharTy, SrcPhi, 1);\n    Value *NextSize = Builder.CreateSub(SizePhi, ConstantInt::get(SizeTy, 1));\n    Value *NextSizeEq0 =\n        Builder.CreateICmpEQ(NextSize, ConstantInt::get(SizeTy, 0));\n    Builder.CreateCondBr(NextSizeEq0, Exit, CmpGT);\n    DstPhi->addIncoming(NextDst, Next);\n    SrcPhi->addIncoming(NextSrc, Next);\n    SizePhi->addIncoming(NextSize, Next);\n\n    EmitBlock(Exit);\n    PHINode *Ret = Builder.CreatePHI(IntTy, 4);\n    Ret->addIncoming(ConstantInt::get(IntTy, 0), Entry);\n    Ret->addIncoming(ConstantInt::get(IntTy, 1), CmpGT);\n    Ret->addIncoming(ConstantInt::get(IntTy, -1), CmpLT);\n    Ret->addIncoming(ConstantInt::get(IntTy, 0), Next);\n    return RValue::get(Ret);\n  }\n  case Builtin::BI__builtin_dwarf_cfa: {\n    // The offset in bytes from the first argument to the CFA.\n    //\n    // Why on earth is this in the frontend?  Is there any reason at\n    // all that the backend can't reasonably determine this while\n    // lowering llvm.eh.dwarf.cfa()?\n    //\n    // TODO: If there's a satisfactory reason, add a target hook for\n    // this instead of hard-coding 0, which is correct for most targets.\n    int32_t Offset = 0;\n\n    Function *F = CGM.getIntrinsic(Intrinsic::eh_dwarf_cfa);\n    return RValue::get(Builder.CreateCall(F,\n                                      llvm::ConstantInt::get(Int32Ty, Offset)));\n  }\n  case Builtin::BI__builtin_return_address: {\n    Value *Depth = ConstantEmitter(*this).emitAbstract(E->getArg(0),\n                                                   getContext().UnsignedIntTy);\n    Function *F = CGM.getIntrinsic(Intrinsic::returnaddress);\n    return RValue::get(Builder.CreateCall(F, Depth));\n  }\n  case Builtin::BI_ReturnAddress: {\n    Function *F = CGM.getIntrinsic(Intrinsic::returnaddress);\n    return RValue::get(Builder.CreateCall(F, Builder.getInt32(0)));\n  }\n  case Builtin::BI__builtin_frame_address: {\n    Value *Depth = ConstantEmitter(*this).emitAbstract(E->getArg(0),\n                                                   getContext().UnsignedIntTy);\n    Function *F = CGM.getIntrinsic(Intrinsic::frameaddress, AllocaInt8PtrTy);\n    return RValue::get(Builder.CreateCall(F, Depth));\n  }\n  case Builtin::BI__builtin_extract_return_addr: {\n    Value *Address = EmitScalarExpr(E->getArg(0));\n    Value *Result = getTargetHooks().decodeReturnAddress(*this, Address);\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_frob_return_addr: {\n    Value *Address = EmitScalarExpr(E->getArg(0));\n    Value *Result = getTargetHooks().encodeReturnAddress(*this, Address);\n    return RValue::get(Result);\n  }\n  case Builtin::BI__builtin_dwarf_sp_column: {\n    llvm::IntegerType *Ty\n      = cast<llvm::IntegerType>(ConvertType(E->getType()));\n    int Column = getTargetHooks().getDwarfEHStackPointer(CGM);\n    if (Column == -1) {\n      CGM.ErrorUnsupported(E, \"__builtin_dwarf_sp_column\");\n      return RValue::get(llvm::UndefValue::get(Ty));\n    }\n    return RValue::get(llvm::ConstantInt::get(Ty, Column, true));\n  }\n  case Builtin::BI__builtin_init_dwarf_reg_size_table: {\n    Value *Address = EmitScalarExpr(E->getArg(0));\n    if (getTargetHooks().initDwarfEHRegSizeTable(*this, Address))\n      CGM.ErrorUnsupported(E, \"__builtin_init_dwarf_reg_size_table\");\n    return RValue::get(llvm::UndefValue::get(ConvertType(E->getType())));\n  }\n  case Builtin::BI__builtin_eh_return: {\n    Value *Int = EmitScalarExpr(E->getArg(0));\n    Value *Ptr = EmitScalarExpr(E->getArg(1));\n\n    llvm::IntegerType *IntTy = cast<llvm::IntegerType>(Int->getType());\n    assert((IntTy->getBitWidth() == 32 || IntTy->getBitWidth() == 64) &&\n           \"LLVM's __builtin_eh_return only supports 32- and 64-bit variants\");\n    Function *F =\n        CGM.getIntrinsic(IntTy->getBitWidth() == 32 ? Intrinsic::eh_return_i32\n                                                    : Intrinsic::eh_return_i64);\n    Builder.CreateCall(F, {Int, Ptr});\n    Builder.CreateUnreachable();\n\n    // We do need to preserve an insertion point.\n    EmitBlock(createBasicBlock(\"builtin_eh_return.cont\"));\n\n    return RValue::get(nullptr);\n  }\n  case Builtin::BI__builtin_unwind_init: {\n    Function *F = CGM.getIntrinsic(Intrinsic::eh_unwind_init);\n    return RValue::get(Builder.CreateCall(F));\n  }\n  case Builtin::BI__builtin_extend_pointer: {\n    // Extends a pointer to the size of an _Unwind_Word, which is\n    // uint64_t on all platforms.  Generally this gets poked into a\n    // register and eventually used as an address, so if the\n    // addressing registers are wider than pointers and the platform\n    // doesn't implicitly ignore high-order bits when doing\n    // addressing, we need to make sure we zext / sext based on\n    // the platform's expectations.\n    //\n    // See: http://gcc.gnu.org/ml/gcc-bugs/2002-02/msg00237.html\n\n    // Cast the pointer to intptr_t.\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Value *Result = Builder.CreatePtrToInt(Ptr, IntPtrTy, \"extend.cast\");\n\n    // If that's 64 bits, we're done.\n    if (IntPtrTy->getBitWidth() == 64)\n      return RValue::get(Result);\n\n    // Otherwise, ask the codegen data what to do.\n    if (getTargetHooks().extendPointerWithSExt())\n      return RValue::get(Builder.CreateSExt(Result, Int64Ty, \"extend.sext\"));\n    else\n      return RValue::get(Builder.CreateZExt(Result, Int64Ty, \"extend.zext\"));\n  }\n  case Builtin::BI__builtin_setjmp: {\n    // Buffer is a void**.\n    Address Buf = EmitPointerWithAlignment(E->getArg(0));\n\n    // Store the frame pointer to the setjmp buffer.\n    Value *FrameAddr = Builder.CreateCall(\n        CGM.getIntrinsic(Intrinsic::frameaddress, AllocaInt8PtrTy),\n        ConstantInt::get(Int32Ty, 0));\n    Builder.CreateStore(FrameAddr, Buf);\n\n    // Store the stack pointer to the setjmp buffer.\n    Value *StackAddr =\n        Builder.CreateCall(CGM.getIntrinsic(Intrinsic::stacksave));\n    Address StackSaveSlot = Builder.CreateConstInBoundsGEP(Buf, 2);\n    Builder.CreateStore(StackAddr, StackSaveSlot);\n\n    // Call LLVM's EH setjmp, which is lightweight.\n    Function *F = CGM.getIntrinsic(Intrinsic::eh_sjlj_setjmp);\n    Buf = Builder.CreateBitCast(Buf, Int8PtrTy);\n    return RValue::get(Builder.CreateCall(F, Buf.getPointer()));\n  }\n  case Builtin::BI__builtin_longjmp: {\n    Value *Buf = EmitScalarExpr(E->getArg(0));\n    Buf = Builder.CreateBitCast(Buf, Int8PtrTy);\n\n    // Call LLVM's EH longjmp, which is lightweight.\n    Builder.CreateCall(CGM.getIntrinsic(Intrinsic::eh_sjlj_longjmp), Buf);\n\n    // longjmp doesn't return; mark this as unreachable.\n    Builder.CreateUnreachable();\n\n    // We do need to preserve an insertion point.\n    EmitBlock(createBasicBlock(\"longjmp.cont\"));\n\n    return RValue::get(nullptr);\n  }\n  case Builtin::BI__builtin_launder: {\n    const Expr *Arg = E->getArg(0);\n    QualType ArgTy = Arg->getType()->getPointeeType();\n    Value *Ptr = EmitScalarExpr(Arg);\n    if (TypeRequiresBuiltinLaunder(CGM, ArgTy))\n      Ptr = Builder.CreateLaunderInvariantGroup(Ptr);\n\n    return RValue::get(Ptr);\n  }\n  case Builtin::BI__sync_fetch_and_add:\n  case Builtin::BI__sync_fetch_and_sub:\n  case Builtin::BI__sync_fetch_and_or:\n  case Builtin::BI__sync_fetch_and_and:\n  case Builtin::BI__sync_fetch_and_xor:\n  case Builtin::BI__sync_fetch_and_nand:\n  case Builtin::BI__sync_add_and_fetch:\n  case Builtin::BI__sync_sub_and_fetch:\n  case Builtin::BI__sync_and_and_fetch:\n  case Builtin::BI__sync_or_and_fetch:\n  case Builtin::BI__sync_xor_and_fetch:\n  case Builtin::BI__sync_nand_and_fetch:\n  case Builtin::BI__sync_val_compare_and_swap:\n  case Builtin::BI__sync_bool_compare_and_swap:\n  case Builtin::BI__sync_lock_test_and_set:\n  case Builtin::BI__sync_lock_release:\n  case Builtin::BI__sync_swap:\n    llvm_unreachable(\"Shouldn't make it through sema\");\n  case Builtin::BI__sync_fetch_and_add_1:\n  case Builtin::BI__sync_fetch_and_add_2:\n  case Builtin::BI__sync_fetch_and_add_4:\n  case Builtin::BI__sync_fetch_and_add_8:\n  case Builtin::BI__sync_fetch_and_add_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Add, E);\n  case Builtin::BI__sync_fetch_and_sub_1:\n  case Builtin::BI__sync_fetch_and_sub_2:\n  case Builtin::BI__sync_fetch_and_sub_4:\n  case Builtin::BI__sync_fetch_and_sub_8:\n  case Builtin::BI__sync_fetch_and_sub_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Sub, E);\n  case Builtin::BI__sync_fetch_and_or_1:\n  case Builtin::BI__sync_fetch_and_or_2:\n  case Builtin::BI__sync_fetch_and_or_4:\n  case Builtin::BI__sync_fetch_and_or_8:\n  case Builtin::BI__sync_fetch_and_or_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Or, E);\n  case Builtin::BI__sync_fetch_and_and_1:\n  case Builtin::BI__sync_fetch_and_and_2:\n  case Builtin::BI__sync_fetch_and_and_4:\n  case Builtin::BI__sync_fetch_and_and_8:\n  case Builtin::BI__sync_fetch_and_and_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::And, E);\n  case Builtin::BI__sync_fetch_and_xor_1:\n  case Builtin::BI__sync_fetch_and_xor_2:\n  case Builtin::BI__sync_fetch_and_xor_4:\n  case Builtin::BI__sync_fetch_and_xor_8:\n  case Builtin::BI__sync_fetch_and_xor_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Xor, E);\n  case Builtin::BI__sync_fetch_and_nand_1:\n  case Builtin::BI__sync_fetch_and_nand_2:\n  case Builtin::BI__sync_fetch_and_nand_4:\n  case Builtin::BI__sync_fetch_and_nand_8:\n  case Builtin::BI__sync_fetch_and_nand_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Nand, E);\n\n  // Clang extensions: not overloaded yet.\n  case Builtin::BI__sync_fetch_and_min:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Min, E);\n  case Builtin::BI__sync_fetch_and_max:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Max, E);\n  case Builtin::BI__sync_fetch_and_umin:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::UMin, E);\n  case Builtin::BI__sync_fetch_and_umax:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::UMax, E);\n\n  case Builtin::BI__sync_add_and_fetch_1:\n  case Builtin::BI__sync_add_and_fetch_2:\n  case Builtin::BI__sync_add_and_fetch_4:\n  case Builtin::BI__sync_add_and_fetch_8:\n  case Builtin::BI__sync_add_and_fetch_16:\n    return EmitBinaryAtomicPost(*this, llvm::AtomicRMWInst::Add, E,\n                                llvm::Instruction::Add);\n  case Builtin::BI__sync_sub_and_fetch_1:\n  case Builtin::BI__sync_sub_and_fetch_2:\n  case Builtin::BI__sync_sub_and_fetch_4:\n  case Builtin::BI__sync_sub_and_fetch_8:\n  case Builtin::BI__sync_sub_and_fetch_16:\n    return EmitBinaryAtomicPost(*this, llvm::AtomicRMWInst::Sub, E,\n                                llvm::Instruction::Sub);\n  case Builtin::BI__sync_and_and_fetch_1:\n  case Builtin::BI__sync_and_and_fetch_2:\n  case Builtin::BI__sync_and_and_fetch_4:\n  case Builtin::BI__sync_and_and_fetch_8:\n  case Builtin::BI__sync_and_and_fetch_16:\n    return EmitBinaryAtomicPost(*this, llvm::AtomicRMWInst::And, E,\n                                llvm::Instruction::And);\n  case Builtin::BI__sync_or_and_fetch_1:\n  case Builtin::BI__sync_or_and_fetch_2:\n  case Builtin::BI__sync_or_and_fetch_4:\n  case Builtin::BI__sync_or_and_fetch_8:\n  case Builtin::BI__sync_or_and_fetch_16:\n    return EmitBinaryAtomicPost(*this, llvm::AtomicRMWInst::Or, E,\n                                llvm::Instruction::Or);\n  case Builtin::BI__sync_xor_and_fetch_1:\n  case Builtin::BI__sync_xor_and_fetch_2:\n  case Builtin::BI__sync_xor_and_fetch_4:\n  case Builtin::BI__sync_xor_and_fetch_8:\n  case Builtin::BI__sync_xor_and_fetch_16:\n    return EmitBinaryAtomicPost(*this, llvm::AtomicRMWInst::Xor, E,\n                                llvm::Instruction::Xor);\n  case Builtin::BI__sync_nand_and_fetch_1:\n  case Builtin::BI__sync_nand_and_fetch_2:\n  case Builtin::BI__sync_nand_and_fetch_4:\n  case Builtin::BI__sync_nand_and_fetch_8:\n  case Builtin::BI__sync_nand_and_fetch_16:\n    return EmitBinaryAtomicPost(*this, llvm::AtomicRMWInst::Nand, E,\n                                llvm::Instruction::And, true);\n\n  case Builtin::BI__sync_val_compare_and_swap_1:\n  case Builtin::BI__sync_val_compare_and_swap_2:\n  case Builtin::BI__sync_val_compare_and_swap_4:\n  case Builtin::BI__sync_val_compare_and_swap_8:\n  case Builtin::BI__sync_val_compare_and_swap_16:\n    return RValue::get(MakeAtomicCmpXchgValue(*this, E, false));\n\n  case Builtin::BI__sync_bool_compare_and_swap_1:\n  case Builtin::BI__sync_bool_compare_and_swap_2:\n  case Builtin::BI__sync_bool_compare_and_swap_4:\n  case Builtin::BI__sync_bool_compare_and_swap_8:\n  case Builtin::BI__sync_bool_compare_and_swap_16:\n    return RValue::get(MakeAtomicCmpXchgValue(*this, E, true));\n\n  case Builtin::BI__sync_swap_1:\n  case Builtin::BI__sync_swap_2:\n  case Builtin::BI__sync_swap_4:\n  case Builtin::BI__sync_swap_8:\n  case Builtin::BI__sync_swap_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Xchg, E);\n\n  case Builtin::BI__sync_lock_test_and_set_1:\n  case Builtin::BI__sync_lock_test_and_set_2:\n  case Builtin::BI__sync_lock_test_and_set_4:\n  case Builtin::BI__sync_lock_test_and_set_8:\n  case Builtin::BI__sync_lock_test_and_set_16:\n    return EmitBinaryAtomic(*this, llvm::AtomicRMWInst::Xchg, E);\n\n  case Builtin::BI__sync_lock_release_1:\n  case Builtin::BI__sync_lock_release_2:\n  case Builtin::BI__sync_lock_release_4:\n  case Builtin::BI__sync_lock_release_8:\n  case Builtin::BI__sync_lock_release_16: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    QualType ElTy = E->getArg(0)->getType()->getPointeeType();\n    CharUnits StoreSize = getContext().getTypeSizeInChars(ElTy);\n    llvm::Type *ITy = llvm::IntegerType::get(getLLVMContext(),\n                                             StoreSize.getQuantity() * 8);\n    Ptr = Builder.CreateBitCast(Ptr, ITy->getPointerTo());\n    llvm::StoreInst *Store =\n      Builder.CreateAlignedStore(llvm::Constant::getNullValue(ITy), Ptr,\n                                 StoreSize);\n    Store->setAtomic(llvm::AtomicOrdering::Release);\n    return RValue::get(nullptr);\n  }\n\n  case Builtin::BI__sync_synchronize: {\n    // We assume this is supposed to correspond to a C++0x-style\n    // sequentially-consistent fence (i.e. this is only usable for\n    // synchronization, not device I/O or anything like that). This intrinsic\n    // is really badly designed in the sense that in theory, there isn't\n    // any way to safely use it... but in practice, it mostly works\n    // to use it with non-atomic loads and stores to get acquire/release\n    // semantics.\n    Builder.CreateFence(llvm::AtomicOrdering::SequentiallyConsistent);\n    return RValue::get(nullptr);\n  }\n\n  case Builtin::BI__builtin_nontemporal_load:\n    return RValue::get(EmitNontemporalLoad(*this, E));\n  case Builtin::BI__builtin_nontemporal_store:\n    return RValue::get(EmitNontemporalStore(*this, E));\n  case Builtin::BI__c11_atomic_is_lock_free:\n  case Builtin::BI__atomic_is_lock_free: {\n    // Call \"bool __atomic_is_lock_free(size_t size, void *ptr)\". For the\n    // __c11 builtin, ptr is 0 (indicating a properly-aligned object), since\n    // _Atomic(T) is always properly-aligned.\n    const char *LibCallName = \"__atomic_is_lock_free\";\n    CallArgList Args;\n    Args.add(RValue::get(EmitScalarExpr(E->getArg(0))),\n             getContext().getSizeType());\n    if (BuiltinID == Builtin::BI__atomic_is_lock_free)\n      Args.add(RValue::get(EmitScalarExpr(E->getArg(1))),\n               getContext().VoidPtrTy);\n    else\n      Args.add(RValue::get(llvm::Constant::getNullValue(VoidPtrTy)),\n               getContext().VoidPtrTy);\n    const CGFunctionInfo &FuncInfo =\n        CGM.getTypes().arrangeBuiltinFunctionCall(E->getType(), Args);\n    llvm::FunctionType *FTy = CGM.getTypes().GetFunctionType(FuncInfo);\n    llvm::FunctionCallee Func = CGM.CreateRuntimeFunction(FTy, LibCallName);\n    return EmitCall(FuncInfo, CGCallee::forDirect(Func),\n                    ReturnValueSlot(), Args);\n  }\n\n  case Builtin::BI__atomic_test_and_set: {\n    // Look at the argument type to determine whether this is a volatile\n    // operation. The parameter type is always volatile.\n    QualType PtrTy = E->getArg(0)->IgnoreImpCasts()->getType();\n    bool Volatile =\n        PtrTy->castAs<PointerType>()->getPointeeType().isVolatileQualified();\n\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    unsigned AddrSpace = Ptr->getType()->getPointerAddressSpace();\n    Ptr = Builder.CreateBitCast(Ptr, Int8Ty->getPointerTo(AddrSpace));\n    Value *NewVal = Builder.getInt8(1);\n    Value *Order = EmitScalarExpr(E->getArg(1));\n    if (isa<llvm::ConstantInt>(Order)) {\n      int ord = cast<llvm::ConstantInt>(Order)->getZExtValue();\n      AtomicRMWInst *Result = nullptr;\n      switch (ord) {\n      case 0:  // memory_order_relaxed\n      default: // invalid order\n        Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,\n                                         llvm::AtomicOrdering::Monotonic);\n        break;\n      case 1: // memory_order_consume\n      case 2: // memory_order_acquire\n        Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,\n                                         llvm::AtomicOrdering::Acquire);\n        break;\n      case 3: // memory_order_release\n        Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,\n                                         llvm::AtomicOrdering::Release);\n        break;\n      case 4: // memory_order_acq_rel\n\n        Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,\n                                         llvm::AtomicOrdering::AcquireRelease);\n        break;\n      case 5: // memory_order_seq_cst\n        Result = Builder.CreateAtomicRMW(\n            llvm::AtomicRMWInst::Xchg, Ptr, NewVal,\n            llvm::AtomicOrdering::SequentiallyConsistent);\n        break;\n      }\n      Result->setVolatile(Volatile);\n      return RValue::get(Builder.CreateIsNotNull(Result, \"tobool\"));\n    }\n\n    llvm::BasicBlock *ContBB = createBasicBlock(\"atomic.continue\", CurFn);\n\n    llvm::BasicBlock *BBs[5] = {\n      createBasicBlock(\"monotonic\", CurFn),\n      createBasicBlock(\"acquire\", CurFn),\n      createBasicBlock(\"release\", CurFn),\n      createBasicBlock(\"acqrel\", CurFn),\n      createBasicBlock(\"seqcst\", CurFn)\n    };\n    llvm::AtomicOrdering Orders[5] = {\n        llvm::AtomicOrdering::Monotonic, llvm::AtomicOrdering::Acquire,\n        llvm::AtomicOrdering::Release, llvm::AtomicOrdering::AcquireRelease,\n        llvm::AtomicOrdering::SequentiallyConsistent};\n\n    Order = Builder.CreateIntCast(Order, Builder.getInt32Ty(), false);\n    llvm::SwitchInst *SI = Builder.CreateSwitch(Order, BBs[0]);\n\n    Builder.SetInsertPoint(ContBB);\n    PHINode *Result = Builder.CreatePHI(Int8Ty, 5, \"was_set\");\n\n    for (unsigned i = 0; i < 5; ++i) {\n      Builder.SetInsertPoint(BBs[i]);\n      AtomicRMWInst *RMW = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg,\n                                                   Ptr, NewVal, Orders[i]);\n      RMW->setVolatile(Volatile);\n      Result->addIncoming(RMW, BBs[i]);\n      Builder.CreateBr(ContBB);\n    }\n\n    SI->addCase(Builder.getInt32(0), BBs[0]);\n    SI->addCase(Builder.getInt32(1), BBs[1]);\n    SI->addCase(Builder.getInt32(2), BBs[1]);\n    SI->addCase(Builder.getInt32(3), BBs[2]);\n    SI->addCase(Builder.getInt32(4), BBs[3]);\n    SI->addCase(Builder.getInt32(5), BBs[4]);\n\n    Builder.SetInsertPoint(ContBB);\n    return RValue::get(Builder.CreateIsNotNull(Result, \"tobool\"));\n  }\n\n  case Builtin::BI__atomic_clear: {\n    QualType PtrTy = E->getArg(0)->IgnoreImpCasts()->getType();\n    bool Volatile =\n        PtrTy->castAs<PointerType>()->getPointeeType().isVolatileQualified();\n\n    Address Ptr = EmitPointerWithAlignment(E->getArg(0));\n    unsigned AddrSpace = Ptr.getPointer()->getType()->getPointerAddressSpace();\n    Ptr = Builder.CreateBitCast(Ptr, Int8Ty->getPointerTo(AddrSpace));\n    Value *NewVal = Builder.getInt8(0);\n    Value *Order = EmitScalarExpr(E->getArg(1));\n    if (isa<llvm::ConstantInt>(Order)) {\n      int ord = cast<llvm::ConstantInt>(Order)->getZExtValue();\n      StoreInst *Store = Builder.CreateStore(NewVal, Ptr, Volatile);\n      switch (ord) {\n      case 0:  // memory_order_relaxed\n      default: // invalid order\n        Store->setOrdering(llvm::AtomicOrdering::Monotonic);\n        break;\n      case 3:  // memory_order_release\n        Store->setOrdering(llvm::AtomicOrdering::Release);\n        break;\n      case 5:  // memory_order_seq_cst\n        Store->setOrdering(llvm::AtomicOrdering::SequentiallyConsistent);\n        break;\n      }\n      return RValue::get(nullptr);\n    }\n\n    llvm::BasicBlock *ContBB = createBasicBlock(\"atomic.continue\", CurFn);\n\n    llvm::BasicBlock *BBs[3] = {\n      createBasicBlock(\"monotonic\", CurFn),\n      createBasicBlock(\"release\", CurFn),\n      createBasicBlock(\"seqcst\", CurFn)\n    };\n    llvm::AtomicOrdering Orders[3] = {\n        llvm::AtomicOrdering::Monotonic, llvm::AtomicOrdering::Release,\n        llvm::AtomicOrdering::SequentiallyConsistent};\n\n    Order = Builder.CreateIntCast(Order, Builder.getInt32Ty(), false);\n    llvm::SwitchInst *SI = Builder.CreateSwitch(Order, BBs[0]);\n\n    for (unsigned i = 0; i < 3; ++i) {\n      Builder.SetInsertPoint(BBs[i]);\n      StoreInst *Store = Builder.CreateStore(NewVal, Ptr, Volatile);\n      Store->setOrdering(Orders[i]);\n      Builder.CreateBr(ContBB);\n    }\n\n    SI->addCase(Builder.getInt32(0), BBs[0]);\n    SI->addCase(Builder.getInt32(3), BBs[1]);\n    SI->addCase(Builder.getInt32(5), BBs[2]);\n\n    Builder.SetInsertPoint(ContBB);\n    return RValue::get(nullptr);\n  }\n\n  case Builtin::BI__atomic_thread_fence:\n  case Builtin::BI__atomic_signal_fence:\n  case Builtin::BI__c11_atomic_thread_fence:\n  case Builtin::BI__c11_atomic_signal_fence: {\n    llvm::SyncScope::ID SSID;\n    if (BuiltinID == Builtin::BI__atomic_signal_fence ||\n        BuiltinID == Builtin::BI__c11_atomic_signal_fence)\n      SSID = llvm::SyncScope::SingleThread;\n    else\n      SSID = llvm::SyncScope::System;\n    Value *Order = EmitScalarExpr(E->getArg(0));\n    if (isa<llvm::ConstantInt>(Order)) {\n      int ord = cast<llvm::ConstantInt>(Order)->getZExtValue();\n      switch (ord) {\n      case 0:  // memory_order_relaxed\n      default: // invalid order\n        break;\n      case 1:  // memory_order_consume\n      case 2:  // memory_order_acquire\n        Builder.CreateFence(llvm::AtomicOrdering::Acquire, SSID);\n        break;\n      case 3:  // memory_order_release\n        Builder.CreateFence(llvm::AtomicOrdering::Release, SSID);\n        break;\n      case 4:  // memory_order_acq_rel\n        Builder.CreateFence(llvm::AtomicOrdering::AcquireRelease, SSID);\n        break;\n      case 5:  // memory_order_seq_cst\n        Builder.CreateFence(llvm::AtomicOrdering::SequentiallyConsistent, SSID);\n        break;\n      }\n      return RValue::get(nullptr);\n    }\n\n    llvm::BasicBlock *AcquireBB, *ReleaseBB, *AcqRelBB, *SeqCstBB;\n    AcquireBB = createBasicBlock(\"acquire\", CurFn);\n    ReleaseBB = createBasicBlock(\"release\", CurFn);\n    AcqRelBB = createBasicBlock(\"acqrel\", CurFn);\n    SeqCstBB = createBasicBlock(\"seqcst\", CurFn);\n    llvm::BasicBlock *ContBB = createBasicBlock(\"atomic.continue\", CurFn);\n\n    Order = Builder.CreateIntCast(Order, Builder.getInt32Ty(), false);\n    llvm::SwitchInst *SI = Builder.CreateSwitch(Order, ContBB);\n\n    Builder.SetInsertPoint(AcquireBB);\n    Builder.CreateFence(llvm::AtomicOrdering::Acquire, SSID);\n    Builder.CreateBr(ContBB);\n    SI->addCase(Builder.getInt32(1), AcquireBB);\n    SI->addCase(Builder.getInt32(2), AcquireBB);\n\n    Builder.SetInsertPoint(ReleaseBB);\n    Builder.CreateFence(llvm::AtomicOrdering::Release, SSID);\n    Builder.CreateBr(ContBB);\n    SI->addCase(Builder.getInt32(3), ReleaseBB);\n\n    Builder.SetInsertPoint(AcqRelBB);\n    Builder.CreateFence(llvm::AtomicOrdering::AcquireRelease, SSID);\n    Builder.CreateBr(ContBB);\n    SI->addCase(Builder.getInt32(4), AcqRelBB);\n\n    Builder.SetInsertPoint(SeqCstBB);\n    Builder.CreateFence(llvm::AtomicOrdering::SequentiallyConsistent, SSID);\n    Builder.CreateBr(ContBB);\n    SI->addCase(Builder.getInt32(5), SeqCstBB);\n\n    Builder.SetInsertPoint(ContBB);\n    return RValue::get(nullptr);\n  }\n\n  case Builtin::BI__builtin_signbit:\n  case Builtin::BI__builtin_signbitf:\n  case Builtin::BI__builtin_signbitl: {\n    return RValue::get(\n        Builder.CreateZExt(EmitSignBit(*this, EmitScalarExpr(E->getArg(0))),\n                           ConvertType(E->getType())));\n  }\n  case Builtin::BI__warn_memset_zero_len:\n    return RValue::getIgnored();\n  case Builtin::BI__annotation: {\n    // Re-encode each wide string to UTF8 and make an MDString.\n    SmallVector<Metadata *, 1> Strings;\n    for (const Expr *Arg : E->arguments()) {\n      const auto *Str = cast<StringLiteral>(Arg->IgnoreParenCasts());\n      assert(Str->getCharByteWidth() == 2);\n      StringRef WideBytes = Str->getBytes();\n      std::string StrUtf8;\n      if (!convertUTF16ToUTF8String(\n              makeArrayRef(WideBytes.data(), WideBytes.size()), StrUtf8)) {\n        CGM.ErrorUnsupported(E, \"non-UTF16 __annotation argument\");\n        continue;\n      }\n      Strings.push_back(llvm::MDString::get(getLLVMContext(), StrUtf8));\n    }\n\n    // Build and MDTuple of MDStrings and emit the intrinsic call.\n    llvm::Function *F =\n        CGM.getIntrinsic(llvm::Intrinsic::codeview_annotation, {});\n    MDTuple *StrTuple = MDTuple::get(getLLVMContext(), Strings);\n    Builder.CreateCall(F, MetadataAsValue::get(getLLVMContext(), StrTuple));\n    return RValue::getIgnored();\n  }\n  case Builtin::BI__builtin_annotation: {\n    llvm::Value *AnnVal = EmitScalarExpr(E->getArg(0));\n    llvm::Function *F = CGM.getIntrinsic(llvm::Intrinsic::annotation,\n                                      AnnVal->getType());\n\n    // Get the annotation string, go through casts. Sema requires this to be a\n    // non-wide string literal, potentially casted, so the cast<> is safe.\n    const Expr *AnnotationStrExpr = E->getArg(1)->IgnoreParenCasts();\n    StringRef Str = cast<StringLiteral>(AnnotationStrExpr)->getString();\n    return RValue::get(\n        EmitAnnotationCall(F, AnnVal, Str, E->getExprLoc(), nullptr));\n  }\n  case Builtin::BI__builtin_addcb:\n  case Builtin::BI__builtin_addcs:\n  case Builtin::BI__builtin_addc:\n  case Builtin::BI__builtin_addcl:\n  case Builtin::BI__builtin_addcll:\n  case Builtin::BI__builtin_subcb:\n  case Builtin::BI__builtin_subcs:\n  case Builtin::BI__builtin_subc:\n  case Builtin::BI__builtin_subcl:\n  case Builtin::BI__builtin_subcll: {\n\n    // We translate all of these builtins from expressions of the form:\n    //   int x = ..., y = ..., carryin = ..., carryout, result;\n    //   result = __builtin_addc(x, y, carryin, &carryout);\n    //\n    // to LLVM IR of the form:\n    //\n    //   %tmp1 = call {i32, i1} @llvm.uadd.with.overflow.i32(i32 %x, i32 %y)\n    //   %tmpsum1 = extractvalue {i32, i1} %tmp1, 0\n    //   %carry1 = extractvalue {i32, i1} %tmp1, 1\n    //   %tmp2 = call {i32, i1} @llvm.uadd.with.overflow.i32(i32 %tmpsum1,\n    //                                                       i32 %carryin)\n    //   %result = extractvalue {i32, i1} %tmp2, 0\n    //   %carry2 = extractvalue {i32, i1} %tmp2, 1\n    //   %tmp3 = or i1 %carry1, %carry2\n    //   %tmp4 = zext i1 %tmp3 to i32\n    //   store i32 %tmp4, i32* %carryout\n\n    // Scalarize our inputs.\n    llvm::Value *X = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Y = EmitScalarExpr(E->getArg(1));\n    llvm::Value *Carryin = EmitScalarExpr(E->getArg(2));\n    Address CarryOutPtr = EmitPointerWithAlignment(E->getArg(3));\n\n    // Decide if we are lowering to a uadd.with.overflow or usub.with.overflow.\n    llvm::Intrinsic::ID IntrinsicId;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unknown multiprecision builtin id.\");\n    case Builtin::BI__builtin_addcb:\n    case Builtin::BI__builtin_addcs:\n    case Builtin::BI__builtin_addc:\n    case Builtin::BI__builtin_addcl:\n    case Builtin::BI__builtin_addcll:\n      IntrinsicId = llvm::Intrinsic::uadd_with_overflow;\n      break;\n    case Builtin::BI__builtin_subcb:\n    case Builtin::BI__builtin_subcs:\n    case Builtin::BI__builtin_subc:\n    case Builtin::BI__builtin_subcl:\n    case Builtin::BI__builtin_subcll:\n      IntrinsicId = llvm::Intrinsic::usub_with_overflow;\n      break;\n    }\n\n    // Construct our resulting LLVM IR expression.\n    llvm::Value *Carry1;\n    llvm::Value *Sum1 = EmitOverflowIntrinsic(*this, IntrinsicId,\n                                              X, Y, Carry1);\n    llvm::Value *Carry2;\n    llvm::Value *Sum2 = EmitOverflowIntrinsic(*this, IntrinsicId,\n                                              Sum1, Carryin, Carry2);\n    llvm::Value *CarryOut = Builder.CreateZExt(Builder.CreateOr(Carry1, Carry2),\n                                               X->getType());\n    Builder.CreateStore(CarryOut, CarryOutPtr);\n    return RValue::get(Sum2);\n  }\n\n  case Builtin::BI__builtin_add_overflow:\n  case Builtin::BI__builtin_sub_overflow:\n  case Builtin::BI__builtin_mul_overflow: {\n    const clang::Expr *LeftArg = E->getArg(0);\n    const clang::Expr *RightArg = E->getArg(1);\n    const clang::Expr *ResultArg = E->getArg(2);\n\n    clang::QualType ResultQTy =\n        ResultArg->getType()->castAs<PointerType>()->getPointeeType();\n\n    WidthAndSignedness LeftInfo =\n        getIntegerWidthAndSignedness(CGM.getContext(), LeftArg->getType());\n    WidthAndSignedness RightInfo =\n        getIntegerWidthAndSignedness(CGM.getContext(), RightArg->getType());\n    WidthAndSignedness ResultInfo =\n        getIntegerWidthAndSignedness(CGM.getContext(), ResultQTy);\n\n    // Handle mixed-sign multiplication as a special case, because adding\n    // runtime or backend support for our generic irgen would be too expensive.\n    if (isSpecialMixedSignMultiply(BuiltinID, LeftInfo, RightInfo, ResultInfo))\n      return EmitCheckedMixedSignMultiply(*this, LeftArg, LeftInfo, RightArg,\n                                          RightInfo, ResultArg, ResultQTy,\n                                          ResultInfo);\n\n    if (isSpecialUnsignedMultiplySignedResult(BuiltinID, LeftInfo, RightInfo,\n                                              ResultInfo))\n      return EmitCheckedUnsignedMultiplySignedResult(\n          *this, LeftArg, LeftInfo, RightArg, RightInfo, ResultArg, ResultQTy,\n          ResultInfo);\n\n    WidthAndSignedness EncompassingInfo =\n        EncompassingIntegerType({LeftInfo, RightInfo, ResultInfo});\n\n    llvm::Type *EncompassingLLVMTy =\n        llvm::IntegerType::get(CGM.getLLVMContext(), EncompassingInfo.Width);\n\n    llvm::Type *ResultLLVMTy = CGM.getTypes().ConvertType(ResultQTy);\n\n    llvm::Intrinsic::ID IntrinsicId;\n    switch (BuiltinID) {\n    default:\n      llvm_unreachable(\"Unknown overflow builtin id.\");\n    case Builtin::BI__builtin_add_overflow:\n      IntrinsicId = EncompassingInfo.Signed\n                        ? llvm::Intrinsic::sadd_with_overflow\n                        : llvm::Intrinsic::uadd_with_overflow;\n      break;\n    case Builtin::BI__builtin_sub_overflow:\n      IntrinsicId = EncompassingInfo.Signed\n                        ? llvm::Intrinsic::ssub_with_overflow\n                        : llvm::Intrinsic::usub_with_overflow;\n      break;\n    case Builtin::BI__builtin_mul_overflow:\n      IntrinsicId = EncompassingInfo.Signed\n                        ? llvm::Intrinsic::smul_with_overflow\n                        : llvm::Intrinsic::umul_with_overflow;\n      break;\n    }\n\n    llvm::Value *Left = EmitScalarExpr(LeftArg);\n    llvm::Value *Right = EmitScalarExpr(RightArg);\n    Address ResultPtr = EmitPointerWithAlignment(ResultArg);\n\n    // Extend each operand to the encompassing type.\n    Left = Builder.CreateIntCast(Left, EncompassingLLVMTy, LeftInfo.Signed);\n    Right = Builder.CreateIntCast(Right, EncompassingLLVMTy, RightInfo.Signed);\n\n    // Perform the operation on the extended values.\n    llvm::Value *Overflow, *Result;\n    Result = EmitOverflowIntrinsic(*this, IntrinsicId, Left, Right, Overflow);\n\n    if (EncompassingInfo.Width > ResultInfo.Width) {\n      // The encompassing type is wider than the result type, so we need to\n      // truncate it.\n      llvm::Value *ResultTrunc = Builder.CreateTrunc(Result, ResultLLVMTy);\n\n      // To see if the truncation caused an overflow, we will extend\n      // the result and then compare it to the original result.\n      llvm::Value *ResultTruncExt = Builder.CreateIntCast(\n          ResultTrunc, EncompassingLLVMTy, ResultInfo.Signed);\n      llvm::Value *TruncationOverflow =\n          Builder.CreateICmpNE(Result, ResultTruncExt);\n\n      Overflow = Builder.CreateOr(Overflow, TruncationOverflow);\n      Result = ResultTrunc;\n    }\n\n    // Finally, store the result using the pointer.\n    bool isVolatile =\n      ResultArg->getType()->getPointeeType().isVolatileQualified();\n    Builder.CreateStore(EmitToMemory(Result, ResultQTy), ResultPtr, isVolatile);\n\n    return RValue::get(Overflow);\n  }\n\n  case Builtin::BI__builtin_uadd_overflow:\n  case Builtin::BI__builtin_uaddl_overflow:\n  case Builtin::BI__builtin_uaddll_overflow:\n  case Builtin::BI__builtin_usub_overflow:\n  case Builtin::BI__builtin_usubl_overflow:\n  case Builtin::BI__builtin_usubll_overflow:\n  case Builtin::BI__builtin_umul_overflow:\n  case Builtin::BI__builtin_umull_overflow:\n  case Builtin::BI__builtin_umulll_overflow:\n  case Builtin::BI__builtin_sadd_overflow:\n  case Builtin::BI__builtin_saddl_overflow:\n  case Builtin::BI__builtin_saddll_overflow:\n  case Builtin::BI__builtin_ssub_overflow:\n  case Builtin::BI__builtin_ssubl_overflow:\n  case Builtin::BI__builtin_ssubll_overflow:\n  case Builtin::BI__builtin_smul_overflow:\n  case Builtin::BI__builtin_smull_overflow:\n  case Builtin::BI__builtin_smulll_overflow: {\n\n    // We translate all of these builtins directly to the relevant llvm IR node.\n\n    // Scalarize our inputs.\n    llvm::Value *X = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Y = EmitScalarExpr(E->getArg(1));\n    Address SumOutPtr = EmitPointerWithAlignment(E->getArg(2));\n\n    // Decide which of the overflow intrinsics we are lowering to:\n    llvm::Intrinsic::ID IntrinsicId;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unknown overflow builtin id.\");\n    case Builtin::BI__builtin_uadd_overflow:\n    case Builtin::BI__builtin_uaddl_overflow:\n    case Builtin::BI__builtin_uaddll_overflow:\n      IntrinsicId = llvm::Intrinsic::uadd_with_overflow;\n      break;\n    case Builtin::BI__builtin_usub_overflow:\n    case Builtin::BI__builtin_usubl_overflow:\n    case Builtin::BI__builtin_usubll_overflow:\n      IntrinsicId = llvm::Intrinsic::usub_with_overflow;\n      break;\n    case Builtin::BI__builtin_umul_overflow:\n    case Builtin::BI__builtin_umull_overflow:\n    case Builtin::BI__builtin_umulll_overflow:\n      IntrinsicId = llvm::Intrinsic::umul_with_overflow;\n      break;\n    case Builtin::BI__builtin_sadd_overflow:\n    case Builtin::BI__builtin_saddl_overflow:\n    case Builtin::BI__builtin_saddll_overflow:\n      IntrinsicId = llvm::Intrinsic::sadd_with_overflow;\n      break;\n    case Builtin::BI__builtin_ssub_overflow:\n    case Builtin::BI__builtin_ssubl_overflow:\n    case Builtin::BI__builtin_ssubll_overflow:\n      IntrinsicId = llvm::Intrinsic::ssub_with_overflow;\n      break;\n    case Builtin::BI__builtin_smul_overflow:\n    case Builtin::BI__builtin_smull_overflow:\n    case Builtin::BI__builtin_smulll_overflow:\n      IntrinsicId = llvm::Intrinsic::smul_with_overflow;\n      break;\n    }\n\n\n    llvm::Value *Carry;\n    llvm::Value *Sum = EmitOverflowIntrinsic(*this, IntrinsicId, X, Y, Carry);\n    Builder.CreateStore(Sum, SumOutPtr);\n\n    return RValue::get(Carry);\n  }\n  case Builtin::BI__builtin_addressof:\n    return RValue::get(EmitLValue(E->getArg(0)).getPointer(*this));\n  case Builtin::BI__builtin_operator_new:\n    return EmitBuiltinNewDeleteCall(\n        E->getCallee()->getType()->castAs<FunctionProtoType>(), E, false);\n  case Builtin::BI__builtin_operator_delete:\n    return EmitBuiltinNewDeleteCall(\n        E->getCallee()->getType()->castAs<FunctionProtoType>(), E, true);\n\n  case Builtin::BI__builtin_is_aligned:\n    return EmitBuiltinIsAligned(E);\n  case Builtin::BI__builtin_align_up:\n    return EmitBuiltinAlignTo(E, true);\n  case Builtin::BI__builtin_align_down:\n    return EmitBuiltinAlignTo(E, false);\n\n  case Builtin::BI__noop:\n    // __noop always evaluates to an integer literal zero.\n    return RValue::get(ConstantInt::get(IntTy, 0));\n  case Builtin::BI__builtin_call_with_static_chain: {\n    const CallExpr *Call = cast<CallExpr>(E->getArg(0));\n    const Expr *Chain = E->getArg(1);\n    return EmitCall(Call->getCallee()->getType(),\n                    EmitCallee(Call->getCallee()), Call, ReturnValue,\n                    EmitScalarExpr(Chain));\n  }\n  case Builtin::BI_InterlockedExchange8:\n  case Builtin::BI_InterlockedExchange16:\n  case Builtin::BI_InterlockedExchange:\n  case Builtin::BI_InterlockedExchangePointer:\n    return RValue::get(\n        EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedExchange, E));\n  case Builtin::BI_InterlockedCompareExchangePointer:\n  case Builtin::BI_InterlockedCompareExchangePointer_nf: {\n    llvm::Type *RTy;\n    llvm::IntegerType *IntType =\n      IntegerType::get(getLLVMContext(),\n                       getContext().getTypeSize(E->getType()));\n    llvm::Type *IntPtrType = IntType->getPointerTo();\n\n    llvm::Value *Destination =\n      Builder.CreateBitCast(EmitScalarExpr(E->getArg(0)), IntPtrType);\n\n    llvm::Value *Exchange = EmitScalarExpr(E->getArg(1));\n    RTy = Exchange->getType();\n    Exchange = Builder.CreatePtrToInt(Exchange, IntType);\n\n    llvm::Value *Comparand =\n      Builder.CreatePtrToInt(EmitScalarExpr(E->getArg(2)), IntType);\n\n    auto Ordering =\n      BuiltinID == Builtin::BI_InterlockedCompareExchangePointer_nf ?\n      AtomicOrdering::Monotonic : AtomicOrdering::SequentiallyConsistent;\n\n    auto Result = Builder.CreateAtomicCmpXchg(Destination, Comparand, Exchange,\n                                              Ordering, Ordering);\n    Result->setVolatile(true);\n\n    return RValue::get(Builder.CreateIntToPtr(Builder.CreateExtractValue(Result,\n                                                                         0),\n                                              RTy));\n  }\n  case Builtin::BI_InterlockedCompareExchange8:\n  case Builtin::BI_InterlockedCompareExchange16:\n  case Builtin::BI_InterlockedCompareExchange:\n  case Builtin::BI_InterlockedCompareExchange64:\n    return RValue::get(EmitAtomicCmpXchgForMSIntrin(*this, E));\n  case Builtin::BI_InterlockedIncrement16:\n  case Builtin::BI_InterlockedIncrement:\n    return RValue::get(\n        EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedIncrement, E));\n  case Builtin::BI_InterlockedDecrement16:\n  case Builtin::BI_InterlockedDecrement:\n    return RValue::get(\n        EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedDecrement, E));\n  case Builtin::BI_InterlockedAnd8:\n  case Builtin::BI_InterlockedAnd16:\n  case Builtin::BI_InterlockedAnd:\n    return RValue::get(EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedAnd, E));\n  case Builtin::BI_InterlockedExchangeAdd8:\n  case Builtin::BI_InterlockedExchangeAdd16:\n  case Builtin::BI_InterlockedExchangeAdd:\n    return RValue::get(\n        EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedExchangeAdd, E));\n  case Builtin::BI_InterlockedExchangeSub8:\n  case Builtin::BI_InterlockedExchangeSub16:\n  case Builtin::BI_InterlockedExchangeSub:\n    return RValue::get(\n        EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedExchangeSub, E));\n  case Builtin::BI_InterlockedOr8:\n  case Builtin::BI_InterlockedOr16:\n  case Builtin::BI_InterlockedOr:\n    return RValue::get(EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedOr, E));\n  case Builtin::BI_InterlockedXor8:\n  case Builtin::BI_InterlockedXor16:\n  case Builtin::BI_InterlockedXor:\n    return RValue::get(EmitMSVCBuiltinExpr(MSVCIntrin::_InterlockedXor, E));\n\n  case Builtin::BI_bittest64:\n  case Builtin::BI_bittest:\n  case Builtin::BI_bittestandcomplement64:\n  case Builtin::BI_bittestandcomplement:\n  case Builtin::BI_bittestandreset64:\n  case Builtin::BI_bittestandreset:\n  case Builtin::BI_bittestandset64:\n  case Builtin::BI_bittestandset:\n  case Builtin::BI_interlockedbittestandreset:\n  case Builtin::BI_interlockedbittestandreset64:\n  case Builtin::BI_interlockedbittestandset64:\n  case Builtin::BI_interlockedbittestandset:\n  case Builtin::BI_interlockedbittestandset_acq:\n  case Builtin::BI_interlockedbittestandset_rel:\n  case Builtin::BI_interlockedbittestandset_nf:\n  case Builtin::BI_interlockedbittestandreset_acq:\n  case Builtin::BI_interlockedbittestandreset_rel:\n  case Builtin::BI_interlockedbittestandreset_nf:\n    return RValue::get(EmitBitTestIntrinsic(*this, BuiltinID, E));\n\n    // These builtins exist to emit regular volatile loads and stores not\n    // affected by the -fms-volatile setting.\n  case Builtin::BI__iso_volatile_load8:\n  case Builtin::BI__iso_volatile_load16:\n  case Builtin::BI__iso_volatile_load32:\n  case Builtin::BI__iso_volatile_load64:\n    return RValue::get(EmitISOVolatileLoad(*this, E));\n  case Builtin::BI__iso_volatile_store8:\n  case Builtin::BI__iso_volatile_store16:\n  case Builtin::BI__iso_volatile_store32:\n  case Builtin::BI__iso_volatile_store64:\n    return RValue::get(EmitISOVolatileStore(*this, E));\n\n  case Builtin::BI__exception_code:\n  case Builtin::BI_exception_code:\n    return RValue::get(EmitSEHExceptionCode());\n  case Builtin::BI__exception_info:\n  case Builtin::BI_exception_info:\n    return RValue::get(EmitSEHExceptionInfo());\n  case Builtin::BI__abnormal_termination:\n  case Builtin::BI_abnormal_termination:\n    return RValue::get(EmitSEHAbnormalTermination());\n  case Builtin::BI_setjmpex:\n    if (getTarget().getTriple().isOSMSVCRT() && E->getNumArgs() == 1 &&\n        E->getArg(0)->getType()->isPointerType())\n      return EmitMSVCRTSetJmp(*this, MSVCSetJmpKind::_setjmpex, E);\n    break;\n  case Builtin::BI_setjmp:\n    if (getTarget().getTriple().isOSMSVCRT() && E->getNumArgs() == 1 &&\n        E->getArg(0)->getType()->isPointerType()) {\n      if (getTarget().getTriple().getArch() == llvm::Triple::x86)\n        return EmitMSVCRTSetJmp(*this, MSVCSetJmpKind::_setjmp3, E);\n      else if (getTarget().getTriple().getArch() == llvm::Triple::aarch64)\n        return EmitMSVCRTSetJmp(*this, MSVCSetJmpKind::_setjmpex, E);\n      return EmitMSVCRTSetJmp(*this, MSVCSetJmpKind::_setjmp, E);\n    }\n    break;\n\n  case Builtin::BI__GetExceptionInfo: {\n    if (llvm::GlobalVariable *GV =\n            CGM.getCXXABI().getThrowInfo(FD->getParamDecl(0)->getType()))\n      return RValue::get(llvm::ConstantExpr::getBitCast(GV, CGM.Int8PtrTy));\n    break;\n  }\n\n  case Builtin::BI__fastfail:\n    return RValue::get(EmitMSVCBuiltinExpr(MSVCIntrin::__fastfail, E));\n\n  case Builtin::BI__builtin_coro_size: {\n    auto & Context = getContext();\n    auto SizeTy = Context.getSizeType();\n    auto T = Builder.getIntNTy(Context.getTypeSize(SizeTy));\n    Function *F = CGM.getIntrinsic(Intrinsic::coro_size, T);\n    return RValue::get(Builder.CreateCall(F));\n  }\n\n  case Builtin::BI__builtin_coro_id:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_id);\n  case Builtin::BI__builtin_coro_promise:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_promise);\n  case Builtin::BI__builtin_coro_resume:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_resume);\n  case Builtin::BI__builtin_coro_frame:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_frame);\n  case Builtin::BI__builtin_coro_noop:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_noop);\n  case Builtin::BI__builtin_coro_free:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_free);\n  case Builtin::BI__builtin_coro_destroy:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_destroy);\n  case Builtin::BI__builtin_coro_done:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_done);\n  case Builtin::BI__builtin_coro_alloc:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_alloc);\n  case Builtin::BI__builtin_coro_begin:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_begin);\n  case Builtin::BI__builtin_coro_end:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_end);\n  case Builtin::BI__builtin_coro_suspend:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_suspend);\n  case Builtin::BI__builtin_coro_param:\n    return EmitCoroutineIntrinsic(E, Intrinsic::coro_param);\n\n  // OpenCL v2.0 s6.13.16.2, Built-in pipe read and write functions\n  case Builtin::BIread_pipe:\n  case Builtin::BIwrite_pipe: {\n    Value *Arg0 = EmitScalarExpr(E->getArg(0)),\n          *Arg1 = EmitScalarExpr(E->getArg(1));\n    CGOpenCLRuntime OpenCLRT(CGM);\n    Value *PacketSize = OpenCLRT.getPipeElemSize(E->getArg(0));\n    Value *PacketAlign = OpenCLRT.getPipeElemAlign(E->getArg(0));\n\n    // Type of the generic packet parameter.\n    unsigned GenericAS =\n        getContext().getTargetAddressSpace(LangAS::opencl_generic);\n    llvm::Type *I8PTy = llvm::PointerType::get(\n        llvm::Type::getInt8Ty(getLLVMContext()), GenericAS);\n\n    // Testing which overloaded version we should generate the call for.\n    if (2U == E->getNumArgs()) {\n      const char *Name = (BuiltinID == Builtin::BIread_pipe) ? \"__read_pipe_2\"\n                                                             : \"__write_pipe_2\";\n      // Creating a generic function type to be able to call with any builtin or\n      // user defined type.\n      llvm::Type *ArgTys[] = {Arg0->getType(), I8PTy, Int32Ty, Int32Ty};\n      llvm::FunctionType *FTy = llvm::FunctionType::get(\n          Int32Ty, llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n      Value *BCast = Builder.CreatePointerCast(Arg1, I8PTy);\n      return RValue::get(\n          EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name),\n                          {Arg0, BCast, PacketSize, PacketAlign}));\n    } else {\n      assert(4 == E->getNumArgs() &&\n             \"Illegal number of parameters to pipe function\");\n      const char *Name = (BuiltinID == Builtin::BIread_pipe) ? \"__read_pipe_4\"\n                                                             : \"__write_pipe_4\";\n\n      llvm::Type *ArgTys[] = {Arg0->getType(), Arg1->getType(), Int32Ty, I8PTy,\n                              Int32Ty, Int32Ty};\n      Value *Arg2 = EmitScalarExpr(E->getArg(2)),\n            *Arg3 = EmitScalarExpr(E->getArg(3));\n      llvm::FunctionType *FTy = llvm::FunctionType::get(\n          Int32Ty, llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n      Value *BCast = Builder.CreatePointerCast(Arg3, I8PTy);\n      // We know the third argument is an integer type, but we may need to cast\n      // it to i32.\n      if (Arg2->getType() != Int32Ty)\n        Arg2 = Builder.CreateZExtOrTrunc(Arg2, Int32Ty);\n      return RValue::get(\n          EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name),\n                          {Arg0, Arg1, Arg2, BCast, PacketSize, PacketAlign}));\n    }\n  }\n  // OpenCL v2.0 s6.13.16 ,s9.17.3.5 - Built-in pipe reserve read and write\n  // functions\n  case Builtin::BIreserve_read_pipe:\n  case Builtin::BIreserve_write_pipe:\n  case Builtin::BIwork_group_reserve_read_pipe:\n  case Builtin::BIwork_group_reserve_write_pipe:\n  case Builtin::BIsub_group_reserve_read_pipe:\n  case Builtin::BIsub_group_reserve_write_pipe: {\n    // Composing the mangled name for the function.\n    const char *Name;\n    if (BuiltinID == Builtin::BIreserve_read_pipe)\n      Name = \"__reserve_read_pipe\";\n    else if (BuiltinID == Builtin::BIreserve_write_pipe)\n      Name = \"__reserve_write_pipe\";\n    else if (BuiltinID == Builtin::BIwork_group_reserve_read_pipe)\n      Name = \"__work_group_reserve_read_pipe\";\n    else if (BuiltinID == Builtin::BIwork_group_reserve_write_pipe)\n      Name = \"__work_group_reserve_write_pipe\";\n    else if (BuiltinID == Builtin::BIsub_group_reserve_read_pipe)\n      Name = \"__sub_group_reserve_read_pipe\";\n    else\n      Name = \"__sub_group_reserve_write_pipe\";\n\n    Value *Arg0 = EmitScalarExpr(E->getArg(0)),\n          *Arg1 = EmitScalarExpr(E->getArg(1));\n    llvm::Type *ReservedIDTy = ConvertType(getContext().OCLReserveIDTy);\n    CGOpenCLRuntime OpenCLRT(CGM);\n    Value *PacketSize = OpenCLRT.getPipeElemSize(E->getArg(0));\n    Value *PacketAlign = OpenCLRT.getPipeElemAlign(E->getArg(0));\n\n    // Building the generic function prototype.\n    llvm::Type *ArgTys[] = {Arg0->getType(), Int32Ty, Int32Ty, Int32Ty};\n    llvm::FunctionType *FTy = llvm::FunctionType::get(\n        ReservedIDTy, llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n    // We know the second argument is an integer type, but we may need to cast\n    // it to i32.\n    if (Arg1->getType() != Int32Ty)\n      Arg1 = Builder.CreateZExtOrTrunc(Arg1, Int32Ty);\n    return RValue::get(EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name),\n                                       {Arg0, Arg1, PacketSize, PacketAlign}));\n  }\n  // OpenCL v2.0 s6.13.16, s9.17.3.5 - Built-in pipe commit read and write\n  // functions\n  case Builtin::BIcommit_read_pipe:\n  case Builtin::BIcommit_write_pipe:\n  case Builtin::BIwork_group_commit_read_pipe:\n  case Builtin::BIwork_group_commit_write_pipe:\n  case Builtin::BIsub_group_commit_read_pipe:\n  case Builtin::BIsub_group_commit_write_pipe: {\n    const char *Name;\n    if (BuiltinID == Builtin::BIcommit_read_pipe)\n      Name = \"__commit_read_pipe\";\n    else if (BuiltinID == Builtin::BIcommit_write_pipe)\n      Name = \"__commit_write_pipe\";\n    else if (BuiltinID == Builtin::BIwork_group_commit_read_pipe)\n      Name = \"__work_group_commit_read_pipe\";\n    else if (BuiltinID == Builtin::BIwork_group_commit_write_pipe)\n      Name = \"__work_group_commit_write_pipe\";\n    else if (BuiltinID == Builtin::BIsub_group_commit_read_pipe)\n      Name = \"__sub_group_commit_read_pipe\";\n    else\n      Name = \"__sub_group_commit_write_pipe\";\n\n    Value *Arg0 = EmitScalarExpr(E->getArg(0)),\n          *Arg1 = EmitScalarExpr(E->getArg(1));\n    CGOpenCLRuntime OpenCLRT(CGM);\n    Value *PacketSize = OpenCLRT.getPipeElemSize(E->getArg(0));\n    Value *PacketAlign = OpenCLRT.getPipeElemAlign(E->getArg(0));\n\n    // Building the generic function prototype.\n    llvm::Type *ArgTys[] = {Arg0->getType(), Arg1->getType(), Int32Ty, Int32Ty};\n    llvm::FunctionType *FTy =\n        llvm::FunctionType::get(llvm::Type::getVoidTy(getLLVMContext()),\n                                llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n\n    return RValue::get(EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name),\n                                       {Arg0, Arg1, PacketSize, PacketAlign}));\n  }\n  // OpenCL v2.0 s6.13.16.4 Built-in pipe query functions\n  case Builtin::BIget_pipe_num_packets:\n  case Builtin::BIget_pipe_max_packets: {\n    const char *BaseName;\n    const auto *PipeTy = E->getArg(0)->getType()->castAs<PipeType>();\n    if (BuiltinID == Builtin::BIget_pipe_num_packets)\n      BaseName = \"__get_pipe_num_packets\";\n    else\n      BaseName = \"__get_pipe_max_packets\";\n    std::string Name = std::string(BaseName) +\n                       std::string(PipeTy->isReadOnly() ? \"_ro\" : \"_wo\");\n\n    // Building the generic function prototype.\n    Value *Arg0 = EmitScalarExpr(E->getArg(0));\n    CGOpenCLRuntime OpenCLRT(CGM);\n    Value *PacketSize = OpenCLRT.getPipeElemSize(E->getArg(0));\n    Value *PacketAlign = OpenCLRT.getPipeElemAlign(E->getArg(0));\n    llvm::Type *ArgTys[] = {Arg0->getType(), Int32Ty, Int32Ty};\n    llvm::FunctionType *FTy = llvm::FunctionType::get(\n        Int32Ty, llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n\n    return RValue::get(EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name),\n                                       {Arg0, PacketSize, PacketAlign}));\n  }\n\n  // OpenCL v2.0 s6.13.9 - Address space qualifier functions.\n  case Builtin::BIto_global:\n  case Builtin::BIto_local:\n  case Builtin::BIto_private: {\n    auto Arg0 = EmitScalarExpr(E->getArg(0));\n    auto NewArgT = llvm::PointerType::get(Int8Ty,\n      CGM.getContext().getTargetAddressSpace(LangAS::opencl_generic));\n    auto NewRetT = llvm::PointerType::get(Int8Ty,\n      CGM.getContext().getTargetAddressSpace(\n        E->getType()->getPointeeType().getAddressSpace()));\n    auto FTy = llvm::FunctionType::get(NewRetT, {NewArgT}, false);\n    llvm::Value *NewArg;\n    if (Arg0->getType()->getPointerAddressSpace() !=\n        NewArgT->getPointerAddressSpace())\n      NewArg = Builder.CreateAddrSpaceCast(Arg0, NewArgT);\n    else\n      NewArg = Builder.CreateBitOrPointerCast(Arg0, NewArgT);\n    auto NewName = std::string(\"__\") + E->getDirectCallee()->getName().str();\n    auto NewCall =\n        EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, NewName), {NewArg});\n    return RValue::get(Builder.CreateBitOrPointerCast(NewCall,\n      ConvertType(E->getType())));\n  }\n\n  // OpenCL v2.0, s6.13.17 - Enqueue kernel function.\n  // It contains four different overload formats specified in Table 6.13.17.1.\n  case Builtin::BIenqueue_kernel: {\n    StringRef Name; // Generated function call name\n    unsigned NumArgs = E->getNumArgs();\n\n    llvm::Type *QueueTy = ConvertType(getContext().OCLQueueTy);\n    llvm::Type *GenericVoidPtrTy = Builder.getInt8PtrTy(\n        getContext().getTargetAddressSpace(LangAS::opencl_generic));\n\n    llvm::Value *Queue = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Flags = EmitScalarExpr(E->getArg(1));\n    LValue NDRangeL = EmitAggExprToLValue(E->getArg(2));\n    llvm::Value *Range = NDRangeL.getAddress(*this).getPointer();\n    llvm::Type *RangeTy = NDRangeL.getAddress(*this).getType();\n\n    if (NumArgs == 4) {\n      // The most basic form of the call with parameters:\n      // queue_t, kernel_enqueue_flags_t, ndrange_t, block(void)\n      Name = \"__enqueue_kernel_basic\";\n      llvm::Type *ArgTys[] = {QueueTy, Int32Ty, RangeTy, GenericVoidPtrTy,\n                              GenericVoidPtrTy};\n      llvm::FunctionType *FTy = llvm::FunctionType::get(\n          Int32Ty, llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n\n      auto Info =\n          CGM.getOpenCLRuntime().emitOpenCLEnqueuedBlock(*this, E->getArg(3));\n      llvm::Value *Kernel =\n          Builder.CreatePointerCast(Info.Kernel, GenericVoidPtrTy);\n      llvm::Value *Block =\n          Builder.CreatePointerCast(Info.BlockArg, GenericVoidPtrTy);\n\n      AttrBuilder B;\n      B.addByValAttr(NDRangeL.getAddress(*this).getElementType());\n      llvm::AttributeList ByValAttrSet =\n          llvm::AttributeList::get(CGM.getModule().getContext(), 3U, B);\n\n      auto RTCall =\n          EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name, ByValAttrSet),\n                          {Queue, Flags, Range, Kernel, Block});\n      RTCall->setAttributes(ByValAttrSet);\n      return RValue::get(RTCall);\n    }\n    assert(NumArgs >= 5 && \"Invalid enqueue_kernel signature\");\n\n    // Create a temporary array to hold the sizes of local pointer arguments\n    // for the block. \\p First is the position of the first size argument.\n    auto CreateArrayForSizeVar = [=](unsigned First)\n        -> std::tuple<llvm::Value *, llvm::Value *, llvm::Value *> {\n      llvm::APInt ArraySize(32, NumArgs - First);\n      QualType SizeArrayTy = getContext().getConstantArrayType(\n          getContext().getSizeType(), ArraySize, nullptr, ArrayType::Normal,\n          /*IndexTypeQuals=*/0);\n      auto Tmp = CreateMemTemp(SizeArrayTy, \"block_sizes\");\n      llvm::Value *TmpPtr = Tmp.getPointer();\n      llvm::Value *TmpSize = EmitLifetimeStart(\n          CGM.getDataLayout().getTypeAllocSize(Tmp.getElementType()), TmpPtr);\n      llvm::Value *ElemPtr;\n      // Each of the following arguments specifies the size of the corresponding\n      // argument passed to the enqueued block.\n      auto *Zero = llvm::ConstantInt::get(IntTy, 0);\n      for (unsigned I = First; I < NumArgs; ++I) {\n        auto *Index = llvm::ConstantInt::get(IntTy, I - First);\n        auto *GEP = Builder.CreateGEP(TmpPtr, {Zero, Index});\n        if (I == First)\n          ElemPtr = GEP;\n        auto *V =\n            Builder.CreateZExtOrTrunc(EmitScalarExpr(E->getArg(I)), SizeTy);\n        Builder.CreateAlignedStore(\n            V, GEP, CGM.getDataLayout().getPrefTypeAlign(SizeTy));\n      }\n      return std::tie(ElemPtr, TmpSize, TmpPtr);\n    };\n\n    // Could have events and/or varargs.\n    if (E->getArg(3)->getType()->isBlockPointerType()) {\n      // No events passed, but has variadic arguments.\n      Name = \"__enqueue_kernel_varargs\";\n      auto Info =\n          CGM.getOpenCLRuntime().emitOpenCLEnqueuedBlock(*this, E->getArg(3));\n      llvm::Value *Kernel =\n          Builder.CreatePointerCast(Info.Kernel, GenericVoidPtrTy);\n      auto *Block = Builder.CreatePointerCast(Info.BlockArg, GenericVoidPtrTy);\n      llvm::Value *ElemPtr, *TmpSize, *TmpPtr;\n      std::tie(ElemPtr, TmpSize, TmpPtr) = CreateArrayForSizeVar(4);\n\n      // Create a vector of the arguments, as well as a constant value to\n      // express to the runtime the number of variadic arguments.\n      llvm::Value *const Args[] = {Queue,  Flags,\n                                   Range,  Kernel,\n                                   Block,  ConstantInt::get(IntTy, NumArgs - 4),\n                                   ElemPtr};\n      llvm::Type *const ArgTys[] = {\n          QueueTy,          IntTy, RangeTy,           GenericVoidPtrTy,\n          GenericVoidPtrTy, IntTy, ElemPtr->getType()};\n\n      llvm::FunctionType *FTy = llvm::FunctionType::get(Int32Ty, ArgTys, false);\n      auto Call = RValue::get(\n          EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name), Args));\n      if (TmpSize)\n        EmitLifetimeEnd(TmpSize, TmpPtr);\n      return Call;\n    }\n    // Any calls now have event arguments passed.\n    if (NumArgs >= 7) {\n      llvm::Type *EventTy = ConvertType(getContext().OCLClkEventTy);\n      llvm::PointerType *EventPtrTy = EventTy->getPointerTo(\n          CGM.getContext().getTargetAddressSpace(LangAS::opencl_generic));\n\n      llvm::Value *NumEvents =\n          Builder.CreateZExtOrTrunc(EmitScalarExpr(E->getArg(3)), Int32Ty);\n\n      // Since SemaOpenCLBuiltinEnqueueKernel allows fifth and sixth arguments\n      // to be a null pointer constant (including `0` literal), we can take it\n      // into account and emit null pointer directly.\n      llvm::Value *EventWaitList = nullptr;\n      if (E->getArg(4)->isNullPointerConstant(\n              getContext(), Expr::NPC_ValueDependentIsNotNull)) {\n        EventWaitList = llvm::ConstantPointerNull::get(EventPtrTy);\n      } else {\n        EventWaitList = E->getArg(4)->getType()->isArrayType()\n                        ? EmitArrayToPointerDecay(E->getArg(4)).getPointer()\n                        : EmitScalarExpr(E->getArg(4));\n        // Convert to generic address space.\n        EventWaitList = Builder.CreatePointerCast(EventWaitList, EventPtrTy);\n      }\n      llvm::Value *EventRet = nullptr;\n      if (E->getArg(5)->isNullPointerConstant(\n              getContext(), Expr::NPC_ValueDependentIsNotNull)) {\n        EventRet = llvm::ConstantPointerNull::get(EventPtrTy);\n      } else {\n        EventRet =\n            Builder.CreatePointerCast(EmitScalarExpr(E->getArg(5)), EventPtrTy);\n      }\n\n      auto Info =\n          CGM.getOpenCLRuntime().emitOpenCLEnqueuedBlock(*this, E->getArg(6));\n      llvm::Value *Kernel =\n          Builder.CreatePointerCast(Info.Kernel, GenericVoidPtrTy);\n      llvm::Value *Block =\n          Builder.CreatePointerCast(Info.BlockArg, GenericVoidPtrTy);\n\n      std::vector<llvm::Type *> ArgTys = {\n          QueueTy,    Int32Ty,    RangeTy,          Int32Ty,\n          EventPtrTy, EventPtrTy, GenericVoidPtrTy, GenericVoidPtrTy};\n\n      std::vector<llvm::Value *> Args = {Queue,     Flags,         Range,\n                                         NumEvents, EventWaitList, EventRet,\n                                         Kernel,    Block};\n\n      if (NumArgs == 7) {\n        // Has events but no variadics.\n        Name = \"__enqueue_kernel_basic_events\";\n        llvm::FunctionType *FTy = llvm::FunctionType::get(\n            Int32Ty, llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n        return RValue::get(\n            EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name),\n                            llvm::ArrayRef<llvm::Value *>(Args)));\n      }\n      // Has event info and variadics\n      // Pass the number of variadics to the runtime function too.\n      Args.push_back(ConstantInt::get(Int32Ty, NumArgs - 7));\n      ArgTys.push_back(Int32Ty);\n      Name = \"__enqueue_kernel_events_varargs\";\n\n      llvm::Value *ElemPtr, *TmpSize, *TmpPtr;\n      std::tie(ElemPtr, TmpSize, TmpPtr) = CreateArrayForSizeVar(7);\n      Args.push_back(ElemPtr);\n      ArgTys.push_back(ElemPtr->getType());\n\n      llvm::FunctionType *FTy = llvm::FunctionType::get(\n          Int32Ty, llvm::ArrayRef<llvm::Type *>(ArgTys), false);\n      auto Call =\n          RValue::get(EmitRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name),\n                                      llvm::ArrayRef<llvm::Value *>(Args)));\n      if (TmpSize)\n        EmitLifetimeEnd(TmpSize, TmpPtr);\n      return Call;\n    }\n    LLVM_FALLTHROUGH;\n  }\n  // OpenCL v2.0 s6.13.17.6 - Kernel query functions need bitcast of block\n  // parameter.\n  case Builtin::BIget_kernel_work_group_size: {\n    llvm::Type *GenericVoidPtrTy = Builder.getInt8PtrTy(\n        getContext().getTargetAddressSpace(LangAS::opencl_generic));\n    auto Info =\n        CGM.getOpenCLRuntime().emitOpenCLEnqueuedBlock(*this, E->getArg(0));\n    Value *Kernel = Builder.CreatePointerCast(Info.Kernel, GenericVoidPtrTy);\n    Value *Arg = Builder.CreatePointerCast(Info.BlockArg, GenericVoidPtrTy);\n    return RValue::get(EmitRuntimeCall(\n        CGM.CreateRuntimeFunction(\n            llvm::FunctionType::get(IntTy, {GenericVoidPtrTy, GenericVoidPtrTy},\n                                    false),\n            \"__get_kernel_work_group_size_impl\"),\n        {Kernel, Arg}));\n  }\n  case Builtin::BIget_kernel_preferred_work_group_size_multiple: {\n    llvm::Type *GenericVoidPtrTy = Builder.getInt8PtrTy(\n        getContext().getTargetAddressSpace(LangAS::opencl_generic));\n    auto Info =\n        CGM.getOpenCLRuntime().emitOpenCLEnqueuedBlock(*this, E->getArg(0));\n    Value *Kernel = Builder.CreatePointerCast(Info.Kernel, GenericVoidPtrTy);\n    Value *Arg = Builder.CreatePointerCast(Info.BlockArg, GenericVoidPtrTy);\n    return RValue::get(EmitRuntimeCall(\n        CGM.CreateRuntimeFunction(\n            llvm::FunctionType::get(IntTy, {GenericVoidPtrTy, GenericVoidPtrTy},\n                                    false),\n            \"__get_kernel_preferred_work_group_size_multiple_impl\"),\n        {Kernel, Arg}));\n  }\n  case Builtin::BIget_kernel_max_sub_group_size_for_ndrange:\n  case Builtin::BIget_kernel_sub_group_count_for_ndrange: {\n    llvm::Type *GenericVoidPtrTy = Builder.getInt8PtrTy(\n        getContext().getTargetAddressSpace(LangAS::opencl_generic));\n    LValue NDRangeL = EmitAggExprToLValue(E->getArg(0));\n    llvm::Value *NDRange = NDRangeL.getAddress(*this).getPointer();\n    auto Info =\n        CGM.getOpenCLRuntime().emitOpenCLEnqueuedBlock(*this, E->getArg(1));\n    Value *Kernel = Builder.CreatePointerCast(Info.Kernel, GenericVoidPtrTy);\n    Value *Block = Builder.CreatePointerCast(Info.BlockArg, GenericVoidPtrTy);\n    const char *Name =\n        BuiltinID == Builtin::BIget_kernel_max_sub_group_size_for_ndrange\n            ? \"__get_kernel_max_sub_group_size_for_ndrange_impl\"\n            : \"__get_kernel_sub_group_count_for_ndrange_impl\";\n    return RValue::get(EmitRuntimeCall(\n        CGM.CreateRuntimeFunction(\n            llvm::FunctionType::get(\n                IntTy, {NDRange->getType(), GenericVoidPtrTy, GenericVoidPtrTy},\n                false),\n            Name),\n        {NDRange, Kernel, Block}));\n  }\n\n  case Builtin::BI__builtin_store_half:\n  case Builtin::BI__builtin_store_halff: {\n    Value *Val = EmitScalarExpr(E->getArg(0));\n    Address Address = EmitPointerWithAlignment(E->getArg(1));\n    Value *HalfVal = Builder.CreateFPTrunc(Val, Builder.getHalfTy());\n    return RValue::get(Builder.CreateStore(HalfVal, Address));\n  }\n  case Builtin::BI__builtin_load_half: {\n    Address Address = EmitPointerWithAlignment(E->getArg(0));\n    Value *HalfVal = Builder.CreateLoad(Address);\n    return RValue::get(Builder.CreateFPExt(HalfVal, Builder.getDoubleTy()));\n  }\n  case Builtin::BI__builtin_load_halff: {\n    Address Address = EmitPointerWithAlignment(E->getArg(0));\n    Value *HalfVal = Builder.CreateLoad(Address);\n    return RValue::get(Builder.CreateFPExt(HalfVal, Builder.getFloatTy()));\n  }\n  case Builtin::BIprintf:\n    if (getTarget().getTriple().isNVPTX())\n      return EmitNVPTXDevicePrintfCallExpr(E, ReturnValue);\n    if (getTarget().getTriple().getArch() == Triple::amdgcn &&\n        getLangOpts().HIP)\n      return EmitAMDGPUDevicePrintfCallExpr(E, ReturnValue);\n    break;\n  case Builtin::BI__builtin_canonicalize:\n  case Builtin::BI__builtin_canonicalizef:\n  case Builtin::BI__builtin_canonicalizef16:\n  case Builtin::BI__builtin_canonicalizel:\n    return RValue::get(emitUnaryBuiltin(*this, E, Intrinsic::canonicalize));\n\n  case Builtin::BI__builtin_thread_pointer: {\n    if (!getContext().getTargetInfo().isTLSSupported())\n      CGM.ErrorUnsupported(E, \"__builtin_thread_pointer\");\n    // Fall through - it's already mapped to the intrinsic by GCCBuiltin.\n    break;\n  }\n  case Builtin::BI__builtin_os_log_format:\n    return emitBuiltinOSLogFormat(*E);\n\n  case Builtin::BI__xray_customevent: {\n    if (!ShouldXRayInstrumentFunction())\n      return RValue::getIgnored();\n\n    if (!CGM.getCodeGenOpts().XRayInstrumentationBundle.has(\n            XRayInstrKind::Custom))\n      return RValue::getIgnored();\n\n    if (const auto *XRayAttr = CurFuncDecl->getAttr<XRayInstrumentAttr>())\n      if (XRayAttr->neverXRayInstrument() && !AlwaysEmitXRayCustomEvents())\n        return RValue::getIgnored();\n\n    Function *F = CGM.getIntrinsic(Intrinsic::xray_customevent);\n    auto FTy = F->getFunctionType();\n    auto Arg0 = E->getArg(0);\n    auto Arg0Val = EmitScalarExpr(Arg0);\n    auto Arg0Ty = Arg0->getType();\n    auto PTy0 = FTy->getParamType(0);\n    if (PTy0 != Arg0Val->getType()) {\n      if (Arg0Ty->isArrayType())\n        Arg0Val = EmitArrayToPointerDecay(Arg0).getPointer();\n      else\n        Arg0Val = Builder.CreatePointerCast(Arg0Val, PTy0);\n    }\n    auto Arg1 = EmitScalarExpr(E->getArg(1));\n    auto PTy1 = FTy->getParamType(1);\n    if (PTy1 != Arg1->getType())\n      Arg1 = Builder.CreateTruncOrBitCast(Arg1, PTy1);\n    return RValue::get(Builder.CreateCall(F, {Arg0Val, Arg1}));\n  }\n\n  case Builtin::BI__xray_typedevent: {\n    // TODO: There should be a way to always emit events even if the current\n    // function is not instrumented. Losing events in a stream can cripple\n    // a trace.\n    if (!ShouldXRayInstrumentFunction())\n      return RValue::getIgnored();\n\n    if (!CGM.getCodeGenOpts().XRayInstrumentationBundle.has(\n            XRayInstrKind::Typed))\n      return RValue::getIgnored();\n\n    if (const auto *XRayAttr = CurFuncDecl->getAttr<XRayInstrumentAttr>())\n      if (XRayAttr->neverXRayInstrument() && !AlwaysEmitXRayTypedEvents())\n        return RValue::getIgnored();\n\n    Function *F = CGM.getIntrinsic(Intrinsic::xray_typedevent);\n    auto FTy = F->getFunctionType();\n    auto Arg0 = EmitScalarExpr(E->getArg(0));\n    auto PTy0 = FTy->getParamType(0);\n    if (PTy0 != Arg0->getType())\n      Arg0 = Builder.CreateTruncOrBitCast(Arg0, PTy0);\n    auto Arg1 = E->getArg(1);\n    auto Arg1Val = EmitScalarExpr(Arg1);\n    auto Arg1Ty = Arg1->getType();\n    auto PTy1 = FTy->getParamType(1);\n    if (PTy1 != Arg1Val->getType()) {\n      if (Arg1Ty->isArrayType())\n        Arg1Val = EmitArrayToPointerDecay(Arg1).getPointer();\n      else\n        Arg1Val = Builder.CreatePointerCast(Arg1Val, PTy1);\n    }\n    auto Arg2 = EmitScalarExpr(E->getArg(2));\n    auto PTy2 = FTy->getParamType(2);\n    if (PTy2 != Arg2->getType())\n      Arg2 = Builder.CreateTruncOrBitCast(Arg2, PTy2);\n    return RValue::get(Builder.CreateCall(F, {Arg0, Arg1Val, Arg2}));\n  }\n\n  case Builtin::BI__builtin_ms_va_start:\n  case Builtin::BI__builtin_ms_va_end:\n    return RValue::get(\n        EmitVAStartEnd(EmitMSVAListRef(E->getArg(0)).getPointer(),\n                       BuiltinID == Builtin::BI__builtin_ms_va_start));\n\n  case Builtin::BI__builtin_ms_va_copy: {\n    // Lower this manually. We can't reliably determine whether or not any\n    // given va_copy() is for a Win64 va_list from the calling convention\n    // alone, because it's legal to do this from a System V ABI function.\n    // With opaque pointer types, we won't have enough information in LLVM\n    // IR to determine this from the argument types, either. Best to do it\n    // now, while we have enough information.\n    Address DestAddr = EmitMSVAListRef(E->getArg(0));\n    Address SrcAddr = EmitMSVAListRef(E->getArg(1));\n\n    llvm::Type *BPP = Int8PtrPtrTy;\n\n    DestAddr = Address(Builder.CreateBitCast(DestAddr.getPointer(), BPP, \"cp\"),\n                       DestAddr.getAlignment());\n    SrcAddr = Address(Builder.CreateBitCast(SrcAddr.getPointer(), BPP, \"ap\"),\n                      SrcAddr.getAlignment());\n\n    Value *ArgPtr = Builder.CreateLoad(SrcAddr, \"ap.val\");\n    return RValue::get(Builder.CreateStore(ArgPtr, DestAddr));\n  }\n  }\n\n  // If this is an alias for a lib function (e.g. __builtin_sin), emit\n  // the call using the normal call path, but using the unmangled\n  // version of the function name.\n  if (getContext().BuiltinInfo.isLibFunction(BuiltinID))\n    return emitLibraryCall(*this, FD, E,\n                           CGM.getBuiltinLibFunction(FD, BuiltinID));\n\n  // If this is a predefined lib function (e.g. malloc), emit the call\n  // using exactly the normal call path.\n  if (getContext().BuiltinInfo.isPredefinedLibFunction(BuiltinID))\n    return emitLibraryCall(*this, FD, E,\n                      cast<llvm::Constant>(EmitScalarExpr(E->getCallee())));\n\n  // Check that a call to a target specific builtin has the correct target\n  // features.\n  // This is down here to avoid non-target specific builtins, however, if\n  // generic builtins start to require generic target features then we\n  // can move this up to the beginning of the function.\n  checkTargetFeatures(E, FD);\n\n  if (unsigned VectorWidth = getContext().BuiltinInfo.getRequiredVectorWidth(BuiltinID))\n    LargestVectorWidth = std::max(LargestVectorWidth, VectorWidth);\n\n  // See if we have a target specific intrinsic.\n  const char *Name = getContext().BuiltinInfo.getName(BuiltinID);\n  Intrinsic::ID IntrinsicID = Intrinsic::not_intrinsic;\n  StringRef Prefix =\n      llvm::Triple::getArchTypePrefix(getTarget().getTriple().getArch());\n  if (!Prefix.empty()) {\n    IntrinsicID = Intrinsic::getIntrinsicForGCCBuiltin(Prefix.data(), Name);\n    // NOTE we don't need to perform a compatibility flag check here since the\n    // intrinsics are declared in Builtins*.def via LANGBUILTIN which filter the\n    // MS builtins via ALL_MS_LANGUAGES and are filtered earlier.\n    if (IntrinsicID == Intrinsic::not_intrinsic)\n      IntrinsicID = Intrinsic::getIntrinsicForMSBuiltin(Prefix.data(), Name);\n  }\n\n  if (IntrinsicID != Intrinsic::not_intrinsic) {\n    SmallVector<Value*, 16> Args;\n\n    // Find out if any arguments are required to be integer constant\n    // expressions.\n    unsigned ICEArguments = 0;\n    ASTContext::GetBuiltinTypeError Error;\n    getContext().GetBuiltinType(BuiltinID, Error, &ICEArguments);\n    assert(Error == ASTContext::GE_None && \"Should not codegen an error\");\n\n    Function *F = CGM.getIntrinsic(IntrinsicID);\n    llvm::FunctionType *FTy = F->getFunctionType();\n\n    for (unsigned i = 0, e = E->getNumArgs(); i != e; ++i) {\n      Value *ArgValue;\n      // If this is a normal argument, just emit it as a scalar.\n      if ((ICEArguments & (1 << i)) == 0) {\n        ArgValue = EmitScalarExpr(E->getArg(i));\n      } else {\n        // If this is required to be a constant, constant fold it so that we\n        // know that the generated intrinsic gets a ConstantInt.\n        ArgValue = llvm::ConstantInt::get(\n            getLLVMContext(),\n            *E->getArg(i)->getIntegerConstantExpr(getContext()));\n      }\n\n      // If the intrinsic arg type is different from the builtin arg type\n      // we need to do a bit cast.\n      llvm::Type *PTy = FTy->getParamType(i);\n      if (PTy != ArgValue->getType()) {\n        // XXX - vector of pointers?\n        if (auto *PtrTy = dyn_cast<llvm::PointerType>(PTy)) {\n          if (PtrTy->getAddressSpace() !=\n              ArgValue->getType()->getPointerAddressSpace()) {\n            ArgValue = Builder.CreateAddrSpaceCast(\n              ArgValue,\n              ArgValue->getType()->getPointerTo(PtrTy->getAddressSpace()));\n          }\n        }\n\n        assert(PTy->canLosslesslyBitCastTo(FTy->getParamType(i)) &&\n               \"Must be able to losslessly bit cast to param\");\n        ArgValue = Builder.CreateBitCast(ArgValue, PTy);\n      }\n\n      Args.push_back(ArgValue);\n    }\n\n    Value *V = Builder.CreateCall(F, Args);\n    QualType BuiltinRetType = E->getType();\n\n    llvm::Type *RetTy = VoidTy;\n    if (!BuiltinRetType->isVoidType())\n      RetTy = ConvertType(BuiltinRetType);\n\n    if (RetTy != V->getType()) {\n      // XXX - vector of pointers?\n      if (auto *PtrTy = dyn_cast<llvm::PointerType>(RetTy)) {\n        if (PtrTy->getAddressSpace() != V->getType()->getPointerAddressSpace()) {\n          V = Builder.CreateAddrSpaceCast(\n            V, V->getType()->getPointerTo(PtrTy->getAddressSpace()));\n        }\n      }\n\n      assert(V->getType()->canLosslesslyBitCastTo(RetTy) &&\n             \"Must be able to losslessly bit cast result type\");\n      V = Builder.CreateBitCast(V, RetTy);\n    }\n\n    return RValue::get(V);\n  }\n\n  // Some target-specific builtins can have aggregate return values, e.g.\n  // __builtin_arm_mve_vld2q_u32. So if the result is an aggregate, force\n  // ReturnValue to be non-null, so that the target-specific emission code can\n  // always just emit into it.\n  TypeEvaluationKind EvalKind = getEvaluationKind(E->getType());\n  if (EvalKind == TEK_Aggregate && ReturnValue.isNull()) {\n    Address DestPtr = CreateMemTemp(E->getType(), \"agg.tmp\");\n    ReturnValue = ReturnValueSlot(DestPtr, false);\n  }\n\n  // Now see if we can emit a target-specific builtin.\n  if (Value *V = EmitTargetBuiltinExpr(BuiltinID, E, ReturnValue)) {\n    switch (EvalKind) {\n    case TEK_Scalar:\n      return RValue::get(V);\n    case TEK_Aggregate:\n      return RValue::getAggregate(ReturnValue.getValue(),\n                                  ReturnValue.isVolatile());\n    case TEK_Complex:\n      llvm_unreachable(\"No current target builtin returns complex\");\n    }\n    llvm_unreachable(\"Bad evaluation kind in EmitBuiltinExpr\");\n  }\n\n  ErrorUnsupported(E, \"builtin function\");\n\n  // Unknown builtin, for now just dump it out and return undef.\n  return GetUndefRValue(E->getType());\n}\n\nstatic Value *EmitTargetArchBuiltinExpr(CodeGenFunction *CGF,\n                                        unsigned BuiltinID, const CallExpr *E,\n                                        ReturnValueSlot ReturnValue,\n                                        llvm::Triple::ArchType Arch) {\n  switch (Arch) {\n  case llvm::Triple::arm:\n  case llvm::Triple::armeb:\n  case llvm::Triple::thumb:\n  case llvm::Triple::thumbeb:\n    return CGF->EmitARMBuiltinExpr(BuiltinID, E, ReturnValue, Arch);\n  case llvm::Triple::aarch64:\n  case llvm::Triple::aarch64_32:\n  case llvm::Triple::aarch64_be:\n    return CGF->EmitAArch64BuiltinExpr(BuiltinID, E, Arch);\n  case llvm::Triple::bpfeb:\n  case llvm::Triple::bpfel:\n    return CGF->EmitBPFBuiltinExpr(BuiltinID, E);\n  case llvm::Triple::x86:\n  case llvm::Triple::x86_64:\n    return CGF->EmitX86BuiltinExpr(BuiltinID, E);\n  case llvm::Triple::ppc:\n  case llvm::Triple::ppcle:\n  case llvm::Triple::ppc64:\n  case llvm::Triple::ppc64le:\n    return CGF->EmitPPCBuiltinExpr(BuiltinID, E);\n  case llvm::Triple::r600:\n  case llvm::Triple::amdgcn:\n    return CGF->EmitAMDGPUBuiltinExpr(BuiltinID, E);\n  case llvm::Triple::systemz:\n    return CGF->EmitSystemZBuiltinExpr(BuiltinID, E);\n  case llvm::Triple::nvptx:\n  case llvm::Triple::nvptx64:\n    return CGF->EmitNVPTXBuiltinExpr(BuiltinID, E);\n  case llvm::Triple::wasm32:\n  case llvm::Triple::wasm64:\n    return CGF->EmitWebAssemblyBuiltinExpr(BuiltinID, E);\n  case llvm::Triple::hexagon:\n    return CGF->EmitHexagonBuiltinExpr(BuiltinID, E);\n  default:\n    return nullptr;\n  }\n}\n\nValue *CodeGenFunction::EmitTargetBuiltinExpr(unsigned BuiltinID,\n                                              const CallExpr *E,\n                                              ReturnValueSlot ReturnValue) {\n  if (getContext().BuiltinInfo.isAuxBuiltinID(BuiltinID)) {\n    assert(getContext().getAuxTargetInfo() && \"Missing aux target info\");\n    return EmitTargetArchBuiltinExpr(\n        this, getContext().BuiltinInfo.getAuxBuiltinID(BuiltinID), E,\n        ReturnValue, getContext().getAuxTargetInfo()->getTriple().getArch());\n  }\n\n  return EmitTargetArchBuiltinExpr(this, BuiltinID, E, ReturnValue,\n                                   getTarget().getTriple().getArch());\n}\n\nstatic llvm::FixedVectorType *GetNeonType(CodeGenFunction *CGF,\n                                          NeonTypeFlags TypeFlags,\n                                          bool HasLegalHalfType = true,\n                                          bool V1Ty = false,\n                                          bool AllowBFloatArgsAndRet = true) {\n  int IsQuad = TypeFlags.isQuad();\n  switch (TypeFlags.getEltType()) {\n  case NeonTypeFlags::Int8:\n  case NeonTypeFlags::Poly8:\n    return llvm::FixedVectorType::get(CGF->Int8Ty, V1Ty ? 1 : (8 << IsQuad));\n  case NeonTypeFlags::Int16:\n  case NeonTypeFlags::Poly16:\n    return llvm::FixedVectorType::get(CGF->Int16Ty, V1Ty ? 1 : (4 << IsQuad));\n  case NeonTypeFlags::BFloat16:\n    if (AllowBFloatArgsAndRet)\n      return llvm::FixedVectorType::get(CGF->BFloatTy, V1Ty ? 1 : (4 << IsQuad));\n    else\n      return llvm::FixedVectorType::get(CGF->Int16Ty, V1Ty ? 1 : (4 << IsQuad));\n  case NeonTypeFlags::Float16:\n    if (HasLegalHalfType)\n      return llvm::FixedVectorType::get(CGF->HalfTy, V1Ty ? 1 : (4 << IsQuad));\n    else\n      return llvm::FixedVectorType::get(CGF->Int16Ty, V1Ty ? 1 : (4 << IsQuad));\n  case NeonTypeFlags::Int32:\n    return llvm::FixedVectorType::get(CGF->Int32Ty, V1Ty ? 1 : (2 << IsQuad));\n  case NeonTypeFlags::Int64:\n  case NeonTypeFlags::Poly64:\n    return llvm::FixedVectorType::get(CGF->Int64Ty, V1Ty ? 1 : (1 << IsQuad));\n  case NeonTypeFlags::Poly128:\n    // FIXME: i128 and f128 doesn't get fully support in Clang and llvm.\n    // There is a lot of i128 and f128 API missing.\n    // so we use v16i8 to represent poly128 and get pattern matched.\n    return llvm::FixedVectorType::get(CGF->Int8Ty, 16);\n  case NeonTypeFlags::Float32:\n    return llvm::FixedVectorType::get(CGF->FloatTy, V1Ty ? 1 : (2 << IsQuad));\n  case NeonTypeFlags::Float64:\n    return llvm::FixedVectorType::get(CGF->DoubleTy, V1Ty ? 1 : (1 << IsQuad));\n  }\n  llvm_unreachable(\"Unknown vector element type!\");\n}\n\nstatic llvm::VectorType *GetFloatNeonType(CodeGenFunction *CGF,\n                                          NeonTypeFlags IntTypeFlags) {\n  int IsQuad = IntTypeFlags.isQuad();\n  switch (IntTypeFlags.getEltType()) {\n  case NeonTypeFlags::Int16:\n    return llvm::FixedVectorType::get(CGF->HalfTy, (4 << IsQuad));\n  case NeonTypeFlags::Int32:\n    return llvm::FixedVectorType::get(CGF->FloatTy, (2 << IsQuad));\n  case NeonTypeFlags::Int64:\n    return llvm::FixedVectorType::get(CGF->DoubleTy, (1 << IsQuad));\n  default:\n    llvm_unreachable(\"Type can't be converted to floating-point!\");\n  }\n}\n\nValue *CodeGenFunction::EmitNeonSplat(Value *V, Constant *C,\n                                      const ElementCount &Count) {\n  Value *SV = llvm::ConstantVector::getSplat(Count, C);\n  return Builder.CreateShuffleVector(V, V, SV, \"lane\");\n}\n\nValue *CodeGenFunction::EmitNeonSplat(Value *V, Constant *C) {\n  ElementCount EC = cast<llvm::VectorType>(V->getType())->getElementCount();\n  return EmitNeonSplat(V, C, EC);\n}\n\nValue *CodeGenFunction::EmitNeonCall(Function *F, SmallVectorImpl<Value*> &Ops,\n                                     const char *name,\n                                     unsigned shift, bool rightshift) {\n  unsigned j = 0;\n  for (Function::const_arg_iterator ai = F->arg_begin(), ae = F->arg_end();\n       ai != ae; ++ai, ++j) {\n    if (F->isConstrainedFPIntrinsic())\n      if (ai->getType()->isMetadataTy())\n        continue;\n    if (shift > 0 && shift == j)\n      Ops[j] = EmitNeonShiftVector(Ops[j], ai->getType(), rightshift);\n    else\n      Ops[j] = Builder.CreateBitCast(Ops[j], ai->getType(), name);\n  }\n\n  if (F->isConstrainedFPIntrinsic())\n    return Builder.CreateConstrainedFPCall(F, Ops, name);\n  else\n    return Builder.CreateCall(F, Ops, name);\n}\n\nValue *CodeGenFunction::EmitNeonShiftVector(Value *V, llvm::Type *Ty,\n                                            bool neg) {\n  int SV = cast<ConstantInt>(V)->getSExtValue();\n  return ConstantInt::get(Ty, neg ? -SV : SV);\n}\n\n// Right-shift a vector by a constant.\nValue *CodeGenFunction::EmitNeonRShiftImm(Value *Vec, Value *Shift,\n                                          llvm::Type *Ty, bool usgn,\n                                          const char *name) {\n  llvm::VectorType *VTy = cast<llvm::VectorType>(Ty);\n\n  int ShiftAmt = cast<ConstantInt>(Shift)->getSExtValue();\n  int EltSize = VTy->getScalarSizeInBits();\n\n  Vec = Builder.CreateBitCast(Vec, Ty);\n\n  // lshr/ashr are undefined when the shift amount is equal to the vector\n  // element size.\n  if (ShiftAmt == EltSize) {\n    if (usgn) {\n      // Right-shifting an unsigned value by its size yields 0.\n      return llvm::ConstantAggregateZero::get(VTy);\n    } else {\n      // Right-shifting a signed value by its size is equivalent\n      // to a shift of size-1.\n      --ShiftAmt;\n      Shift = ConstantInt::get(VTy->getElementType(), ShiftAmt);\n    }\n  }\n\n  Shift = EmitNeonShiftVector(Shift, Ty, false);\n  if (usgn)\n    return Builder.CreateLShr(Vec, Shift, name);\n  else\n    return Builder.CreateAShr(Vec, Shift, name);\n}\n\nenum {\n  AddRetType = (1 << 0),\n  Add1ArgType = (1 << 1),\n  Add2ArgTypes = (1 << 2),\n\n  VectorizeRetType = (1 << 3),\n  VectorizeArgTypes = (1 << 4),\n\n  InventFloatType = (1 << 5),\n  UnsignedAlts = (1 << 6),\n\n  Use64BitVectors = (1 << 7),\n  Use128BitVectors = (1 << 8),\n\n  Vectorize1ArgType = Add1ArgType | VectorizeArgTypes,\n  VectorRet = AddRetType | VectorizeRetType,\n  VectorRetGetArgs01 =\n      AddRetType | Add2ArgTypes | VectorizeRetType | VectorizeArgTypes,\n  FpCmpzModifiers =\n      AddRetType | VectorizeRetType | Add1ArgType | InventFloatType\n};\n\nnamespace {\nstruct ARMVectorIntrinsicInfo {\n  const char *NameHint;\n  unsigned BuiltinID;\n  unsigned LLVMIntrinsic;\n  unsigned AltLLVMIntrinsic;\n  uint64_t TypeModifier;\n\n  bool operator<(unsigned RHSBuiltinID) const {\n    return BuiltinID < RHSBuiltinID;\n  }\n  bool operator<(const ARMVectorIntrinsicInfo &TE) const {\n    return BuiltinID < TE.BuiltinID;\n  }\n};\n} // end anonymous namespace\n\n#define NEONMAP0(NameBase) \\\n  { #NameBase, NEON::BI__builtin_neon_ ## NameBase, 0, 0, 0 }\n\n#define NEONMAP1(NameBase, LLVMIntrinsic, TypeModifier) \\\n  { #NameBase, NEON:: BI__builtin_neon_ ## NameBase, \\\n      Intrinsic::LLVMIntrinsic, 0, TypeModifier }\n\n#define NEONMAP2(NameBase, LLVMIntrinsic, AltLLVMIntrinsic, TypeModifier) \\\n  { #NameBase, NEON:: BI__builtin_neon_ ## NameBase, \\\n      Intrinsic::LLVMIntrinsic, Intrinsic::AltLLVMIntrinsic, \\\n      TypeModifier }\n\nstatic const ARMVectorIntrinsicInfo ARMSIMDIntrinsicMap [] = {\n  NEONMAP1(__a32_vcvt_bf16_v, arm_neon_vcvtfp2bf, 0),\n  NEONMAP0(splat_lane_v),\n  NEONMAP0(splat_laneq_v),\n  NEONMAP0(splatq_lane_v),\n  NEONMAP0(splatq_laneq_v),\n  NEONMAP2(vabd_v, arm_neon_vabdu, arm_neon_vabds, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vabdq_v, arm_neon_vabdu, arm_neon_vabds, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vabs_v, arm_neon_vabs, 0),\n  NEONMAP1(vabsq_v, arm_neon_vabs, 0),\n  NEONMAP0(vaddhn_v),\n  NEONMAP1(vaesdq_v, arm_neon_aesd, 0),\n  NEONMAP1(vaeseq_v, arm_neon_aese, 0),\n  NEONMAP1(vaesimcq_v, arm_neon_aesimc, 0),\n  NEONMAP1(vaesmcq_v, arm_neon_aesmc, 0),\n  NEONMAP1(vbfdot_v, arm_neon_bfdot, 0),\n  NEONMAP1(vbfdotq_v, arm_neon_bfdot, 0),\n  NEONMAP1(vbfmlalbq_v, arm_neon_bfmlalb, 0),\n  NEONMAP1(vbfmlaltq_v, arm_neon_bfmlalt, 0),\n  NEONMAP1(vbfmmlaq_v, arm_neon_bfmmla, 0),\n  NEONMAP1(vbsl_v, arm_neon_vbsl, AddRetType),\n  NEONMAP1(vbslq_v, arm_neon_vbsl, AddRetType),\n  NEONMAP1(vcadd_rot270_v, arm_neon_vcadd_rot270, Add1ArgType),\n  NEONMAP1(vcadd_rot90_v, arm_neon_vcadd_rot90, Add1ArgType),\n  NEONMAP1(vcaddq_rot270_v, arm_neon_vcadd_rot270, Add1ArgType),\n  NEONMAP1(vcaddq_rot90_v, arm_neon_vcadd_rot90, Add1ArgType),\n  NEONMAP1(vcage_v, arm_neon_vacge, 0),\n  NEONMAP1(vcageq_v, arm_neon_vacge, 0),\n  NEONMAP1(vcagt_v, arm_neon_vacgt, 0),\n  NEONMAP1(vcagtq_v, arm_neon_vacgt, 0),\n  NEONMAP1(vcale_v, arm_neon_vacge, 0),\n  NEONMAP1(vcaleq_v, arm_neon_vacge, 0),\n  NEONMAP1(vcalt_v, arm_neon_vacgt, 0),\n  NEONMAP1(vcaltq_v, arm_neon_vacgt, 0),\n  NEONMAP0(vceqz_v),\n  NEONMAP0(vceqzq_v),\n  NEONMAP0(vcgez_v),\n  NEONMAP0(vcgezq_v),\n  NEONMAP0(vcgtz_v),\n  NEONMAP0(vcgtzq_v),\n  NEONMAP0(vclez_v),\n  NEONMAP0(vclezq_v),\n  NEONMAP1(vcls_v, arm_neon_vcls, Add1ArgType),\n  NEONMAP1(vclsq_v, arm_neon_vcls, Add1ArgType),\n  NEONMAP0(vcltz_v),\n  NEONMAP0(vcltzq_v),\n  NEONMAP1(vclz_v, ctlz, Add1ArgType),\n  NEONMAP1(vclzq_v, ctlz, Add1ArgType),\n  NEONMAP1(vcnt_v, ctpop, Add1ArgType),\n  NEONMAP1(vcntq_v, ctpop, Add1ArgType),\n  NEONMAP1(vcvt_f16_f32, arm_neon_vcvtfp2hf, 0),\n  NEONMAP0(vcvt_f16_v),\n  NEONMAP1(vcvt_f32_f16, arm_neon_vcvthf2fp, 0),\n  NEONMAP0(vcvt_f32_v),\n  NEONMAP2(vcvt_n_f16_v, arm_neon_vcvtfxu2fp, arm_neon_vcvtfxs2fp, 0),\n  NEONMAP2(vcvt_n_f32_v, arm_neon_vcvtfxu2fp, arm_neon_vcvtfxs2fp, 0),\n  NEONMAP1(vcvt_n_s16_v, arm_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvt_n_s32_v, arm_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvt_n_s64_v, arm_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvt_n_u16_v, arm_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvt_n_u32_v, arm_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvt_n_u64_v, arm_neon_vcvtfp2fxu, 0),\n  NEONMAP0(vcvt_s16_v),\n  NEONMAP0(vcvt_s32_v),\n  NEONMAP0(vcvt_s64_v),\n  NEONMAP0(vcvt_u16_v),\n  NEONMAP0(vcvt_u32_v),\n  NEONMAP0(vcvt_u64_v),\n  NEONMAP1(vcvta_s16_v, arm_neon_vcvtas, 0),\n  NEONMAP1(vcvta_s32_v, arm_neon_vcvtas, 0),\n  NEONMAP1(vcvta_s64_v, arm_neon_vcvtas, 0),\n  NEONMAP1(vcvta_u16_v, arm_neon_vcvtau, 0),\n  NEONMAP1(vcvta_u32_v, arm_neon_vcvtau, 0),\n  NEONMAP1(vcvta_u64_v, arm_neon_vcvtau, 0),\n  NEONMAP1(vcvtaq_s16_v, arm_neon_vcvtas, 0),\n  NEONMAP1(vcvtaq_s32_v, arm_neon_vcvtas, 0),\n  NEONMAP1(vcvtaq_s64_v, arm_neon_vcvtas, 0),\n  NEONMAP1(vcvtaq_u16_v, arm_neon_vcvtau, 0),\n  NEONMAP1(vcvtaq_u32_v, arm_neon_vcvtau, 0),\n  NEONMAP1(vcvtaq_u64_v, arm_neon_vcvtau, 0),\n  NEONMAP1(vcvth_bf16_f32, arm_neon_vcvtbfp2bf, 0),\n  NEONMAP1(vcvtm_s16_v, arm_neon_vcvtms, 0),\n  NEONMAP1(vcvtm_s32_v, arm_neon_vcvtms, 0),\n  NEONMAP1(vcvtm_s64_v, arm_neon_vcvtms, 0),\n  NEONMAP1(vcvtm_u16_v, arm_neon_vcvtmu, 0),\n  NEONMAP1(vcvtm_u32_v, arm_neon_vcvtmu, 0),\n  NEONMAP1(vcvtm_u64_v, arm_neon_vcvtmu, 0),\n  NEONMAP1(vcvtmq_s16_v, arm_neon_vcvtms, 0),\n  NEONMAP1(vcvtmq_s32_v, arm_neon_vcvtms, 0),\n  NEONMAP1(vcvtmq_s64_v, arm_neon_vcvtms, 0),\n  NEONMAP1(vcvtmq_u16_v, arm_neon_vcvtmu, 0),\n  NEONMAP1(vcvtmq_u32_v, arm_neon_vcvtmu, 0),\n  NEONMAP1(vcvtmq_u64_v, arm_neon_vcvtmu, 0),\n  NEONMAP1(vcvtn_s16_v, arm_neon_vcvtns, 0),\n  NEONMAP1(vcvtn_s32_v, arm_neon_vcvtns, 0),\n  NEONMAP1(vcvtn_s64_v, arm_neon_vcvtns, 0),\n  NEONMAP1(vcvtn_u16_v, arm_neon_vcvtnu, 0),\n  NEONMAP1(vcvtn_u32_v, arm_neon_vcvtnu, 0),\n  NEONMAP1(vcvtn_u64_v, arm_neon_vcvtnu, 0),\n  NEONMAP1(vcvtnq_s16_v, arm_neon_vcvtns, 0),\n  NEONMAP1(vcvtnq_s32_v, arm_neon_vcvtns, 0),\n  NEONMAP1(vcvtnq_s64_v, arm_neon_vcvtns, 0),\n  NEONMAP1(vcvtnq_u16_v, arm_neon_vcvtnu, 0),\n  NEONMAP1(vcvtnq_u32_v, arm_neon_vcvtnu, 0),\n  NEONMAP1(vcvtnq_u64_v, arm_neon_vcvtnu, 0),\n  NEONMAP1(vcvtp_s16_v, arm_neon_vcvtps, 0),\n  NEONMAP1(vcvtp_s32_v, arm_neon_vcvtps, 0),\n  NEONMAP1(vcvtp_s64_v, arm_neon_vcvtps, 0),\n  NEONMAP1(vcvtp_u16_v, arm_neon_vcvtpu, 0),\n  NEONMAP1(vcvtp_u32_v, arm_neon_vcvtpu, 0),\n  NEONMAP1(vcvtp_u64_v, arm_neon_vcvtpu, 0),\n  NEONMAP1(vcvtpq_s16_v, arm_neon_vcvtps, 0),\n  NEONMAP1(vcvtpq_s32_v, arm_neon_vcvtps, 0),\n  NEONMAP1(vcvtpq_s64_v, arm_neon_vcvtps, 0),\n  NEONMAP1(vcvtpq_u16_v, arm_neon_vcvtpu, 0),\n  NEONMAP1(vcvtpq_u32_v, arm_neon_vcvtpu, 0),\n  NEONMAP1(vcvtpq_u64_v, arm_neon_vcvtpu, 0),\n  NEONMAP0(vcvtq_f16_v),\n  NEONMAP0(vcvtq_f32_v),\n  NEONMAP2(vcvtq_n_f16_v, arm_neon_vcvtfxu2fp, arm_neon_vcvtfxs2fp, 0),\n  NEONMAP2(vcvtq_n_f32_v, arm_neon_vcvtfxu2fp, arm_neon_vcvtfxs2fp, 0),\n  NEONMAP1(vcvtq_n_s16_v, arm_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvtq_n_s32_v, arm_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvtq_n_s64_v, arm_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvtq_n_u16_v, arm_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvtq_n_u32_v, arm_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvtq_n_u64_v, arm_neon_vcvtfp2fxu, 0),\n  NEONMAP0(vcvtq_s16_v),\n  NEONMAP0(vcvtq_s32_v),\n  NEONMAP0(vcvtq_s64_v),\n  NEONMAP0(vcvtq_u16_v),\n  NEONMAP0(vcvtq_u32_v),\n  NEONMAP0(vcvtq_u64_v),\n  NEONMAP2(vdot_v, arm_neon_udot, arm_neon_sdot, 0),\n  NEONMAP2(vdotq_v, arm_neon_udot, arm_neon_sdot, 0),\n  NEONMAP0(vext_v),\n  NEONMAP0(vextq_v),\n  NEONMAP0(vfma_v),\n  NEONMAP0(vfmaq_v),\n  NEONMAP2(vhadd_v, arm_neon_vhaddu, arm_neon_vhadds, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vhaddq_v, arm_neon_vhaddu, arm_neon_vhadds, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vhsub_v, arm_neon_vhsubu, arm_neon_vhsubs, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vhsubq_v, arm_neon_vhsubu, arm_neon_vhsubs, Add1ArgType | UnsignedAlts),\n  NEONMAP0(vld1_dup_v),\n  NEONMAP1(vld1_v, arm_neon_vld1, 0),\n  NEONMAP1(vld1_x2_v, arm_neon_vld1x2, 0),\n  NEONMAP1(vld1_x3_v, arm_neon_vld1x3, 0),\n  NEONMAP1(vld1_x4_v, arm_neon_vld1x4, 0),\n  NEONMAP0(vld1q_dup_v),\n  NEONMAP1(vld1q_v, arm_neon_vld1, 0),\n  NEONMAP1(vld1q_x2_v, arm_neon_vld1x2, 0),\n  NEONMAP1(vld1q_x3_v, arm_neon_vld1x3, 0),\n  NEONMAP1(vld1q_x4_v, arm_neon_vld1x4, 0),\n  NEONMAP1(vld2_dup_v, arm_neon_vld2dup, 0),\n  NEONMAP1(vld2_lane_v, arm_neon_vld2lane, 0),\n  NEONMAP1(vld2_v, arm_neon_vld2, 0),\n  NEONMAP1(vld2q_dup_v, arm_neon_vld2dup, 0),\n  NEONMAP1(vld2q_lane_v, arm_neon_vld2lane, 0),\n  NEONMAP1(vld2q_v, arm_neon_vld2, 0),\n  NEONMAP1(vld3_dup_v, arm_neon_vld3dup, 0),\n  NEONMAP1(vld3_lane_v, arm_neon_vld3lane, 0),\n  NEONMAP1(vld3_v, arm_neon_vld3, 0),\n  NEONMAP1(vld3q_dup_v, arm_neon_vld3dup, 0),\n  NEONMAP1(vld3q_lane_v, arm_neon_vld3lane, 0),\n  NEONMAP1(vld3q_v, arm_neon_vld3, 0),\n  NEONMAP1(vld4_dup_v, arm_neon_vld4dup, 0),\n  NEONMAP1(vld4_lane_v, arm_neon_vld4lane, 0),\n  NEONMAP1(vld4_v, arm_neon_vld4, 0),\n  NEONMAP1(vld4q_dup_v, arm_neon_vld4dup, 0),\n  NEONMAP1(vld4q_lane_v, arm_neon_vld4lane, 0),\n  NEONMAP1(vld4q_v, arm_neon_vld4, 0),\n  NEONMAP2(vmax_v, arm_neon_vmaxu, arm_neon_vmaxs, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vmaxnm_v, arm_neon_vmaxnm, Add1ArgType),\n  NEONMAP1(vmaxnmq_v, arm_neon_vmaxnm, Add1ArgType),\n  NEONMAP2(vmaxq_v, arm_neon_vmaxu, arm_neon_vmaxs, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vmin_v, arm_neon_vminu, arm_neon_vmins, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vminnm_v, arm_neon_vminnm, Add1ArgType),\n  NEONMAP1(vminnmq_v, arm_neon_vminnm, Add1ArgType),\n  NEONMAP2(vminq_v, arm_neon_vminu, arm_neon_vmins, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vmmlaq_v, arm_neon_ummla, arm_neon_smmla, 0),\n  NEONMAP0(vmovl_v),\n  NEONMAP0(vmovn_v),\n  NEONMAP1(vmul_v, arm_neon_vmulp, Add1ArgType),\n  NEONMAP0(vmull_v),\n  NEONMAP1(vmulq_v, arm_neon_vmulp, Add1ArgType),\n  NEONMAP2(vpadal_v, arm_neon_vpadalu, arm_neon_vpadals, UnsignedAlts),\n  NEONMAP2(vpadalq_v, arm_neon_vpadalu, arm_neon_vpadals, UnsignedAlts),\n  NEONMAP1(vpadd_v, arm_neon_vpadd, Add1ArgType),\n  NEONMAP2(vpaddl_v, arm_neon_vpaddlu, arm_neon_vpaddls, UnsignedAlts),\n  NEONMAP2(vpaddlq_v, arm_neon_vpaddlu, arm_neon_vpaddls, UnsignedAlts),\n  NEONMAP1(vpaddq_v, arm_neon_vpadd, Add1ArgType),\n  NEONMAP2(vpmax_v, arm_neon_vpmaxu, arm_neon_vpmaxs, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vpmin_v, arm_neon_vpminu, arm_neon_vpmins, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vqabs_v, arm_neon_vqabs, Add1ArgType),\n  NEONMAP1(vqabsq_v, arm_neon_vqabs, Add1ArgType),\n  NEONMAP2(vqadd_v, uadd_sat, sadd_sat, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqaddq_v, uadd_sat, sadd_sat, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqdmlal_v, arm_neon_vqdmull, sadd_sat, 0),\n  NEONMAP2(vqdmlsl_v, arm_neon_vqdmull, ssub_sat, 0),\n  NEONMAP1(vqdmulh_v, arm_neon_vqdmulh, Add1ArgType),\n  NEONMAP1(vqdmulhq_v, arm_neon_vqdmulh, Add1ArgType),\n  NEONMAP1(vqdmull_v, arm_neon_vqdmull, Add1ArgType),\n  NEONMAP2(vqmovn_v, arm_neon_vqmovnu, arm_neon_vqmovns, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vqmovun_v, arm_neon_vqmovnsu, Add1ArgType),\n  NEONMAP1(vqneg_v, arm_neon_vqneg, Add1ArgType),\n  NEONMAP1(vqnegq_v, arm_neon_vqneg, Add1ArgType),\n  NEONMAP1(vqrdmulh_v, arm_neon_vqrdmulh, Add1ArgType),\n  NEONMAP1(vqrdmulhq_v, arm_neon_vqrdmulh, Add1ArgType),\n  NEONMAP2(vqrshl_v, arm_neon_vqrshiftu, arm_neon_vqrshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqrshlq_v, arm_neon_vqrshiftu, arm_neon_vqrshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqshl_n_v, arm_neon_vqshiftu, arm_neon_vqshifts, UnsignedAlts),\n  NEONMAP2(vqshl_v, arm_neon_vqshiftu, arm_neon_vqshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqshlq_n_v, arm_neon_vqshiftu, arm_neon_vqshifts, UnsignedAlts),\n  NEONMAP2(vqshlq_v, arm_neon_vqshiftu, arm_neon_vqshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vqshlu_n_v, arm_neon_vqshiftsu, 0),\n  NEONMAP1(vqshluq_n_v, arm_neon_vqshiftsu, 0),\n  NEONMAP2(vqsub_v, usub_sat, ssub_sat, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqsubq_v, usub_sat, ssub_sat, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vraddhn_v, arm_neon_vraddhn, Add1ArgType),\n  NEONMAP2(vrecpe_v, arm_neon_vrecpe, arm_neon_vrecpe, 0),\n  NEONMAP2(vrecpeq_v, arm_neon_vrecpe, arm_neon_vrecpe, 0),\n  NEONMAP1(vrecps_v, arm_neon_vrecps, Add1ArgType),\n  NEONMAP1(vrecpsq_v, arm_neon_vrecps, Add1ArgType),\n  NEONMAP2(vrhadd_v, arm_neon_vrhaddu, arm_neon_vrhadds, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vrhaddq_v, arm_neon_vrhaddu, arm_neon_vrhadds, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vrnd_v, arm_neon_vrintz, Add1ArgType),\n  NEONMAP1(vrnda_v, arm_neon_vrinta, Add1ArgType),\n  NEONMAP1(vrndaq_v, arm_neon_vrinta, Add1ArgType),\n  NEONMAP0(vrndi_v),\n  NEONMAP0(vrndiq_v),\n  NEONMAP1(vrndm_v, arm_neon_vrintm, Add1ArgType),\n  NEONMAP1(vrndmq_v, arm_neon_vrintm, Add1ArgType),\n  NEONMAP1(vrndn_v, arm_neon_vrintn, Add1ArgType),\n  NEONMAP1(vrndnq_v, arm_neon_vrintn, Add1ArgType),\n  NEONMAP1(vrndp_v, arm_neon_vrintp, Add1ArgType),\n  NEONMAP1(vrndpq_v, arm_neon_vrintp, Add1ArgType),\n  NEONMAP1(vrndq_v, arm_neon_vrintz, Add1ArgType),\n  NEONMAP1(vrndx_v, arm_neon_vrintx, Add1ArgType),\n  NEONMAP1(vrndxq_v, arm_neon_vrintx, Add1ArgType),\n  NEONMAP2(vrshl_v, arm_neon_vrshiftu, arm_neon_vrshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vrshlq_v, arm_neon_vrshiftu, arm_neon_vrshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vrshr_n_v, arm_neon_vrshiftu, arm_neon_vrshifts, UnsignedAlts),\n  NEONMAP2(vrshrq_n_v, arm_neon_vrshiftu, arm_neon_vrshifts, UnsignedAlts),\n  NEONMAP2(vrsqrte_v, arm_neon_vrsqrte, arm_neon_vrsqrte, 0),\n  NEONMAP2(vrsqrteq_v, arm_neon_vrsqrte, arm_neon_vrsqrte, 0),\n  NEONMAP1(vrsqrts_v, arm_neon_vrsqrts, Add1ArgType),\n  NEONMAP1(vrsqrtsq_v, arm_neon_vrsqrts, Add1ArgType),\n  NEONMAP1(vrsubhn_v, arm_neon_vrsubhn, Add1ArgType),\n  NEONMAP1(vsha1su0q_v, arm_neon_sha1su0, 0),\n  NEONMAP1(vsha1su1q_v, arm_neon_sha1su1, 0),\n  NEONMAP1(vsha256h2q_v, arm_neon_sha256h2, 0),\n  NEONMAP1(vsha256hq_v, arm_neon_sha256h, 0),\n  NEONMAP1(vsha256su0q_v, arm_neon_sha256su0, 0),\n  NEONMAP1(vsha256su1q_v, arm_neon_sha256su1, 0),\n  NEONMAP0(vshl_n_v),\n  NEONMAP2(vshl_v, arm_neon_vshiftu, arm_neon_vshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP0(vshll_n_v),\n  NEONMAP0(vshlq_n_v),\n  NEONMAP2(vshlq_v, arm_neon_vshiftu, arm_neon_vshifts, Add1ArgType | UnsignedAlts),\n  NEONMAP0(vshr_n_v),\n  NEONMAP0(vshrn_n_v),\n  NEONMAP0(vshrq_n_v),\n  NEONMAP1(vst1_v, arm_neon_vst1, 0),\n  NEONMAP1(vst1_x2_v, arm_neon_vst1x2, 0),\n  NEONMAP1(vst1_x3_v, arm_neon_vst1x3, 0),\n  NEONMAP1(vst1_x4_v, arm_neon_vst1x4, 0),\n  NEONMAP1(vst1q_v, arm_neon_vst1, 0),\n  NEONMAP1(vst1q_x2_v, arm_neon_vst1x2, 0),\n  NEONMAP1(vst1q_x3_v, arm_neon_vst1x3, 0),\n  NEONMAP1(vst1q_x4_v, arm_neon_vst1x4, 0),\n  NEONMAP1(vst2_lane_v, arm_neon_vst2lane, 0),\n  NEONMAP1(vst2_v, arm_neon_vst2, 0),\n  NEONMAP1(vst2q_lane_v, arm_neon_vst2lane, 0),\n  NEONMAP1(vst2q_v, arm_neon_vst2, 0),\n  NEONMAP1(vst3_lane_v, arm_neon_vst3lane, 0),\n  NEONMAP1(vst3_v, arm_neon_vst3, 0),\n  NEONMAP1(vst3q_lane_v, arm_neon_vst3lane, 0),\n  NEONMAP1(vst3q_v, arm_neon_vst3, 0),\n  NEONMAP1(vst4_lane_v, arm_neon_vst4lane, 0),\n  NEONMAP1(vst4_v, arm_neon_vst4, 0),\n  NEONMAP1(vst4q_lane_v, arm_neon_vst4lane, 0),\n  NEONMAP1(vst4q_v, arm_neon_vst4, 0),\n  NEONMAP0(vsubhn_v),\n  NEONMAP0(vtrn_v),\n  NEONMAP0(vtrnq_v),\n  NEONMAP0(vtst_v),\n  NEONMAP0(vtstq_v),\n  NEONMAP1(vusdot_v, arm_neon_usdot, 0),\n  NEONMAP1(vusdotq_v, arm_neon_usdot, 0),\n  NEONMAP1(vusmmlaq_v, arm_neon_usmmla, 0),\n  NEONMAP0(vuzp_v),\n  NEONMAP0(vuzpq_v),\n  NEONMAP0(vzip_v),\n  NEONMAP0(vzipq_v)\n};\n\nstatic const ARMVectorIntrinsicInfo AArch64SIMDIntrinsicMap[] = {\n  NEONMAP1(__a64_vcvtq_low_bf16_v, aarch64_neon_bfcvtn, 0),\n  NEONMAP0(splat_lane_v),\n  NEONMAP0(splat_laneq_v),\n  NEONMAP0(splatq_lane_v),\n  NEONMAP0(splatq_laneq_v),\n  NEONMAP1(vabs_v, aarch64_neon_abs, 0),\n  NEONMAP1(vabsq_v, aarch64_neon_abs, 0),\n  NEONMAP0(vaddhn_v),\n  NEONMAP1(vaesdq_v, aarch64_crypto_aesd, 0),\n  NEONMAP1(vaeseq_v, aarch64_crypto_aese, 0),\n  NEONMAP1(vaesimcq_v, aarch64_crypto_aesimc, 0),\n  NEONMAP1(vaesmcq_v, aarch64_crypto_aesmc, 0),\n  NEONMAP1(vbfdot_v, aarch64_neon_bfdot, 0),\n  NEONMAP1(vbfdotq_v, aarch64_neon_bfdot, 0),\n  NEONMAP1(vbfmlalbq_v, aarch64_neon_bfmlalb, 0),\n  NEONMAP1(vbfmlaltq_v, aarch64_neon_bfmlalt, 0),\n  NEONMAP1(vbfmmlaq_v, aarch64_neon_bfmmla, 0),\n  NEONMAP1(vcadd_rot270_v, aarch64_neon_vcadd_rot270, Add1ArgType),\n  NEONMAP1(vcadd_rot90_v, aarch64_neon_vcadd_rot90, Add1ArgType),\n  NEONMAP1(vcaddq_rot270_v, aarch64_neon_vcadd_rot270, Add1ArgType),\n  NEONMAP1(vcaddq_rot90_v, aarch64_neon_vcadd_rot90, Add1ArgType),\n  NEONMAP1(vcage_v, aarch64_neon_facge, 0),\n  NEONMAP1(vcageq_v, aarch64_neon_facge, 0),\n  NEONMAP1(vcagt_v, aarch64_neon_facgt, 0),\n  NEONMAP1(vcagtq_v, aarch64_neon_facgt, 0),\n  NEONMAP1(vcale_v, aarch64_neon_facge, 0),\n  NEONMAP1(vcaleq_v, aarch64_neon_facge, 0),\n  NEONMAP1(vcalt_v, aarch64_neon_facgt, 0),\n  NEONMAP1(vcaltq_v, aarch64_neon_facgt, 0),\n  NEONMAP0(vceqz_v),\n  NEONMAP0(vceqzq_v),\n  NEONMAP0(vcgez_v),\n  NEONMAP0(vcgezq_v),\n  NEONMAP0(vcgtz_v),\n  NEONMAP0(vcgtzq_v),\n  NEONMAP0(vclez_v),\n  NEONMAP0(vclezq_v),\n  NEONMAP1(vcls_v, aarch64_neon_cls, Add1ArgType),\n  NEONMAP1(vclsq_v, aarch64_neon_cls, Add1ArgType),\n  NEONMAP0(vcltz_v),\n  NEONMAP0(vcltzq_v),\n  NEONMAP1(vclz_v, ctlz, Add1ArgType),\n  NEONMAP1(vclzq_v, ctlz, Add1ArgType),\n  NEONMAP1(vcmla_rot180_v, aarch64_neon_vcmla_rot180, Add1ArgType),\n  NEONMAP1(vcmla_rot270_v, aarch64_neon_vcmla_rot270, Add1ArgType),\n  NEONMAP1(vcmla_rot90_v, aarch64_neon_vcmla_rot90, Add1ArgType),\n  NEONMAP1(vcmla_v, aarch64_neon_vcmla_rot0, Add1ArgType),\n  NEONMAP1(vcmlaq_rot180_v, aarch64_neon_vcmla_rot180, Add1ArgType),\n  NEONMAP1(vcmlaq_rot270_v, aarch64_neon_vcmla_rot270, Add1ArgType),\n  NEONMAP1(vcmlaq_rot90_v, aarch64_neon_vcmla_rot90, Add1ArgType),\n  NEONMAP1(vcmlaq_v, aarch64_neon_vcmla_rot0, Add1ArgType),\n  NEONMAP1(vcnt_v, ctpop, Add1ArgType),\n  NEONMAP1(vcntq_v, ctpop, Add1ArgType),\n  NEONMAP1(vcvt_f16_f32, aarch64_neon_vcvtfp2hf, 0),\n  NEONMAP0(vcvt_f16_v),\n  NEONMAP1(vcvt_f32_f16, aarch64_neon_vcvthf2fp, 0),\n  NEONMAP0(vcvt_f32_v),\n  NEONMAP2(vcvt_n_f16_v, aarch64_neon_vcvtfxu2fp, aarch64_neon_vcvtfxs2fp, 0),\n  NEONMAP2(vcvt_n_f32_v, aarch64_neon_vcvtfxu2fp, aarch64_neon_vcvtfxs2fp, 0),\n  NEONMAP2(vcvt_n_f64_v, aarch64_neon_vcvtfxu2fp, aarch64_neon_vcvtfxs2fp, 0),\n  NEONMAP1(vcvt_n_s16_v, aarch64_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvt_n_s32_v, aarch64_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvt_n_s64_v, aarch64_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvt_n_u16_v, aarch64_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvt_n_u32_v, aarch64_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvt_n_u64_v, aarch64_neon_vcvtfp2fxu, 0),\n  NEONMAP0(vcvtq_f16_v),\n  NEONMAP0(vcvtq_f32_v),\n  NEONMAP1(vcvtq_high_bf16_v, aarch64_neon_bfcvtn2, 0),\n  NEONMAP2(vcvtq_n_f16_v, aarch64_neon_vcvtfxu2fp, aarch64_neon_vcvtfxs2fp, 0),\n  NEONMAP2(vcvtq_n_f32_v, aarch64_neon_vcvtfxu2fp, aarch64_neon_vcvtfxs2fp, 0),\n  NEONMAP2(vcvtq_n_f64_v, aarch64_neon_vcvtfxu2fp, aarch64_neon_vcvtfxs2fp, 0),\n  NEONMAP1(vcvtq_n_s16_v, aarch64_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvtq_n_s32_v, aarch64_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvtq_n_s64_v, aarch64_neon_vcvtfp2fxs, 0),\n  NEONMAP1(vcvtq_n_u16_v, aarch64_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvtq_n_u32_v, aarch64_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvtq_n_u64_v, aarch64_neon_vcvtfp2fxu, 0),\n  NEONMAP1(vcvtx_f32_v, aarch64_neon_fcvtxn, AddRetType | Add1ArgType),\n  NEONMAP2(vdot_v, aarch64_neon_udot, aarch64_neon_sdot, 0),\n  NEONMAP2(vdotq_v, aarch64_neon_udot, aarch64_neon_sdot, 0),\n  NEONMAP0(vext_v),\n  NEONMAP0(vextq_v),\n  NEONMAP0(vfma_v),\n  NEONMAP0(vfmaq_v),\n  NEONMAP1(vfmlal_high_v, aarch64_neon_fmlal2, 0),\n  NEONMAP1(vfmlal_low_v, aarch64_neon_fmlal, 0),\n  NEONMAP1(vfmlalq_high_v, aarch64_neon_fmlal2, 0),\n  NEONMAP1(vfmlalq_low_v, aarch64_neon_fmlal, 0),\n  NEONMAP1(vfmlsl_high_v, aarch64_neon_fmlsl2, 0),\n  NEONMAP1(vfmlsl_low_v, aarch64_neon_fmlsl, 0),\n  NEONMAP1(vfmlslq_high_v, aarch64_neon_fmlsl2, 0),\n  NEONMAP1(vfmlslq_low_v, aarch64_neon_fmlsl, 0),\n  NEONMAP2(vhadd_v, aarch64_neon_uhadd, aarch64_neon_shadd, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vhaddq_v, aarch64_neon_uhadd, aarch64_neon_shadd, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vhsub_v, aarch64_neon_uhsub, aarch64_neon_shsub, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vhsubq_v, aarch64_neon_uhsub, aarch64_neon_shsub, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vld1_x2_v, aarch64_neon_ld1x2, 0),\n  NEONMAP1(vld1_x3_v, aarch64_neon_ld1x3, 0),\n  NEONMAP1(vld1_x4_v, aarch64_neon_ld1x4, 0),\n  NEONMAP1(vld1q_x2_v, aarch64_neon_ld1x2, 0),\n  NEONMAP1(vld1q_x3_v, aarch64_neon_ld1x3, 0),\n  NEONMAP1(vld1q_x4_v, aarch64_neon_ld1x4, 0),\n  NEONMAP2(vmmlaq_v, aarch64_neon_ummla, aarch64_neon_smmla, 0),\n  NEONMAP0(vmovl_v),\n  NEONMAP0(vmovn_v),\n  NEONMAP1(vmul_v, aarch64_neon_pmul, Add1ArgType),\n  NEONMAP1(vmulq_v, aarch64_neon_pmul, Add1ArgType),\n  NEONMAP1(vpadd_v, aarch64_neon_addp, Add1ArgType),\n  NEONMAP2(vpaddl_v, aarch64_neon_uaddlp, aarch64_neon_saddlp, UnsignedAlts),\n  NEONMAP2(vpaddlq_v, aarch64_neon_uaddlp, aarch64_neon_saddlp, UnsignedAlts),\n  NEONMAP1(vpaddq_v, aarch64_neon_addp, Add1ArgType),\n  NEONMAP1(vqabs_v, aarch64_neon_sqabs, Add1ArgType),\n  NEONMAP1(vqabsq_v, aarch64_neon_sqabs, Add1ArgType),\n  NEONMAP2(vqadd_v, aarch64_neon_uqadd, aarch64_neon_sqadd, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqaddq_v, aarch64_neon_uqadd, aarch64_neon_sqadd, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqdmlal_v, aarch64_neon_sqdmull, aarch64_neon_sqadd, 0),\n  NEONMAP2(vqdmlsl_v, aarch64_neon_sqdmull, aarch64_neon_sqsub, 0),\n  NEONMAP1(vqdmulh_lane_v, aarch64_neon_sqdmulh_lane, 0),\n  NEONMAP1(vqdmulh_laneq_v, aarch64_neon_sqdmulh_laneq, 0),\n  NEONMAP1(vqdmulh_v, aarch64_neon_sqdmulh, Add1ArgType),\n  NEONMAP1(vqdmulhq_lane_v, aarch64_neon_sqdmulh_lane, 0),\n  NEONMAP1(vqdmulhq_laneq_v, aarch64_neon_sqdmulh_laneq, 0),\n  NEONMAP1(vqdmulhq_v, aarch64_neon_sqdmulh, Add1ArgType),\n  NEONMAP1(vqdmull_v, aarch64_neon_sqdmull, Add1ArgType),\n  NEONMAP2(vqmovn_v, aarch64_neon_uqxtn, aarch64_neon_sqxtn, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vqmovun_v, aarch64_neon_sqxtun, Add1ArgType),\n  NEONMAP1(vqneg_v, aarch64_neon_sqneg, Add1ArgType),\n  NEONMAP1(vqnegq_v, aarch64_neon_sqneg, Add1ArgType),\n  NEONMAP1(vqrdmulh_lane_v, aarch64_neon_sqrdmulh_lane, 0),\n  NEONMAP1(vqrdmulh_laneq_v, aarch64_neon_sqrdmulh_laneq, 0),\n  NEONMAP1(vqrdmulh_v, aarch64_neon_sqrdmulh, Add1ArgType),\n  NEONMAP1(vqrdmulhq_lane_v, aarch64_neon_sqrdmulh_lane, 0),\n  NEONMAP1(vqrdmulhq_laneq_v, aarch64_neon_sqrdmulh_laneq, 0),\n  NEONMAP1(vqrdmulhq_v, aarch64_neon_sqrdmulh, Add1ArgType),\n  NEONMAP2(vqrshl_v, aarch64_neon_uqrshl, aarch64_neon_sqrshl, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqrshlq_v, aarch64_neon_uqrshl, aarch64_neon_sqrshl, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqshl_n_v, aarch64_neon_uqshl, aarch64_neon_sqshl, UnsignedAlts),\n  NEONMAP2(vqshl_v, aarch64_neon_uqshl, aarch64_neon_sqshl, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqshlq_n_v, aarch64_neon_uqshl, aarch64_neon_sqshl,UnsignedAlts),\n  NEONMAP2(vqshlq_v, aarch64_neon_uqshl, aarch64_neon_sqshl, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vqshlu_n_v, aarch64_neon_sqshlu, 0),\n  NEONMAP1(vqshluq_n_v, aarch64_neon_sqshlu, 0),\n  NEONMAP2(vqsub_v, aarch64_neon_uqsub, aarch64_neon_sqsub, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vqsubq_v, aarch64_neon_uqsub, aarch64_neon_sqsub, Add1ArgType | UnsignedAlts),\n  NEONMAP1(vraddhn_v, aarch64_neon_raddhn, Add1ArgType),\n  NEONMAP2(vrecpe_v, aarch64_neon_frecpe, aarch64_neon_urecpe, 0),\n  NEONMAP2(vrecpeq_v, aarch64_neon_frecpe, aarch64_neon_urecpe, 0),\n  NEONMAP1(vrecps_v, aarch64_neon_frecps, Add1ArgType),\n  NEONMAP1(vrecpsq_v, aarch64_neon_frecps, Add1ArgType),\n  NEONMAP2(vrhadd_v, aarch64_neon_urhadd, aarch64_neon_srhadd, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vrhaddq_v, aarch64_neon_urhadd, aarch64_neon_srhadd, Add1ArgType | UnsignedAlts),\n  NEONMAP0(vrndi_v),\n  NEONMAP0(vrndiq_v),\n  NEONMAP2(vrshl_v, aarch64_neon_urshl, aarch64_neon_srshl, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vrshlq_v, aarch64_neon_urshl, aarch64_neon_srshl, Add1ArgType | UnsignedAlts),\n  NEONMAP2(vrshr_n_v, aarch64_neon_urshl, aarch64_neon_srshl, UnsignedAlts),\n  NEONMAP2(vrshrq_n_v, aarch64_neon_urshl, aarch64_neon_srshl, UnsignedAlts),\n  NEONMAP2(vrsqrte_v, aarch64_neon_frsqrte, aarch64_neon_ursqrte, 0),\n  NEONMAP2(vrsqrteq_v, aarch64_neon_frsqrte, aarch64_neon_ursqrte, 0),\n  NEONMAP1(vrsqrts_v, aarch64_neon_frsqrts, Add1ArgType),\n  NEONMAP1(vrsqrtsq_v, aarch64_neon_frsqrts, Add1ArgType),\n  NEONMAP1(vrsubhn_v, aarch64_neon_rsubhn, Add1ArgType),\n  NEONMAP1(vsha1su0q_v, aarch64_crypto_sha1su0, 0),\n  NEONMAP1(vsha1su1q_v, aarch64_crypto_sha1su1, 0),\n  NEONMAP1(vsha256h2q_v, aarch64_crypto_sha256h2, 0),\n  NEONMAP1(vsha256hq_v, aarch64_crypto_sha256h, 0),\n  NEONMAP1(vsha256su0q_v, aarch64_crypto_sha256su0, 0),\n  NEONMAP1(vsha256su1q_v, aarch64_crypto_sha256su1, 0),\n  NEONMAP0(vshl_n_v),\n  NEONMAP2(vshl_v, aarch64_neon_ushl, aarch64_neon_sshl, Add1ArgType | UnsignedAlts),\n  NEONMAP0(vshll_n_v),\n  NEONMAP0(vshlq_n_v),\n  NEONMAP2(vshlq_v, aarch64_neon_ushl, aarch64_neon_sshl, Add1ArgType | UnsignedAlts),\n  NEONMAP0(vshr_n_v),\n  NEONMAP0(vshrn_n_v),\n  NEONMAP0(vshrq_n_v),\n  NEONMAP1(vst1_x2_v, aarch64_neon_st1x2, 0),\n  NEONMAP1(vst1_x3_v, aarch64_neon_st1x3, 0),\n  NEONMAP1(vst1_x4_v, aarch64_neon_st1x4, 0),\n  NEONMAP1(vst1q_x2_v, aarch64_neon_st1x2, 0),\n  NEONMAP1(vst1q_x3_v, aarch64_neon_st1x3, 0),\n  NEONMAP1(vst1q_x4_v, aarch64_neon_st1x4, 0),\n  NEONMAP0(vsubhn_v),\n  NEONMAP0(vtst_v),\n  NEONMAP0(vtstq_v),\n  NEONMAP1(vusdot_v, aarch64_neon_usdot, 0),\n  NEONMAP1(vusdotq_v, aarch64_neon_usdot, 0),\n  NEONMAP1(vusmmlaq_v, aarch64_neon_usmmla, 0),\n};\n\nstatic const ARMVectorIntrinsicInfo AArch64SISDIntrinsicMap[] = {\n  NEONMAP1(vabdd_f64, aarch64_sisd_fabd, Add1ArgType),\n  NEONMAP1(vabds_f32, aarch64_sisd_fabd, Add1ArgType),\n  NEONMAP1(vabsd_s64, aarch64_neon_abs, Add1ArgType),\n  NEONMAP1(vaddlv_s32, aarch64_neon_saddlv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddlv_u32, aarch64_neon_uaddlv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddlvq_s32, aarch64_neon_saddlv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddlvq_u32, aarch64_neon_uaddlv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddv_f32, aarch64_neon_faddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddv_s32, aarch64_neon_saddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddv_u32, aarch64_neon_uaddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddvq_f32, aarch64_neon_faddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddvq_f64, aarch64_neon_faddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddvq_s32, aarch64_neon_saddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddvq_s64, aarch64_neon_saddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddvq_u32, aarch64_neon_uaddv, AddRetType | Add1ArgType),\n  NEONMAP1(vaddvq_u64, aarch64_neon_uaddv, AddRetType | Add1ArgType),\n  NEONMAP1(vcaged_f64, aarch64_neon_facge, AddRetType | Add1ArgType),\n  NEONMAP1(vcages_f32, aarch64_neon_facge, AddRetType | Add1ArgType),\n  NEONMAP1(vcagtd_f64, aarch64_neon_facgt, AddRetType | Add1ArgType),\n  NEONMAP1(vcagts_f32, aarch64_neon_facgt, AddRetType | Add1ArgType),\n  NEONMAP1(vcaled_f64, aarch64_neon_facge, AddRetType | Add1ArgType),\n  NEONMAP1(vcales_f32, aarch64_neon_facge, AddRetType | Add1ArgType),\n  NEONMAP1(vcaltd_f64, aarch64_neon_facgt, AddRetType | Add1ArgType),\n  NEONMAP1(vcalts_f32, aarch64_neon_facgt, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtad_s64_f64, aarch64_neon_fcvtas, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtad_u64_f64, aarch64_neon_fcvtau, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtas_s32_f32, aarch64_neon_fcvtas, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtas_u32_f32, aarch64_neon_fcvtau, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtd_n_f64_s64, aarch64_neon_vcvtfxs2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtd_n_f64_u64, aarch64_neon_vcvtfxu2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtd_n_s64_f64, aarch64_neon_vcvtfp2fxs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtd_n_u64_f64, aarch64_neon_vcvtfp2fxu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtd_s64_f64, aarch64_neon_fcvtzs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtd_u64_f64, aarch64_neon_fcvtzu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_bf16_f32, aarch64_neon_bfcvt, 0),\n  NEONMAP1(vcvtmd_s64_f64, aarch64_neon_fcvtms, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtmd_u64_f64, aarch64_neon_fcvtmu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtms_s32_f32, aarch64_neon_fcvtms, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtms_u32_f32, aarch64_neon_fcvtmu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtnd_s64_f64, aarch64_neon_fcvtns, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtnd_u64_f64, aarch64_neon_fcvtnu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtns_s32_f32, aarch64_neon_fcvtns, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtns_u32_f32, aarch64_neon_fcvtnu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtpd_s64_f64, aarch64_neon_fcvtps, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtpd_u64_f64, aarch64_neon_fcvtpu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtps_s32_f32, aarch64_neon_fcvtps, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtps_u32_f32, aarch64_neon_fcvtpu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvts_n_f32_s32, aarch64_neon_vcvtfxs2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvts_n_f32_u32, aarch64_neon_vcvtfxu2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvts_n_s32_f32, aarch64_neon_vcvtfp2fxs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvts_n_u32_f32, aarch64_neon_vcvtfp2fxu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvts_s32_f32, aarch64_neon_fcvtzs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvts_u32_f32, aarch64_neon_fcvtzu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtxd_f32_f64, aarch64_sisd_fcvtxn, 0),\n  NEONMAP1(vmaxnmv_f32, aarch64_neon_fmaxnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxnmvq_f32, aarch64_neon_fmaxnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxnmvq_f64, aarch64_neon_fmaxnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxv_f32, aarch64_neon_fmaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxv_s32, aarch64_neon_smaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxv_u32, aarch64_neon_umaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxvq_f32, aarch64_neon_fmaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxvq_f64, aarch64_neon_fmaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxvq_s32, aarch64_neon_smaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vmaxvq_u32, aarch64_neon_umaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vminnmv_f32, aarch64_neon_fminnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vminnmvq_f32, aarch64_neon_fminnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vminnmvq_f64, aarch64_neon_fminnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vminv_f32, aarch64_neon_fminv, AddRetType | Add1ArgType),\n  NEONMAP1(vminv_s32, aarch64_neon_sminv, AddRetType | Add1ArgType),\n  NEONMAP1(vminv_u32, aarch64_neon_uminv, AddRetType | Add1ArgType),\n  NEONMAP1(vminvq_f32, aarch64_neon_fminv, AddRetType | Add1ArgType),\n  NEONMAP1(vminvq_f64, aarch64_neon_fminv, AddRetType | Add1ArgType),\n  NEONMAP1(vminvq_s32, aarch64_neon_sminv, AddRetType | Add1ArgType),\n  NEONMAP1(vminvq_u32, aarch64_neon_uminv, AddRetType | Add1ArgType),\n  NEONMAP1(vmull_p64, aarch64_neon_pmull64, 0),\n  NEONMAP1(vmulxd_f64, aarch64_neon_fmulx, Add1ArgType),\n  NEONMAP1(vmulxs_f32, aarch64_neon_fmulx, Add1ArgType),\n  NEONMAP1(vpaddd_s64, aarch64_neon_uaddv, AddRetType | Add1ArgType),\n  NEONMAP1(vpaddd_u64, aarch64_neon_uaddv, AddRetType | Add1ArgType),\n  NEONMAP1(vpmaxnmqd_f64, aarch64_neon_fmaxnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vpmaxnms_f32, aarch64_neon_fmaxnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vpmaxqd_f64, aarch64_neon_fmaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vpmaxs_f32, aarch64_neon_fmaxv, AddRetType | Add1ArgType),\n  NEONMAP1(vpminnmqd_f64, aarch64_neon_fminnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vpminnms_f32, aarch64_neon_fminnmv, AddRetType | Add1ArgType),\n  NEONMAP1(vpminqd_f64, aarch64_neon_fminv, AddRetType | Add1ArgType),\n  NEONMAP1(vpmins_f32, aarch64_neon_fminv, AddRetType | Add1ArgType),\n  NEONMAP1(vqabsb_s8, aarch64_neon_sqabs, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqabsd_s64, aarch64_neon_sqabs, Add1ArgType),\n  NEONMAP1(vqabsh_s16, aarch64_neon_sqabs, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqabss_s32, aarch64_neon_sqabs, Add1ArgType),\n  NEONMAP1(vqaddb_s8, aarch64_neon_sqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqaddb_u8, aarch64_neon_uqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqaddd_s64, aarch64_neon_sqadd, Add1ArgType),\n  NEONMAP1(vqaddd_u64, aarch64_neon_uqadd, Add1ArgType),\n  NEONMAP1(vqaddh_s16, aarch64_neon_sqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqaddh_u16, aarch64_neon_uqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqadds_s32, aarch64_neon_sqadd, Add1ArgType),\n  NEONMAP1(vqadds_u32, aarch64_neon_uqadd, Add1ArgType),\n  NEONMAP1(vqdmulhh_s16, aarch64_neon_sqdmulh, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqdmulhs_s32, aarch64_neon_sqdmulh, Add1ArgType),\n  NEONMAP1(vqdmullh_s16, aarch64_neon_sqdmull, VectorRet | Use128BitVectors),\n  NEONMAP1(vqdmulls_s32, aarch64_neon_sqdmulls_scalar, 0),\n  NEONMAP1(vqmovnd_s64, aarch64_neon_scalar_sqxtn, AddRetType | Add1ArgType),\n  NEONMAP1(vqmovnd_u64, aarch64_neon_scalar_uqxtn, AddRetType | Add1ArgType),\n  NEONMAP1(vqmovnh_s16, aarch64_neon_sqxtn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqmovnh_u16, aarch64_neon_uqxtn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqmovns_s32, aarch64_neon_sqxtn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqmovns_u32, aarch64_neon_uqxtn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqmovund_s64, aarch64_neon_scalar_sqxtun, AddRetType | Add1ArgType),\n  NEONMAP1(vqmovunh_s16, aarch64_neon_sqxtun, VectorRet | Use64BitVectors),\n  NEONMAP1(vqmovuns_s32, aarch64_neon_sqxtun, VectorRet | Use64BitVectors),\n  NEONMAP1(vqnegb_s8, aarch64_neon_sqneg, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqnegd_s64, aarch64_neon_sqneg, Add1ArgType),\n  NEONMAP1(vqnegh_s16, aarch64_neon_sqneg, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqnegs_s32, aarch64_neon_sqneg, Add1ArgType),\n  NEONMAP1(vqrdmulhh_s16, aarch64_neon_sqrdmulh, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqrdmulhs_s32, aarch64_neon_sqrdmulh, Add1ArgType),\n  NEONMAP1(vqrshlb_s8, aarch64_neon_sqrshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqrshlb_u8, aarch64_neon_uqrshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqrshld_s64, aarch64_neon_sqrshl, Add1ArgType),\n  NEONMAP1(vqrshld_u64, aarch64_neon_uqrshl, Add1ArgType),\n  NEONMAP1(vqrshlh_s16, aarch64_neon_sqrshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqrshlh_u16, aarch64_neon_uqrshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqrshls_s32, aarch64_neon_sqrshl, Add1ArgType),\n  NEONMAP1(vqrshls_u32, aarch64_neon_uqrshl, Add1ArgType),\n  NEONMAP1(vqrshrnd_n_s64, aarch64_neon_sqrshrn, AddRetType),\n  NEONMAP1(vqrshrnd_n_u64, aarch64_neon_uqrshrn, AddRetType),\n  NEONMAP1(vqrshrnh_n_s16, aarch64_neon_sqrshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqrshrnh_n_u16, aarch64_neon_uqrshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqrshrns_n_s32, aarch64_neon_sqrshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqrshrns_n_u32, aarch64_neon_uqrshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqrshrund_n_s64, aarch64_neon_sqrshrun, AddRetType),\n  NEONMAP1(vqrshrunh_n_s16, aarch64_neon_sqrshrun, VectorRet | Use64BitVectors),\n  NEONMAP1(vqrshruns_n_s32, aarch64_neon_sqrshrun, VectorRet | Use64BitVectors),\n  NEONMAP1(vqshlb_n_s8, aarch64_neon_sqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshlb_n_u8, aarch64_neon_uqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshlb_s8, aarch64_neon_sqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshlb_u8, aarch64_neon_uqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshld_s64, aarch64_neon_sqshl, Add1ArgType),\n  NEONMAP1(vqshld_u64, aarch64_neon_uqshl, Add1ArgType),\n  NEONMAP1(vqshlh_n_s16, aarch64_neon_sqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshlh_n_u16, aarch64_neon_uqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshlh_s16, aarch64_neon_sqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshlh_u16, aarch64_neon_uqshl, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshls_n_s32, aarch64_neon_sqshl, Add1ArgType),\n  NEONMAP1(vqshls_n_u32, aarch64_neon_uqshl, Add1ArgType),\n  NEONMAP1(vqshls_s32, aarch64_neon_sqshl, Add1ArgType),\n  NEONMAP1(vqshls_u32, aarch64_neon_uqshl, Add1ArgType),\n  NEONMAP1(vqshlub_n_s8, aarch64_neon_sqshlu, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshluh_n_s16, aarch64_neon_sqshlu, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqshlus_n_s32, aarch64_neon_sqshlu, Add1ArgType),\n  NEONMAP1(vqshrnd_n_s64, aarch64_neon_sqshrn, AddRetType),\n  NEONMAP1(vqshrnd_n_u64, aarch64_neon_uqshrn, AddRetType),\n  NEONMAP1(vqshrnh_n_s16, aarch64_neon_sqshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqshrnh_n_u16, aarch64_neon_uqshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqshrns_n_s32, aarch64_neon_sqshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqshrns_n_u32, aarch64_neon_uqshrn, VectorRet | Use64BitVectors),\n  NEONMAP1(vqshrund_n_s64, aarch64_neon_sqshrun, AddRetType),\n  NEONMAP1(vqshrunh_n_s16, aarch64_neon_sqshrun, VectorRet | Use64BitVectors),\n  NEONMAP1(vqshruns_n_s32, aarch64_neon_sqshrun, VectorRet | Use64BitVectors),\n  NEONMAP1(vqsubb_s8, aarch64_neon_sqsub, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqsubb_u8, aarch64_neon_uqsub, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqsubd_s64, aarch64_neon_sqsub, Add1ArgType),\n  NEONMAP1(vqsubd_u64, aarch64_neon_uqsub, Add1ArgType),\n  NEONMAP1(vqsubh_s16, aarch64_neon_sqsub, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqsubh_u16, aarch64_neon_uqsub, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vqsubs_s32, aarch64_neon_sqsub, Add1ArgType),\n  NEONMAP1(vqsubs_u32, aarch64_neon_uqsub, Add1ArgType),\n  NEONMAP1(vrecped_f64, aarch64_neon_frecpe, Add1ArgType),\n  NEONMAP1(vrecpes_f32, aarch64_neon_frecpe, Add1ArgType),\n  NEONMAP1(vrecpxd_f64, aarch64_neon_frecpx, Add1ArgType),\n  NEONMAP1(vrecpxs_f32, aarch64_neon_frecpx, Add1ArgType),\n  NEONMAP1(vrshld_s64, aarch64_neon_srshl, Add1ArgType),\n  NEONMAP1(vrshld_u64, aarch64_neon_urshl, Add1ArgType),\n  NEONMAP1(vrsqrted_f64, aarch64_neon_frsqrte, Add1ArgType),\n  NEONMAP1(vrsqrtes_f32, aarch64_neon_frsqrte, Add1ArgType),\n  NEONMAP1(vrsqrtsd_f64, aarch64_neon_frsqrts, Add1ArgType),\n  NEONMAP1(vrsqrtss_f32, aarch64_neon_frsqrts, Add1ArgType),\n  NEONMAP1(vsha1cq_u32, aarch64_crypto_sha1c, 0),\n  NEONMAP1(vsha1h_u32, aarch64_crypto_sha1h, 0),\n  NEONMAP1(vsha1mq_u32, aarch64_crypto_sha1m, 0),\n  NEONMAP1(vsha1pq_u32, aarch64_crypto_sha1p, 0),\n  NEONMAP1(vshld_s64, aarch64_neon_sshl, Add1ArgType),\n  NEONMAP1(vshld_u64, aarch64_neon_ushl, Add1ArgType),\n  NEONMAP1(vslid_n_s64, aarch64_neon_vsli, Vectorize1ArgType),\n  NEONMAP1(vslid_n_u64, aarch64_neon_vsli, Vectorize1ArgType),\n  NEONMAP1(vsqaddb_u8, aarch64_neon_usqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vsqaddd_u64, aarch64_neon_usqadd, Add1ArgType),\n  NEONMAP1(vsqaddh_u16, aarch64_neon_usqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vsqadds_u32, aarch64_neon_usqadd, Add1ArgType),\n  NEONMAP1(vsrid_n_s64, aarch64_neon_vsri, Vectorize1ArgType),\n  NEONMAP1(vsrid_n_u64, aarch64_neon_vsri, Vectorize1ArgType),\n  NEONMAP1(vuqaddb_s8, aarch64_neon_suqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vuqaddd_s64, aarch64_neon_suqadd, Add1ArgType),\n  NEONMAP1(vuqaddh_s16, aarch64_neon_suqadd, Vectorize1ArgType | Use64BitVectors),\n  NEONMAP1(vuqadds_s32, aarch64_neon_suqadd, Add1ArgType),\n  // FP16 scalar intrinisics go here.\n  NEONMAP1(vabdh_f16, aarch64_sisd_fabd, Add1ArgType),\n  NEONMAP1(vcvtah_s32_f16, aarch64_neon_fcvtas, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtah_s64_f16, aarch64_neon_fcvtas, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtah_u32_f16, aarch64_neon_fcvtau, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtah_u64_f16, aarch64_neon_fcvtau, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_f16_s32, aarch64_neon_vcvtfxs2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_f16_s64, aarch64_neon_vcvtfxs2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_f16_u32, aarch64_neon_vcvtfxu2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_f16_u64, aarch64_neon_vcvtfxu2fp, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_s32_f16, aarch64_neon_vcvtfp2fxs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_s64_f16, aarch64_neon_vcvtfp2fxs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_u32_f16, aarch64_neon_vcvtfp2fxu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_n_u64_f16, aarch64_neon_vcvtfp2fxu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_s32_f16, aarch64_neon_fcvtzs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_s64_f16, aarch64_neon_fcvtzs, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_u32_f16, aarch64_neon_fcvtzu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvth_u64_f16, aarch64_neon_fcvtzu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtmh_s32_f16, aarch64_neon_fcvtms, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtmh_s64_f16, aarch64_neon_fcvtms, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtmh_u32_f16, aarch64_neon_fcvtmu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtmh_u64_f16, aarch64_neon_fcvtmu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtnh_s32_f16, aarch64_neon_fcvtns, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtnh_s64_f16, aarch64_neon_fcvtns, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtnh_u32_f16, aarch64_neon_fcvtnu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtnh_u64_f16, aarch64_neon_fcvtnu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtph_s32_f16, aarch64_neon_fcvtps, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtph_s64_f16, aarch64_neon_fcvtps, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtph_u32_f16, aarch64_neon_fcvtpu, AddRetType | Add1ArgType),\n  NEONMAP1(vcvtph_u64_f16, aarch64_neon_fcvtpu, AddRetType | Add1ArgType),\n  NEONMAP1(vmulxh_f16, aarch64_neon_fmulx, Add1ArgType),\n  NEONMAP1(vrecpeh_f16, aarch64_neon_frecpe, Add1ArgType),\n  NEONMAP1(vrecpxh_f16, aarch64_neon_frecpx, Add1ArgType),\n  NEONMAP1(vrsqrteh_f16, aarch64_neon_frsqrte, Add1ArgType),\n  NEONMAP1(vrsqrtsh_f16, aarch64_neon_frsqrts, Add1ArgType),\n};\n\n#undef NEONMAP0\n#undef NEONMAP1\n#undef NEONMAP2\n\n#define SVEMAP1(NameBase, LLVMIntrinsic, TypeModifier)                         \\\n  {                                                                            \\\n    #NameBase, SVE::BI__builtin_sve_##NameBase, Intrinsic::LLVMIntrinsic, 0,   \\\n        TypeModifier                                                           \\\n  }\n\n#define SVEMAP2(NameBase, TypeModifier)                                        \\\n  { #NameBase, SVE::BI__builtin_sve_##NameBase, 0, 0, TypeModifier }\nstatic const ARMVectorIntrinsicInfo AArch64SVEIntrinsicMap[] = {\n#define GET_SVE_LLVM_INTRINSIC_MAP\n#include \"clang/Basic/arm_sve_builtin_cg.inc\"\n#undef GET_SVE_LLVM_INTRINSIC_MAP\n};\n\n#undef SVEMAP1\n#undef SVEMAP2\n\nstatic bool NEONSIMDIntrinsicsProvenSorted = false;\n\nstatic bool AArch64SIMDIntrinsicsProvenSorted = false;\nstatic bool AArch64SISDIntrinsicsProvenSorted = false;\nstatic bool AArch64SVEIntrinsicsProvenSorted = false;\n\nstatic const ARMVectorIntrinsicInfo *\nfindARMVectorIntrinsicInMap(ArrayRef<ARMVectorIntrinsicInfo> IntrinsicMap,\n                            unsigned BuiltinID, bool &MapProvenSorted) {\n\n#ifndef NDEBUG\n  if (!MapProvenSorted) {\n    assert(llvm::is_sorted(IntrinsicMap));\n    MapProvenSorted = true;\n  }\n#endif\n\n  const ARMVectorIntrinsicInfo *Builtin =\n      llvm::lower_bound(IntrinsicMap, BuiltinID);\n\n  if (Builtin != IntrinsicMap.end() && Builtin->BuiltinID == BuiltinID)\n    return Builtin;\n\n  return nullptr;\n}\n\nFunction *CodeGenFunction::LookupNeonLLVMIntrinsic(unsigned IntrinsicID,\n                                                   unsigned Modifier,\n                                                   llvm::Type *ArgType,\n                                                   const CallExpr *E) {\n  int VectorSize = 0;\n  if (Modifier & Use64BitVectors)\n    VectorSize = 64;\n  else if (Modifier & Use128BitVectors)\n    VectorSize = 128;\n\n  // Return type.\n  SmallVector<llvm::Type *, 3> Tys;\n  if (Modifier & AddRetType) {\n    llvm::Type *Ty = ConvertType(E->getCallReturnType(getContext()));\n    if (Modifier & VectorizeRetType)\n      Ty = llvm::FixedVectorType::get(\n          Ty, VectorSize ? VectorSize / Ty->getPrimitiveSizeInBits() : 1);\n\n    Tys.push_back(Ty);\n  }\n\n  // Arguments.\n  if (Modifier & VectorizeArgTypes) {\n    int Elts = VectorSize ? VectorSize / ArgType->getPrimitiveSizeInBits() : 1;\n    ArgType = llvm::FixedVectorType::get(ArgType, Elts);\n  }\n\n  if (Modifier & (Add1ArgType | Add2ArgTypes))\n    Tys.push_back(ArgType);\n\n  if (Modifier & Add2ArgTypes)\n    Tys.push_back(ArgType);\n\n  if (Modifier & InventFloatType)\n    Tys.push_back(FloatTy);\n\n  return CGM.getIntrinsic(IntrinsicID, Tys);\n}\n\nstatic Value *EmitCommonNeonSISDBuiltinExpr(\n    CodeGenFunction &CGF, const ARMVectorIntrinsicInfo &SISDInfo,\n    SmallVectorImpl<Value *> &Ops, const CallExpr *E) {\n  unsigned BuiltinID = SISDInfo.BuiltinID;\n  unsigned int Int = SISDInfo.LLVMIntrinsic;\n  unsigned Modifier = SISDInfo.TypeModifier;\n  const char *s = SISDInfo.NameHint;\n\n  switch (BuiltinID) {\n  case NEON::BI__builtin_neon_vcled_s64:\n  case NEON::BI__builtin_neon_vcled_u64:\n  case NEON::BI__builtin_neon_vcles_f32:\n  case NEON::BI__builtin_neon_vcled_f64:\n  case NEON::BI__builtin_neon_vcltd_s64:\n  case NEON::BI__builtin_neon_vcltd_u64:\n  case NEON::BI__builtin_neon_vclts_f32:\n  case NEON::BI__builtin_neon_vcltd_f64:\n  case NEON::BI__builtin_neon_vcales_f32:\n  case NEON::BI__builtin_neon_vcaled_f64:\n  case NEON::BI__builtin_neon_vcalts_f32:\n  case NEON::BI__builtin_neon_vcaltd_f64:\n    // Only one direction of comparisons actually exist, cmle is actually a cmge\n    // with swapped operands. The table gives us the right intrinsic but we\n    // still need to do the swap.\n    std::swap(Ops[0], Ops[1]);\n    break;\n  }\n\n  assert(Int && \"Generic code assumes a valid intrinsic\");\n\n  // Determine the type(s) of this overloaded AArch64 intrinsic.\n  const Expr *Arg = E->getArg(0);\n  llvm::Type *ArgTy = CGF.ConvertType(Arg->getType());\n  Function *F = CGF.LookupNeonLLVMIntrinsic(Int, Modifier, ArgTy, E);\n\n  int j = 0;\n  ConstantInt *C0 = ConstantInt::get(CGF.SizeTy, 0);\n  for (Function::const_arg_iterator ai = F->arg_begin(), ae = F->arg_end();\n       ai != ae; ++ai, ++j) {\n    llvm::Type *ArgTy = ai->getType();\n    if (Ops[j]->getType()->getPrimitiveSizeInBits() ==\n             ArgTy->getPrimitiveSizeInBits())\n      continue;\n\n    assert(ArgTy->isVectorTy() && !Ops[j]->getType()->isVectorTy());\n    // The constant argument to an _n_ intrinsic always has Int32Ty, so truncate\n    // it before inserting.\n    Ops[j] = CGF.Builder.CreateTruncOrBitCast(\n        Ops[j], cast<llvm::VectorType>(ArgTy)->getElementType());\n    Ops[j] =\n        CGF.Builder.CreateInsertElement(UndefValue::get(ArgTy), Ops[j], C0);\n  }\n\n  Value *Result = CGF.EmitNeonCall(F, Ops, s);\n  llvm::Type *ResultType = CGF.ConvertType(E->getType());\n  if (ResultType->getPrimitiveSizeInBits().getFixedSize() <\n      Result->getType()->getPrimitiveSizeInBits().getFixedSize())\n    return CGF.Builder.CreateExtractElement(Result, C0);\n\n  return CGF.Builder.CreateBitCast(Result, ResultType, s);\n}\n\nValue *CodeGenFunction::EmitCommonNeonBuiltinExpr(\n    unsigned BuiltinID, unsigned LLVMIntrinsic, unsigned AltLLVMIntrinsic,\n    const char *NameHint, unsigned Modifier, const CallExpr *E,\n    SmallVectorImpl<llvm::Value *> &Ops, Address PtrOp0, Address PtrOp1,\n    llvm::Triple::ArchType Arch) {\n  // Get the last argument, which specifies the vector type.\n  const Expr *Arg = E->getArg(E->getNumArgs() - 1);\n  Optional<llvm::APSInt> NeonTypeConst =\n      Arg->getIntegerConstantExpr(getContext());\n  if (!NeonTypeConst)\n    return nullptr;\n\n  // Determine the type of this overloaded NEON intrinsic.\n  NeonTypeFlags Type(NeonTypeConst->getZExtValue());\n  bool Usgn = Type.isUnsigned();\n  bool Quad = Type.isQuad();\n  const bool HasLegalHalfType = getTarget().hasLegalHalfType();\n  const bool AllowBFloatArgsAndRet =\n      getTargetHooks().getABIInfo().allowBFloatArgsAndRet();\n\n  llvm::FixedVectorType *VTy =\n      GetNeonType(this, Type, HasLegalHalfType, false, AllowBFloatArgsAndRet);\n  llvm::Type *Ty = VTy;\n  if (!Ty)\n    return nullptr;\n\n  auto getAlignmentValue32 = [&](Address addr) -> Value* {\n    return Builder.getInt32(addr.getAlignment().getQuantity());\n  };\n\n  unsigned Int = LLVMIntrinsic;\n  if ((Modifier & UnsignedAlts) && !Usgn)\n    Int = AltLLVMIntrinsic;\n\n  switch (BuiltinID) {\n  default: break;\n  case NEON::BI__builtin_neon_splat_lane_v:\n  case NEON::BI__builtin_neon_splat_laneq_v:\n  case NEON::BI__builtin_neon_splatq_lane_v:\n  case NEON::BI__builtin_neon_splatq_laneq_v: {\n    auto NumElements = VTy->getElementCount();\n    if (BuiltinID == NEON::BI__builtin_neon_splatq_lane_v)\n      NumElements = NumElements * 2;\n    if (BuiltinID == NEON::BI__builtin_neon_splat_laneq_v)\n      NumElements = NumElements.divideCoefficientBy(2);\n\n    Ops[0] = Builder.CreateBitCast(Ops[0], VTy);\n    return EmitNeonSplat(Ops[0], cast<ConstantInt>(Ops[1]), NumElements);\n  }\n  case NEON::BI__builtin_neon_vpadd_v:\n  case NEON::BI__builtin_neon_vpaddq_v:\n    // We don't allow fp/int overloading of intrinsics.\n    if (VTy->getElementType()->isFloatingPointTy() &&\n        Int == Intrinsic::aarch64_neon_addp)\n      Int = Intrinsic::aarch64_neon_faddp;\n    break;\n  case NEON::BI__builtin_neon_vabs_v:\n  case NEON::BI__builtin_neon_vabsq_v:\n    if (VTy->getElementType()->isFloatingPointTy())\n      return EmitNeonCall(CGM.getIntrinsic(Intrinsic::fabs, Ty), Ops, \"vabs\");\n    return EmitNeonCall(CGM.getIntrinsic(LLVMIntrinsic, Ty), Ops, \"vabs\");\n  case NEON::BI__builtin_neon_vaddhn_v: {\n    llvm::FixedVectorType *SrcTy =\n        llvm::FixedVectorType::getExtendedElementVectorType(VTy);\n\n    // %sum = add <4 x i32> %lhs, %rhs\n    Ops[0] = Builder.CreateBitCast(Ops[0], SrcTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], SrcTy);\n    Ops[0] = Builder.CreateAdd(Ops[0], Ops[1], \"vaddhn\");\n\n    // %high = lshr <4 x i32> %sum, <i32 16, i32 16, i32 16, i32 16>\n    Constant *ShiftAmt =\n        ConstantInt::get(SrcTy, SrcTy->getScalarSizeInBits() / 2);\n    Ops[0] = Builder.CreateLShr(Ops[0], ShiftAmt, \"vaddhn\");\n\n    // %res = trunc <4 x i32> %high to <4 x i16>\n    return Builder.CreateTrunc(Ops[0], VTy, \"vaddhn\");\n  }\n  case NEON::BI__builtin_neon_vcale_v:\n  case NEON::BI__builtin_neon_vcaleq_v:\n  case NEON::BI__builtin_neon_vcalt_v:\n  case NEON::BI__builtin_neon_vcaltq_v:\n    std::swap(Ops[0], Ops[1]);\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vcage_v:\n  case NEON::BI__builtin_neon_vcageq_v:\n  case NEON::BI__builtin_neon_vcagt_v:\n  case NEON::BI__builtin_neon_vcagtq_v: {\n    llvm::Type *Ty;\n    switch (VTy->getScalarSizeInBits()) {\n    default: llvm_unreachable(\"unexpected type\");\n    case 32:\n      Ty = FloatTy;\n      break;\n    case 64:\n      Ty = DoubleTy;\n      break;\n    case 16:\n      Ty = HalfTy;\n      break;\n    }\n    auto *VecFlt = llvm::FixedVectorType::get(Ty, VTy->getNumElements());\n    llvm::Type *Tys[] = { VTy, VecFlt };\n    Function *F = CGM.getIntrinsic(LLVMIntrinsic, Tys);\n    return EmitNeonCall(F, Ops, NameHint);\n  }\n  case NEON::BI__builtin_neon_vceqz_v:\n  case NEON::BI__builtin_neon_vceqzq_v:\n    return EmitAArch64CompareBuiltinExpr(Ops[0], Ty, ICmpInst::FCMP_OEQ,\n                                         ICmpInst::ICMP_EQ, \"vceqz\");\n  case NEON::BI__builtin_neon_vcgez_v:\n  case NEON::BI__builtin_neon_vcgezq_v:\n    return EmitAArch64CompareBuiltinExpr(Ops[0], Ty, ICmpInst::FCMP_OGE,\n                                         ICmpInst::ICMP_SGE, \"vcgez\");\n  case NEON::BI__builtin_neon_vclez_v:\n  case NEON::BI__builtin_neon_vclezq_v:\n    return EmitAArch64CompareBuiltinExpr(Ops[0], Ty, ICmpInst::FCMP_OLE,\n                                         ICmpInst::ICMP_SLE, \"vclez\");\n  case NEON::BI__builtin_neon_vcgtz_v:\n  case NEON::BI__builtin_neon_vcgtzq_v:\n    return EmitAArch64CompareBuiltinExpr(Ops[0], Ty, ICmpInst::FCMP_OGT,\n                                         ICmpInst::ICMP_SGT, \"vcgtz\");\n  case NEON::BI__builtin_neon_vcltz_v:\n  case NEON::BI__builtin_neon_vcltzq_v:\n    return EmitAArch64CompareBuiltinExpr(Ops[0], Ty, ICmpInst::FCMP_OLT,\n                                         ICmpInst::ICMP_SLT, \"vcltz\");\n  case NEON::BI__builtin_neon_vclz_v:\n  case NEON::BI__builtin_neon_vclzq_v:\n    // We generate target-independent intrinsic, which needs a second argument\n    // for whether or not clz of zero is undefined; on ARM it isn't.\n    Ops.push_back(Builder.getInt1(getTarget().isCLZForZeroUndef()));\n    break;\n  case NEON::BI__builtin_neon_vcvt_f32_v:\n  case NEON::BI__builtin_neon_vcvtq_f32_v:\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ty = GetNeonType(this, NeonTypeFlags(NeonTypeFlags::Float32, false, Quad),\n                     HasLegalHalfType);\n    return Usgn ? Builder.CreateUIToFP(Ops[0], Ty, \"vcvt\")\n                : Builder.CreateSIToFP(Ops[0], Ty, \"vcvt\");\n  case NEON::BI__builtin_neon_vcvt_f16_v:\n  case NEON::BI__builtin_neon_vcvtq_f16_v:\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ty = GetNeonType(this, NeonTypeFlags(NeonTypeFlags::Float16, false, Quad),\n                     HasLegalHalfType);\n    return Usgn ? Builder.CreateUIToFP(Ops[0], Ty, \"vcvt\")\n                : Builder.CreateSIToFP(Ops[0], Ty, \"vcvt\");\n  case NEON::BI__builtin_neon_vcvt_n_f16_v:\n  case NEON::BI__builtin_neon_vcvt_n_f32_v:\n  case NEON::BI__builtin_neon_vcvt_n_f64_v:\n  case NEON::BI__builtin_neon_vcvtq_n_f16_v:\n  case NEON::BI__builtin_neon_vcvtq_n_f32_v:\n  case NEON::BI__builtin_neon_vcvtq_n_f64_v: {\n    llvm::Type *Tys[2] = { GetFloatNeonType(this, Type), Ty };\n    Int = Usgn ? LLVMIntrinsic : AltLLVMIntrinsic;\n    Function *F = CGM.getIntrinsic(Int, Tys);\n    return EmitNeonCall(F, Ops, \"vcvt_n\");\n  }\n  case NEON::BI__builtin_neon_vcvt_n_s16_v:\n  case NEON::BI__builtin_neon_vcvt_n_s32_v:\n  case NEON::BI__builtin_neon_vcvt_n_u16_v:\n  case NEON::BI__builtin_neon_vcvt_n_u32_v:\n  case NEON::BI__builtin_neon_vcvt_n_s64_v:\n  case NEON::BI__builtin_neon_vcvt_n_u64_v:\n  case NEON::BI__builtin_neon_vcvtq_n_s16_v:\n  case NEON::BI__builtin_neon_vcvtq_n_s32_v:\n  case NEON::BI__builtin_neon_vcvtq_n_u16_v:\n  case NEON::BI__builtin_neon_vcvtq_n_u32_v:\n  case NEON::BI__builtin_neon_vcvtq_n_s64_v:\n  case NEON::BI__builtin_neon_vcvtq_n_u64_v: {\n    llvm::Type *Tys[2] = { Ty, GetFloatNeonType(this, Type) };\n    Function *F = CGM.getIntrinsic(LLVMIntrinsic, Tys);\n    return EmitNeonCall(F, Ops, \"vcvt_n\");\n  }\n  case NEON::BI__builtin_neon_vcvt_s32_v:\n  case NEON::BI__builtin_neon_vcvt_u32_v:\n  case NEON::BI__builtin_neon_vcvt_s64_v:\n  case NEON::BI__builtin_neon_vcvt_u64_v:\n  case NEON::BI__builtin_neon_vcvt_s16_v:\n  case NEON::BI__builtin_neon_vcvt_u16_v:\n  case NEON::BI__builtin_neon_vcvtq_s32_v:\n  case NEON::BI__builtin_neon_vcvtq_u32_v:\n  case NEON::BI__builtin_neon_vcvtq_s64_v:\n  case NEON::BI__builtin_neon_vcvtq_u64_v:\n  case NEON::BI__builtin_neon_vcvtq_s16_v:\n  case NEON::BI__builtin_neon_vcvtq_u16_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], GetFloatNeonType(this, Type));\n    return Usgn ? Builder.CreateFPToUI(Ops[0], Ty, \"vcvt\")\n                : Builder.CreateFPToSI(Ops[0], Ty, \"vcvt\");\n  }\n  case NEON::BI__builtin_neon_vcvta_s16_v:\n  case NEON::BI__builtin_neon_vcvta_s32_v:\n  case NEON::BI__builtin_neon_vcvta_s64_v:\n  case NEON::BI__builtin_neon_vcvta_u16_v:\n  case NEON::BI__builtin_neon_vcvta_u32_v:\n  case NEON::BI__builtin_neon_vcvta_u64_v:\n  case NEON::BI__builtin_neon_vcvtaq_s16_v:\n  case NEON::BI__builtin_neon_vcvtaq_s32_v:\n  case NEON::BI__builtin_neon_vcvtaq_s64_v:\n  case NEON::BI__builtin_neon_vcvtaq_u16_v:\n  case NEON::BI__builtin_neon_vcvtaq_u32_v:\n  case NEON::BI__builtin_neon_vcvtaq_u64_v:\n  case NEON::BI__builtin_neon_vcvtn_s16_v:\n  case NEON::BI__builtin_neon_vcvtn_s32_v:\n  case NEON::BI__builtin_neon_vcvtn_s64_v:\n  case NEON::BI__builtin_neon_vcvtn_u16_v:\n  case NEON::BI__builtin_neon_vcvtn_u32_v:\n  case NEON::BI__builtin_neon_vcvtn_u64_v:\n  case NEON::BI__builtin_neon_vcvtnq_s16_v:\n  case NEON::BI__builtin_neon_vcvtnq_s32_v:\n  case NEON::BI__builtin_neon_vcvtnq_s64_v:\n  case NEON::BI__builtin_neon_vcvtnq_u16_v:\n  case NEON::BI__builtin_neon_vcvtnq_u32_v:\n  case NEON::BI__builtin_neon_vcvtnq_u64_v:\n  case NEON::BI__builtin_neon_vcvtp_s16_v:\n  case NEON::BI__builtin_neon_vcvtp_s32_v:\n  case NEON::BI__builtin_neon_vcvtp_s64_v:\n  case NEON::BI__builtin_neon_vcvtp_u16_v:\n  case NEON::BI__builtin_neon_vcvtp_u32_v:\n  case NEON::BI__builtin_neon_vcvtp_u64_v:\n  case NEON::BI__builtin_neon_vcvtpq_s16_v:\n  case NEON::BI__builtin_neon_vcvtpq_s32_v:\n  case NEON::BI__builtin_neon_vcvtpq_s64_v:\n  case NEON::BI__builtin_neon_vcvtpq_u16_v:\n  case NEON::BI__builtin_neon_vcvtpq_u32_v:\n  case NEON::BI__builtin_neon_vcvtpq_u64_v:\n  case NEON::BI__builtin_neon_vcvtm_s16_v:\n  case NEON::BI__builtin_neon_vcvtm_s32_v:\n  case NEON::BI__builtin_neon_vcvtm_s64_v:\n  case NEON::BI__builtin_neon_vcvtm_u16_v:\n  case NEON::BI__builtin_neon_vcvtm_u32_v:\n  case NEON::BI__builtin_neon_vcvtm_u64_v:\n  case NEON::BI__builtin_neon_vcvtmq_s16_v:\n  case NEON::BI__builtin_neon_vcvtmq_s32_v:\n  case NEON::BI__builtin_neon_vcvtmq_s64_v:\n  case NEON::BI__builtin_neon_vcvtmq_u16_v:\n  case NEON::BI__builtin_neon_vcvtmq_u32_v:\n  case NEON::BI__builtin_neon_vcvtmq_u64_v: {\n    llvm::Type *Tys[2] = { Ty, GetFloatNeonType(this, Type) };\n    return EmitNeonCall(CGM.getIntrinsic(LLVMIntrinsic, Tys), Ops, NameHint);\n  }\n  case NEON::BI__builtin_neon_vcvtx_f32_v: {\n    llvm::Type *Tys[2] = { VTy->getTruncatedElementVectorType(VTy), Ty};\n    return EmitNeonCall(CGM.getIntrinsic(LLVMIntrinsic, Tys), Ops, NameHint);\n\n  }\n  case NEON::BI__builtin_neon_vext_v:\n  case NEON::BI__builtin_neon_vextq_v: {\n    int CV = cast<ConstantInt>(Ops[2])->getSExtValue();\n    SmallVector<int, 16> Indices;\n    for (unsigned i = 0, e = VTy->getNumElements(); i != e; ++i)\n      Indices.push_back(i+CV);\n\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    return Builder.CreateShuffleVector(Ops[0], Ops[1], Indices, \"vext\");\n  }\n  case NEON::BI__builtin_neon_vfma_v:\n  case NEON::BI__builtin_neon_vfmaq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n\n    // NEON intrinsic puts accumulator first, unlike the LLVM fma.\n    return emitCallMaybeConstrainedFPBuiltin(\n        *this, Intrinsic::fma, Intrinsic::experimental_constrained_fma, Ty,\n        {Ops[1], Ops[2], Ops[0]});\n  }\n  case NEON::BI__builtin_neon_vld1_v:\n  case NEON::BI__builtin_neon_vld1q_v: {\n    llvm::Type *Tys[] = {Ty, Int8PtrTy};\n    Ops.push_back(getAlignmentValue32(PtrOp0));\n    return EmitNeonCall(CGM.getIntrinsic(LLVMIntrinsic, Tys), Ops, \"vld1\");\n  }\n  case NEON::BI__builtin_neon_vld1_x2_v:\n  case NEON::BI__builtin_neon_vld1q_x2_v:\n  case NEON::BI__builtin_neon_vld1_x3_v:\n  case NEON::BI__builtin_neon_vld1q_x3_v:\n  case NEON::BI__builtin_neon_vld1_x4_v:\n  case NEON::BI__builtin_neon_vld1q_x4_v: {\n    llvm::Type *PTy = llvm::PointerType::getUnqual(VTy->getElementType());\n    Ops[1] = Builder.CreateBitCast(Ops[1], PTy);\n    llvm::Type *Tys[2] = { VTy, PTy };\n    Function *F = CGM.getIntrinsic(LLVMIntrinsic, Tys);\n    Ops[1] = Builder.CreateCall(F, Ops[1], \"vld1xN\");\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld2_v:\n  case NEON::BI__builtin_neon_vld2q_v:\n  case NEON::BI__builtin_neon_vld3_v:\n  case NEON::BI__builtin_neon_vld3q_v:\n  case NEON::BI__builtin_neon_vld4_v:\n  case NEON::BI__builtin_neon_vld4q_v:\n  case NEON::BI__builtin_neon_vld2_dup_v:\n  case NEON::BI__builtin_neon_vld2q_dup_v:\n  case NEON::BI__builtin_neon_vld3_dup_v:\n  case NEON::BI__builtin_neon_vld3q_dup_v:\n  case NEON::BI__builtin_neon_vld4_dup_v:\n  case NEON::BI__builtin_neon_vld4q_dup_v: {\n    llvm::Type *Tys[] = {Ty, Int8PtrTy};\n    Function *F = CGM.getIntrinsic(LLVMIntrinsic, Tys);\n    Value *Align = getAlignmentValue32(PtrOp1);\n    Ops[1] = Builder.CreateCall(F, {Ops[1], Align}, NameHint);\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld1_dup_v:\n  case NEON::BI__builtin_neon_vld1q_dup_v: {\n    Value *V = UndefValue::get(Ty);\n    Ty = llvm::PointerType::getUnqual(VTy->getElementType());\n    PtrOp0 = Builder.CreateBitCast(PtrOp0, Ty);\n    LoadInst *Ld = Builder.CreateLoad(PtrOp0);\n    llvm::Constant *CI = ConstantInt::get(SizeTy, 0);\n    Ops[0] = Builder.CreateInsertElement(V, Ld, CI);\n    return EmitNeonSplat(Ops[0], CI);\n  }\n  case NEON::BI__builtin_neon_vld2_lane_v:\n  case NEON::BI__builtin_neon_vld2q_lane_v:\n  case NEON::BI__builtin_neon_vld3_lane_v:\n  case NEON::BI__builtin_neon_vld3q_lane_v:\n  case NEON::BI__builtin_neon_vld4_lane_v:\n  case NEON::BI__builtin_neon_vld4q_lane_v: {\n    llvm::Type *Tys[] = {Ty, Int8PtrTy};\n    Function *F = CGM.getIntrinsic(LLVMIntrinsic, Tys);\n    for (unsigned I = 2; I < Ops.size() - 1; ++I)\n      Ops[I] = Builder.CreateBitCast(Ops[I], Ty);\n    Ops.push_back(getAlignmentValue32(PtrOp1));\n    Ops[1] = Builder.CreateCall(F, makeArrayRef(Ops).slice(1), NameHint);\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vmovl_v: {\n    llvm::FixedVectorType *DTy =\n        llvm::FixedVectorType::getTruncatedElementVectorType(VTy);\n    Ops[0] = Builder.CreateBitCast(Ops[0], DTy);\n    if (Usgn)\n      return Builder.CreateZExt(Ops[0], Ty, \"vmovl\");\n    return Builder.CreateSExt(Ops[0], Ty, \"vmovl\");\n  }\n  case NEON::BI__builtin_neon_vmovn_v: {\n    llvm::FixedVectorType *QTy =\n        llvm::FixedVectorType::getExtendedElementVectorType(VTy);\n    Ops[0] = Builder.CreateBitCast(Ops[0], QTy);\n    return Builder.CreateTrunc(Ops[0], Ty, \"vmovn\");\n  }\n  case NEON::BI__builtin_neon_vmull_v:\n    // FIXME: the integer vmull operations could be emitted in terms of pure\n    // LLVM IR (2 exts followed by a mul). Unfortunately LLVM has a habit of\n    // hoisting the exts outside loops. Until global ISel comes along that can\n    // see through such movement this leads to bad CodeGen. So we need an\n    // intrinsic for now.\n    Int = Usgn ? Intrinsic::arm_neon_vmullu : Intrinsic::arm_neon_vmulls;\n    Int = Type.isPoly() ? (unsigned)Intrinsic::arm_neon_vmullp : Int;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vmull\");\n  case NEON::BI__builtin_neon_vpadal_v:\n  case NEON::BI__builtin_neon_vpadalq_v: {\n    // The source operand type has twice as many elements of half the size.\n    unsigned EltBits = VTy->getElementType()->getPrimitiveSizeInBits();\n    llvm::Type *EltTy =\n      llvm::IntegerType::get(getLLVMContext(), EltBits / 2);\n    auto *NarrowTy =\n        llvm::FixedVectorType::get(EltTy, VTy->getNumElements() * 2);\n    llvm::Type *Tys[2] = { Ty, NarrowTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, NameHint);\n  }\n  case NEON::BI__builtin_neon_vpaddl_v:\n  case NEON::BI__builtin_neon_vpaddlq_v: {\n    // The source operand type has twice as many elements of half the size.\n    unsigned EltBits = VTy->getElementType()->getPrimitiveSizeInBits();\n    llvm::Type *EltTy = llvm::IntegerType::get(getLLVMContext(), EltBits / 2);\n    auto *NarrowTy =\n        llvm::FixedVectorType::get(EltTy, VTy->getNumElements() * 2);\n    llvm::Type *Tys[2] = { Ty, NarrowTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vpaddl\");\n  }\n  case NEON::BI__builtin_neon_vqdmlal_v:\n  case NEON::BI__builtin_neon_vqdmlsl_v: {\n    SmallVector<Value *, 2> MulOps(Ops.begin() + 1, Ops.end());\n    Ops[1] =\n        EmitNeonCall(CGM.getIntrinsic(LLVMIntrinsic, Ty), MulOps, \"vqdmlal\");\n    Ops.resize(2);\n    return EmitNeonCall(CGM.getIntrinsic(AltLLVMIntrinsic, Ty), Ops, NameHint);\n  }\n  case NEON::BI__builtin_neon_vqdmulhq_lane_v:\n  case NEON::BI__builtin_neon_vqdmulh_lane_v:\n  case NEON::BI__builtin_neon_vqrdmulhq_lane_v:\n  case NEON::BI__builtin_neon_vqrdmulh_lane_v: {\n    auto *RTy = cast<llvm::FixedVectorType>(Ty);\n    if (BuiltinID == NEON::BI__builtin_neon_vqdmulhq_lane_v ||\n        BuiltinID == NEON::BI__builtin_neon_vqrdmulhq_lane_v)\n      RTy = llvm::FixedVectorType::get(RTy->getElementType(),\n                                       RTy->getNumElements() * 2);\n    llvm::Type *Tys[2] = {\n        RTy, GetNeonType(this, NeonTypeFlags(Type.getEltType(), false,\n                                             /*isQuad*/ false))};\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, NameHint);\n  }\n  case NEON::BI__builtin_neon_vqdmulhq_laneq_v:\n  case NEON::BI__builtin_neon_vqdmulh_laneq_v:\n  case NEON::BI__builtin_neon_vqrdmulhq_laneq_v:\n  case NEON::BI__builtin_neon_vqrdmulh_laneq_v: {\n    llvm::Type *Tys[2] = {\n        Ty, GetNeonType(this, NeonTypeFlags(Type.getEltType(), false,\n                                            /*isQuad*/ true))};\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, NameHint);\n  }\n  case NEON::BI__builtin_neon_vqshl_n_v:\n  case NEON::BI__builtin_neon_vqshlq_n_v:\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqshl_n\",\n                        1, false);\n  case NEON::BI__builtin_neon_vqshlu_n_v:\n  case NEON::BI__builtin_neon_vqshluq_n_v:\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqshlu_n\",\n                        1, false);\n  case NEON::BI__builtin_neon_vrecpe_v:\n  case NEON::BI__builtin_neon_vrecpeq_v:\n  case NEON::BI__builtin_neon_vrsqrte_v:\n  case NEON::BI__builtin_neon_vrsqrteq_v:\n    Int = Ty->isFPOrFPVectorTy() ? LLVMIntrinsic : AltLLVMIntrinsic;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, NameHint);\n  case NEON::BI__builtin_neon_vrndi_v:\n  case NEON::BI__builtin_neon_vrndiq_v:\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_nearbyint\n              : Intrinsic::nearbyint;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, NameHint);\n  case NEON::BI__builtin_neon_vrshr_n_v:\n  case NEON::BI__builtin_neon_vrshrq_n_v:\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrshr_n\",\n                        1, true);\n  case NEON::BI__builtin_neon_vshl_n_v:\n  case NEON::BI__builtin_neon_vshlq_n_v:\n    Ops[1] = EmitNeonShiftVector(Ops[1], Ty, false);\n    return Builder.CreateShl(Builder.CreateBitCast(Ops[0],Ty), Ops[1],\n                             \"vshl_n\");\n  case NEON::BI__builtin_neon_vshll_n_v: {\n    llvm::FixedVectorType *SrcTy =\n        llvm::FixedVectorType::getTruncatedElementVectorType(VTy);\n    Ops[0] = Builder.CreateBitCast(Ops[0], SrcTy);\n    if (Usgn)\n      Ops[0] = Builder.CreateZExt(Ops[0], VTy);\n    else\n      Ops[0] = Builder.CreateSExt(Ops[0], VTy);\n    Ops[1] = EmitNeonShiftVector(Ops[1], VTy, false);\n    return Builder.CreateShl(Ops[0], Ops[1], \"vshll_n\");\n  }\n  case NEON::BI__builtin_neon_vshrn_n_v: {\n    llvm::FixedVectorType *SrcTy =\n        llvm::FixedVectorType::getExtendedElementVectorType(VTy);\n    Ops[0] = Builder.CreateBitCast(Ops[0], SrcTy);\n    Ops[1] = EmitNeonShiftVector(Ops[1], SrcTy, false);\n    if (Usgn)\n      Ops[0] = Builder.CreateLShr(Ops[0], Ops[1]);\n    else\n      Ops[0] = Builder.CreateAShr(Ops[0], Ops[1]);\n    return Builder.CreateTrunc(Ops[0], Ty, \"vshrn_n\");\n  }\n  case NEON::BI__builtin_neon_vshr_n_v:\n  case NEON::BI__builtin_neon_vshrq_n_v:\n    return EmitNeonRShiftImm(Ops[0], Ops[1], Ty, Usgn, \"vshr_n\");\n  case NEON::BI__builtin_neon_vst1_v:\n  case NEON::BI__builtin_neon_vst1q_v:\n  case NEON::BI__builtin_neon_vst2_v:\n  case NEON::BI__builtin_neon_vst2q_v:\n  case NEON::BI__builtin_neon_vst3_v:\n  case NEON::BI__builtin_neon_vst3q_v:\n  case NEON::BI__builtin_neon_vst4_v:\n  case NEON::BI__builtin_neon_vst4q_v:\n  case NEON::BI__builtin_neon_vst2_lane_v:\n  case NEON::BI__builtin_neon_vst2q_lane_v:\n  case NEON::BI__builtin_neon_vst3_lane_v:\n  case NEON::BI__builtin_neon_vst3q_lane_v:\n  case NEON::BI__builtin_neon_vst4_lane_v:\n  case NEON::BI__builtin_neon_vst4q_lane_v: {\n    llvm::Type *Tys[] = {Int8PtrTy, Ty};\n    Ops.push_back(getAlignmentValue32(PtrOp0));\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vst1_x2_v:\n  case NEON::BI__builtin_neon_vst1q_x2_v:\n  case NEON::BI__builtin_neon_vst1_x3_v:\n  case NEON::BI__builtin_neon_vst1q_x3_v:\n  case NEON::BI__builtin_neon_vst1_x4_v:\n  case NEON::BI__builtin_neon_vst1q_x4_v: {\n    llvm::Type *PTy = llvm::PointerType::getUnqual(VTy->getElementType());\n    // TODO: Currently in AArch32 mode the pointer operand comes first, whereas\n    // in AArch64 it comes last. We may want to stick to one or another.\n    if (Arch == llvm::Triple::aarch64 || Arch == llvm::Triple::aarch64_be ||\n        Arch == llvm::Triple::aarch64_32) {\n      llvm::Type *Tys[2] = { VTy, PTy };\n      std::rotate(Ops.begin(), Ops.begin() + 1, Ops.end());\n      return EmitNeonCall(CGM.getIntrinsic(LLVMIntrinsic, Tys), Ops, \"\");\n    }\n    llvm::Type *Tys[2] = { PTy, VTy };\n    return EmitNeonCall(CGM.getIntrinsic(LLVMIntrinsic, Tys), Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vsubhn_v: {\n    llvm::FixedVectorType *SrcTy =\n        llvm::FixedVectorType::getExtendedElementVectorType(VTy);\n\n    // %sum = add <4 x i32> %lhs, %rhs\n    Ops[0] = Builder.CreateBitCast(Ops[0], SrcTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], SrcTy);\n    Ops[0] = Builder.CreateSub(Ops[0], Ops[1], \"vsubhn\");\n\n    // %high = lshr <4 x i32> %sum, <i32 16, i32 16, i32 16, i32 16>\n    Constant *ShiftAmt =\n        ConstantInt::get(SrcTy, SrcTy->getScalarSizeInBits() / 2);\n    Ops[0] = Builder.CreateLShr(Ops[0], ShiftAmt, \"vsubhn\");\n\n    // %res = trunc <4 x i32> %high to <4 x i16>\n    return Builder.CreateTrunc(Ops[0], VTy, \"vsubhn\");\n  }\n  case NEON::BI__builtin_neon_vtrn_v:\n  case NEON::BI__builtin_neon_vtrnq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(Ty));\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Value *SV = nullptr;\n\n    for (unsigned vi = 0; vi != 2; ++vi) {\n      SmallVector<int, 16> Indices;\n      for (unsigned i = 0, e = VTy->getNumElements(); i != e; i += 2) {\n        Indices.push_back(i+vi);\n        Indices.push_back(i+e+vi);\n      }\n      Value *Addr = Builder.CreateConstInBoundsGEP1_32(Ty, Ops[0], vi);\n      SV = Builder.CreateShuffleVector(Ops[1], Ops[2], Indices, \"vtrn\");\n      SV = Builder.CreateDefaultAlignedStore(SV, Addr);\n    }\n    return SV;\n  }\n  case NEON::BI__builtin_neon_vtst_v:\n  case NEON::BI__builtin_neon_vtstq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[0] = Builder.CreateAnd(Ops[0], Ops[1]);\n    Ops[0] = Builder.CreateICmp(ICmpInst::ICMP_NE, Ops[0],\n                                ConstantAggregateZero::get(Ty));\n    return Builder.CreateSExt(Ops[0], Ty, \"vtst\");\n  }\n  case NEON::BI__builtin_neon_vuzp_v:\n  case NEON::BI__builtin_neon_vuzpq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(Ty));\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Value *SV = nullptr;\n\n    for (unsigned vi = 0; vi != 2; ++vi) {\n      SmallVector<int, 16> Indices;\n      for (unsigned i = 0, e = VTy->getNumElements(); i != e; ++i)\n        Indices.push_back(2*i+vi);\n\n      Value *Addr = Builder.CreateConstInBoundsGEP1_32(Ty, Ops[0], vi);\n      SV = Builder.CreateShuffleVector(Ops[1], Ops[2], Indices, \"vuzp\");\n      SV = Builder.CreateDefaultAlignedStore(SV, Addr);\n    }\n    return SV;\n  }\n  case NEON::BI__builtin_neon_vzip_v:\n  case NEON::BI__builtin_neon_vzipq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(Ty));\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Value *SV = nullptr;\n\n    for (unsigned vi = 0; vi != 2; ++vi) {\n      SmallVector<int, 16> Indices;\n      for (unsigned i = 0, e = VTy->getNumElements(); i != e; i += 2) {\n        Indices.push_back((i + vi*e) >> 1);\n        Indices.push_back(((i + vi*e) >> 1)+e);\n      }\n      Value *Addr = Builder.CreateConstInBoundsGEP1_32(Ty, Ops[0], vi);\n      SV = Builder.CreateShuffleVector(Ops[1], Ops[2], Indices, \"vzip\");\n      SV = Builder.CreateDefaultAlignedStore(SV, Addr);\n    }\n    return SV;\n  }\n  case NEON::BI__builtin_neon_vdot_v:\n  case NEON::BI__builtin_neon_vdotq_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(Int8Ty, Ty->getPrimitiveSizeInBits() / 8);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    Int = Usgn ? LLVMIntrinsic : AltLLVMIntrinsic;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vdot\");\n  }\n  case NEON::BI__builtin_neon_vfmlal_low_v:\n  case NEON::BI__builtin_neon_vfmlalq_low_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(HalfTy, Ty->getPrimitiveSizeInBits() / 16);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vfmlal_low\");\n  }\n  case NEON::BI__builtin_neon_vfmlsl_low_v:\n  case NEON::BI__builtin_neon_vfmlslq_low_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(HalfTy, Ty->getPrimitiveSizeInBits() / 16);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vfmlsl_low\");\n  }\n  case NEON::BI__builtin_neon_vfmlal_high_v:\n  case NEON::BI__builtin_neon_vfmlalq_high_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(HalfTy, Ty->getPrimitiveSizeInBits() / 16);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vfmlal_high\");\n  }\n  case NEON::BI__builtin_neon_vfmlsl_high_v:\n  case NEON::BI__builtin_neon_vfmlslq_high_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(HalfTy, Ty->getPrimitiveSizeInBits() / 16);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vfmlsl_high\");\n  }\n  case NEON::BI__builtin_neon_vmmlaq_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(Int8Ty, Ty->getPrimitiveSizeInBits() / 8);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    Int = Usgn ? LLVMIntrinsic : AltLLVMIntrinsic;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmmla\");\n  }\n  case NEON::BI__builtin_neon_vusmmlaq_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(Int8Ty, Ty->getPrimitiveSizeInBits() / 8);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vusmmla\");\n  }\n  case NEON::BI__builtin_neon_vusdot_v:\n  case NEON::BI__builtin_neon_vusdotq_v: {\n    auto *InputTy =\n        llvm::FixedVectorType::get(Int8Ty, Ty->getPrimitiveSizeInBits() / 8);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vusdot\");\n  }\n  case NEON::BI__builtin_neon_vbfdot_v:\n  case NEON::BI__builtin_neon_vbfdotq_v: {\n    llvm::Type *InputTy =\n        llvm::FixedVectorType::get(BFloatTy, Ty->getPrimitiveSizeInBits() / 16);\n    llvm::Type *Tys[2] = { Ty, InputTy };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vbfdot\");\n  }\n  case NEON::BI__builtin_neon___a32_vcvt_bf16_v: {\n    llvm::Type *Tys[1] = { Ty };\n    Function *F = CGM.getIntrinsic(Int, Tys);\n    return EmitNeonCall(F, Ops, \"vcvtfp2bf\");\n  }\n\n  }\n\n  assert(Int && \"Expected valid intrinsic number\");\n\n  // Determine the type(s) of this overloaded AArch64 intrinsic.\n  Function *F = LookupNeonLLVMIntrinsic(Int, Modifier, Ty, E);\n\n  Value *Result = EmitNeonCall(F, Ops, NameHint);\n  llvm::Type *ResultType = ConvertType(E->getType());\n  // AArch64 intrinsic one-element vector type cast to\n  // scalar type expected by the builtin\n  return Builder.CreateBitCast(Result, ResultType, NameHint);\n}\n\nValue *CodeGenFunction::EmitAArch64CompareBuiltinExpr(\n    Value *Op, llvm::Type *Ty, const CmpInst::Predicate Fp,\n    const CmpInst::Predicate Ip, const Twine &Name) {\n  llvm::Type *OTy = Op->getType();\n\n  // FIXME: this is utterly horrific. We should not be looking at previous\n  // codegen context to find out what needs doing. Unfortunately TableGen\n  // currently gives us exactly the same calls for vceqz_f32 and vceqz_s32\n  // (etc).\n  if (BitCastInst *BI = dyn_cast<BitCastInst>(Op))\n    OTy = BI->getOperand(0)->getType();\n\n  Op = Builder.CreateBitCast(Op, OTy);\n  if (OTy->getScalarType()->isFloatingPointTy()) {\n    Op = Builder.CreateFCmp(Fp, Op, Constant::getNullValue(OTy));\n  } else {\n    Op = Builder.CreateICmp(Ip, Op, Constant::getNullValue(OTy));\n  }\n  return Builder.CreateSExt(Op, Ty, Name);\n}\n\nstatic Value *packTBLDVectorList(CodeGenFunction &CGF, ArrayRef<Value *> Ops,\n                                 Value *ExtOp, Value *IndexOp,\n                                 llvm::Type *ResTy, unsigned IntID,\n                                 const char *Name) {\n  SmallVector<Value *, 2> TblOps;\n  if (ExtOp)\n    TblOps.push_back(ExtOp);\n\n  // Build a vector containing sequential number like (0, 1, 2, ..., 15)\n  SmallVector<int, 16> Indices;\n  auto *TblTy = cast<llvm::FixedVectorType>(Ops[0]->getType());\n  for (unsigned i = 0, e = TblTy->getNumElements(); i != e; ++i) {\n    Indices.push_back(2*i);\n    Indices.push_back(2*i+1);\n  }\n\n  int PairPos = 0, End = Ops.size() - 1;\n  while (PairPos < End) {\n    TblOps.push_back(CGF.Builder.CreateShuffleVector(Ops[PairPos],\n                                                     Ops[PairPos+1], Indices,\n                                                     Name));\n    PairPos += 2;\n  }\n\n  // If there's an odd number of 64-bit lookup table, fill the high 64-bit\n  // of the 128-bit lookup table with zero.\n  if (PairPos == End) {\n    Value *ZeroTbl = ConstantAggregateZero::get(TblTy);\n    TblOps.push_back(CGF.Builder.CreateShuffleVector(Ops[PairPos],\n                                                     ZeroTbl, Indices, Name));\n  }\n\n  Function *TblF;\n  TblOps.push_back(IndexOp);\n  TblF = CGF.CGM.getIntrinsic(IntID, ResTy);\n\n  return CGF.EmitNeonCall(TblF, TblOps, Name);\n}\n\nValue *CodeGenFunction::GetValueForARMHint(unsigned BuiltinID) {\n  unsigned Value;\n  switch (BuiltinID) {\n  default:\n    return nullptr;\n  case ARM::BI__builtin_arm_nop:\n    Value = 0;\n    break;\n  case ARM::BI__builtin_arm_yield:\n  case ARM::BI__yield:\n    Value = 1;\n    break;\n  case ARM::BI__builtin_arm_wfe:\n  case ARM::BI__wfe:\n    Value = 2;\n    break;\n  case ARM::BI__builtin_arm_wfi:\n  case ARM::BI__wfi:\n    Value = 3;\n    break;\n  case ARM::BI__builtin_arm_sev:\n  case ARM::BI__sev:\n    Value = 4;\n    break;\n  case ARM::BI__builtin_arm_sevl:\n  case ARM::BI__sevl:\n    Value = 5;\n    break;\n  }\n\n  return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::arm_hint),\n                            llvm::ConstantInt::get(Int32Ty, Value));\n}\n\nenum SpecialRegisterAccessKind {\n  NormalRead,\n  VolatileRead,\n  Write,\n};\n\n// Generates the IR for the read/write special register builtin,\n// ValueType is the type of the value that is to be written or read,\n// RegisterType is the type of the register being written to or read from.\nstatic Value *EmitSpecialRegisterBuiltin(CodeGenFunction &CGF,\n                                         const CallExpr *E,\n                                         llvm::Type *RegisterType,\n                                         llvm::Type *ValueType,\n                                         SpecialRegisterAccessKind AccessKind,\n                                         StringRef SysReg = \"\") {\n  // write and register intrinsics only support 32 and 64 bit operations.\n  assert((RegisterType->isIntegerTy(32) || RegisterType->isIntegerTy(64))\n          && \"Unsupported size for register.\");\n\n  CodeGen::CGBuilderTy &Builder = CGF.Builder;\n  CodeGen::CodeGenModule &CGM = CGF.CGM;\n  LLVMContext &Context = CGM.getLLVMContext();\n\n  if (SysReg.empty()) {\n    const Expr *SysRegStrExpr = E->getArg(0)->IgnoreParenCasts();\n    SysReg = cast<clang::StringLiteral>(SysRegStrExpr)->getString();\n  }\n\n  llvm::Metadata *Ops[] = { llvm::MDString::get(Context, SysReg) };\n  llvm::MDNode *RegName = llvm::MDNode::get(Context, Ops);\n  llvm::Value *Metadata = llvm::MetadataAsValue::get(Context, RegName);\n\n  llvm::Type *Types[] = { RegisterType };\n\n  bool MixedTypes = RegisterType->isIntegerTy(64) && ValueType->isIntegerTy(32);\n  assert(!(RegisterType->isIntegerTy(32) && ValueType->isIntegerTy(64))\n            && \"Can't fit 64-bit value in 32-bit register\");\n\n  if (AccessKind != Write) {\n    assert(AccessKind == NormalRead || AccessKind == VolatileRead);\n    llvm::Function *F = CGM.getIntrinsic(\n        AccessKind == VolatileRead ? llvm::Intrinsic::read_volatile_register\n                                   : llvm::Intrinsic::read_register,\n        Types);\n    llvm::Value *Call = Builder.CreateCall(F, Metadata);\n\n    if (MixedTypes)\n      // Read into 64 bit register and then truncate result to 32 bit.\n      return Builder.CreateTrunc(Call, ValueType);\n\n    if (ValueType->isPointerTy())\n      // Have i32/i64 result (Call) but want to return a VoidPtrTy (i8*).\n      return Builder.CreateIntToPtr(Call, ValueType);\n\n    return Call;\n  }\n\n  llvm::Function *F = CGM.getIntrinsic(llvm::Intrinsic::write_register, Types);\n  llvm::Value *ArgValue = CGF.EmitScalarExpr(E->getArg(1));\n  if (MixedTypes) {\n    // Extend 32 bit write value to 64 bit to pass to write.\n    ArgValue = Builder.CreateZExt(ArgValue, RegisterType);\n    return Builder.CreateCall(F, { Metadata, ArgValue });\n  }\n\n  if (ValueType->isPointerTy()) {\n    // Have VoidPtrTy ArgValue but want to return an i32/i64.\n    ArgValue = Builder.CreatePtrToInt(ArgValue, RegisterType);\n    return Builder.CreateCall(F, { Metadata, ArgValue });\n  }\n\n  return Builder.CreateCall(F, { Metadata, ArgValue });\n}\n\n/// Return true if BuiltinID is an overloaded Neon intrinsic with an extra\n/// argument that specifies the vector type.\nstatic bool HasExtraNeonArgument(unsigned BuiltinID) {\n  switch (BuiltinID) {\n  default: break;\n  case NEON::BI__builtin_neon_vget_lane_i8:\n  case NEON::BI__builtin_neon_vget_lane_i16:\n  case NEON::BI__builtin_neon_vget_lane_bf16:\n  case NEON::BI__builtin_neon_vget_lane_i32:\n  case NEON::BI__builtin_neon_vget_lane_i64:\n  case NEON::BI__builtin_neon_vget_lane_f32:\n  case NEON::BI__builtin_neon_vgetq_lane_i8:\n  case NEON::BI__builtin_neon_vgetq_lane_i16:\n  case NEON::BI__builtin_neon_vgetq_lane_bf16:\n  case NEON::BI__builtin_neon_vgetq_lane_i32:\n  case NEON::BI__builtin_neon_vgetq_lane_i64:\n  case NEON::BI__builtin_neon_vgetq_lane_f32:\n  case NEON::BI__builtin_neon_vduph_lane_bf16:\n  case NEON::BI__builtin_neon_vduph_laneq_bf16:\n  case NEON::BI__builtin_neon_vset_lane_i8:\n  case NEON::BI__builtin_neon_vset_lane_i16:\n  case NEON::BI__builtin_neon_vset_lane_bf16:\n  case NEON::BI__builtin_neon_vset_lane_i32:\n  case NEON::BI__builtin_neon_vset_lane_i64:\n  case NEON::BI__builtin_neon_vset_lane_f32:\n  case NEON::BI__builtin_neon_vsetq_lane_i8:\n  case NEON::BI__builtin_neon_vsetq_lane_i16:\n  case NEON::BI__builtin_neon_vsetq_lane_bf16:\n  case NEON::BI__builtin_neon_vsetq_lane_i32:\n  case NEON::BI__builtin_neon_vsetq_lane_i64:\n  case NEON::BI__builtin_neon_vsetq_lane_f32:\n  case NEON::BI__builtin_neon_vsha1h_u32:\n  case NEON::BI__builtin_neon_vsha1cq_u32:\n  case NEON::BI__builtin_neon_vsha1pq_u32:\n  case NEON::BI__builtin_neon_vsha1mq_u32:\n  case NEON::BI__builtin_neon_vcvth_bf16_f32:\n  case clang::ARM::BI_MoveToCoprocessor:\n  case clang::ARM::BI_MoveToCoprocessor2:\n    return false;\n  }\n  return true;\n}\n\nValue *CodeGenFunction::EmitARMBuiltinExpr(unsigned BuiltinID,\n                                           const CallExpr *E,\n                                           ReturnValueSlot ReturnValue,\n                                           llvm::Triple::ArchType Arch) {\n  if (auto Hint = GetValueForARMHint(BuiltinID))\n    return Hint;\n\n  if (BuiltinID == ARM::BI__emit) {\n    bool IsThumb = getTarget().getTriple().getArch() == llvm::Triple::thumb;\n    llvm::FunctionType *FTy =\n        llvm::FunctionType::get(VoidTy, /*Variadic=*/false);\n\n    Expr::EvalResult Result;\n    if (!E->getArg(0)->EvaluateAsInt(Result, CGM.getContext()))\n      llvm_unreachable(\"Sema will ensure that the parameter is constant\");\n\n    llvm::APSInt Value = Result.Val.getInt();\n    uint64_t ZExtValue = Value.zextOrTrunc(IsThumb ? 16 : 32).getZExtValue();\n\n    llvm::InlineAsm *Emit =\n        IsThumb ? InlineAsm::get(FTy, \".inst.n 0x\" + utohexstr(ZExtValue), \"\",\n                                 /*hasSideEffects=*/true)\n                : InlineAsm::get(FTy, \".inst 0x\" + utohexstr(ZExtValue), \"\",\n                                 /*hasSideEffects=*/true);\n\n    return Builder.CreateCall(Emit);\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_dbg) {\n    Value *Option = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::arm_dbg), Option);\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_prefetch) {\n    Value *Address = EmitScalarExpr(E->getArg(0));\n    Value *RW      = EmitScalarExpr(E->getArg(1));\n    Value *IsData  = EmitScalarExpr(E->getArg(2));\n\n    // Locality is not supported on ARM target\n    Value *Locality = llvm::ConstantInt::get(Int32Ty, 3);\n\n    Function *F = CGM.getIntrinsic(Intrinsic::prefetch, Address->getType());\n    return Builder.CreateCall(F, {Address, RW, Locality, IsData});\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_rbit) {\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(\n        CGM.getIntrinsic(Intrinsic::bitreverse, Arg->getType()), Arg, \"rbit\");\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_cls) {\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::arm_cls), Arg, \"cls\");\n  }\n  if (BuiltinID == ARM::BI__builtin_arm_cls64) {\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::arm_cls64), Arg,\n                              \"cls\");\n  }\n\n  if (BuiltinID == ARM::BI__clear_cache) {\n    assert(E->getNumArgs() == 2 && \"__clear_cache takes 2 arguments\");\n    const FunctionDecl *FD = E->getDirectCallee();\n    Value *Ops[2];\n    for (unsigned i = 0; i < 2; i++)\n      Ops[i] = EmitScalarExpr(E->getArg(i));\n    llvm::Type *Ty = CGM.getTypes().ConvertType(FD->getType());\n    llvm::FunctionType *FTy = cast<llvm::FunctionType>(Ty);\n    StringRef Name = FD->getName();\n    return EmitNounwindRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name), Ops);\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_mcrr ||\n      BuiltinID == ARM::BI__builtin_arm_mcrr2) {\n    Function *F;\n\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"unexpected builtin\");\n    case ARM::BI__builtin_arm_mcrr:\n      F = CGM.getIntrinsic(Intrinsic::arm_mcrr);\n      break;\n    case ARM::BI__builtin_arm_mcrr2:\n      F = CGM.getIntrinsic(Intrinsic::arm_mcrr2);\n      break;\n    }\n\n    // MCRR{2} instruction has 5 operands but\n    // the intrinsic has 4 because Rt and Rt2\n    // are represented as a single unsigned 64\n    // bit integer in the intrinsic definition\n    // but internally it's represented as 2 32\n    // bit integers.\n\n    Value *Coproc = EmitScalarExpr(E->getArg(0));\n    Value *Opc1 = EmitScalarExpr(E->getArg(1));\n    Value *RtAndRt2 = EmitScalarExpr(E->getArg(2));\n    Value *CRm = EmitScalarExpr(E->getArg(3));\n\n    Value *C1 = llvm::ConstantInt::get(Int64Ty, 32);\n    Value *Rt = Builder.CreateTruncOrBitCast(RtAndRt2, Int32Ty);\n    Value *Rt2 = Builder.CreateLShr(RtAndRt2, C1);\n    Rt2 = Builder.CreateTruncOrBitCast(Rt2, Int32Ty);\n\n    return Builder.CreateCall(F, {Coproc, Opc1, Rt, Rt2, CRm});\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_mrrc ||\n      BuiltinID == ARM::BI__builtin_arm_mrrc2) {\n    Function *F;\n\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"unexpected builtin\");\n    case ARM::BI__builtin_arm_mrrc:\n      F = CGM.getIntrinsic(Intrinsic::arm_mrrc);\n      break;\n    case ARM::BI__builtin_arm_mrrc2:\n      F = CGM.getIntrinsic(Intrinsic::arm_mrrc2);\n      break;\n    }\n\n    Value *Coproc = EmitScalarExpr(E->getArg(0));\n    Value *Opc1 = EmitScalarExpr(E->getArg(1));\n    Value *CRm  = EmitScalarExpr(E->getArg(2));\n    Value *RtAndRt2 = Builder.CreateCall(F, {Coproc, Opc1, CRm});\n\n    // Returns an unsigned 64 bit integer, represented\n    // as two 32 bit integers.\n\n    Value *Rt = Builder.CreateExtractValue(RtAndRt2, 1);\n    Value *Rt1 = Builder.CreateExtractValue(RtAndRt2, 0);\n    Rt = Builder.CreateZExt(Rt, Int64Ty);\n    Rt1 = Builder.CreateZExt(Rt1, Int64Ty);\n\n    Value *ShiftCast = llvm::ConstantInt::get(Int64Ty, 32);\n    RtAndRt2 = Builder.CreateShl(Rt, ShiftCast, \"shl\", true);\n    RtAndRt2 = Builder.CreateOr(RtAndRt2, Rt1);\n\n    return Builder.CreateBitCast(RtAndRt2, ConvertType(E->getType()));\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_ldrexd ||\n      ((BuiltinID == ARM::BI__builtin_arm_ldrex ||\n        BuiltinID == ARM::BI__builtin_arm_ldaex) &&\n       getContext().getTypeSize(E->getType()) == 64) ||\n      BuiltinID == ARM::BI__ldrexd) {\n    Function *F;\n\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"unexpected builtin\");\n    case ARM::BI__builtin_arm_ldaex:\n      F = CGM.getIntrinsic(Intrinsic::arm_ldaexd);\n      break;\n    case ARM::BI__builtin_arm_ldrexd:\n    case ARM::BI__builtin_arm_ldrex:\n    case ARM::BI__ldrexd:\n      F = CGM.getIntrinsic(Intrinsic::arm_ldrexd);\n      break;\n    }\n\n    Value *LdPtr = EmitScalarExpr(E->getArg(0));\n    Value *Val = Builder.CreateCall(F, Builder.CreateBitCast(LdPtr, Int8PtrTy),\n                                    \"ldrexd\");\n\n    Value *Val0 = Builder.CreateExtractValue(Val, 1);\n    Value *Val1 = Builder.CreateExtractValue(Val, 0);\n    Val0 = Builder.CreateZExt(Val0, Int64Ty);\n    Val1 = Builder.CreateZExt(Val1, Int64Ty);\n\n    Value *ShiftCst = llvm::ConstantInt::get(Int64Ty, 32);\n    Val = Builder.CreateShl(Val0, ShiftCst, \"shl\", true /* nuw */);\n    Val = Builder.CreateOr(Val, Val1);\n    return Builder.CreateBitCast(Val, ConvertType(E->getType()));\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_ldrex ||\n      BuiltinID == ARM::BI__builtin_arm_ldaex) {\n    Value *LoadAddr = EmitScalarExpr(E->getArg(0));\n\n    QualType Ty = E->getType();\n    llvm::Type *RealResTy = ConvertType(Ty);\n    llvm::Type *PtrTy = llvm::IntegerType::get(\n        getLLVMContext(), getContext().getTypeSize(Ty))->getPointerTo();\n    LoadAddr = Builder.CreateBitCast(LoadAddr, PtrTy);\n\n    Function *F = CGM.getIntrinsic(BuiltinID == ARM::BI__builtin_arm_ldaex\n                                       ? Intrinsic::arm_ldaex\n                                       : Intrinsic::arm_ldrex,\n                                   PtrTy);\n    Value *Val = Builder.CreateCall(F, LoadAddr, \"ldrex\");\n\n    if (RealResTy->isPointerTy())\n      return Builder.CreateIntToPtr(Val, RealResTy);\n    else {\n      llvm::Type *IntResTy = llvm::IntegerType::get(\n          getLLVMContext(), CGM.getDataLayout().getTypeSizeInBits(RealResTy));\n      Val = Builder.CreateTruncOrBitCast(Val, IntResTy);\n      return Builder.CreateBitCast(Val, RealResTy);\n    }\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_strexd ||\n      ((BuiltinID == ARM::BI__builtin_arm_stlex ||\n        BuiltinID == ARM::BI__builtin_arm_strex) &&\n       getContext().getTypeSize(E->getArg(0)->getType()) == 64)) {\n    Function *F = CGM.getIntrinsic(BuiltinID == ARM::BI__builtin_arm_stlex\n                                       ? Intrinsic::arm_stlexd\n                                       : Intrinsic::arm_strexd);\n    llvm::Type *STy = llvm::StructType::get(Int32Ty, Int32Ty);\n\n    Address Tmp = CreateMemTemp(E->getArg(0)->getType());\n    Value *Val = EmitScalarExpr(E->getArg(0));\n    Builder.CreateStore(Val, Tmp);\n\n    Address LdPtr = Builder.CreateBitCast(Tmp,llvm::PointerType::getUnqual(STy));\n    Val = Builder.CreateLoad(LdPtr);\n\n    Value *Arg0 = Builder.CreateExtractValue(Val, 0);\n    Value *Arg1 = Builder.CreateExtractValue(Val, 1);\n    Value *StPtr = Builder.CreateBitCast(EmitScalarExpr(E->getArg(1)), Int8PtrTy);\n    return Builder.CreateCall(F, {Arg0, Arg1, StPtr}, \"strexd\");\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_strex ||\n      BuiltinID == ARM::BI__builtin_arm_stlex) {\n    Value *StoreVal = EmitScalarExpr(E->getArg(0));\n    Value *StoreAddr = EmitScalarExpr(E->getArg(1));\n\n    QualType Ty = E->getArg(0)->getType();\n    llvm::Type *StoreTy = llvm::IntegerType::get(getLLVMContext(),\n                                                 getContext().getTypeSize(Ty));\n    StoreAddr = Builder.CreateBitCast(StoreAddr, StoreTy->getPointerTo());\n\n    if (StoreVal->getType()->isPointerTy())\n      StoreVal = Builder.CreatePtrToInt(StoreVal, Int32Ty);\n    else {\n      llvm::Type *IntTy = llvm::IntegerType::get(\n          getLLVMContext(),\n          CGM.getDataLayout().getTypeSizeInBits(StoreVal->getType()));\n      StoreVal = Builder.CreateBitCast(StoreVal, IntTy);\n      StoreVal = Builder.CreateZExtOrBitCast(StoreVal, Int32Ty);\n    }\n\n    Function *F = CGM.getIntrinsic(BuiltinID == ARM::BI__builtin_arm_stlex\n                                       ? Intrinsic::arm_stlex\n                                       : Intrinsic::arm_strex,\n                                   StoreAddr->getType());\n    return Builder.CreateCall(F, {StoreVal, StoreAddr}, \"strex\");\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_clrex) {\n    Function *F = CGM.getIntrinsic(Intrinsic::arm_clrex);\n    return Builder.CreateCall(F);\n  }\n\n  // CRC32\n  Intrinsic::ID CRCIntrinsicID = Intrinsic::not_intrinsic;\n  switch (BuiltinID) {\n  case ARM::BI__builtin_arm_crc32b:\n    CRCIntrinsicID = Intrinsic::arm_crc32b; break;\n  case ARM::BI__builtin_arm_crc32cb:\n    CRCIntrinsicID = Intrinsic::arm_crc32cb; break;\n  case ARM::BI__builtin_arm_crc32h:\n    CRCIntrinsicID = Intrinsic::arm_crc32h; break;\n  case ARM::BI__builtin_arm_crc32ch:\n    CRCIntrinsicID = Intrinsic::arm_crc32ch; break;\n  case ARM::BI__builtin_arm_crc32w:\n  case ARM::BI__builtin_arm_crc32d:\n    CRCIntrinsicID = Intrinsic::arm_crc32w; break;\n  case ARM::BI__builtin_arm_crc32cw:\n  case ARM::BI__builtin_arm_crc32cd:\n    CRCIntrinsicID = Intrinsic::arm_crc32cw; break;\n  }\n\n  if (CRCIntrinsicID != Intrinsic::not_intrinsic) {\n    Value *Arg0 = EmitScalarExpr(E->getArg(0));\n    Value *Arg1 = EmitScalarExpr(E->getArg(1));\n\n    // crc32{c,}d intrinsics are implemnted as two calls to crc32{c,}w\n    // intrinsics, hence we need different codegen for these cases.\n    if (BuiltinID == ARM::BI__builtin_arm_crc32d ||\n        BuiltinID == ARM::BI__builtin_arm_crc32cd) {\n      Value *C1 = llvm::ConstantInt::get(Int64Ty, 32);\n      Value *Arg1a = Builder.CreateTruncOrBitCast(Arg1, Int32Ty);\n      Value *Arg1b = Builder.CreateLShr(Arg1, C1);\n      Arg1b = Builder.CreateTruncOrBitCast(Arg1b, Int32Ty);\n\n      Function *F = CGM.getIntrinsic(CRCIntrinsicID);\n      Value *Res = Builder.CreateCall(F, {Arg0, Arg1a});\n      return Builder.CreateCall(F, {Res, Arg1b});\n    } else {\n      Arg1 = Builder.CreateZExtOrBitCast(Arg1, Int32Ty);\n\n      Function *F = CGM.getIntrinsic(CRCIntrinsicID);\n      return Builder.CreateCall(F, {Arg0, Arg1});\n    }\n  }\n\n  if (BuiltinID == ARM::BI__builtin_arm_rsr ||\n      BuiltinID == ARM::BI__builtin_arm_rsr64 ||\n      BuiltinID == ARM::BI__builtin_arm_rsrp ||\n      BuiltinID == ARM::BI__builtin_arm_wsr ||\n      BuiltinID == ARM::BI__builtin_arm_wsr64 ||\n      BuiltinID == ARM::BI__builtin_arm_wsrp) {\n\n    SpecialRegisterAccessKind AccessKind = Write;\n    if (BuiltinID == ARM::BI__builtin_arm_rsr ||\n        BuiltinID == ARM::BI__builtin_arm_rsr64 ||\n        BuiltinID == ARM::BI__builtin_arm_rsrp)\n      AccessKind = VolatileRead;\n\n    bool IsPointerBuiltin = BuiltinID == ARM::BI__builtin_arm_rsrp ||\n                            BuiltinID == ARM::BI__builtin_arm_wsrp;\n\n    bool Is64Bit = BuiltinID == ARM::BI__builtin_arm_rsr64 ||\n                   BuiltinID == ARM::BI__builtin_arm_wsr64;\n\n    llvm::Type *ValueType;\n    llvm::Type *RegisterType;\n    if (IsPointerBuiltin) {\n      ValueType = VoidPtrTy;\n      RegisterType = Int32Ty;\n    } else if (Is64Bit) {\n      ValueType = RegisterType = Int64Ty;\n    } else {\n      ValueType = RegisterType = Int32Ty;\n    }\n\n    return EmitSpecialRegisterBuiltin(*this, E, RegisterType, ValueType,\n                                      AccessKind);\n  }\n\n  // Handle MSVC intrinsics before argument evaluation to prevent double\n  // evaluation.\n  if (Optional<MSVCIntrin> MsvcIntId = translateArmToMsvcIntrin(BuiltinID))\n    return EmitMSVCBuiltinExpr(*MsvcIntId, E);\n\n  // Deal with MVE builtins\n  if (Value *Result = EmitARMMVEBuiltinExpr(BuiltinID, E, ReturnValue, Arch))\n    return Result;\n  // Handle CDE builtins\n  if (Value *Result = EmitARMCDEBuiltinExpr(BuiltinID, E, ReturnValue, Arch))\n    return Result;\n\n  // Find out if any arguments are required to be integer constant\n  // expressions.\n  unsigned ICEArguments = 0;\n  ASTContext::GetBuiltinTypeError Error;\n  getContext().GetBuiltinType(BuiltinID, Error, &ICEArguments);\n  assert(Error == ASTContext::GE_None && \"Should not codegen an error\");\n\n  auto getAlignmentValue32 = [&](Address addr) -> Value* {\n    return Builder.getInt32(addr.getAlignment().getQuantity());\n  };\n\n  Address PtrOp0 = Address::invalid();\n  Address PtrOp1 = Address::invalid();\n  SmallVector<Value*, 4> Ops;\n  bool HasExtraArg = HasExtraNeonArgument(BuiltinID);\n  unsigned NumArgs = E->getNumArgs() - (HasExtraArg ? 1 : 0);\n  for (unsigned i = 0, e = NumArgs; i != e; i++) {\n    if (i == 0) {\n      switch (BuiltinID) {\n      case NEON::BI__builtin_neon_vld1_v:\n      case NEON::BI__builtin_neon_vld1q_v:\n      case NEON::BI__builtin_neon_vld1q_lane_v:\n      case NEON::BI__builtin_neon_vld1_lane_v:\n      case NEON::BI__builtin_neon_vld1_dup_v:\n      case NEON::BI__builtin_neon_vld1q_dup_v:\n      case NEON::BI__builtin_neon_vst1_v:\n      case NEON::BI__builtin_neon_vst1q_v:\n      case NEON::BI__builtin_neon_vst1q_lane_v:\n      case NEON::BI__builtin_neon_vst1_lane_v:\n      case NEON::BI__builtin_neon_vst2_v:\n      case NEON::BI__builtin_neon_vst2q_v:\n      case NEON::BI__builtin_neon_vst2_lane_v:\n      case NEON::BI__builtin_neon_vst2q_lane_v:\n      case NEON::BI__builtin_neon_vst3_v:\n      case NEON::BI__builtin_neon_vst3q_v:\n      case NEON::BI__builtin_neon_vst3_lane_v:\n      case NEON::BI__builtin_neon_vst3q_lane_v:\n      case NEON::BI__builtin_neon_vst4_v:\n      case NEON::BI__builtin_neon_vst4q_v:\n      case NEON::BI__builtin_neon_vst4_lane_v:\n      case NEON::BI__builtin_neon_vst4q_lane_v:\n        // Get the alignment for the argument in addition to the value;\n        // we'll use it later.\n        PtrOp0 = EmitPointerWithAlignment(E->getArg(0));\n        Ops.push_back(PtrOp0.getPointer());\n        continue;\n      }\n    }\n    if (i == 1) {\n      switch (BuiltinID) {\n      case NEON::BI__builtin_neon_vld2_v:\n      case NEON::BI__builtin_neon_vld2q_v:\n      case NEON::BI__builtin_neon_vld3_v:\n      case NEON::BI__builtin_neon_vld3q_v:\n      case NEON::BI__builtin_neon_vld4_v:\n      case NEON::BI__builtin_neon_vld4q_v:\n      case NEON::BI__builtin_neon_vld2_lane_v:\n      case NEON::BI__builtin_neon_vld2q_lane_v:\n      case NEON::BI__builtin_neon_vld3_lane_v:\n      case NEON::BI__builtin_neon_vld3q_lane_v:\n      case NEON::BI__builtin_neon_vld4_lane_v:\n      case NEON::BI__builtin_neon_vld4q_lane_v:\n      case NEON::BI__builtin_neon_vld2_dup_v:\n      case NEON::BI__builtin_neon_vld2q_dup_v:\n      case NEON::BI__builtin_neon_vld3_dup_v:\n      case NEON::BI__builtin_neon_vld3q_dup_v:\n      case NEON::BI__builtin_neon_vld4_dup_v:\n      case NEON::BI__builtin_neon_vld4q_dup_v:\n        // Get the alignment for the argument in addition to the value;\n        // we'll use it later.\n        PtrOp1 = EmitPointerWithAlignment(E->getArg(1));\n        Ops.push_back(PtrOp1.getPointer());\n        continue;\n      }\n    }\n\n    if ((ICEArguments & (1 << i)) == 0) {\n      Ops.push_back(EmitScalarExpr(E->getArg(i)));\n    } else {\n      // If this is required to be a constant, constant fold it so that we know\n      // that the generated intrinsic gets a ConstantInt.\n      Ops.push_back(llvm::ConstantInt::get(\n          getLLVMContext(),\n          *E->getArg(i)->getIntegerConstantExpr(getContext())));\n    }\n  }\n\n  switch (BuiltinID) {\n  default: break;\n\n  case NEON::BI__builtin_neon_vget_lane_i8:\n  case NEON::BI__builtin_neon_vget_lane_i16:\n  case NEON::BI__builtin_neon_vget_lane_i32:\n  case NEON::BI__builtin_neon_vget_lane_i64:\n  case NEON::BI__builtin_neon_vget_lane_bf16:\n  case NEON::BI__builtin_neon_vget_lane_f32:\n  case NEON::BI__builtin_neon_vgetq_lane_i8:\n  case NEON::BI__builtin_neon_vgetq_lane_i16:\n  case NEON::BI__builtin_neon_vgetq_lane_i32:\n  case NEON::BI__builtin_neon_vgetq_lane_i64:\n  case NEON::BI__builtin_neon_vgetq_lane_bf16:\n  case NEON::BI__builtin_neon_vgetq_lane_f32:\n  case NEON::BI__builtin_neon_vduph_lane_bf16:\n  case NEON::BI__builtin_neon_vduph_laneq_bf16:\n    return Builder.CreateExtractElement(Ops[0], Ops[1], \"vget_lane\");\n\n  case NEON::BI__builtin_neon_vrndns_f32: {\n    Value *Arg = EmitScalarExpr(E->getArg(0));\n    llvm::Type *Tys[] = {Arg->getType()};\n    Function *F = CGM.getIntrinsic(Intrinsic::arm_neon_vrintn, Tys);\n    return Builder.CreateCall(F, {Arg}, \"vrndn\"); }\n\n  case NEON::BI__builtin_neon_vset_lane_i8:\n  case NEON::BI__builtin_neon_vset_lane_i16:\n  case NEON::BI__builtin_neon_vset_lane_i32:\n  case NEON::BI__builtin_neon_vset_lane_i64:\n  case NEON::BI__builtin_neon_vset_lane_bf16:\n  case NEON::BI__builtin_neon_vset_lane_f32:\n  case NEON::BI__builtin_neon_vsetq_lane_i8:\n  case NEON::BI__builtin_neon_vsetq_lane_i16:\n  case NEON::BI__builtin_neon_vsetq_lane_i32:\n  case NEON::BI__builtin_neon_vsetq_lane_i64:\n  case NEON::BI__builtin_neon_vsetq_lane_bf16:\n  case NEON::BI__builtin_neon_vsetq_lane_f32:\n    return Builder.CreateInsertElement(Ops[1], Ops[0], Ops[2], \"vset_lane\");\n\n  case NEON::BI__builtin_neon_vsha1h_u32:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_sha1h), Ops,\n                        \"vsha1h\");\n  case NEON::BI__builtin_neon_vsha1cq_u32:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_sha1c), Ops,\n                        \"vsha1h\");\n  case NEON::BI__builtin_neon_vsha1pq_u32:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_sha1p), Ops,\n                        \"vsha1h\");\n  case NEON::BI__builtin_neon_vsha1mq_u32:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_sha1m), Ops,\n                        \"vsha1h\");\n\n  case NEON::BI__builtin_neon_vcvth_bf16_f32: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vcvtbfp2bf), Ops,\n                        \"vcvtbfp2bf\");\n  }\n\n  // The ARM _MoveToCoprocessor builtins put the input register value as\n  // the first argument, but the LLVM intrinsic expects it as the third one.\n  case ARM::BI_MoveToCoprocessor:\n  case ARM::BI_MoveToCoprocessor2: {\n    Function *F = CGM.getIntrinsic(BuiltinID == ARM::BI_MoveToCoprocessor ?\n                                   Intrinsic::arm_mcr : Intrinsic::arm_mcr2);\n    return Builder.CreateCall(F, {Ops[1], Ops[2], Ops[0],\n                                  Ops[3], Ops[4], Ops[5]});\n  }\n  }\n\n  // Get the last argument, which specifies the vector type.\n  assert(HasExtraArg);\n  const Expr *Arg = E->getArg(E->getNumArgs()-1);\n  Optional<llvm::APSInt> Result = Arg->getIntegerConstantExpr(getContext());\n  if (!Result)\n    return nullptr;\n\n  if (BuiltinID == ARM::BI__builtin_arm_vcvtr_f ||\n      BuiltinID == ARM::BI__builtin_arm_vcvtr_d) {\n    // Determine the overloaded type of this builtin.\n    llvm::Type *Ty;\n    if (BuiltinID == ARM::BI__builtin_arm_vcvtr_f)\n      Ty = FloatTy;\n    else\n      Ty = DoubleTy;\n\n    // Determine whether this is an unsigned conversion or not.\n    bool usgn = Result->getZExtValue() == 1;\n    unsigned Int = usgn ? Intrinsic::arm_vcvtru : Intrinsic::arm_vcvtr;\n\n    // Call the appropriate intrinsic.\n    Function *F = CGM.getIntrinsic(Int, Ty);\n    return Builder.CreateCall(F, Ops, \"vcvtr\");\n  }\n\n  // Determine the type of this overloaded NEON intrinsic.\n  NeonTypeFlags Type = Result->getZExtValue();\n  bool usgn = Type.isUnsigned();\n  bool rightShift = false;\n\n  llvm::FixedVectorType *VTy =\n      GetNeonType(this, Type, getTarget().hasLegalHalfType(), false,\n                  getTarget().hasBFloat16Type());\n  llvm::Type *Ty = VTy;\n  if (!Ty)\n    return nullptr;\n\n  // Many NEON builtins have identical semantics and uses in ARM and\n  // AArch64. Emit these in a single function.\n  auto IntrinsicMap = makeArrayRef(ARMSIMDIntrinsicMap);\n  const ARMVectorIntrinsicInfo *Builtin = findARMVectorIntrinsicInMap(\n      IntrinsicMap, BuiltinID, NEONSIMDIntrinsicsProvenSorted);\n  if (Builtin)\n    return EmitCommonNeonBuiltinExpr(\n        Builtin->BuiltinID, Builtin->LLVMIntrinsic, Builtin->AltLLVMIntrinsic,\n        Builtin->NameHint, Builtin->TypeModifier, E, Ops, PtrOp0, PtrOp1, Arch);\n\n  unsigned Int;\n  switch (BuiltinID) {\n  default: return nullptr;\n  case NEON::BI__builtin_neon_vld1q_lane_v:\n    // Handle 64-bit integer elements as a special case.  Use shuffles of\n    // one-element vectors to avoid poor code for i64 in the backend.\n    if (VTy->getElementType()->isIntegerTy(64)) {\n      // Extract the other lane.\n      Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n      int Lane = cast<ConstantInt>(Ops[2])->getZExtValue();\n      Value *SV = llvm::ConstantVector::get(ConstantInt::get(Int32Ty, 1-Lane));\n      Ops[1] = Builder.CreateShuffleVector(Ops[1], Ops[1], SV);\n      // Load the value as a one-element vector.\n      Ty = llvm::FixedVectorType::get(VTy->getElementType(), 1);\n      llvm::Type *Tys[] = {Ty, Int8PtrTy};\n      Function *F = CGM.getIntrinsic(Intrinsic::arm_neon_vld1, Tys);\n      Value *Align = getAlignmentValue32(PtrOp0);\n      Value *Ld = Builder.CreateCall(F, {Ops[0], Align});\n      // Combine them.\n      int Indices[] = {1 - Lane, Lane};\n      return Builder.CreateShuffleVector(Ops[1], Ld, Indices, \"vld1q_lane\");\n    }\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vld1_lane_v: {\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    PtrOp0 = Builder.CreateElementBitCast(PtrOp0, VTy->getElementType());\n    Value *Ld = Builder.CreateLoad(PtrOp0);\n    return Builder.CreateInsertElement(Ops[1], Ld, Ops[2], \"vld1_lane\");\n  }\n  case NEON::BI__builtin_neon_vqrshrn_n_v:\n    Int =\n      usgn ? Intrinsic::arm_neon_vqrshiftnu : Intrinsic::arm_neon_vqrshiftns;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqrshrn_n\",\n                        1, true);\n  case NEON::BI__builtin_neon_vqrshrun_n_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vqrshiftnsu, Ty),\n                        Ops, \"vqrshrun_n\", 1, true);\n  case NEON::BI__builtin_neon_vqshrn_n_v:\n    Int = usgn ? Intrinsic::arm_neon_vqshiftnu : Intrinsic::arm_neon_vqshiftns;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqshrn_n\",\n                        1, true);\n  case NEON::BI__builtin_neon_vqshrun_n_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vqshiftnsu, Ty),\n                        Ops, \"vqshrun_n\", 1, true);\n  case NEON::BI__builtin_neon_vrecpe_v:\n  case NEON::BI__builtin_neon_vrecpeq_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vrecpe, Ty),\n                        Ops, \"vrecpe\");\n  case NEON::BI__builtin_neon_vrshrn_n_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vrshiftn, Ty),\n                        Ops, \"vrshrn_n\", 1, true);\n  case NEON::BI__builtin_neon_vrsra_n_v:\n  case NEON::BI__builtin_neon_vrsraq_n_v:\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = EmitNeonShiftVector(Ops[2], Ty, true);\n    Int = usgn ? Intrinsic::arm_neon_vrshiftu : Intrinsic::arm_neon_vrshifts;\n    Ops[1] = Builder.CreateCall(CGM.getIntrinsic(Int, Ty), {Ops[1], Ops[2]});\n    return Builder.CreateAdd(Ops[0], Ops[1], \"vrsra_n\");\n  case NEON::BI__builtin_neon_vsri_n_v:\n  case NEON::BI__builtin_neon_vsriq_n_v:\n    rightShift = true;\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vsli_n_v:\n  case NEON::BI__builtin_neon_vsliq_n_v:\n    Ops[2] = EmitNeonShiftVector(Ops[2], Ty, rightShift);\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vshiftins, Ty),\n                        Ops, \"vsli_n\");\n  case NEON::BI__builtin_neon_vsra_n_v:\n  case NEON::BI__builtin_neon_vsraq_n_v:\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = EmitNeonRShiftImm(Ops[1], Ops[2], Ty, usgn, \"vsra_n\");\n    return Builder.CreateAdd(Ops[0], Ops[1]);\n  case NEON::BI__builtin_neon_vst1q_lane_v:\n    // Handle 64-bit integer elements as a special case.  Use a shuffle to get\n    // a one-element vector and avoid poor code for i64 in the backend.\n    if (VTy->getElementType()->isIntegerTy(64)) {\n      Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n      Value *SV = llvm::ConstantVector::get(cast<llvm::Constant>(Ops[2]));\n      Ops[1] = Builder.CreateShuffleVector(Ops[1], Ops[1], SV);\n      Ops[2] = getAlignmentValue32(PtrOp0);\n      llvm::Type *Tys[] = {Int8PtrTy, Ops[1]->getType()};\n      return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::arm_neon_vst1,\n                                                 Tys), Ops);\n    }\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vst1_lane_v: {\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[1] = Builder.CreateExtractElement(Ops[1], Ops[2]);\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    auto St = Builder.CreateStore(Ops[1], Builder.CreateBitCast(PtrOp0, Ty));\n    return St;\n  }\n  case NEON::BI__builtin_neon_vtbl1_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbl1),\n                        Ops, \"vtbl1\");\n  case NEON::BI__builtin_neon_vtbl2_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbl2),\n                        Ops, \"vtbl2\");\n  case NEON::BI__builtin_neon_vtbl3_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbl3),\n                        Ops, \"vtbl3\");\n  case NEON::BI__builtin_neon_vtbl4_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbl4),\n                        Ops, \"vtbl4\");\n  case NEON::BI__builtin_neon_vtbx1_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbx1),\n                        Ops, \"vtbx1\");\n  case NEON::BI__builtin_neon_vtbx2_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbx2),\n                        Ops, \"vtbx2\");\n  case NEON::BI__builtin_neon_vtbx3_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbx3),\n                        Ops, \"vtbx3\");\n  case NEON::BI__builtin_neon_vtbx4_v:\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::arm_neon_vtbx4),\n                        Ops, \"vtbx4\");\n  }\n}\n\ntemplate<typename Integer>\nstatic Integer GetIntegerConstantValue(const Expr *E, ASTContext &Context) {\n  return E->getIntegerConstantExpr(Context)->getExtValue();\n}\n\nstatic llvm::Value *SignOrZeroExtend(CGBuilderTy &Builder, llvm::Value *V,\n                                     llvm::Type *T, bool Unsigned) {\n  // Helper function called by Tablegen-constructed ARM MVE builtin codegen,\n  // which finds it convenient to specify signed/unsigned as a boolean flag.\n  return Unsigned ? Builder.CreateZExt(V, T) : Builder.CreateSExt(V, T);\n}\n\nstatic llvm::Value *MVEImmediateShr(CGBuilderTy &Builder, llvm::Value *V,\n                                    uint32_t Shift, bool Unsigned) {\n  // MVE helper function for integer shift right. This must handle signed vs\n  // unsigned, and also deal specially with the case where the shift count is\n  // equal to the lane size. In LLVM IR, an LShr with that parameter would be\n  // undefined behavior, but in MVE it's legal, so we must convert it to code\n  // that is not undefined in IR.\n  unsigned LaneBits = cast<llvm::VectorType>(V->getType())\n                          ->getElementType()\n                          ->getPrimitiveSizeInBits();\n  if (Shift == LaneBits) {\n    // An unsigned shift of the full lane size always generates zero, so we can\n    // simply emit a zero vector. A signed shift of the full lane size does the\n    // same thing as shifting by one bit fewer.\n    if (Unsigned)\n      return llvm::Constant::getNullValue(V->getType());\n    else\n      --Shift;\n  }\n  return Unsigned ? Builder.CreateLShr(V, Shift) : Builder.CreateAShr(V, Shift);\n}\n\nstatic llvm::Value *ARMMVEVectorSplat(CGBuilderTy &Builder, llvm::Value *V) {\n  // MVE-specific helper function for a vector splat, which infers the element\n  // count of the output vector by knowing that MVE vectors are all 128 bits\n  // wide.\n  unsigned Elements = 128 / V->getType()->getPrimitiveSizeInBits();\n  return Builder.CreateVectorSplat(Elements, V);\n}\n\nstatic llvm::Value *ARMMVEVectorReinterpret(CGBuilderTy &Builder,\n                                            CodeGenFunction *CGF,\n                                            llvm::Value *V,\n                                            llvm::Type *DestType) {\n  // Convert one MVE vector type into another by reinterpreting its in-register\n  // format.\n  //\n  // Little-endian, this is identical to a bitcast (which reinterprets the\n  // memory format). But big-endian, they're not necessarily the same, because\n  // the register and memory formats map to each other differently depending on\n  // the lane size.\n  //\n  // We generate a bitcast whenever we can (if we're little-endian, or if the\n  // lane sizes are the same anyway). Otherwise we fall back to an IR intrinsic\n  // that performs the different kind of reinterpretation.\n  if (CGF->getTarget().isBigEndian() &&\n      V->getType()->getScalarSizeInBits() != DestType->getScalarSizeInBits()) {\n    return Builder.CreateCall(\n        CGF->CGM.getIntrinsic(Intrinsic::arm_mve_vreinterpretq,\n                              {DestType, V->getType()}),\n        V);\n  } else {\n    return Builder.CreateBitCast(V, DestType);\n  }\n}\n\nstatic llvm::Value *VectorUnzip(CGBuilderTy &Builder, llvm::Value *V, bool Odd) {\n  // Make a shufflevector that extracts every other element of a vector (evens\n  // or odds, as desired).\n  SmallVector<int, 16> Indices;\n  unsigned InputElements =\n      cast<llvm::FixedVectorType>(V->getType())->getNumElements();\n  for (unsigned i = 0; i < InputElements; i += 2)\n    Indices.push_back(i + Odd);\n  return Builder.CreateShuffleVector(V, Indices);\n}\n\nstatic llvm::Value *VectorZip(CGBuilderTy &Builder, llvm::Value *V0,\n                              llvm::Value *V1) {\n  // Make a shufflevector that interleaves two vectors element by element.\n  assert(V0->getType() == V1->getType() && \"Can't zip different vector types\");\n  SmallVector<int, 16> Indices;\n  unsigned InputElements =\n      cast<llvm::FixedVectorType>(V0->getType())->getNumElements();\n  for (unsigned i = 0; i < InputElements; i++) {\n    Indices.push_back(i);\n    Indices.push_back(i + InputElements);\n  }\n  return Builder.CreateShuffleVector(V0, V1, Indices);\n}\n\ntemplate<unsigned HighBit, unsigned OtherBits>\nstatic llvm::Value *ARMMVEConstantSplat(CGBuilderTy &Builder, llvm::Type *VT) {\n  // MVE-specific helper function to make a vector splat of a constant such as\n  // UINT_MAX or INT_MIN, in which all bits below the highest one are equal.\n  llvm::Type *T = cast<llvm::VectorType>(VT)->getElementType();\n  unsigned LaneBits = T->getPrimitiveSizeInBits();\n  uint32_t Value = HighBit << (LaneBits - 1);\n  if (OtherBits)\n    Value |= (1UL << (LaneBits - 1)) - 1;\n  llvm::Value *Lane = llvm::ConstantInt::get(T, Value);\n  return ARMMVEVectorSplat(Builder, Lane);\n}\n\nstatic llvm::Value *ARMMVEVectorElementReverse(CGBuilderTy &Builder,\n                                               llvm::Value *V,\n                                               unsigned ReverseWidth) {\n  // MVE-specific helper function which reverses the elements of a\n  // vector within every (ReverseWidth)-bit collection of lanes.\n  SmallVector<int, 16> Indices;\n  unsigned LaneSize = V->getType()->getScalarSizeInBits();\n  unsigned Elements = 128 / LaneSize;\n  unsigned Mask = ReverseWidth / LaneSize - 1;\n  for (unsigned i = 0; i < Elements; i++)\n    Indices.push_back(i ^ Mask);\n  return Builder.CreateShuffleVector(V, Indices);\n}\n\nValue *CodeGenFunction::EmitARMMVEBuiltinExpr(unsigned BuiltinID,\n                                              const CallExpr *E,\n                                              ReturnValueSlot ReturnValue,\n                                              llvm::Triple::ArchType Arch) {\n  enum class CustomCodeGen { VLD24, VST24 } CustomCodeGenType;\n  Intrinsic::ID IRIntr;\n  unsigned NumVectors;\n\n  // Code autogenerated by Tablegen will handle all the simple builtins.\n  switch (BuiltinID) {\n    #include \"clang/Basic/arm_mve_builtin_cg.inc\"\n\n    // If we didn't match an MVE builtin id at all, go back to the\n    // main EmitARMBuiltinExpr.\n  default:\n    return nullptr;\n  }\n\n  // Anything that breaks from that switch is an MVE builtin that\n  // needs handwritten code to generate.\n\n  switch (CustomCodeGenType) {\n\n  case CustomCodeGen::VLD24: {\n    llvm::SmallVector<Value *, 4> Ops;\n    llvm::SmallVector<llvm::Type *, 4> Tys;\n\n    auto MvecCType = E->getType();\n    auto MvecLType = ConvertType(MvecCType);\n    assert(MvecLType->isStructTy() &&\n           \"Return type for vld[24]q should be a struct\");\n    assert(MvecLType->getStructNumElements() == 1 &&\n           \"Return-type struct for vld[24]q should have one element\");\n    auto MvecLTypeInner = MvecLType->getStructElementType(0);\n    assert(MvecLTypeInner->isArrayTy() &&\n           \"Return-type struct for vld[24]q should contain an array\");\n    assert(MvecLTypeInner->getArrayNumElements() == NumVectors &&\n           \"Array member of return-type struct vld[24]q has wrong length\");\n    auto VecLType = MvecLTypeInner->getArrayElementType();\n\n    Tys.push_back(VecLType);\n\n    auto Addr = E->getArg(0);\n    Ops.push_back(EmitScalarExpr(Addr));\n    Tys.push_back(ConvertType(Addr->getType()));\n\n    Function *F = CGM.getIntrinsic(IRIntr, makeArrayRef(Tys));\n    Value *LoadResult = Builder.CreateCall(F, Ops);\n    Value *MvecOut = UndefValue::get(MvecLType);\n    for (unsigned i = 0; i < NumVectors; ++i) {\n      Value *Vec = Builder.CreateExtractValue(LoadResult, i);\n      MvecOut = Builder.CreateInsertValue(MvecOut, Vec, {0, i});\n    }\n\n    if (ReturnValue.isNull())\n      return MvecOut;\n    else\n      return Builder.CreateStore(MvecOut, ReturnValue.getValue());\n  }\n\n  case CustomCodeGen::VST24: {\n    llvm::SmallVector<Value *, 4> Ops;\n    llvm::SmallVector<llvm::Type *, 4> Tys;\n\n    auto Addr = E->getArg(0);\n    Ops.push_back(EmitScalarExpr(Addr));\n    Tys.push_back(ConvertType(Addr->getType()));\n\n    auto MvecCType = E->getArg(1)->getType();\n    auto MvecLType = ConvertType(MvecCType);\n    assert(MvecLType->isStructTy() && \"Data type for vst2q should be a struct\");\n    assert(MvecLType->getStructNumElements() == 1 &&\n           \"Data-type struct for vst2q should have one element\");\n    auto MvecLTypeInner = MvecLType->getStructElementType(0);\n    assert(MvecLTypeInner->isArrayTy() &&\n           \"Data-type struct for vst2q should contain an array\");\n    assert(MvecLTypeInner->getArrayNumElements() == NumVectors &&\n           \"Array member of return-type struct vld[24]q has wrong length\");\n    auto VecLType = MvecLTypeInner->getArrayElementType();\n\n    Tys.push_back(VecLType);\n\n    AggValueSlot MvecSlot = CreateAggTemp(MvecCType);\n    EmitAggExpr(E->getArg(1), MvecSlot);\n    auto Mvec = Builder.CreateLoad(MvecSlot.getAddress());\n    for (unsigned i = 0; i < NumVectors; i++)\n      Ops.push_back(Builder.CreateExtractValue(Mvec, {0, i}));\n\n    Function *F = CGM.getIntrinsic(IRIntr, makeArrayRef(Tys));\n    Value *ToReturn = nullptr;\n    for (unsigned i = 0; i < NumVectors; i++) {\n      Ops.push_back(llvm::ConstantInt::get(Int32Ty, i));\n      ToReturn = Builder.CreateCall(F, Ops);\n      Ops.pop_back();\n    }\n    return ToReturn;\n  }\n  }\n  llvm_unreachable(\"unknown custom codegen type.\");\n}\n\nValue *CodeGenFunction::EmitARMCDEBuiltinExpr(unsigned BuiltinID,\n                                              const CallExpr *E,\n                                              ReturnValueSlot ReturnValue,\n                                              llvm::Triple::ArchType Arch) {\n  switch (BuiltinID) {\n  default:\n    return nullptr;\n#include \"clang/Basic/arm_cde_builtin_cg.inc\"\n  }\n}\n\nstatic Value *EmitAArch64TblBuiltinExpr(CodeGenFunction &CGF, unsigned BuiltinID,\n                                      const CallExpr *E,\n                                      SmallVectorImpl<Value *> &Ops,\n                                      llvm::Triple::ArchType Arch) {\n  unsigned int Int = 0;\n  const char *s = nullptr;\n\n  switch (BuiltinID) {\n  default:\n    return nullptr;\n  case NEON::BI__builtin_neon_vtbl1_v:\n  case NEON::BI__builtin_neon_vqtbl1_v:\n  case NEON::BI__builtin_neon_vqtbl1q_v:\n  case NEON::BI__builtin_neon_vtbl2_v:\n  case NEON::BI__builtin_neon_vqtbl2_v:\n  case NEON::BI__builtin_neon_vqtbl2q_v:\n  case NEON::BI__builtin_neon_vtbl3_v:\n  case NEON::BI__builtin_neon_vqtbl3_v:\n  case NEON::BI__builtin_neon_vqtbl3q_v:\n  case NEON::BI__builtin_neon_vtbl4_v:\n  case NEON::BI__builtin_neon_vqtbl4_v:\n  case NEON::BI__builtin_neon_vqtbl4q_v:\n    break;\n  case NEON::BI__builtin_neon_vtbx1_v:\n  case NEON::BI__builtin_neon_vqtbx1_v:\n  case NEON::BI__builtin_neon_vqtbx1q_v:\n  case NEON::BI__builtin_neon_vtbx2_v:\n  case NEON::BI__builtin_neon_vqtbx2_v:\n  case NEON::BI__builtin_neon_vqtbx2q_v:\n  case NEON::BI__builtin_neon_vtbx3_v:\n  case NEON::BI__builtin_neon_vqtbx3_v:\n  case NEON::BI__builtin_neon_vqtbx3q_v:\n  case NEON::BI__builtin_neon_vtbx4_v:\n  case NEON::BI__builtin_neon_vqtbx4_v:\n  case NEON::BI__builtin_neon_vqtbx4q_v:\n    break;\n  }\n\n  assert(E->getNumArgs() >= 3);\n\n  // Get the last argument, which specifies the vector type.\n  const Expr *Arg = E->getArg(E->getNumArgs() - 1);\n  Optional<llvm::APSInt> Result = Arg->getIntegerConstantExpr(CGF.getContext());\n  if (!Result)\n    return nullptr;\n\n  // Determine the type of this overloaded NEON intrinsic.\n  NeonTypeFlags Type = Result->getZExtValue();\n  llvm::FixedVectorType *Ty = GetNeonType(&CGF, Type);\n  if (!Ty)\n    return nullptr;\n\n  CodeGen::CGBuilderTy &Builder = CGF.Builder;\n\n  // AArch64 scalar builtins are not overloaded, they do not have an extra\n  // argument that specifies the vector type, need to handle each case.\n  switch (BuiltinID) {\n  case NEON::BI__builtin_neon_vtbl1_v: {\n    return packTBLDVectorList(CGF, makeArrayRef(Ops).slice(0, 1), nullptr,\n                              Ops[1], Ty, Intrinsic::aarch64_neon_tbl1,\n                              \"vtbl1\");\n  }\n  case NEON::BI__builtin_neon_vtbl2_v: {\n    return packTBLDVectorList(CGF, makeArrayRef(Ops).slice(0, 2), nullptr,\n                              Ops[2], Ty, Intrinsic::aarch64_neon_tbl1,\n                              \"vtbl1\");\n  }\n  case NEON::BI__builtin_neon_vtbl3_v: {\n    return packTBLDVectorList(CGF, makeArrayRef(Ops).slice(0, 3), nullptr,\n                              Ops[3], Ty, Intrinsic::aarch64_neon_tbl2,\n                              \"vtbl2\");\n  }\n  case NEON::BI__builtin_neon_vtbl4_v: {\n    return packTBLDVectorList(CGF, makeArrayRef(Ops).slice(0, 4), nullptr,\n                              Ops[4], Ty, Intrinsic::aarch64_neon_tbl2,\n                              \"vtbl2\");\n  }\n  case NEON::BI__builtin_neon_vtbx1_v: {\n    Value *TblRes =\n        packTBLDVectorList(CGF, makeArrayRef(Ops).slice(1, 1), nullptr, Ops[2],\n                           Ty, Intrinsic::aarch64_neon_tbl1, \"vtbl1\");\n\n    llvm::Constant *EightV = ConstantInt::get(Ty, 8);\n    Value *CmpRes = Builder.CreateICmp(ICmpInst::ICMP_UGE, Ops[2], EightV);\n    CmpRes = Builder.CreateSExt(CmpRes, Ty);\n\n    Value *EltsFromInput = Builder.CreateAnd(CmpRes, Ops[0]);\n    Value *EltsFromTbl = Builder.CreateAnd(Builder.CreateNot(CmpRes), TblRes);\n    return Builder.CreateOr(EltsFromInput, EltsFromTbl, \"vtbx\");\n  }\n  case NEON::BI__builtin_neon_vtbx2_v: {\n    return packTBLDVectorList(CGF, makeArrayRef(Ops).slice(1, 2), Ops[0],\n                              Ops[3], Ty, Intrinsic::aarch64_neon_tbx1,\n                              \"vtbx1\");\n  }\n  case NEON::BI__builtin_neon_vtbx3_v: {\n    Value *TblRes =\n        packTBLDVectorList(CGF, makeArrayRef(Ops).slice(1, 3), nullptr, Ops[4],\n                           Ty, Intrinsic::aarch64_neon_tbl2, \"vtbl2\");\n\n    llvm::Constant *TwentyFourV = ConstantInt::get(Ty, 24);\n    Value *CmpRes = Builder.CreateICmp(ICmpInst::ICMP_UGE, Ops[4],\n                                           TwentyFourV);\n    CmpRes = Builder.CreateSExt(CmpRes, Ty);\n\n    Value *EltsFromInput = Builder.CreateAnd(CmpRes, Ops[0]);\n    Value *EltsFromTbl = Builder.CreateAnd(Builder.CreateNot(CmpRes), TblRes);\n    return Builder.CreateOr(EltsFromInput, EltsFromTbl, \"vtbx\");\n  }\n  case NEON::BI__builtin_neon_vtbx4_v: {\n    return packTBLDVectorList(CGF, makeArrayRef(Ops).slice(1, 4), Ops[0],\n                              Ops[5], Ty, Intrinsic::aarch64_neon_tbx2,\n                              \"vtbx2\");\n  }\n  case NEON::BI__builtin_neon_vqtbl1_v:\n  case NEON::BI__builtin_neon_vqtbl1q_v:\n    Int = Intrinsic::aarch64_neon_tbl1; s = \"vtbl1\"; break;\n  case NEON::BI__builtin_neon_vqtbl2_v:\n  case NEON::BI__builtin_neon_vqtbl2q_v: {\n    Int = Intrinsic::aarch64_neon_tbl2; s = \"vtbl2\"; break;\n  case NEON::BI__builtin_neon_vqtbl3_v:\n  case NEON::BI__builtin_neon_vqtbl3q_v:\n    Int = Intrinsic::aarch64_neon_tbl3; s = \"vtbl3\"; break;\n  case NEON::BI__builtin_neon_vqtbl4_v:\n  case NEON::BI__builtin_neon_vqtbl4q_v:\n    Int = Intrinsic::aarch64_neon_tbl4; s = \"vtbl4\"; break;\n  case NEON::BI__builtin_neon_vqtbx1_v:\n  case NEON::BI__builtin_neon_vqtbx1q_v:\n    Int = Intrinsic::aarch64_neon_tbx1; s = \"vtbx1\"; break;\n  case NEON::BI__builtin_neon_vqtbx2_v:\n  case NEON::BI__builtin_neon_vqtbx2q_v:\n    Int = Intrinsic::aarch64_neon_tbx2; s = \"vtbx2\"; break;\n  case NEON::BI__builtin_neon_vqtbx3_v:\n  case NEON::BI__builtin_neon_vqtbx3q_v:\n    Int = Intrinsic::aarch64_neon_tbx3; s = \"vtbx3\"; break;\n  case NEON::BI__builtin_neon_vqtbx4_v:\n  case NEON::BI__builtin_neon_vqtbx4q_v:\n    Int = Intrinsic::aarch64_neon_tbx4; s = \"vtbx4\"; break;\n  }\n  }\n\n  if (!Int)\n    return nullptr;\n\n  Function *F = CGF.CGM.getIntrinsic(Int, Ty);\n  return CGF.EmitNeonCall(F, Ops, s);\n}\n\nValue *CodeGenFunction::vectorWrapScalar16(Value *Op) {\n  auto *VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n  Op = Builder.CreateBitCast(Op, Int16Ty);\n  Value *V = UndefValue::get(VTy);\n  llvm::Constant *CI = ConstantInt::get(SizeTy, 0);\n  Op = Builder.CreateInsertElement(V, Op, CI);\n  return Op;\n}\n\n/// SVEBuiltinMemEltTy - Returns the memory element type for this memory\n/// access builtin.  Only required if it can't be inferred from the base pointer\n/// operand.\nllvm::Type *CodeGenFunction::SVEBuiltinMemEltTy(SVETypeFlags TypeFlags) {\n  switch (TypeFlags.getMemEltType()) {\n  case SVETypeFlags::MemEltTyDefault:\n    return getEltType(TypeFlags);\n  case SVETypeFlags::MemEltTyInt8:\n    return Builder.getInt8Ty();\n  case SVETypeFlags::MemEltTyInt16:\n    return Builder.getInt16Ty();\n  case SVETypeFlags::MemEltTyInt32:\n    return Builder.getInt32Ty();\n  case SVETypeFlags::MemEltTyInt64:\n    return Builder.getInt64Ty();\n  }\n  llvm_unreachable(\"Unknown MemEltType\");\n}\n\nllvm::Type *CodeGenFunction::getEltType(SVETypeFlags TypeFlags) {\n  switch (TypeFlags.getEltType()) {\n  default:\n    llvm_unreachable(\"Invalid SVETypeFlag!\");\n\n  case SVETypeFlags::EltTyInt8:\n    return Builder.getInt8Ty();\n  case SVETypeFlags::EltTyInt16:\n    return Builder.getInt16Ty();\n  case SVETypeFlags::EltTyInt32:\n    return Builder.getInt32Ty();\n  case SVETypeFlags::EltTyInt64:\n    return Builder.getInt64Ty();\n\n  case SVETypeFlags::EltTyFloat16:\n    return Builder.getHalfTy();\n  case SVETypeFlags::EltTyFloat32:\n    return Builder.getFloatTy();\n  case SVETypeFlags::EltTyFloat64:\n    return Builder.getDoubleTy();\n\n  case SVETypeFlags::EltTyBFloat16:\n    return Builder.getBFloatTy();\n\n  case SVETypeFlags::EltTyBool8:\n  case SVETypeFlags::EltTyBool16:\n  case SVETypeFlags::EltTyBool32:\n  case SVETypeFlags::EltTyBool64:\n    return Builder.getInt1Ty();\n  }\n}\n\n// Return the llvm predicate vector type corresponding to the specified element\n// TypeFlags.\nllvm::ScalableVectorType *\nCodeGenFunction::getSVEPredType(SVETypeFlags TypeFlags) {\n  switch (TypeFlags.getEltType()) {\n  default: llvm_unreachable(\"Unhandled SVETypeFlag!\");\n\n  case SVETypeFlags::EltTyInt8:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 16);\n  case SVETypeFlags::EltTyInt16:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 8);\n  case SVETypeFlags::EltTyInt32:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 4);\n  case SVETypeFlags::EltTyInt64:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 2);\n\n  case SVETypeFlags::EltTyBFloat16:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 8);\n  case SVETypeFlags::EltTyFloat16:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 8);\n  case SVETypeFlags::EltTyFloat32:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 4);\n  case SVETypeFlags::EltTyFloat64:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 2);\n\n  case SVETypeFlags::EltTyBool8:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 16);\n  case SVETypeFlags::EltTyBool16:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 8);\n  case SVETypeFlags::EltTyBool32:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 4);\n  case SVETypeFlags::EltTyBool64:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 2);\n  }\n}\n\n// Return the llvm vector type corresponding to the specified element TypeFlags.\nllvm::ScalableVectorType *\nCodeGenFunction::getSVEType(const SVETypeFlags &TypeFlags) {\n  switch (TypeFlags.getEltType()) {\n  default:\n    llvm_unreachable(\"Invalid SVETypeFlag!\");\n\n  case SVETypeFlags::EltTyInt8:\n    return llvm::ScalableVectorType::get(Builder.getInt8Ty(), 16);\n  case SVETypeFlags::EltTyInt16:\n    return llvm::ScalableVectorType::get(Builder.getInt16Ty(), 8);\n  case SVETypeFlags::EltTyInt32:\n    return llvm::ScalableVectorType::get(Builder.getInt32Ty(), 4);\n  case SVETypeFlags::EltTyInt64:\n    return llvm::ScalableVectorType::get(Builder.getInt64Ty(), 2);\n\n  case SVETypeFlags::EltTyFloat16:\n    return llvm::ScalableVectorType::get(Builder.getHalfTy(), 8);\n  case SVETypeFlags::EltTyBFloat16:\n    return llvm::ScalableVectorType::get(Builder.getBFloatTy(), 8);\n  case SVETypeFlags::EltTyFloat32:\n    return llvm::ScalableVectorType::get(Builder.getFloatTy(), 4);\n  case SVETypeFlags::EltTyFloat64:\n    return llvm::ScalableVectorType::get(Builder.getDoubleTy(), 2);\n\n  case SVETypeFlags::EltTyBool8:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 16);\n  case SVETypeFlags::EltTyBool16:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 8);\n  case SVETypeFlags::EltTyBool32:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 4);\n  case SVETypeFlags::EltTyBool64:\n    return llvm::ScalableVectorType::get(Builder.getInt1Ty(), 2);\n  }\n}\n\nllvm::Value *CodeGenFunction::EmitSVEAllTruePred(SVETypeFlags TypeFlags) {\n  Function *Ptrue =\n      CGM.getIntrinsic(Intrinsic::aarch64_sve_ptrue, getSVEPredType(TypeFlags));\n  return Builder.CreateCall(Ptrue, {Builder.getInt32(/*SV_ALL*/ 31)});\n}\n\nconstexpr unsigned SVEBitsPerBlock = 128;\n\nstatic llvm::ScalableVectorType *getSVEVectorForElementType(llvm::Type *EltTy) {\n  unsigned NumElts = SVEBitsPerBlock / EltTy->getScalarSizeInBits();\n  return llvm::ScalableVectorType::get(EltTy, NumElts);\n}\n\n// Reinterpret the input predicate so that it can be used to correctly isolate\n// the elements of the specified datatype.\nValue *CodeGenFunction::EmitSVEPredicateCast(Value *Pred,\n                                             llvm::ScalableVectorType *VTy) {\n  auto *RTy = llvm::VectorType::get(IntegerType::get(getLLVMContext(), 1), VTy);\n  if (Pred->getType() == RTy)\n    return Pred;\n\n  unsigned IntID;\n  llvm::Type *IntrinsicTy;\n  switch (VTy->getMinNumElements()) {\n  default:\n    llvm_unreachable(\"unsupported element count!\");\n  case 2:\n  case 4:\n  case 8:\n    IntID = Intrinsic::aarch64_sve_convert_from_svbool;\n    IntrinsicTy = RTy;\n    break;\n  case 16:\n    IntID = Intrinsic::aarch64_sve_convert_to_svbool;\n    IntrinsicTy = Pred->getType();\n    break;\n  }\n\n  Function *F = CGM.getIntrinsic(IntID, IntrinsicTy);\n  Value *C = Builder.CreateCall(F, Pred);\n  assert(C->getType() == RTy && \"Unexpected return type!\");\n  return C;\n}\n\nValue *CodeGenFunction::EmitSVEGatherLoad(SVETypeFlags TypeFlags,\n                                          SmallVectorImpl<Value *> &Ops,\n                                          unsigned IntID) {\n  auto *ResultTy = getSVEType(TypeFlags);\n  auto *OverloadedTy =\n      llvm::ScalableVectorType::get(SVEBuiltinMemEltTy(TypeFlags), ResultTy);\n\n  // At the ACLE level there's only one predicate type, svbool_t, which is\n  // mapped to <n x 16 x i1>. However, this might be incompatible with the\n  // actual type being loaded. For example, when loading doubles (i64) the\n  // predicated should be <n x 2 x i1> instead. At the IR level the type of\n  // the predicate and the data being loaded must match. Cast accordingly.\n  Ops[0] = EmitSVEPredicateCast(Ops[0], OverloadedTy);\n\n  Function *F = nullptr;\n  if (Ops[1]->getType()->isVectorTy())\n    // This is the \"vector base, scalar offset\" case. In order to uniquely\n    // map this built-in to an LLVM IR intrinsic, we need both the return type\n    // and the type of the vector base.\n    F = CGM.getIntrinsic(IntID, {OverloadedTy, Ops[1]->getType()});\n  else\n    // This is the \"scalar base, vector offset case\". The type of the offset\n    // is encoded in the name of the intrinsic. We only need to specify the\n    // return type in order to uniquely map this built-in to an LLVM IR\n    // intrinsic.\n    F = CGM.getIntrinsic(IntID, OverloadedTy);\n\n  // Pass 0 when the offset is missing. This can only be applied when using\n  // the \"vector base\" addressing mode for which ACLE allows no offset. The\n  // corresponding LLVM IR always requires an offset.\n  if (Ops.size() == 2) {\n    assert(Ops[1]->getType()->isVectorTy() && \"Scalar base requires an offset\");\n    Ops.push_back(ConstantInt::get(Int64Ty, 0));\n  }\n\n  // For \"vector base, scalar index\" scale the index so that it becomes a\n  // scalar offset.\n  if (!TypeFlags.isByteIndexed() && Ops[1]->getType()->isVectorTy()) {\n    unsigned BytesPerElt =\n        OverloadedTy->getElementType()->getScalarSizeInBits() / 8;\n    Value *Scale = ConstantInt::get(Int64Ty, BytesPerElt);\n    Ops[2] = Builder.CreateMul(Ops[2], Scale);\n  }\n\n  Value *Call = Builder.CreateCall(F, Ops);\n\n  // The following sext/zext is only needed when ResultTy != OverloadedTy. In\n  // other cases it's folded into a nop.\n  return TypeFlags.isZExtReturn() ? Builder.CreateZExt(Call, ResultTy)\n                                  : Builder.CreateSExt(Call, ResultTy);\n}\n\nValue *CodeGenFunction::EmitSVEScatterStore(SVETypeFlags TypeFlags,\n                                            SmallVectorImpl<Value *> &Ops,\n                                            unsigned IntID) {\n  auto *SrcDataTy = getSVEType(TypeFlags);\n  auto *OverloadedTy =\n      llvm::ScalableVectorType::get(SVEBuiltinMemEltTy(TypeFlags), SrcDataTy);\n\n  // In ACLE the source data is passed in the last argument, whereas in LLVM IR\n  // it's the first argument. Move it accordingly.\n  Ops.insert(Ops.begin(), Ops.pop_back_val());\n\n  Function *F = nullptr;\n  if (Ops[2]->getType()->isVectorTy())\n    // This is the \"vector base, scalar offset\" case. In order to uniquely\n    // map this built-in to an LLVM IR intrinsic, we need both the return type\n    // and the type of the vector base.\n    F = CGM.getIntrinsic(IntID, {OverloadedTy, Ops[2]->getType()});\n  else\n    // This is the \"scalar base, vector offset case\". The type of the offset\n    // is encoded in the name of the intrinsic. We only need to specify the\n    // return type in order to uniquely map this built-in to an LLVM IR\n    // intrinsic.\n    F = CGM.getIntrinsic(IntID, OverloadedTy);\n\n  // Pass 0 when the offset is missing. This can only be applied when using\n  // the \"vector base\" addressing mode for which ACLE allows no offset. The\n  // corresponding LLVM IR always requires an offset.\n  if (Ops.size() == 3) {\n    assert(Ops[1]->getType()->isVectorTy() && \"Scalar base requires an offset\");\n    Ops.push_back(ConstantInt::get(Int64Ty, 0));\n  }\n\n  // Truncation is needed when SrcDataTy != OverloadedTy. In other cases it's\n  // folded into a nop.\n  Ops[0] = Builder.CreateTrunc(Ops[0], OverloadedTy);\n\n  // At the ACLE level there's only one predicate type, svbool_t, which is\n  // mapped to <n x 16 x i1>. However, this might be incompatible with the\n  // actual type being stored. For example, when storing doubles (i64) the\n  // predicated should be <n x 2 x i1> instead. At the IR level the type of\n  // the predicate and the data being stored must match. Cast accordingly.\n  Ops[1] = EmitSVEPredicateCast(Ops[1], OverloadedTy);\n\n  // For \"vector base, scalar index\" scale the index so that it becomes a\n  // scalar offset.\n  if (!TypeFlags.isByteIndexed() && Ops[2]->getType()->isVectorTy()) {\n    unsigned BytesPerElt =\n        OverloadedTy->getElementType()->getScalarSizeInBits() / 8;\n    Value *Scale = ConstantInt::get(Int64Ty, BytesPerElt);\n    Ops[3] = Builder.CreateMul(Ops[3], Scale);\n  }\n\n  return Builder.CreateCall(F, Ops);\n}\n\nValue *CodeGenFunction::EmitSVEGatherPrefetch(SVETypeFlags TypeFlags,\n                                              SmallVectorImpl<Value *> &Ops,\n                                              unsigned IntID) {\n  // The gather prefetches are overloaded on the vector input - this can either\n  // be the vector of base addresses or vector of offsets.\n  auto *OverloadedTy = dyn_cast<llvm::ScalableVectorType>(Ops[1]->getType());\n  if (!OverloadedTy)\n    OverloadedTy = cast<llvm::ScalableVectorType>(Ops[2]->getType());\n\n  // Cast the predicate from svbool_t to the right number of elements.\n  Ops[0] = EmitSVEPredicateCast(Ops[0], OverloadedTy);\n\n  // vector + imm addressing modes\n  if (Ops[1]->getType()->isVectorTy()) {\n    if (Ops.size() == 3) {\n      // Pass 0 for 'vector+imm' when the index is omitted.\n      Ops.push_back(ConstantInt::get(Int64Ty, 0));\n\n      // The sv_prfop is the last operand in the builtin and IR intrinsic.\n      std::swap(Ops[2], Ops[3]);\n    } else {\n      // Index needs to be passed as scaled offset.\n      llvm::Type *MemEltTy = SVEBuiltinMemEltTy(TypeFlags);\n      unsigned BytesPerElt = MemEltTy->getPrimitiveSizeInBits() / 8;\n      Value *Scale = ConstantInt::get(Int64Ty, BytesPerElt);\n      Ops[2] = Builder.CreateMul(Ops[2], Scale);\n    }\n  }\n\n  Function *F = CGM.getIntrinsic(IntID, OverloadedTy);\n  return Builder.CreateCall(F, Ops);\n}\n\nValue *CodeGenFunction::EmitSVEStructLoad(SVETypeFlags TypeFlags,\n                                          SmallVectorImpl<Value*> &Ops,\n                                          unsigned IntID) {\n  llvm::ScalableVectorType *VTy = getSVEType(TypeFlags);\n  auto VecPtrTy = llvm::PointerType::getUnqual(VTy);\n  auto EltPtrTy = llvm::PointerType::getUnqual(VTy->getElementType());\n\n  unsigned N;\n  switch (IntID) {\n  case Intrinsic::aarch64_sve_ld2:\n    N = 2;\n    break;\n  case Intrinsic::aarch64_sve_ld3:\n    N = 3;\n    break;\n  case Intrinsic::aarch64_sve_ld4:\n    N = 4;\n    break;\n  default:\n    llvm_unreachable(\"unknown intrinsic!\");\n  }\n  auto RetTy = llvm::VectorType::get(VTy->getElementType(),\n                                     VTy->getElementCount() * N);\n\n\tValue *Predicate = EmitSVEPredicateCast(Ops[0], VTy);\n  Value *BasePtr= Builder.CreateBitCast(Ops[1], VecPtrTy);\n  Value *Offset = Ops.size() > 2 ? Ops[2] : Builder.getInt32(0);\n  BasePtr = Builder.CreateGEP(VTy, BasePtr, Offset);\n  BasePtr = Builder.CreateBitCast(BasePtr, EltPtrTy);\n\n  Function *F = CGM.getIntrinsic(IntID, {RetTy, Predicate->getType()});\n  return Builder.CreateCall(F, { Predicate, BasePtr });\n}\n\nValue *CodeGenFunction::EmitSVEStructStore(SVETypeFlags TypeFlags,\n                                           SmallVectorImpl<Value*> &Ops,\n                                           unsigned IntID) {\n  llvm::ScalableVectorType *VTy = getSVEType(TypeFlags);\n  auto VecPtrTy = llvm::PointerType::getUnqual(VTy);\n  auto EltPtrTy = llvm::PointerType::getUnqual(VTy->getElementType());\n\n  unsigned N;\n  switch (IntID) {\n  case Intrinsic::aarch64_sve_st2:\n    N = 2;\n    break;\n  case Intrinsic::aarch64_sve_st3:\n    N = 3;\n    break;\n  case Intrinsic::aarch64_sve_st4:\n    N = 4;\n    break;\n  default:\n    llvm_unreachable(\"unknown intrinsic!\");\n  }\n  auto TupleTy =\n      llvm::VectorType::get(VTy->getElementType(), VTy->getElementCount() * N);\n\n  Value *Predicate = EmitSVEPredicateCast(Ops[0], VTy);\n  Value *BasePtr = Builder.CreateBitCast(Ops[1], VecPtrTy);\n  Value *Offset = Ops.size() > 3 ? Ops[2] : Builder.getInt32(0);\n  Value *Val = Ops.back();\n  BasePtr = Builder.CreateGEP(VTy, BasePtr, Offset);\n  BasePtr = Builder.CreateBitCast(BasePtr, EltPtrTy);\n\n  // The llvm.aarch64.sve.st2/3/4 intrinsics take legal part vectors, so we\n  // need to break up the tuple vector.\n  SmallVector<llvm::Value*, 5> Operands;\n  Function *FExtr =\n      CGM.getIntrinsic(Intrinsic::aarch64_sve_tuple_get, {VTy, TupleTy});\n  for (unsigned I = 0; I < N; ++I)\n    Operands.push_back(Builder.CreateCall(FExtr, {Val, Builder.getInt32(I)}));\n  Operands.append({Predicate, BasePtr});\n\n  Function *F = CGM.getIntrinsic(IntID, { VTy });\n  return Builder.CreateCall(F, Operands);\n}\n\n// SVE2's svpmullb and svpmullt builtins are similar to the svpmullb_pair and\n// svpmullt_pair intrinsics, with the exception that their results are bitcast\n// to a wider type.\nValue *CodeGenFunction::EmitSVEPMull(SVETypeFlags TypeFlags,\n                                     SmallVectorImpl<Value *> &Ops,\n                                     unsigned BuiltinID) {\n  // Splat scalar operand to vector (intrinsics with _n infix)\n  if (TypeFlags.hasSplatOperand()) {\n    unsigned OpNo = TypeFlags.getSplatOperand();\n    Ops[OpNo] = EmitSVEDupX(Ops[OpNo]);\n  }\n\n  // The pair-wise function has a narrower overloaded type.\n  Function *F = CGM.getIntrinsic(BuiltinID, Ops[0]->getType());\n  Value *Call = Builder.CreateCall(F, {Ops[0], Ops[1]});\n\n  // Now bitcast to the wider result type.\n  llvm::ScalableVectorType *Ty = getSVEType(TypeFlags);\n  return EmitSVEReinterpret(Call, Ty);\n}\n\nValue *CodeGenFunction::EmitSVEMovl(SVETypeFlags TypeFlags,\n                                    ArrayRef<Value *> Ops, unsigned BuiltinID) {\n  llvm::Type *OverloadedTy = getSVEType(TypeFlags);\n  Function *F = CGM.getIntrinsic(BuiltinID, OverloadedTy);\n  return Builder.CreateCall(F, {Ops[0], Builder.getInt32(0)});\n}\n\nValue *CodeGenFunction::EmitSVEPrefetchLoad(SVETypeFlags TypeFlags,\n                                            SmallVectorImpl<Value *> &Ops,\n                                            unsigned BuiltinID) {\n  auto *MemEltTy = SVEBuiltinMemEltTy(TypeFlags);\n  auto *VectorTy = getSVEVectorForElementType(MemEltTy);\n  auto *MemoryTy = llvm::ScalableVectorType::get(MemEltTy, VectorTy);\n\n  Value *Predicate = EmitSVEPredicateCast(Ops[0], MemoryTy);\n  Value *BasePtr = Ops[1];\n\n  // Implement the index operand if not omitted.\n  if (Ops.size() > 3) {\n    BasePtr = Builder.CreateBitCast(BasePtr, MemoryTy->getPointerTo());\n    BasePtr = Builder.CreateGEP(MemoryTy, BasePtr, Ops[2]);\n  }\n\n  // Prefetch intriniscs always expect an i8*\n  BasePtr = Builder.CreateBitCast(BasePtr, llvm::PointerType::getUnqual(Int8Ty));\n  Value *PrfOp = Ops.back();\n\n  Function *F = CGM.getIntrinsic(BuiltinID, Predicate->getType());\n  return Builder.CreateCall(F, {Predicate, BasePtr, PrfOp});\n}\n\nValue *CodeGenFunction::EmitSVEMaskedLoad(const CallExpr *E,\n                                          llvm::Type *ReturnTy,\n                                          SmallVectorImpl<Value *> &Ops,\n                                          unsigned BuiltinID,\n                                          bool IsZExtReturn) {\n  QualType LangPTy = E->getArg(1)->getType();\n  llvm::Type *MemEltTy = CGM.getTypes().ConvertType(\n      LangPTy->getAs<PointerType>()->getPointeeType());\n\n  // The vector type that is returned may be different from the\n  // eventual type loaded from memory.\n  auto VectorTy = cast<llvm::ScalableVectorType>(ReturnTy);\n  auto MemoryTy = llvm::ScalableVectorType::get(MemEltTy, VectorTy);\n\n  Value *Predicate = EmitSVEPredicateCast(Ops[0], MemoryTy);\n  Value *BasePtr = Builder.CreateBitCast(Ops[1], MemoryTy->getPointerTo());\n  Value *Offset = Ops.size() > 2 ? Ops[2] : Builder.getInt32(0);\n  BasePtr = Builder.CreateGEP(MemoryTy, BasePtr, Offset);\n\n  BasePtr = Builder.CreateBitCast(BasePtr, MemEltTy->getPointerTo());\n  Function *F = CGM.getIntrinsic(BuiltinID, MemoryTy);\n  Value *Load = Builder.CreateCall(F, {Predicate, BasePtr});\n\n  return IsZExtReturn ? Builder.CreateZExt(Load, VectorTy)\n                     : Builder.CreateSExt(Load, VectorTy);\n}\n\nValue *CodeGenFunction::EmitSVEMaskedStore(const CallExpr *E,\n                                           SmallVectorImpl<Value *> &Ops,\n                                           unsigned BuiltinID) {\n  QualType LangPTy = E->getArg(1)->getType();\n  llvm::Type *MemEltTy = CGM.getTypes().ConvertType(\n      LangPTy->getAs<PointerType>()->getPointeeType());\n\n  // The vector type that is stored may be different from the\n  // eventual type stored to memory.\n  auto VectorTy = cast<llvm::ScalableVectorType>(Ops.back()->getType());\n  auto MemoryTy = llvm::ScalableVectorType::get(MemEltTy, VectorTy);\n\n  Value *Predicate = EmitSVEPredicateCast(Ops[0], MemoryTy);\n  Value *BasePtr = Builder.CreateBitCast(Ops[1], MemoryTy->getPointerTo());\n  Value *Offset = Ops.size() == 4 ? Ops[2] : Builder.getInt32(0);\n  BasePtr = Builder.CreateGEP(MemoryTy, BasePtr, Offset);\n\n  // Last value is always the data\n  llvm::Value *Val = Builder.CreateTrunc(Ops.back(), MemoryTy);\n\n  BasePtr = Builder.CreateBitCast(BasePtr, MemEltTy->getPointerTo());\n  Function *F = CGM.getIntrinsic(BuiltinID, MemoryTy);\n  return Builder.CreateCall(F, {Val, Predicate, BasePtr});\n}\n\n// Limit the usage of scalable llvm IR generated by the ACLE by using the\n// sve dup.x intrinsic instead of IRBuilder::CreateVectorSplat.\nValue *CodeGenFunction::EmitSVEDupX(Value *Scalar, llvm::Type *Ty) {\n  auto F = CGM.getIntrinsic(Intrinsic::aarch64_sve_dup_x, Ty);\n  return Builder.CreateCall(F, Scalar);\n}\n\nValue *CodeGenFunction::EmitSVEDupX(Value* Scalar) {\n  return EmitSVEDupX(Scalar, getSVEVectorForElementType(Scalar->getType()));\n}\n\nValue *CodeGenFunction::EmitSVEReinterpret(Value *Val, llvm::Type *Ty) {\n  // FIXME: For big endian this needs an additional REV, or needs a separate\n  // intrinsic that is code-generated as a no-op, because the LLVM bitcast\n  // instruction is defined as 'bitwise' equivalent from memory point of\n  // view (when storing/reloading), whereas the svreinterpret builtin\n  // implements bitwise equivalent cast from register point of view.\n  // LLVM CodeGen for a bitcast must add an explicit REV for big-endian.\n  return Builder.CreateBitCast(Val, Ty);\n}\n\nstatic void InsertExplicitZeroOperand(CGBuilderTy &Builder, llvm::Type *Ty,\n                                      SmallVectorImpl<Value *> &Ops) {\n  auto *SplatZero = Constant::getNullValue(Ty);\n  Ops.insert(Ops.begin(), SplatZero);\n}\n\nstatic void InsertExplicitUndefOperand(CGBuilderTy &Builder, llvm::Type *Ty,\n                                       SmallVectorImpl<Value *> &Ops) {\n  auto *SplatUndef = UndefValue::get(Ty);\n  Ops.insert(Ops.begin(), SplatUndef);\n}\n\nSmallVector<llvm::Type *, 2> CodeGenFunction::getSVEOverloadTypes(\n    SVETypeFlags TypeFlags, llvm::Type *ResultType, ArrayRef<Value *> Ops) {\n  if (TypeFlags.isOverloadNone())\n    return {};\n\n  llvm::Type *DefaultType = getSVEType(TypeFlags);\n\n  if (TypeFlags.isOverloadWhile())\n    return {DefaultType, Ops[1]->getType()};\n\n  if (TypeFlags.isOverloadWhileRW())\n    return {getSVEPredType(TypeFlags), Ops[0]->getType()};\n\n  if (TypeFlags.isOverloadCvt() || TypeFlags.isTupleSet())\n    return {Ops[0]->getType(), Ops.back()->getType()};\n\n  if (TypeFlags.isTupleCreate() || TypeFlags.isTupleGet())\n    return {ResultType, Ops[0]->getType()};\n\n  assert(TypeFlags.isOverloadDefault() && \"Unexpected value for overloads\");\n  return {DefaultType};\n}\n\nValue *CodeGenFunction::EmitAArch64SVEBuiltinExpr(unsigned BuiltinID,\n                                                  const CallExpr *E) {\n  // Find out if any arguments are required to be integer constant expressions.\n  unsigned ICEArguments = 0;\n  ASTContext::GetBuiltinTypeError Error;\n  getContext().GetBuiltinType(BuiltinID, Error, &ICEArguments);\n  assert(Error == ASTContext::GE_None && \"Should not codegen an error\");\n\n  llvm::Type *Ty = ConvertType(E->getType());\n  if (BuiltinID >= SVE::BI__builtin_sve_reinterpret_s8_s8 &&\n      BuiltinID <= SVE::BI__builtin_sve_reinterpret_f64_f64) {\n    Value *Val = EmitScalarExpr(E->getArg(0));\n    return EmitSVEReinterpret(Val, Ty);\n  }\n\n  llvm::SmallVector<Value *, 4> Ops;\n  for (unsigned i = 0, e = E->getNumArgs(); i != e; i++) {\n    if ((ICEArguments & (1 << i)) == 0)\n      Ops.push_back(EmitScalarExpr(E->getArg(i)));\n    else {\n      // If this is required to be a constant, constant fold it so that we know\n      // that the generated intrinsic gets a ConstantInt.\n      Optional<llvm::APSInt> Result =\n          E->getArg(i)->getIntegerConstantExpr(getContext());\n      assert(Result && \"Expected argument to be a constant\");\n\n      // Immediates for SVE llvm intrinsics are always 32bit.  We can safely\n      // truncate because the immediate has been range checked and no valid\n      // immediate requires more than a handful of bits.\n      *Result = Result->extOrTrunc(32);\n      Ops.push_back(llvm::ConstantInt::get(getLLVMContext(), *Result));\n    }\n  }\n\n  auto *Builtin = findARMVectorIntrinsicInMap(AArch64SVEIntrinsicMap, BuiltinID,\n                                              AArch64SVEIntrinsicsProvenSorted);\n  SVETypeFlags TypeFlags(Builtin->TypeModifier);\n  if (TypeFlags.isLoad())\n    return EmitSVEMaskedLoad(E, Ty, Ops, Builtin->LLVMIntrinsic,\n                             TypeFlags.isZExtReturn());\n  else if (TypeFlags.isStore())\n    return EmitSVEMaskedStore(E, Ops, Builtin->LLVMIntrinsic);\n  else if (TypeFlags.isGatherLoad())\n    return EmitSVEGatherLoad(TypeFlags, Ops, Builtin->LLVMIntrinsic);\n  else if (TypeFlags.isScatterStore())\n    return EmitSVEScatterStore(TypeFlags, Ops, Builtin->LLVMIntrinsic);\n  else if (TypeFlags.isPrefetch())\n    return EmitSVEPrefetchLoad(TypeFlags, Ops, Builtin->LLVMIntrinsic);\n  else if (TypeFlags.isGatherPrefetch())\n    return EmitSVEGatherPrefetch(TypeFlags, Ops, Builtin->LLVMIntrinsic);\n\telse if (TypeFlags.isStructLoad())\n\t\treturn EmitSVEStructLoad(TypeFlags, Ops, Builtin->LLVMIntrinsic);\n\telse if (TypeFlags.isStructStore())\n\t\treturn EmitSVEStructStore(TypeFlags, Ops, Builtin->LLVMIntrinsic);\n  else if (TypeFlags.isUndef())\n    return UndefValue::get(Ty);\n  else if (Builtin->LLVMIntrinsic != 0) {\n    if (TypeFlags.getMergeType() == SVETypeFlags::MergeZeroExp)\n      InsertExplicitZeroOperand(Builder, Ty, Ops);\n\n    if (TypeFlags.getMergeType() == SVETypeFlags::MergeAnyExp)\n      InsertExplicitUndefOperand(Builder, Ty, Ops);\n\n    // Some ACLE builtins leave out the argument to specify the predicate\n    // pattern, which is expected to be expanded to an SV_ALL pattern.\n    if (TypeFlags.isAppendSVALL())\n      Ops.push_back(Builder.getInt32(/*SV_ALL*/ 31));\n    if (TypeFlags.isInsertOp1SVALL())\n      Ops.insert(&Ops[1], Builder.getInt32(/*SV_ALL*/ 31));\n\n    // Predicates must match the main datatype.\n    for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n      if (auto PredTy = dyn_cast<llvm::VectorType>(Ops[i]->getType()))\n        if (PredTy->getElementType()->isIntegerTy(1))\n          Ops[i] = EmitSVEPredicateCast(Ops[i], getSVEType(TypeFlags));\n\n    // Splat scalar operand to vector (intrinsics with _n infix)\n    if (TypeFlags.hasSplatOperand()) {\n      unsigned OpNo = TypeFlags.getSplatOperand();\n      Ops[OpNo] = EmitSVEDupX(Ops[OpNo]);\n    }\n\n    if (TypeFlags.isReverseCompare())\n      std::swap(Ops[1], Ops[2]);\n\n    if (TypeFlags.isReverseUSDOT())\n      std::swap(Ops[1], Ops[2]);\n\n    // Predicated intrinsics with _z suffix need a select w/ zeroinitializer.\n    if (TypeFlags.getMergeType() == SVETypeFlags::MergeZero) {\n      llvm::Type *OpndTy = Ops[1]->getType();\n      auto *SplatZero = Constant::getNullValue(OpndTy);\n      Function *Sel = CGM.getIntrinsic(Intrinsic::aarch64_sve_sel, OpndTy);\n      Ops[1] = Builder.CreateCall(Sel, {Ops[0], Ops[1], SplatZero});\n    }\n\n    Function *F = CGM.getIntrinsic(Builtin->LLVMIntrinsic,\n                                   getSVEOverloadTypes(TypeFlags, Ty, Ops));\n    Value *Call = Builder.CreateCall(F, Ops);\n\n    // Predicate results must be converted to svbool_t.\n    if (auto PredTy = dyn_cast<llvm::VectorType>(Call->getType()))\n      if (PredTy->getScalarType()->isIntegerTy(1))\n        Call = EmitSVEPredicateCast(Call, cast<llvm::ScalableVectorType>(Ty));\n\n    return Call;\n  }\n\n  switch (BuiltinID) {\n  default:\n    return nullptr;\n\n  case SVE::BI__builtin_sve_svmov_b_z: {\n    // svmov_b_z(pg, op) <=> svand_b_z(pg, op, op)\n    SVETypeFlags TypeFlags(Builtin->TypeModifier);\n    llvm::Type* OverloadedTy = getSVEType(TypeFlags);\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_sve_and_z, OverloadedTy);\n    return Builder.CreateCall(F, {Ops[0], Ops[1], Ops[1]});\n  }\n\n  case SVE::BI__builtin_sve_svnot_b_z: {\n    // svnot_b_z(pg, op) <=> sveor_b_z(pg, op, pg)\n    SVETypeFlags TypeFlags(Builtin->TypeModifier);\n    llvm::Type* OverloadedTy = getSVEType(TypeFlags);\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_sve_eor_z, OverloadedTy);\n    return Builder.CreateCall(F, {Ops[0], Ops[1], Ops[0]});\n  }\n\n  case SVE::BI__builtin_sve_svmovlb_u16:\n  case SVE::BI__builtin_sve_svmovlb_u32:\n  case SVE::BI__builtin_sve_svmovlb_u64:\n    return EmitSVEMovl(TypeFlags, Ops, Intrinsic::aarch64_sve_ushllb);\n\n  case SVE::BI__builtin_sve_svmovlb_s16:\n  case SVE::BI__builtin_sve_svmovlb_s32:\n  case SVE::BI__builtin_sve_svmovlb_s64:\n    return EmitSVEMovl(TypeFlags, Ops, Intrinsic::aarch64_sve_sshllb);\n\n  case SVE::BI__builtin_sve_svmovlt_u16:\n  case SVE::BI__builtin_sve_svmovlt_u32:\n  case SVE::BI__builtin_sve_svmovlt_u64:\n    return EmitSVEMovl(TypeFlags, Ops, Intrinsic::aarch64_sve_ushllt);\n\n  case SVE::BI__builtin_sve_svmovlt_s16:\n  case SVE::BI__builtin_sve_svmovlt_s32:\n  case SVE::BI__builtin_sve_svmovlt_s64:\n    return EmitSVEMovl(TypeFlags, Ops, Intrinsic::aarch64_sve_sshllt);\n\n  case SVE::BI__builtin_sve_svpmullt_u16:\n  case SVE::BI__builtin_sve_svpmullt_u64:\n  case SVE::BI__builtin_sve_svpmullt_n_u16:\n  case SVE::BI__builtin_sve_svpmullt_n_u64:\n    return EmitSVEPMull(TypeFlags, Ops, Intrinsic::aarch64_sve_pmullt_pair);\n\n  case SVE::BI__builtin_sve_svpmullb_u16:\n  case SVE::BI__builtin_sve_svpmullb_u64:\n  case SVE::BI__builtin_sve_svpmullb_n_u16:\n  case SVE::BI__builtin_sve_svpmullb_n_u64:\n    return EmitSVEPMull(TypeFlags, Ops, Intrinsic::aarch64_sve_pmullb_pair);\n\n  case SVE::BI__builtin_sve_svdup_n_b8:\n  case SVE::BI__builtin_sve_svdup_n_b16:\n  case SVE::BI__builtin_sve_svdup_n_b32:\n  case SVE::BI__builtin_sve_svdup_n_b64: {\n    Value *CmpNE =\n        Builder.CreateICmpNE(Ops[0], Constant::getNullValue(Ops[0]->getType()));\n    llvm::ScalableVectorType *OverloadedTy = getSVEType(TypeFlags);\n    Value *Dup = EmitSVEDupX(CmpNE, OverloadedTy);\n    return EmitSVEPredicateCast(Dup, cast<llvm::ScalableVectorType>(Ty));\n  }\n\n  case SVE::BI__builtin_sve_svdupq_n_b8:\n  case SVE::BI__builtin_sve_svdupq_n_b16:\n  case SVE::BI__builtin_sve_svdupq_n_b32:\n  case SVE::BI__builtin_sve_svdupq_n_b64:\n  case SVE::BI__builtin_sve_svdupq_n_u8:\n  case SVE::BI__builtin_sve_svdupq_n_s8:\n  case SVE::BI__builtin_sve_svdupq_n_u64:\n  case SVE::BI__builtin_sve_svdupq_n_f64:\n  case SVE::BI__builtin_sve_svdupq_n_s64:\n  case SVE::BI__builtin_sve_svdupq_n_u16:\n  case SVE::BI__builtin_sve_svdupq_n_f16:\n  case SVE::BI__builtin_sve_svdupq_n_bf16:\n  case SVE::BI__builtin_sve_svdupq_n_s16:\n  case SVE::BI__builtin_sve_svdupq_n_u32:\n  case SVE::BI__builtin_sve_svdupq_n_f32:\n  case SVE::BI__builtin_sve_svdupq_n_s32: {\n    // These builtins are implemented by storing each element to an array and using\n    // ld1rq to materialize a vector.\n    unsigned NumOpnds = Ops.size();\n\n    bool IsBoolTy =\n        cast<llvm::VectorType>(Ty)->getElementType()->isIntegerTy(1);\n\n    // For svdupq_n_b* the element type of is an integer of type 128/numelts,\n    // so that the compare can use the width that is natural for the expected\n    // number of predicate lanes.\n    llvm::Type *EltTy = Ops[0]->getType();\n    if (IsBoolTy)\n      EltTy = IntegerType::get(getLLVMContext(), SVEBitsPerBlock / NumOpnds);\n\n    Address Alloca = CreateTempAlloca(llvm::ArrayType::get(EltTy, NumOpnds),\n                                     CharUnits::fromQuantity(16));\n    for (unsigned I = 0; I < NumOpnds; ++I)\n      Builder.CreateDefaultAlignedStore(\n          IsBoolTy ? Builder.CreateZExt(Ops[I], EltTy) : Ops[I],\n          Builder.CreateGEP(Alloca.getPointer(),\n                            {Builder.getInt64(0), Builder.getInt64(I)}));\n\n    SVETypeFlags TypeFlags(Builtin->TypeModifier);\n    Value *Pred = EmitSVEAllTruePred(TypeFlags);\n\n    llvm::Type *OverloadedTy = getSVEVectorForElementType(EltTy);\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_sve_ld1rq, OverloadedTy);\n    Value *Alloca0 = Builder.CreateGEP(\n        Alloca.getPointer(), {Builder.getInt64(0), Builder.getInt64(0)});\n    Value *LD1RQ = Builder.CreateCall(F, {Pred, Alloca0});\n\n    if (!IsBoolTy)\n      return LD1RQ;\n\n    // For svdupq_n_b* we need to add an additional 'cmpne' with '0'.\n    F = CGM.getIntrinsic(NumOpnds == 2 ? Intrinsic::aarch64_sve_cmpne\n                                       : Intrinsic::aarch64_sve_cmpne_wide,\n                         OverloadedTy);\n    Value *Call =\n        Builder.CreateCall(F, {Pred, LD1RQ, EmitSVEDupX(Builder.getInt64(0))});\n    return EmitSVEPredicateCast(Call, cast<llvm::ScalableVectorType>(Ty));\n  }\n\n  case SVE::BI__builtin_sve_svpfalse_b:\n    return ConstantInt::getFalse(Ty);\n\n  case SVE::BI__builtin_sve_svlen_bf16:\n  case SVE::BI__builtin_sve_svlen_f16:\n  case SVE::BI__builtin_sve_svlen_f32:\n  case SVE::BI__builtin_sve_svlen_f64:\n  case SVE::BI__builtin_sve_svlen_s8:\n  case SVE::BI__builtin_sve_svlen_s16:\n  case SVE::BI__builtin_sve_svlen_s32:\n  case SVE::BI__builtin_sve_svlen_s64:\n  case SVE::BI__builtin_sve_svlen_u8:\n  case SVE::BI__builtin_sve_svlen_u16:\n  case SVE::BI__builtin_sve_svlen_u32:\n  case SVE::BI__builtin_sve_svlen_u64: {\n    SVETypeFlags TF(Builtin->TypeModifier);\n    auto VTy = cast<llvm::VectorType>(getSVEType(TF));\n    auto *NumEls =\n        llvm::ConstantInt::get(Ty, VTy->getElementCount().getKnownMinValue());\n\n    Function *F = CGM.getIntrinsic(Intrinsic::vscale, Ty);\n    return Builder.CreateMul(NumEls, Builder.CreateCall(F));\n  }\n\n  case SVE::BI__builtin_sve_svtbl2_u8:\n  case SVE::BI__builtin_sve_svtbl2_s8:\n  case SVE::BI__builtin_sve_svtbl2_u16:\n  case SVE::BI__builtin_sve_svtbl2_s16:\n  case SVE::BI__builtin_sve_svtbl2_u32:\n  case SVE::BI__builtin_sve_svtbl2_s32:\n  case SVE::BI__builtin_sve_svtbl2_u64:\n  case SVE::BI__builtin_sve_svtbl2_s64:\n  case SVE::BI__builtin_sve_svtbl2_f16:\n  case SVE::BI__builtin_sve_svtbl2_bf16:\n  case SVE::BI__builtin_sve_svtbl2_f32:\n  case SVE::BI__builtin_sve_svtbl2_f64: {\n    SVETypeFlags TF(Builtin->TypeModifier);\n    auto VTy = cast<llvm::VectorType>(getSVEType(TF));\n    auto TupleTy = llvm::VectorType::getDoubleElementsVectorType(VTy);\n    Function *FExtr =\n        CGM.getIntrinsic(Intrinsic::aarch64_sve_tuple_get, {VTy, TupleTy});\n    Value *V0 = Builder.CreateCall(FExtr, {Ops[0], Builder.getInt32(0)});\n    Value *V1 = Builder.CreateCall(FExtr, {Ops[0], Builder.getInt32(1)});\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_sve_tbl2, VTy);\n    return Builder.CreateCall(F, {V0, V1, Ops[1]});\n  }\n  }\n\n  /// Should not happen\n  return nullptr;\n}\n\nValue *CodeGenFunction::EmitAArch64BuiltinExpr(unsigned BuiltinID,\n                                               const CallExpr *E,\n                                               llvm::Triple::ArchType Arch) {\n  if (BuiltinID >= AArch64::FirstSVEBuiltin &&\n      BuiltinID <= AArch64::LastSVEBuiltin)\n    return EmitAArch64SVEBuiltinExpr(BuiltinID, E);\n\n  unsigned HintID = static_cast<unsigned>(-1);\n  switch (BuiltinID) {\n  default: break;\n  case AArch64::BI__builtin_arm_nop:\n    HintID = 0;\n    break;\n  case AArch64::BI__builtin_arm_yield:\n  case AArch64::BI__yield:\n    HintID = 1;\n    break;\n  case AArch64::BI__builtin_arm_wfe:\n  case AArch64::BI__wfe:\n    HintID = 2;\n    break;\n  case AArch64::BI__builtin_arm_wfi:\n  case AArch64::BI__wfi:\n    HintID = 3;\n    break;\n  case AArch64::BI__builtin_arm_sev:\n  case AArch64::BI__sev:\n    HintID = 4;\n    break;\n  case AArch64::BI__builtin_arm_sevl:\n  case AArch64::BI__sevl:\n    HintID = 5;\n    break;\n  }\n\n  if (HintID != static_cast<unsigned>(-1)) {\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_hint);\n    return Builder.CreateCall(F, llvm::ConstantInt::get(Int32Ty, HintID));\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_prefetch) {\n    Value *Address         = EmitScalarExpr(E->getArg(0));\n    Value *RW              = EmitScalarExpr(E->getArg(1));\n    Value *CacheLevel      = EmitScalarExpr(E->getArg(2));\n    Value *RetentionPolicy = EmitScalarExpr(E->getArg(3));\n    Value *IsData          = EmitScalarExpr(E->getArg(4));\n\n    Value *Locality = nullptr;\n    if (cast<llvm::ConstantInt>(RetentionPolicy)->isZero()) {\n      // Temporal fetch, needs to convert cache level to locality.\n      Locality = llvm::ConstantInt::get(Int32Ty,\n        -cast<llvm::ConstantInt>(CacheLevel)->getValue() + 3);\n    } else {\n      // Streaming fetch.\n      Locality = llvm::ConstantInt::get(Int32Ty, 0);\n    }\n\n    // FIXME: We need AArch64 specific LLVM intrinsic if we want to specify\n    // PLDL3STRM or PLDL2STRM.\n    Function *F = CGM.getIntrinsic(Intrinsic::prefetch, Address->getType());\n    return Builder.CreateCall(F, {Address, RW, Locality, IsData});\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_rbit) {\n    assert((getContext().getTypeSize(E->getType()) == 32) &&\n           \"rbit of unusual size!\");\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(\n        CGM.getIntrinsic(Intrinsic::bitreverse, Arg->getType()), Arg, \"rbit\");\n  }\n  if (BuiltinID == AArch64::BI__builtin_arm_rbit64) {\n    assert((getContext().getTypeSize(E->getType()) == 64) &&\n           \"rbit of unusual size!\");\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(\n        CGM.getIntrinsic(Intrinsic::bitreverse, Arg->getType()), Arg, \"rbit\");\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_cls) {\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::aarch64_cls), Arg,\n                              \"cls\");\n  }\n  if (BuiltinID == AArch64::BI__builtin_arm_cls64) {\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::aarch64_cls64), Arg,\n                              \"cls\");\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_jcvt) {\n    assert((getContext().getTypeSize(E->getType()) == 32) &&\n           \"__jcvt of unusual size!\");\n    llvm::Value *Arg = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(\n        CGM.getIntrinsic(Intrinsic::aarch64_fjcvtzs), Arg);\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_ld64b ||\n      BuiltinID == AArch64::BI__builtin_arm_st64b ||\n      BuiltinID == AArch64::BI__builtin_arm_st64bv ||\n      BuiltinID == AArch64::BI__builtin_arm_st64bv0) {\n    llvm::Value *MemAddr = EmitScalarExpr(E->getArg(0));\n    llvm::Value *ValPtr = EmitScalarExpr(E->getArg(1));\n\n    if (BuiltinID == AArch64::BI__builtin_arm_ld64b) {\n      // Load from the address via an LLVM intrinsic, receiving a\n      // tuple of 8 i64 words, and store each one to ValPtr.\n      Function *F = CGM.getIntrinsic(Intrinsic::aarch64_ld64b);\n      llvm::Value *Val = Builder.CreateCall(F, MemAddr);\n      llvm::Value *ToRet;\n      for (size_t i = 0; i < 8; i++) {\n        llvm::Value *ValOffsetPtr = Builder.CreateGEP(ValPtr, Builder.getInt32(i));\n        Address Addr(ValOffsetPtr, CharUnits::fromQuantity(8));\n        ToRet = Builder.CreateStore(Builder.CreateExtractValue(Val, i), Addr);\n      }\n      return ToRet;\n    } else {\n      // Load 8 i64 words from ValPtr, and store them to the address\n      // via an LLVM intrinsic.\n      SmallVector<llvm::Value *, 9> Args;\n      Args.push_back(MemAddr);\n      for (size_t i = 0; i < 8; i++) {\n        llvm::Value *ValOffsetPtr = Builder.CreateGEP(ValPtr, Builder.getInt32(i));\n        Address Addr(ValOffsetPtr, CharUnits::fromQuantity(8));\n        Args.push_back(Builder.CreateLoad(Addr));\n      }\n\n      auto Intr = (BuiltinID == AArch64::BI__builtin_arm_st64b\n                       ? Intrinsic::aarch64_st64b\n                       : BuiltinID == AArch64::BI__builtin_arm_st64bv\n                             ? Intrinsic::aarch64_st64bv\n                             : Intrinsic::aarch64_st64bv0);\n      Function *F = CGM.getIntrinsic(Intr);\n      return Builder.CreateCall(F, Args);\n    }\n  }\n\n  if (BuiltinID == AArch64::BI__clear_cache) {\n    assert(E->getNumArgs() == 2 && \"__clear_cache takes 2 arguments\");\n    const FunctionDecl *FD = E->getDirectCallee();\n    Value *Ops[2];\n    for (unsigned i = 0; i < 2; i++)\n      Ops[i] = EmitScalarExpr(E->getArg(i));\n    llvm::Type *Ty = CGM.getTypes().ConvertType(FD->getType());\n    llvm::FunctionType *FTy = cast<llvm::FunctionType>(Ty);\n    StringRef Name = FD->getName();\n    return EmitNounwindRuntimeCall(CGM.CreateRuntimeFunction(FTy, Name), Ops);\n  }\n\n  if ((BuiltinID == AArch64::BI__builtin_arm_ldrex ||\n      BuiltinID == AArch64::BI__builtin_arm_ldaex) &&\n      getContext().getTypeSize(E->getType()) == 128) {\n    Function *F = CGM.getIntrinsic(BuiltinID == AArch64::BI__builtin_arm_ldaex\n                                       ? Intrinsic::aarch64_ldaxp\n                                       : Intrinsic::aarch64_ldxp);\n\n    Value *LdPtr = EmitScalarExpr(E->getArg(0));\n    Value *Val = Builder.CreateCall(F, Builder.CreateBitCast(LdPtr, Int8PtrTy),\n                                    \"ldxp\");\n\n    Value *Val0 = Builder.CreateExtractValue(Val, 1);\n    Value *Val1 = Builder.CreateExtractValue(Val, 0);\n    llvm::Type *Int128Ty = llvm::IntegerType::get(getLLVMContext(), 128);\n    Val0 = Builder.CreateZExt(Val0, Int128Ty);\n    Val1 = Builder.CreateZExt(Val1, Int128Ty);\n\n    Value *ShiftCst = llvm::ConstantInt::get(Int128Ty, 64);\n    Val = Builder.CreateShl(Val0, ShiftCst, \"shl\", true /* nuw */);\n    Val = Builder.CreateOr(Val, Val1);\n    return Builder.CreateBitCast(Val, ConvertType(E->getType()));\n  } else if (BuiltinID == AArch64::BI__builtin_arm_ldrex ||\n             BuiltinID == AArch64::BI__builtin_arm_ldaex) {\n    Value *LoadAddr = EmitScalarExpr(E->getArg(0));\n\n    QualType Ty = E->getType();\n    llvm::Type *RealResTy = ConvertType(Ty);\n    llvm::Type *PtrTy = llvm::IntegerType::get(\n        getLLVMContext(), getContext().getTypeSize(Ty))->getPointerTo();\n    LoadAddr = Builder.CreateBitCast(LoadAddr, PtrTy);\n\n    Function *F = CGM.getIntrinsic(BuiltinID == AArch64::BI__builtin_arm_ldaex\n                                       ? Intrinsic::aarch64_ldaxr\n                                       : Intrinsic::aarch64_ldxr,\n                                   PtrTy);\n    Value *Val = Builder.CreateCall(F, LoadAddr, \"ldxr\");\n\n    if (RealResTy->isPointerTy())\n      return Builder.CreateIntToPtr(Val, RealResTy);\n\n    llvm::Type *IntResTy = llvm::IntegerType::get(\n        getLLVMContext(), CGM.getDataLayout().getTypeSizeInBits(RealResTy));\n    Val = Builder.CreateTruncOrBitCast(Val, IntResTy);\n    return Builder.CreateBitCast(Val, RealResTy);\n  }\n\n  if ((BuiltinID == AArch64::BI__builtin_arm_strex ||\n       BuiltinID == AArch64::BI__builtin_arm_stlex) &&\n      getContext().getTypeSize(E->getArg(0)->getType()) == 128) {\n    Function *F = CGM.getIntrinsic(BuiltinID == AArch64::BI__builtin_arm_stlex\n                                       ? Intrinsic::aarch64_stlxp\n                                       : Intrinsic::aarch64_stxp);\n    llvm::Type *STy = llvm::StructType::get(Int64Ty, Int64Ty);\n\n    Address Tmp = CreateMemTemp(E->getArg(0)->getType());\n    EmitAnyExprToMem(E->getArg(0), Tmp, Qualifiers(), /*init*/ true);\n\n    Tmp = Builder.CreateBitCast(Tmp, llvm::PointerType::getUnqual(STy));\n    llvm::Value *Val = Builder.CreateLoad(Tmp);\n\n    Value *Arg0 = Builder.CreateExtractValue(Val, 0);\n    Value *Arg1 = Builder.CreateExtractValue(Val, 1);\n    Value *StPtr = Builder.CreateBitCast(EmitScalarExpr(E->getArg(1)),\n                                         Int8PtrTy);\n    return Builder.CreateCall(F, {Arg0, Arg1, StPtr}, \"stxp\");\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_strex ||\n      BuiltinID == AArch64::BI__builtin_arm_stlex) {\n    Value *StoreVal = EmitScalarExpr(E->getArg(0));\n    Value *StoreAddr = EmitScalarExpr(E->getArg(1));\n\n    QualType Ty = E->getArg(0)->getType();\n    llvm::Type *StoreTy = llvm::IntegerType::get(getLLVMContext(),\n                                                 getContext().getTypeSize(Ty));\n    StoreAddr = Builder.CreateBitCast(StoreAddr, StoreTy->getPointerTo());\n\n    if (StoreVal->getType()->isPointerTy())\n      StoreVal = Builder.CreatePtrToInt(StoreVal, Int64Ty);\n    else {\n      llvm::Type *IntTy = llvm::IntegerType::get(\n          getLLVMContext(),\n          CGM.getDataLayout().getTypeSizeInBits(StoreVal->getType()));\n      StoreVal = Builder.CreateBitCast(StoreVal, IntTy);\n      StoreVal = Builder.CreateZExtOrBitCast(StoreVal, Int64Ty);\n    }\n\n    Function *F = CGM.getIntrinsic(BuiltinID == AArch64::BI__builtin_arm_stlex\n                                       ? Intrinsic::aarch64_stlxr\n                                       : Intrinsic::aarch64_stxr,\n                                   StoreAddr->getType());\n    return Builder.CreateCall(F, {StoreVal, StoreAddr}, \"stxr\");\n  }\n\n  if (BuiltinID == AArch64::BI__getReg) {\n    Expr::EvalResult Result;\n    if (!E->getArg(0)->EvaluateAsInt(Result, CGM.getContext()))\n      llvm_unreachable(\"Sema will ensure that the parameter is constant\");\n\n    llvm::APSInt Value = Result.Val.getInt();\n    LLVMContext &Context = CGM.getLLVMContext();\n    std::string Reg = Value == 31 ? \"sp\" : \"x\" + Value.toString(10);\n\n    llvm::Metadata *Ops[] = {llvm::MDString::get(Context, Reg)};\n    llvm::MDNode *RegName = llvm::MDNode::get(Context, Ops);\n    llvm::Value *Metadata = llvm::MetadataAsValue::get(Context, RegName);\n\n    llvm::Function *F =\n        CGM.getIntrinsic(llvm::Intrinsic::read_register, {Int64Ty});\n    return Builder.CreateCall(F, Metadata);\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_clrex) {\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_clrex);\n    return Builder.CreateCall(F);\n  }\n\n  if (BuiltinID == AArch64::BI_ReadWriteBarrier)\n    return Builder.CreateFence(llvm::AtomicOrdering::SequentiallyConsistent,\n                               llvm::SyncScope::SingleThread);\n\n  // CRC32\n  Intrinsic::ID CRCIntrinsicID = Intrinsic::not_intrinsic;\n  switch (BuiltinID) {\n  case AArch64::BI__builtin_arm_crc32b:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32b; break;\n  case AArch64::BI__builtin_arm_crc32cb:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32cb; break;\n  case AArch64::BI__builtin_arm_crc32h:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32h; break;\n  case AArch64::BI__builtin_arm_crc32ch:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32ch; break;\n  case AArch64::BI__builtin_arm_crc32w:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32w; break;\n  case AArch64::BI__builtin_arm_crc32cw:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32cw; break;\n  case AArch64::BI__builtin_arm_crc32d:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32x; break;\n  case AArch64::BI__builtin_arm_crc32cd:\n    CRCIntrinsicID = Intrinsic::aarch64_crc32cx; break;\n  }\n\n  if (CRCIntrinsicID != Intrinsic::not_intrinsic) {\n    Value *Arg0 = EmitScalarExpr(E->getArg(0));\n    Value *Arg1 = EmitScalarExpr(E->getArg(1));\n    Function *F = CGM.getIntrinsic(CRCIntrinsicID);\n\n    llvm::Type *DataTy = F->getFunctionType()->getParamType(1);\n    Arg1 = Builder.CreateZExtOrBitCast(Arg1, DataTy);\n\n    return Builder.CreateCall(F, {Arg0, Arg1});\n  }\n\n  // Memory Tagging Extensions (MTE) Intrinsics\n  Intrinsic::ID MTEIntrinsicID = Intrinsic::not_intrinsic;\n  switch (BuiltinID) {\n  case AArch64::BI__builtin_arm_irg:\n    MTEIntrinsicID = Intrinsic::aarch64_irg; break;\n  case  AArch64::BI__builtin_arm_addg:\n    MTEIntrinsicID = Intrinsic::aarch64_addg; break;\n  case  AArch64::BI__builtin_arm_gmi:\n    MTEIntrinsicID = Intrinsic::aarch64_gmi; break;\n  case  AArch64::BI__builtin_arm_ldg:\n    MTEIntrinsicID = Intrinsic::aarch64_ldg; break;\n  case AArch64::BI__builtin_arm_stg:\n    MTEIntrinsicID = Intrinsic::aarch64_stg; break;\n  case AArch64::BI__builtin_arm_subp:\n    MTEIntrinsicID = Intrinsic::aarch64_subp; break;\n  }\n\n  if (MTEIntrinsicID != Intrinsic::not_intrinsic) {\n    llvm::Type *T = ConvertType(E->getType());\n\n    if (MTEIntrinsicID == Intrinsic::aarch64_irg) {\n      Value *Pointer = EmitScalarExpr(E->getArg(0));\n      Value *Mask = EmitScalarExpr(E->getArg(1));\n\n      Pointer = Builder.CreatePointerCast(Pointer, Int8PtrTy);\n      Mask = Builder.CreateZExt(Mask, Int64Ty);\n      Value *RV = Builder.CreateCall(\n                       CGM.getIntrinsic(MTEIntrinsicID), {Pointer, Mask});\n       return Builder.CreatePointerCast(RV, T);\n    }\n    if (MTEIntrinsicID == Intrinsic::aarch64_addg) {\n      Value *Pointer = EmitScalarExpr(E->getArg(0));\n      Value *TagOffset = EmitScalarExpr(E->getArg(1));\n\n      Pointer = Builder.CreatePointerCast(Pointer, Int8PtrTy);\n      TagOffset = Builder.CreateZExt(TagOffset, Int64Ty);\n      Value *RV = Builder.CreateCall(\n                       CGM.getIntrinsic(MTEIntrinsicID), {Pointer, TagOffset});\n      return Builder.CreatePointerCast(RV, T);\n    }\n    if (MTEIntrinsicID == Intrinsic::aarch64_gmi) {\n      Value *Pointer = EmitScalarExpr(E->getArg(0));\n      Value *ExcludedMask = EmitScalarExpr(E->getArg(1));\n\n      ExcludedMask = Builder.CreateZExt(ExcludedMask, Int64Ty);\n      Pointer = Builder.CreatePointerCast(Pointer, Int8PtrTy);\n      return Builder.CreateCall(\n                       CGM.getIntrinsic(MTEIntrinsicID), {Pointer, ExcludedMask});\n    }\n    // Although it is possible to supply a different return\n    // address (first arg) to this intrinsic, for now we set\n    // return address same as input address.\n    if (MTEIntrinsicID == Intrinsic::aarch64_ldg) {\n      Value *TagAddress = EmitScalarExpr(E->getArg(0));\n      TagAddress = Builder.CreatePointerCast(TagAddress, Int8PtrTy);\n      Value *RV = Builder.CreateCall(\n                    CGM.getIntrinsic(MTEIntrinsicID), {TagAddress, TagAddress});\n      return Builder.CreatePointerCast(RV, T);\n    }\n    // Although it is possible to supply a different tag (to set)\n    // to this intrinsic (as first arg), for now we supply\n    // the tag that is in input address arg (common use case).\n    if (MTEIntrinsicID == Intrinsic::aarch64_stg) {\n        Value *TagAddress = EmitScalarExpr(E->getArg(0));\n        TagAddress = Builder.CreatePointerCast(TagAddress, Int8PtrTy);\n        return Builder.CreateCall(\n                 CGM.getIntrinsic(MTEIntrinsicID), {TagAddress, TagAddress});\n    }\n    if (MTEIntrinsicID == Intrinsic::aarch64_subp) {\n      Value *PointerA = EmitScalarExpr(E->getArg(0));\n      Value *PointerB = EmitScalarExpr(E->getArg(1));\n      PointerA = Builder.CreatePointerCast(PointerA, Int8PtrTy);\n      PointerB = Builder.CreatePointerCast(PointerB, Int8PtrTy);\n      return Builder.CreateCall(\n                       CGM.getIntrinsic(MTEIntrinsicID), {PointerA, PointerB});\n    }\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_arm_rsr ||\n      BuiltinID == AArch64::BI__builtin_arm_rsr64 ||\n      BuiltinID == AArch64::BI__builtin_arm_rsrp ||\n      BuiltinID == AArch64::BI__builtin_arm_wsr ||\n      BuiltinID == AArch64::BI__builtin_arm_wsr64 ||\n      BuiltinID == AArch64::BI__builtin_arm_wsrp) {\n\n    SpecialRegisterAccessKind AccessKind = Write;\n    if (BuiltinID == AArch64::BI__builtin_arm_rsr ||\n        BuiltinID == AArch64::BI__builtin_arm_rsr64 ||\n        BuiltinID == AArch64::BI__builtin_arm_rsrp)\n      AccessKind = VolatileRead;\n\n    bool IsPointerBuiltin = BuiltinID == AArch64::BI__builtin_arm_rsrp ||\n                            BuiltinID == AArch64::BI__builtin_arm_wsrp;\n\n    bool Is64Bit = BuiltinID != AArch64::BI__builtin_arm_rsr &&\n                   BuiltinID != AArch64::BI__builtin_arm_wsr;\n\n    llvm::Type *ValueType;\n    llvm::Type *RegisterType = Int64Ty;\n    if (IsPointerBuiltin) {\n      ValueType = VoidPtrTy;\n    } else if (Is64Bit) {\n      ValueType = Int64Ty;\n    } else {\n      ValueType = Int32Ty;\n    }\n\n    return EmitSpecialRegisterBuiltin(*this, E, RegisterType, ValueType,\n                                      AccessKind);\n  }\n\n  if (BuiltinID == AArch64::BI_ReadStatusReg ||\n      BuiltinID == AArch64::BI_WriteStatusReg) {\n    LLVMContext &Context = CGM.getLLVMContext();\n\n    unsigned SysReg =\n      E->getArg(0)->EvaluateKnownConstInt(getContext()).getZExtValue();\n\n    std::string SysRegStr;\n    llvm::raw_string_ostream(SysRegStr) <<\n                       ((1 << 1) | ((SysReg >> 14) & 1))  << \":\" <<\n                       ((SysReg >> 11) & 7)               << \":\" <<\n                       ((SysReg >> 7)  & 15)              << \":\" <<\n                       ((SysReg >> 3)  & 15)              << \":\" <<\n                       ( SysReg        & 7);\n\n    llvm::Metadata *Ops[] = { llvm::MDString::get(Context, SysRegStr) };\n    llvm::MDNode *RegName = llvm::MDNode::get(Context, Ops);\n    llvm::Value *Metadata = llvm::MetadataAsValue::get(Context, RegName);\n\n    llvm::Type *RegisterType = Int64Ty;\n    llvm::Type *Types[] = { RegisterType };\n\n    if (BuiltinID == AArch64::BI_ReadStatusReg) {\n      llvm::Function *F = CGM.getIntrinsic(llvm::Intrinsic::read_register, Types);\n\n      return Builder.CreateCall(F, Metadata);\n    }\n\n    llvm::Function *F = CGM.getIntrinsic(llvm::Intrinsic::write_register, Types);\n    llvm::Value *ArgValue = EmitScalarExpr(E->getArg(1));\n\n    return Builder.CreateCall(F, { Metadata, ArgValue });\n  }\n\n  if (BuiltinID == AArch64::BI_AddressOfReturnAddress) {\n    llvm::Function *F =\n        CGM.getIntrinsic(Intrinsic::addressofreturnaddress, AllocaInt8PtrTy);\n    return Builder.CreateCall(F);\n  }\n\n  if (BuiltinID == AArch64::BI__builtin_sponentry) {\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::sponentry, AllocaInt8PtrTy);\n    return Builder.CreateCall(F);\n  }\n\n  // Handle MSVC intrinsics before argument evaluation to prevent double\n  // evaluation.\n  if (Optional<MSVCIntrin> MsvcIntId = translateAarch64ToMsvcIntrin(BuiltinID))\n    return EmitMSVCBuiltinExpr(*MsvcIntId, E);\n\n  // Find out if any arguments are required to be integer constant\n  // expressions.\n  unsigned ICEArguments = 0;\n  ASTContext::GetBuiltinTypeError Error;\n  getContext().GetBuiltinType(BuiltinID, Error, &ICEArguments);\n  assert(Error == ASTContext::GE_None && \"Should not codegen an error\");\n\n  llvm::SmallVector<Value*, 4> Ops;\n  Address PtrOp0 = Address::invalid();\n  for (unsigned i = 0, e = E->getNumArgs() - 1; i != e; i++) {\n    if (i == 0) {\n      switch (BuiltinID) {\n      case NEON::BI__builtin_neon_vld1_v:\n      case NEON::BI__builtin_neon_vld1q_v:\n      case NEON::BI__builtin_neon_vld1_dup_v:\n      case NEON::BI__builtin_neon_vld1q_dup_v:\n      case NEON::BI__builtin_neon_vld1_lane_v:\n      case NEON::BI__builtin_neon_vld1q_lane_v:\n      case NEON::BI__builtin_neon_vst1_v:\n      case NEON::BI__builtin_neon_vst1q_v:\n      case NEON::BI__builtin_neon_vst1_lane_v:\n      case NEON::BI__builtin_neon_vst1q_lane_v:\n        // Get the alignment for the argument in addition to the value;\n        // we'll use it later.\n        PtrOp0 = EmitPointerWithAlignment(E->getArg(0));\n        Ops.push_back(PtrOp0.getPointer());\n        continue;\n      }\n    }\n    if ((ICEArguments & (1 << i)) == 0) {\n      Ops.push_back(EmitScalarExpr(E->getArg(i)));\n    } else {\n      // If this is required to be a constant, constant fold it so that we know\n      // that the generated intrinsic gets a ConstantInt.\n      Ops.push_back(llvm::ConstantInt::get(\n          getLLVMContext(),\n          *E->getArg(i)->getIntegerConstantExpr(getContext())));\n    }\n  }\n\n  auto SISDMap = makeArrayRef(AArch64SISDIntrinsicMap);\n  const ARMVectorIntrinsicInfo *Builtin = findARMVectorIntrinsicInMap(\n      SISDMap, BuiltinID, AArch64SISDIntrinsicsProvenSorted);\n\n  if (Builtin) {\n    Ops.push_back(EmitScalarExpr(E->getArg(E->getNumArgs() - 1)));\n    Value *Result = EmitCommonNeonSISDBuiltinExpr(*this, *Builtin, Ops, E);\n    assert(Result && \"SISD intrinsic should have been handled\");\n    return Result;\n  }\n\n  const Expr *Arg = E->getArg(E->getNumArgs()-1);\n  NeonTypeFlags Type(0);\n  if (Optional<llvm::APSInt> Result = Arg->getIntegerConstantExpr(getContext()))\n    // Determine the type of this overloaded NEON intrinsic.\n    Type = NeonTypeFlags(Result->getZExtValue());\n\n  bool usgn = Type.isUnsigned();\n  bool quad = Type.isQuad();\n\n  // Handle non-overloaded intrinsics first.\n  switch (BuiltinID) {\n  default: break;\n  case NEON::BI__builtin_neon_vabsh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::fabs, HalfTy), Ops, \"vabs\");\n  case NEON::BI__builtin_neon_vldrq_p128: {\n    llvm::Type *Int128Ty = llvm::Type::getIntNTy(getLLVMContext(), 128);\n    llvm::Type *Int128PTy = llvm::PointerType::get(Int128Ty, 0);\n    Value *Ptr = Builder.CreateBitCast(EmitScalarExpr(E->getArg(0)), Int128PTy);\n    return Builder.CreateAlignedLoad(Int128Ty, Ptr,\n                                     CharUnits::fromQuantity(16));\n  }\n  case NEON::BI__builtin_neon_vstrq_p128: {\n    llvm::Type *Int128PTy = llvm::Type::getIntNPtrTy(getLLVMContext(), 128);\n    Value *Ptr = Builder.CreateBitCast(Ops[0], Int128PTy);\n    return Builder.CreateDefaultAlignedStore(EmitScalarExpr(E->getArg(1)), Ptr);\n  }\n  case NEON::BI__builtin_neon_vcvts_f32_u32:\n  case NEON::BI__builtin_neon_vcvtd_f64_u64:\n    usgn = true;\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vcvts_f32_s32:\n  case NEON::BI__builtin_neon_vcvtd_f64_s64: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    bool Is64 = Ops[0]->getType()->getPrimitiveSizeInBits() == 64;\n    llvm::Type *InTy = Is64 ? Int64Ty : Int32Ty;\n    llvm::Type *FTy = Is64 ? DoubleTy : FloatTy;\n    Ops[0] = Builder.CreateBitCast(Ops[0], InTy);\n    if (usgn)\n      return Builder.CreateUIToFP(Ops[0], FTy);\n    return Builder.CreateSIToFP(Ops[0], FTy);\n  }\n  case NEON::BI__builtin_neon_vcvth_f16_u16:\n  case NEON::BI__builtin_neon_vcvth_f16_u32:\n  case NEON::BI__builtin_neon_vcvth_f16_u64:\n    usgn = true;\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vcvth_f16_s16:\n  case NEON::BI__builtin_neon_vcvth_f16_s32:\n  case NEON::BI__builtin_neon_vcvth_f16_s64: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    llvm::Type *FTy = HalfTy;\n    llvm::Type *InTy;\n    if (Ops[0]->getType()->getPrimitiveSizeInBits() == 64)\n      InTy = Int64Ty;\n    else if (Ops[0]->getType()->getPrimitiveSizeInBits() == 32)\n      InTy = Int32Ty;\n    else\n      InTy = Int16Ty;\n    Ops[0] = Builder.CreateBitCast(Ops[0], InTy);\n    if (usgn)\n      return Builder.CreateUIToFP(Ops[0], FTy);\n    return Builder.CreateSIToFP(Ops[0], FTy);\n  }\n  case NEON::BI__builtin_neon_vcvtah_u16_f16:\n  case NEON::BI__builtin_neon_vcvtmh_u16_f16:\n  case NEON::BI__builtin_neon_vcvtnh_u16_f16:\n  case NEON::BI__builtin_neon_vcvtph_u16_f16:\n  case NEON::BI__builtin_neon_vcvth_u16_f16:\n  case NEON::BI__builtin_neon_vcvtah_s16_f16:\n  case NEON::BI__builtin_neon_vcvtmh_s16_f16:\n  case NEON::BI__builtin_neon_vcvtnh_s16_f16:\n  case NEON::BI__builtin_neon_vcvtph_s16_f16:\n  case NEON::BI__builtin_neon_vcvth_s16_f16: {\n    unsigned Int;\n    llvm::Type* InTy = Int32Ty;\n    llvm::Type* FTy  = HalfTy;\n    llvm::Type *Tys[2] = {InTy, FTy};\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vcvtah_u16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtau; break;\n    case NEON::BI__builtin_neon_vcvtmh_u16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtmu; break;\n    case NEON::BI__builtin_neon_vcvtnh_u16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtnu; break;\n    case NEON::BI__builtin_neon_vcvtph_u16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtpu; break;\n    case NEON::BI__builtin_neon_vcvth_u16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtzu; break;\n    case NEON::BI__builtin_neon_vcvtah_s16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtas; break;\n    case NEON::BI__builtin_neon_vcvtmh_s16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtms; break;\n    case NEON::BI__builtin_neon_vcvtnh_s16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtns; break;\n    case NEON::BI__builtin_neon_vcvtph_s16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtps; break;\n    case NEON::BI__builtin_neon_vcvth_s16_f16:\n      Int = Intrinsic::aarch64_neon_fcvtzs; break;\n    }\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"fcvt\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vcaleh_f16:\n  case NEON::BI__builtin_neon_vcalth_f16:\n  case NEON::BI__builtin_neon_vcageh_f16:\n  case NEON::BI__builtin_neon_vcagth_f16: {\n    unsigned Int;\n    llvm::Type* InTy = Int32Ty;\n    llvm::Type* FTy  = HalfTy;\n    llvm::Type *Tys[2] = {InTy, FTy};\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vcageh_f16:\n      Int = Intrinsic::aarch64_neon_facge; break;\n    case NEON::BI__builtin_neon_vcagth_f16:\n      Int = Intrinsic::aarch64_neon_facgt; break;\n    case NEON::BI__builtin_neon_vcaleh_f16:\n      Int = Intrinsic::aarch64_neon_facge; std::swap(Ops[0], Ops[1]); break;\n    case NEON::BI__builtin_neon_vcalth_f16:\n      Int = Intrinsic::aarch64_neon_facgt; std::swap(Ops[0], Ops[1]); break;\n    }\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"facg\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vcvth_n_s16_f16:\n  case NEON::BI__builtin_neon_vcvth_n_u16_f16: {\n    unsigned Int;\n    llvm::Type* InTy = Int32Ty;\n    llvm::Type* FTy  = HalfTy;\n    llvm::Type *Tys[2] = {InTy, FTy};\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vcvth_n_s16_f16:\n      Int = Intrinsic::aarch64_neon_vcvtfp2fxs; break;\n    case NEON::BI__builtin_neon_vcvth_n_u16_f16:\n      Int = Intrinsic::aarch64_neon_vcvtfp2fxu; break;\n    }\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"fcvth_n\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vcvth_n_f16_s16:\n  case NEON::BI__builtin_neon_vcvth_n_f16_u16: {\n    unsigned Int;\n    llvm::Type* FTy  = HalfTy;\n    llvm::Type* InTy = Int32Ty;\n    llvm::Type *Tys[2] = {FTy, InTy};\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vcvth_n_f16_s16:\n      Int = Intrinsic::aarch64_neon_vcvtfxs2fp;\n      Ops[0] = Builder.CreateSExt(Ops[0], InTy, \"sext\");\n      break;\n    case NEON::BI__builtin_neon_vcvth_n_f16_u16:\n      Int = Intrinsic::aarch64_neon_vcvtfxu2fp;\n      Ops[0] = Builder.CreateZExt(Ops[0], InTy);\n      break;\n    }\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"fcvth_n\");\n  }\n  case NEON::BI__builtin_neon_vpaddd_s64: {\n    auto *Ty = llvm::FixedVectorType::get(Int64Ty, 2);\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    // The vector is v2f64, so make sure it's bitcast to that.\n    Vec = Builder.CreateBitCast(Vec, Ty, \"v2i64\");\n    llvm::Value *Idx0 = llvm::ConstantInt::get(SizeTy, 0);\n    llvm::Value *Idx1 = llvm::ConstantInt::get(SizeTy, 1);\n    Value *Op0 = Builder.CreateExtractElement(Vec, Idx0, \"lane0\");\n    Value *Op1 = Builder.CreateExtractElement(Vec, Idx1, \"lane1\");\n    // Pairwise addition of a v2f64 into a scalar f64.\n    return Builder.CreateAdd(Op0, Op1, \"vpaddd\");\n  }\n  case NEON::BI__builtin_neon_vpaddd_f64: {\n    auto *Ty = llvm::FixedVectorType::get(DoubleTy, 2);\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    // The vector is v2f64, so make sure it's bitcast to that.\n    Vec = Builder.CreateBitCast(Vec, Ty, \"v2f64\");\n    llvm::Value *Idx0 = llvm::ConstantInt::get(SizeTy, 0);\n    llvm::Value *Idx1 = llvm::ConstantInt::get(SizeTy, 1);\n    Value *Op0 = Builder.CreateExtractElement(Vec, Idx0, \"lane0\");\n    Value *Op1 = Builder.CreateExtractElement(Vec, Idx1, \"lane1\");\n    // Pairwise addition of a v2f64 into a scalar f64.\n    return Builder.CreateFAdd(Op0, Op1, \"vpaddd\");\n  }\n  case NEON::BI__builtin_neon_vpadds_f32: {\n    auto *Ty = llvm::FixedVectorType::get(FloatTy, 2);\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    // The vector is v2f32, so make sure it's bitcast to that.\n    Vec = Builder.CreateBitCast(Vec, Ty, \"v2f32\");\n    llvm::Value *Idx0 = llvm::ConstantInt::get(SizeTy, 0);\n    llvm::Value *Idx1 = llvm::ConstantInt::get(SizeTy, 1);\n    Value *Op0 = Builder.CreateExtractElement(Vec, Idx0, \"lane0\");\n    Value *Op1 = Builder.CreateExtractElement(Vec, Idx1, \"lane1\");\n    // Pairwise addition of a v2f32 into a scalar f32.\n    return Builder.CreateFAdd(Op0, Op1, \"vpaddd\");\n  }\n  case NEON::BI__builtin_neon_vceqzd_s64:\n  case NEON::BI__builtin_neon_vceqzd_f64:\n  case NEON::BI__builtin_neon_vceqzs_f32:\n  case NEON::BI__builtin_neon_vceqzh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitAArch64CompareBuiltinExpr(\n        Ops[0], ConvertType(E->getCallReturnType(getContext())),\n        ICmpInst::FCMP_OEQ, ICmpInst::ICMP_EQ, \"vceqz\");\n  case NEON::BI__builtin_neon_vcgezd_s64:\n  case NEON::BI__builtin_neon_vcgezd_f64:\n  case NEON::BI__builtin_neon_vcgezs_f32:\n  case NEON::BI__builtin_neon_vcgezh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitAArch64CompareBuiltinExpr(\n        Ops[0], ConvertType(E->getCallReturnType(getContext())),\n        ICmpInst::FCMP_OGE, ICmpInst::ICMP_SGE, \"vcgez\");\n  case NEON::BI__builtin_neon_vclezd_s64:\n  case NEON::BI__builtin_neon_vclezd_f64:\n  case NEON::BI__builtin_neon_vclezs_f32:\n  case NEON::BI__builtin_neon_vclezh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitAArch64CompareBuiltinExpr(\n        Ops[0], ConvertType(E->getCallReturnType(getContext())),\n        ICmpInst::FCMP_OLE, ICmpInst::ICMP_SLE, \"vclez\");\n  case NEON::BI__builtin_neon_vcgtzd_s64:\n  case NEON::BI__builtin_neon_vcgtzd_f64:\n  case NEON::BI__builtin_neon_vcgtzs_f32:\n  case NEON::BI__builtin_neon_vcgtzh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitAArch64CompareBuiltinExpr(\n        Ops[0], ConvertType(E->getCallReturnType(getContext())),\n        ICmpInst::FCMP_OGT, ICmpInst::ICMP_SGT, \"vcgtz\");\n  case NEON::BI__builtin_neon_vcltzd_s64:\n  case NEON::BI__builtin_neon_vcltzd_f64:\n  case NEON::BI__builtin_neon_vcltzs_f32:\n  case NEON::BI__builtin_neon_vcltzh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitAArch64CompareBuiltinExpr(\n        Ops[0], ConvertType(E->getCallReturnType(getContext())),\n        ICmpInst::FCMP_OLT, ICmpInst::ICMP_SLT, \"vcltz\");\n\n  case NEON::BI__builtin_neon_vceqzd_u64: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = Builder.CreateBitCast(Ops[0], Int64Ty);\n    Ops[0] =\n        Builder.CreateICmpEQ(Ops[0], llvm::Constant::getNullValue(Int64Ty));\n    return Builder.CreateSExt(Ops[0], Int64Ty, \"vceqzd\");\n  }\n  case NEON::BI__builtin_neon_vceqd_f64:\n  case NEON::BI__builtin_neon_vcled_f64:\n  case NEON::BI__builtin_neon_vcltd_f64:\n  case NEON::BI__builtin_neon_vcged_f64:\n  case NEON::BI__builtin_neon_vcgtd_f64: {\n    llvm::CmpInst::Predicate P;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vceqd_f64: P = llvm::FCmpInst::FCMP_OEQ; break;\n    case NEON::BI__builtin_neon_vcled_f64: P = llvm::FCmpInst::FCMP_OLE; break;\n    case NEON::BI__builtin_neon_vcltd_f64: P = llvm::FCmpInst::FCMP_OLT; break;\n    case NEON::BI__builtin_neon_vcged_f64: P = llvm::FCmpInst::FCMP_OGE; break;\n    case NEON::BI__builtin_neon_vcgtd_f64: P = llvm::FCmpInst::FCMP_OGT; break;\n    }\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Ops[0] = Builder.CreateBitCast(Ops[0], DoubleTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], DoubleTy);\n    Ops[0] = Builder.CreateFCmp(P, Ops[0], Ops[1]);\n    return Builder.CreateSExt(Ops[0], Int64Ty, \"vcmpd\");\n  }\n  case NEON::BI__builtin_neon_vceqs_f32:\n  case NEON::BI__builtin_neon_vcles_f32:\n  case NEON::BI__builtin_neon_vclts_f32:\n  case NEON::BI__builtin_neon_vcges_f32:\n  case NEON::BI__builtin_neon_vcgts_f32: {\n    llvm::CmpInst::Predicate P;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vceqs_f32: P = llvm::FCmpInst::FCMP_OEQ; break;\n    case NEON::BI__builtin_neon_vcles_f32: P = llvm::FCmpInst::FCMP_OLE; break;\n    case NEON::BI__builtin_neon_vclts_f32: P = llvm::FCmpInst::FCMP_OLT; break;\n    case NEON::BI__builtin_neon_vcges_f32: P = llvm::FCmpInst::FCMP_OGE; break;\n    case NEON::BI__builtin_neon_vcgts_f32: P = llvm::FCmpInst::FCMP_OGT; break;\n    }\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Ops[0] = Builder.CreateBitCast(Ops[0], FloatTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], FloatTy);\n    Ops[0] = Builder.CreateFCmp(P, Ops[0], Ops[1]);\n    return Builder.CreateSExt(Ops[0], Int32Ty, \"vcmpd\");\n  }\n  case NEON::BI__builtin_neon_vceqh_f16:\n  case NEON::BI__builtin_neon_vcleh_f16:\n  case NEON::BI__builtin_neon_vclth_f16:\n  case NEON::BI__builtin_neon_vcgeh_f16:\n  case NEON::BI__builtin_neon_vcgth_f16: {\n    llvm::CmpInst::Predicate P;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vceqh_f16: P = llvm::FCmpInst::FCMP_OEQ; break;\n    case NEON::BI__builtin_neon_vcleh_f16: P = llvm::FCmpInst::FCMP_OLE; break;\n    case NEON::BI__builtin_neon_vclth_f16: P = llvm::FCmpInst::FCMP_OLT; break;\n    case NEON::BI__builtin_neon_vcgeh_f16: P = llvm::FCmpInst::FCMP_OGE; break;\n    case NEON::BI__builtin_neon_vcgth_f16: P = llvm::FCmpInst::FCMP_OGT; break;\n    }\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Ops[0] = Builder.CreateBitCast(Ops[0], HalfTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], HalfTy);\n    Ops[0] = Builder.CreateFCmp(P, Ops[0], Ops[1]);\n    return Builder.CreateSExt(Ops[0], Int16Ty, \"vcmpd\");\n  }\n  case NEON::BI__builtin_neon_vceqd_s64:\n  case NEON::BI__builtin_neon_vceqd_u64:\n  case NEON::BI__builtin_neon_vcgtd_s64:\n  case NEON::BI__builtin_neon_vcgtd_u64:\n  case NEON::BI__builtin_neon_vcltd_s64:\n  case NEON::BI__builtin_neon_vcltd_u64:\n  case NEON::BI__builtin_neon_vcged_u64:\n  case NEON::BI__builtin_neon_vcged_s64:\n  case NEON::BI__builtin_neon_vcled_u64:\n  case NEON::BI__builtin_neon_vcled_s64: {\n    llvm::CmpInst::Predicate P;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"missing builtin ID in switch!\");\n    case NEON::BI__builtin_neon_vceqd_s64:\n    case NEON::BI__builtin_neon_vceqd_u64:P = llvm::ICmpInst::ICMP_EQ;break;\n    case NEON::BI__builtin_neon_vcgtd_s64:P = llvm::ICmpInst::ICMP_SGT;break;\n    case NEON::BI__builtin_neon_vcgtd_u64:P = llvm::ICmpInst::ICMP_UGT;break;\n    case NEON::BI__builtin_neon_vcltd_s64:P = llvm::ICmpInst::ICMP_SLT;break;\n    case NEON::BI__builtin_neon_vcltd_u64:P = llvm::ICmpInst::ICMP_ULT;break;\n    case NEON::BI__builtin_neon_vcged_u64:P = llvm::ICmpInst::ICMP_UGE;break;\n    case NEON::BI__builtin_neon_vcged_s64:P = llvm::ICmpInst::ICMP_SGE;break;\n    case NEON::BI__builtin_neon_vcled_u64:P = llvm::ICmpInst::ICMP_ULE;break;\n    case NEON::BI__builtin_neon_vcled_s64:P = llvm::ICmpInst::ICMP_SLE;break;\n    }\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Ops[0] = Builder.CreateBitCast(Ops[0], Int64Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Int64Ty);\n    Ops[0] = Builder.CreateICmp(P, Ops[0], Ops[1]);\n    return Builder.CreateSExt(Ops[0], Int64Ty, \"vceqd\");\n  }\n  case NEON::BI__builtin_neon_vtstd_s64:\n  case NEON::BI__builtin_neon_vtstd_u64: {\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Ops[0] = Builder.CreateBitCast(Ops[0], Int64Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Int64Ty);\n    Ops[0] = Builder.CreateAnd(Ops[0], Ops[1]);\n    Ops[0] = Builder.CreateICmp(ICmpInst::ICMP_NE, Ops[0],\n                                llvm::Constant::getNullValue(Int64Ty));\n    return Builder.CreateSExt(Ops[0], Int64Ty, \"vtstd\");\n  }\n  case NEON::BI__builtin_neon_vset_lane_i8:\n  case NEON::BI__builtin_neon_vset_lane_i16:\n  case NEON::BI__builtin_neon_vset_lane_i32:\n  case NEON::BI__builtin_neon_vset_lane_i64:\n  case NEON::BI__builtin_neon_vset_lane_bf16:\n  case NEON::BI__builtin_neon_vset_lane_f32:\n  case NEON::BI__builtin_neon_vsetq_lane_i8:\n  case NEON::BI__builtin_neon_vsetq_lane_i16:\n  case NEON::BI__builtin_neon_vsetq_lane_i32:\n  case NEON::BI__builtin_neon_vsetq_lane_i64:\n  case NEON::BI__builtin_neon_vsetq_lane_bf16:\n  case NEON::BI__builtin_neon_vsetq_lane_f32:\n    Ops.push_back(EmitScalarExpr(E->getArg(2)));\n    return Builder.CreateInsertElement(Ops[1], Ops[0], Ops[2], \"vset_lane\");\n  case NEON::BI__builtin_neon_vset_lane_f64:\n    // The vector type needs a cast for the v1f64 variant.\n    Ops[1] =\n        Builder.CreateBitCast(Ops[1], llvm::FixedVectorType::get(DoubleTy, 1));\n    Ops.push_back(EmitScalarExpr(E->getArg(2)));\n    return Builder.CreateInsertElement(Ops[1], Ops[0], Ops[2], \"vset_lane\");\n  case NEON::BI__builtin_neon_vsetq_lane_f64:\n    // The vector type needs a cast for the v2f64 variant.\n    Ops[1] =\n        Builder.CreateBitCast(Ops[1], llvm::FixedVectorType::get(DoubleTy, 2));\n    Ops.push_back(EmitScalarExpr(E->getArg(2)));\n    return Builder.CreateInsertElement(Ops[1], Ops[0], Ops[2], \"vset_lane\");\n\n  case NEON::BI__builtin_neon_vget_lane_i8:\n  case NEON::BI__builtin_neon_vdupb_lane_i8:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int8Ty, 8));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vget_lane\");\n  case NEON::BI__builtin_neon_vgetq_lane_i8:\n  case NEON::BI__builtin_neon_vdupb_laneq_i8:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int8Ty, 16));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vgetq_lane\");\n  case NEON::BI__builtin_neon_vget_lane_i16:\n  case NEON::BI__builtin_neon_vduph_lane_i16:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int16Ty, 4));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vget_lane\");\n  case NEON::BI__builtin_neon_vgetq_lane_i16:\n  case NEON::BI__builtin_neon_vduph_laneq_i16:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int16Ty, 8));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vgetq_lane\");\n  case NEON::BI__builtin_neon_vget_lane_i32:\n  case NEON::BI__builtin_neon_vdups_lane_i32:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int32Ty, 2));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vget_lane\");\n  case NEON::BI__builtin_neon_vdups_lane_f32:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(FloatTy, 2));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vdups_lane\");\n  case NEON::BI__builtin_neon_vgetq_lane_i32:\n  case NEON::BI__builtin_neon_vdups_laneq_i32:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int32Ty, 4));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vgetq_lane\");\n  case NEON::BI__builtin_neon_vget_lane_i64:\n  case NEON::BI__builtin_neon_vdupd_lane_i64:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int64Ty, 1));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vget_lane\");\n  case NEON::BI__builtin_neon_vdupd_lane_f64:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(DoubleTy, 1));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vdupd_lane\");\n  case NEON::BI__builtin_neon_vgetq_lane_i64:\n  case NEON::BI__builtin_neon_vdupd_laneq_i64:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int64Ty, 2));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vgetq_lane\");\n  case NEON::BI__builtin_neon_vget_lane_f32:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(FloatTy, 2));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vget_lane\");\n  case NEON::BI__builtin_neon_vget_lane_f64:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(DoubleTy, 1));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vget_lane\");\n  case NEON::BI__builtin_neon_vgetq_lane_f32:\n  case NEON::BI__builtin_neon_vdups_laneq_f32:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(FloatTy, 4));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vgetq_lane\");\n  case NEON::BI__builtin_neon_vgetq_lane_f64:\n  case NEON::BI__builtin_neon_vdupd_laneq_f64:\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(DoubleTy, 2));\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vgetq_lane\");\n  case NEON::BI__builtin_neon_vaddh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    return Builder.CreateFAdd(Ops[0], Ops[1], \"vaddh\");\n  case NEON::BI__builtin_neon_vsubh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    return Builder.CreateFSub(Ops[0], Ops[1], \"vsubh\");\n  case NEON::BI__builtin_neon_vmulh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    return Builder.CreateFMul(Ops[0], Ops[1], \"vmulh\");\n  case NEON::BI__builtin_neon_vdivh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    return Builder.CreateFDiv(Ops[0], Ops[1], \"vdivh\");\n  case NEON::BI__builtin_neon_vfmah_f16:\n    // NEON intrinsic puts accumulator first, unlike the LLVM fma.\n    return emitCallMaybeConstrainedFPBuiltin(\n        *this, Intrinsic::fma, Intrinsic::experimental_constrained_fma, HalfTy,\n        {EmitScalarExpr(E->getArg(1)), EmitScalarExpr(E->getArg(2)), Ops[0]});\n  case NEON::BI__builtin_neon_vfmsh_f16: {\n    // FIXME: This should be an fneg instruction:\n    Value *Zero = llvm::ConstantFP::getZeroValueForNegation(HalfTy);\n    Value* Sub = Builder.CreateFSub(Zero, EmitScalarExpr(E->getArg(1)), \"vsubh\");\n\n    // NEON intrinsic puts accumulator first, unlike the LLVM fma.\n    return emitCallMaybeConstrainedFPBuiltin(\n        *this, Intrinsic::fma, Intrinsic::experimental_constrained_fma, HalfTy,\n        {Sub, EmitScalarExpr(E->getArg(2)), Ops[0]});\n  }\n  case NEON::BI__builtin_neon_vaddd_s64:\n  case NEON::BI__builtin_neon_vaddd_u64:\n    return Builder.CreateAdd(Ops[0], EmitScalarExpr(E->getArg(1)), \"vaddd\");\n  case NEON::BI__builtin_neon_vsubd_s64:\n  case NEON::BI__builtin_neon_vsubd_u64:\n    return Builder.CreateSub(Ops[0], EmitScalarExpr(E->getArg(1)), \"vsubd\");\n  case NEON::BI__builtin_neon_vqdmlalh_s16:\n  case NEON::BI__builtin_neon_vqdmlslh_s16: {\n    SmallVector<Value *, 2> ProductOps;\n    ProductOps.push_back(vectorWrapScalar16(Ops[1]));\n    ProductOps.push_back(vectorWrapScalar16(EmitScalarExpr(E->getArg(2))));\n    auto *VTy = llvm::FixedVectorType::get(Int32Ty, 4);\n    Ops[1] = EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_sqdmull, VTy),\n                          ProductOps, \"vqdmlXl\");\n    Constant *CI = ConstantInt::get(SizeTy, 0);\n    Ops[1] = Builder.CreateExtractElement(Ops[1], CI, \"lane0\");\n\n    unsigned AccumInt = BuiltinID == NEON::BI__builtin_neon_vqdmlalh_s16\n                                        ? Intrinsic::aarch64_neon_sqadd\n                                        : Intrinsic::aarch64_neon_sqsub;\n    return EmitNeonCall(CGM.getIntrinsic(AccumInt, Int32Ty), Ops, \"vqdmlXl\");\n  }\n  case NEON::BI__builtin_neon_vqshlud_n_s64: {\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Ops[1] = Builder.CreateZExt(Ops[1], Int64Ty);\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_sqshlu, Int64Ty),\n                        Ops, \"vqshlu_n\");\n  }\n  case NEON::BI__builtin_neon_vqshld_n_u64:\n  case NEON::BI__builtin_neon_vqshld_n_s64: {\n    unsigned Int = BuiltinID == NEON::BI__builtin_neon_vqshld_n_u64\n                                   ? Intrinsic::aarch64_neon_uqshl\n                                   : Intrinsic::aarch64_neon_sqshl;\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Ops[1] = Builder.CreateZExt(Ops[1], Int64Ty);\n    return EmitNeonCall(CGM.getIntrinsic(Int, Int64Ty), Ops, \"vqshl_n\");\n  }\n  case NEON::BI__builtin_neon_vrshrd_n_u64:\n  case NEON::BI__builtin_neon_vrshrd_n_s64: {\n    unsigned Int = BuiltinID == NEON::BI__builtin_neon_vrshrd_n_u64\n                                   ? Intrinsic::aarch64_neon_urshl\n                                   : Intrinsic::aarch64_neon_srshl;\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    int SV = cast<ConstantInt>(Ops[1])->getSExtValue();\n    Ops[1] = ConstantInt::get(Int64Ty, -SV);\n    return EmitNeonCall(CGM.getIntrinsic(Int, Int64Ty), Ops, \"vrshr_n\");\n  }\n  case NEON::BI__builtin_neon_vrsrad_n_u64:\n  case NEON::BI__builtin_neon_vrsrad_n_s64: {\n    unsigned Int = BuiltinID == NEON::BI__builtin_neon_vrsrad_n_u64\n                                   ? Intrinsic::aarch64_neon_urshl\n                                   : Intrinsic::aarch64_neon_srshl;\n    Ops[1] = Builder.CreateBitCast(Ops[1], Int64Ty);\n    Ops.push_back(Builder.CreateNeg(EmitScalarExpr(E->getArg(2))));\n    Ops[1] = Builder.CreateCall(CGM.getIntrinsic(Int, Int64Ty),\n                                {Ops[1], Builder.CreateSExt(Ops[2], Int64Ty)});\n    return Builder.CreateAdd(Ops[0], Builder.CreateBitCast(Ops[1], Int64Ty));\n  }\n  case NEON::BI__builtin_neon_vshld_n_s64:\n  case NEON::BI__builtin_neon_vshld_n_u64: {\n    llvm::ConstantInt *Amt = cast<ConstantInt>(EmitScalarExpr(E->getArg(1)));\n    return Builder.CreateShl(\n        Ops[0], ConstantInt::get(Int64Ty, Amt->getZExtValue()), \"shld_n\");\n  }\n  case NEON::BI__builtin_neon_vshrd_n_s64: {\n    llvm::ConstantInt *Amt = cast<ConstantInt>(EmitScalarExpr(E->getArg(1)));\n    return Builder.CreateAShr(\n        Ops[0], ConstantInt::get(Int64Ty, std::min(static_cast<uint64_t>(63),\n                                                   Amt->getZExtValue())),\n        \"shrd_n\");\n  }\n  case NEON::BI__builtin_neon_vshrd_n_u64: {\n    llvm::ConstantInt *Amt = cast<ConstantInt>(EmitScalarExpr(E->getArg(1)));\n    uint64_t ShiftAmt = Amt->getZExtValue();\n    // Right-shifting an unsigned value by its size yields 0.\n    if (ShiftAmt == 64)\n      return ConstantInt::get(Int64Ty, 0);\n    return Builder.CreateLShr(Ops[0], ConstantInt::get(Int64Ty, ShiftAmt),\n                              \"shrd_n\");\n  }\n  case NEON::BI__builtin_neon_vsrad_n_s64: {\n    llvm::ConstantInt *Amt = cast<ConstantInt>(EmitScalarExpr(E->getArg(2)));\n    Ops[1] = Builder.CreateAShr(\n        Ops[1], ConstantInt::get(Int64Ty, std::min(static_cast<uint64_t>(63),\n                                                   Amt->getZExtValue())),\n        \"shrd_n\");\n    return Builder.CreateAdd(Ops[0], Ops[1]);\n  }\n  case NEON::BI__builtin_neon_vsrad_n_u64: {\n    llvm::ConstantInt *Amt = cast<ConstantInt>(EmitScalarExpr(E->getArg(2)));\n    uint64_t ShiftAmt = Amt->getZExtValue();\n    // Right-shifting an unsigned value by its size yields 0.\n    // As Op + 0 = Op, return Ops[0] directly.\n    if (ShiftAmt == 64)\n      return Ops[0];\n    Ops[1] = Builder.CreateLShr(Ops[1], ConstantInt::get(Int64Ty, ShiftAmt),\n                                \"shrd_n\");\n    return Builder.CreateAdd(Ops[0], Ops[1]);\n  }\n  case NEON::BI__builtin_neon_vqdmlalh_lane_s16:\n  case NEON::BI__builtin_neon_vqdmlalh_laneq_s16:\n  case NEON::BI__builtin_neon_vqdmlslh_lane_s16:\n  case NEON::BI__builtin_neon_vqdmlslh_laneq_s16: {\n    Ops[2] = Builder.CreateExtractElement(Ops[2], EmitScalarExpr(E->getArg(3)),\n                                          \"lane\");\n    SmallVector<Value *, 2> ProductOps;\n    ProductOps.push_back(vectorWrapScalar16(Ops[1]));\n    ProductOps.push_back(vectorWrapScalar16(Ops[2]));\n    auto *VTy = llvm::FixedVectorType::get(Int32Ty, 4);\n    Ops[1] = EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_sqdmull, VTy),\n                          ProductOps, \"vqdmlXl\");\n    Constant *CI = ConstantInt::get(SizeTy, 0);\n    Ops[1] = Builder.CreateExtractElement(Ops[1], CI, \"lane0\");\n    Ops.pop_back();\n\n    unsigned AccInt = (BuiltinID == NEON::BI__builtin_neon_vqdmlalh_lane_s16 ||\n                       BuiltinID == NEON::BI__builtin_neon_vqdmlalh_laneq_s16)\n                          ? Intrinsic::aarch64_neon_sqadd\n                          : Intrinsic::aarch64_neon_sqsub;\n    return EmitNeonCall(CGM.getIntrinsic(AccInt, Int32Ty), Ops, \"vqdmlXl\");\n  }\n  case NEON::BI__builtin_neon_vqdmlals_s32:\n  case NEON::BI__builtin_neon_vqdmlsls_s32: {\n    SmallVector<Value *, 2> ProductOps;\n    ProductOps.push_back(Ops[1]);\n    ProductOps.push_back(EmitScalarExpr(E->getArg(2)));\n    Ops[1] =\n        EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_sqdmulls_scalar),\n                     ProductOps, \"vqdmlXl\");\n\n    unsigned AccumInt = BuiltinID == NEON::BI__builtin_neon_vqdmlals_s32\n                                        ? Intrinsic::aarch64_neon_sqadd\n                                        : Intrinsic::aarch64_neon_sqsub;\n    return EmitNeonCall(CGM.getIntrinsic(AccumInt, Int64Ty), Ops, \"vqdmlXl\");\n  }\n  case NEON::BI__builtin_neon_vqdmlals_lane_s32:\n  case NEON::BI__builtin_neon_vqdmlals_laneq_s32:\n  case NEON::BI__builtin_neon_vqdmlsls_lane_s32:\n  case NEON::BI__builtin_neon_vqdmlsls_laneq_s32: {\n    Ops[2] = Builder.CreateExtractElement(Ops[2], EmitScalarExpr(E->getArg(3)),\n                                          \"lane\");\n    SmallVector<Value *, 2> ProductOps;\n    ProductOps.push_back(Ops[1]);\n    ProductOps.push_back(Ops[2]);\n    Ops[1] =\n        EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_sqdmulls_scalar),\n                     ProductOps, \"vqdmlXl\");\n    Ops.pop_back();\n\n    unsigned AccInt = (BuiltinID == NEON::BI__builtin_neon_vqdmlals_lane_s32 ||\n                       BuiltinID == NEON::BI__builtin_neon_vqdmlals_laneq_s32)\n                          ? Intrinsic::aarch64_neon_sqadd\n                          : Intrinsic::aarch64_neon_sqsub;\n    return EmitNeonCall(CGM.getIntrinsic(AccInt, Int64Ty), Ops, \"vqdmlXl\");\n  }\n  case NEON::BI__builtin_neon_vget_lane_bf16:\n  case NEON::BI__builtin_neon_vduph_lane_bf16:\n  case NEON::BI__builtin_neon_vduph_lane_f16: {\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vget_lane\");\n  }\n  case NEON::BI__builtin_neon_vgetq_lane_bf16:\n  case NEON::BI__builtin_neon_vduph_laneq_bf16:\n  case NEON::BI__builtin_neon_vduph_laneq_f16: {\n    return Builder.CreateExtractElement(Ops[0], EmitScalarExpr(E->getArg(1)),\n                                        \"vgetq_lane\");\n  }\n\n  case AArch64::BI_InterlockedAdd: {\n    Value *Arg0 = EmitScalarExpr(E->getArg(0));\n    Value *Arg1 = EmitScalarExpr(E->getArg(1));\n    AtomicRMWInst *RMWI = Builder.CreateAtomicRMW(\n      AtomicRMWInst::Add, Arg0, Arg1,\n      llvm::AtomicOrdering::SequentiallyConsistent);\n    return Builder.CreateAdd(RMWI, Arg1);\n  }\n  }\n\n  llvm::FixedVectorType *VTy = GetNeonType(this, Type);\n  llvm::Type *Ty = VTy;\n  if (!Ty)\n    return nullptr;\n\n  // Not all intrinsics handled by the common case work for AArch64 yet, so only\n  // defer to common code if it's been added to our special map.\n  Builtin = findARMVectorIntrinsicInMap(AArch64SIMDIntrinsicMap, BuiltinID,\n                                        AArch64SIMDIntrinsicsProvenSorted);\n\n  if (Builtin)\n    return EmitCommonNeonBuiltinExpr(\n        Builtin->BuiltinID, Builtin->LLVMIntrinsic, Builtin->AltLLVMIntrinsic,\n        Builtin->NameHint, Builtin->TypeModifier, E, Ops,\n        /*never use addresses*/ Address::invalid(), Address::invalid(), Arch);\n\n  if (Value *V = EmitAArch64TblBuiltinExpr(*this, BuiltinID, E, Ops, Arch))\n    return V;\n\n  unsigned Int;\n  switch (BuiltinID) {\n  default: return nullptr;\n  case NEON::BI__builtin_neon_vbsl_v:\n  case NEON::BI__builtin_neon_vbslq_v: {\n    llvm::Type *BitTy = llvm::VectorType::getInteger(VTy);\n    Ops[0] = Builder.CreateBitCast(Ops[0], BitTy, \"vbsl\");\n    Ops[1] = Builder.CreateBitCast(Ops[1], BitTy, \"vbsl\");\n    Ops[2] = Builder.CreateBitCast(Ops[2], BitTy, \"vbsl\");\n\n    Ops[1] = Builder.CreateAnd(Ops[0], Ops[1], \"vbsl\");\n    Ops[2] = Builder.CreateAnd(Builder.CreateNot(Ops[0]), Ops[2], \"vbsl\");\n    Ops[0] = Builder.CreateOr(Ops[1], Ops[2], \"vbsl\");\n    return Builder.CreateBitCast(Ops[0], Ty);\n  }\n  case NEON::BI__builtin_neon_vfma_lane_v:\n  case NEON::BI__builtin_neon_vfmaq_lane_v: { // Only used for FP types\n    // The ARM builtins (and instructions) have the addend as the first\n    // operand, but the 'fma' intrinsics have it last. Swap it around here.\n    Value *Addend = Ops[0];\n    Value *Multiplicand = Ops[1];\n    Value *LaneSource = Ops[2];\n    Ops[0] = Multiplicand;\n    Ops[1] = LaneSource;\n    Ops[2] = Addend;\n\n    // Now adjust things to handle the lane access.\n    auto *SourceTy = BuiltinID == NEON::BI__builtin_neon_vfmaq_lane_v\n                         ? llvm::FixedVectorType::get(VTy->getElementType(),\n                                                      VTy->getNumElements() / 2)\n                         : VTy;\n    llvm::Constant *cst = cast<Constant>(Ops[3]);\n    Value *SV = llvm::ConstantVector::getSplat(VTy->getElementCount(), cst);\n    Ops[1] = Builder.CreateBitCast(Ops[1], SourceTy);\n    Ops[1] = Builder.CreateShuffleVector(Ops[1], Ops[1], SV, \"lane\");\n\n    Ops.pop_back();\n    Int = Builder.getIsFPConstrained() ? Intrinsic::experimental_constrained_fma\n                                       : Intrinsic::fma;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"fmla\");\n  }\n  case NEON::BI__builtin_neon_vfma_laneq_v: {\n    auto *VTy = cast<llvm::FixedVectorType>(Ty);\n    // v1f64 fma should be mapped to Neon scalar f64 fma\n    if (VTy && VTy->getElementType() == DoubleTy) {\n      Ops[0] = Builder.CreateBitCast(Ops[0], DoubleTy);\n      Ops[1] = Builder.CreateBitCast(Ops[1], DoubleTy);\n      llvm::FixedVectorType *VTy =\n          GetNeonType(this, NeonTypeFlags(NeonTypeFlags::Float64, false, true));\n      Ops[2] = Builder.CreateBitCast(Ops[2], VTy);\n      Ops[2] = Builder.CreateExtractElement(Ops[2], Ops[3], \"extract\");\n      Value *Result;\n      Result = emitCallMaybeConstrainedFPBuiltin(\n          *this, Intrinsic::fma, Intrinsic::experimental_constrained_fma,\n          DoubleTy, {Ops[1], Ops[2], Ops[0]});\n      return Builder.CreateBitCast(Result, Ty);\n    }\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n\n    auto *STy = llvm::FixedVectorType::get(VTy->getElementType(),\n                                           VTy->getNumElements() * 2);\n    Ops[2] = Builder.CreateBitCast(Ops[2], STy);\n    Value *SV = llvm::ConstantVector::getSplat(VTy->getElementCount(),\n                                               cast<ConstantInt>(Ops[3]));\n    Ops[2] = Builder.CreateShuffleVector(Ops[2], Ops[2], SV, \"lane\");\n\n    return emitCallMaybeConstrainedFPBuiltin(\n        *this, Intrinsic::fma, Intrinsic::experimental_constrained_fma, Ty,\n        {Ops[2], Ops[1], Ops[0]});\n  }\n  case NEON::BI__builtin_neon_vfmaq_laneq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Ops[2] = EmitNeonSplat(Ops[2], cast<ConstantInt>(Ops[3]));\n    return emitCallMaybeConstrainedFPBuiltin(\n        *this, Intrinsic::fma, Intrinsic::experimental_constrained_fma, Ty,\n        {Ops[2], Ops[1], Ops[0]});\n  }\n  case NEON::BI__builtin_neon_vfmah_lane_f16:\n  case NEON::BI__builtin_neon_vfmas_lane_f32:\n  case NEON::BI__builtin_neon_vfmah_laneq_f16:\n  case NEON::BI__builtin_neon_vfmas_laneq_f32:\n  case NEON::BI__builtin_neon_vfmad_lane_f64:\n  case NEON::BI__builtin_neon_vfmad_laneq_f64: {\n    Ops.push_back(EmitScalarExpr(E->getArg(3)));\n    llvm::Type *Ty = ConvertType(E->getCallReturnType(getContext()));\n    Ops[2] = Builder.CreateExtractElement(Ops[2], Ops[3], \"extract\");\n    return emitCallMaybeConstrainedFPBuiltin(\n        *this, Intrinsic::fma, Intrinsic::experimental_constrained_fma, Ty,\n        {Ops[1], Ops[2], Ops[0]});\n  }\n  case NEON::BI__builtin_neon_vmull_v:\n    // FIXME: improve sharing scheme to cope with 3 alternative LLVM intrinsics.\n    Int = usgn ? Intrinsic::aarch64_neon_umull : Intrinsic::aarch64_neon_smull;\n    if (Type.isPoly()) Int = Intrinsic::aarch64_neon_pmull;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vmull\");\n  case NEON::BI__builtin_neon_vmax_v:\n  case NEON::BI__builtin_neon_vmaxq_v:\n    // FIXME: improve sharing scheme to cope with 3 alternative LLVM intrinsics.\n    Int = usgn ? Intrinsic::aarch64_neon_umax : Intrinsic::aarch64_neon_smax;\n    if (Ty->isFPOrFPVectorTy()) Int = Intrinsic::aarch64_neon_fmax;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vmax\");\n  case NEON::BI__builtin_neon_vmaxh_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Int = Intrinsic::aarch64_neon_fmax;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vmax\");\n  }\n  case NEON::BI__builtin_neon_vmin_v:\n  case NEON::BI__builtin_neon_vminq_v:\n    // FIXME: improve sharing scheme to cope with 3 alternative LLVM intrinsics.\n    Int = usgn ? Intrinsic::aarch64_neon_umin : Intrinsic::aarch64_neon_smin;\n    if (Ty->isFPOrFPVectorTy()) Int = Intrinsic::aarch64_neon_fmin;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vmin\");\n  case NEON::BI__builtin_neon_vminh_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Int = Intrinsic::aarch64_neon_fmin;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vmin\");\n  }\n  case NEON::BI__builtin_neon_vabd_v:\n  case NEON::BI__builtin_neon_vabdq_v:\n    // FIXME: improve sharing scheme to cope with 3 alternative LLVM intrinsics.\n    Int = usgn ? Intrinsic::aarch64_neon_uabd : Intrinsic::aarch64_neon_sabd;\n    if (Ty->isFPOrFPVectorTy()) Int = Intrinsic::aarch64_neon_fabd;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vabd\");\n  case NEON::BI__builtin_neon_vpadal_v:\n  case NEON::BI__builtin_neon_vpadalq_v: {\n    unsigned ArgElts = VTy->getNumElements();\n    llvm::IntegerType *EltTy = cast<IntegerType>(VTy->getElementType());\n    unsigned BitWidth = EltTy->getBitWidth();\n    auto *ArgTy = llvm::FixedVectorType::get(\n        llvm::IntegerType::get(getLLVMContext(), BitWidth / 2), 2 * ArgElts);\n    llvm::Type* Tys[2] = { VTy, ArgTy };\n    Int = usgn ? Intrinsic::aarch64_neon_uaddlp : Intrinsic::aarch64_neon_saddlp;\n    SmallVector<llvm::Value*, 1> TmpOps;\n    TmpOps.push_back(Ops[1]);\n    Function *F = CGM.getIntrinsic(Int, Tys);\n    llvm::Value *tmp = EmitNeonCall(F, TmpOps, \"vpadal\");\n    llvm::Value *addend = Builder.CreateBitCast(Ops[0], tmp->getType());\n    return Builder.CreateAdd(tmp, addend);\n  }\n  case NEON::BI__builtin_neon_vpmin_v:\n  case NEON::BI__builtin_neon_vpminq_v:\n    // FIXME: improve sharing scheme to cope with 3 alternative LLVM intrinsics.\n    Int = usgn ? Intrinsic::aarch64_neon_uminp : Intrinsic::aarch64_neon_sminp;\n    if (Ty->isFPOrFPVectorTy()) Int = Intrinsic::aarch64_neon_fminp;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vpmin\");\n  case NEON::BI__builtin_neon_vpmax_v:\n  case NEON::BI__builtin_neon_vpmaxq_v:\n    // FIXME: improve sharing scheme to cope with 3 alternative LLVM intrinsics.\n    Int = usgn ? Intrinsic::aarch64_neon_umaxp : Intrinsic::aarch64_neon_smaxp;\n    if (Ty->isFPOrFPVectorTy()) Int = Intrinsic::aarch64_neon_fmaxp;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vpmax\");\n  case NEON::BI__builtin_neon_vminnm_v:\n  case NEON::BI__builtin_neon_vminnmq_v:\n    Int = Intrinsic::aarch64_neon_fminnm;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vminnm\");\n  case NEON::BI__builtin_neon_vminnmh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Int = Intrinsic::aarch64_neon_fminnm;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vminnm\");\n  case NEON::BI__builtin_neon_vmaxnm_v:\n  case NEON::BI__builtin_neon_vmaxnmq_v:\n    Int = Intrinsic::aarch64_neon_fmaxnm;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vmaxnm\");\n  case NEON::BI__builtin_neon_vmaxnmh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    Int = Intrinsic::aarch64_neon_fmaxnm;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vmaxnm\");\n  case NEON::BI__builtin_neon_vrecpss_f32: {\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_frecps, FloatTy),\n                        Ops, \"vrecps\");\n  }\n  case NEON::BI__builtin_neon_vrecpsd_f64:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_frecps, DoubleTy),\n                        Ops, \"vrecps\");\n  case NEON::BI__builtin_neon_vrecpsh_f16:\n    Ops.push_back(EmitScalarExpr(E->getArg(1)));\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_frecps, HalfTy),\n                        Ops, \"vrecps\");\n  case NEON::BI__builtin_neon_vqshrun_n_v:\n    Int = Intrinsic::aarch64_neon_sqshrun;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqshrun_n\");\n  case NEON::BI__builtin_neon_vqrshrun_n_v:\n    Int = Intrinsic::aarch64_neon_sqrshrun;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqrshrun_n\");\n  case NEON::BI__builtin_neon_vqshrn_n_v:\n    Int = usgn ? Intrinsic::aarch64_neon_uqshrn : Intrinsic::aarch64_neon_sqshrn;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqshrn_n\");\n  case NEON::BI__builtin_neon_vrshrn_n_v:\n    Int = Intrinsic::aarch64_neon_rshrn;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrshrn_n\");\n  case NEON::BI__builtin_neon_vqrshrn_n_v:\n    Int = usgn ? Intrinsic::aarch64_neon_uqrshrn : Intrinsic::aarch64_neon_sqrshrn;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vqrshrn_n\");\n  case NEON::BI__builtin_neon_vrndah_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_round\n              : Intrinsic::round;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vrnda\");\n  }\n  case NEON::BI__builtin_neon_vrnda_v:\n  case NEON::BI__builtin_neon_vrndaq_v: {\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_round\n              : Intrinsic::round;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrnda\");\n  }\n  case NEON::BI__builtin_neon_vrndih_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_nearbyint\n              : Intrinsic::nearbyint;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vrndi\");\n  }\n  case NEON::BI__builtin_neon_vrndmh_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_floor\n              : Intrinsic::floor;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vrndm\");\n  }\n  case NEON::BI__builtin_neon_vrndm_v:\n  case NEON::BI__builtin_neon_vrndmq_v: {\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_floor\n              : Intrinsic::floor;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrndm\");\n  }\n  case NEON::BI__builtin_neon_vrndnh_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Intrinsic::aarch64_neon_frintn;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vrndn\");\n  }\n  case NEON::BI__builtin_neon_vrndn_v:\n  case NEON::BI__builtin_neon_vrndnq_v: {\n    Int = Intrinsic::aarch64_neon_frintn;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrndn\");\n  }\n  case NEON::BI__builtin_neon_vrndns_f32: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Intrinsic::aarch64_neon_frintn;\n    return EmitNeonCall(CGM.getIntrinsic(Int, FloatTy), Ops, \"vrndn\");\n  }\n  case NEON::BI__builtin_neon_vrndph_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_ceil\n              : Intrinsic::ceil;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vrndp\");\n  }\n  case NEON::BI__builtin_neon_vrndp_v:\n  case NEON::BI__builtin_neon_vrndpq_v: {\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_ceil\n              : Intrinsic::ceil;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrndp\");\n  }\n  case NEON::BI__builtin_neon_vrndxh_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_rint\n              : Intrinsic::rint;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vrndx\");\n  }\n  case NEON::BI__builtin_neon_vrndx_v:\n  case NEON::BI__builtin_neon_vrndxq_v: {\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_rint\n              : Intrinsic::rint;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrndx\");\n  }\n  case NEON::BI__builtin_neon_vrndh_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_trunc\n              : Intrinsic::trunc;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vrndz\");\n  }\n  case NEON::BI__builtin_neon_vrnd_v:\n  case NEON::BI__builtin_neon_vrndq_v: {\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_trunc\n              : Intrinsic::trunc;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrndz\");\n  }\n  case NEON::BI__builtin_neon_vcvt_f64_v:\n  case NEON::BI__builtin_neon_vcvtq_f64_v:\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ty = GetNeonType(this, NeonTypeFlags(NeonTypeFlags::Float64, false, quad));\n    return usgn ? Builder.CreateUIToFP(Ops[0], Ty, \"vcvt\")\n                : Builder.CreateSIToFP(Ops[0], Ty, \"vcvt\");\n  case NEON::BI__builtin_neon_vcvt_f64_f32: {\n    assert(Type.getEltType() == NeonTypeFlags::Float64 && quad &&\n           \"unexpected vcvt_f64_f32 builtin\");\n    NeonTypeFlags SrcFlag = NeonTypeFlags(NeonTypeFlags::Float32, false, false);\n    Ops[0] = Builder.CreateBitCast(Ops[0], GetNeonType(this, SrcFlag));\n\n    return Builder.CreateFPExt(Ops[0], Ty, \"vcvt\");\n  }\n  case NEON::BI__builtin_neon_vcvt_f32_f64: {\n    assert(Type.getEltType() == NeonTypeFlags::Float32 &&\n           \"unexpected vcvt_f32_f64 builtin\");\n    NeonTypeFlags SrcFlag = NeonTypeFlags(NeonTypeFlags::Float64, false, true);\n    Ops[0] = Builder.CreateBitCast(Ops[0], GetNeonType(this, SrcFlag));\n\n    return Builder.CreateFPTrunc(Ops[0], Ty, \"vcvt\");\n  }\n  case NEON::BI__builtin_neon_vcvt_s32_v:\n  case NEON::BI__builtin_neon_vcvt_u32_v:\n  case NEON::BI__builtin_neon_vcvt_s64_v:\n  case NEON::BI__builtin_neon_vcvt_u64_v:\n  case NEON::BI__builtin_neon_vcvt_s16_v:\n  case NEON::BI__builtin_neon_vcvt_u16_v:\n  case NEON::BI__builtin_neon_vcvtq_s32_v:\n  case NEON::BI__builtin_neon_vcvtq_u32_v:\n  case NEON::BI__builtin_neon_vcvtq_s64_v:\n  case NEON::BI__builtin_neon_vcvtq_u64_v:\n  case NEON::BI__builtin_neon_vcvtq_s16_v:\n  case NEON::BI__builtin_neon_vcvtq_u16_v: {\n    Int =\n        usgn ? Intrinsic::aarch64_neon_fcvtzu : Intrinsic::aarch64_neon_fcvtzs;\n    llvm::Type *Tys[2] = {Ty, GetFloatNeonType(this, Type)};\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vcvtz\");\n  }\n  case NEON::BI__builtin_neon_vcvta_s16_v:\n  case NEON::BI__builtin_neon_vcvta_u16_v:\n  case NEON::BI__builtin_neon_vcvta_s32_v:\n  case NEON::BI__builtin_neon_vcvtaq_s16_v:\n  case NEON::BI__builtin_neon_vcvtaq_s32_v:\n  case NEON::BI__builtin_neon_vcvta_u32_v:\n  case NEON::BI__builtin_neon_vcvtaq_u16_v:\n  case NEON::BI__builtin_neon_vcvtaq_u32_v:\n  case NEON::BI__builtin_neon_vcvta_s64_v:\n  case NEON::BI__builtin_neon_vcvtaq_s64_v:\n  case NEON::BI__builtin_neon_vcvta_u64_v:\n  case NEON::BI__builtin_neon_vcvtaq_u64_v: {\n    Int = usgn ? Intrinsic::aarch64_neon_fcvtau : Intrinsic::aarch64_neon_fcvtas;\n    llvm::Type *Tys[2] = { Ty, GetFloatNeonType(this, Type) };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vcvta\");\n  }\n  case NEON::BI__builtin_neon_vcvtm_s16_v:\n  case NEON::BI__builtin_neon_vcvtm_s32_v:\n  case NEON::BI__builtin_neon_vcvtmq_s16_v:\n  case NEON::BI__builtin_neon_vcvtmq_s32_v:\n  case NEON::BI__builtin_neon_vcvtm_u16_v:\n  case NEON::BI__builtin_neon_vcvtm_u32_v:\n  case NEON::BI__builtin_neon_vcvtmq_u16_v:\n  case NEON::BI__builtin_neon_vcvtmq_u32_v:\n  case NEON::BI__builtin_neon_vcvtm_s64_v:\n  case NEON::BI__builtin_neon_vcvtmq_s64_v:\n  case NEON::BI__builtin_neon_vcvtm_u64_v:\n  case NEON::BI__builtin_neon_vcvtmq_u64_v: {\n    Int = usgn ? Intrinsic::aarch64_neon_fcvtmu : Intrinsic::aarch64_neon_fcvtms;\n    llvm::Type *Tys[2] = { Ty, GetFloatNeonType(this, Type) };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vcvtm\");\n  }\n  case NEON::BI__builtin_neon_vcvtn_s16_v:\n  case NEON::BI__builtin_neon_vcvtn_s32_v:\n  case NEON::BI__builtin_neon_vcvtnq_s16_v:\n  case NEON::BI__builtin_neon_vcvtnq_s32_v:\n  case NEON::BI__builtin_neon_vcvtn_u16_v:\n  case NEON::BI__builtin_neon_vcvtn_u32_v:\n  case NEON::BI__builtin_neon_vcvtnq_u16_v:\n  case NEON::BI__builtin_neon_vcvtnq_u32_v:\n  case NEON::BI__builtin_neon_vcvtn_s64_v:\n  case NEON::BI__builtin_neon_vcvtnq_s64_v:\n  case NEON::BI__builtin_neon_vcvtn_u64_v:\n  case NEON::BI__builtin_neon_vcvtnq_u64_v: {\n    Int = usgn ? Intrinsic::aarch64_neon_fcvtnu : Intrinsic::aarch64_neon_fcvtns;\n    llvm::Type *Tys[2] = { Ty, GetFloatNeonType(this, Type) };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vcvtn\");\n  }\n  case NEON::BI__builtin_neon_vcvtp_s16_v:\n  case NEON::BI__builtin_neon_vcvtp_s32_v:\n  case NEON::BI__builtin_neon_vcvtpq_s16_v:\n  case NEON::BI__builtin_neon_vcvtpq_s32_v:\n  case NEON::BI__builtin_neon_vcvtp_u16_v:\n  case NEON::BI__builtin_neon_vcvtp_u32_v:\n  case NEON::BI__builtin_neon_vcvtpq_u16_v:\n  case NEON::BI__builtin_neon_vcvtpq_u32_v:\n  case NEON::BI__builtin_neon_vcvtp_s64_v:\n  case NEON::BI__builtin_neon_vcvtpq_s64_v:\n  case NEON::BI__builtin_neon_vcvtp_u64_v:\n  case NEON::BI__builtin_neon_vcvtpq_u64_v: {\n    Int = usgn ? Intrinsic::aarch64_neon_fcvtpu : Intrinsic::aarch64_neon_fcvtps;\n    llvm::Type *Tys[2] = { Ty, GetFloatNeonType(this, Type) };\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vcvtp\");\n  }\n  case NEON::BI__builtin_neon_vmulx_v:\n  case NEON::BI__builtin_neon_vmulxq_v: {\n    Int = Intrinsic::aarch64_neon_fmulx;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vmulx\");\n  }\n  case NEON::BI__builtin_neon_vmulxh_lane_f16:\n  case NEON::BI__builtin_neon_vmulxh_laneq_f16: {\n    // vmulx_lane should be mapped to Neon scalar mulx after\n    // extracting the scalar element\n    Ops.push_back(EmitScalarExpr(E->getArg(2)));\n    Ops[1] = Builder.CreateExtractElement(Ops[1], Ops[2], \"extract\");\n    Ops.pop_back();\n    Int = Intrinsic::aarch64_neon_fmulx;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vmulx\");\n  }\n  case NEON::BI__builtin_neon_vmul_lane_v:\n  case NEON::BI__builtin_neon_vmul_laneq_v: {\n    // v1f64 vmul_lane should be mapped to Neon scalar mul lane\n    bool Quad = false;\n    if (BuiltinID == NEON::BI__builtin_neon_vmul_laneq_v)\n      Quad = true;\n    Ops[0] = Builder.CreateBitCast(Ops[0], DoubleTy);\n    llvm::FixedVectorType *VTy =\n        GetNeonType(this, NeonTypeFlags(NeonTypeFlags::Float64, false, Quad));\n    Ops[1] = Builder.CreateBitCast(Ops[1], VTy);\n    Ops[1] = Builder.CreateExtractElement(Ops[1], Ops[2], \"extract\");\n    Value *Result = Builder.CreateFMul(Ops[0], Ops[1]);\n    return Builder.CreateBitCast(Result, Ty);\n  }\n  case NEON::BI__builtin_neon_vnegd_s64:\n    return Builder.CreateNeg(EmitScalarExpr(E->getArg(0)), \"vnegd\");\n  case NEON::BI__builtin_neon_vnegh_f16:\n    return Builder.CreateFNeg(EmitScalarExpr(E->getArg(0)), \"vnegh\");\n  case NEON::BI__builtin_neon_vpmaxnm_v:\n  case NEON::BI__builtin_neon_vpmaxnmq_v: {\n    Int = Intrinsic::aarch64_neon_fmaxnmp;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vpmaxnm\");\n  }\n  case NEON::BI__builtin_neon_vpminnm_v:\n  case NEON::BI__builtin_neon_vpminnmq_v: {\n    Int = Intrinsic::aarch64_neon_fminnmp;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vpminnm\");\n  }\n  case NEON::BI__builtin_neon_vsqrth_f16: {\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_sqrt\n              : Intrinsic::sqrt;\n    return EmitNeonCall(CGM.getIntrinsic(Int, HalfTy), Ops, \"vsqrt\");\n  }\n  case NEON::BI__builtin_neon_vsqrt_v:\n  case NEON::BI__builtin_neon_vsqrtq_v: {\n    Int = Builder.getIsFPConstrained()\n              ? Intrinsic::experimental_constrained_sqrt\n              : Intrinsic::sqrt;\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vsqrt\");\n  }\n  case NEON::BI__builtin_neon_vrbit_v:\n  case NEON::BI__builtin_neon_vrbitq_v: {\n    Int = Intrinsic::aarch64_neon_rbit;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vrbit\");\n  }\n  case NEON::BI__builtin_neon_vaddv_u8:\n    // FIXME: These are handled by the AArch64 scalar code.\n    usgn = true;\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vaddv_s8: {\n    Int = usgn ? Intrinsic::aarch64_neon_uaddv : Intrinsic::aarch64_neon_saddv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vaddv_u16:\n    usgn = true;\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vaddv_s16: {\n    Int = usgn ? Intrinsic::aarch64_neon_uaddv : Intrinsic::aarch64_neon_saddv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vaddvq_u8:\n    usgn = true;\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vaddvq_s8: {\n    Int = usgn ? Intrinsic::aarch64_neon_uaddv : Intrinsic::aarch64_neon_saddv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vaddvq_u16:\n    usgn = true;\n    LLVM_FALLTHROUGH;\n  case NEON::BI__builtin_neon_vaddvq_s16: {\n    Int = usgn ? Intrinsic::aarch64_neon_uaddv : Intrinsic::aarch64_neon_saddv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxv_u8: {\n    Int = Intrinsic::aarch64_neon_umaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxv_u16: {\n    Int = Intrinsic::aarch64_neon_umaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxvq_u8: {\n    Int = Intrinsic::aarch64_neon_umaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxvq_u16: {\n    Int = Intrinsic::aarch64_neon_umaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxv_s8: {\n    Int = Intrinsic::aarch64_neon_smaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxv_s16: {\n    Int = Intrinsic::aarch64_neon_smaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxvq_s8: {\n    Int = Intrinsic::aarch64_neon_smaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxvq_s16: {\n    Int = Intrinsic::aarch64_neon_smaxv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vmaxv_f16: {\n    Int = Intrinsic::aarch64_neon_fmaxv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vmaxvq_f16: {\n    Int = Intrinsic::aarch64_neon_fmaxv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vminv_u8: {\n    Int = Intrinsic::aarch64_neon_uminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vminv_u16: {\n    Int = Intrinsic::aarch64_neon_uminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vminvq_u8: {\n    Int = Intrinsic::aarch64_neon_uminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vminvq_u16: {\n    Int = Intrinsic::aarch64_neon_uminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vminv_s8: {\n    Int = Intrinsic::aarch64_neon_sminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vminv_s16: {\n    Int = Intrinsic::aarch64_neon_sminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vminvq_s8: {\n    Int = Intrinsic::aarch64_neon_sminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int8Ty);\n  }\n  case NEON::BI__builtin_neon_vminvq_s16: {\n    Int = Intrinsic::aarch64_neon_sminv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vminv_f16: {\n    Int = Intrinsic::aarch64_neon_fminv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vminvq_f16: {\n    Int = Intrinsic::aarch64_neon_fminv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vmaxnmv_f16: {\n    Int = Intrinsic::aarch64_neon_fmaxnmv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxnmv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vmaxnmvq_f16: {\n    Int = Intrinsic::aarch64_neon_fmaxnmv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vmaxnmv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vminnmv_f16: {\n    Int = Intrinsic::aarch64_neon_fminnmv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminnmv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vminnmvq_f16: {\n    Int = Intrinsic::aarch64_neon_fminnmv;\n    Ty = HalfTy;\n    VTy = llvm::FixedVectorType::get(HalfTy, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vminnmv\");\n    return Builder.CreateTrunc(Ops[0], HalfTy);\n  }\n  case NEON::BI__builtin_neon_vmul_n_f64: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], DoubleTy);\n    Value *RHS = Builder.CreateBitCast(EmitScalarExpr(E->getArg(1)), DoubleTy);\n    return Builder.CreateFMul(Ops[0], RHS);\n  }\n  case NEON::BI__builtin_neon_vaddlv_u8: {\n    Int = Intrinsic::aarch64_neon_uaddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vaddlv_u16: {\n    Int = Intrinsic::aarch64_neon_uaddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n  }\n  case NEON::BI__builtin_neon_vaddlvq_u8: {\n    Int = Intrinsic::aarch64_neon_uaddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vaddlvq_u16: {\n    Int = Intrinsic::aarch64_neon_uaddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n  }\n  case NEON::BI__builtin_neon_vaddlv_s8: {\n    Int = Intrinsic::aarch64_neon_saddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vaddlv_s16: {\n    Int = Intrinsic::aarch64_neon_saddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 4);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n  }\n  case NEON::BI__builtin_neon_vaddlvq_s8: {\n    Int = Intrinsic::aarch64_neon_saddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    Ops[0] = EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n    return Builder.CreateTrunc(Ops[0], Int16Ty);\n  }\n  case NEON::BI__builtin_neon_vaddlvq_s16: {\n    Int = Intrinsic::aarch64_neon_saddlv;\n    Ty = Int32Ty;\n    VTy = llvm::FixedVectorType::get(Int16Ty, 8);\n    llvm::Type *Tys[2] = { Ty, VTy };\n    Ops.push_back(EmitScalarExpr(E->getArg(0)));\n    return EmitNeonCall(CGM.getIntrinsic(Int, Tys), Ops, \"vaddlv\");\n  }\n  case NEON::BI__builtin_neon_vsri_n_v:\n  case NEON::BI__builtin_neon_vsriq_n_v: {\n    Int = Intrinsic::aarch64_neon_vsri;\n    llvm::Function *Intrin = CGM.getIntrinsic(Int, Ty);\n    return EmitNeonCall(Intrin, Ops, \"vsri_n\");\n  }\n  case NEON::BI__builtin_neon_vsli_n_v:\n  case NEON::BI__builtin_neon_vsliq_n_v: {\n    Int = Intrinsic::aarch64_neon_vsli;\n    llvm::Function *Intrin = CGM.getIntrinsic(Int, Ty);\n    return EmitNeonCall(Intrin, Ops, \"vsli_n\");\n  }\n  case NEON::BI__builtin_neon_vsra_n_v:\n  case NEON::BI__builtin_neon_vsraq_n_v:\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[1] = EmitNeonRShiftImm(Ops[1], Ops[2], Ty, usgn, \"vsra_n\");\n    return Builder.CreateAdd(Ops[0], Ops[1]);\n  case NEON::BI__builtin_neon_vrsra_n_v:\n  case NEON::BI__builtin_neon_vrsraq_n_v: {\n    Int = usgn ? Intrinsic::aarch64_neon_urshl : Intrinsic::aarch64_neon_srshl;\n    SmallVector<llvm::Value*,2> TmpOps;\n    TmpOps.push_back(Ops[1]);\n    TmpOps.push_back(Ops[2]);\n    Function* F = CGM.getIntrinsic(Int, Ty);\n    llvm::Value *tmp = EmitNeonCall(F, TmpOps, \"vrshr_n\", 1, true);\n    Ops[0] = Builder.CreateBitCast(Ops[0], VTy);\n    return Builder.CreateAdd(Ops[0], tmp);\n  }\n  case NEON::BI__builtin_neon_vld1_v:\n  case NEON::BI__builtin_neon_vld1q_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(VTy));\n    return Builder.CreateAlignedLoad(VTy, Ops[0], PtrOp0.getAlignment());\n  }\n  case NEON::BI__builtin_neon_vst1_v:\n  case NEON::BI__builtin_neon_vst1q_v:\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(VTy));\n    Ops[1] = Builder.CreateBitCast(Ops[1], VTy);\n    return Builder.CreateAlignedStore(Ops[1], Ops[0], PtrOp0.getAlignment());\n  case NEON::BI__builtin_neon_vld1_lane_v:\n  case NEON::BI__builtin_neon_vld1q_lane_v: {\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ty = llvm::PointerType::getUnqual(VTy->getElementType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[0] = Builder.CreateAlignedLoad(VTy->getElementType(), Ops[0],\n                                       PtrOp0.getAlignment());\n    return Builder.CreateInsertElement(Ops[1], Ops[0], Ops[2], \"vld1_lane\");\n  }\n  case NEON::BI__builtin_neon_vld1_dup_v:\n  case NEON::BI__builtin_neon_vld1q_dup_v: {\n    Value *V = UndefValue::get(Ty);\n    Ty = llvm::PointerType::getUnqual(VTy->getElementType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    Ops[0] = Builder.CreateAlignedLoad(VTy->getElementType(), Ops[0],\n                                       PtrOp0.getAlignment());\n    llvm::Constant *CI = ConstantInt::get(Int32Ty, 0);\n    Ops[0] = Builder.CreateInsertElement(V, Ops[0], CI);\n    return EmitNeonSplat(Ops[0], CI);\n  }\n  case NEON::BI__builtin_neon_vst1_lane_v:\n  case NEON::BI__builtin_neon_vst1q_lane_v:\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[1] = Builder.CreateExtractElement(Ops[1], Ops[2]);\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    return Builder.CreateAlignedStore(Ops[1], Builder.CreateBitCast(Ops[0], Ty),\n                                      PtrOp0.getAlignment());\n  case NEON::BI__builtin_neon_vld2_v:\n  case NEON::BI__builtin_neon_vld2q_v: {\n    llvm::Type *PTy = llvm::PointerType::getUnqual(VTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], PTy);\n    llvm::Type *Tys[2] = { VTy, PTy };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld2, Tys);\n    Ops[1] = Builder.CreateCall(F, Ops[1], \"vld2\");\n    Ops[0] = Builder.CreateBitCast(Ops[0],\n                llvm::PointerType::getUnqual(Ops[1]->getType()));\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld3_v:\n  case NEON::BI__builtin_neon_vld3q_v: {\n    llvm::Type *PTy = llvm::PointerType::getUnqual(VTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], PTy);\n    llvm::Type *Tys[2] = { VTy, PTy };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld3, Tys);\n    Ops[1] = Builder.CreateCall(F, Ops[1], \"vld3\");\n    Ops[0] = Builder.CreateBitCast(Ops[0],\n                llvm::PointerType::getUnqual(Ops[1]->getType()));\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld4_v:\n  case NEON::BI__builtin_neon_vld4q_v: {\n    llvm::Type *PTy = llvm::PointerType::getUnqual(VTy);\n    Ops[1] = Builder.CreateBitCast(Ops[1], PTy);\n    llvm::Type *Tys[2] = { VTy, PTy };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld4, Tys);\n    Ops[1] = Builder.CreateCall(F, Ops[1], \"vld4\");\n    Ops[0] = Builder.CreateBitCast(Ops[0],\n                llvm::PointerType::getUnqual(Ops[1]->getType()));\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld2_dup_v:\n  case NEON::BI__builtin_neon_vld2q_dup_v: {\n    llvm::Type *PTy =\n      llvm::PointerType::getUnqual(VTy->getElementType());\n    Ops[1] = Builder.CreateBitCast(Ops[1], PTy);\n    llvm::Type *Tys[2] = { VTy, PTy };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld2r, Tys);\n    Ops[1] = Builder.CreateCall(F, Ops[1], \"vld2\");\n    Ops[0] = Builder.CreateBitCast(Ops[0],\n                llvm::PointerType::getUnqual(Ops[1]->getType()));\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld3_dup_v:\n  case NEON::BI__builtin_neon_vld3q_dup_v: {\n    llvm::Type *PTy =\n      llvm::PointerType::getUnqual(VTy->getElementType());\n    Ops[1] = Builder.CreateBitCast(Ops[1], PTy);\n    llvm::Type *Tys[2] = { VTy, PTy };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld3r, Tys);\n    Ops[1] = Builder.CreateCall(F, Ops[1], \"vld3\");\n    Ops[0] = Builder.CreateBitCast(Ops[0],\n                llvm::PointerType::getUnqual(Ops[1]->getType()));\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld4_dup_v:\n  case NEON::BI__builtin_neon_vld4q_dup_v: {\n    llvm::Type *PTy =\n      llvm::PointerType::getUnqual(VTy->getElementType());\n    Ops[1] = Builder.CreateBitCast(Ops[1], PTy);\n    llvm::Type *Tys[2] = { VTy, PTy };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld4r, Tys);\n    Ops[1] = Builder.CreateCall(F, Ops[1], \"vld4\");\n    Ops[0] = Builder.CreateBitCast(Ops[0],\n                llvm::PointerType::getUnqual(Ops[1]->getType()));\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld2_lane_v:\n  case NEON::BI__builtin_neon_vld2q_lane_v: {\n    llvm::Type *Tys[2] = { VTy, Ops[1]->getType() };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld2lane, Tys);\n    std::rotate(Ops.begin() + 1, Ops.begin() + 2, Ops.end());\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Ops[3] = Builder.CreateZExt(Ops[3], Int64Ty);\n    Ops[1] = Builder.CreateCall(F, makeArrayRef(Ops).slice(1), \"vld2_lane\");\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld3_lane_v:\n  case NEON::BI__builtin_neon_vld3q_lane_v: {\n    llvm::Type *Tys[2] = { VTy, Ops[1]->getType() };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld3lane, Tys);\n    std::rotate(Ops.begin() + 1, Ops.begin() + 2, Ops.end());\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Ops[3] = Builder.CreateBitCast(Ops[3], Ty);\n    Ops[4] = Builder.CreateZExt(Ops[4], Int64Ty);\n    Ops[1] = Builder.CreateCall(F, makeArrayRef(Ops).slice(1), \"vld3_lane\");\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vld4_lane_v:\n  case NEON::BI__builtin_neon_vld4q_lane_v: {\n    llvm::Type *Tys[2] = { VTy, Ops[1]->getType() };\n    Function *F = CGM.getIntrinsic(Intrinsic::aarch64_neon_ld4lane, Tys);\n    std::rotate(Ops.begin() + 1, Ops.begin() + 2, Ops.end());\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Ops[3] = Builder.CreateBitCast(Ops[3], Ty);\n    Ops[4] = Builder.CreateBitCast(Ops[4], Ty);\n    Ops[5] = Builder.CreateZExt(Ops[5], Int64Ty);\n    Ops[1] = Builder.CreateCall(F, makeArrayRef(Ops).slice(1), \"vld4_lane\");\n    Ty = llvm::PointerType::getUnqual(Ops[1]->getType());\n    Ops[0] = Builder.CreateBitCast(Ops[0], Ty);\n    return Builder.CreateDefaultAlignedStore(Ops[1], Ops[0]);\n  }\n  case NEON::BI__builtin_neon_vst2_v:\n  case NEON::BI__builtin_neon_vst2q_v: {\n    std::rotate(Ops.begin(), Ops.begin() + 1, Ops.end());\n    llvm::Type *Tys[2] = { VTy, Ops[2]->getType() };\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_st2, Tys),\n                        Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vst2_lane_v:\n  case NEON::BI__builtin_neon_vst2q_lane_v: {\n    std::rotate(Ops.begin(), Ops.begin() + 1, Ops.end());\n    Ops[2] = Builder.CreateZExt(Ops[2], Int64Ty);\n    llvm::Type *Tys[2] = { VTy, Ops[3]->getType() };\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_st2lane, Tys),\n                        Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vst3_v:\n  case NEON::BI__builtin_neon_vst3q_v: {\n    std::rotate(Ops.begin(), Ops.begin() + 1, Ops.end());\n    llvm::Type *Tys[2] = { VTy, Ops[3]->getType() };\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_st3, Tys),\n                        Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vst3_lane_v:\n  case NEON::BI__builtin_neon_vst3q_lane_v: {\n    std::rotate(Ops.begin(), Ops.begin() + 1, Ops.end());\n    Ops[3] = Builder.CreateZExt(Ops[3], Int64Ty);\n    llvm::Type *Tys[2] = { VTy, Ops[4]->getType() };\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_st3lane, Tys),\n                        Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vst4_v:\n  case NEON::BI__builtin_neon_vst4q_v: {\n    std::rotate(Ops.begin(), Ops.begin() + 1, Ops.end());\n    llvm::Type *Tys[2] = { VTy, Ops[4]->getType() };\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_st4, Tys),\n                        Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vst4_lane_v:\n  case NEON::BI__builtin_neon_vst4q_lane_v: {\n    std::rotate(Ops.begin(), Ops.begin() + 1, Ops.end());\n    Ops[4] = Builder.CreateZExt(Ops[4], Int64Ty);\n    llvm::Type *Tys[2] = { VTy, Ops[5]->getType() };\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_st4lane, Tys),\n                        Ops, \"\");\n  }\n  case NEON::BI__builtin_neon_vtrn_v:\n  case NEON::BI__builtin_neon_vtrnq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(Ty));\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Value *SV = nullptr;\n\n    for (unsigned vi = 0; vi != 2; ++vi) {\n      SmallVector<int, 16> Indices;\n      for (unsigned i = 0, e = VTy->getNumElements(); i != e; i += 2) {\n        Indices.push_back(i+vi);\n        Indices.push_back(i+e+vi);\n      }\n      Value *Addr = Builder.CreateConstInBoundsGEP1_32(Ty, Ops[0], vi);\n      SV = Builder.CreateShuffleVector(Ops[1], Ops[2], Indices, \"vtrn\");\n      SV = Builder.CreateDefaultAlignedStore(SV, Addr);\n    }\n    return SV;\n  }\n  case NEON::BI__builtin_neon_vuzp_v:\n  case NEON::BI__builtin_neon_vuzpq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(Ty));\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Value *SV = nullptr;\n\n    for (unsigned vi = 0; vi != 2; ++vi) {\n      SmallVector<int, 16> Indices;\n      for (unsigned i = 0, e = VTy->getNumElements(); i != e; ++i)\n        Indices.push_back(2*i+vi);\n\n      Value *Addr = Builder.CreateConstInBoundsGEP1_32(Ty, Ops[0], vi);\n      SV = Builder.CreateShuffleVector(Ops[1], Ops[2], Indices, \"vuzp\");\n      SV = Builder.CreateDefaultAlignedStore(SV, Addr);\n    }\n    return SV;\n  }\n  case NEON::BI__builtin_neon_vzip_v:\n  case NEON::BI__builtin_neon_vzipq_v: {\n    Ops[0] = Builder.CreateBitCast(Ops[0], llvm::PointerType::getUnqual(Ty));\n    Ops[1] = Builder.CreateBitCast(Ops[1], Ty);\n    Ops[2] = Builder.CreateBitCast(Ops[2], Ty);\n    Value *SV = nullptr;\n\n    for (unsigned vi = 0; vi != 2; ++vi) {\n      SmallVector<int, 16> Indices;\n      for (unsigned i = 0, e = VTy->getNumElements(); i != e; i += 2) {\n        Indices.push_back((i + vi*e) >> 1);\n        Indices.push_back(((i + vi*e) >> 1)+e);\n      }\n      Value *Addr = Builder.CreateConstInBoundsGEP1_32(Ty, Ops[0], vi);\n      SV = Builder.CreateShuffleVector(Ops[1], Ops[2], Indices, \"vzip\");\n      SV = Builder.CreateDefaultAlignedStore(SV, Addr);\n    }\n    return SV;\n  }\n  case NEON::BI__builtin_neon_vqtbl1q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbl1, Ty),\n                        Ops, \"vtbl1\");\n  }\n  case NEON::BI__builtin_neon_vqtbl2q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbl2, Ty),\n                        Ops, \"vtbl2\");\n  }\n  case NEON::BI__builtin_neon_vqtbl3q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbl3, Ty),\n                        Ops, \"vtbl3\");\n  }\n  case NEON::BI__builtin_neon_vqtbl4q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbl4, Ty),\n                        Ops, \"vtbl4\");\n  }\n  case NEON::BI__builtin_neon_vqtbx1q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbx1, Ty),\n                        Ops, \"vtbx1\");\n  }\n  case NEON::BI__builtin_neon_vqtbx2q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbx2, Ty),\n                        Ops, \"vtbx2\");\n  }\n  case NEON::BI__builtin_neon_vqtbx3q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbx3, Ty),\n                        Ops, \"vtbx3\");\n  }\n  case NEON::BI__builtin_neon_vqtbx4q_v: {\n    return EmitNeonCall(CGM.getIntrinsic(Intrinsic::aarch64_neon_tbx4, Ty),\n                        Ops, \"vtbx4\");\n  }\n  case NEON::BI__builtin_neon_vsqadd_v:\n  case NEON::BI__builtin_neon_vsqaddq_v: {\n    Int = Intrinsic::aarch64_neon_usqadd;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vsqadd\");\n  }\n  case NEON::BI__builtin_neon_vuqadd_v:\n  case NEON::BI__builtin_neon_vuqaddq_v: {\n    Int = Intrinsic::aarch64_neon_suqadd;\n    return EmitNeonCall(CGM.getIntrinsic(Int, Ty), Ops, \"vuqadd\");\n  }\n  }\n}\n\nValue *CodeGenFunction::EmitBPFBuiltinExpr(unsigned BuiltinID,\n                                           const CallExpr *E) {\n  assert((BuiltinID == BPF::BI__builtin_preserve_field_info ||\n          BuiltinID == BPF::BI__builtin_btf_type_id ||\n          BuiltinID == BPF::BI__builtin_preserve_type_info ||\n          BuiltinID == BPF::BI__builtin_preserve_enum_value) &&\n         \"unexpected BPF builtin\");\n\n  // A sequence number, injected into IR builtin functions, to\n  // prevent CSE given the only difference of the funciton\n  // may just be the debuginfo metadata.\n  static uint32_t BuiltinSeqNum;\n\n  switch (BuiltinID) {\n  default:\n    llvm_unreachable(\"Unexpected BPF builtin\");\n  case BPF::BI__builtin_preserve_field_info: {\n    const Expr *Arg = E->getArg(0);\n    bool IsBitField = Arg->IgnoreParens()->getObjectKind() == OK_BitField;\n\n    if (!getDebugInfo()) {\n      CGM.Error(E->getExprLoc(),\n                \"using __builtin_preserve_field_info() without -g\");\n      return IsBitField ? EmitLValue(Arg).getBitFieldPointer()\n                        : EmitLValue(Arg).getPointer(*this);\n    }\n\n    // Enable underlying preserve_*_access_index() generation.\n    bool OldIsInPreservedAIRegion = IsInPreservedAIRegion;\n    IsInPreservedAIRegion = true;\n    Value *FieldAddr = IsBitField ? EmitLValue(Arg).getBitFieldPointer()\n                                  : EmitLValue(Arg).getPointer(*this);\n    IsInPreservedAIRegion = OldIsInPreservedAIRegion;\n\n    ConstantInt *C = cast<ConstantInt>(EmitScalarExpr(E->getArg(1)));\n    Value *InfoKind = ConstantInt::get(Int64Ty, C->getSExtValue());\n\n    // Built the IR for the preserve_field_info intrinsic.\n    llvm::Function *FnGetFieldInfo = llvm::Intrinsic::getDeclaration(\n        &CGM.getModule(), llvm::Intrinsic::bpf_preserve_field_info,\n        {FieldAddr->getType()});\n    return Builder.CreateCall(FnGetFieldInfo, {FieldAddr, InfoKind});\n  }\n  case BPF::BI__builtin_btf_type_id:\n  case BPF::BI__builtin_preserve_type_info: {\n    if (!getDebugInfo()) {\n      CGM.Error(E->getExprLoc(), \"using builtin function without -g\");\n      return nullptr;\n    }\n\n    const Expr *Arg0 = E->getArg(0);\n    llvm::DIType *DbgInfo = getDebugInfo()->getOrCreateStandaloneType(\n        Arg0->getType(), Arg0->getExprLoc());\n\n    ConstantInt *Flag = cast<ConstantInt>(EmitScalarExpr(E->getArg(1)));\n    Value *FlagValue = ConstantInt::get(Int64Ty, Flag->getSExtValue());\n    Value *SeqNumVal = ConstantInt::get(Int32Ty, BuiltinSeqNum++);\n\n    llvm::Function *FnDecl;\n    if (BuiltinID == BPF::BI__builtin_btf_type_id)\n      FnDecl = llvm::Intrinsic::getDeclaration(\n          &CGM.getModule(), llvm::Intrinsic::bpf_btf_type_id, {});\n    else\n      FnDecl = llvm::Intrinsic::getDeclaration(\n          &CGM.getModule(), llvm::Intrinsic::bpf_preserve_type_info, {});\n    CallInst *Fn = Builder.CreateCall(FnDecl, {SeqNumVal, FlagValue});\n    Fn->setMetadata(LLVMContext::MD_preserve_access_index, DbgInfo);\n    return Fn;\n  }\n  case BPF::BI__builtin_preserve_enum_value: {\n    if (!getDebugInfo()) {\n      CGM.Error(E->getExprLoc(), \"using builtin function without -g\");\n      return nullptr;\n    }\n\n    const Expr *Arg0 = E->getArg(0);\n    llvm::DIType *DbgInfo = getDebugInfo()->getOrCreateStandaloneType(\n        Arg0->getType(), Arg0->getExprLoc());\n\n    // Find enumerator\n    const auto *UO = cast<UnaryOperator>(Arg0->IgnoreParens());\n    const auto *CE = cast<CStyleCastExpr>(UO->getSubExpr());\n    const auto *DR = cast<DeclRefExpr>(CE->getSubExpr());\n    const auto *Enumerator = cast<EnumConstantDecl>(DR->getDecl());\n\n    auto &InitVal = Enumerator->getInitVal();\n    std::string InitValStr;\n    if (InitVal.isNegative() || InitVal > uint64_t(INT64_MAX))\n      InitValStr = std::to_string(InitVal.getSExtValue());\n    else\n      InitValStr = std::to_string(InitVal.getZExtValue());\n    std::string EnumStr = Enumerator->getNameAsString() + \":\" + InitValStr;\n    Value *EnumStrVal = Builder.CreateGlobalStringPtr(EnumStr);\n\n    ConstantInt *Flag = cast<ConstantInt>(EmitScalarExpr(E->getArg(1)));\n    Value *FlagValue = ConstantInt::get(Int64Ty, Flag->getSExtValue());\n    Value *SeqNumVal = ConstantInt::get(Int32Ty, BuiltinSeqNum++);\n\n    llvm::Function *IntrinsicFn = llvm::Intrinsic::getDeclaration(\n        &CGM.getModule(), llvm::Intrinsic::bpf_preserve_enum_value, {});\n    CallInst *Fn =\n        Builder.CreateCall(IntrinsicFn, {SeqNumVal, EnumStrVal, FlagValue});\n    Fn->setMetadata(LLVMContext::MD_preserve_access_index, DbgInfo);\n    return Fn;\n  }\n  }\n}\n\nllvm::Value *CodeGenFunction::\nBuildVector(ArrayRef<llvm::Value*> Ops) {\n  assert((Ops.size() & (Ops.size() - 1)) == 0 &&\n         \"Not a power-of-two sized vector!\");\n  bool AllConstants = true;\n  for (unsigned i = 0, e = Ops.size(); i != e && AllConstants; ++i)\n    AllConstants &= isa<Constant>(Ops[i]);\n\n  // If this is a constant vector, create a ConstantVector.\n  if (AllConstants) {\n    SmallVector<llvm::Constant*, 16> CstOps;\n    for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n      CstOps.push_back(cast<Constant>(Ops[i]));\n    return llvm::ConstantVector::get(CstOps);\n  }\n\n  // Otherwise, insertelement the values to build the vector.\n  Value *Result = llvm::UndefValue::get(\n      llvm::FixedVectorType::get(Ops[0]->getType(), Ops.size()));\n\n  for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n    Result = Builder.CreateInsertElement(Result, Ops[i], Builder.getInt32(i));\n\n  return Result;\n}\n\n// Convert the mask from an integer type to a vector of i1.\nstatic Value *getMaskVecValue(CodeGenFunction &CGF, Value *Mask,\n                              unsigned NumElts) {\n\n  auto *MaskTy = llvm::FixedVectorType::get(\n      CGF.Builder.getInt1Ty(),\n      cast<IntegerType>(Mask->getType())->getBitWidth());\n  Value *MaskVec = CGF.Builder.CreateBitCast(Mask, MaskTy);\n\n  // If we have less than 8 elements, then the starting mask was an i8 and\n  // we need to extract down to the right number of elements.\n  if (NumElts < 8) {\n    int Indices[4];\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = i;\n    MaskVec = CGF.Builder.CreateShuffleVector(MaskVec, MaskVec,\n                                             makeArrayRef(Indices, NumElts),\n                                             \"extract\");\n  }\n  return MaskVec;\n}\n\nstatic Value *EmitX86MaskedStore(CodeGenFunction &CGF, ArrayRef<Value *> Ops,\n                                 Align Alignment) {\n  // Cast the pointer to right type.\n  Value *Ptr = CGF.Builder.CreateBitCast(Ops[0],\n                               llvm::PointerType::getUnqual(Ops[1]->getType()));\n\n  Value *MaskVec = getMaskVecValue(\n      CGF, Ops[2],\n      cast<llvm::FixedVectorType>(Ops[1]->getType())->getNumElements());\n\n  return CGF.Builder.CreateMaskedStore(Ops[1], Ptr, Alignment, MaskVec);\n}\n\nstatic Value *EmitX86MaskedLoad(CodeGenFunction &CGF, ArrayRef<Value *> Ops,\n                                Align Alignment) {\n  // Cast the pointer to right type.\n  Value *Ptr = CGF.Builder.CreateBitCast(Ops[0],\n                               llvm::PointerType::getUnqual(Ops[1]->getType()));\n\n  Value *MaskVec = getMaskVecValue(\n      CGF, Ops[2],\n      cast<llvm::FixedVectorType>(Ops[1]->getType())->getNumElements());\n\n  return CGF.Builder.CreateMaskedLoad(Ptr, Alignment, MaskVec, Ops[1]);\n}\n\nstatic Value *EmitX86ExpandLoad(CodeGenFunction &CGF,\n                                ArrayRef<Value *> Ops) {\n  auto *ResultTy = cast<llvm::VectorType>(Ops[1]->getType());\n  llvm::Type *PtrTy = ResultTy->getElementType();\n\n  // Cast the pointer to element type.\n  Value *Ptr = CGF.Builder.CreateBitCast(Ops[0],\n                                         llvm::PointerType::getUnqual(PtrTy));\n\n  Value *MaskVec = getMaskVecValue(\n      CGF, Ops[2], cast<FixedVectorType>(ResultTy)->getNumElements());\n\n  llvm::Function *F = CGF.CGM.getIntrinsic(Intrinsic::masked_expandload,\n                                           ResultTy);\n  return CGF.Builder.CreateCall(F, { Ptr, MaskVec, Ops[1] });\n}\n\nstatic Value *EmitX86CompressExpand(CodeGenFunction &CGF,\n                                    ArrayRef<Value *> Ops,\n                                    bool IsCompress) {\n  auto *ResultTy = cast<llvm::FixedVectorType>(Ops[1]->getType());\n\n  Value *MaskVec = getMaskVecValue(CGF, Ops[2], ResultTy->getNumElements());\n\n  Intrinsic::ID IID = IsCompress ? Intrinsic::x86_avx512_mask_compress\n                                 : Intrinsic::x86_avx512_mask_expand;\n  llvm::Function *F = CGF.CGM.getIntrinsic(IID, ResultTy);\n  return CGF.Builder.CreateCall(F, { Ops[0], Ops[1], MaskVec });\n}\n\nstatic Value *EmitX86CompressStore(CodeGenFunction &CGF,\n                                   ArrayRef<Value *> Ops) {\n  auto *ResultTy = cast<llvm::FixedVectorType>(Ops[1]->getType());\n  llvm::Type *PtrTy = ResultTy->getElementType();\n\n  // Cast the pointer to element type.\n  Value *Ptr = CGF.Builder.CreateBitCast(Ops[0],\n                                         llvm::PointerType::getUnqual(PtrTy));\n\n  Value *MaskVec = getMaskVecValue(CGF, Ops[2], ResultTy->getNumElements());\n\n  llvm::Function *F = CGF.CGM.getIntrinsic(Intrinsic::masked_compressstore,\n                                           ResultTy);\n  return CGF.Builder.CreateCall(F, { Ops[1], Ptr, MaskVec });\n}\n\nstatic Value *EmitX86MaskLogic(CodeGenFunction &CGF, Instruction::BinaryOps Opc,\n                              ArrayRef<Value *> Ops,\n                              bool InvertLHS = false) {\n  unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n  Value *LHS = getMaskVecValue(CGF, Ops[0], NumElts);\n  Value *RHS = getMaskVecValue(CGF, Ops[1], NumElts);\n\n  if (InvertLHS)\n    LHS = CGF.Builder.CreateNot(LHS);\n\n  return CGF.Builder.CreateBitCast(CGF.Builder.CreateBinOp(Opc, LHS, RHS),\n                                   Ops[0]->getType());\n}\n\nstatic Value *EmitX86FunnelShift(CodeGenFunction &CGF, Value *Op0, Value *Op1,\n                                 Value *Amt, bool IsRight) {\n  llvm::Type *Ty = Op0->getType();\n\n  // Amount may be scalar immediate, in which case create a splat vector.\n  // Funnel shifts amounts are treated as modulo and types are all power-of-2 so\n  // we only care about the lowest log2 bits anyway.\n  if (Amt->getType() != Ty) {\n    unsigned NumElts = cast<llvm::FixedVectorType>(Ty)->getNumElements();\n    Amt = CGF.Builder.CreateIntCast(Amt, Ty->getScalarType(), false);\n    Amt = CGF.Builder.CreateVectorSplat(NumElts, Amt);\n  }\n\n  unsigned IID = IsRight ? Intrinsic::fshr : Intrinsic::fshl;\n  Function *F = CGF.CGM.getIntrinsic(IID, Ty);\n  return CGF.Builder.CreateCall(F, {Op0, Op1, Amt});\n}\n\nstatic Value *EmitX86vpcom(CodeGenFunction &CGF, ArrayRef<Value *> Ops,\n                           bool IsSigned) {\n  Value *Op0 = Ops[0];\n  Value *Op1 = Ops[1];\n  llvm::Type *Ty = Op0->getType();\n  uint64_t Imm = cast<llvm::ConstantInt>(Ops[2])->getZExtValue() & 0x7;\n\n  CmpInst::Predicate Pred;\n  switch (Imm) {\n  case 0x0:\n    Pred = IsSigned ? ICmpInst::ICMP_SLT : ICmpInst::ICMP_ULT;\n    break;\n  case 0x1:\n    Pred = IsSigned ? ICmpInst::ICMP_SLE : ICmpInst::ICMP_ULE;\n    break;\n  case 0x2:\n    Pred = IsSigned ? ICmpInst::ICMP_SGT : ICmpInst::ICMP_UGT;\n    break;\n  case 0x3:\n    Pred = IsSigned ? ICmpInst::ICMP_SGE : ICmpInst::ICMP_UGE;\n    break;\n  case 0x4:\n    Pred = ICmpInst::ICMP_EQ;\n    break;\n  case 0x5:\n    Pred = ICmpInst::ICMP_NE;\n    break;\n  case 0x6:\n    return llvm::Constant::getNullValue(Ty); // FALSE\n  case 0x7:\n    return llvm::Constant::getAllOnesValue(Ty); // TRUE\n  default:\n    llvm_unreachable(\"Unexpected XOP vpcom/vpcomu predicate\");\n  }\n\n  Value *Cmp = CGF.Builder.CreateICmp(Pred, Op0, Op1);\n  Value *Res = CGF.Builder.CreateSExt(Cmp, Ty);\n  return Res;\n}\n\nstatic Value *EmitX86Select(CodeGenFunction &CGF,\n                            Value *Mask, Value *Op0, Value *Op1) {\n\n  // If the mask is all ones just return first argument.\n  if (const auto *C = dyn_cast<Constant>(Mask))\n    if (C->isAllOnesValue())\n      return Op0;\n\n  Mask = getMaskVecValue(\n      CGF, Mask, cast<llvm::FixedVectorType>(Op0->getType())->getNumElements());\n\n  return CGF.Builder.CreateSelect(Mask, Op0, Op1);\n}\n\nstatic Value *EmitX86ScalarSelect(CodeGenFunction &CGF,\n                                  Value *Mask, Value *Op0, Value *Op1) {\n  // If the mask is all ones just return first argument.\n  if (const auto *C = dyn_cast<Constant>(Mask))\n    if (C->isAllOnesValue())\n      return Op0;\n\n  auto *MaskTy = llvm::FixedVectorType::get(\n      CGF.Builder.getInt1Ty(), Mask->getType()->getIntegerBitWidth());\n  Mask = CGF.Builder.CreateBitCast(Mask, MaskTy);\n  Mask = CGF.Builder.CreateExtractElement(Mask, (uint64_t)0);\n  return CGF.Builder.CreateSelect(Mask, Op0, Op1);\n}\n\nstatic Value *EmitX86MaskedCompareResult(CodeGenFunction &CGF, Value *Cmp,\n                                         unsigned NumElts, Value *MaskIn) {\n  if (MaskIn) {\n    const auto *C = dyn_cast<Constant>(MaskIn);\n    if (!C || !C->isAllOnesValue())\n      Cmp = CGF.Builder.CreateAnd(Cmp, getMaskVecValue(CGF, MaskIn, NumElts));\n  }\n\n  if (NumElts < 8) {\n    int Indices[8];\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = i;\n    for (unsigned i = NumElts; i != 8; ++i)\n      Indices[i] = i % NumElts + NumElts;\n    Cmp = CGF.Builder.CreateShuffleVector(\n        Cmp, llvm::Constant::getNullValue(Cmp->getType()), Indices);\n  }\n\n  return CGF.Builder.CreateBitCast(Cmp,\n                                   IntegerType::get(CGF.getLLVMContext(),\n                                                    std::max(NumElts, 8U)));\n}\n\nstatic Value *EmitX86MaskedCompare(CodeGenFunction &CGF, unsigned CC,\n                                   bool Signed, ArrayRef<Value *> Ops) {\n  assert((Ops.size() == 2 || Ops.size() == 4) &&\n         \"Unexpected number of arguments\");\n  unsigned NumElts =\n      cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n  Value *Cmp;\n\n  if (CC == 3) {\n    Cmp = Constant::getNullValue(\n        llvm::FixedVectorType::get(CGF.Builder.getInt1Ty(), NumElts));\n  } else if (CC == 7) {\n    Cmp = Constant::getAllOnesValue(\n        llvm::FixedVectorType::get(CGF.Builder.getInt1Ty(), NumElts));\n  } else {\n    ICmpInst::Predicate Pred;\n    switch (CC) {\n    default: llvm_unreachable(\"Unknown condition code\");\n    case 0: Pred = ICmpInst::ICMP_EQ;  break;\n    case 1: Pred = Signed ? ICmpInst::ICMP_SLT : ICmpInst::ICMP_ULT; break;\n    case 2: Pred = Signed ? ICmpInst::ICMP_SLE : ICmpInst::ICMP_ULE; break;\n    case 4: Pred = ICmpInst::ICMP_NE;  break;\n    case 5: Pred = Signed ? ICmpInst::ICMP_SGE : ICmpInst::ICMP_UGE; break;\n    case 6: Pred = Signed ? ICmpInst::ICMP_SGT : ICmpInst::ICMP_UGT; break;\n    }\n    Cmp = CGF.Builder.CreateICmp(Pred, Ops[0], Ops[1]);\n  }\n\n  Value *MaskIn = nullptr;\n  if (Ops.size() == 4)\n    MaskIn = Ops[3];\n\n  return EmitX86MaskedCompareResult(CGF, Cmp, NumElts, MaskIn);\n}\n\nstatic Value *EmitX86ConvertToMask(CodeGenFunction &CGF, Value *In) {\n  Value *Zero = Constant::getNullValue(In->getType());\n  return EmitX86MaskedCompare(CGF, 1, true, { In, Zero });\n}\n\nstatic Value *EmitX86ConvertIntToFp(CodeGenFunction &CGF,\n                                    ArrayRef<Value *> Ops, bool IsSigned) {\n  unsigned Rnd = cast<llvm::ConstantInt>(Ops[3])->getZExtValue();\n  llvm::Type *Ty = Ops[1]->getType();\n\n  Value *Res;\n  if (Rnd != 4) {\n    Intrinsic::ID IID = IsSigned ? Intrinsic::x86_avx512_sitofp_round\n                                 : Intrinsic::x86_avx512_uitofp_round;\n    Function *F = CGF.CGM.getIntrinsic(IID, { Ty, Ops[0]->getType() });\n    Res = CGF.Builder.CreateCall(F, { Ops[0], Ops[3] });\n  } else {\n    Res = IsSigned ? CGF.Builder.CreateSIToFP(Ops[0], Ty)\n                   : CGF.Builder.CreateUIToFP(Ops[0], Ty);\n  }\n\n  return EmitX86Select(CGF, Ops[2], Res, Ops[1]);\n}\n\n// Lowers X86 FMA intrinsics to IR.\nstatic Value *EmitX86FMAExpr(CodeGenFunction &CGF, ArrayRef<Value *> Ops,\n                             unsigned BuiltinID, bool IsAddSub) {\n\n  bool Subtract = false;\n  Intrinsic::ID IID = Intrinsic::not_intrinsic;\n  switch (BuiltinID) {\n  default: break;\n  case clang::X86::BI__builtin_ia32_vfmsubps512_mask3:\n    Subtract = true;\n    LLVM_FALLTHROUGH;\n  case clang::X86::BI__builtin_ia32_vfmaddps512_mask:\n  case clang::X86::BI__builtin_ia32_vfmaddps512_maskz:\n  case clang::X86::BI__builtin_ia32_vfmaddps512_mask3:\n    IID = llvm::Intrinsic::x86_avx512_vfmadd_ps_512; break;\n  case clang::X86::BI__builtin_ia32_vfmsubpd512_mask3:\n    Subtract = true;\n    LLVM_FALLTHROUGH;\n  case clang::X86::BI__builtin_ia32_vfmaddpd512_mask:\n  case clang::X86::BI__builtin_ia32_vfmaddpd512_maskz:\n  case clang::X86::BI__builtin_ia32_vfmaddpd512_mask3:\n    IID = llvm::Intrinsic::x86_avx512_vfmadd_pd_512; break;\n  case clang::X86::BI__builtin_ia32_vfmsubaddps512_mask3:\n    Subtract = true;\n    LLVM_FALLTHROUGH;\n  case clang::X86::BI__builtin_ia32_vfmaddsubps512_mask:\n  case clang::X86::BI__builtin_ia32_vfmaddsubps512_maskz:\n  case clang::X86::BI__builtin_ia32_vfmaddsubps512_mask3:\n    IID = llvm::Intrinsic::x86_avx512_vfmaddsub_ps_512;\n    break;\n  case clang::X86::BI__builtin_ia32_vfmsubaddpd512_mask3:\n    Subtract = true;\n    LLVM_FALLTHROUGH;\n  case clang::X86::BI__builtin_ia32_vfmaddsubpd512_mask:\n  case clang::X86::BI__builtin_ia32_vfmaddsubpd512_maskz:\n  case clang::X86::BI__builtin_ia32_vfmaddsubpd512_mask3:\n    IID = llvm::Intrinsic::x86_avx512_vfmaddsub_pd_512;\n    break;\n  }\n\n  Value *A = Ops[0];\n  Value *B = Ops[1];\n  Value *C = Ops[2];\n\n  if (Subtract)\n    C = CGF.Builder.CreateFNeg(C);\n\n  Value *Res;\n\n  // Only handle in case of _MM_FROUND_CUR_DIRECTION/4 (no rounding).\n  if (IID != Intrinsic::not_intrinsic &&\n      (cast<llvm::ConstantInt>(Ops.back())->getZExtValue() != (uint64_t)4 ||\n       IsAddSub)) {\n    Function *Intr = CGF.CGM.getIntrinsic(IID);\n    Res = CGF.Builder.CreateCall(Intr, {A, B, C, Ops.back() });\n  } else {\n    llvm::Type *Ty = A->getType();\n    Function *FMA;\n    if (CGF.Builder.getIsFPConstrained()) {\n      FMA = CGF.CGM.getIntrinsic(Intrinsic::experimental_constrained_fma, Ty);\n      Res = CGF.Builder.CreateConstrainedFPCall(FMA, {A, B, C});\n    } else {\n      FMA = CGF.CGM.getIntrinsic(Intrinsic::fma, Ty);\n      Res = CGF.Builder.CreateCall(FMA, {A, B, C});\n    }\n  }\n\n  // Handle any required masking.\n  Value *MaskFalseVal = nullptr;\n  switch (BuiltinID) {\n  case clang::X86::BI__builtin_ia32_vfmaddps512_mask:\n  case clang::X86::BI__builtin_ia32_vfmaddpd512_mask:\n  case clang::X86::BI__builtin_ia32_vfmaddsubps512_mask:\n  case clang::X86::BI__builtin_ia32_vfmaddsubpd512_mask:\n    MaskFalseVal = Ops[0];\n    break;\n  case clang::X86::BI__builtin_ia32_vfmaddps512_maskz:\n  case clang::X86::BI__builtin_ia32_vfmaddpd512_maskz:\n  case clang::X86::BI__builtin_ia32_vfmaddsubps512_maskz:\n  case clang::X86::BI__builtin_ia32_vfmaddsubpd512_maskz:\n    MaskFalseVal = Constant::getNullValue(Ops[0]->getType());\n    break;\n  case clang::X86::BI__builtin_ia32_vfmsubps512_mask3:\n  case clang::X86::BI__builtin_ia32_vfmaddps512_mask3:\n  case clang::X86::BI__builtin_ia32_vfmsubpd512_mask3:\n  case clang::X86::BI__builtin_ia32_vfmaddpd512_mask3:\n  case clang::X86::BI__builtin_ia32_vfmsubaddps512_mask3:\n  case clang::X86::BI__builtin_ia32_vfmaddsubps512_mask3:\n  case clang::X86::BI__builtin_ia32_vfmsubaddpd512_mask3:\n  case clang::X86::BI__builtin_ia32_vfmaddsubpd512_mask3:\n    MaskFalseVal = Ops[2];\n    break;\n  }\n\n  if (MaskFalseVal)\n    return EmitX86Select(CGF, Ops[3], Res, MaskFalseVal);\n\n  return Res;\n}\n\nstatic Value *\nEmitScalarFMAExpr(CodeGenFunction &CGF, MutableArrayRef<Value *> Ops,\n                  Value *Upper, bool ZeroMask = false, unsigned PTIdx = 0,\n                  bool NegAcc = false) {\n  unsigned Rnd = 4;\n  if (Ops.size() > 4)\n    Rnd = cast<llvm::ConstantInt>(Ops[4])->getZExtValue();\n\n  if (NegAcc)\n    Ops[2] = CGF.Builder.CreateFNeg(Ops[2]);\n\n  Ops[0] = CGF.Builder.CreateExtractElement(Ops[0], (uint64_t)0);\n  Ops[1] = CGF.Builder.CreateExtractElement(Ops[1], (uint64_t)0);\n  Ops[2] = CGF.Builder.CreateExtractElement(Ops[2], (uint64_t)0);\n  Value *Res;\n  if (Rnd != 4) {\n    Intrinsic::ID IID = Ops[0]->getType()->getPrimitiveSizeInBits() == 32 ?\n                        Intrinsic::x86_avx512_vfmadd_f32 :\n                        Intrinsic::x86_avx512_vfmadd_f64;\n    Res = CGF.Builder.CreateCall(CGF.CGM.getIntrinsic(IID),\n                                 {Ops[0], Ops[1], Ops[2], Ops[4]});\n  } else if (CGF.Builder.getIsFPConstrained()) {\n    Function *FMA = CGF.CGM.getIntrinsic(\n        Intrinsic::experimental_constrained_fma, Ops[0]->getType());\n    Res = CGF.Builder.CreateConstrainedFPCall(FMA, Ops.slice(0, 3));\n  } else {\n    Function *FMA = CGF.CGM.getIntrinsic(Intrinsic::fma, Ops[0]->getType());\n    Res = CGF.Builder.CreateCall(FMA, Ops.slice(0, 3));\n  }\n  // If we have more than 3 arguments, we need to do masking.\n  if (Ops.size() > 3) {\n    Value *PassThru = ZeroMask ? Constant::getNullValue(Res->getType())\n                               : Ops[PTIdx];\n\n    // If we negated the accumulator and the its the PassThru value we need to\n    // bypass the negate. Conveniently Upper should be the same thing in this\n    // case.\n    if (NegAcc && PTIdx == 2)\n      PassThru = CGF.Builder.CreateExtractElement(Upper, (uint64_t)0);\n\n    Res = EmitX86ScalarSelect(CGF, Ops[3], Res, PassThru);\n  }\n  return CGF.Builder.CreateInsertElement(Upper, Res, (uint64_t)0);\n}\n\nstatic Value *EmitX86Muldq(CodeGenFunction &CGF, bool IsSigned,\n                           ArrayRef<Value *> Ops) {\n  llvm::Type *Ty = Ops[0]->getType();\n  // Arguments have a vXi32 type so cast to vXi64.\n  Ty = llvm::FixedVectorType::get(CGF.Int64Ty,\n                                  Ty->getPrimitiveSizeInBits() / 64);\n  Value *LHS = CGF.Builder.CreateBitCast(Ops[0], Ty);\n  Value *RHS = CGF.Builder.CreateBitCast(Ops[1], Ty);\n\n  if (IsSigned) {\n    // Shift left then arithmetic shift right.\n    Constant *ShiftAmt = ConstantInt::get(Ty, 32);\n    LHS = CGF.Builder.CreateShl(LHS, ShiftAmt);\n    LHS = CGF.Builder.CreateAShr(LHS, ShiftAmt);\n    RHS = CGF.Builder.CreateShl(RHS, ShiftAmt);\n    RHS = CGF.Builder.CreateAShr(RHS, ShiftAmt);\n  } else {\n    // Clear the upper bits.\n    Constant *Mask = ConstantInt::get(Ty, 0xffffffff);\n    LHS = CGF.Builder.CreateAnd(LHS, Mask);\n    RHS = CGF.Builder.CreateAnd(RHS, Mask);\n  }\n\n  return CGF.Builder.CreateMul(LHS, RHS);\n}\n\n// Emit a masked pternlog intrinsic. This only exists because the header has to\n// use a macro and we aren't able to pass the input argument to a pternlog\n// builtin and a select builtin without evaluating it twice.\nstatic Value *EmitX86Ternlog(CodeGenFunction &CGF, bool ZeroMask,\n                             ArrayRef<Value *> Ops) {\n  llvm::Type *Ty = Ops[0]->getType();\n\n  unsigned VecWidth = Ty->getPrimitiveSizeInBits();\n  unsigned EltWidth = Ty->getScalarSizeInBits();\n  Intrinsic::ID IID;\n  if (VecWidth == 128 && EltWidth == 32)\n    IID = Intrinsic::x86_avx512_pternlog_d_128;\n  else if (VecWidth == 256 && EltWidth == 32)\n    IID = Intrinsic::x86_avx512_pternlog_d_256;\n  else if (VecWidth == 512 && EltWidth == 32)\n    IID = Intrinsic::x86_avx512_pternlog_d_512;\n  else if (VecWidth == 128 && EltWidth == 64)\n    IID = Intrinsic::x86_avx512_pternlog_q_128;\n  else if (VecWidth == 256 && EltWidth == 64)\n    IID = Intrinsic::x86_avx512_pternlog_q_256;\n  else if (VecWidth == 512 && EltWidth == 64)\n    IID = Intrinsic::x86_avx512_pternlog_q_512;\n  else\n    llvm_unreachable(\"Unexpected intrinsic\");\n\n  Value *Ternlog = CGF.Builder.CreateCall(CGF.CGM.getIntrinsic(IID),\n                                          Ops.drop_back());\n  Value *PassThru = ZeroMask ? ConstantAggregateZero::get(Ty) : Ops[0];\n  return EmitX86Select(CGF, Ops[4], Ternlog, PassThru);\n}\n\nstatic Value *EmitX86SExtMask(CodeGenFunction &CGF, Value *Op,\n                              llvm::Type *DstTy) {\n  unsigned NumberOfElements =\n      cast<llvm::FixedVectorType>(DstTy)->getNumElements();\n  Value *Mask = getMaskVecValue(CGF, Op, NumberOfElements);\n  return CGF.Builder.CreateSExt(Mask, DstTy, \"vpmovm2\");\n}\n\n// Emit binary intrinsic with the same type used in result/args.\nstatic Value *EmitX86BinaryIntrinsic(CodeGenFunction &CGF,\n                                     ArrayRef<Value *> Ops, Intrinsic::ID IID) {\n  llvm::Function *F = CGF.CGM.getIntrinsic(IID, Ops[0]->getType());\n  return CGF.Builder.CreateCall(F, {Ops[0], Ops[1]});\n}\n\nValue *CodeGenFunction::EmitX86CpuIs(const CallExpr *E) {\n  const Expr *CPUExpr = E->getArg(0)->IgnoreParenCasts();\n  StringRef CPUStr = cast<clang::StringLiteral>(CPUExpr)->getString();\n  return EmitX86CpuIs(CPUStr);\n}\n\n// Convert F16 halfs to floats.\nstatic Value *EmitX86CvtF16ToFloatExpr(CodeGenFunction &CGF,\n                                       ArrayRef<Value *> Ops,\n                                       llvm::Type *DstTy) {\n  assert((Ops.size() == 1 || Ops.size() == 3 || Ops.size() == 4) &&\n         \"Unknown cvtph2ps intrinsic\");\n\n  // If the SAE intrinsic doesn't use default rounding then we can't upgrade.\n  if (Ops.size() == 4 && cast<llvm::ConstantInt>(Ops[3])->getZExtValue() != 4) {\n    Function *F =\n        CGF.CGM.getIntrinsic(Intrinsic::x86_avx512_mask_vcvtph2ps_512);\n    return CGF.Builder.CreateCall(F, {Ops[0], Ops[1], Ops[2], Ops[3]});\n  }\n\n  unsigned NumDstElts = cast<llvm::FixedVectorType>(DstTy)->getNumElements();\n  Value *Src = Ops[0];\n\n  // Extract the subvector.\n  if (NumDstElts !=\n      cast<llvm::FixedVectorType>(Src->getType())->getNumElements()) {\n    assert(NumDstElts == 4 && \"Unexpected vector size\");\n    Src = CGF.Builder.CreateShuffleVector(Src, ArrayRef<int>{0, 1, 2, 3});\n  }\n\n  // Bitcast from vXi16 to vXf16.\n  auto *HalfTy = llvm::FixedVectorType::get(\n      llvm::Type::getHalfTy(CGF.getLLVMContext()), NumDstElts);\n  Src = CGF.Builder.CreateBitCast(Src, HalfTy);\n\n  // Perform the fp-extension.\n  Value *Res = CGF.Builder.CreateFPExt(Src, DstTy, \"cvtph2ps\");\n\n  if (Ops.size() >= 3)\n    Res = EmitX86Select(CGF, Ops[2], Res, Ops[1]);\n  return Res;\n}\n\n// Convert a BF16 to a float.\nstatic Value *EmitX86CvtBF16ToFloatExpr(CodeGenFunction &CGF,\n                                        const CallExpr *E,\n                                        ArrayRef<Value *> Ops) {\n  llvm::Type *Int32Ty = CGF.Builder.getInt32Ty();\n  Value *ZeroExt = CGF.Builder.CreateZExt(Ops[0], Int32Ty);\n  Value *Shl = CGF.Builder.CreateShl(ZeroExt, 16);\n  llvm::Type *ResultType = CGF.ConvertType(E->getType());\n  Value *BitCast = CGF.Builder.CreateBitCast(Shl, ResultType);\n  return BitCast;\n}\n\nValue *CodeGenFunction::EmitX86CpuIs(StringRef CPUStr) {\n\n  llvm::Type *Int32Ty = Builder.getInt32Ty();\n\n  // Matching the struct layout from the compiler-rt/libgcc structure that is\n  // filled in:\n  // unsigned int __cpu_vendor;\n  // unsigned int __cpu_type;\n  // unsigned int __cpu_subtype;\n  // unsigned int __cpu_features[1];\n  llvm::Type *STy = llvm::StructType::get(Int32Ty, Int32Ty, Int32Ty,\n                                          llvm::ArrayType::get(Int32Ty, 1));\n\n  // Grab the global __cpu_model.\n  llvm::Constant *CpuModel = CGM.CreateRuntimeVariable(STy, \"__cpu_model\");\n  cast<llvm::GlobalValue>(CpuModel)->setDSOLocal(true);\n\n  // Calculate the index needed to access the correct field based on the\n  // range. Also adjust the expected value.\n  unsigned Index;\n  unsigned Value;\n  std::tie(Index, Value) = StringSwitch<std::pair<unsigned, unsigned>>(CPUStr)\n#define X86_VENDOR(ENUM, STRING)                                               \\\n  .Case(STRING, {0u, static_cast<unsigned>(llvm::X86::ENUM)})\n#define X86_CPU_TYPE_ALIAS(ENUM, ALIAS)                                        \\\n  .Case(ALIAS, {1u, static_cast<unsigned>(llvm::X86::ENUM)})\n#define X86_CPU_TYPE(ENUM, STR)                                                \\\n  .Case(STR, {1u, static_cast<unsigned>(llvm::X86::ENUM)})\n#define X86_CPU_SUBTYPE(ENUM, STR)                                             \\\n  .Case(STR, {2u, static_cast<unsigned>(llvm::X86::ENUM)})\n#include \"llvm/Support/X86TargetParser.def\"\n                               .Default({0, 0});\n  assert(Value != 0 && \"Invalid CPUStr passed to CpuIs\");\n\n  // Grab the appropriate field from __cpu_model.\n  llvm::Value *Idxs[] = {ConstantInt::get(Int32Ty, 0),\n                         ConstantInt::get(Int32Ty, Index)};\n  llvm::Value *CpuValue = Builder.CreateGEP(STy, CpuModel, Idxs);\n  CpuValue = Builder.CreateAlignedLoad(CpuValue, CharUnits::fromQuantity(4));\n\n  // Check the value of the field against the requested value.\n  return Builder.CreateICmpEQ(CpuValue,\n                                  llvm::ConstantInt::get(Int32Ty, Value));\n}\n\nValue *CodeGenFunction::EmitX86CpuSupports(const CallExpr *E) {\n  const Expr *FeatureExpr = E->getArg(0)->IgnoreParenCasts();\n  StringRef FeatureStr = cast<StringLiteral>(FeatureExpr)->getString();\n  return EmitX86CpuSupports(FeatureStr);\n}\n\nuint64_t\nCodeGenFunction::GetX86CpuSupportsMask(ArrayRef<StringRef> FeatureStrs) {\n  // Processor features and mapping to processor feature value.\n  uint64_t FeaturesMask = 0;\n  for (const StringRef &FeatureStr : FeatureStrs) {\n    unsigned Feature =\n        StringSwitch<unsigned>(FeatureStr)\n#define X86_FEATURE_COMPAT(ENUM, STR) .Case(STR, llvm::X86::FEATURE_##ENUM)\n#include \"llvm/Support/X86TargetParser.def\"\n        ;\n    FeaturesMask |= (1ULL << Feature);\n  }\n  return FeaturesMask;\n}\n\nValue *CodeGenFunction::EmitX86CpuSupports(ArrayRef<StringRef> FeatureStrs) {\n  return EmitX86CpuSupports(GetX86CpuSupportsMask(FeatureStrs));\n}\n\nllvm::Value *CodeGenFunction::EmitX86CpuSupports(uint64_t FeaturesMask) {\n  uint32_t Features1 = Lo_32(FeaturesMask);\n  uint32_t Features2 = Hi_32(FeaturesMask);\n\n  Value *Result = Builder.getTrue();\n\n  if (Features1 != 0) {\n    // Matching the struct layout from the compiler-rt/libgcc structure that is\n    // filled in:\n    // unsigned int __cpu_vendor;\n    // unsigned int __cpu_type;\n    // unsigned int __cpu_subtype;\n    // unsigned int __cpu_features[1];\n    llvm::Type *STy = llvm::StructType::get(Int32Ty, Int32Ty, Int32Ty,\n                                            llvm::ArrayType::get(Int32Ty, 1));\n\n    // Grab the global __cpu_model.\n    llvm::Constant *CpuModel = CGM.CreateRuntimeVariable(STy, \"__cpu_model\");\n    cast<llvm::GlobalValue>(CpuModel)->setDSOLocal(true);\n\n    // Grab the first (0th) element from the field __cpu_features off of the\n    // global in the struct STy.\n    Value *Idxs[] = {Builder.getInt32(0), Builder.getInt32(3),\n                     Builder.getInt32(0)};\n    Value *CpuFeatures = Builder.CreateGEP(STy, CpuModel, Idxs);\n    Value *Features =\n        Builder.CreateAlignedLoad(CpuFeatures, CharUnits::fromQuantity(4));\n\n    // Check the value of the bit corresponding to the feature requested.\n    Value *Mask = Builder.getInt32(Features1);\n    Value *Bitset = Builder.CreateAnd(Features, Mask);\n    Value *Cmp = Builder.CreateICmpEQ(Bitset, Mask);\n    Result = Builder.CreateAnd(Result, Cmp);\n  }\n\n  if (Features2 != 0) {\n    llvm::Constant *CpuFeatures2 = CGM.CreateRuntimeVariable(Int32Ty,\n                                                             \"__cpu_features2\");\n    cast<llvm::GlobalValue>(CpuFeatures2)->setDSOLocal(true);\n\n    Value *Features =\n        Builder.CreateAlignedLoad(CpuFeatures2, CharUnits::fromQuantity(4));\n\n    // Check the value of the bit corresponding to the feature requested.\n    Value *Mask = Builder.getInt32(Features2);\n    Value *Bitset = Builder.CreateAnd(Features, Mask);\n    Value *Cmp = Builder.CreateICmpEQ(Bitset, Mask);\n    Result = Builder.CreateAnd(Result, Cmp);\n  }\n\n  return Result;\n}\n\nValue *CodeGenFunction::EmitX86CpuInit() {\n  llvm::FunctionType *FTy = llvm::FunctionType::get(VoidTy,\n                                                    /*Variadic*/ false);\n  llvm::FunctionCallee Func =\n      CGM.CreateRuntimeFunction(FTy, \"__cpu_indicator_init\");\n  cast<llvm::GlobalValue>(Func.getCallee())->setDSOLocal(true);\n  cast<llvm::GlobalValue>(Func.getCallee())\n      ->setDLLStorageClass(llvm::GlobalValue::DefaultStorageClass);\n  return Builder.CreateCall(Func);\n}\n\nValue *CodeGenFunction::EmitX86BuiltinExpr(unsigned BuiltinID,\n                                           const CallExpr *E) {\n  if (BuiltinID == X86::BI__builtin_cpu_is)\n    return EmitX86CpuIs(E);\n  if (BuiltinID == X86::BI__builtin_cpu_supports)\n    return EmitX86CpuSupports(E);\n  if (BuiltinID == X86::BI__builtin_cpu_init)\n    return EmitX86CpuInit();\n\n  // Handle MSVC intrinsics before argument evaluation to prevent double\n  // evaluation.\n  if (Optional<MSVCIntrin> MsvcIntId = translateX86ToMsvcIntrin(BuiltinID))\n    return EmitMSVCBuiltinExpr(*MsvcIntId, E);\n\n  SmallVector<Value*, 4> Ops;\n  bool IsMaskFCmp = false;\n\n  // Find out if any arguments are required to be integer constant expressions.\n  unsigned ICEArguments = 0;\n  ASTContext::GetBuiltinTypeError Error;\n  getContext().GetBuiltinType(BuiltinID, Error, &ICEArguments);\n  assert(Error == ASTContext::GE_None && \"Should not codegen an error\");\n\n  for (unsigned i = 0, e = E->getNumArgs(); i != e; i++) {\n    // If this is a normal argument, just emit it as a scalar.\n    if ((ICEArguments & (1 << i)) == 0) {\n      Ops.push_back(EmitScalarExpr(E->getArg(i)));\n      continue;\n    }\n\n    // If this is required to be a constant, constant fold it so that we know\n    // that the generated intrinsic gets a ConstantInt.\n    Ops.push_back(llvm::ConstantInt::get(\n        getLLVMContext(), *E->getArg(i)->getIntegerConstantExpr(getContext())));\n  }\n\n  // These exist so that the builtin that takes an immediate can be bounds\n  // checked by clang to avoid passing bad immediates to the backend. Since\n  // AVX has a larger immediate than SSE we would need separate builtins to\n  // do the different bounds checking. Rather than create a clang specific\n  // SSE only builtin, this implements eight separate builtins to match gcc\n  // implementation.\n  auto getCmpIntrinsicCall = [this, &Ops](Intrinsic::ID ID, unsigned Imm) {\n    Ops.push_back(llvm::ConstantInt::get(Int8Ty, Imm));\n    llvm::Function *F = CGM.getIntrinsic(ID);\n    return Builder.CreateCall(F, Ops);\n  };\n\n  // For the vector forms of FP comparisons, translate the builtins directly to\n  // IR.\n  // TODO: The builtins could be removed if the SSE header files used vector\n  // extension comparisons directly (vector ordered/unordered may need\n  // additional support via __builtin_isnan()).\n  auto getVectorFCmpIR = [this, &Ops](CmpInst::Predicate Pred,\n                                      bool IsSignaling) {\n    Value *Cmp;\n    if (IsSignaling)\n      Cmp = Builder.CreateFCmpS(Pred, Ops[0], Ops[1]);\n    else\n      Cmp = Builder.CreateFCmp(Pred, Ops[0], Ops[1]);\n    llvm::VectorType *FPVecTy = cast<llvm::VectorType>(Ops[0]->getType());\n    llvm::VectorType *IntVecTy = llvm::VectorType::getInteger(FPVecTy);\n    Value *Sext = Builder.CreateSExt(Cmp, IntVecTy);\n    return Builder.CreateBitCast(Sext, FPVecTy);\n  };\n\n  switch (BuiltinID) {\n  default: return nullptr;\n  case X86::BI_mm_prefetch: {\n    Value *Address = Ops[0];\n    ConstantInt *C = cast<ConstantInt>(Ops[1]);\n    Value *RW = ConstantInt::get(Int32Ty, (C->getZExtValue() >> 2) & 0x1);\n    Value *Locality = ConstantInt::get(Int32Ty, C->getZExtValue() & 0x3);\n    Value *Data = ConstantInt::get(Int32Ty, 1);\n    Function *F = CGM.getIntrinsic(Intrinsic::prefetch, Address->getType());\n    return Builder.CreateCall(F, {Address, RW, Locality, Data});\n  }\n  case X86::BI_mm_clflush: {\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_sse2_clflush),\n                              Ops[0]);\n  }\n  case X86::BI_mm_lfence: {\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_sse2_lfence));\n  }\n  case X86::BI_mm_mfence: {\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_sse2_mfence));\n  }\n  case X86::BI_mm_sfence: {\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_sse_sfence));\n  }\n  case X86::BI_mm_pause: {\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_sse2_pause));\n  }\n  case X86::BI__rdtsc: {\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_rdtsc));\n  }\n  case X86::BI__builtin_ia32_rdtscp: {\n    Value *Call = Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_rdtscp));\n    Builder.CreateDefaultAlignedStore(Builder.CreateExtractValue(Call, 1),\n                                      Ops[0]);\n    return Builder.CreateExtractValue(Call, 0);\n  }\n  case X86::BI__builtin_ia32_lzcnt_u16:\n  case X86::BI__builtin_ia32_lzcnt_u32:\n  case X86::BI__builtin_ia32_lzcnt_u64: {\n    Function *F = CGM.getIntrinsic(Intrinsic::ctlz, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0], Builder.getInt1(false)});\n  }\n  case X86::BI__builtin_ia32_tzcnt_u16:\n  case X86::BI__builtin_ia32_tzcnt_u32:\n  case X86::BI__builtin_ia32_tzcnt_u64: {\n    Function *F = CGM.getIntrinsic(Intrinsic::cttz, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0], Builder.getInt1(false)});\n  }\n  case X86::BI__builtin_ia32_undef128:\n  case X86::BI__builtin_ia32_undef256:\n  case X86::BI__builtin_ia32_undef512:\n    // The x86 definition of \"undef\" is not the same as the LLVM definition\n    // (PR32176). We leave optimizing away an unnecessary zero constant to the\n    // IR optimizer and backend.\n    // TODO: If we had a \"freeze\" IR instruction to generate a fixed undef\n    // value, we should use that here instead of a zero.\n    return llvm::Constant::getNullValue(ConvertType(E->getType()));\n  case X86::BI__builtin_ia32_vec_init_v8qi:\n  case X86::BI__builtin_ia32_vec_init_v4hi:\n  case X86::BI__builtin_ia32_vec_init_v2si:\n    return Builder.CreateBitCast(BuildVector(Ops),\n                                 llvm::Type::getX86_MMXTy(getLLVMContext()));\n  case X86::BI__builtin_ia32_vec_ext_v2si:\n  case X86::BI__builtin_ia32_vec_ext_v16qi:\n  case X86::BI__builtin_ia32_vec_ext_v8hi:\n  case X86::BI__builtin_ia32_vec_ext_v4si:\n  case X86::BI__builtin_ia32_vec_ext_v4sf:\n  case X86::BI__builtin_ia32_vec_ext_v2di:\n  case X86::BI__builtin_ia32_vec_ext_v32qi:\n  case X86::BI__builtin_ia32_vec_ext_v16hi:\n  case X86::BI__builtin_ia32_vec_ext_v8si:\n  case X86::BI__builtin_ia32_vec_ext_v4di: {\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    uint64_t Index = cast<ConstantInt>(Ops[1])->getZExtValue();\n    Index &= NumElts - 1;\n    // These builtins exist so we can ensure the index is an ICE and in range.\n    // Otherwise we could just do this in the header file.\n    return Builder.CreateExtractElement(Ops[0], Index);\n  }\n  case X86::BI__builtin_ia32_vec_set_v16qi:\n  case X86::BI__builtin_ia32_vec_set_v8hi:\n  case X86::BI__builtin_ia32_vec_set_v4si:\n  case X86::BI__builtin_ia32_vec_set_v2di:\n  case X86::BI__builtin_ia32_vec_set_v32qi:\n  case X86::BI__builtin_ia32_vec_set_v16hi:\n  case X86::BI__builtin_ia32_vec_set_v8si:\n  case X86::BI__builtin_ia32_vec_set_v4di: {\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    unsigned Index = cast<ConstantInt>(Ops[2])->getZExtValue();\n    Index &= NumElts - 1;\n    // These builtins exist so we can ensure the index is an ICE and in range.\n    // Otherwise we could just do this in the header file.\n    return Builder.CreateInsertElement(Ops[0], Ops[1], Index);\n  }\n  case X86::BI_mm_setcsr:\n  case X86::BI__builtin_ia32_ldmxcsr: {\n    Address Tmp = CreateMemTemp(E->getArg(0)->getType());\n    Builder.CreateStore(Ops[0], Tmp);\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_sse_ldmxcsr),\n                          Builder.CreateBitCast(Tmp.getPointer(), Int8PtrTy));\n  }\n  case X86::BI_mm_getcsr:\n  case X86::BI__builtin_ia32_stmxcsr: {\n    Address Tmp = CreateMemTemp(E->getType());\n    Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_sse_stmxcsr),\n                       Builder.CreateBitCast(Tmp.getPointer(), Int8PtrTy));\n    return Builder.CreateLoad(Tmp, \"stmxcsr\");\n  }\n  case X86::BI__builtin_ia32_xsave:\n  case X86::BI__builtin_ia32_xsave64:\n  case X86::BI__builtin_ia32_xrstor:\n  case X86::BI__builtin_ia32_xrstor64:\n  case X86::BI__builtin_ia32_xsaveopt:\n  case X86::BI__builtin_ia32_xsaveopt64:\n  case X86::BI__builtin_ia32_xrstors:\n  case X86::BI__builtin_ia32_xrstors64:\n  case X86::BI__builtin_ia32_xsavec:\n  case X86::BI__builtin_ia32_xsavec64:\n  case X86::BI__builtin_ia32_xsaves:\n  case X86::BI__builtin_ia32_xsaves64:\n  case X86::BI__builtin_ia32_xsetbv:\n  case X86::BI_xsetbv: {\n    Intrinsic::ID ID;\n#define INTRINSIC_X86_XSAVE_ID(NAME) \\\n    case X86::BI__builtin_ia32_##NAME: \\\n      ID = Intrinsic::x86_##NAME; \\\n      break\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    INTRINSIC_X86_XSAVE_ID(xsave);\n    INTRINSIC_X86_XSAVE_ID(xsave64);\n    INTRINSIC_X86_XSAVE_ID(xrstor);\n    INTRINSIC_X86_XSAVE_ID(xrstor64);\n    INTRINSIC_X86_XSAVE_ID(xsaveopt);\n    INTRINSIC_X86_XSAVE_ID(xsaveopt64);\n    INTRINSIC_X86_XSAVE_ID(xrstors);\n    INTRINSIC_X86_XSAVE_ID(xrstors64);\n    INTRINSIC_X86_XSAVE_ID(xsavec);\n    INTRINSIC_X86_XSAVE_ID(xsavec64);\n    INTRINSIC_X86_XSAVE_ID(xsaves);\n    INTRINSIC_X86_XSAVE_ID(xsaves64);\n    INTRINSIC_X86_XSAVE_ID(xsetbv);\n    case X86::BI_xsetbv:\n      ID = Intrinsic::x86_xsetbv;\n      break;\n    }\n#undef INTRINSIC_X86_XSAVE_ID\n    Value *Mhi = Builder.CreateTrunc(\n      Builder.CreateLShr(Ops[1], ConstantInt::get(Int64Ty, 32)), Int32Ty);\n    Value *Mlo = Builder.CreateTrunc(Ops[1], Int32Ty);\n    Ops[1] = Mhi;\n    Ops.push_back(Mlo);\n    return Builder.CreateCall(CGM.getIntrinsic(ID), Ops);\n  }\n  case X86::BI__builtin_ia32_xgetbv:\n  case X86::BI_xgetbv:\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::x86_xgetbv), Ops);\n  case X86::BI__builtin_ia32_storedqudi128_mask:\n  case X86::BI__builtin_ia32_storedqusi128_mask:\n  case X86::BI__builtin_ia32_storedquhi128_mask:\n  case X86::BI__builtin_ia32_storedquqi128_mask:\n  case X86::BI__builtin_ia32_storeupd128_mask:\n  case X86::BI__builtin_ia32_storeups128_mask:\n  case X86::BI__builtin_ia32_storedqudi256_mask:\n  case X86::BI__builtin_ia32_storedqusi256_mask:\n  case X86::BI__builtin_ia32_storedquhi256_mask:\n  case X86::BI__builtin_ia32_storedquqi256_mask:\n  case X86::BI__builtin_ia32_storeupd256_mask:\n  case X86::BI__builtin_ia32_storeups256_mask:\n  case X86::BI__builtin_ia32_storedqudi512_mask:\n  case X86::BI__builtin_ia32_storedqusi512_mask:\n  case X86::BI__builtin_ia32_storedquhi512_mask:\n  case X86::BI__builtin_ia32_storedquqi512_mask:\n  case X86::BI__builtin_ia32_storeupd512_mask:\n  case X86::BI__builtin_ia32_storeups512_mask:\n    return EmitX86MaskedStore(*this, Ops, Align(1));\n\n  case X86::BI__builtin_ia32_storess128_mask:\n  case X86::BI__builtin_ia32_storesd128_mask:\n    return EmitX86MaskedStore(*this, Ops, Align(1));\n\n  case X86::BI__builtin_ia32_vpopcntb_128:\n  case X86::BI__builtin_ia32_vpopcntd_128:\n  case X86::BI__builtin_ia32_vpopcntq_128:\n  case X86::BI__builtin_ia32_vpopcntw_128:\n  case X86::BI__builtin_ia32_vpopcntb_256:\n  case X86::BI__builtin_ia32_vpopcntd_256:\n  case X86::BI__builtin_ia32_vpopcntq_256:\n  case X86::BI__builtin_ia32_vpopcntw_256:\n  case X86::BI__builtin_ia32_vpopcntb_512:\n  case X86::BI__builtin_ia32_vpopcntd_512:\n  case X86::BI__builtin_ia32_vpopcntq_512:\n  case X86::BI__builtin_ia32_vpopcntw_512: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::ctpop, ResultType);\n    return Builder.CreateCall(F, Ops);\n  }\n  case X86::BI__builtin_ia32_cvtmask2b128:\n  case X86::BI__builtin_ia32_cvtmask2b256:\n  case X86::BI__builtin_ia32_cvtmask2b512:\n  case X86::BI__builtin_ia32_cvtmask2w128:\n  case X86::BI__builtin_ia32_cvtmask2w256:\n  case X86::BI__builtin_ia32_cvtmask2w512:\n  case X86::BI__builtin_ia32_cvtmask2d128:\n  case X86::BI__builtin_ia32_cvtmask2d256:\n  case X86::BI__builtin_ia32_cvtmask2d512:\n  case X86::BI__builtin_ia32_cvtmask2q128:\n  case X86::BI__builtin_ia32_cvtmask2q256:\n  case X86::BI__builtin_ia32_cvtmask2q512:\n    return EmitX86SExtMask(*this, Ops[0], ConvertType(E->getType()));\n\n  case X86::BI__builtin_ia32_cvtb2mask128:\n  case X86::BI__builtin_ia32_cvtb2mask256:\n  case X86::BI__builtin_ia32_cvtb2mask512:\n  case X86::BI__builtin_ia32_cvtw2mask128:\n  case X86::BI__builtin_ia32_cvtw2mask256:\n  case X86::BI__builtin_ia32_cvtw2mask512:\n  case X86::BI__builtin_ia32_cvtd2mask128:\n  case X86::BI__builtin_ia32_cvtd2mask256:\n  case X86::BI__builtin_ia32_cvtd2mask512:\n  case X86::BI__builtin_ia32_cvtq2mask128:\n  case X86::BI__builtin_ia32_cvtq2mask256:\n  case X86::BI__builtin_ia32_cvtq2mask512:\n    return EmitX86ConvertToMask(*this, Ops[0]);\n\n  case X86::BI__builtin_ia32_cvtdq2ps512_mask:\n  case X86::BI__builtin_ia32_cvtqq2ps512_mask:\n  case X86::BI__builtin_ia32_cvtqq2pd512_mask:\n    return EmitX86ConvertIntToFp(*this, Ops, /*IsSigned*/true);\n  case X86::BI__builtin_ia32_cvtudq2ps512_mask:\n  case X86::BI__builtin_ia32_cvtuqq2ps512_mask:\n  case X86::BI__builtin_ia32_cvtuqq2pd512_mask:\n    return EmitX86ConvertIntToFp(*this, Ops, /*IsSigned*/false);\n\n  case X86::BI__builtin_ia32_vfmaddss3:\n  case X86::BI__builtin_ia32_vfmaddsd3:\n  case X86::BI__builtin_ia32_vfmaddss3_mask:\n  case X86::BI__builtin_ia32_vfmaddsd3_mask:\n    return EmitScalarFMAExpr(*this, Ops, Ops[0]);\n  case X86::BI__builtin_ia32_vfmaddss:\n  case X86::BI__builtin_ia32_vfmaddsd:\n    return EmitScalarFMAExpr(*this, Ops,\n                             Constant::getNullValue(Ops[0]->getType()));\n  case X86::BI__builtin_ia32_vfmaddss3_maskz:\n  case X86::BI__builtin_ia32_vfmaddsd3_maskz:\n    return EmitScalarFMAExpr(*this, Ops, Ops[0], /*ZeroMask*/true);\n  case X86::BI__builtin_ia32_vfmaddss3_mask3:\n  case X86::BI__builtin_ia32_vfmaddsd3_mask3:\n    return EmitScalarFMAExpr(*this, Ops, Ops[2], /*ZeroMask*/false, 2);\n  case X86::BI__builtin_ia32_vfmsubss3_mask3:\n  case X86::BI__builtin_ia32_vfmsubsd3_mask3:\n    return EmitScalarFMAExpr(*this, Ops, Ops[2], /*ZeroMask*/false, 2,\n                             /*NegAcc*/true);\n  case X86::BI__builtin_ia32_vfmaddps:\n  case X86::BI__builtin_ia32_vfmaddpd:\n  case X86::BI__builtin_ia32_vfmaddps256:\n  case X86::BI__builtin_ia32_vfmaddpd256:\n  case X86::BI__builtin_ia32_vfmaddps512_mask:\n  case X86::BI__builtin_ia32_vfmaddps512_maskz:\n  case X86::BI__builtin_ia32_vfmaddps512_mask3:\n  case X86::BI__builtin_ia32_vfmsubps512_mask3:\n  case X86::BI__builtin_ia32_vfmaddpd512_mask:\n  case X86::BI__builtin_ia32_vfmaddpd512_maskz:\n  case X86::BI__builtin_ia32_vfmaddpd512_mask3:\n  case X86::BI__builtin_ia32_vfmsubpd512_mask3:\n    return EmitX86FMAExpr(*this, Ops, BuiltinID, /*IsAddSub*/false);\n  case X86::BI__builtin_ia32_vfmaddsubps512_mask:\n  case X86::BI__builtin_ia32_vfmaddsubps512_maskz:\n  case X86::BI__builtin_ia32_vfmaddsubps512_mask3:\n  case X86::BI__builtin_ia32_vfmsubaddps512_mask3:\n  case X86::BI__builtin_ia32_vfmaddsubpd512_mask:\n  case X86::BI__builtin_ia32_vfmaddsubpd512_maskz:\n  case X86::BI__builtin_ia32_vfmaddsubpd512_mask3:\n  case X86::BI__builtin_ia32_vfmsubaddpd512_mask3:\n    return EmitX86FMAExpr(*this, Ops, BuiltinID, /*IsAddSub*/true);\n\n  case X86::BI__builtin_ia32_movdqa32store128_mask:\n  case X86::BI__builtin_ia32_movdqa64store128_mask:\n  case X86::BI__builtin_ia32_storeaps128_mask:\n  case X86::BI__builtin_ia32_storeapd128_mask:\n  case X86::BI__builtin_ia32_movdqa32store256_mask:\n  case X86::BI__builtin_ia32_movdqa64store256_mask:\n  case X86::BI__builtin_ia32_storeaps256_mask:\n  case X86::BI__builtin_ia32_storeapd256_mask:\n  case X86::BI__builtin_ia32_movdqa32store512_mask:\n  case X86::BI__builtin_ia32_movdqa64store512_mask:\n  case X86::BI__builtin_ia32_storeaps512_mask:\n  case X86::BI__builtin_ia32_storeapd512_mask:\n    return EmitX86MaskedStore(\n        *this, Ops,\n        getContext().getTypeAlignInChars(E->getArg(1)->getType()).getAsAlign());\n\n  case X86::BI__builtin_ia32_loadups128_mask:\n  case X86::BI__builtin_ia32_loadups256_mask:\n  case X86::BI__builtin_ia32_loadups512_mask:\n  case X86::BI__builtin_ia32_loadupd128_mask:\n  case X86::BI__builtin_ia32_loadupd256_mask:\n  case X86::BI__builtin_ia32_loadupd512_mask:\n  case X86::BI__builtin_ia32_loaddquqi128_mask:\n  case X86::BI__builtin_ia32_loaddquqi256_mask:\n  case X86::BI__builtin_ia32_loaddquqi512_mask:\n  case X86::BI__builtin_ia32_loaddquhi128_mask:\n  case X86::BI__builtin_ia32_loaddquhi256_mask:\n  case X86::BI__builtin_ia32_loaddquhi512_mask:\n  case X86::BI__builtin_ia32_loaddqusi128_mask:\n  case X86::BI__builtin_ia32_loaddqusi256_mask:\n  case X86::BI__builtin_ia32_loaddqusi512_mask:\n  case X86::BI__builtin_ia32_loaddqudi128_mask:\n  case X86::BI__builtin_ia32_loaddqudi256_mask:\n  case X86::BI__builtin_ia32_loaddqudi512_mask:\n    return EmitX86MaskedLoad(*this, Ops, Align(1));\n\n  case X86::BI__builtin_ia32_loadss128_mask:\n  case X86::BI__builtin_ia32_loadsd128_mask:\n    return EmitX86MaskedLoad(*this, Ops, Align(1));\n\n  case X86::BI__builtin_ia32_loadaps128_mask:\n  case X86::BI__builtin_ia32_loadaps256_mask:\n  case X86::BI__builtin_ia32_loadaps512_mask:\n  case X86::BI__builtin_ia32_loadapd128_mask:\n  case X86::BI__builtin_ia32_loadapd256_mask:\n  case X86::BI__builtin_ia32_loadapd512_mask:\n  case X86::BI__builtin_ia32_movdqa32load128_mask:\n  case X86::BI__builtin_ia32_movdqa32load256_mask:\n  case X86::BI__builtin_ia32_movdqa32load512_mask:\n  case X86::BI__builtin_ia32_movdqa64load128_mask:\n  case X86::BI__builtin_ia32_movdqa64load256_mask:\n  case X86::BI__builtin_ia32_movdqa64load512_mask:\n    return EmitX86MaskedLoad(\n        *this, Ops,\n        getContext().getTypeAlignInChars(E->getArg(1)->getType()).getAsAlign());\n\n  case X86::BI__builtin_ia32_expandloaddf128_mask:\n  case X86::BI__builtin_ia32_expandloaddf256_mask:\n  case X86::BI__builtin_ia32_expandloaddf512_mask:\n  case X86::BI__builtin_ia32_expandloadsf128_mask:\n  case X86::BI__builtin_ia32_expandloadsf256_mask:\n  case X86::BI__builtin_ia32_expandloadsf512_mask:\n  case X86::BI__builtin_ia32_expandloaddi128_mask:\n  case X86::BI__builtin_ia32_expandloaddi256_mask:\n  case X86::BI__builtin_ia32_expandloaddi512_mask:\n  case X86::BI__builtin_ia32_expandloadsi128_mask:\n  case X86::BI__builtin_ia32_expandloadsi256_mask:\n  case X86::BI__builtin_ia32_expandloadsi512_mask:\n  case X86::BI__builtin_ia32_expandloadhi128_mask:\n  case X86::BI__builtin_ia32_expandloadhi256_mask:\n  case X86::BI__builtin_ia32_expandloadhi512_mask:\n  case X86::BI__builtin_ia32_expandloadqi128_mask:\n  case X86::BI__builtin_ia32_expandloadqi256_mask:\n  case X86::BI__builtin_ia32_expandloadqi512_mask:\n    return EmitX86ExpandLoad(*this, Ops);\n\n  case X86::BI__builtin_ia32_compressstoredf128_mask:\n  case X86::BI__builtin_ia32_compressstoredf256_mask:\n  case X86::BI__builtin_ia32_compressstoredf512_mask:\n  case X86::BI__builtin_ia32_compressstoresf128_mask:\n  case X86::BI__builtin_ia32_compressstoresf256_mask:\n  case X86::BI__builtin_ia32_compressstoresf512_mask:\n  case X86::BI__builtin_ia32_compressstoredi128_mask:\n  case X86::BI__builtin_ia32_compressstoredi256_mask:\n  case X86::BI__builtin_ia32_compressstoredi512_mask:\n  case X86::BI__builtin_ia32_compressstoresi128_mask:\n  case X86::BI__builtin_ia32_compressstoresi256_mask:\n  case X86::BI__builtin_ia32_compressstoresi512_mask:\n  case X86::BI__builtin_ia32_compressstorehi128_mask:\n  case X86::BI__builtin_ia32_compressstorehi256_mask:\n  case X86::BI__builtin_ia32_compressstorehi512_mask:\n  case X86::BI__builtin_ia32_compressstoreqi128_mask:\n  case X86::BI__builtin_ia32_compressstoreqi256_mask:\n  case X86::BI__builtin_ia32_compressstoreqi512_mask:\n    return EmitX86CompressStore(*this, Ops);\n\n  case X86::BI__builtin_ia32_expanddf128_mask:\n  case X86::BI__builtin_ia32_expanddf256_mask:\n  case X86::BI__builtin_ia32_expanddf512_mask:\n  case X86::BI__builtin_ia32_expandsf128_mask:\n  case X86::BI__builtin_ia32_expandsf256_mask:\n  case X86::BI__builtin_ia32_expandsf512_mask:\n  case X86::BI__builtin_ia32_expanddi128_mask:\n  case X86::BI__builtin_ia32_expanddi256_mask:\n  case X86::BI__builtin_ia32_expanddi512_mask:\n  case X86::BI__builtin_ia32_expandsi128_mask:\n  case X86::BI__builtin_ia32_expandsi256_mask:\n  case X86::BI__builtin_ia32_expandsi512_mask:\n  case X86::BI__builtin_ia32_expandhi128_mask:\n  case X86::BI__builtin_ia32_expandhi256_mask:\n  case X86::BI__builtin_ia32_expandhi512_mask:\n  case X86::BI__builtin_ia32_expandqi128_mask:\n  case X86::BI__builtin_ia32_expandqi256_mask:\n  case X86::BI__builtin_ia32_expandqi512_mask:\n    return EmitX86CompressExpand(*this, Ops, /*IsCompress*/false);\n\n  case X86::BI__builtin_ia32_compressdf128_mask:\n  case X86::BI__builtin_ia32_compressdf256_mask:\n  case X86::BI__builtin_ia32_compressdf512_mask:\n  case X86::BI__builtin_ia32_compresssf128_mask:\n  case X86::BI__builtin_ia32_compresssf256_mask:\n  case X86::BI__builtin_ia32_compresssf512_mask:\n  case X86::BI__builtin_ia32_compressdi128_mask:\n  case X86::BI__builtin_ia32_compressdi256_mask:\n  case X86::BI__builtin_ia32_compressdi512_mask:\n  case X86::BI__builtin_ia32_compresssi128_mask:\n  case X86::BI__builtin_ia32_compresssi256_mask:\n  case X86::BI__builtin_ia32_compresssi512_mask:\n  case X86::BI__builtin_ia32_compresshi128_mask:\n  case X86::BI__builtin_ia32_compresshi256_mask:\n  case X86::BI__builtin_ia32_compresshi512_mask:\n  case X86::BI__builtin_ia32_compressqi128_mask:\n  case X86::BI__builtin_ia32_compressqi256_mask:\n  case X86::BI__builtin_ia32_compressqi512_mask:\n    return EmitX86CompressExpand(*this, Ops, /*IsCompress*/true);\n\n  case X86::BI__builtin_ia32_gather3div2df:\n  case X86::BI__builtin_ia32_gather3div2di:\n  case X86::BI__builtin_ia32_gather3div4df:\n  case X86::BI__builtin_ia32_gather3div4di:\n  case X86::BI__builtin_ia32_gather3div4sf:\n  case X86::BI__builtin_ia32_gather3div4si:\n  case X86::BI__builtin_ia32_gather3div8sf:\n  case X86::BI__builtin_ia32_gather3div8si:\n  case X86::BI__builtin_ia32_gather3siv2df:\n  case X86::BI__builtin_ia32_gather3siv2di:\n  case X86::BI__builtin_ia32_gather3siv4df:\n  case X86::BI__builtin_ia32_gather3siv4di:\n  case X86::BI__builtin_ia32_gather3siv4sf:\n  case X86::BI__builtin_ia32_gather3siv4si:\n  case X86::BI__builtin_ia32_gather3siv8sf:\n  case X86::BI__builtin_ia32_gather3siv8si:\n  case X86::BI__builtin_ia32_gathersiv8df:\n  case X86::BI__builtin_ia32_gathersiv16sf:\n  case X86::BI__builtin_ia32_gatherdiv8df:\n  case X86::BI__builtin_ia32_gatherdiv16sf:\n  case X86::BI__builtin_ia32_gathersiv8di:\n  case X86::BI__builtin_ia32_gathersiv16si:\n  case X86::BI__builtin_ia32_gatherdiv8di:\n  case X86::BI__builtin_ia32_gatherdiv16si: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unexpected builtin\");\n    case X86::BI__builtin_ia32_gather3div2df:\n      IID = Intrinsic::x86_avx512_mask_gather3div2_df;\n      break;\n    case X86::BI__builtin_ia32_gather3div2di:\n      IID = Intrinsic::x86_avx512_mask_gather3div2_di;\n      break;\n    case X86::BI__builtin_ia32_gather3div4df:\n      IID = Intrinsic::x86_avx512_mask_gather3div4_df;\n      break;\n    case X86::BI__builtin_ia32_gather3div4di:\n      IID = Intrinsic::x86_avx512_mask_gather3div4_di;\n      break;\n    case X86::BI__builtin_ia32_gather3div4sf:\n      IID = Intrinsic::x86_avx512_mask_gather3div4_sf;\n      break;\n    case X86::BI__builtin_ia32_gather3div4si:\n      IID = Intrinsic::x86_avx512_mask_gather3div4_si;\n      break;\n    case X86::BI__builtin_ia32_gather3div8sf:\n      IID = Intrinsic::x86_avx512_mask_gather3div8_sf;\n      break;\n    case X86::BI__builtin_ia32_gather3div8si:\n      IID = Intrinsic::x86_avx512_mask_gather3div8_si;\n      break;\n    case X86::BI__builtin_ia32_gather3siv2df:\n      IID = Intrinsic::x86_avx512_mask_gather3siv2_df;\n      break;\n    case X86::BI__builtin_ia32_gather3siv2di:\n      IID = Intrinsic::x86_avx512_mask_gather3siv2_di;\n      break;\n    case X86::BI__builtin_ia32_gather3siv4df:\n      IID = Intrinsic::x86_avx512_mask_gather3siv4_df;\n      break;\n    case X86::BI__builtin_ia32_gather3siv4di:\n      IID = Intrinsic::x86_avx512_mask_gather3siv4_di;\n      break;\n    case X86::BI__builtin_ia32_gather3siv4sf:\n      IID = Intrinsic::x86_avx512_mask_gather3siv4_sf;\n      break;\n    case X86::BI__builtin_ia32_gather3siv4si:\n      IID = Intrinsic::x86_avx512_mask_gather3siv4_si;\n      break;\n    case X86::BI__builtin_ia32_gather3siv8sf:\n      IID = Intrinsic::x86_avx512_mask_gather3siv8_sf;\n      break;\n    case X86::BI__builtin_ia32_gather3siv8si:\n      IID = Intrinsic::x86_avx512_mask_gather3siv8_si;\n      break;\n    case X86::BI__builtin_ia32_gathersiv8df:\n      IID = Intrinsic::x86_avx512_mask_gather_dpd_512;\n      break;\n    case X86::BI__builtin_ia32_gathersiv16sf:\n      IID = Intrinsic::x86_avx512_mask_gather_dps_512;\n      break;\n    case X86::BI__builtin_ia32_gatherdiv8df:\n      IID = Intrinsic::x86_avx512_mask_gather_qpd_512;\n      break;\n    case X86::BI__builtin_ia32_gatherdiv16sf:\n      IID = Intrinsic::x86_avx512_mask_gather_qps_512;\n      break;\n    case X86::BI__builtin_ia32_gathersiv8di:\n      IID = Intrinsic::x86_avx512_mask_gather_dpq_512;\n      break;\n    case X86::BI__builtin_ia32_gathersiv16si:\n      IID = Intrinsic::x86_avx512_mask_gather_dpi_512;\n      break;\n    case X86::BI__builtin_ia32_gatherdiv8di:\n      IID = Intrinsic::x86_avx512_mask_gather_qpq_512;\n      break;\n    case X86::BI__builtin_ia32_gatherdiv16si:\n      IID = Intrinsic::x86_avx512_mask_gather_qpi_512;\n      break;\n    }\n\n    unsigned MinElts = std::min(\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements(),\n        cast<llvm::FixedVectorType>(Ops[2]->getType())->getNumElements());\n    Ops[3] = getMaskVecValue(*this, Ops[3], MinElts);\n    Function *Intr = CGM.getIntrinsic(IID);\n    return Builder.CreateCall(Intr, Ops);\n  }\n\n  case X86::BI__builtin_ia32_scattersiv8df:\n  case X86::BI__builtin_ia32_scattersiv16sf:\n  case X86::BI__builtin_ia32_scatterdiv8df:\n  case X86::BI__builtin_ia32_scatterdiv16sf:\n  case X86::BI__builtin_ia32_scattersiv8di:\n  case X86::BI__builtin_ia32_scattersiv16si:\n  case X86::BI__builtin_ia32_scatterdiv8di:\n  case X86::BI__builtin_ia32_scatterdiv16si:\n  case X86::BI__builtin_ia32_scatterdiv2df:\n  case X86::BI__builtin_ia32_scatterdiv2di:\n  case X86::BI__builtin_ia32_scatterdiv4df:\n  case X86::BI__builtin_ia32_scatterdiv4di:\n  case X86::BI__builtin_ia32_scatterdiv4sf:\n  case X86::BI__builtin_ia32_scatterdiv4si:\n  case X86::BI__builtin_ia32_scatterdiv8sf:\n  case X86::BI__builtin_ia32_scatterdiv8si:\n  case X86::BI__builtin_ia32_scattersiv2df:\n  case X86::BI__builtin_ia32_scattersiv2di:\n  case X86::BI__builtin_ia32_scattersiv4df:\n  case X86::BI__builtin_ia32_scattersiv4di:\n  case X86::BI__builtin_ia32_scattersiv4sf:\n  case X86::BI__builtin_ia32_scattersiv4si:\n  case X86::BI__builtin_ia32_scattersiv8sf:\n  case X86::BI__builtin_ia32_scattersiv8si: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unexpected builtin\");\n    case X86::BI__builtin_ia32_scattersiv8df:\n      IID = Intrinsic::x86_avx512_mask_scatter_dpd_512;\n      break;\n    case X86::BI__builtin_ia32_scattersiv16sf:\n      IID = Intrinsic::x86_avx512_mask_scatter_dps_512;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv8df:\n      IID = Intrinsic::x86_avx512_mask_scatter_qpd_512;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv16sf:\n      IID = Intrinsic::x86_avx512_mask_scatter_qps_512;\n      break;\n    case X86::BI__builtin_ia32_scattersiv8di:\n      IID = Intrinsic::x86_avx512_mask_scatter_dpq_512;\n      break;\n    case X86::BI__builtin_ia32_scattersiv16si:\n      IID = Intrinsic::x86_avx512_mask_scatter_dpi_512;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv8di:\n      IID = Intrinsic::x86_avx512_mask_scatter_qpq_512;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv16si:\n      IID = Intrinsic::x86_avx512_mask_scatter_qpi_512;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv2df:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv2_df;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv2di:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv2_di;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv4df:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv4_df;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv4di:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv4_di;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv4sf:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv4_sf;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv4si:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv4_si;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv8sf:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv8_sf;\n      break;\n    case X86::BI__builtin_ia32_scatterdiv8si:\n      IID = Intrinsic::x86_avx512_mask_scatterdiv8_si;\n      break;\n    case X86::BI__builtin_ia32_scattersiv2df:\n      IID = Intrinsic::x86_avx512_mask_scattersiv2_df;\n      break;\n    case X86::BI__builtin_ia32_scattersiv2di:\n      IID = Intrinsic::x86_avx512_mask_scattersiv2_di;\n      break;\n    case X86::BI__builtin_ia32_scattersiv4df:\n      IID = Intrinsic::x86_avx512_mask_scattersiv4_df;\n      break;\n    case X86::BI__builtin_ia32_scattersiv4di:\n      IID = Intrinsic::x86_avx512_mask_scattersiv4_di;\n      break;\n    case X86::BI__builtin_ia32_scattersiv4sf:\n      IID = Intrinsic::x86_avx512_mask_scattersiv4_sf;\n      break;\n    case X86::BI__builtin_ia32_scattersiv4si:\n      IID = Intrinsic::x86_avx512_mask_scattersiv4_si;\n      break;\n    case X86::BI__builtin_ia32_scattersiv8sf:\n      IID = Intrinsic::x86_avx512_mask_scattersiv8_sf;\n      break;\n    case X86::BI__builtin_ia32_scattersiv8si:\n      IID = Intrinsic::x86_avx512_mask_scattersiv8_si;\n      break;\n    }\n\n    unsigned MinElts = std::min(\n        cast<llvm::FixedVectorType>(Ops[2]->getType())->getNumElements(),\n        cast<llvm::FixedVectorType>(Ops[3]->getType())->getNumElements());\n    Ops[1] = getMaskVecValue(*this, Ops[1], MinElts);\n    Function *Intr = CGM.getIntrinsic(IID);\n    return Builder.CreateCall(Intr, Ops);\n  }\n\n  case X86::BI__builtin_ia32_vextractf128_pd256:\n  case X86::BI__builtin_ia32_vextractf128_ps256:\n  case X86::BI__builtin_ia32_vextractf128_si256:\n  case X86::BI__builtin_ia32_extract128i256:\n  case X86::BI__builtin_ia32_extractf64x4_mask:\n  case X86::BI__builtin_ia32_extractf32x4_mask:\n  case X86::BI__builtin_ia32_extracti64x4_mask:\n  case X86::BI__builtin_ia32_extracti32x4_mask:\n  case X86::BI__builtin_ia32_extractf32x8_mask:\n  case X86::BI__builtin_ia32_extracti32x8_mask:\n  case X86::BI__builtin_ia32_extractf32x4_256_mask:\n  case X86::BI__builtin_ia32_extracti32x4_256_mask:\n  case X86::BI__builtin_ia32_extractf64x2_256_mask:\n  case X86::BI__builtin_ia32_extracti64x2_256_mask:\n  case X86::BI__builtin_ia32_extractf64x2_512_mask:\n  case X86::BI__builtin_ia32_extracti64x2_512_mask: {\n    auto *DstTy = cast<llvm::FixedVectorType>(ConvertType(E->getType()));\n    unsigned NumElts = DstTy->getNumElements();\n    unsigned SrcNumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    unsigned SubVectors = SrcNumElts / NumElts;\n    unsigned Index = cast<ConstantInt>(Ops[1])->getZExtValue();\n    assert(llvm::isPowerOf2_32(SubVectors) && \"Expected power of 2 subvectors\");\n    Index &= SubVectors - 1; // Remove any extra bits.\n    Index *= NumElts;\n\n    int Indices[16];\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = i + Index;\n\n    Value *Res = Builder.CreateShuffleVector(Ops[0],\n                                             makeArrayRef(Indices, NumElts),\n                                             \"extract\");\n\n    if (Ops.size() == 4)\n      Res = EmitX86Select(*this, Ops[3], Res, Ops[2]);\n\n    return Res;\n  }\n  case X86::BI__builtin_ia32_vinsertf128_pd256:\n  case X86::BI__builtin_ia32_vinsertf128_ps256:\n  case X86::BI__builtin_ia32_vinsertf128_si256:\n  case X86::BI__builtin_ia32_insert128i256:\n  case X86::BI__builtin_ia32_insertf64x4:\n  case X86::BI__builtin_ia32_insertf32x4:\n  case X86::BI__builtin_ia32_inserti64x4:\n  case X86::BI__builtin_ia32_inserti32x4:\n  case X86::BI__builtin_ia32_insertf32x8:\n  case X86::BI__builtin_ia32_inserti32x8:\n  case X86::BI__builtin_ia32_insertf32x4_256:\n  case X86::BI__builtin_ia32_inserti32x4_256:\n  case X86::BI__builtin_ia32_insertf64x2_256:\n  case X86::BI__builtin_ia32_inserti64x2_256:\n  case X86::BI__builtin_ia32_insertf64x2_512:\n  case X86::BI__builtin_ia32_inserti64x2_512: {\n    unsigned DstNumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    unsigned SrcNumElts =\n        cast<llvm::FixedVectorType>(Ops[1]->getType())->getNumElements();\n    unsigned SubVectors = DstNumElts / SrcNumElts;\n    unsigned Index = cast<ConstantInt>(Ops[2])->getZExtValue();\n    assert(llvm::isPowerOf2_32(SubVectors) && \"Expected power of 2 subvectors\");\n    Index &= SubVectors - 1; // Remove any extra bits.\n    Index *= SrcNumElts;\n\n    int Indices[16];\n    for (unsigned i = 0; i != DstNumElts; ++i)\n      Indices[i] = (i >= SrcNumElts) ? SrcNumElts + (i % SrcNumElts) : i;\n\n    Value *Op1 = Builder.CreateShuffleVector(Ops[1],\n                                             makeArrayRef(Indices, DstNumElts),\n                                             \"widen\");\n\n    for (unsigned i = 0; i != DstNumElts; ++i) {\n      if (i >= Index && i < (Index + SrcNumElts))\n        Indices[i] = (i - Index) + DstNumElts;\n      else\n        Indices[i] = i;\n    }\n\n    return Builder.CreateShuffleVector(Ops[0], Op1,\n                                       makeArrayRef(Indices, DstNumElts),\n                                       \"insert\");\n  }\n  case X86::BI__builtin_ia32_pmovqd512_mask:\n  case X86::BI__builtin_ia32_pmovwb512_mask: {\n    Value *Res = Builder.CreateTrunc(Ops[0], Ops[1]->getType());\n    return EmitX86Select(*this, Ops[2], Res, Ops[1]);\n  }\n  case X86::BI__builtin_ia32_pmovdb512_mask:\n  case X86::BI__builtin_ia32_pmovdw512_mask:\n  case X86::BI__builtin_ia32_pmovqw512_mask: {\n    if (const auto *C = dyn_cast<Constant>(Ops[2]))\n      if (C->isAllOnesValue())\n        return Builder.CreateTrunc(Ops[0], Ops[1]->getType());\n\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_pmovdb512_mask:\n      IID = Intrinsic::x86_avx512_mask_pmov_db_512;\n      break;\n    case X86::BI__builtin_ia32_pmovdw512_mask:\n      IID = Intrinsic::x86_avx512_mask_pmov_dw_512;\n      break;\n    case X86::BI__builtin_ia32_pmovqw512_mask:\n      IID = Intrinsic::x86_avx512_mask_pmov_qw_512;\n      break;\n    }\n\n    Function *Intr = CGM.getIntrinsic(IID);\n    return Builder.CreateCall(Intr, Ops);\n  }\n  case X86::BI__builtin_ia32_pblendw128:\n  case X86::BI__builtin_ia32_blendpd:\n  case X86::BI__builtin_ia32_blendps:\n  case X86::BI__builtin_ia32_blendpd256:\n  case X86::BI__builtin_ia32_blendps256:\n  case X86::BI__builtin_ia32_pblendw256:\n  case X86::BI__builtin_ia32_pblendd128:\n  case X86::BI__builtin_ia32_pblendd256: {\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    unsigned Imm = cast<llvm::ConstantInt>(Ops[2])->getZExtValue();\n\n    int Indices[16];\n    // If there are more than 8 elements, the immediate is used twice so make\n    // sure we handle that.\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = ((Imm >> (i % 8)) & 0x1) ? NumElts + i : i;\n\n    return Builder.CreateShuffleVector(Ops[0], Ops[1],\n                                       makeArrayRef(Indices, NumElts),\n                                       \"blend\");\n  }\n  case X86::BI__builtin_ia32_pshuflw:\n  case X86::BI__builtin_ia32_pshuflw256:\n  case X86::BI__builtin_ia32_pshuflw512: {\n    uint32_t Imm = cast<llvm::ConstantInt>(Ops[1])->getZExtValue();\n    auto *Ty = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    unsigned NumElts = Ty->getNumElements();\n\n    // Splat the 8-bits of immediate 4 times to help the loop wrap around.\n    Imm = (Imm & 0xff) * 0x01010101;\n\n    int Indices[32];\n    for (unsigned l = 0; l != NumElts; l += 8) {\n      for (unsigned i = 0; i != 4; ++i) {\n        Indices[l + i] = l + (Imm & 3);\n        Imm >>= 2;\n      }\n      for (unsigned i = 4; i != 8; ++i)\n        Indices[l + i] = l + i;\n    }\n\n    return Builder.CreateShuffleVector(Ops[0], makeArrayRef(Indices, NumElts),\n                                       \"pshuflw\");\n  }\n  case X86::BI__builtin_ia32_pshufhw:\n  case X86::BI__builtin_ia32_pshufhw256:\n  case X86::BI__builtin_ia32_pshufhw512: {\n    uint32_t Imm = cast<llvm::ConstantInt>(Ops[1])->getZExtValue();\n    auto *Ty = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    unsigned NumElts = Ty->getNumElements();\n\n    // Splat the 8-bits of immediate 4 times to help the loop wrap around.\n    Imm = (Imm & 0xff) * 0x01010101;\n\n    int Indices[32];\n    for (unsigned l = 0; l != NumElts; l += 8) {\n      for (unsigned i = 0; i != 4; ++i)\n        Indices[l + i] = l + i;\n      for (unsigned i = 4; i != 8; ++i) {\n        Indices[l + i] = l + 4 + (Imm & 3);\n        Imm >>= 2;\n      }\n    }\n\n    return Builder.CreateShuffleVector(Ops[0], makeArrayRef(Indices, NumElts),\n                                       \"pshufhw\");\n  }\n  case X86::BI__builtin_ia32_pshufd:\n  case X86::BI__builtin_ia32_pshufd256:\n  case X86::BI__builtin_ia32_pshufd512:\n  case X86::BI__builtin_ia32_vpermilpd:\n  case X86::BI__builtin_ia32_vpermilps:\n  case X86::BI__builtin_ia32_vpermilpd256:\n  case X86::BI__builtin_ia32_vpermilps256:\n  case X86::BI__builtin_ia32_vpermilpd512:\n  case X86::BI__builtin_ia32_vpermilps512: {\n    uint32_t Imm = cast<llvm::ConstantInt>(Ops[1])->getZExtValue();\n    auto *Ty = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    unsigned NumElts = Ty->getNumElements();\n    unsigned NumLanes = Ty->getPrimitiveSizeInBits() / 128;\n    unsigned NumLaneElts = NumElts / NumLanes;\n\n    // Splat the 8-bits of immediate 4 times to help the loop wrap around.\n    Imm = (Imm & 0xff) * 0x01010101;\n\n    int Indices[16];\n    for (unsigned l = 0; l != NumElts; l += NumLaneElts) {\n      for (unsigned i = 0; i != NumLaneElts; ++i) {\n        Indices[i + l] = (Imm % NumLaneElts) + l;\n        Imm /= NumLaneElts;\n      }\n    }\n\n    return Builder.CreateShuffleVector(Ops[0], makeArrayRef(Indices, NumElts),\n                                       \"permil\");\n  }\n  case X86::BI__builtin_ia32_shufpd:\n  case X86::BI__builtin_ia32_shufpd256:\n  case X86::BI__builtin_ia32_shufpd512:\n  case X86::BI__builtin_ia32_shufps:\n  case X86::BI__builtin_ia32_shufps256:\n  case X86::BI__builtin_ia32_shufps512: {\n    uint32_t Imm = cast<llvm::ConstantInt>(Ops[2])->getZExtValue();\n    auto *Ty = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    unsigned NumElts = Ty->getNumElements();\n    unsigned NumLanes = Ty->getPrimitiveSizeInBits() / 128;\n    unsigned NumLaneElts = NumElts / NumLanes;\n\n    // Splat the 8-bits of immediate 4 times to help the loop wrap around.\n    Imm = (Imm & 0xff) * 0x01010101;\n\n    int Indices[16];\n    for (unsigned l = 0; l != NumElts; l += NumLaneElts) {\n      for (unsigned i = 0; i != NumLaneElts; ++i) {\n        unsigned Index = Imm % NumLaneElts;\n        Imm /= NumLaneElts;\n        if (i >= (NumLaneElts / 2))\n          Index += NumElts;\n        Indices[l + i] = l + Index;\n      }\n    }\n\n    return Builder.CreateShuffleVector(Ops[0], Ops[1],\n                                       makeArrayRef(Indices, NumElts),\n                                       \"shufp\");\n  }\n  case X86::BI__builtin_ia32_permdi256:\n  case X86::BI__builtin_ia32_permdf256:\n  case X86::BI__builtin_ia32_permdi512:\n  case X86::BI__builtin_ia32_permdf512: {\n    unsigned Imm = cast<llvm::ConstantInt>(Ops[1])->getZExtValue();\n    auto *Ty = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    unsigned NumElts = Ty->getNumElements();\n\n    // These intrinsics operate on 256-bit lanes of four 64-bit elements.\n    int Indices[8];\n    for (unsigned l = 0; l != NumElts; l += 4)\n      for (unsigned i = 0; i != 4; ++i)\n        Indices[l + i] = l + ((Imm >> (2 * i)) & 0x3);\n\n    return Builder.CreateShuffleVector(Ops[0], makeArrayRef(Indices, NumElts),\n                                       \"perm\");\n  }\n  case X86::BI__builtin_ia32_palignr128:\n  case X86::BI__builtin_ia32_palignr256:\n  case X86::BI__builtin_ia32_palignr512: {\n    unsigned ShiftVal = cast<llvm::ConstantInt>(Ops[2])->getZExtValue() & 0xff;\n\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    assert(NumElts % 16 == 0);\n\n    // If palignr is shifting the pair of vectors more than the size of two\n    // lanes, emit zero.\n    if (ShiftVal >= 32)\n      return llvm::Constant::getNullValue(ConvertType(E->getType()));\n\n    // If palignr is shifting the pair of input vectors more than one lane,\n    // but less than two lanes, convert to shifting in zeroes.\n    if (ShiftVal > 16) {\n      ShiftVal -= 16;\n      Ops[1] = Ops[0];\n      Ops[0] = llvm::Constant::getNullValue(Ops[0]->getType());\n    }\n\n    int Indices[64];\n    // 256-bit palignr operates on 128-bit lanes so we need to handle that\n    for (unsigned l = 0; l != NumElts; l += 16) {\n      for (unsigned i = 0; i != 16; ++i) {\n        unsigned Idx = ShiftVal + i;\n        if (Idx >= 16)\n          Idx += NumElts - 16; // End of lane, switch operand.\n        Indices[l + i] = Idx + l;\n      }\n    }\n\n    return Builder.CreateShuffleVector(Ops[1], Ops[0],\n                                       makeArrayRef(Indices, NumElts),\n                                       \"palignr\");\n  }\n  case X86::BI__builtin_ia32_alignd128:\n  case X86::BI__builtin_ia32_alignd256:\n  case X86::BI__builtin_ia32_alignd512:\n  case X86::BI__builtin_ia32_alignq128:\n  case X86::BI__builtin_ia32_alignq256:\n  case X86::BI__builtin_ia32_alignq512: {\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    unsigned ShiftVal = cast<llvm::ConstantInt>(Ops[2])->getZExtValue() & 0xff;\n\n    // Mask the shift amount to width of two vectors.\n    ShiftVal &= (2 * NumElts) - 1;\n\n    int Indices[16];\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = i + ShiftVal;\n\n    return Builder.CreateShuffleVector(Ops[1], Ops[0],\n                                       makeArrayRef(Indices, NumElts),\n                                       \"valign\");\n  }\n  case X86::BI__builtin_ia32_shuf_f32x4_256:\n  case X86::BI__builtin_ia32_shuf_f64x2_256:\n  case X86::BI__builtin_ia32_shuf_i32x4_256:\n  case X86::BI__builtin_ia32_shuf_i64x2_256:\n  case X86::BI__builtin_ia32_shuf_f32x4:\n  case X86::BI__builtin_ia32_shuf_f64x2:\n  case X86::BI__builtin_ia32_shuf_i32x4:\n  case X86::BI__builtin_ia32_shuf_i64x2: {\n    unsigned Imm = cast<llvm::ConstantInt>(Ops[2])->getZExtValue();\n    auto *Ty = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    unsigned NumElts = Ty->getNumElements();\n    unsigned NumLanes = Ty->getPrimitiveSizeInBits() == 512 ? 4 : 2;\n    unsigned NumLaneElts = NumElts / NumLanes;\n\n    int Indices[16];\n    for (unsigned l = 0; l != NumElts; l += NumLaneElts) {\n      unsigned Index = (Imm % NumLanes) * NumLaneElts;\n      Imm /= NumLanes; // Discard the bits we just used.\n      if (l >= (NumElts / 2))\n        Index += NumElts; // Switch to other source.\n      for (unsigned i = 0; i != NumLaneElts; ++i) {\n        Indices[l + i] = Index + i;\n      }\n    }\n\n    return Builder.CreateShuffleVector(Ops[0], Ops[1],\n                                       makeArrayRef(Indices, NumElts),\n                                       \"shuf\");\n  }\n\n  case X86::BI__builtin_ia32_vperm2f128_pd256:\n  case X86::BI__builtin_ia32_vperm2f128_ps256:\n  case X86::BI__builtin_ia32_vperm2f128_si256:\n  case X86::BI__builtin_ia32_permti256: {\n    unsigned Imm = cast<llvm::ConstantInt>(Ops[2])->getZExtValue();\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n\n    // This takes a very simple approach since there are two lanes and a\n    // shuffle can have 2 inputs. So we reserve the first input for the first\n    // lane and the second input for the second lane. This may result in\n    // duplicate sources, but this can be dealt with in the backend.\n\n    Value *OutOps[2];\n    int Indices[8];\n    for (unsigned l = 0; l != 2; ++l) {\n      // Determine the source for this lane.\n      if (Imm & (1 << ((l * 4) + 3)))\n        OutOps[l] = llvm::ConstantAggregateZero::get(Ops[0]->getType());\n      else if (Imm & (1 << ((l * 4) + 1)))\n        OutOps[l] = Ops[1];\n      else\n        OutOps[l] = Ops[0];\n\n      for (unsigned i = 0; i != NumElts/2; ++i) {\n        // Start with ith element of the source for this lane.\n        unsigned Idx = (l * NumElts) + i;\n        // If bit 0 of the immediate half is set, switch to the high half of\n        // the source.\n        if (Imm & (1 << (l * 4)))\n          Idx += NumElts/2;\n        Indices[(l * (NumElts/2)) + i] = Idx;\n      }\n    }\n\n    return Builder.CreateShuffleVector(OutOps[0], OutOps[1],\n                                       makeArrayRef(Indices, NumElts),\n                                       \"vperm\");\n  }\n\n  case X86::BI__builtin_ia32_pslldqi128_byteshift:\n  case X86::BI__builtin_ia32_pslldqi256_byteshift:\n  case X86::BI__builtin_ia32_pslldqi512_byteshift: {\n    unsigned ShiftVal = cast<llvm::ConstantInt>(Ops[1])->getZExtValue() & 0xff;\n    auto *ResultType = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    // Builtin type is vXi64 so multiply by 8 to get bytes.\n    unsigned NumElts = ResultType->getNumElements() * 8;\n\n    // If pslldq is shifting the vector more than 15 bytes, emit zero.\n    if (ShiftVal >= 16)\n      return llvm::Constant::getNullValue(ResultType);\n\n    int Indices[64];\n    // 256/512-bit pslldq operates on 128-bit lanes so we need to handle that\n    for (unsigned l = 0; l != NumElts; l += 16) {\n      for (unsigned i = 0; i != 16; ++i) {\n        unsigned Idx = NumElts + i - ShiftVal;\n        if (Idx < NumElts) Idx -= NumElts - 16; // end of lane, switch operand.\n        Indices[l + i] = Idx + l;\n      }\n    }\n\n    auto *VecTy = llvm::FixedVectorType::get(Int8Ty, NumElts);\n    Value *Cast = Builder.CreateBitCast(Ops[0], VecTy, \"cast\");\n    Value *Zero = llvm::Constant::getNullValue(VecTy);\n    Value *SV = Builder.CreateShuffleVector(Zero, Cast,\n                                            makeArrayRef(Indices, NumElts),\n                                            \"pslldq\");\n    return Builder.CreateBitCast(SV, Ops[0]->getType(), \"cast\");\n  }\n  case X86::BI__builtin_ia32_psrldqi128_byteshift:\n  case X86::BI__builtin_ia32_psrldqi256_byteshift:\n  case X86::BI__builtin_ia32_psrldqi512_byteshift: {\n    unsigned ShiftVal = cast<llvm::ConstantInt>(Ops[1])->getZExtValue() & 0xff;\n    auto *ResultType = cast<llvm::FixedVectorType>(Ops[0]->getType());\n    // Builtin type is vXi64 so multiply by 8 to get bytes.\n    unsigned NumElts = ResultType->getNumElements() * 8;\n\n    // If psrldq is shifting the vector more than 15 bytes, emit zero.\n    if (ShiftVal >= 16)\n      return llvm::Constant::getNullValue(ResultType);\n\n    int Indices[64];\n    // 256/512-bit psrldq operates on 128-bit lanes so we need to handle that\n    for (unsigned l = 0; l != NumElts; l += 16) {\n      for (unsigned i = 0; i != 16; ++i) {\n        unsigned Idx = i + ShiftVal;\n        if (Idx >= 16) Idx += NumElts - 16; // end of lane, switch operand.\n        Indices[l + i] = Idx + l;\n      }\n    }\n\n    auto *VecTy = llvm::FixedVectorType::get(Int8Ty, NumElts);\n    Value *Cast = Builder.CreateBitCast(Ops[0], VecTy, \"cast\");\n    Value *Zero = llvm::Constant::getNullValue(VecTy);\n    Value *SV = Builder.CreateShuffleVector(Cast, Zero,\n                                            makeArrayRef(Indices, NumElts),\n                                            \"psrldq\");\n    return Builder.CreateBitCast(SV, ResultType, \"cast\");\n  }\n  case X86::BI__builtin_ia32_kshiftliqi:\n  case X86::BI__builtin_ia32_kshiftlihi:\n  case X86::BI__builtin_ia32_kshiftlisi:\n  case X86::BI__builtin_ia32_kshiftlidi: {\n    unsigned ShiftVal = cast<llvm::ConstantInt>(Ops[1])->getZExtValue() & 0xff;\n    unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n\n    if (ShiftVal >= NumElts)\n      return llvm::Constant::getNullValue(Ops[0]->getType());\n\n    Value *In = getMaskVecValue(*this, Ops[0], NumElts);\n\n    int Indices[64];\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = NumElts + i - ShiftVal;\n\n    Value *Zero = llvm::Constant::getNullValue(In->getType());\n    Value *SV = Builder.CreateShuffleVector(Zero, In,\n                                            makeArrayRef(Indices, NumElts),\n                                            \"kshiftl\");\n    return Builder.CreateBitCast(SV, Ops[0]->getType());\n  }\n  case X86::BI__builtin_ia32_kshiftriqi:\n  case X86::BI__builtin_ia32_kshiftrihi:\n  case X86::BI__builtin_ia32_kshiftrisi:\n  case X86::BI__builtin_ia32_kshiftridi: {\n    unsigned ShiftVal = cast<llvm::ConstantInt>(Ops[1])->getZExtValue() & 0xff;\n    unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n\n    if (ShiftVal >= NumElts)\n      return llvm::Constant::getNullValue(Ops[0]->getType());\n\n    Value *In = getMaskVecValue(*this, Ops[0], NumElts);\n\n    int Indices[64];\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = i + ShiftVal;\n\n    Value *Zero = llvm::Constant::getNullValue(In->getType());\n    Value *SV = Builder.CreateShuffleVector(In, Zero,\n                                            makeArrayRef(Indices, NumElts),\n                                            \"kshiftr\");\n    return Builder.CreateBitCast(SV, Ops[0]->getType());\n  }\n  case X86::BI__builtin_ia32_movnti:\n  case X86::BI__builtin_ia32_movnti64:\n  case X86::BI__builtin_ia32_movntsd:\n  case X86::BI__builtin_ia32_movntss: {\n    llvm::MDNode *Node = llvm::MDNode::get(\n        getLLVMContext(), llvm::ConstantAsMetadata::get(Builder.getInt32(1)));\n\n    Value *Ptr = Ops[0];\n    Value *Src = Ops[1];\n\n    // Extract the 0'th element of the source vector.\n    if (BuiltinID == X86::BI__builtin_ia32_movntsd ||\n        BuiltinID == X86::BI__builtin_ia32_movntss)\n      Src = Builder.CreateExtractElement(Src, (uint64_t)0, \"extract\");\n\n    // Convert the type of the pointer to a pointer to the stored type.\n    Value *BC = Builder.CreateBitCast(\n        Ptr, llvm::PointerType::getUnqual(Src->getType()), \"cast\");\n\n    // Unaligned nontemporal store of the scalar value.\n    StoreInst *SI = Builder.CreateDefaultAlignedStore(Src, BC);\n    SI->setMetadata(CGM.getModule().getMDKindID(\"nontemporal\"), Node);\n    SI->setAlignment(llvm::Align(1));\n    return SI;\n  }\n  // Rotate is a special case of funnel shift - 1st 2 args are the same.\n  case X86::BI__builtin_ia32_vprotb:\n  case X86::BI__builtin_ia32_vprotw:\n  case X86::BI__builtin_ia32_vprotd:\n  case X86::BI__builtin_ia32_vprotq:\n  case X86::BI__builtin_ia32_vprotbi:\n  case X86::BI__builtin_ia32_vprotwi:\n  case X86::BI__builtin_ia32_vprotdi:\n  case X86::BI__builtin_ia32_vprotqi:\n  case X86::BI__builtin_ia32_prold128:\n  case X86::BI__builtin_ia32_prold256:\n  case X86::BI__builtin_ia32_prold512:\n  case X86::BI__builtin_ia32_prolq128:\n  case X86::BI__builtin_ia32_prolq256:\n  case X86::BI__builtin_ia32_prolq512:\n  case X86::BI__builtin_ia32_prolvd128:\n  case X86::BI__builtin_ia32_prolvd256:\n  case X86::BI__builtin_ia32_prolvd512:\n  case X86::BI__builtin_ia32_prolvq128:\n  case X86::BI__builtin_ia32_prolvq256:\n  case X86::BI__builtin_ia32_prolvq512:\n    return EmitX86FunnelShift(*this, Ops[0], Ops[0], Ops[1], false);\n  case X86::BI__builtin_ia32_prord128:\n  case X86::BI__builtin_ia32_prord256:\n  case X86::BI__builtin_ia32_prord512:\n  case X86::BI__builtin_ia32_prorq128:\n  case X86::BI__builtin_ia32_prorq256:\n  case X86::BI__builtin_ia32_prorq512:\n  case X86::BI__builtin_ia32_prorvd128:\n  case X86::BI__builtin_ia32_prorvd256:\n  case X86::BI__builtin_ia32_prorvd512:\n  case X86::BI__builtin_ia32_prorvq128:\n  case X86::BI__builtin_ia32_prorvq256:\n  case X86::BI__builtin_ia32_prorvq512:\n    return EmitX86FunnelShift(*this, Ops[0], Ops[0], Ops[1], true);\n  case X86::BI__builtin_ia32_selectb_128:\n  case X86::BI__builtin_ia32_selectb_256:\n  case X86::BI__builtin_ia32_selectb_512:\n  case X86::BI__builtin_ia32_selectw_128:\n  case X86::BI__builtin_ia32_selectw_256:\n  case X86::BI__builtin_ia32_selectw_512:\n  case X86::BI__builtin_ia32_selectd_128:\n  case X86::BI__builtin_ia32_selectd_256:\n  case X86::BI__builtin_ia32_selectd_512:\n  case X86::BI__builtin_ia32_selectq_128:\n  case X86::BI__builtin_ia32_selectq_256:\n  case X86::BI__builtin_ia32_selectq_512:\n  case X86::BI__builtin_ia32_selectps_128:\n  case X86::BI__builtin_ia32_selectps_256:\n  case X86::BI__builtin_ia32_selectps_512:\n  case X86::BI__builtin_ia32_selectpd_128:\n  case X86::BI__builtin_ia32_selectpd_256:\n  case X86::BI__builtin_ia32_selectpd_512:\n    return EmitX86Select(*this, Ops[0], Ops[1], Ops[2]);\n  case X86::BI__builtin_ia32_selectss_128:\n  case X86::BI__builtin_ia32_selectsd_128: {\n    Value *A = Builder.CreateExtractElement(Ops[1], (uint64_t)0);\n    Value *B = Builder.CreateExtractElement(Ops[2], (uint64_t)0);\n    A = EmitX86ScalarSelect(*this, Ops[0], A, B);\n    return Builder.CreateInsertElement(Ops[1], A, (uint64_t)0);\n  }\n  case X86::BI__builtin_ia32_cmpb128_mask:\n  case X86::BI__builtin_ia32_cmpb256_mask:\n  case X86::BI__builtin_ia32_cmpb512_mask:\n  case X86::BI__builtin_ia32_cmpw128_mask:\n  case X86::BI__builtin_ia32_cmpw256_mask:\n  case X86::BI__builtin_ia32_cmpw512_mask:\n  case X86::BI__builtin_ia32_cmpd128_mask:\n  case X86::BI__builtin_ia32_cmpd256_mask:\n  case X86::BI__builtin_ia32_cmpd512_mask:\n  case X86::BI__builtin_ia32_cmpq128_mask:\n  case X86::BI__builtin_ia32_cmpq256_mask:\n  case X86::BI__builtin_ia32_cmpq512_mask: {\n    unsigned CC = cast<llvm::ConstantInt>(Ops[2])->getZExtValue() & 0x7;\n    return EmitX86MaskedCompare(*this, CC, true, Ops);\n  }\n  case X86::BI__builtin_ia32_ucmpb128_mask:\n  case X86::BI__builtin_ia32_ucmpb256_mask:\n  case X86::BI__builtin_ia32_ucmpb512_mask:\n  case X86::BI__builtin_ia32_ucmpw128_mask:\n  case X86::BI__builtin_ia32_ucmpw256_mask:\n  case X86::BI__builtin_ia32_ucmpw512_mask:\n  case X86::BI__builtin_ia32_ucmpd128_mask:\n  case X86::BI__builtin_ia32_ucmpd256_mask:\n  case X86::BI__builtin_ia32_ucmpd512_mask:\n  case X86::BI__builtin_ia32_ucmpq128_mask:\n  case X86::BI__builtin_ia32_ucmpq256_mask:\n  case X86::BI__builtin_ia32_ucmpq512_mask: {\n    unsigned CC = cast<llvm::ConstantInt>(Ops[2])->getZExtValue() & 0x7;\n    return EmitX86MaskedCompare(*this, CC, false, Ops);\n  }\n  case X86::BI__builtin_ia32_vpcomb:\n  case X86::BI__builtin_ia32_vpcomw:\n  case X86::BI__builtin_ia32_vpcomd:\n  case X86::BI__builtin_ia32_vpcomq:\n    return EmitX86vpcom(*this, Ops, true);\n  case X86::BI__builtin_ia32_vpcomub:\n  case X86::BI__builtin_ia32_vpcomuw:\n  case X86::BI__builtin_ia32_vpcomud:\n  case X86::BI__builtin_ia32_vpcomuq:\n    return EmitX86vpcom(*this, Ops, false);\n\n  case X86::BI__builtin_ia32_kortestcqi:\n  case X86::BI__builtin_ia32_kortestchi:\n  case X86::BI__builtin_ia32_kortestcsi:\n  case X86::BI__builtin_ia32_kortestcdi: {\n    Value *Or = EmitX86MaskLogic(*this, Instruction::Or, Ops);\n    Value *C = llvm::Constant::getAllOnesValue(Ops[0]->getType());\n    Value *Cmp = Builder.CreateICmpEQ(Or, C);\n    return Builder.CreateZExt(Cmp, ConvertType(E->getType()));\n  }\n  case X86::BI__builtin_ia32_kortestzqi:\n  case X86::BI__builtin_ia32_kortestzhi:\n  case X86::BI__builtin_ia32_kortestzsi:\n  case X86::BI__builtin_ia32_kortestzdi: {\n    Value *Or = EmitX86MaskLogic(*this, Instruction::Or, Ops);\n    Value *C = llvm::Constant::getNullValue(Ops[0]->getType());\n    Value *Cmp = Builder.CreateICmpEQ(Or, C);\n    return Builder.CreateZExt(Cmp, ConvertType(E->getType()));\n  }\n\n  case X86::BI__builtin_ia32_ktestcqi:\n  case X86::BI__builtin_ia32_ktestzqi:\n  case X86::BI__builtin_ia32_ktestchi:\n  case X86::BI__builtin_ia32_ktestzhi:\n  case X86::BI__builtin_ia32_ktestcsi:\n  case X86::BI__builtin_ia32_ktestzsi:\n  case X86::BI__builtin_ia32_ktestcdi:\n  case X86::BI__builtin_ia32_ktestzdi: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_ktestcqi:\n      IID = Intrinsic::x86_avx512_ktestc_b;\n      break;\n    case X86::BI__builtin_ia32_ktestzqi:\n      IID = Intrinsic::x86_avx512_ktestz_b;\n      break;\n    case X86::BI__builtin_ia32_ktestchi:\n      IID = Intrinsic::x86_avx512_ktestc_w;\n      break;\n    case X86::BI__builtin_ia32_ktestzhi:\n      IID = Intrinsic::x86_avx512_ktestz_w;\n      break;\n    case X86::BI__builtin_ia32_ktestcsi:\n      IID = Intrinsic::x86_avx512_ktestc_d;\n      break;\n    case X86::BI__builtin_ia32_ktestzsi:\n      IID = Intrinsic::x86_avx512_ktestz_d;\n      break;\n    case X86::BI__builtin_ia32_ktestcdi:\n      IID = Intrinsic::x86_avx512_ktestc_q;\n      break;\n    case X86::BI__builtin_ia32_ktestzdi:\n      IID = Intrinsic::x86_avx512_ktestz_q;\n      break;\n    }\n\n    unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n    Value *LHS = getMaskVecValue(*this, Ops[0], NumElts);\n    Value *RHS = getMaskVecValue(*this, Ops[1], NumElts);\n    Function *Intr = CGM.getIntrinsic(IID);\n    return Builder.CreateCall(Intr, {LHS, RHS});\n  }\n\n  case X86::BI__builtin_ia32_kaddqi:\n  case X86::BI__builtin_ia32_kaddhi:\n  case X86::BI__builtin_ia32_kaddsi:\n  case X86::BI__builtin_ia32_kadddi: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_kaddqi:\n      IID = Intrinsic::x86_avx512_kadd_b;\n      break;\n    case X86::BI__builtin_ia32_kaddhi:\n      IID = Intrinsic::x86_avx512_kadd_w;\n      break;\n    case X86::BI__builtin_ia32_kaddsi:\n      IID = Intrinsic::x86_avx512_kadd_d;\n      break;\n    case X86::BI__builtin_ia32_kadddi:\n      IID = Intrinsic::x86_avx512_kadd_q;\n      break;\n    }\n\n    unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n    Value *LHS = getMaskVecValue(*this, Ops[0], NumElts);\n    Value *RHS = getMaskVecValue(*this, Ops[1], NumElts);\n    Function *Intr = CGM.getIntrinsic(IID);\n    Value *Res = Builder.CreateCall(Intr, {LHS, RHS});\n    return Builder.CreateBitCast(Res, Ops[0]->getType());\n  }\n  case X86::BI__builtin_ia32_kandqi:\n  case X86::BI__builtin_ia32_kandhi:\n  case X86::BI__builtin_ia32_kandsi:\n  case X86::BI__builtin_ia32_kanddi:\n    return EmitX86MaskLogic(*this, Instruction::And, Ops);\n  case X86::BI__builtin_ia32_kandnqi:\n  case X86::BI__builtin_ia32_kandnhi:\n  case X86::BI__builtin_ia32_kandnsi:\n  case X86::BI__builtin_ia32_kandndi:\n    return EmitX86MaskLogic(*this, Instruction::And, Ops, true);\n  case X86::BI__builtin_ia32_korqi:\n  case X86::BI__builtin_ia32_korhi:\n  case X86::BI__builtin_ia32_korsi:\n  case X86::BI__builtin_ia32_kordi:\n    return EmitX86MaskLogic(*this, Instruction::Or, Ops);\n  case X86::BI__builtin_ia32_kxnorqi:\n  case X86::BI__builtin_ia32_kxnorhi:\n  case X86::BI__builtin_ia32_kxnorsi:\n  case X86::BI__builtin_ia32_kxnordi:\n    return EmitX86MaskLogic(*this, Instruction::Xor, Ops, true);\n  case X86::BI__builtin_ia32_kxorqi:\n  case X86::BI__builtin_ia32_kxorhi:\n  case X86::BI__builtin_ia32_kxorsi:\n  case X86::BI__builtin_ia32_kxordi:\n    return EmitX86MaskLogic(*this, Instruction::Xor,  Ops);\n  case X86::BI__builtin_ia32_knotqi:\n  case X86::BI__builtin_ia32_knothi:\n  case X86::BI__builtin_ia32_knotsi:\n  case X86::BI__builtin_ia32_knotdi: {\n    unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n    Value *Res = getMaskVecValue(*this, Ops[0], NumElts);\n    return Builder.CreateBitCast(Builder.CreateNot(Res),\n                                 Ops[0]->getType());\n  }\n  case X86::BI__builtin_ia32_kmovb:\n  case X86::BI__builtin_ia32_kmovw:\n  case X86::BI__builtin_ia32_kmovd:\n  case X86::BI__builtin_ia32_kmovq: {\n    // Bitcast to vXi1 type and then back to integer. This gets the mask\n    // register type into the IR, but might be optimized out depending on\n    // what's around it.\n    unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n    Value *Res = getMaskVecValue(*this, Ops[0], NumElts);\n    return Builder.CreateBitCast(Res, Ops[0]->getType());\n  }\n\n  case X86::BI__builtin_ia32_kunpckdi:\n  case X86::BI__builtin_ia32_kunpcksi:\n  case X86::BI__builtin_ia32_kunpckhi: {\n    unsigned NumElts = Ops[0]->getType()->getIntegerBitWidth();\n    Value *LHS = getMaskVecValue(*this, Ops[0], NumElts);\n    Value *RHS = getMaskVecValue(*this, Ops[1], NumElts);\n    int Indices[64];\n    for (unsigned i = 0; i != NumElts; ++i)\n      Indices[i] = i;\n\n    // First extract half of each vector. This gives better codegen than\n    // doing it in a single shuffle.\n    LHS = Builder.CreateShuffleVector(LHS, LHS,\n                                      makeArrayRef(Indices, NumElts / 2));\n    RHS = Builder.CreateShuffleVector(RHS, RHS,\n                                      makeArrayRef(Indices, NumElts / 2));\n    // Concat the vectors.\n    // NOTE: Operands are swapped to match the intrinsic definition.\n    Value *Res = Builder.CreateShuffleVector(RHS, LHS,\n                                             makeArrayRef(Indices, NumElts));\n    return Builder.CreateBitCast(Res, Ops[0]->getType());\n  }\n\n  case X86::BI__builtin_ia32_vplzcntd_128:\n  case X86::BI__builtin_ia32_vplzcntd_256:\n  case X86::BI__builtin_ia32_vplzcntd_512:\n  case X86::BI__builtin_ia32_vplzcntq_128:\n  case X86::BI__builtin_ia32_vplzcntq_256:\n  case X86::BI__builtin_ia32_vplzcntq_512: {\n    Function *F = CGM.getIntrinsic(Intrinsic::ctlz, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0],Builder.getInt1(false)});\n  }\n  case X86::BI__builtin_ia32_sqrtss:\n  case X86::BI__builtin_ia32_sqrtsd: {\n    Value *A = Builder.CreateExtractElement(Ops[0], (uint64_t)0);\n    Function *F;\n    if (Builder.getIsFPConstrained()) {\n      F = CGM.getIntrinsic(Intrinsic::experimental_constrained_sqrt,\n                           A->getType());\n      A = Builder.CreateConstrainedFPCall(F, {A});\n    } else {\n      F = CGM.getIntrinsic(Intrinsic::sqrt, A->getType());\n      A = Builder.CreateCall(F, {A});\n    }\n    return Builder.CreateInsertElement(Ops[0], A, (uint64_t)0);\n  }\n  case X86::BI__builtin_ia32_sqrtsd_round_mask:\n  case X86::BI__builtin_ia32_sqrtss_round_mask: {\n    unsigned CC = cast<llvm::ConstantInt>(Ops[4])->getZExtValue();\n    // Support only if the rounding mode is 4 (AKA CUR_DIRECTION),\n    // otherwise keep the intrinsic.\n    if (CC != 4) {\n      Intrinsic::ID IID = BuiltinID == X86::BI__builtin_ia32_sqrtsd_round_mask ?\n                          Intrinsic::x86_avx512_mask_sqrt_sd :\n                          Intrinsic::x86_avx512_mask_sqrt_ss;\n      return Builder.CreateCall(CGM.getIntrinsic(IID), Ops);\n    }\n    Value *A = Builder.CreateExtractElement(Ops[1], (uint64_t)0);\n    Function *F;\n    if (Builder.getIsFPConstrained()) {\n      F = CGM.getIntrinsic(Intrinsic::experimental_constrained_sqrt,\n                           A->getType());\n      A = Builder.CreateConstrainedFPCall(F, A);\n    } else {\n      F = CGM.getIntrinsic(Intrinsic::sqrt, A->getType());\n      A = Builder.CreateCall(F, A);\n    }\n    Value *Src = Builder.CreateExtractElement(Ops[2], (uint64_t)0);\n    A = EmitX86ScalarSelect(*this, Ops[3], A, Src);\n    return Builder.CreateInsertElement(Ops[0], A, (uint64_t)0);\n  }\n  case X86::BI__builtin_ia32_sqrtpd256:\n  case X86::BI__builtin_ia32_sqrtpd:\n  case X86::BI__builtin_ia32_sqrtps256:\n  case X86::BI__builtin_ia32_sqrtps:\n  case X86::BI__builtin_ia32_sqrtps512:\n  case X86::BI__builtin_ia32_sqrtpd512: {\n    if (Ops.size() == 2) {\n      unsigned CC = cast<llvm::ConstantInt>(Ops[1])->getZExtValue();\n      // Support only if the rounding mode is 4 (AKA CUR_DIRECTION),\n      // otherwise keep the intrinsic.\n      if (CC != 4) {\n        Intrinsic::ID IID = BuiltinID == X86::BI__builtin_ia32_sqrtps512 ?\n                            Intrinsic::x86_avx512_sqrt_ps_512 :\n                            Intrinsic::x86_avx512_sqrt_pd_512;\n        return Builder.CreateCall(CGM.getIntrinsic(IID), Ops);\n      }\n    }\n    if (Builder.getIsFPConstrained()) {\n      Function *F = CGM.getIntrinsic(Intrinsic::experimental_constrained_sqrt,\n                                     Ops[0]->getType());\n      return Builder.CreateConstrainedFPCall(F, Ops[0]);\n    } else {\n      Function *F = CGM.getIntrinsic(Intrinsic::sqrt, Ops[0]->getType());\n      return Builder.CreateCall(F, Ops[0]);\n    }\n  }\n  case X86::BI__builtin_ia32_pabsb128:\n  case X86::BI__builtin_ia32_pabsw128:\n  case X86::BI__builtin_ia32_pabsd128:\n  case X86::BI__builtin_ia32_pabsb256:\n  case X86::BI__builtin_ia32_pabsw256:\n  case X86::BI__builtin_ia32_pabsd256:\n  case X86::BI__builtin_ia32_pabsq128:\n  case X86::BI__builtin_ia32_pabsq256:\n  case X86::BI__builtin_ia32_pabsb512:\n  case X86::BI__builtin_ia32_pabsw512:\n  case X86::BI__builtin_ia32_pabsd512:\n  case X86::BI__builtin_ia32_pabsq512: {\n    Function *F = CGM.getIntrinsic(Intrinsic::abs, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0], Builder.getInt1(false)});\n  }\n  case X86::BI__builtin_ia32_pmaxsb128:\n  case X86::BI__builtin_ia32_pmaxsw128:\n  case X86::BI__builtin_ia32_pmaxsd128:\n  case X86::BI__builtin_ia32_pmaxsq128:\n  case X86::BI__builtin_ia32_pmaxsb256:\n  case X86::BI__builtin_ia32_pmaxsw256:\n  case X86::BI__builtin_ia32_pmaxsd256:\n  case X86::BI__builtin_ia32_pmaxsq256:\n  case X86::BI__builtin_ia32_pmaxsb512:\n  case X86::BI__builtin_ia32_pmaxsw512:\n  case X86::BI__builtin_ia32_pmaxsd512:\n  case X86::BI__builtin_ia32_pmaxsq512:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::smax);\n  case X86::BI__builtin_ia32_pmaxub128:\n  case X86::BI__builtin_ia32_pmaxuw128:\n  case X86::BI__builtin_ia32_pmaxud128:\n  case X86::BI__builtin_ia32_pmaxuq128:\n  case X86::BI__builtin_ia32_pmaxub256:\n  case X86::BI__builtin_ia32_pmaxuw256:\n  case X86::BI__builtin_ia32_pmaxud256:\n  case X86::BI__builtin_ia32_pmaxuq256:\n  case X86::BI__builtin_ia32_pmaxub512:\n  case X86::BI__builtin_ia32_pmaxuw512:\n  case X86::BI__builtin_ia32_pmaxud512:\n  case X86::BI__builtin_ia32_pmaxuq512:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::umax);\n  case X86::BI__builtin_ia32_pminsb128:\n  case X86::BI__builtin_ia32_pminsw128:\n  case X86::BI__builtin_ia32_pminsd128:\n  case X86::BI__builtin_ia32_pminsq128:\n  case X86::BI__builtin_ia32_pminsb256:\n  case X86::BI__builtin_ia32_pminsw256:\n  case X86::BI__builtin_ia32_pminsd256:\n  case X86::BI__builtin_ia32_pminsq256:\n  case X86::BI__builtin_ia32_pminsb512:\n  case X86::BI__builtin_ia32_pminsw512:\n  case X86::BI__builtin_ia32_pminsd512:\n  case X86::BI__builtin_ia32_pminsq512:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::smin);\n  case X86::BI__builtin_ia32_pminub128:\n  case X86::BI__builtin_ia32_pminuw128:\n  case X86::BI__builtin_ia32_pminud128:\n  case X86::BI__builtin_ia32_pminuq128:\n  case X86::BI__builtin_ia32_pminub256:\n  case X86::BI__builtin_ia32_pminuw256:\n  case X86::BI__builtin_ia32_pminud256:\n  case X86::BI__builtin_ia32_pminuq256:\n  case X86::BI__builtin_ia32_pminub512:\n  case X86::BI__builtin_ia32_pminuw512:\n  case X86::BI__builtin_ia32_pminud512:\n  case X86::BI__builtin_ia32_pminuq512:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::umin);\n\n  case X86::BI__builtin_ia32_pmuludq128:\n  case X86::BI__builtin_ia32_pmuludq256:\n  case X86::BI__builtin_ia32_pmuludq512:\n    return EmitX86Muldq(*this, /*IsSigned*/false, Ops);\n\n  case X86::BI__builtin_ia32_pmuldq128:\n  case X86::BI__builtin_ia32_pmuldq256:\n  case X86::BI__builtin_ia32_pmuldq512:\n    return EmitX86Muldq(*this, /*IsSigned*/true, Ops);\n\n  case X86::BI__builtin_ia32_pternlogd512_mask:\n  case X86::BI__builtin_ia32_pternlogq512_mask:\n  case X86::BI__builtin_ia32_pternlogd128_mask:\n  case X86::BI__builtin_ia32_pternlogd256_mask:\n  case X86::BI__builtin_ia32_pternlogq128_mask:\n  case X86::BI__builtin_ia32_pternlogq256_mask:\n    return EmitX86Ternlog(*this, /*ZeroMask*/false, Ops);\n\n  case X86::BI__builtin_ia32_pternlogd512_maskz:\n  case X86::BI__builtin_ia32_pternlogq512_maskz:\n  case X86::BI__builtin_ia32_pternlogd128_maskz:\n  case X86::BI__builtin_ia32_pternlogd256_maskz:\n  case X86::BI__builtin_ia32_pternlogq128_maskz:\n  case X86::BI__builtin_ia32_pternlogq256_maskz:\n    return EmitX86Ternlog(*this, /*ZeroMask*/true, Ops);\n\n  case X86::BI__builtin_ia32_vpshldd128:\n  case X86::BI__builtin_ia32_vpshldd256:\n  case X86::BI__builtin_ia32_vpshldd512:\n  case X86::BI__builtin_ia32_vpshldq128:\n  case X86::BI__builtin_ia32_vpshldq256:\n  case X86::BI__builtin_ia32_vpshldq512:\n  case X86::BI__builtin_ia32_vpshldw128:\n  case X86::BI__builtin_ia32_vpshldw256:\n  case X86::BI__builtin_ia32_vpshldw512:\n    return EmitX86FunnelShift(*this, Ops[0], Ops[1], Ops[2], false);\n\n  case X86::BI__builtin_ia32_vpshrdd128:\n  case X86::BI__builtin_ia32_vpshrdd256:\n  case X86::BI__builtin_ia32_vpshrdd512:\n  case X86::BI__builtin_ia32_vpshrdq128:\n  case X86::BI__builtin_ia32_vpshrdq256:\n  case X86::BI__builtin_ia32_vpshrdq512:\n  case X86::BI__builtin_ia32_vpshrdw128:\n  case X86::BI__builtin_ia32_vpshrdw256:\n  case X86::BI__builtin_ia32_vpshrdw512:\n    // Ops 0 and 1 are swapped.\n    return EmitX86FunnelShift(*this, Ops[1], Ops[0], Ops[2], true);\n\n  case X86::BI__builtin_ia32_vpshldvd128:\n  case X86::BI__builtin_ia32_vpshldvd256:\n  case X86::BI__builtin_ia32_vpshldvd512:\n  case X86::BI__builtin_ia32_vpshldvq128:\n  case X86::BI__builtin_ia32_vpshldvq256:\n  case X86::BI__builtin_ia32_vpshldvq512:\n  case X86::BI__builtin_ia32_vpshldvw128:\n  case X86::BI__builtin_ia32_vpshldvw256:\n  case X86::BI__builtin_ia32_vpshldvw512:\n    return EmitX86FunnelShift(*this, Ops[0], Ops[1], Ops[2], false);\n\n  case X86::BI__builtin_ia32_vpshrdvd128:\n  case X86::BI__builtin_ia32_vpshrdvd256:\n  case X86::BI__builtin_ia32_vpshrdvd512:\n  case X86::BI__builtin_ia32_vpshrdvq128:\n  case X86::BI__builtin_ia32_vpshrdvq256:\n  case X86::BI__builtin_ia32_vpshrdvq512:\n  case X86::BI__builtin_ia32_vpshrdvw128:\n  case X86::BI__builtin_ia32_vpshrdvw256:\n  case X86::BI__builtin_ia32_vpshrdvw512:\n    // Ops 0 and 1 are swapped.\n    return EmitX86FunnelShift(*this, Ops[1], Ops[0], Ops[2], true);\n\n  // Reductions\n  case X86::BI__builtin_ia32_reduce_add_d512:\n  case X86::BI__builtin_ia32_reduce_add_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_add, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n  case X86::BI__builtin_ia32_reduce_and_d512:\n  case X86::BI__builtin_ia32_reduce_and_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_and, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n  case X86::BI__builtin_ia32_reduce_fadd_pd512:\n  case X86::BI__builtin_ia32_reduce_fadd_ps512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_fadd, Ops[1]->getType());\n    return Builder.CreateCall(F, {Ops[0], Ops[1]});\n  }\n  case X86::BI__builtin_ia32_reduce_fmul_pd512:\n  case X86::BI__builtin_ia32_reduce_fmul_ps512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_fmul, Ops[1]->getType());\n    return Builder.CreateCall(F, {Ops[0], Ops[1]});\n  }\n  case X86::BI__builtin_ia32_reduce_mul_d512:\n  case X86::BI__builtin_ia32_reduce_mul_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_mul, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n  case X86::BI__builtin_ia32_reduce_or_d512:\n  case X86::BI__builtin_ia32_reduce_or_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_or, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n  case X86::BI__builtin_ia32_reduce_smax_d512:\n  case X86::BI__builtin_ia32_reduce_smax_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_smax, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n  case X86::BI__builtin_ia32_reduce_smin_d512:\n  case X86::BI__builtin_ia32_reduce_smin_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_smin, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n  case X86::BI__builtin_ia32_reduce_umax_d512:\n  case X86::BI__builtin_ia32_reduce_umax_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_umax, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n  case X86::BI__builtin_ia32_reduce_umin_d512:\n  case X86::BI__builtin_ia32_reduce_umin_q512: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::vector_reduce_umin, Ops[0]->getType());\n    return Builder.CreateCall(F, {Ops[0]});\n  }\n\n  // 3DNow!\n  case X86::BI__builtin_ia32_pswapdsf:\n  case X86::BI__builtin_ia32_pswapdsi: {\n    llvm::Type *MMXTy = llvm::Type::getX86_MMXTy(getLLVMContext());\n    Ops[0] = Builder.CreateBitCast(Ops[0], MMXTy, \"cast\");\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::x86_3dnowa_pswapd);\n    return Builder.CreateCall(F, Ops, \"pswapd\");\n  }\n  case X86::BI__builtin_ia32_rdrand16_step:\n  case X86::BI__builtin_ia32_rdrand32_step:\n  case X86::BI__builtin_ia32_rdrand64_step:\n  case X86::BI__builtin_ia32_rdseed16_step:\n  case X86::BI__builtin_ia32_rdseed32_step:\n  case X86::BI__builtin_ia32_rdseed64_step: {\n    Intrinsic::ID ID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_rdrand16_step:\n      ID = Intrinsic::x86_rdrand_16;\n      break;\n    case X86::BI__builtin_ia32_rdrand32_step:\n      ID = Intrinsic::x86_rdrand_32;\n      break;\n    case X86::BI__builtin_ia32_rdrand64_step:\n      ID = Intrinsic::x86_rdrand_64;\n      break;\n    case X86::BI__builtin_ia32_rdseed16_step:\n      ID = Intrinsic::x86_rdseed_16;\n      break;\n    case X86::BI__builtin_ia32_rdseed32_step:\n      ID = Intrinsic::x86_rdseed_32;\n      break;\n    case X86::BI__builtin_ia32_rdseed64_step:\n      ID = Intrinsic::x86_rdseed_64;\n      break;\n    }\n\n    Value *Call = Builder.CreateCall(CGM.getIntrinsic(ID));\n    Builder.CreateDefaultAlignedStore(Builder.CreateExtractValue(Call, 0),\n                                      Ops[0]);\n    return Builder.CreateExtractValue(Call, 1);\n  }\n  case X86::BI__builtin_ia32_addcarryx_u32:\n  case X86::BI__builtin_ia32_addcarryx_u64:\n  case X86::BI__builtin_ia32_subborrow_u32:\n  case X86::BI__builtin_ia32_subborrow_u64: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_addcarryx_u32:\n      IID = Intrinsic::x86_addcarry_32;\n      break;\n    case X86::BI__builtin_ia32_addcarryx_u64:\n      IID = Intrinsic::x86_addcarry_64;\n      break;\n    case X86::BI__builtin_ia32_subborrow_u32:\n      IID = Intrinsic::x86_subborrow_32;\n      break;\n    case X86::BI__builtin_ia32_subborrow_u64:\n      IID = Intrinsic::x86_subborrow_64;\n      break;\n    }\n\n    Value *Call = Builder.CreateCall(CGM.getIntrinsic(IID),\n                                     { Ops[0], Ops[1], Ops[2] });\n    Builder.CreateDefaultAlignedStore(Builder.CreateExtractValue(Call, 1),\n                                      Ops[3]);\n    return Builder.CreateExtractValue(Call, 0);\n  }\n\n  case X86::BI__builtin_ia32_fpclassps128_mask:\n  case X86::BI__builtin_ia32_fpclassps256_mask:\n  case X86::BI__builtin_ia32_fpclassps512_mask:\n  case X86::BI__builtin_ia32_fpclasspd128_mask:\n  case X86::BI__builtin_ia32_fpclasspd256_mask:\n  case X86::BI__builtin_ia32_fpclasspd512_mask: {\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    Value *MaskIn = Ops[2];\n    Ops.erase(&Ops[2]);\n\n    Intrinsic::ID ID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_fpclassps128_mask:\n      ID = Intrinsic::x86_avx512_fpclass_ps_128;\n      break;\n    case X86::BI__builtin_ia32_fpclassps256_mask:\n      ID = Intrinsic::x86_avx512_fpclass_ps_256;\n      break;\n    case X86::BI__builtin_ia32_fpclassps512_mask:\n      ID = Intrinsic::x86_avx512_fpclass_ps_512;\n      break;\n    case X86::BI__builtin_ia32_fpclasspd128_mask:\n      ID = Intrinsic::x86_avx512_fpclass_pd_128;\n      break;\n    case X86::BI__builtin_ia32_fpclasspd256_mask:\n      ID = Intrinsic::x86_avx512_fpclass_pd_256;\n      break;\n    case X86::BI__builtin_ia32_fpclasspd512_mask:\n      ID = Intrinsic::x86_avx512_fpclass_pd_512;\n      break;\n    }\n\n    Value *Fpclass = Builder.CreateCall(CGM.getIntrinsic(ID), Ops);\n    return EmitX86MaskedCompareResult(*this, Fpclass, NumElts, MaskIn);\n  }\n\n  case X86::BI__builtin_ia32_vp2intersect_q_512:\n  case X86::BI__builtin_ia32_vp2intersect_q_256:\n  case X86::BI__builtin_ia32_vp2intersect_q_128:\n  case X86::BI__builtin_ia32_vp2intersect_d_512:\n  case X86::BI__builtin_ia32_vp2intersect_d_256:\n  case X86::BI__builtin_ia32_vp2intersect_d_128: {\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    Intrinsic::ID ID;\n\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_vp2intersect_q_512:\n      ID = Intrinsic::x86_avx512_vp2intersect_q_512;\n      break;\n    case X86::BI__builtin_ia32_vp2intersect_q_256:\n      ID = Intrinsic::x86_avx512_vp2intersect_q_256;\n      break;\n    case X86::BI__builtin_ia32_vp2intersect_q_128:\n      ID = Intrinsic::x86_avx512_vp2intersect_q_128;\n      break;\n    case X86::BI__builtin_ia32_vp2intersect_d_512:\n      ID = Intrinsic::x86_avx512_vp2intersect_d_512;\n      break;\n    case X86::BI__builtin_ia32_vp2intersect_d_256:\n      ID = Intrinsic::x86_avx512_vp2intersect_d_256;\n      break;\n    case X86::BI__builtin_ia32_vp2intersect_d_128:\n      ID = Intrinsic::x86_avx512_vp2intersect_d_128;\n      break;\n    }\n\n    Value *Call = Builder.CreateCall(CGM.getIntrinsic(ID), {Ops[0], Ops[1]});\n    Value *Result = Builder.CreateExtractValue(Call, 0);\n    Result = EmitX86MaskedCompareResult(*this, Result, NumElts, nullptr);\n    Builder.CreateDefaultAlignedStore(Result, Ops[2]);\n\n    Result = Builder.CreateExtractValue(Call, 1);\n    Result = EmitX86MaskedCompareResult(*this, Result, NumElts, nullptr);\n    return Builder.CreateDefaultAlignedStore(Result, Ops[3]);\n  }\n\n  case X86::BI__builtin_ia32_vpmultishiftqb128:\n  case X86::BI__builtin_ia32_vpmultishiftqb256:\n  case X86::BI__builtin_ia32_vpmultishiftqb512: {\n    Intrinsic::ID ID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_vpmultishiftqb128:\n      ID = Intrinsic::x86_avx512_pmultishift_qb_128;\n      break;\n    case X86::BI__builtin_ia32_vpmultishiftqb256:\n      ID = Intrinsic::x86_avx512_pmultishift_qb_256;\n      break;\n    case X86::BI__builtin_ia32_vpmultishiftqb512:\n      ID = Intrinsic::x86_avx512_pmultishift_qb_512;\n      break;\n    }\n\n    return Builder.CreateCall(CGM.getIntrinsic(ID), Ops);\n  }\n\n  case X86::BI__builtin_ia32_vpshufbitqmb128_mask:\n  case X86::BI__builtin_ia32_vpshufbitqmb256_mask:\n  case X86::BI__builtin_ia32_vpshufbitqmb512_mask: {\n    unsigned NumElts =\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n    Value *MaskIn = Ops[2];\n    Ops.erase(&Ops[2]);\n\n    Intrinsic::ID ID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_vpshufbitqmb128_mask:\n      ID = Intrinsic::x86_avx512_vpshufbitqmb_128;\n      break;\n    case X86::BI__builtin_ia32_vpshufbitqmb256_mask:\n      ID = Intrinsic::x86_avx512_vpshufbitqmb_256;\n      break;\n    case X86::BI__builtin_ia32_vpshufbitqmb512_mask:\n      ID = Intrinsic::x86_avx512_vpshufbitqmb_512;\n      break;\n    }\n\n    Value *Shufbit = Builder.CreateCall(CGM.getIntrinsic(ID), Ops);\n    return EmitX86MaskedCompareResult(*this, Shufbit, NumElts, MaskIn);\n  }\n\n  // packed comparison intrinsics\n  case X86::BI__builtin_ia32_cmpeqps:\n  case X86::BI__builtin_ia32_cmpeqpd:\n    return getVectorFCmpIR(CmpInst::FCMP_OEQ, /*IsSignaling*/false);\n  case X86::BI__builtin_ia32_cmpltps:\n  case X86::BI__builtin_ia32_cmpltpd:\n    return getVectorFCmpIR(CmpInst::FCMP_OLT, /*IsSignaling*/true);\n  case X86::BI__builtin_ia32_cmpleps:\n  case X86::BI__builtin_ia32_cmplepd:\n    return getVectorFCmpIR(CmpInst::FCMP_OLE, /*IsSignaling*/true);\n  case X86::BI__builtin_ia32_cmpunordps:\n  case X86::BI__builtin_ia32_cmpunordpd:\n    return getVectorFCmpIR(CmpInst::FCMP_UNO, /*IsSignaling*/false);\n  case X86::BI__builtin_ia32_cmpneqps:\n  case X86::BI__builtin_ia32_cmpneqpd:\n    return getVectorFCmpIR(CmpInst::FCMP_UNE, /*IsSignaling*/false);\n  case X86::BI__builtin_ia32_cmpnltps:\n  case X86::BI__builtin_ia32_cmpnltpd:\n    return getVectorFCmpIR(CmpInst::FCMP_UGE, /*IsSignaling*/true);\n  case X86::BI__builtin_ia32_cmpnleps:\n  case X86::BI__builtin_ia32_cmpnlepd:\n    return getVectorFCmpIR(CmpInst::FCMP_UGT, /*IsSignaling*/true);\n  case X86::BI__builtin_ia32_cmpordps:\n  case X86::BI__builtin_ia32_cmpordpd:\n    return getVectorFCmpIR(CmpInst::FCMP_ORD, /*IsSignaling*/false);\n  case X86::BI__builtin_ia32_cmpps128_mask:\n  case X86::BI__builtin_ia32_cmpps256_mask:\n  case X86::BI__builtin_ia32_cmpps512_mask:\n  case X86::BI__builtin_ia32_cmppd128_mask:\n  case X86::BI__builtin_ia32_cmppd256_mask:\n  case X86::BI__builtin_ia32_cmppd512_mask:\n    IsMaskFCmp = true;\n    LLVM_FALLTHROUGH;\n  case X86::BI__builtin_ia32_cmpps:\n  case X86::BI__builtin_ia32_cmpps256:\n  case X86::BI__builtin_ia32_cmppd:\n  case X86::BI__builtin_ia32_cmppd256: {\n    // Lowering vector comparisons to fcmp instructions, while\n    // ignoring signalling behaviour requested\n    // ignoring rounding mode requested\n    // This is only possible if fp-model is not strict and FENV_ACCESS is off.\n\n    // The third argument is the comparison condition, and integer in the\n    // range [0, 31]\n    unsigned CC = cast<llvm::ConstantInt>(Ops[2])->getZExtValue() & 0x1f;\n\n    // Lowering to IR fcmp instruction.\n    // Ignoring requested signaling behaviour,\n    // e.g. both _CMP_GT_OS & _CMP_GT_OQ are translated to FCMP_OGT.\n    FCmpInst::Predicate Pred;\n    bool IsSignaling;\n    // Predicates for 16-31 repeat the 0-15 predicates. Only the signalling\n    // behavior is inverted. We'll handle that after the switch.\n    switch (CC & 0xf) {\n    case 0x00: Pred = FCmpInst::FCMP_OEQ;   IsSignaling = false; break;\n    case 0x01: Pred = FCmpInst::FCMP_OLT;   IsSignaling = true;  break;\n    case 0x02: Pred = FCmpInst::FCMP_OLE;   IsSignaling = true;  break;\n    case 0x03: Pred = FCmpInst::FCMP_UNO;   IsSignaling = false; break;\n    case 0x04: Pred = FCmpInst::FCMP_UNE;   IsSignaling = false; break;\n    case 0x05: Pred = FCmpInst::FCMP_UGE;   IsSignaling = true;  break;\n    case 0x06: Pred = FCmpInst::FCMP_UGT;   IsSignaling = true;  break;\n    case 0x07: Pred = FCmpInst::FCMP_ORD;   IsSignaling = false; break;\n    case 0x08: Pred = FCmpInst::FCMP_UEQ;   IsSignaling = false; break;\n    case 0x09: Pred = FCmpInst::FCMP_ULT;   IsSignaling = true;  break;\n    case 0x0a: Pred = FCmpInst::FCMP_ULE;   IsSignaling = true;  break;\n    case 0x0b: Pred = FCmpInst::FCMP_FALSE; IsSignaling = false; break;\n    case 0x0c: Pred = FCmpInst::FCMP_ONE;   IsSignaling = false; break;\n    case 0x0d: Pred = FCmpInst::FCMP_OGE;   IsSignaling = true;  break;\n    case 0x0e: Pred = FCmpInst::FCMP_OGT;   IsSignaling = true;  break;\n    case 0x0f: Pred = FCmpInst::FCMP_TRUE;  IsSignaling = false; break;\n    default: llvm_unreachable(\"Unhandled CC\");\n    }\n\n    // Invert the signalling behavior for 16-31.\n    if (CC & 0x10)\n      IsSignaling = !IsSignaling;\n\n    // If the predicate is true or false and we're using constrained intrinsics,\n    // we don't have a compare intrinsic we can use. Just use the legacy X86\n    // specific intrinsic.\n    // If the intrinsic is mask enabled and we're using constrained intrinsics,\n    // use the legacy X86 specific intrinsic.\n    if (Builder.getIsFPConstrained() &&\n        (Pred == FCmpInst::FCMP_TRUE || Pred == FCmpInst::FCMP_FALSE ||\n         IsMaskFCmp)) {\n\n      Intrinsic::ID IID;\n      switch (BuiltinID) {\n      default: llvm_unreachable(\"Unexpected builtin\");\n      case X86::BI__builtin_ia32_cmpps:\n        IID = Intrinsic::x86_sse_cmp_ps;\n        break;\n      case X86::BI__builtin_ia32_cmpps256:\n        IID = Intrinsic::x86_avx_cmp_ps_256;\n        break;\n      case X86::BI__builtin_ia32_cmppd:\n        IID = Intrinsic::x86_sse2_cmp_pd;\n        break;\n      case X86::BI__builtin_ia32_cmppd256:\n        IID = Intrinsic::x86_avx_cmp_pd_256;\n        break;\n      case X86::BI__builtin_ia32_cmpps512_mask:\n        IID = Intrinsic::x86_avx512_mask_cmp_ps_512;\n        break;\n      case X86::BI__builtin_ia32_cmppd512_mask:\n        IID = Intrinsic::x86_avx512_mask_cmp_pd_512;\n        break;\n      case X86::BI__builtin_ia32_cmpps128_mask:\n        IID = Intrinsic::x86_avx512_mask_cmp_ps_128;\n        break;\n      case X86::BI__builtin_ia32_cmpps256_mask:\n        IID = Intrinsic::x86_avx512_mask_cmp_ps_256;\n        break;\n      case X86::BI__builtin_ia32_cmppd128_mask:\n        IID = Intrinsic::x86_avx512_mask_cmp_pd_128;\n        break;\n      case X86::BI__builtin_ia32_cmppd256_mask:\n        IID = Intrinsic::x86_avx512_mask_cmp_pd_256;\n        break;\n      }\n\n      Function *Intr = CGM.getIntrinsic(IID);\n      if (IsMaskFCmp) {\n        unsigned NumElts =\n            cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n        Ops[3] = getMaskVecValue(*this, Ops[3], NumElts);\n        Value *Cmp = Builder.CreateCall(Intr, Ops);\n        return EmitX86MaskedCompareResult(*this, Cmp, NumElts, nullptr);\n      }\n\n      return Builder.CreateCall(Intr, Ops);\n    }\n\n    // Builtins without the _mask suffix return a vector of integers\n    // of the same width as the input vectors\n    if (IsMaskFCmp) {\n      // We ignore SAE if strict FP is disabled. We only keep precise\n      // exception behavior under strict FP.\n      unsigned NumElts =\n          cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements();\n      Value *Cmp;\n      if (IsSignaling)\n        Cmp = Builder.CreateFCmpS(Pred, Ops[0], Ops[1]);\n      else\n        Cmp = Builder.CreateFCmp(Pred, Ops[0], Ops[1]);\n      return EmitX86MaskedCompareResult(*this, Cmp, NumElts, Ops[3]);\n    }\n\n    return getVectorFCmpIR(Pred, IsSignaling);\n  }\n\n  // SSE scalar comparison intrinsics\n  case X86::BI__builtin_ia32_cmpeqss:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 0);\n  case X86::BI__builtin_ia32_cmpltss:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 1);\n  case X86::BI__builtin_ia32_cmpless:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 2);\n  case X86::BI__builtin_ia32_cmpunordss:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 3);\n  case X86::BI__builtin_ia32_cmpneqss:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 4);\n  case X86::BI__builtin_ia32_cmpnltss:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 5);\n  case X86::BI__builtin_ia32_cmpnless:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 6);\n  case X86::BI__builtin_ia32_cmpordss:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse_cmp_ss, 7);\n  case X86::BI__builtin_ia32_cmpeqsd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 0);\n  case X86::BI__builtin_ia32_cmpltsd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 1);\n  case X86::BI__builtin_ia32_cmplesd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 2);\n  case X86::BI__builtin_ia32_cmpunordsd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 3);\n  case X86::BI__builtin_ia32_cmpneqsd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 4);\n  case X86::BI__builtin_ia32_cmpnltsd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 5);\n  case X86::BI__builtin_ia32_cmpnlesd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 6);\n  case X86::BI__builtin_ia32_cmpordsd:\n    return getCmpIntrinsicCall(Intrinsic::x86_sse2_cmp_sd, 7);\n\n  // f16c half2float intrinsics\n  case X86::BI__builtin_ia32_vcvtph2ps:\n  case X86::BI__builtin_ia32_vcvtph2ps256:\n  case X86::BI__builtin_ia32_vcvtph2ps_mask:\n  case X86::BI__builtin_ia32_vcvtph2ps256_mask:\n  case X86::BI__builtin_ia32_vcvtph2ps512_mask:\n    return EmitX86CvtF16ToFloatExpr(*this, Ops, ConvertType(E->getType()));\n\n// AVX512 bf16 intrinsics\n  case X86::BI__builtin_ia32_cvtneps2bf16_128_mask: {\n    Ops[2] = getMaskVecValue(\n        *this, Ops[2],\n        cast<llvm::FixedVectorType>(Ops[0]->getType())->getNumElements());\n    Intrinsic::ID IID = Intrinsic::x86_avx512bf16_mask_cvtneps2bf16_128;\n    return Builder.CreateCall(CGM.getIntrinsic(IID), Ops);\n  }\n  case X86::BI__builtin_ia32_cvtsbf162ss_32:\n    return EmitX86CvtBF16ToFloatExpr(*this, E, Ops);\n\n  case X86::BI__builtin_ia32_cvtneps2bf16_256_mask:\n  case X86::BI__builtin_ia32_cvtneps2bf16_512_mask: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported intrinsic!\");\n    case X86::BI__builtin_ia32_cvtneps2bf16_256_mask:\n      IID = Intrinsic::x86_avx512bf16_cvtneps2bf16_256;\n      break;\n    case X86::BI__builtin_ia32_cvtneps2bf16_512_mask:\n      IID = Intrinsic::x86_avx512bf16_cvtneps2bf16_512;\n      break;\n    }\n    Value *Res = Builder.CreateCall(CGM.getIntrinsic(IID), Ops[0]);\n    return EmitX86Select(*this, Ops[2], Res, Ops[1]);\n  }\n\n  case X86::BI__emul:\n  case X86::BI__emulu: {\n    llvm::Type *Int64Ty = llvm::IntegerType::get(getLLVMContext(), 64);\n    bool isSigned = (BuiltinID == X86::BI__emul);\n    Value *LHS = Builder.CreateIntCast(Ops[0], Int64Ty, isSigned);\n    Value *RHS = Builder.CreateIntCast(Ops[1], Int64Ty, isSigned);\n    return Builder.CreateMul(LHS, RHS, \"\", !isSigned, isSigned);\n  }\n  case X86::BI__mulh:\n  case X86::BI__umulh:\n  case X86::BI_mul128:\n  case X86::BI_umul128: {\n    llvm::Type *ResType = ConvertType(E->getType());\n    llvm::Type *Int128Ty = llvm::IntegerType::get(getLLVMContext(), 128);\n\n    bool IsSigned = (BuiltinID == X86::BI__mulh || BuiltinID == X86::BI_mul128);\n    Value *LHS = Builder.CreateIntCast(Ops[0], Int128Ty, IsSigned);\n    Value *RHS = Builder.CreateIntCast(Ops[1], Int128Ty, IsSigned);\n\n    Value *MulResult, *HigherBits;\n    if (IsSigned) {\n      MulResult = Builder.CreateNSWMul(LHS, RHS);\n      HigherBits = Builder.CreateAShr(MulResult, 64);\n    } else {\n      MulResult = Builder.CreateNUWMul(LHS, RHS);\n      HigherBits = Builder.CreateLShr(MulResult, 64);\n    }\n    HigherBits = Builder.CreateIntCast(HigherBits, ResType, IsSigned);\n\n    if (BuiltinID == X86::BI__mulh || BuiltinID == X86::BI__umulh)\n      return HigherBits;\n\n    Address HighBitsAddress = EmitPointerWithAlignment(E->getArg(2));\n    Builder.CreateStore(HigherBits, HighBitsAddress);\n    return Builder.CreateIntCast(MulResult, ResType, IsSigned);\n  }\n\n  case X86::BI__faststorefence: {\n    return Builder.CreateFence(llvm::AtomicOrdering::SequentiallyConsistent,\n                               llvm::SyncScope::System);\n  }\n  case X86::BI__shiftleft128:\n  case X86::BI__shiftright128: {\n    llvm::Function *F = CGM.getIntrinsic(\n        BuiltinID == X86::BI__shiftleft128 ? Intrinsic::fshl : Intrinsic::fshr,\n        Int64Ty);\n    // Flip low/high ops and zero-extend amount to matching type.\n    // shiftleft128(Low, High, Amt) -> fshl(High, Low, Amt)\n    // shiftright128(Low, High, Amt) -> fshr(High, Low, Amt)\n    std::swap(Ops[0], Ops[1]);\n    Ops[2] = Builder.CreateZExt(Ops[2], Int64Ty);\n    return Builder.CreateCall(F, Ops);\n  }\n  case X86::BI_ReadWriteBarrier:\n  case X86::BI_ReadBarrier:\n  case X86::BI_WriteBarrier: {\n    return Builder.CreateFence(llvm::AtomicOrdering::SequentiallyConsistent,\n                               llvm::SyncScope::SingleThread);\n  }\n\n  case X86::BI_AddressOfReturnAddress: {\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::addressofreturnaddress, AllocaInt8PtrTy);\n    return Builder.CreateCall(F);\n  }\n  case X86::BI__stosb: {\n    // We treat __stosb as a volatile memset - it may not generate \"rep stosb\"\n    // instruction, but it will create a memset that won't be optimized away.\n    return Builder.CreateMemSet(Ops[0], Ops[1], Ops[2], Align(1), true);\n  }\n  case X86::BI__ud2:\n    // llvm.trap makes a ud2a instruction on x86.\n    return EmitTrapCall(Intrinsic::trap);\n  case X86::BI__int2c: {\n    // This syscall signals a driver assertion failure in x86 NT kernels.\n    llvm::FunctionType *FTy = llvm::FunctionType::get(VoidTy, false);\n    llvm::InlineAsm *IA =\n        llvm::InlineAsm::get(FTy, \"int $$0x2c\", \"\", /*hasSideEffects=*/true);\n    llvm::AttributeList NoReturnAttr = llvm::AttributeList::get(\n        getLLVMContext(), llvm::AttributeList::FunctionIndex,\n        llvm::Attribute::NoReturn);\n    llvm::CallInst *CI = Builder.CreateCall(IA);\n    CI->setAttributes(NoReturnAttr);\n    return CI;\n  }\n  case X86::BI__readfsbyte:\n  case X86::BI__readfsword:\n  case X86::BI__readfsdword:\n  case X86::BI__readfsqword: {\n    llvm::Type *IntTy = ConvertType(E->getType());\n    Value *Ptr =\n        Builder.CreateIntToPtr(Ops[0], llvm::PointerType::get(IntTy, 257));\n    LoadInst *Load = Builder.CreateAlignedLoad(\n        IntTy, Ptr, getContext().getTypeAlignInChars(E->getType()));\n    Load->setVolatile(true);\n    return Load;\n  }\n  case X86::BI__readgsbyte:\n  case X86::BI__readgsword:\n  case X86::BI__readgsdword:\n  case X86::BI__readgsqword: {\n    llvm::Type *IntTy = ConvertType(E->getType());\n    Value *Ptr =\n        Builder.CreateIntToPtr(Ops[0], llvm::PointerType::get(IntTy, 256));\n    LoadInst *Load = Builder.CreateAlignedLoad(\n        IntTy, Ptr, getContext().getTypeAlignInChars(E->getType()));\n    Load->setVolatile(true);\n    return Load;\n  }\n  case X86::BI__builtin_ia32_paddsb512:\n  case X86::BI__builtin_ia32_paddsw512:\n  case X86::BI__builtin_ia32_paddsb256:\n  case X86::BI__builtin_ia32_paddsw256:\n  case X86::BI__builtin_ia32_paddsb128:\n  case X86::BI__builtin_ia32_paddsw128:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::sadd_sat);\n  case X86::BI__builtin_ia32_paddusb512:\n  case X86::BI__builtin_ia32_paddusw512:\n  case X86::BI__builtin_ia32_paddusb256:\n  case X86::BI__builtin_ia32_paddusw256:\n  case X86::BI__builtin_ia32_paddusb128:\n  case X86::BI__builtin_ia32_paddusw128:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::uadd_sat);\n  case X86::BI__builtin_ia32_psubsb512:\n  case X86::BI__builtin_ia32_psubsw512:\n  case X86::BI__builtin_ia32_psubsb256:\n  case X86::BI__builtin_ia32_psubsw256:\n  case X86::BI__builtin_ia32_psubsb128:\n  case X86::BI__builtin_ia32_psubsw128:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::ssub_sat);\n  case X86::BI__builtin_ia32_psubusb512:\n  case X86::BI__builtin_ia32_psubusw512:\n  case X86::BI__builtin_ia32_psubusb256:\n  case X86::BI__builtin_ia32_psubusw256:\n  case X86::BI__builtin_ia32_psubusb128:\n  case X86::BI__builtin_ia32_psubusw128:\n    return EmitX86BinaryIntrinsic(*this, Ops, Intrinsic::usub_sat);\n  case X86::BI__builtin_ia32_encodekey128_u32: {\n    Intrinsic::ID IID = Intrinsic::x86_encodekey128;\n\n    Value *Call = Builder.CreateCall(CGM.getIntrinsic(IID), {Ops[0], Ops[1]});\n\n    for (int i = 0; i < 6; ++i) {\n      Value *Extract = Builder.CreateExtractValue(Call, i + 1);\n      Value *Ptr = Builder.CreateConstGEP1_32(Ops[2], i * 16);\n      Ptr = Builder.CreateBitCast(\n          Ptr, llvm::PointerType::getUnqual(Extract->getType()));\n      Builder.CreateAlignedStore(Extract, Ptr, Align(1));\n    }\n\n    return Builder.CreateExtractValue(Call, 0);\n  }\n  case X86::BI__builtin_ia32_encodekey256_u32: {\n    Intrinsic::ID IID = Intrinsic::x86_encodekey256;\n\n    Value *Call =\n        Builder.CreateCall(CGM.getIntrinsic(IID), {Ops[0], Ops[1], Ops[2]});\n\n    for (int i = 0; i < 7; ++i) {\n      Value *Extract = Builder.CreateExtractValue(Call, i + 1);\n      Value *Ptr = Builder.CreateConstGEP1_32(Ops[3], i * 16);\n      Ptr = Builder.CreateBitCast(\n          Ptr, llvm::PointerType::getUnqual(Extract->getType()));\n      Builder.CreateAlignedStore(Extract, Ptr, Align(1));\n    }\n\n    return Builder.CreateExtractValue(Call, 0);\n  }\n  case X86::BI__builtin_ia32_aesenc128kl_u8:\n  case X86::BI__builtin_ia32_aesdec128kl_u8:\n  case X86::BI__builtin_ia32_aesenc256kl_u8:\n  case X86::BI__builtin_ia32_aesdec256kl_u8: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unexpected builtin\");\n    case X86::BI__builtin_ia32_aesenc128kl_u8:\n      IID = Intrinsic::x86_aesenc128kl;\n      break;\n    case X86::BI__builtin_ia32_aesdec128kl_u8:\n      IID = Intrinsic::x86_aesdec128kl;\n      break;\n    case X86::BI__builtin_ia32_aesenc256kl_u8:\n      IID = Intrinsic::x86_aesenc256kl;\n      break;\n    case X86::BI__builtin_ia32_aesdec256kl_u8:\n      IID = Intrinsic::x86_aesdec256kl;\n      break;\n    }\n\n    Value *Call = Builder.CreateCall(CGM.getIntrinsic(IID), {Ops[1], Ops[2]});\n\n    Builder.CreateDefaultAlignedStore(Builder.CreateExtractValue(Call, 1),\n                                      Ops[0]);\n\n    return Builder.CreateExtractValue(Call, 0);\n  }\n  case X86::BI__builtin_ia32_aesencwide128kl_u8:\n  case X86::BI__builtin_ia32_aesdecwide128kl_u8:\n  case X86::BI__builtin_ia32_aesencwide256kl_u8:\n  case X86::BI__builtin_ia32_aesdecwide256kl_u8: {\n    Intrinsic::ID IID;\n    switch (BuiltinID) {\n    case X86::BI__builtin_ia32_aesencwide128kl_u8:\n      IID = Intrinsic::x86_aesencwide128kl;\n      break;\n    case X86::BI__builtin_ia32_aesdecwide128kl_u8:\n      IID = Intrinsic::x86_aesdecwide128kl;\n      break;\n    case X86::BI__builtin_ia32_aesencwide256kl_u8:\n      IID = Intrinsic::x86_aesencwide256kl;\n      break;\n    case X86::BI__builtin_ia32_aesdecwide256kl_u8:\n      IID = Intrinsic::x86_aesdecwide256kl;\n      break;\n    }\n\n    Value *InOps[9];\n    InOps[0] = Ops[2];\n    for (int i = 0; i != 8; ++i) {\n      Value *Ptr = Builder.CreateConstGEP1_32(Ops[1], i);\n      InOps[i + 1] = Builder.CreateAlignedLoad(Ptr, Align(16));\n    }\n\n    Value *Call = Builder.CreateCall(CGM.getIntrinsic(IID), InOps);\n\n    for (int i = 0; i != 8; ++i) {\n      Value *Extract = Builder.CreateExtractValue(Call, i + 1);\n      Value *Ptr = Builder.CreateConstGEP1_32(Ops[0], i);\n      Builder.CreateAlignedStore(Extract, Ptr, Align(16));\n    }\n\n    return Builder.CreateExtractValue(Call, 0);\n  }\n  }\n}\n\nValue *CodeGenFunction::EmitPPCBuiltinExpr(unsigned BuiltinID,\n                                           const CallExpr *E) {\n  SmallVector<Value*, 4> Ops;\n\n  for (unsigned i = 0, e = E->getNumArgs(); i != e; i++)\n    Ops.push_back(EmitScalarExpr(E->getArg(i)));\n\n  Intrinsic::ID ID = Intrinsic::not_intrinsic;\n\n  switch (BuiltinID) {\n  default: return nullptr;\n\n  // __builtin_ppc_get_timebase is GCC 4.8+'s PowerPC-specific name for what we\n  // call __builtin_readcyclecounter.\n  case PPC::BI__builtin_ppc_get_timebase:\n    return Builder.CreateCall(CGM.getIntrinsic(Intrinsic::readcyclecounter));\n\n  // vec_ld, vec_xl_be, vec_lvsl, vec_lvsr\n  case PPC::BI__builtin_altivec_lvx:\n  case PPC::BI__builtin_altivec_lvxl:\n  case PPC::BI__builtin_altivec_lvebx:\n  case PPC::BI__builtin_altivec_lvehx:\n  case PPC::BI__builtin_altivec_lvewx:\n  case PPC::BI__builtin_altivec_lvsl:\n  case PPC::BI__builtin_altivec_lvsr:\n  case PPC::BI__builtin_vsx_lxvd2x:\n  case PPC::BI__builtin_vsx_lxvw4x:\n  case PPC::BI__builtin_vsx_lxvd2x_be:\n  case PPC::BI__builtin_vsx_lxvw4x_be:\n  case PPC::BI__builtin_vsx_lxvl:\n  case PPC::BI__builtin_vsx_lxvll:\n  {\n    if(BuiltinID == PPC::BI__builtin_vsx_lxvl ||\n       BuiltinID == PPC::BI__builtin_vsx_lxvll){\n      Ops[0] = Builder.CreateBitCast(Ops[0], Int8PtrTy);\n    }else {\n      Ops[1] = Builder.CreateBitCast(Ops[1], Int8PtrTy);\n      Ops[0] = Builder.CreateGEP(Ops[1], Ops[0]);\n      Ops.pop_back();\n    }\n\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported ld/lvsl/lvsr intrinsic!\");\n    case PPC::BI__builtin_altivec_lvx:\n      ID = Intrinsic::ppc_altivec_lvx;\n      break;\n    case PPC::BI__builtin_altivec_lvxl:\n      ID = Intrinsic::ppc_altivec_lvxl;\n      break;\n    case PPC::BI__builtin_altivec_lvebx:\n      ID = Intrinsic::ppc_altivec_lvebx;\n      break;\n    case PPC::BI__builtin_altivec_lvehx:\n      ID = Intrinsic::ppc_altivec_lvehx;\n      break;\n    case PPC::BI__builtin_altivec_lvewx:\n      ID = Intrinsic::ppc_altivec_lvewx;\n      break;\n    case PPC::BI__builtin_altivec_lvsl:\n      ID = Intrinsic::ppc_altivec_lvsl;\n      break;\n    case PPC::BI__builtin_altivec_lvsr:\n      ID = Intrinsic::ppc_altivec_lvsr;\n      break;\n    case PPC::BI__builtin_vsx_lxvd2x:\n      ID = Intrinsic::ppc_vsx_lxvd2x;\n      break;\n    case PPC::BI__builtin_vsx_lxvw4x:\n      ID = Intrinsic::ppc_vsx_lxvw4x;\n      break;\n    case PPC::BI__builtin_vsx_lxvd2x_be:\n      ID = Intrinsic::ppc_vsx_lxvd2x_be;\n      break;\n    case PPC::BI__builtin_vsx_lxvw4x_be:\n      ID = Intrinsic::ppc_vsx_lxvw4x_be;\n      break;\n    case PPC::BI__builtin_vsx_lxvl:\n      ID = Intrinsic::ppc_vsx_lxvl;\n      break;\n    case PPC::BI__builtin_vsx_lxvll:\n      ID = Intrinsic::ppc_vsx_lxvll;\n      break;\n    }\n    llvm::Function *F = CGM.getIntrinsic(ID);\n    return Builder.CreateCall(F, Ops, \"\");\n  }\n\n  // vec_st, vec_xst_be\n  case PPC::BI__builtin_altivec_stvx:\n  case PPC::BI__builtin_altivec_stvxl:\n  case PPC::BI__builtin_altivec_stvebx:\n  case PPC::BI__builtin_altivec_stvehx:\n  case PPC::BI__builtin_altivec_stvewx:\n  case PPC::BI__builtin_vsx_stxvd2x:\n  case PPC::BI__builtin_vsx_stxvw4x:\n  case PPC::BI__builtin_vsx_stxvd2x_be:\n  case PPC::BI__builtin_vsx_stxvw4x_be:\n  case PPC::BI__builtin_vsx_stxvl:\n  case PPC::BI__builtin_vsx_stxvll:\n  {\n    if(BuiltinID == PPC::BI__builtin_vsx_stxvl ||\n      BuiltinID == PPC::BI__builtin_vsx_stxvll ){\n      Ops[1] = Builder.CreateBitCast(Ops[1], Int8PtrTy);\n    }else {\n      Ops[2] = Builder.CreateBitCast(Ops[2], Int8PtrTy);\n      Ops[1] = Builder.CreateGEP(Ops[2], Ops[1]);\n      Ops.pop_back();\n    }\n\n    switch (BuiltinID) {\n    default: llvm_unreachable(\"Unsupported st intrinsic!\");\n    case PPC::BI__builtin_altivec_stvx:\n      ID = Intrinsic::ppc_altivec_stvx;\n      break;\n    case PPC::BI__builtin_altivec_stvxl:\n      ID = Intrinsic::ppc_altivec_stvxl;\n      break;\n    case PPC::BI__builtin_altivec_stvebx:\n      ID = Intrinsic::ppc_altivec_stvebx;\n      break;\n    case PPC::BI__builtin_altivec_stvehx:\n      ID = Intrinsic::ppc_altivec_stvehx;\n      break;\n    case PPC::BI__builtin_altivec_stvewx:\n      ID = Intrinsic::ppc_altivec_stvewx;\n      break;\n    case PPC::BI__builtin_vsx_stxvd2x:\n      ID = Intrinsic::ppc_vsx_stxvd2x;\n      break;\n    case PPC::BI__builtin_vsx_stxvw4x:\n      ID = Intrinsic::ppc_vsx_stxvw4x;\n      break;\n    case PPC::BI__builtin_vsx_stxvd2x_be:\n      ID = Intrinsic::ppc_vsx_stxvd2x_be;\n      break;\n    case PPC::BI__builtin_vsx_stxvw4x_be:\n      ID = Intrinsic::ppc_vsx_stxvw4x_be;\n      break;\n    case PPC::BI__builtin_vsx_stxvl:\n      ID = Intrinsic::ppc_vsx_stxvl;\n      break;\n    case PPC::BI__builtin_vsx_stxvll:\n      ID = Intrinsic::ppc_vsx_stxvll;\n      break;\n    }\n    llvm::Function *F = CGM.getIntrinsic(ID);\n    return Builder.CreateCall(F, Ops, \"\");\n  }\n  // Square root\n  case PPC::BI__builtin_vsx_xvsqrtsp:\n  case PPC::BI__builtin_vsx_xvsqrtdp: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    if (Builder.getIsFPConstrained()) {\n      llvm::Function *F = CGM.getIntrinsic(\n          Intrinsic::experimental_constrained_sqrt, ResultType);\n      return Builder.CreateConstrainedFPCall(F, X);\n    } else {\n      llvm::Function *F = CGM.getIntrinsic(Intrinsic::sqrt, ResultType);\n      return Builder.CreateCall(F, X);\n    }\n  }\n  // Count leading zeros\n  case PPC::BI__builtin_altivec_vclzb:\n  case PPC::BI__builtin_altivec_vclzh:\n  case PPC::BI__builtin_altivec_vclzw:\n  case PPC::BI__builtin_altivec_vclzd: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Undef = ConstantInt::get(Builder.getInt1Ty(), false);\n    Function *F = CGM.getIntrinsic(Intrinsic::ctlz, ResultType);\n    return Builder.CreateCall(F, {X, Undef});\n  }\n  case PPC::BI__builtin_altivec_vctzb:\n  case PPC::BI__builtin_altivec_vctzh:\n  case PPC::BI__builtin_altivec_vctzw:\n  case PPC::BI__builtin_altivec_vctzd: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Undef = ConstantInt::get(Builder.getInt1Ty(), false);\n    Function *F = CGM.getIntrinsic(Intrinsic::cttz, ResultType);\n    return Builder.CreateCall(F, {X, Undef});\n  }\n  case PPC::BI__builtin_altivec_vec_replace_elt:\n  case PPC::BI__builtin_altivec_vec_replace_unaligned: {\n    // The third argument of vec_replace_elt and vec_replace_unaligned must\n    // be a compile time constant and will be emitted either to the vinsw\n    // or vinsd instruction.\n    ConstantInt *ArgCI = dyn_cast<ConstantInt>(Ops[2]);\n    assert(ArgCI &&\n           \"Third Arg to vinsw/vinsd intrinsic must be a constant integer!\");\n    llvm::Type *ResultType = ConvertType(E->getType());\n    llvm::Function *F = nullptr;\n    Value *Call = nullptr;\n    int64_t ConstArg = ArgCI->getSExtValue();\n    unsigned ArgWidth = Ops[1]->getType()->getPrimitiveSizeInBits();\n    bool Is32Bit = false;\n    assert((ArgWidth == 32 || ArgWidth == 64) && \"Invalid argument width\");\n    // The input to vec_replace_elt is an element index, not a byte index.\n    if (BuiltinID == PPC::BI__builtin_altivec_vec_replace_elt)\n      ConstArg *= ArgWidth / 8;\n    if (ArgWidth == 32) {\n      Is32Bit = true;\n      // When the second argument is 32 bits, it can either be an integer or\n      // a float. The vinsw intrinsic is used in this case.\n      F = CGM.getIntrinsic(Intrinsic::ppc_altivec_vinsw);\n      // Fix the constant according to endianess.\n      if (getTarget().isLittleEndian())\n        ConstArg = 12 - ConstArg;\n    } else {\n      // When the second argument is 64 bits, it can either be a long long or\n      // a double. The vinsd intrinsic is used in this case.\n      F = CGM.getIntrinsic(Intrinsic::ppc_altivec_vinsd);\n      // Fix the constant for little endian.\n      if (getTarget().isLittleEndian())\n        ConstArg = 8 - ConstArg;\n    }\n    Ops[2] = ConstantInt::getSigned(Int32Ty, ConstArg);\n    // Depending on ArgWidth, the input vector could be a float or a double.\n    // If the input vector is a float type, bitcast the inputs to integers. Or,\n    // if the input vector is a double, bitcast the inputs to 64-bit integers.\n    if (!Ops[1]->getType()->isIntegerTy(ArgWidth)) {\n      Ops[0] = Builder.CreateBitCast(\n          Ops[0], Is32Bit ? llvm::FixedVectorType::get(Int32Ty, 4)\n                          : llvm::FixedVectorType::get(Int64Ty, 2));\n      Ops[1] = Builder.CreateBitCast(Ops[1], Is32Bit ? Int32Ty : Int64Ty);\n    }\n    // Emit the call to vinsw or vinsd.\n    Call = Builder.CreateCall(F, Ops);\n    // Depending on the builtin, bitcast to the approriate result type.\n    if (BuiltinID == PPC::BI__builtin_altivec_vec_replace_elt &&\n        !Ops[1]->getType()->isIntegerTy())\n      return Builder.CreateBitCast(Call, ResultType);\n    else if (BuiltinID == PPC::BI__builtin_altivec_vec_replace_elt &&\n             Ops[1]->getType()->isIntegerTy())\n      return Call;\n    else\n      return Builder.CreateBitCast(Call,\n                                   llvm::FixedVectorType::get(Int8Ty, 16));\n  }\n  case PPC::BI__builtin_altivec_vpopcntb:\n  case PPC::BI__builtin_altivec_vpopcnth:\n  case PPC::BI__builtin_altivec_vpopcntw:\n  case PPC::BI__builtin_altivec_vpopcntd: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::ctpop, ResultType);\n    return Builder.CreateCall(F, X);\n  }\n  // Copy sign\n  case PPC::BI__builtin_vsx_xvcpsgnsp:\n  case PPC::BI__builtin_vsx_xvcpsgndp: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    ID = Intrinsic::copysign;\n    llvm::Function *F = CGM.getIntrinsic(ID, ResultType);\n    return Builder.CreateCall(F, {X, Y});\n  }\n  // Rounding/truncation\n  case PPC::BI__builtin_vsx_xvrspip:\n  case PPC::BI__builtin_vsx_xvrdpip:\n  case PPC::BI__builtin_vsx_xvrdpim:\n  case PPC::BI__builtin_vsx_xvrspim:\n  case PPC::BI__builtin_vsx_xvrdpi:\n  case PPC::BI__builtin_vsx_xvrspi:\n  case PPC::BI__builtin_vsx_xvrdpic:\n  case PPC::BI__builtin_vsx_xvrspic:\n  case PPC::BI__builtin_vsx_xvrdpiz:\n  case PPC::BI__builtin_vsx_xvrspiz: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    if (BuiltinID == PPC::BI__builtin_vsx_xvrdpim ||\n        BuiltinID == PPC::BI__builtin_vsx_xvrspim)\n      ID = Builder.getIsFPConstrained()\n               ? Intrinsic::experimental_constrained_floor\n               : Intrinsic::floor;\n    else if (BuiltinID == PPC::BI__builtin_vsx_xvrdpi ||\n             BuiltinID == PPC::BI__builtin_vsx_xvrspi)\n      ID = Builder.getIsFPConstrained()\n               ? Intrinsic::experimental_constrained_round\n               : Intrinsic::round;\n    else if (BuiltinID == PPC::BI__builtin_vsx_xvrdpic ||\n             BuiltinID == PPC::BI__builtin_vsx_xvrspic)\n      ID = Builder.getIsFPConstrained()\n               ? Intrinsic::experimental_constrained_rint\n               : Intrinsic::rint;\n    else if (BuiltinID == PPC::BI__builtin_vsx_xvrdpip ||\n             BuiltinID == PPC::BI__builtin_vsx_xvrspip)\n      ID = Builder.getIsFPConstrained()\n               ? Intrinsic::experimental_constrained_ceil\n               : Intrinsic::ceil;\n    else if (BuiltinID == PPC::BI__builtin_vsx_xvrdpiz ||\n             BuiltinID == PPC::BI__builtin_vsx_xvrspiz)\n      ID = Builder.getIsFPConstrained()\n               ? Intrinsic::experimental_constrained_trunc\n               : Intrinsic::trunc;\n    llvm::Function *F = CGM.getIntrinsic(ID, ResultType);\n    return Builder.getIsFPConstrained() ? Builder.CreateConstrainedFPCall(F, X)\n                                        : Builder.CreateCall(F, X);\n  }\n\n  // Absolute value\n  case PPC::BI__builtin_vsx_xvabsdp:\n  case PPC::BI__builtin_vsx_xvabssp: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::fabs, ResultType);\n    return Builder.CreateCall(F, X);\n  }\n\n  // FMA variations\n  case PPC::BI__builtin_vsx_xvmaddadp:\n  case PPC::BI__builtin_vsx_xvmaddasp:\n  case PPC::BI__builtin_vsx_xvnmaddadp:\n  case PPC::BI__builtin_vsx_xvnmaddasp:\n  case PPC::BI__builtin_vsx_xvmsubadp:\n  case PPC::BI__builtin_vsx_xvmsubasp:\n  case PPC::BI__builtin_vsx_xvnmsubadp:\n  case PPC::BI__builtin_vsx_xvnmsubasp: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    Value *Z = EmitScalarExpr(E->getArg(2));\n    llvm::Function *F;\n    if (Builder.getIsFPConstrained())\n      F = CGM.getIntrinsic(Intrinsic::experimental_constrained_fma, ResultType);\n    else\n      F = CGM.getIntrinsic(Intrinsic::fma, ResultType);\n    switch (BuiltinID) {\n      case PPC::BI__builtin_vsx_xvmaddadp:\n      case PPC::BI__builtin_vsx_xvmaddasp:\n        if (Builder.getIsFPConstrained())\n          return Builder.CreateConstrainedFPCall(F, {X, Y, Z});\n        else\n          return Builder.CreateCall(F, {X, Y, Z});\n      case PPC::BI__builtin_vsx_xvnmaddadp:\n      case PPC::BI__builtin_vsx_xvnmaddasp:\n        if (Builder.getIsFPConstrained())\n          return Builder.CreateFNeg(\n              Builder.CreateConstrainedFPCall(F, {X, Y, Z}), \"neg\");\n        else\n          return Builder.CreateFNeg(Builder.CreateCall(F, {X, Y, Z}), \"neg\");\n      case PPC::BI__builtin_vsx_xvmsubadp:\n      case PPC::BI__builtin_vsx_xvmsubasp:\n        if (Builder.getIsFPConstrained())\n          return Builder.CreateConstrainedFPCall(\n              F, {X, Y, Builder.CreateFNeg(Z, \"neg\")});\n        else\n          return Builder.CreateCall(F, {X, Y, Builder.CreateFNeg(Z, \"neg\")});\n      case PPC::BI__builtin_vsx_xvnmsubadp:\n      case PPC::BI__builtin_vsx_xvnmsubasp:\n        if (Builder.getIsFPConstrained())\n          return Builder.CreateFNeg(\n              Builder.CreateConstrainedFPCall(\n                  F, {X, Y, Builder.CreateFNeg(Z, \"neg\")}),\n              \"neg\");\n        else\n          return Builder.CreateFNeg(\n              Builder.CreateCall(F, {X, Y, Builder.CreateFNeg(Z, \"neg\")}),\n              \"neg\");\n    }\n    llvm_unreachable(\"Unknown FMA operation\");\n    return nullptr; // Suppress no-return warning\n  }\n\n  case PPC::BI__builtin_vsx_insertword: {\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::ppc_vsx_xxinsertw);\n\n    // Third argument is a compile time constant int. It must be clamped to\n    // to the range [0, 12].\n    ConstantInt *ArgCI = dyn_cast<ConstantInt>(Ops[2]);\n    assert(ArgCI &&\n           \"Third arg to xxinsertw intrinsic must be constant integer\");\n    const int64_t MaxIndex = 12;\n    int64_t Index = clamp(ArgCI->getSExtValue(), 0, MaxIndex);\n\n    // The builtin semantics don't exactly match the xxinsertw instructions\n    // semantics (which ppc_vsx_xxinsertw follows). The builtin extracts the\n    // word from the first argument, and inserts it in the second argument. The\n    // instruction extracts the word from its second input register and inserts\n    // it into its first input register, so swap the first and second arguments.\n    std::swap(Ops[0], Ops[1]);\n\n    // Need to cast the second argument from a vector of unsigned int to a\n    // vector of long long.\n    Ops[1] =\n        Builder.CreateBitCast(Ops[1], llvm::FixedVectorType::get(Int64Ty, 2));\n\n    if (getTarget().isLittleEndian()) {\n      // Reverse the double words in the vector we will extract from.\n      Ops[0] =\n          Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int64Ty, 2));\n      Ops[0] = Builder.CreateShuffleVector(Ops[0], Ops[0], ArrayRef<int>{1, 0});\n\n      // Reverse the index.\n      Index = MaxIndex - Index;\n    }\n\n    // Intrinsic expects the first arg to be a vector of int.\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int32Ty, 4));\n    Ops[2] = ConstantInt::getSigned(Int32Ty, Index);\n    return Builder.CreateCall(F, Ops);\n  }\n\n  case PPC::BI__builtin_vsx_extractuword: {\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::ppc_vsx_xxextractuw);\n\n    // Intrinsic expects the first argument to be a vector of doublewords.\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int64Ty, 2));\n\n    // The second argument is a compile time constant int that needs to\n    // be clamped to the range [0, 12].\n    ConstantInt *ArgCI = dyn_cast<ConstantInt>(Ops[1]);\n    assert(ArgCI &&\n           \"Second Arg to xxextractuw intrinsic must be a constant integer!\");\n    const int64_t MaxIndex = 12;\n    int64_t Index = clamp(ArgCI->getSExtValue(), 0, MaxIndex);\n\n    if (getTarget().isLittleEndian()) {\n      // Reverse the index.\n      Index = MaxIndex - Index;\n      Ops[1] = ConstantInt::getSigned(Int32Ty, Index);\n\n      // Emit the call, then reverse the double words of the results vector.\n      Value *Call = Builder.CreateCall(F, Ops);\n\n      Value *ShuffleCall =\n          Builder.CreateShuffleVector(Call, Call, ArrayRef<int>{1, 0});\n      return ShuffleCall;\n    } else {\n      Ops[1] = ConstantInt::getSigned(Int32Ty, Index);\n      return Builder.CreateCall(F, Ops);\n    }\n  }\n\n  case PPC::BI__builtin_vsx_xxpermdi: {\n    ConstantInt *ArgCI = dyn_cast<ConstantInt>(Ops[2]);\n    assert(ArgCI && \"Third arg must be constant integer!\");\n\n    unsigned Index = ArgCI->getZExtValue();\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int64Ty, 2));\n    Ops[1] =\n        Builder.CreateBitCast(Ops[1], llvm::FixedVectorType::get(Int64Ty, 2));\n\n    // Account for endianness by treating this as just a shuffle. So we use the\n    // same indices for both LE and BE in order to produce expected results in\n    // both cases.\n    int ElemIdx0 = (Index & 2) >> 1;\n    int ElemIdx1 = 2 + (Index & 1);\n\n    int ShuffleElts[2] = {ElemIdx0, ElemIdx1};\n    Value *ShuffleCall =\n        Builder.CreateShuffleVector(Ops[0], Ops[1], ShuffleElts);\n    QualType BIRetType = E->getType();\n    auto RetTy = ConvertType(BIRetType);\n    return Builder.CreateBitCast(ShuffleCall, RetTy);\n  }\n\n  case PPC::BI__builtin_vsx_xxsldwi: {\n    ConstantInt *ArgCI = dyn_cast<ConstantInt>(Ops[2]);\n    assert(ArgCI && \"Third argument must be a compile time constant\");\n    unsigned Index = ArgCI->getZExtValue() & 0x3;\n    Ops[0] =\n        Builder.CreateBitCast(Ops[0], llvm::FixedVectorType::get(Int32Ty, 4));\n    Ops[1] =\n        Builder.CreateBitCast(Ops[1], llvm::FixedVectorType::get(Int32Ty, 4));\n\n    // Create a shuffle mask\n    int ElemIdx0;\n    int ElemIdx1;\n    int ElemIdx2;\n    int ElemIdx3;\n    if (getTarget().isLittleEndian()) {\n      // Little endian element N comes from element 8+N-Index of the\n      // concatenated wide vector (of course, using modulo arithmetic on\n      // the total number of elements).\n      ElemIdx0 = (8 - Index) % 8;\n      ElemIdx1 = (9 - Index) % 8;\n      ElemIdx2 = (10 - Index) % 8;\n      ElemIdx3 = (11 - Index) % 8;\n    } else {\n      // Big endian ElemIdx<N> = Index + N\n      ElemIdx0 = Index;\n      ElemIdx1 = Index + 1;\n      ElemIdx2 = Index + 2;\n      ElemIdx3 = Index + 3;\n    }\n\n    int ShuffleElts[4] = {ElemIdx0, ElemIdx1, ElemIdx2, ElemIdx3};\n    Value *ShuffleCall =\n        Builder.CreateShuffleVector(Ops[0], Ops[1], ShuffleElts);\n    QualType BIRetType = E->getType();\n    auto RetTy = ConvertType(BIRetType);\n    return Builder.CreateBitCast(ShuffleCall, RetTy);\n  }\n\n  case PPC::BI__builtin_pack_vector_int128: {\n    bool isLittleEndian = getTarget().isLittleEndian();\n    Value *UndefValue =\n        llvm::UndefValue::get(llvm::FixedVectorType::get(Ops[0]->getType(), 2));\n    Value *Res = Builder.CreateInsertElement(\n        UndefValue, Ops[0], (uint64_t)(isLittleEndian ? 1 : 0));\n    Res = Builder.CreateInsertElement(Res, Ops[1],\n                                      (uint64_t)(isLittleEndian ? 0 : 1));\n    return Builder.CreateBitCast(Res, ConvertType(E->getType()));\n  }\n\n  case PPC::BI__builtin_unpack_vector_int128: {\n    ConstantInt *Index = cast<ConstantInt>(Ops[1]);\n    Value *Unpacked = Builder.CreateBitCast(\n        Ops[0], llvm::FixedVectorType::get(ConvertType(E->getType()), 2));\n\n    if (getTarget().isLittleEndian())\n      Index = ConstantInt::get(Index->getType(), 1 - Index->getZExtValue());\n\n    return Builder.CreateExtractElement(Unpacked, Index);\n  }\n\n  // The PPC MMA builtins take a pointer to a __vector_quad as an argument.\n  // Some of the MMA instructions accumulate their result into an existing\n  // accumulator whereas the others generate a new accumulator. So we need to\n  // use custom code generation to expand a builtin call with a pointer to a\n  // load (if the corresponding instruction accumulates its result) followed by\n  // the call to the intrinsic and a store of the result.\n#define CUSTOM_BUILTIN(Name, Types, Accumulate) \\\n  case PPC::BI__builtin_##Name:\n#include \"clang/Basic/BuiltinsPPC.def\"\n  {\n    // The first argument of these two builtins is a pointer used to store their\n    // result. However, the llvm intrinsics return their result in multiple\n    // return values. So, here we emit code extracting these values from the\n    // intrinsic results and storing them using that pointer.\n    if (BuiltinID == PPC::BI__builtin_mma_disassemble_acc ||\n        BuiltinID == PPC::BI__builtin_vsx_disassemble_pair) {\n      unsigned NumVecs = 2;\n      auto Intrinsic = Intrinsic::ppc_vsx_disassemble_pair;\n      if (BuiltinID == PPC::BI__builtin_mma_disassemble_acc) {\n        NumVecs = 4;\n        Intrinsic = Intrinsic::ppc_mma_disassemble_acc;\n      }\n      llvm::Function *F = CGM.getIntrinsic(Intrinsic);\n      Address Addr = EmitPointerWithAlignment(E->getArg(1));\n      Value *Vec = Builder.CreateLoad(Addr);\n      Value *Call = Builder.CreateCall(F, {Vec});\n      llvm::Type *VTy = llvm::FixedVectorType::get(Int8Ty, 16);\n      Value *Ptr = Builder.CreateBitCast(Ops[0], VTy->getPointerTo());\n      for (unsigned i=0; i<NumVecs; i++) {\n        Value *Vec = Builder.CreateExtractValue(Call, i);\n        llvm::ConstantInt* Index = llvm::ConstantInt::get(IntTy, i);\n        Value *GEP = Builder.CreateInBoundsGEP(Ptr, Index);\n        Builder.CreateAlignedStore(Vec, GEP, MaybeAlign(16));\n      }\n      return Call;\n    }\n    bool Accumulate;\n    switch (BuiltinID) {\n  #define CUSTOM_BUILTIN(Name, Types, Acc) \\\n    case PPC::BI__builtin_##Name: \\\n      ID = Intrinsic::ppc_##Name; \\\n      Accumulate = Acc; \\\n      break;\n  #include \"clang/Basic/BuiltinsPPC.def\"\n    }\n    if (BuiltinID == PPC::BI__builtin_vsx_lxvp ||\n        BuiltinID == PPC::BI__builtin_vsx_stxvp) {\n      if (BuiltinID == PPC::BI__builtin_vsx_lxvp) {\n        Ops[1] = Builder.CreateBitCast(Ops[1], Int8PtrTy);\n        Ops[0] = Builder.CreateGEP(Ops[1], Ops[0]);\n      } else {\n        Ops[2] = Builder.CreateBitCast(Ops[2], Int8PtrTy);\n        Ops[1] = Builder.CreateGEP(Ops[2], Ops[1]);\n      }\n      Ops.pop_back();\n      llvm::Function *F = CGM.getIntrinsic(ID);\n      return Builder.CreateCall(F, Ops, \"\");\n    }\n    SmallVector<Value*, 4> CallOps;\n    if (Accumulate) {\n      Address Addr = EmitPointerWithAlignment(E->getArg(0));\n      Value *Acc = Builder.CreateLoad(Addr);\n      CallOps.push_back(Acc);\n    }\n    for (unsigned i=1; i<Ops.size(); i++)\n      CallOps.push_back(Ops[i]);\n    llvm::Function *F = CGM.getIntrinsic(ID);\n    Value *Call = Builder.CreateCall(F, CallOps);\n    return Builder.CreateAlignedStore(Call, Ops[0], MaybeAlign(64));\n  }\n  }\n}\n\nnamespace {\n// If \\p E is not null pointer, insert address space cast to match return\n// type of \\p E if necessary.\nValue *EmitAMDGPUDispatchPtr(CodeGenFunction &CGF,\n                             const CallExpr *E = nullptr) {\n  auto *F = CGF.CGM.getIntrinsic(Intrinsic::amdgcn_dispatch_ptr);\n  auto *Call = CGF.Builder.CreateCall(F);\n  Call->addAttribute(\n      AttributeList::ReturnIndex,\n      Attribute::getWithDereferenceableBytes(Call->getContext(), 64));\n  Call->addAttribute(AttributeList::ReturnIndex,\n                     Attribute::getWithAlignment(Call->getContext(), Align(4)));\n  if (!E)\n    return Call;\n  QualType BuiltinRetType = E->getType();\n  auto *RetTy = cast<llvm::PointerType>(CGF.ConvertType(BuiltinRetType));\n  if (RetTy == Call->getType())\n    return Call;\n  return CGF.Builder.CreateAddrSpaceCast(Call, RetTy);\n}\n\n// \\p Index is 0, 1, and 2 for x, y, and z dimension, respectively.\nValue *EmitAMDGPUWorkGroupSize(CodeGenFunction &CGF, unsigned Index) {\n  const unsigned XOffset = 4;\n  auto *DP = EmitAMDGPUDispatchPtr(CGF);\n  // Indexing the HSA kernel_dispatch_packet struct.\n  auto *Offset = llvm::ConstantInt::get(CGF.Int32Ty, XOffset + Index * 2);\n  auto *GEP = CGF.Builder.CreateGEP(DP, Offset);\n  auto *DstTy =\n      CGF.Int16Ty->getPointerTo(GEP->getType()->getPointerAddressSpace());\n  auto *Cast = CGF.Builder.CreateBitCast(GEP, DstTy);\n  auto *LD = CGF.Builder.CreateLoad(Address(Cast, CharUnits::fromQuantity(2)));\n  llvm::MDBuilder MDHelper(CGF.getLLVMContext());\n  llvm::MDNode *RNode = MDHelper.createRange(APInt(16, 1),\n      APInt(16, CGF.getTarget().getMaxOpenCLWorkGroupSize() + 1));\n  LD->setMetadata(llvm::LLVMContext::MD_range, RNode);\n  LD->setMetadata(llvm::LLVMContext::MD_invariant_load,\n      llvm::MDNode::get(CGF.getLLVMContext(), None));\n  return LD;\n}\n\n// \\p Index is 0, 1, and 2 for x, y, and z dimension, respectively.\nValue *EmitAMDGPUGridSize(CodeGenFunction &CGF, unsigned Index) {\n  const unsigned XOffset = 12;\n  auto *DP = EmitAMDGPUDispatchPtr(CGF);\n  // Indexing the HSA kernel_dispatch_packet struct.\n  auto *Offset = llvm::ConstantInt::get(CGF.Int32Ty, XOffset + Index * 4);\n  auto *GEP = CGF.Builder.CreateGEP(DP, Offset);\n  auto *DstTy =\n      CGF.Int32Ty->getPointerTo(GEP->getType()->getPointerAddressSpace());\n  auto *Cast = CGF.Builder.CreateBitCast(GEP, DstTy);\n  auto *LD = CGF.Builder.CreateLoad(Address(Cast, CharUnits::fromQuantity(4)));\n  LD->setMetadata(llvm::LLVMContext::MD_invariant_load,\n                  llvm::MDNode::get(CGF.getLLVMContext(), None));\n  return LD;\n}\n} // namespace\n\n// For processing memory ordering and memory scope arguments of various\n// amdgcn builtins.\n// \\p Order takes a C++11 comptabile memory-ordering specifier and converts\n// it into LLVM's memory ordering specifier using atomic C ABI, and writes\n// to \\p AO. \\p Scope takes a const char * and converts it into AMDGCN\n// specific SyncScopeID and writes it to \\p SSID.\nbool CodeGenFunction::ProcessOrderScopeAMDGCN(Value *Order, Value *Scope,\n                                              llvm::AtomicOrdering &AO,\n                                              llvm::SyncScope::ID &SSID) {\n  if (isa<llvm::ConstantInt>(Order)) {\n    int ord = cast<llvm::ConstantInt>(Order)->getZExtValue();\n\n    // Map C11/C++11 memory ordering to LLVM memory ordering\n    switch (static_cast<llvm::AtomicOrderingCABI>(ord)) {\n    case llvm::AtomicOrderingCABI::acquire:\n      AO = llvm::AtomicOrdering::Acquire;\n      break;\n    case llvm::AtomicOrderingCABI::release:\n      AO = llvm::AtomicOrdering::Release;\n      break;\n    case llvm::AtomicOrderingCABI::acq_rel:\n      AO = llvm::AtomicOrdering::AcquireRelease;\n      break;\n    case llvm::AtomicOrderingCABI::seq_cst:\n      AO = llvm::AtomicOrdering::SequentiallyConsistent;\n      break;\n    case llvm::AtomicOrderingCABI::consume:\n    case llvm::AtomicOrderingCABI::relaxed:\n      break;\n    }\n\n    StringRef scp;\n    llvm::getConstantStringInfo(Scope, scp);\n    SSID = getLLVMContext().getOrInsertSyncScopeID(scp);\n    return true;\n  }\n  return false;\n}\n\nValue *CodeGenFunction::EmitAMDGPUBuiltinExpr(unsigned BuiltinID,\n                                              const CallExpr *E) {\n  llvm::AtomicOrdering AO = llvm::AtomicOrdering::SequentiallyConsistent;\n  llvm::SyncScope::ID SSID;\n  switch (BuiltinID) {\n  case AMDGPU::BI__builtin_amdgcn_div_scale:\n  case AMDGPU::BI__builtin_amdgcn_div_scalef: {\n    // Translate from the intrinsics's struct return to the builtin's out\n    // argument.\n\n    Address FlagOutPtr = EmitPointerWithAlignment(E->getArg(3));\n\n    llvm::Value *X = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Y = EmitScalarExpr(E->getArg(1));\n    llvm::Value *Z = EmitScalarExpr(E->getArg(2));\n\n    llvm::Function *Callee = CGM.getIntrinsic(Intrinsic::amdgcn_div_scale,\n                                           X->getType());\n\n    llvm::Value *Tmp = Builder.CreateCall(Callee, {X, Y, Z});\n\n    llvm::Value *Result = Builder.CreateExtractValue(Tmp, 0);\n    llvm::Value *Flag = Builder.CreateExtractValue(Tmp, 1);\n\n    llvm::Type *RealFlagType\n      = FlagOutPtr.getPointer()->getType()->getPointerElementType();\n\n    llvm::Value *FlagExt = Builder.CreateZExt(Flag, RealFlagType);\n    Builder.CreateStore(FlagExt, FlagOutPtr);\n    return Result;\n  }\n  case AMDGPU::BI__builtin_amdgcn_div_fmas:\n  case AMDGPU::BI__builtin_amdgcn_div_fmasf: {\n    llvm::Value *Src0 = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Src1 = EmitScalarExpr(E->getArg(1));\n    llvm::Value *Src2 = EmitScalarExpr(E->getArg(2));\n    llvm::Value *Src3 = EmitScalarExpr(E->getArg(3));\n\n    llvm::Function *F = CGM.getIntrinsic(Intrinsic::amdgcn_div_fmas,\n                                      Src0->getType());\n    llvm::Value *Src3ToBool = Builder.CreateIsNotNull(Src3);\n    return Builder.CreateCall(F, {Src0, Src1, Src2, Src3ToBool});\n  }\n\n  case AMDGPU::BI__builtin_amdgcn_ds_swizzle:\n    return emitBinaryBuiltin(*this, E, Intrinsic::amdgcn_ds_swizzle);\n  case AMDGPU::BI__builtin_amdgcn_mov_dpp8:\n    return emitBinaryBuiltin(*this, E, Intrinsic::amdgcn_mov_dpp8);\n  case AMDGPU::BI__builtin_amdgcn_mov_dpp:\n  case AMDGPU::BI__builtin_amdgcn_update_dpp: {\n    llvm::SmallVector<llvm::Value *, 6> Args;\n    for (unsigned I = 0; I != E->getNumArgs(); ++I)\n      Args.push_back(EmitScalarExpr(E->getArg(I)));\n    assert(Args.size() == 5 || Args.size() == 6);\n    if (Args.size() == 5)\n      Args.insert(Args.begin(), llvm::UndefValue::get(Args[0]->getType()));\n    Function *F =\n        CGM.getIntrinsic(Intrinsic::amdgcn_update_dpp, Args[0]->getType());\n    return Builder.CreateCall(F, Args);\n  }\n  case AMDGPU::BI__builtin_amdgcn_div_fixup:\n  case AMDGPU::BI__builtin_amdgcn_div_fixupf:\n  case AMDGPU::BI__builtin_amdgcn_div_fixuph:\n    return emitTernaryBuiltin(*this, E, Intrinsic::amdgcn_div_fixup);\n  case AMDGPU::BI__builtin_amdgcn_trig_preop:\n  case AMDGPU::BI__builtin_amdgcn_trig_preopf:\n    return emitFPIntBuiltin(*this, E, Intrinsic::amdgcn_trig_preop);\n  case AMDGPU::BI__builtin_amdgcn_rcp:\n  case AMDGPU::BI__builtin_amdgcn_rcpf:\n  case AMDGPU::BI__builtin_amdgcn_rcph:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_rcp);\n  case AMDGPU::BI__builtin_amdgcn_sqrt:\n  case AMDGPU::BI__builtin_amdgcn_sqrtf:\n  case AMDGPU::BI__builtin_amdgcn_sqrth:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_sqrt);\n  case AMDGPU::BI__builtin_amdgcn_rsq:\n  case AMDGPU::BI__builtin_amdgcn_rsqf:\n  case AMDGPU::BI__builtin_amdgcn_rsqh:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_rsq);\n  case AMDGPU::BI__builtin_amdgcn_rsq_clamp:\n  case AMDGPU::BI__builtin_amdgcn_rsq_clampf:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_rsq_clamp);\n  case AMDGPU::BI__builtin_amdgcn_sinf:\n  case AMDGPU::BI__builtin_amdgcn_sinh:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_sin);\n  case AMDGPU::BI__builtin_amdgcn_cosf:\n  case AMDGPU::BI__builtin_amdgcn_cosh:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_cos);\n  case AMDGPU::BI__builtin_amdgcn_dispatch_ptr:\n    return EmitAMDGPUDispatchPtr(*this, E);\n  case AMDGPU::BI__builtin_amdgcn_log_clampf:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_log_clamp);\n  case AMDGPU::BI__builtin_amdgcn_ldexp:\n  case AMDGPU::BI__builtin_amdgcn_ldexpf:\n  case AMDGPU::BI__builtin_amdgcn_ldexph:\n    return emitFPIntBuiltin(*this, E, Intrinsic::amdgcn_ldexp);\n  case AMDGPU::BI__builtin_amdgcn_frexp_mant:\n  case AMDGPU::BI__builtin_amdgcn_frexp_mantf:\n  case AMDGPU::BI__builtin_amdgcn_frexp_manth:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_frexp_mant);\n  case AMDGPU::BI__builtin_amdgcn_frexp_exp:\n  case AMDGPU::BI__builtin_amdgcn_frexp_expf: {\n    Value *Src0 = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrinsic::amdgcn_frexp_exp,\n                                { Builder.getInt32Ty(), Src0->getType() });\n    return Builder.CreateCall(F, Src0);\n  }\n  case AMDGPU::BI__builtin_amdgcn_frexp_exph: {\n    Value *Src0 = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrinsic::amdgcn_frexp_exp,\n                                { Builder.getInt16Ty(), Src0->getType() });\n    return Builder.CreateCall(F, Src0);\n  }\n  case AMDGPU::BI__builtin_amdgcn_fract:\n  case AMDGPU::BI__builtin_amdgcn_fractf:\n  case AMDGPU::BI__builtin_amdgcn_fracth:\n    return emitUnaryBuiltin(*this, E, Intrinsic::amdgcn_fract);\n  case AMDGPU::BI__builtin_amdgcn_lerp:\n    return emitTernaryBuiltin(*this, E, Intrinsic::amdgcn_lerp);\n  case AMDGPU::BI__builtin_amdgcn_ubfe:\n    return emitTernaryBuiltin(*this, E, Intrinsic::amdgcn_ubfe);\n  case AMDGPU::BI__builtin_amdgcn_sbfe:\n    return emitTernaryBuiltin(*this, E, Intrinsic::amdgcn_sbfe);\n  case AMDGPU::BI__builtin_amdgcn_uicmp:\n  case AMDGPU::BI__builtin_amdgcn_uicmpl:\n  case AMDGPU::BI__builtin_amdgcn_sicmp:\n  case AMDGPU::BI__builtin_amdgcn_sicmpl: {\n    llvm::Value *Src0 = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Src1 = EmitScalarExpr(E->getArg(1));\n    llvm::Value *Src2 = EmitScalarExpr(E->getArg(2));\n\n    // FIXME-GFX10: How should 32 bit mask be handled?\n    Function *F = CGM.getIntrinsic(Intrinsic::amdgcn_icmp,\n      { Builder.getInt64Ty(), Src0->getType() });\n    return Builder.CreateCall(F, { Src0, Src1, Src2 });\n  }\n  case AMDGPU::BI__builtin_amdgcn_fcmp:\n  case AMDGPU::BI__builtin_amdgcn_fcmpf: {\n    llvm::Value *Src0 = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Src1 = EmitScalarExpr(E->getArg(1));\n    llvm::Value *Src2 = EmitScalarExpr(E->getArg(2));\n\n    // FIXME-GFX10: How should 32 bit mask be handled?\n    Function *F = CGM.getIntrinsic(Intrinsic::amdgcn_fcmp,\n      { Builder.getInt64Ty(), Src0->getType() });\n    return Builder.CreateCall(F, { Src0, Src1, Src2 });\n  }\n  case AMDGPU::BI__builtin_amdgcn_class:\n  case AMDGPU::BI__builtin_amdgcn_classf:\n  case AMDGPU::BI__builtin_amdgcn_classh:\n    return emitFPIntBuiltin(*this, E, Intrinsic::amdgcn_class);\n  case AMDGPU::BI__builtin_amdgcn_fmed3f:\n  case AMDGPU::BI__builtin_amdgcn_fmed3h:\n    return emitTernaryBuiltin(*this, E, Intrinsic::amdgcn_fmed3);\n  case AMDGPU::BI__builtin_amdgcn_ds_append:\n  case AMDGPU::BI__builtin_amdgcn_ds_consume: {\n    Intrinsic::ID Intrin = BuiltinID == AMDGPU::BI__builtin_amdgcn_ds_append ?\n      Intrinsic::amdgcn_ds_append : Intrinsic::amdgcn_ds_consume;\n    Value *Src0 = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrin, { Src0->getType() });\n    return Builder.CreateCall(F, { Src0, Builder.getFalse() });\n  }\n  case AMDGPU::BI__builtin_amdgcn_ds_faddf:\n  case AMDGPU::BI__builtin_amdgcn_ds_fminf:\n  case AMDGPU::BI__builtin_amdgcn_ds_fmaxf: {\n    Intrinsic::ID Intrin;\n    switch (BuiltinID) {\n    case AMDGPU::BI__builtin_amdgcn_ds_faddf:\n      Intrin = Intrinsic::amdgcn_ds_fadd;\n      break;\n    case AMDGPU::BI__builtin_amdgcn_ds_fminf:\n      Intrin = Intrinsic::amdgcn_ds_fmin;\n      break;\n    case AMDGPU::BI__builtin_amdgcn_ds_fmaxf:\n      Intrin = Intrinsic::amdgcn_ds_fmax;\n      break;\n    }\n    llvm::Value *Src0 = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Src1 = EmitScalarExpr(E->getArg(1));\n    llvm::Value *Src2 = EmitScalarExpr(E->getArg(2));\n    llvm::Value *Src3 = EmitScalarExpr(E->getArg(3));\n    llvm::Value *Src4 = EmitScalarExpr(E->getArg(4));\n    llvm::Function *F = CGM.getIntrinsic(Intrin, { Src1->getType() });\n    llvm::FunctionType *FTy = F->getFunctionType();\n    llvm::Type *PTy = FTy->getParamType(0);\n    Src0 = Builder.CreatePointerBitCastOrAddrSpaceCast(Src0, PTy);\n    return Builder.CreateCall(F, { Src0, Src1, Src2, Src3, Src4 });\n  }\n  case AMDGPU::BI__builtin_amdgcn_read_exec: {\n    CallInst *CI = cast<CallInst>(\n      EmitSpecialRegisterBuiltin(*this, E, Int64Ty, Int64Ty, NormalRead, \"exec\"));\n    CI->setConvergent();\n    return CI;\n  }\n  case AMDGPU::BI__builtin_amdgcn_read_exec_lo:\n  case AMDGPU::BI__builtin_amdgcn_read_exec_hi: {\n    StringRef RegName = BuiltinID == AMDGPU::BI__builtin_amdgcn_read_exec_lo ?\n      \"exec_lo\" : \"exec_hi\";\n    CallInst *CI = cast<CallInst>(\n      EmitSpecialRegisterBuiltin(*this, E, Int32Ty, Int32Ty, NormalRead, RegName));\n    CI->setConvergent();\n    return CI;\n  }\n  // amdgcn workitem\n  case AMDGPU::BI__builtin_amdgcn_workitem_id_x:\n    return emitRangedBuiltin(*this, Intrinsic::amdgcn_workitem_id_x, 0, 1024);\n  case AMDGPU::BI__builtin_amdgcn_workitem_id_y:\n    return emitRangedBuiltin(*this, Intrinsic::amdgcn_workitem_id_y, 0, 1024);\n  case AMDGPU::BI__builtin_amdgcn_workitem_id_z:\n    return emitRangedBuiltin(*this, Intrinsic::amdgcn_workitem_id_z, 0, 1024);\n\n  // amdgcn workgroup size\n  case AMDGPU::BI__builtin_amdgcn_workgroup_size_x:\n    return EmitAMDGPUWorkGroupSize(*this, 0);\n  case AMDGPU::BI__builtin_amdgcn_workgroup_size_y:\n    return EmitAMDGPUWorkGroupSize(*this, 1);\n  case AMDGPU::BI__builtin_amdgcn_workgroup_size_z:\n    return EmitAMDGPUWorkGroupSize(*this, 2);\n\n  // amdgcn grid size\n  case AMDGPU::BI__builtin_amdgcn_grid_size_x:\n    return EmitAMDGPUGridSize(*this, 0);\n  case AMDGPU::BI__builtin_amdgcn_grid_size_y:\n    return EmitAMDGPUGridSize(*this, 1);\n  case AMDGPU::BI__builtin_amdgcn_grid_size_z:\n    return EmitAMDGPUGridSize(*this, 2);\n\n  // r600 intrinsics\n  case AMDGPU::BI__builtin_r600_recipsqrt_ieee:\n  case AMDGPU::BI__builtin_r600_recipsqrt_ieeef:\n    return emitUnaryBuiltin(*this, E, Intrinsic::r600_recipsqrt_ieee);\n  case AMDGPU::BI__builtin_r600_read_tidig_x:\n    return emitRangedBuiltin(*this, Intrinsic::r600_read_tidig_x, 0, 1024);\n  case AMDGPU::BI__builtin_r600_read_tidig_y:\n    return emitRangedBuiltin(*this, Intrinsic::r600_read_tidig_y, 0, 1024);\n  case AMDGPU::BI__builtin_r600_read_tidig_z:\n    return emitRangedBuiltin(*this, Intrinsic::r600_read_tidig_z, 0, 1024);\n  case AMDGPU::BI__builtin_amdgcn_alignbit: {\n    llvm::Value *Src0 = EmitScalarExpr(E->getArg(0));\n    llvm::Value *Src1 = EmitScalarExpr(E->getArg(1));\n    llvm::Value *Src2 = EmitScalarExpr(E->getArg(2));\n    Function *F = CGM.getIntrinsic(Intrinsic::fshr, Src0->getType());\n    return Builder.CreateCall(F, { Src0, Src1, Src2 });\n  }\n\n  case AMDGPU::BI__builtin_amdgcn_fence: {\n    if (ProcessOrderScopeAMDGCN(EmitScalarExpr(E->getArg(0)),\n                                EmitScalarExpr(E->getArg(1)), AO, SSID))\n      return Builder.CreateFence(AO, SSID);\n    LLVM_FALLTHROUGH;\n  }\n  case AMDGPU::BI__builtin_amdgcn_atomic_inc32:\n  case AMDGPU::BI__builtin_amdgcn_atomic_inc64:\n  case AMDGPU::BI__builtin_amdgcn_atomic_dec32:\n  case AMDGPU::BI__builtin_amdgcn_atomic_dec64: {\n    unsigned BuiltinAtomicOp;\n    llvm::Type *ResultType = ConvertType(E->getType());\n\n    switch (BuiltinID) {\n    case AMDGPU::BI__builtin_amdgcn_atomic_inc32:\n    case AMDGPU::BI__builtin_amdgcn_atomic_inc64:\n      BuiltinAtomicOp = Intrinsic::amdgcn_atomic_inc;\n      break;\n    case AMDGPU::BI__builtin_amdgcn_atomic_dec32:\n    case AMDGPU::BI__builtin_amdgcn_atomic_dec64:\n      BuiltinAtomicOp = Intrinsic::amdgcn_atomic_dec;\n      break;\n    }\n\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Value *Val = EmitScalarExpr(E->getArg(1));\n\n    llvm::Function *F =\n        CGM.getIntrinsic(BuiltinAtomicOp, {ResultType, Ptr->getType()});\n\n    if (ProcessOrderScopeAMDGCN(EmitScalarExpr(E->getArg(2)),\n                                EmitScalarExpr(E->getArg(3)), AO, SSID)) {\n\n      // llvm.amdgcn.atomic.inc and llvm.amdgcn.atomic.dec expects ordering and\n      // scope as unsigned values\n      Value *MemOrder = Builder.getInt32(static_cast<int>(AO));\n      Value *MemScope = Builder.getInt32(static_cast<int>(SSID));\n\n      QualType PtrTy = E->getArg(0)->IgnoreImpCasts()->getType();\n      bool Volatile =\n          PtrTy->castAs<PointerType>()->getPointeeType().isVolatileQualified();\n      Value *IsVolatile = Builder.getInt1(static_cast<bool>(Volatile));\n\n      return Builder.CreateCall(F, {Ptr, Val, MemOrder, MemScope, IsVolatile});\n    }\n    LLVM_FALLTHROUGH;\n  }\n  default:\n    return nullptr;\n  }\n}\n\n/// Handle a SystemZ function in which the final argument is a pointer\n/// to an int that receives the post-instruction CC value.  At the LLVM level\n/// this is represented as a function that returns a {result, cc} pair.\nstatic Value *EmitSystemZIntrinsicWithCC(CodeGenFunction &CGF,\n                                         unsigned IntrinsicID,\n                                         const CallExpr *E) {\n  unsigned NumArgs = E->getNumArgs() - 1;\n  SmallVector<Value *, 8> Args(NumArgs);\n  for (unsigned I = 0; I < NumArgs; ++I)\n    Args[I] = CGF.EmitScalarExpr(E->getArg(I));\n  Address CCPtr = CGF.EmitPointerWithAlignment(E->getArg(NumArgs));\n  Function *F = CGF.CGM.getIntrinsic(IntrinsicID);\n  Value *Call = CGF.Builder.CreateCall(F, Args);\n  Value *CC = CGF.Builder.CreateExtractValue(Call, 1);\n  CGF.Builder.CreateStore(CC, CCPtr);\n  return CGF.Builder.CreateExtractValue(Call, 0);\n}\n\nValue *CodeGenFunction::EmitSystemZBuiltinExpr(unsigned BuiltinID,\n                                               const CallExpr *E) {\n  switch (BuiltinID) {\n  case SystemZ::BI__builtin_tbegin: {\n    Value *TDB = EmitScalarExpr(E->getArg(0));\n    Value *Control = llvm::ConstantInt::get(Int32Ty, 0xff0c);\n    Function *F = CGM.getIntrinsic(Intrinsic::s390_tbegin);\n    return Builder.CreateCall(F, {TDB, Control});\n  }\n  case SystemZ::BI__builtin_tbegin_nofloat: {\n    Value *TDB = EmitScalarExpr(E->getArg(0));\n    Value *Control = llvm::ConstantInt::get(Int32Ty, 0xff0c);\n    Function *F = CGM.getIntrinsic(Intrinsic::s390_tbegin_nofloat);\n    return Builder.CreateCall(F, {TDB, Control});\n  }\n  case SystemZ::BI__builtin_tbeginc: {\n    Value *TDB = llvm::ConstantPointerNull::get(Int8PtrTy);\n    Value *Control = llvm::ConstantInt::get(Int32Ty, 0xff08);\n    Function *F = CGM.getIntrinsic(Intrinsic::s390_tbeginc);\n    return Builder.CreateCall(F, {TDB, Control});\n  }\n  case SystemZ::BI__builtin_tabort: {\n    Value *Data = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrinsic::s390_tabort);\n    return Builder.CreateCall(F, Builder.CreateSExt(Data, Int64Ty, \"tabort\"));\n  }\n  case SystemZ::BI__builtin_non_tx_store: {\n    Value *Address = EmitScalarExpr(E->getArg(0));\n    Value *Data = EmitScalarExpr(E->getArg(1));\n    Function *F = CGM.getIntrinsic(Intrinsic::s390_ntstg);\n    return Builder.CreateCall(F, {Data, Address});\n  }\n\n  // Vector builtins.  Note that most vector builtins are mapped automatically\n  // to target-specific LLVM intrinsics.  The ones handled specially here can\n  // be represented via standard LLVM IR, which is preferable to enable common\n  // LLVM optimizations.\n\n  case SystemZ::BI__builtin_s390_vpopctb:\n  case SystemZ::BI__builtin_s390_vpopcth:\n  case SystemZ::BI__builtin_s390_vpopctf:\n  case SystemZ::BI__builtin_s390_vpopctg: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrinsic::ctpop, ResultType);\n    return Builder.CreateCall(F, X);\n  }\n\n  case SystemZ::BI__builtin_s390_vclzb:\n  case SystemZ::BI__builtin_s390_vclzh:\n  case SystemZ::BI__builtin_s390_vclzf:\n  case SystemZ::BI__builtin_s390_vclzg: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Undef = ConstantInt::get(Builder.getInt1Ty(), false);\n    Function *F = CGM.getIntrinsic(Intrinsic::ctlz, ResultType);\n    return Builder.CreateCall(F, {X, Undef});\n  }\n\n  case SystemZ::BI__builtin_s390_vctzb:\n  case SystemZ::BI__builtin_s390_vctzh:\n  case SystemZ::BI__builtin_s390_vctzf:\n  case SystemZ::BI__builtin_s390_vctzg: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Undef = ConstantInt::get(Builder.getInt1Ty(), false);\n    Function *F = CGM.getIntrinsic(Intrinsic::cttz, ResultType);\n    return Builder.CreateCall(F, {X, Undef});\n  }\n\n  case SystemZ::BI__builtin_s390_vfsqsb:\n  case SystemZ::BI__builtin_s390_vfsqdb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    if (Builder.getIsFPConstrained()) {\n      Function *F = CGM.getIntrinsic(Intrinsic::experimental_constrained_sqrt, ResultType);\n      return Builder.CreateConstrainedFPCall(F, { X });\n    } else {\n      Function *F = CGM.getIntrinsic(Intrinsic::sqrt, ResultType);\n      return Builder.CreateCall(F, X);\n    }\n  }\n  case SystemZ::BI__builtin_s390_vfmasb:\n  case SystemZ::BI__builtin_s390_vfmadb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    Value *Z = EmitScalarExpr(E->getArg(2));\n    if (Builder.getIsFPConstrained()) {\n      Function *F = CGM.getIntrinsic(Intrinsic::experimental_constrained_fma, ResultType);\n      return Builder.CreateConstrainedFPCall(F, {X, Y, Z});\n    } else {\n      Function *F = CGM.getIntrinsic(Intrinsic::fma, ResultType);\n      return Builder.CreateCall(F, {X, Y, Z});\n    }\n  }\n  case SystemZ::BI__builtin_s390_vfmssb:\n  case SystemZ::BI__builtin_s390_vfmsdb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    Value *Z = EmitScalarExpr(E->getArg(2));\n    if (Builder.getIsFPConstrained()) {\n      Function *F = CGM.getIntrinsic(Intrinsic::experimental_constrained_fma, ResultType);\n      return Builder.CreateConstrainedFPCall(F, {X, Y, Builder.CreateFNeg(Z, \"neg\")});\n    } else {\n      Function *F = CGM.getIntrinsic(Intrinsic::fma, ResultType);\n      return Builder.CreateCall(F, {X, Y, Builder.CreateFNeg(Z, \"neg\")});\n    }\n  }\n  case SystemZ::BI__builtin_s390_vfnmasb:\n  case SystemZ::BI__builtin_s390_vfnmadb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    Value *Z = EmitScalarExpr(E->getArg(2));\n    if (Builder.getIsFPConstrained()) {\n      Function *F = CGM.getIntrinsic(Intrinsic::experimental_constrained_fma, ResultType);\n      return Builder.CreateFNeg(Builder.CreateConstrainedFPCall(F, {X, Y,  Z}), \"neg\");\n    } else {\n      Function *F = CGM.getIntrinsic(Intrinsic::fma, ResultType);\n      return Builder.CreateFNeg(Builder.CreateCall(F, {X, Y, Z}), \"neg\");\n    }\n  }\n  case SystemZ::BI__builtin_s390_vfnmssb:\n  case SystemZ::BI__builtin_s390_vfnmsdb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    Value *Z = EmitScalarExpr(E->getArg(2));\n    if (Builder.getIsFPConstrained()) {\n      Function *F = CGM.getIntrinsic(Intrinsic::experimental_constrained_fma, ResultType);\n      Value *NegZ = Builder.CreateFNeg(Z, \"sub\");\n      return Builder.CreateFNeg(Builder.CreateConstrainedFPCall(F, {X, Y, NegZ}));\n    } else {\n      Function *F = CGM.getIntrinsic(Intrinsic::fma, ResultType);\n      Value *NegZ = Builder.CreateFNeg(Z, \"neg\");\n      return Builder.CreateFNeg(Builder.CreateCall(F, {X, Y, NegZ}));\n    }\n  }\n  case SystemZ::BI__builtin_s390_vflpsb:\n  case SystemZ::BI__builtin_s390_vflpdb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrinsic::fabs, ResultType);\n    return Builder.CreateCall(F, X);\n  }\n  case SystemZ::BI__builtin_s390_vflnsb:\n  case SystemZ::BI__builtin_s390_vflndb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrinsic::fabs, ResultType);\n    return Builder.CreateFNeg(Builder.CreateCall(F, X), \"neg\");\n  }\n  case SystemZ::BI__builtin_s390_vfisb:\n  case SystemZ::BI__builtin_s390_vfidb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    // Constant-fold the M4 and M5 mask arguments.\n    llvm::APSInt M4 = *E->getArg(1)->getIntegerConstantExpr(getContext());\n    llvm::APSInt M5 = *E->getArg(2)->getIntegerConstantExpr(getContext());\n    // Check whether this instance can be represented via a LLVM standard\n    // intrinsic.  We only support some combinations of M4 and M5.\n    Intrinsic::ID ID = Intrinsic::not_intrinsic;\n    Intrinsic::ID CI;\n    switch (M4.getZExtValue()) {\n    default: break;\n    case 0:  // IEEE-inexact exception allowed\n      switch (M5.getZExtValue()) {\n      default: break;\n      case 0: ID = Intrinsic::rint;\n              CI = Intrinsic::experimental_constrained_rint; break;\n      }\n      break;\n    case 4:  // IEEE-inexact exception suppressed\n      switch (M5.getZExtValue()) {\n      default: break;\n      case 0: ID = Intrinsic::nearbyint;\n              CI = Intrinsic::experimental_constrained_nearbyint; break;\n      case 1: ID = Intrinsic::round;\n              CI = Intrinsic::experimental_constrained_round; break;\n      case 5: ID = Intrinsic::trunc;\n              CI = Intrinsic::experimental_constrained_trunc; break;\n      case 6: ID = Intrinsic::ceil;\n              CI = Intrinsic::experimental_constrained_ceil; break;\n      case 7: ID = Intrinsic::floor;\n              CI = Intrinsic::experimental_constrained_floor; break;\n      }\n      break;\n    }\n    if (ID != Intrinsic::not_intrinsic) {\n      if (Builder.getIsFPConstrained()) {\n        Function *F = CGM.getIntrinsic(CI, ResultType);\n        return Builder.CreateConstrainedFPCall(F, X);\n      } else {\n        Function *F = CGM.getIntrinsic(ID, ResultType);\n        return Builder.CreateCall(F, X);\n      }\n    }\n    switch (BuiltinID) { // FIXME: constrained version?\n      case SystemZ::BI__builtin_s390_vfisb: ID = Intrinsic::s390_vfisb; break;\n      case SystemZ::BI__builtin_s390_vfidb: ID = Intrinsic::s390_vfidb; break;\n      default: llvm_unreachable(\"Unknown BuiltinID\");\n    }\n    Function *F = CGM.getIntrinsic(ID);\n    Value *M4Value = llvm::ConstantInt::get(getLLVMContext(), M4);\n    Value *M5Value = llvm::ConstantInt::get(getLLVMContext(), M5);\n    return Builder.CreateCall(F, {X, M4Value, M5Value});\n  }\n  case SystemZ::BI__builtin_s390_vfmaxsb:\n  case SystemZ::BI__builtin_s390_vfmaxdb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    // Constant-fold the M4 mask argument.\n    llvm::APSInt M4 = *E->getArg(2)->getIntegerConstantExpr(getContext());\n    // Check whether this instance can be represented via a LLVM standard\n    // intrinsic.  We only support some values of M4.\n    Intrinsic::ID ID = Intrinsic::not_intrinsic;\n    Intrinsic::ID CI;\n    switch (M4.getZExtValue()) {\n    default: break;\n    case 4: ID = Intrinsic::maxnum;\n            CI = Intrinsic::experimental_constrained_maxnum; break;\n    }\n    if (ID != Intrinsic::not_intrinsic) {\n      if (Builder.getIsFPConstrained()) {\n        Function *F = CGM.getIntrinsic(CI, ResultType);\n        return Builder.CreateConstrainedFPCall(F, {X, Y});\n      } else {\n        Function *F = CGM.getIntrinsic(ID, ResultType);\n        return Builder.CreateCall(F, {X, Y});\n      }\n    }\n    switch (BuiltinID) {\n      case SystemZ::BI__builtin_s390_vfmaxsb: ID = Intrinsic::s390_vfmaxsb; break;\n      case SystemZ::BI__builtin_s390_vfmaxdb: ID = Intrinsic::s390_vfmaxdb; break;\n      default: llvm_unreachable(\"Unknown BuiltinID\");\n    }\n    Function *F = CGM.getIntrinsic(ID);\n    Value *M4Value = llvm::ConstantInt::get(getLLVMContext(), M4);\n    return Builder.CreateCall(F, {X, Y, M4Value});\n  }\n  case SystemZ::BI__builtin_s390_vfminsb:\n  case SystemZ::BI__builtin_s390_vfmindb: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Value *Y = EmitScalarExpr(E->getArg(1));\n    // Constant-fold the M4 mask argument.\n    llvm::APSInt M4 = *E->getArg(2)->getIntegerConstantExpr(getContext());\n    // Check whether this instance can be represented via a LLVM standard\n    // intrinsic.  We only support some values of M4.\n    Intrinsic::ID ID = Intrinsic::not_intrinsic;\n    Intrinsic::ID CI;\n    switch (M4.getZExtValue()) {\n    default: break;\n    case 4: ID = Intrinsic::minnum;\n            CI = Intrinsic::experimental_constrained_minnum; break;\n    }\n    if (ID != Intrinsic::not_intrinsic) {\n      if (Builder.getIsFPConstrained()) {\n        Function *F = CGM.getIntrinsic(CI, ResultType);\n        return Builder.CreateConstrainedFPCall(F, {X, Y});\n      } else {\n        Function *F = CGM.getIntrinsic(ID, ResultType);\n        return Builder.CreateCall(F, {X, Y});\n      }\n    }\n    switch (BuiltinID) {\n      case SystemZ::BI__builtin_s390_vfminsb: ID = Intrinsic::s390_vfminsb; break;\n      case SystemZ::BI__builtin_s390_vfmindb: ID = Intrinsic::s390_vfmindb; break;\n      default: llvm_unreachable(\"Unknown BuiltinID\");\n    }\n    Function *F = CGM.getIntrinsic(ID);\n    Value *M4Value = llvm::ConstantInt::get(getLLVMContext(), M4);\n    return Builder.CreateCall(F, {X, Y, M4Value});\n  }\n\n  case SystemZ::BI__builtin_s390_vlbrh:\n  case SystemZ::BI__builtin_s390_vlbrf:\n  case SystemZ::BI__builtin_s390_vlbrg: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *X = EmitScalarExpr(E->getArg(0));\n    Function *F = CGM.getIntrinsic(Intrinsic::bswap, ResultType);\n    return Builder.CreateCall(F, X);\n  }\n\n  // Vector intrinsics that output the post-instruction CC value.\n\n#define INTRINSIC_WITH_CC(NAME) \\\n    case SystemZ::BI__builtin_##NAME: \\\n      return EmitSystemZIntrinsicWithCC(*this, Intrinsic::NAME, E)\n\n  INTRINSIC_WITH_CC(s390_vpkshs);\n  INTRINSIC_WITH_CC(s390_vpksfs);\n  INTRINSIC_WITH_CC(s390_vpksgs);\n\n  INTRINSIC_WITH_CC(s390_vpklshs);\n  INTRINSIC_WITH_CC(s390_vpklsfs);\n  INTRINSIC_WITH_CC(s390_vpklsgs);\n\n  INTRINSIC_WITH_CC(s390_vceqbs);\n  INTRINSIC_WITH_CC(s390_vceqhs);\n  INTRINSIC_WITH_CC(s390_vceqfs);\n  INTRINSIC_WITH_CC(s390_vceqgs);\n\n  INTRINSIC_WITH_CC(s390_vchbs);\n  INTRINSIC_WITH_CC(s390_vchhs);\n  INTRINSIC_WITH_CC(s390_vchfs);\n  INTRINSIC_WITH_CC(s390_vchgs);\n\n  INTRINSIC_WITH_CC(s390_vchlbs);\n  INTRINSIC_WITH_CC(s390_vchlhs);\n  INTRINSIC_WITH_CC(s390_vchlfs);\n  INTRINSIC_WITH_CC(s390_vchlgs);\n\n  INTRINSIC_WITH_CC(s390_vfaebs);\n  INTRINSIC_WITH_CC(s390_vfaehs);\n  INTRINSIC_WITH_CC(s390_vfaefs);\n\n  INTRINSIC_WITH_CC(s390_vfaezbs);\n  INTRINSIC_WITH_CC(s390_vfaezhs);\n  INTRINSIC_WITH_CC(s390_vfaezfs);\n\n  INTRINSIC_WITH_CC(s390_vfeebs);\n  INTRINSIC_WITH_CC(s390_vfeehs);\n  INTRINSIC_WITH_CC(s390_vfeefs);\n\n  INTRINSIC_WITH_CC(s390_vfeezbs);\n  INTRINSIC_WITH_CC(s390_vfeezhs);\n  INTRINSIC_WITH_CC(s390_vfeezfs);\n\n  INTRINSIC_WITH_CC(s390_vfenebs);\n  INTRINSIC_WITH_CC(s390_vfenehs);\n  INTRINSIC_WITH_CC(s390_vfenefs);\n\n  INTRINSIC_WITH_CC(s390_vfenezbs);\n  INTRINSIC_WITH_CC(s390_vfenezhs);\n  INTRINSIC_WITH_CC(s390_vfenezfs);\n\n  INTRINSIC_WITH_CC(s390_vistrbs);\n  INTRINSIC_WITH_CC(s390_vistrhs);\n  INTRINSIC_WITH_CC(s390_vistrfs);\n\n  INTRINSIC_WITH_CC(s390_vstrcbs);\n  INTRINSIC_WITH_CC(s390_vstrchs);\n  INTRINSIC_WITH_CC(s390_vstrcfs);\n\n  INTRINSIC_WITH_CC(s390_vstrczbs);\n  INTRINSIC_WITH_CC(s390_vstrczhs);\n  INTRINSIC_WITH_CC(s390_vstrczfs);\n\n  INTRINSIC_WITH_CC(s390_vfcesbs);\n  INTRINSIC_WITH_CC(s390_vfcedbs);\n  INTRINSIC_WITH_CC(s390_vfchsbs);\n  INTRINSIC_WITH_CC(s390_vfchdbs);\n  INTRINSIC_WITH_CC(s390_vfchesbs);\n  INTRINSIC_WITH_CC(s390_vfchedbs);\n\n  INTRINSIC_WITH_CC(s390_vftcisb);\n  INTRINSIC_WITH_CC(s390_vftcidb);\n\n  INTRINSIC_WITH_CC(s390_vstrsb);\n  INTRINSIC_WITH_CC(s390_vstrsh);\n  INTRINSIC_WITH_CC(s390_vstrsf);\n\n  INTRINSIC_WITH_CC(s390_vstrszb);\n  INTRINSIC_WITH_CC(s390_vstrszh);\n  INTRINSIC_WITH_CC(s390_vstrszf);\n\n#undef INTRINSIC_WITH_CC\n\n  default:\n    return nullptr;\n  }\n}\n\nnamespace {\n// Helper classes for mapping MMA builtins to particular LLVM intrinsic variant.\nstruct NVPTXMmaLdstInfo {\n  unsigned NumResults;  // Number of elements to load/store\n  // Intrinsic IDs for row/col variants. 0 if particular layout is unsupported.\n  unsigned IID_col;\n  unsigned IID_row;\n};\n\n#define MMA_INTR(geom_op_type, layout) \\\n  Intrinsic::nvvm_wmma_##geom_op_type##_##layout##_stride\n#define MMA_LDST(n, geom_op_type)                                              \\\n  { n, MMA_INTR(geom_op_type, col), MMA_INTR(geom_op_type, row) }\n\nstatic NVPTXMmaLdstInfo getNVPTXMmaLdstInfo(unsigned BuiltinID) {\n  switch (BuiltinID) {\n  // FP MMA loads\n  case NVPTX::BI__hmma_m16n16k16_ld_a:\n    return MMA_LDST(8, m16n16k16_load_a_f16);\n  case NVPTX::BI__hmma_m16n16k16_ld_b:\n    return MMA_LDST(8, m16n16k16_load_b_f16);\n  case NVPTX::BI__hmma_m16n16k16_ld_c_f16:\n    return MMA_LDST(4, m16n16k16_load_c_f16);\n  case NVPTX::BI__hmma_m16n16k16_ld_c_f32:\n    return MMA_LDST(8, m16n16k16_load_c_f32);\n  case NVPTX::BI__hmma_m32n8k16_ld_a:\n    return MMA_LDST(8, m32n8k16_load_a_f16);\n  case NVPTX::BI__hmma_m32n8k16_ld_b:\n    return MMA_LDST(8, m32n8k16_load_b_f16);\n  case NVPTX::BI__hmma_m32n8k16_ld_c_f16:\n    return MMA_LDST(4, m32n8k16_load_c_f16);\n  case NVPTX::BI__hmma_m32n8k16_ld_c_f32:\n    return MMA_LDST(8, m32n8k16_load_c_f32);\n  case NVPTX::BI__hmma_m8n32k16_ld_a:\n    return MMA_LDST(8, m8n32k16_load_a_f16);\n  case NVPTX::BI__hmma_m8n32k16_ld_b:\n    return MMA_LDST(8, m8n32k16_load_b_f16);\n  case NVPTX::BI__hmma_m8n32k16_ld_c_f16:\n    return MMA_LDST(4, m8n32k16_load_c_f16);\n  case NVPTX::BI__hmma_m8n32k16_ld_c_f32:\n    return MMA_LDST(8, m8n32k16_load_c_f32);\n\n  // Integer MMA loads\n  case NVPTX::BI__imma_m16n16k16_ld_a_s8:\n    return MMA_LDST(2, m16n16k16_load_a_s8);\n  case NVPTX::BI__imma_m16n16k16_ld_a_u8:\n    return MMA_LDST(2, m16n16k16_load_a_u8);\n  case NVPTX::BI__imma_m16n16k16_ld_b_s8:\n    return MMA_LDST(2, m16n16k16_load_b_s8);\n  case NVPTX::BI__imma_m16n16k16_ld_b_u8:\n    return MMA_LDST(2, m16n16k16_load_b_u8);\n  case NVPTX::BI__imma_m16n16k16_ld_c:\n    return MMA_LDST(8, m16n16k16_load_c_s32);\n  case NVPTX::BI__imma_m32n8k16_ld_a_s8:\n    return MMA_LDST(4, m32n8k16_load_a_s8);\n  case NVPTX::BI__imma_m32n8k16_ld_a_u8:\n    return MMA_LDST(4, m32n8k16_load_a_u8);\n  case NVPTX::BI__imma_m32n8k16_ld_b_s8:\n    return MMA_LDST(1, m32n8k16_load_b_s8);\n  case NVPTX::BI__imma_m32n8k16_ld_b_u8:\n    return MMA_LDST(1, m32n8k16_load_b_u8);\n  case NVPTX::BI__imma_m32n8k16_ld_c:\n    return MMA_LDST(8, m32n8k16_load_c_s32);\n  case NVPTX::BI__imma_m8n32k16_ld_a_s8:\n    return MMA_LDST(1, m8n32k16_load_a_s8);\n  case NVPTX::BI__imma_m8n32k16_ld_a_u8:\n    return MMA_LDST(1, m8n32k16_load_a_u8);\n  case NVPTX::BI__imma_m8n32k16_ld_b_s8:\n    return MMA_LDST(4, m8n32k16_load_b_s8);\n  case NVPTX::BI__imma_m8n32k16_ld_b_u8:\n    return MMA_LDST(4, m8n32k16_load_b_u8);\n  case NVPTX::BI__imma_m8n32k16_ld_c:\n    return MMA_LDST(8, m8n32k16_load_c_s32);\n\n  // Sub-integer MMA loads.\n  // Only row/col layout is supported by A/B fragments.\n  case NVPTX::BI__imma_m8n8k32_ld_a_s4:\n    return {1, 0, MMA_INTR(m8n8k32_load_a_s4, row)};\n  case NVPTX::BI__imma_m8n8k32_ld_a_u4:\n    return {1, 0, MMA_INTR(m8n8k32_load_a_u4, row)};\n  case NVPTX::BI__imma_m8n8k32_ld_b_s4:\n    return {1, MMA_INTR(m8n8k32_load_b_s4, col), 0};\n  case NVPTX::BI__imma_m8n8k32_ld_b_u4:\n    return {1, MMA_INTR(m8n8k32_load_b_u4, col), 0};\n  case NVPTX::BI__imma_m8n8k32_ld_c:\n    return MMA_LDST(2, m8n8k32_load_c_s32);\n  case NVPTX::BI__bmma_m8n8k128_ld_a_b1:\n    return {1, 0, MMA_INTR(m8n8k128_load_a_b1, row)};\n  case NVPTX::BI__bmma_m8n8k128_ld_b_b1:\n    return {1, MMA_INTR(m8n8k128_load_b_b1, col), 0};\n  case NVPTX::BI__bmma_m8n8k128_ld_c:\n    return MMA_LDST(2, m8n8k128_load_c_s32);\n\n  // NOTE: We need to follow inconsitent naming scheme used by NVCC.  Unlike\n  // PTX and LLVM IR where stores always use fragment D, NVCC builtins always\n  // use fragment C for both loads and stores.\n  // FP MMA stores.\n  case NVPTX::BI__hmma_m16n16k16_st_c_f16:\n    return MMA_LDST(4, m16n16k16_store_d_f16);\n  case NVPTX::BI__hmma_m16n16k16_st_c_f32:\n    return MMA_LDST(8, m16n16k16_store_d_f32);\n  case NVPTX::BI__hmma_m32n8k16_st_c_f16:\n    return MMA_LDST(4, m32n8k16_store_d_f16);\n  case NVPTX::BI__hmma_m32n8k16_st_c_f32:\n    return MMA_LDST(8, m32n8k16_store_d_f32);\n  case NVPTX::BI__hmma_m8n32k16_st_c_f16:\n    return MMA_LDST(4, m8n32k16_store_d_f16);\n  case NVPTX::BI__hmma_m8n32k16_st_c_f32:\n    return MMA_LDST(8, m8n32k16_store_d_f32);\n\n  // Integer and sub-integer MMA stores.\n  // Another naming quirk. Unlike other MMA builtins that use PTX types in the\n  // name, integer loads/stores use LLVM's i32.\n  case NVPTX::BI__imma_m16n16k16_st_c_i32:\n    return MMA_LDST(8, m16n16k16_store_d_s32);\n  case NVPTX::BI__imma_m32n8k16_st_c_i32:\n    return MMA_LDST(8, m32n8k16_store_d_s32);\n  case NVPTX::BI__imma_m8n32k16_st_c_i32:\n    return MMA_LDST(8, m8n32k16_store_d_s32);\n  case NVPTX::BI__imma_m8n8k32_st_c_i32:\n    return MMA_LDST(2, m8n8k32_store_d_s32);\n  case NVPTX::BI__bmma_m8n8k128_st_c_i32:\n    return MMA_LDST(2, m8n8k128_store_d_s32);\n\n  default:\n    llvm_unreachable(\"Unknown MMA builtin\");\n  }\n}\n#undef MMA_LDST\n#undef MMA_INTR\n\n\nstruct NVPTXMmaInfo {\n  unsigned NumEltsA;\n  unsigned NumEltsB;\n  unsigned NumEltsC;\n  unsigned NumEltsD;\n  std::array<unsigned, 8> Variants;\n\n  unsigned getMMAIntrinsic(int Layout, bool Satf) {\n    unsigned Index = Layout * 2 + Satf;\n    if (Index >= Variants.size())\n      return 0;\n    return Variants[Index];\n  }\n};\n\n  // Returns an intrinsic that matches Layout and Satf for valid combinations of\n  // Layout and Satf, 0 otherwise.\nstatic NVPTXMmaInfo getNVPTXMmaInfo(unsigned BuiltinID) {\n  // clang-format off\n#define MMA_VARIANTS(geom, type) {{                                 \\\n      Intrinsic::nvvm_wmma_##geom##_mma_row_row_##type,             \\\n      Intrinsic::nvvm_wmma_##geom##_mma_row_row_##type##_satfinite, \\\n      Intrinsic::nvvm_wmma_##geom##_mma_row_col_##type,             \\\n      Intrinsic::nvvm_wmma_##geom##_mma_row_col_##type##_satfinite, \\\n      Intrinsic::nvvm_wmma_##geom##_mma_col_row_##type,             \\\n      Intrinsic::nvvm_wmma_##geom##_mma_col_row_##type##_satfinite, \\\n      Intrinsic::nvvm_wmma_##geom##_mma_col_col_##type,             \\\n      Intrinsic::nvvm_wmma_##geom##_mma_col_col_##type##_satfinite  \\\n    }}\n// Sub-integer MMA only supports row.col layout.\n#define MMA_VARIANTS_I4(geom, type) {{ \\\n      0, \\\n      0, \\\n      Intrinsic::nvvm_wmma_##geom##_mma_row_col_##type,             \\\n      Intrinsic::nvvm_wmma_##geom##_mma_row_col_##type##_satfinite, \\\n      0, \\\n      0, \\\n      0, \\\n      0  \\\n    }}\n// b1 MMA does not support .satfinite.\n#define MMA_VARIANTS_B1(geom, type) {{ \\\n      0, \\\n      0, \\\n      Intrinsic::nvvm_wmma_##geom##_mma_row_col_##type,             \\\n      0, \\\n      0, \\\n      0, \\\n      0, \\\n      0  \\\n    }}\n    // clang-format on\n    switch (BuiltinID) {\n    // FP MMA\n    // Note that 'type' argument of MMA_VARIANT uses D_C notation, while\n    // NumEltsN of return value are ordered as A,B,C,D.\n    case NVPTX::BI__hmma_m16n16k16_mma_f16f16:\n      return {8, 8, 4, 4, MMA_VARIANTS(m16n16k16, f16_f16)};\n    case NVPTX::BI__hmma_m16n16k16_mma_f32f16:\n      return {8, 8, 4, 8, MMA_VARIANTS(m16n16k16, f32_f16)};\n    case NVPTX::BI__hmma_m16n16k16_mma_f16f32:\n      return {8, 8, 8, 4, MMA_VARIANTS(m16n16k16, f16_f32)};\n    case NVPTX::BI__hmma_m16n16k16_mma_f32f32:\n      return {8, 8, 8, 8, MMA_VARIANTS(m16n16k16, f32_f32)};\n    case NVPTX::BI__hmma_m32n8k16_mma_f16f16:\n      return {8, 8, 4, 4, MMA_VARIANTS(m32n8k16, f16_f16)};\n    case NVPTX::BI__hmma_m32n8k16_mma_f32f16:\n      return {8, 8, 4, 8, MMA_VARIANTS(m32n8k16, f32_f16)};\n    case NVPTX::BI__hmma_m32n8k16_mma_f16f32:\n      return {8, 8, 8, 4, MMA_VARIANTS(m32n8k16, f16_f32)};\n    case NVPTX::BI__hmma_m32n8k16_mma_f32f32:\n      return {8, 8, 8, 8, MMA_VARIANTS(m32n8k16, f32_f32)};\n    case NVPTX::BI__hmma_m8n32k16_mma_f16f16:\n      return {8, 8, 4, 4, MMA_VARIANTS(m8n32k16, f16_f16)};\n    case NVPTX::BI__hmma_m8n32k16_mma_f32f16:\n      return {8, 8, 4, 8, MMA_VARIANTS(m8n32k16, f32_f16)};\n    case NVPTX::BI__hmma_m8n32k16_mma_f16f32:\n      return {8, 8, 8, 4, MMA_VARIANTS(m8n32k16, f16_f32)};\n    case NVPTX::BI__hmma_m8n32k16_mma_f32f32:\n      return {8, 8, 8, 8, MMA_VARIANTS(m8n32k16, f32_f32)};\n\n    // Integer MMA\n    case NVPTX::BI__imma_m16n16k16_mma_s8:\n      return {2, 2, 8, 8, MMA_VARIANTS(m16n16k16, s8)};\n    case NVPTX::BI__imma_m16n16k16_mma_u8:\n      return {2, 2, 8, 8, MMA_VARIANTS(m16n16k16, u8)};\n    case NVPTX::BI__imma_m32n8k16_mma_s8:\n      return {4, 1, 8, 8, MMA_VARIANTS(m32n8k16, s8)};\n    case NVPTX::BI__imma_m32n8k16_mma_u8:\n      return {4, 1, 8, 8, MMA_VARIANTS(m32n8k16, u8)};\n    case NVPTX::BI__imma_m8n32k16_mma_s8:\n      return {1, 4, 8, 8, MMA_VARIANTS(m8n32k16, s8)};\n    case NVPTX::BI__imma_m8n32k16_mma_u8:\n      return {1, 4, 8, 8, MMA_VARIANTS(m8n32k16, u8)};\n\n    // Sub-integer MMA\n    case NVPTX::BI__imma_m8n8k32_mma_s4:\n      return {1, 1, 2, 2, MMA_VARIANTS_I4(m8n8k32, s4)};\n    case NVPTX::BI__imma_m8n8k32_mma_u4:\n      return {1, 1, 2, 2, MMA_VARIANTS_I4(m8n8k32, u4)};\n    case NVPTX::BI__bmma_m8n8k128_mma_xor_popc_b1:\n      return {1, 1, 2, 2, MMA_VARIANTS_B1(m8n8k128, b1)};\n    default:\n      llvm_unreachable(\"Unexpected builtin ID.\");\n    }\n#undef MMA_VARIANTS\n#undef MMA_VARIANTS_I4\n#undef MMA_VARIANTS_B1\n}\n\n} // namespace\n\nValue *\nCodeGenFunction::EmitNVPTXBuiltinExpr(unsigned BuiltinID, const CallExpr *E) {\n  auto MakeLdg = [&](unsigned IntrinsicID) {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    clang::CharUnits Align =\n        CGM.getNaturalPointeeTypeAlignment(E->getArg(0)->getType());\n    return Builder.CreateCall(\n        CGM.getIntrinsic(IntrinsicID, {Ptr->getType()->getPointerElementType(),\n                                       Ptr->getType()}),\n        {Ptr, ConstantInt::get(Builder.getInt32Ty(), Align.getQuantity())});\n  };\n  auto MakeScopedAtomic = [&](unsigned IntrinsicID) {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(\n        CGM.getIntrinsic(IntrinsicID, {Ptr->getType()->getPointerElementType(),\n                                       Ptr->getType()}),\n        {Ptr, EmitScalarExpr(E->getArg(1))});\n  };\n  switch (BuiltinID) {\n  case NVPTX::BI__nvvm_atom_add_gen_i:\n  case NVPTX::BI__nvvm_atom_add_gen_l:\n  case NVPTX::BI__nvvm_atom_add_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::Add, E);\n\n  case NVPTX::BI__nvvm_atom_sub_gen_i:\n  case NVPTX::BI__nvvm_atom_sub_gen_l:\n  case NVPTX::BI__nvvm_atom_sub_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::Sub, E);\n\n  case NVPTX::BI__nvvm_atom_and_gen_i:\n  case NVPTX::BI__nvvm_atom_and_gen_l:\n  case NVPTX::BI__nvvm_atom_and_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::And, E);\n\n  case NVPTX::BI__nvvm_atom_or_gen_i:\n  case NVPTX::BI__nvvm_atom_or_gen_l:\n  case NVPTX::BI__nvvm_atom_or_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::Or, E);\n\n  case NVPTX::BI__nvvm_atom_xor_gen_i:\n  case NVPTX::BI__nvvm_atom_xor_gen_l:\n  case NVPTX::BI__nvvm_atom_xor_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::Xor, E);\n\n  case NVPTX::BI__nvvm_atom_xchg_gen_i:\n  case NVPTX::BI__nvvm_atom_xchg_gen_l:\n  case NVPTX::BI__nvvm_atom_xchg_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::Xchg, E);\n\n  case NVPTX::BI__nvvm_atom_max_gen_i:\n  case NVPTX::BI__nvvm_atom_max_gen_l:\n  case NVPTX::BI__nvvm_atom_max_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::Max, E);\n\n  case NVPTX::BI__nvvm_atom_max_gen_ui:\n  case NVPTX::BI__nvvm_atom_max_gen_ul:\n  case NVPTX::BI__nvvm_atom_max_gen_ull:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::UMax, E);\n\n  case NVPTX::BI__nvvm_atom_min_gen_i:\n  case NVPTX::BI__nvvm_atom_min_gen_l:\n  case NVPTX::BI__nvvm_atom_min_gen_ll:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::Min, E);\n\n  case NVPTX::BI__nvvm_atom_min_gen_ui:\n  case NVPTX::BI__nvvm_atom_min_gen_ul:\n  case NVPTX::BI__nvvm_atom_min_gen_ull:\n    return MakeBinaryAtomicValue(*this, llvm::AtomicRMWInst::UMin, E);\n\n  case NVPTX::BI__nvvm_atom_cas_gen_i:\n  case NVPTX::BI__nvvm_atom_cas_gen_l:\n  case NVPTX::BI__nvvm_atom_cas_gen_ll:\n    // __nvvm_atom_cas_gen_* should return the old value rather than the\n    // success flag.\n    return MakeAtomicCmpXchgValue(*this, E, /*ReturnBool=*/false);\n\n  case NVPTX::BI__nvvm_atom_add_gen_f:\n  case NVPTX::BI__nvvm_atom_add_gen_d: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Value *Val = EmitScalarExpr(E->getArg(1));\n    return Builder.CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, Ptr, Val,\n                                   AtomicOrdering::SequentiallyConsistent);\n  }\n\n  case NVPTX::BI__nvvm_atom_inc_gen_ui: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Value *Val = EmitScalarExpr(E->getArg(1));\n    Function *FnALI32 =\n        CGM.getIntrinsic(Intrinsic::nvvm_atomic_load_inc_32, Ptr->getType());\n    return Builder.CreateCall(FnALI32, {Ptr, Val});\n  }\n\n  case NVPTX::BI__nvvm_atom_dec_gen_ui: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Value *Val = EmitScalarExpr(E->getArg(1));\n    Function *FnALD32 =\n        CGM.getIntrinsic(Intrinsic::nvvm_atomic_load_dec_32, Ptr->getType());\n    return Builder.CreateCall(FnALD32, {Ptr, Val});\n  }\n\n  case NVPTX::BI__nvvm_ldg_c:\n  case NVPTX::BI__nvvm_ldg_c2:\n  case NVPTX::BI__nvvm_ldg_c4:\n  case NVPTX::BI__nvvm_ldg_s:\n  case NVPTX::BI__nvvm_ldg_s2:\n  case NVPTX::BI__nvvm_ldg_s4:\n  case NVPTX::BI__nvvm_ldg_i:\n  case NVPTX::BI__nvvm_ldg_i2:\n  case NVPTX::BI__nvvm_ldg_i4:\n  case NVPTX::BI__nvvm_ldg_l:\n  case NVPTX::BI__nvvm_ldg_ll:\n  case NVPTX::BI__nvvm_ldg_ll2:\n  case NVPTX::BI__nvvm_ldg_uc:\n  case NVPTX::BI__nvvm_ldg_uc2:\n  case NVPTX::BI__nvvm_ldg_uc4:\n  case NVPTX::BI__nvvm_ldg_us:\n  case NVPTX::BI__nvvm_ldg_us2:\n  case NVPTX::BI__nvvm_ldg_us4:\n  case NVPTX::BI__nvvm_ldg_ui:\n  case NVPTX::BI__nvvm_ldg_ui2:\n  case NVPTX::BI__nvvm_ldg_ui4:\n  case NVPTX::BI__nvvm_ldg_ul:\n  case NVPTX::BI__nvvm_ldg_ull:\n  case NVPTX::BI__nvvm_ldg_ull2:\n    // PTX Interoperability section 2.2: \"For a vector with an even number of\n    // elements, its alignment is set to number of elements times the alignment\n    // of its member: n*alignof(t).\"\n    return MakeLdg(Intrinsic::nvvm_ldg_global_i);\n  case NVPTX::BI__nvvm_ldg_f:\n  case NVPTX::BI__nvvm_ldg_f2:\n  case NVPTX::BI__nvvm_ldg_f4:\n  case NVPTX::BI__nvvm_ldg_d:\n  case NVPTX::BI__nvvm_ldg_d2:\n    return MakeLdg(Intrinsic::nvvm_ldg_global_f);\n\n  case NVPTX::BI__nvvm_atom_cta_add_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_add_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_add_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_add_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_add_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_add_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_add_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_add_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_add_gen_f:\n  case NVPTX::BI__nvvm_atom_cta_add_gen_d:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_add_gen_f_cta);\n  case NVPTX::BI__nvvm_atom_sys_add_gen_f:\n  case NVPTX::BI__nvvm_atom_sys_add_gen_d:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_add_gen_f_sys);\n  case NVPTX::BI__nvvm_atom_cta_xchg_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_xchg_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_xchg_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_exch_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_xchg_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_xchg_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_xchg_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_exch_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_max_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_max_gen_ui:\n  case NVPTX::BI__nvvm_atom_cta_max_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_max_gen_ul:\n  case NVPTX::BI__nvvm_atom_cta_max_gen_ll:\n  case NVPTX::BI__nvvm_atom_cta_max_gen_ull:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_max_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_max_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_max_gen_ui:\n  case NVPTX::BI__nvvm_atom_sys_max_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_max_gen_ul:\n  case NVPTX::BI__nvvm_atom_sys_max_gen_ll:\n  case NVPTX::BI__nvvm_atom_sys_max_gen_ull:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_max_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_min_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_min_gen_ui:\n  case NVPTX::BI__nvvm_atom_cta_min_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_min_gen_ul:\n  case NVPTX::BI__nvvm_atom_cta_min_gen_ll:\n  case NVPTX::BI__nvvm_atom_cta_min_gen_ull:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_min_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_min_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_min_gen_ui:\n  case NVPTX::BI__nvvm_atom_sys_min_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_min_gen_ul:\n  case NVPTX::BI__nvvm_atom_sys_min_gen_ll:\n  case NVPTX::BI__nvvm_atom_sys_min_gen_ull:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_min_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_inc_gen_ui:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_inc_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_cta_dec_gen_ui:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_dec_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_inc_gen_ui:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_inc_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_sys_dec_gen_ui:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_dec_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_and_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_and_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_and_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_and_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_and_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_and_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_and_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_and_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_or_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_or_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_or_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_or_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_or_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_or_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_or_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_or_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_xor_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_xor_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_xor_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_xor_gen_i_cta);\n  case NVPTX::BI__nvvm_atom_sys_xor_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_xor_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_xor_gen_ll:\n    return MakeScopedAtomic(Intrinsic::nvvm_atomic_xor_gen_i_sys);\n  case NVPTX::BI__nvvm_atom_cta_cas_gen_i:\n  case NVPTX::BI__nvvm_atom_cta_cas_gen_l:\n  case NVPTX::BI__nvvm_atom_cta_cas_gen_ll: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(\n        CGM.getIntrinsic(\n            Intrinsic::nvvm_atomic_cas_gen_i_cta,\n            {Ptr->getType()->getPointerElementType(), Ptr->getType()}),\n        {Ptr, EmitScalarExpr(E->getArg(1)), EmitScalarExpr(E->getArg(2))});\n  }\n  case NVPTX::BI__nvvm_atom_sys_cas_gen_i:\n  case NVPTX::BI__nvvm_atom_sys_cas_gen_l:\n  case NVPTX::BI__nvvm_atom_sys_cas_gen_ll: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    return Builder.CreateCall(\n        CGM.getIntrinsic(\n            Intrinsic::nvvm_atomic_cas_gen_i_sys,\n            {Ptr->getType()->getPointerElementType(), Ptr->getType()}),\n        {Ptr, EmitScalarExpr(E->getArg(1)), EmitScalarExpr(E->getArg(2))});\n  }\n  case NVPTX::BI__nvvm_match_all_sync_i32p:\n  case NVPTX::BI__nvvm_match_all_sync_i64p: {\n    Value *Mask = EmitScalarExpr(E->getArg(0));\n    Value *Val = EmitScalarExpr(E->getArg(1));\n    Address PredOutPtr = EmitPointerWithAlignment(E->getArg(2));\n    Value *ResultPair = Builder.CreateCall(\n        CGM.getIntrinsic(BuiltinID == NVPTX::BI__nvvm_match_all_sync_i32p\n                             ? Intrinsic::nvvm_match_all_sync_i32p\n                             : Intrinsic::nvvm_match_all_sync_i64p),\n        {Mask, Val});\n    Value *Pred = Builder.CreateZExt(Builder.CreateExtractValue(ResultPair, 1),\n                                     PredOutPtr.getElementType());\n    Builder.CreateStore(Pred, PredOutPtr);\n    return Builder.CreateExtractValue(ResultPair, 0);\n  }\n\n  // FP MMA loads\n  case NVPTX::BI__hmma_m16n16k16_ld_a:\n  case NVPTX::BI__hmma_m16n16k16_ld_b:\n  case NVPTX::BI__hmma_m16n16k16_ld_c_f16:\n  case NVPTX::BI__hmma_m16n16k16_ld_c_f32:\n  case NVPTX::BI__hmma_m32n8k16_ld_a:\n  case NVPTX::BI__hmma_m32n8k16_ld_b:\n  case NVPTX::BI__hmma_m32n8k16_ld_c_f16:\n  case NVPTX::BI__hmma_m32n8k16_ld_c_f32:\n  case NVPTX::BI__hmma_m8n32k16_ld_a:\n  case NVPTX::BI__hmma_m8n32k16_ld_b:\n  case NVPTX::BI__hmma_m8n32k16_ld_c_f16:\n  case NVPTX::BI__hmma_m8n32k16_ld_c_f32:\n  // Integer MMA loads.\n  case NVPTX::BI__imma_m16n16k16_ld_a_s8:\n  case NVPTX::BI__imma_m16n16k16_ld_a_u8:\n  case NVPTX::BI__imma_m16n16k16_ld_b_s8:\n  case NVPTX::BI__imma_m16n16k16_ld_b_u8:\n  case NVPTX::BI__imma_m16n16k16_ld_c:\n  case NVPTX::BI__imma_m32n8k16_ld_a_s8:\n  case NVPTX::BI__imma_m32n8k16_ld_a_u8:\n  case NVPTX::BI__imma_m32n8k16_ld_b_s8:\n  case NVPTX::BI__imma_m32n8k16_ld_b_u8:\n  case NVPTX::BI__imma_m32n8k16_ld_c:\n  case NVPTX::BI__imma_m8n32k16_ld_a_s8:\n  case NVPTX::BI__imma_m8n32k16_ld_a_u8:\n  case NVPTX::BI__imma_m8n32k16_ld_b_s8:\n  case NVPTX::BI__imma_m8n32k16_ld_b_u8:\n  case NVPTX::BI__imma_m8n32k16_ld_c:\n  // Sub-integer MMA loads.\n  case NVPTX::BI__imma_m8n8k32_ld_a_s4:\n  case NVPTX::BI__imma_m8n8k32_ld_a_u4:\n  case NVPTX::BI__imma_m8n8k32_ld_b_s4:\n  case NVPTX::BI__imma_m8n8k32_ld_b_u4:\n  case NVPTX::BI__imma_m8n8k32_ld_c:\n  case NVPTX::BI__bmma_m8n8k128_ld_a_b1:\n  case NVPTX::BI__bmma_m8n8k128_ld_b_b1:\n  case NVPTX::BI__bmma_m8n8k128_ld_c:\n  {\n    Address Dst = EmitPointerWithAlignment(E->getArg(0));\n    Value *Src = EmitScalarExpr(E->getArg(1));\n    Value *Ldm = EmitScalarExpr(E->getArg(2));\n    Optional<llvm::APSInt> isColMajorArg =\n        E->getArg(3)->getIntegerConstantExpr(getContext());\n    if (!isColMajorArg)\n      return nullptr;\n    bool isColMajor = isColMajorArg->getSExtValue();\n    NVPTXMmaLdstInfo II = getNVPTXMmaLdstInfo(BuiltinID);\n    unsigned IID = isColMajor ? II.IID_col : II.IID_row;\n    if (IID == 0)\n      return nullptr;\n\n    Value *Result =\n        Builder.CreateCall(CGM.getIntrinsic(IID, Src->getType()), {Src, Ldm});\n\n    // Save returned values.\n    assert(II.NumResults);\n    if (II.NumResults == 1) {\n      Builder.CreateAlignedStore(Result, Dst.getPointer(),\n                                 CharUnits::fromQuantity(4));\n    } else {\n      for (unsigned i = 0; i < II.NumResults; ++i) {\n        Builder.CreateAlignedStore(\n            Builder.CreateBitCast(Builder.CreateExtractValue(Result, i),\n                                  Dst.getElementType()),\n            Builder.CreateGEP(Dst.getPointer(),\n                              llvm::ConstantInt::get(IntTy, i)),\n            CharUnits::fromQuantity(4));\n      }\n    }\n    return Result;\n  }\n\n  case NVPTX::BI__hmma_m16n16k16_st_c_f16:\n  case NVPTX::BI__hmma_m16n16k16_st_c_f32:\n  case NVPTX::BI__hmma_m32n8k16_st_c_f16:\n  case NVPTX::BI__hmma_m32n8k16_st_c_f32:\n  case NVPTX::BI__hmma_m8n32k16_st_c_f16:\n  case NVPTX::BI__hmma_m8n32k16_st_c_f32:\n  case NVPTX::BI__imma_m16n16k16_st_c_i32:\n  case NVPTX::BI__imma_m32n8k16_st_c_i32:\n  case NVPTX::BI__imma_m8n32k16_st_c_i32:\n  case NVPTX::BI__imma_m8n8k32_st_c_i32:\n  case NVPTX::BI__bmma_m8n8k128_st_c_i32: {\n    Value *Dst = EmitScalarExpr(E->getArg(0));\n    Address Src = EmitPointerWithAlignment(E->getArg(1));\n    Value *Ldm = EmitScalarExpr(E->getArg(2));\n    Optional<llvm::APSInt> isColMajorArg =\n        E->getArg(3)->getIntegerConstantExpr(getContext());\n    if (!isColMajorArg)\n      return nullptr;\n    bool isColMajor = isColMajorArg->getSExtValue();\n    NVPTXMmaLdstInfo II = getNVPTXMmaLdstInfo(BuiltinID);\n    unsigned IID = isColMajor ? II.IID_col : II.IID_row;\n    if (IID == 0)\n      return nullptr;\n    Function *Intrinsic =\n        CGM.getIntrinsic(IID, Dst->getType());\n    llvm::Type *ParamType = Intrinsic->getFunctionType()->getParamType(1);\n    SmallVector<Value *, 10> Values = {Dst};\n    for (unsigned i = 0; i < II.NumResults; ++i) {\n      Value *V = Builder.CreateAlignedLoad(\n          Builder.CreateGEP(Src.getPointer(), llvm::ConstantInt::get(IntTy, i)),\n          CharUnits::fromQuantity(4));\n      Values.push_back(Builder.CreateBitCast(V, ParamType));\n    }\n    Values.push_back(Ldm);\n    Value *Result = Builder.CreateCall(Intrinsic, Values);\n    return Result;\n  }\n\n  // BI__hmma_m16n16k16_mma_<Dtype><CType>(d, a, b, c, layout, satf) -->\n  // Intrinsic::nvvm_wmma_m16n16k16_mma_sync<layout A,B><DType><CType><Satf>\n  case NVPTX::BI__hmma_m16n16k16_mma_f16f16:\n  case NVPTX::BI__hmma_m16n16k16_mma_f32f16:\n  case NVPTX::BI__hmma_m16n16k16_mma_f32f32:\n  case NVPTX::BI__hmma_m16n16k16_mma_f16f32:\n  case NVPTX::BI__hmma_m32n8k16_mma_f16f16:\n  case NVPTX::BI__hmma_m32n8k16_mma_f32f16:\n  case NVPTX::BI__hmma_m32n8k16_mma_f32f32:\n  case NVPTX::BI__hmma_m32n8k16_mma_f16f32:\n  case NVPTX::BI__hmma_m8n32k16_mma_f16f16:\n  case NVPTX::BI__hmma_m8n32k16_mma_f32f16:\n  case NVPTX::BI__hmma_m8n32k16_mma_f32f32:\n  case NVPTX::BI__hmma_m8n32k16_mma_f16f32:\n  case NVPTX::BI__imma_m16n16k16_mma_s8:\n  case NVPTX::BI__imma_m16n16k16_mma_u8:\n  case NVPTX::BI__imma_m32n8k16_mma_s8:\n  case NVPTX::BI__imma_m32n8k16_mma_u8:\n  case NVPTX::BI__imma_m8n32k16_mma_s8:\n  case NVPTX::BI__imma_m8n32k16_mma_u8:\n  case NVPTX::BI__imma_m8n8k32_mma_s4:\n  case NVPTX::BI__imma_m8n8k32_mma_u4:\n  case NVPTX::BI__bmma_m8n8k128_mma_xor_popc_b1: {\n    Address Dst = EmitPointerWithAlignment(E->getArg(0));\n    Address SrcA = EmitPointerWithAlignment(E->getArg(1));\n    Address SrcB = EmitPointerWithAlignment(E->getArg(2));\n    Address SrcC = EmitPointerWithAlignment(E->getArg(3));\n    Optional<llvm::APSInt> LayoutArg =\n        E->getArg(4)->getIntegerConstantExpr(getContext());\n    if (!LayoutArg)\n      return nullptr;\n    int Layout = LayoutArg->getSExtValue();\n    if (Layout < 0 || Layout > 3)\n      return nullptr;\n    llvm::APSInt SatfArg;\n    if (BuiltinID == NVPTX::BI__bmma_m8n8k128_mma_xor_popc_b1)\n      SatfArg = 0;  // .b1 does not have satf argument.\n    else if (Optional<llvm::APSInt> OptSatfArg =\n                 E->getArg(5)->getIntegerConstantExpr(getContext()))\n      SatfArg = *OptSatfArg;\n    else\n      return nullptr;\n    bool Satf = SatfArg.getSExtValue();\n    NVPTXMmaInfo MI = getNVPTXMmaInfo(BuiltinID);\n    unsigned IID = MI.getMMAIntrinsic(Layout, Satf);\n    if (IID == 0)  // Unsupported combination of Layout/Satf.\n      return nullptr;\n\n    SmallVector<Value *, 24> Values;\n    Function *Intrinsic = CGM.getIntrinsic(IID);\n    llvm::Type *AType = Intrinsic->getFunctionType()->getParamType(0);\n    // Load A\n    for (unsigned i = 0; i < MI.NumEltsA; ++i) {\n      Value *V = Builder.CreateAlignedLoad(\n          Builder.CreateGEP(SrcA.getPointer(),\n                            llvm::ConstantInt::get(IntTy, i)),\n          CharUnits::fromQuantity(4));\n      Values.push_back(Builder.CreateBitCast(V, AType));\n    }\n    // Load B\n    llvm::Type *BType = Intrinsic->getFunctionType()->getParamType(MI.NumEltsA);\n    for (unsigned i = 0; i < MI.NumEltsB; ++i) {\n      Value *V = Builder.CreateAlignedLoad(\n          Builder.CreateGEP(SrcB.getPointer(),\n                            llvm::ConstantInt::get(IntTy, i)),\n          CharUnits::fromQuantity(4));\n      Values.push_back(Builder.CreateBitCast(V, BType));\n    }\n    // Load C\n    llvm::Type *CType =\n        Intrinsic->getFunctionType()->getParamType(MI.NumEltsA + MI.NumEltsB);\n    for (unsigned i = 0; i < MI.NumEltsC; ++i) {\n      Value *V = Builder.CreateAlignedLoad(\n          Builder.CreateGEP(SrcC.getPointer(),\n                            llvm::ConstantInt::get(IntTy, i)),\n          CharUnits::fromQuantity(4));\n      Values.push_back(Builder.CreateBitCast(V, CType));\n    }\n    Value *Result = Builder.CreateCall(Intrinsic, Values);\n    llvm::Type *DType = Dst.getElementType();\n    for (unsigned i = 0; i < MI.NumEltsD; ++i)\n      Builder.CreateAlignedStore(\n          Builder.CreateBitCast(Builder.CreateExtractValue(Result, i), DType),\n          Builder.CreateGEP(Dst.getPointer(), llvm::ConstantInt::get(IntTy, i)),\n          CharUnits::fromQuantity(4));\n    return Result;\n  }\n  default:\n    return nullptr;\n  }\n}\n\nnamespace {\nstruct BuiltinAlignArgs {\n  llvm::Value *Src = nullptr;\n  llvm::Type *SrcType = nullptr;\n  llvm::Value *Alignment = nullptr;\n  llvm::Value *Mask = nullptr;\n  llvm::IntegerType *IntType = nullptr;\n\n  BuiltinAlignArgs(const CallExpr *E, CodeGenFunction &CGF) {\n    QualType AstType = E->getArg(0)->getType();\n    if (AstType->isArrayType())\n      Src = CGF.EmitArrayToPointerDecay(E->getArg(0)).getPointer();\n    else\n      Src = CGF.EmitScalarExpr(E->getArg(0));\n    SrcType = Src->getType();\n    if (SrcType->isPointerTy()) {\n      IntType = IntegerType::get(\n          CGF.getLLVMContext(),\n          CGF.CGM.getDataLayout().getIndexTypeSizeInBits(SrcType));\n    } else {\n      assert(SrcType->isIntegerTy());\n      IntType = cast<llvm::IntegerType>(SrcType);\n    }\n    Alignment = CGF.EmitScalarExpr(E->getArg(1));\n    Alignment = CGF.Builder.CreateZExtOrTrunc(Alignment, IntType, \"alignment\");\n    auto *One = llvm::ConstantInt::get(IntType, 1);\n    Mask = CGF.Builder.CreateSub(Alignment, One, \"mask\");\n  }\n};\n} // namespace\n\n/// Generate (x & (y-1)) == 0.\nRValue CodeGenFunction::EmitBuiltinIsAligned(const CallExpr *E) {\n  BuiltinAlignArgs Args(E, *this);\n  llvm::Value *SrcAddress = Args.Src;\n  if (Args.SrcType->isPointerTy())\n    SrcAddress =\n        Builder.CreateBitOrPointerCast(Args.Src, Args.IntType, \"src_addr\");\n  return RValue::get(Builder.CreateICmpEQ(\n      Builder.CreateAnd(SrcAddress, Args.Mask, \"set_bits\"),\n      llvm::Constant::getNullValue(Args.IntType), \"is_aligned\"));\n}\n\n/// Generate (x & ~(y-1)) to align down or ((x+(y-1)) & ~(y-1)) to align up.\n/// Note: For pointer types we can avoid ptrtoint/inttoptr pairs by using the\n/// llvm.ptrmask instrinsic (with a GEP before in the align_up case).\n/// TODO: actually use ptrmask once most optimization passes know about it.\nRValue CodeGenFunction::EmitBuiltinAlignTo(const CallExpr *E, bool AlignUp) {\n  BuiltinAlignArgs Args(E, *this);\n  llvm::Value *SrcAddr = Args.Src;\n  if (Args.Src->getType()->isPointerTy())\n    SrcAddr = Builder.CreatePtrToInt(Args.Src, Args.IntType, \"intptr\");\n  llvm::Value *SrcForMask = SrcAddr;\n  if (AlignUp) {\n    // When aligning up we have to first add the mask to ensure we go over the\n    // next alignment value and then align down to the next valid multiple.\n    // By adding the mask, we ensure that align_up on an already aligned\n    // value will not change the value.\n    SrcForMask = Builder.CreateAdd(SrcForMask, Args.Mask, \"over_boundary\");\n  }\n  // Invert the mask to only clear the lower bits.\n  llvm::Value *InvertedMask = Builder.CreateNot(Args.Mask, \"inverted_mask\");\n  llvm::Value *Result =\n      Builder.CreateAnd(SrcForMask, InvertedMask, \"aligned_result\");\n  if (Args.Src->getType()->isPointerTy()) {\n    /// TODO: Use ptrmask instead of ptrtoint+gep once it is optimized well.\n    // Result = Builder.CreateIntrinsic(\n    //  Intrinsic::ptrmask, {Args.SrcType, SrcForMask->getType(), Args.IntType},\n    //  {SrcForMask, NegatedMask}, nullptr, \"aligned_result\");\n    Result->setName(\"aligned_intptr\");\n    llvm::Value *Difference = Builder.CreateSub(Result, SrcAddr, \"diff\");\n    // The result must point to the same underlying allocation. This means we\n    // can use an inbounds GEP to enable better optimization.\n    Value *Base = EmitCastToVoidPtr(Args.Src);\n    if (getLangOpts().isSignedOverflowDefined())\n      Result = Builder.CreateGEP(Base, Difference, \"aligned_result\");\n    else\n      Result = EmitCheckedInBoundsGEP(Base, Difference,\n                                      /*SignedIndices=*/true,\n                                      /*isSubtraction=*/!AlignUp,\n                                      E->getExprLoc(), \"aligned_result\");\n    Result = Builder.CreatePointerCast(Result, Args.SrcType);\n    // Emit an alignment assumption to ensure that the new alignment is\n    // propagated to loads/stores, etc.\n    emitAlignmentAssumption(Result, E, E->getExprLoc(), Args.Alignment);\n  }\n  assert(Result->getType() == Args.SrcType);\n  return RValue::get(Result);\n}\n\nValue *CodeGenFunction::EmitWebAssemblyBuiltinExpr(unsigned BuiltinID,\n                                                   const CallExpr *E) {\n  switch (BuiltinID) {\n  case WebAssembly::BI__builtin_wasm_memory_size: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *I = EmitScalarExpr(E->getArg(0));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_memory_size, ResultType);\n    return Builder.CreateCall(Callee, I);\n  }\n  case WebAssembly::BI__builtin_wasm_memory_grow: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Value *Args[] = {EmitScalarExpr(E->getArg(0)),\n                     EmitScalarExpr(E->getArg(1))};\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_memory_grow, ResultType);\n    return Builder.CreateCall(Callee, Args);\n  }\n  case WebAssembly::BI__builtin_wasm_tls_size: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_tls_size, ResultType);\n    return Builder.CreateCall(Callee);\n  }\n  case WebAssembly::BI__builtin_wasm_tls_align: {\n    llvm::Type *ResultType = ConvertType(E->getType());\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_tls_align, ResultType);\n    return Builder.CreateCall(Callee);\n  }\n  case WebAssembly::BI__builtin_wasm_tls_base: {\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_tls_base);\n    return Builder.CreateCall(Callee);\n  }\n  case WebAssembly::BI__builtin_wasm_throw: {\n    Value *Tag = EmitScalarExpr(E->getArg(0));\n    Value *Obj = EmitScalarExpr(E->getArg(1));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_throw);\n    return Builder.CreateCall(Callee, {Tag, Obj});\n  }\n  case WebAssembly::BI__builtin_wasm_rethrow: {\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_rethrow);\n    return Builder.CreateCall(Callee);\n  }\n  case WebAssembly::BI__builtin_wasm_memory_atomic_wait32: {\n    Value *Addr = EmitScalarExpr(E->getArg(0));\n    Value *Expected = EmitScalarExpr(E->getArg(1));\n    Value *Timeout = EmitScalarExpr(E->getArg(2));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_memory_atomic_wait32);\n    return Builder.CreateCall(Callee, {Addr, Expected, Timeout});\n  }\n  case WebAssembly::BI__builtin_wasm_memory_atomic_wait64: {\n    Value *Addr = EmitScalarExpr(E->getArg(0));\n    Value *Expected = EmitScalarExpr(E->getArg(1));\n    Value *Timeout = EmitScalarExpr(E->getArg(2));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_memory_atomic_wait64);\n    return Builder.CreateCall(Callee, {Addr, Expected, Timeout});\n  }\n  case WebAssembly::BI__builtin_wasm_memory_atomic_notify: {\n    Value *Addr = EmitScalarExpr(E->getArg(0));\n    Value *Count = EmitScalarExpr(E->getArg(1));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_memory_atomic_notify);\n    return Builder.CreateCall(Callee, {Addr, Count});\n  }\n  case WebAssembly::BI__builtin_wasm_trunc_s_i32_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_s_i32_f64:\n  case WebAssembly::BI__builtin_wasm_trunc_s_i64_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_s_i64_f64: {\n    Value *Src = EmitScalarExpr(E->getArg(0));\n    llvm::Type *ResT = ConvertType(E->getType());\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_trunc_signed, {ResT, Src->getType()});\n    return Builder.CreateCall(Callee, {Src});\n  }\n  case WebAssembly::BI__builtin_wasm_trunc_u_i32_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_u_i32_f64:\n  case WebAssembly::BI__builtin_wasm_trunc_u_i64_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_u_i64_f64: {\n    Value *Src = EmitScalarExpr(E->getArg(0));\n    llvm::Type *ResT = ConvertType(E->getType());\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_trunc_unsigned,\n                                        {ResT, Src->getType()});\n    return Builder.CreateCall(Callee, {Src});\n  }\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_s_i32_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_s_i32_f64:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_s_i64_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_s_i64_f64:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_s_i32x4_f32x4: {\n    Value *Src = EmitScalarExpr(E->getArg(0));\n    llvm::Type *ResT = ConvertType(E->getType());\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_trunc_saturate_signed,\n                                        {ResT, Src->getType()});\n    return Builder.CreateCall(Callee, {Src});\n  }\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_u_i32_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_u_i32_f64:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_u_i64_f32:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_u_i64_f64:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_u_i32x4_f32x4: {\n    Value *Src = EmitScalarExpr(E->getArg(0));\n    llvm::Type *ResT = ConvertType(E->getType());\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_trunc_saturate_unsigned,\n                                        {ResT, Src->getType()});\n    return Builder.CreateCall(Callee, {Src});\n  }\n  case WebAssembly::BI__builtin_wasm_min_f32:\n  case WebAssembly::BI__builtin_wasm_min_f64:\n  case WebAssembly::BI__builtin_wasm_min_f32x4:\n  case WebAssembly::BI__builtin_wasm_min_f64x2: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::minimum, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_max_f32:\n  case WebAssembly::BI__builtin_wasm_max_f64:\n  case WebAssembly::BI__builtin_wasm_max_f32x4:\n  case WebAssembly::BI__builtin_wasm_max_f64x2: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::maximum, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_pmin_f32x4:\n  case WebAssembly::BI__builtin_wasm_pmin_f64x2: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_pmin, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_pmax_f32x4:\n  case WebAssembly::BI__builtin_wasm_pmax_f64x2: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_pmax, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_ceil_f32x4:\n  case WebAssembly::BI__builtin_wasm_floor_f32x4:\n  case WebAssembly::BI__builtin_wasm_trunc_f32x4:\n  case WebAssembly::BI__builtin_wasm_nearest_f32x4:\n  case WebAssembly::BI__builtin_wasm_ceil_f64x2:\n  case WebAssembly::BI__builtin_wasm_floor_f64x2:\n  case WebAssembly::BI__builtin_wasm_trunc_f64x2:\n  case WebAssembly::BI__builtin_wasm_nearest_f64x2: {\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_ceil_f32x4:\n    case WebAssembly::BI__builtin_wasm_ceil_f64x2:\n      IntNo = Intrinsic::wasm_ceil;\n      break;\n    case WebAssembly::BI__builtin_wasm_floor_f32x4:\n    case WebAssembly::BI__builtin_wasm_floor_f64x2:\n      IntNo = Intrinsic::wasm_floor;\n      break;\n    case WebAssembly::BI__builtin_wasm_trunc_f32x4:\n    case WebAssembly::BI__builtin_wasm_trunc_f64x2:\n      IntNo = Intrinsic::wasm_trunc;\n      break;\n    case WebAssembly::BI__builtin_wasm_nearest_f32x4:\n    case WebAssembly::BI__builtin_wasm_nearest_f64x2:\n      IntNo = Intrinsic::wasm_nearest;\n      break;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n    Value *Value = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(IntNo, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, Value);\n  }\n  case WebAssembly::BI__builtin_wasm_swizzle_v8x16: {\n    Value *Src = EmitScalarExpr(E->getArg(0));\n    Value *Indices = EmitScalarExpr(E->getArg(1));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_swizzle);\n    return Builder.CreateCall(Callee, {Src, Indices});\n  }\n  case WebAssembly::BI__builtin_wasm_extract_lane_s_i8x16:\n  case WebAssembly::BI__builtin_wasm_extract_lane_u_i8x16:\n  case WebAssembly::BI__builtin_wasm_extract_lane_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_extract_lane_u_i16x8:\n  case WebAssembly::BI__builtin_wasm_extract_lane_i32x4:\n  case WebAssembly::BI__builtin_wasm_extract_lane_i64x2:\n  case WebAssembly::BI__builtin_wasm_extract_lane_f32x4:\n  case WebAssembly::BI__builtin_wasm_extract_lane_f64x2: {\n    llvm::APSInt LaneConst =\n        *E->getArg(1)->getIntegerConstantExpr(getContext());\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Value *Lane = llvm::ConstantInt::get(getLLVMContext(), LaneConst);\n    Value *Extract = Builder.CreateExtractElement(Vec, Lane);\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_extract_lane_s_i8x16:\n    case WebAssembly::BI__builtin_wasm_extract_lane_s_i16x8:\n      return Builder.CreateSExt(Extract, ConvertType(E->getType()));\n    case WebAssembly::BI__builtin_wasm_extract_lane_u_i8x16:\n    case WebAssembly::BI__builtin_wasm_extract_lane_u_i16x8:\n      return Builder.CreateZExt(Extract, ConvertType(E->getType()));\n    case WebAssembly::BI__builtin_wasm_extract_lane_i32x4:\n    case WebAssembly::BI__builtin_wasm_extract_lane_i64x2:\n    case WebAssembly::BI__builtin_wasm_extract_lane_f32x4:\n    case WebAssembly::BI__builtin_wasm_extract_lane_f64x2:\n      return Extract;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n  }\n  case WebAssembly::BI__builtin_wasm_replace_lane_i8x16:\n  case WebAssembly::BI__builtin_wasm_replace_lane_i16x8:\n  case WebAssembly::BI__builtin_wasm_replace_lane_i32x4:\n  case WebAssembly::BI__builtin_wasm_replace_lane_i64x2:\n  case WebAssembly::BI__builtin_wasm_replace_lane_f32x4:\n  case WebAssembly::BI__builtin_wasm_replace_lane_f64x2: {\n    llvm::APSInt LaneConst =\n        *E->getArg(1)->getIntegerConstantExpr(getContext());\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Value *Lane = llvm::ConstantInt::get(getLLVMContext(), LaneConst);\n    Value *Val = EmitScalarExpr(E->getArg(2));\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_replace_lane_i8x16:\n    case WebAssembly::BI__builtin_wasm_replace_lane_i16x8: {\n      llvm::Type *ElemType =\n          cast<llvm::VectorType>(ConvertType(E->getType()))->getElementType();\n      Value *Trunc = Builder.CreateTrunc(Val, ElemType);\n      return Builder.CreateInsertElement(Vec, Trunc, Lane);\n    }\n    case WebAssembly::BI__builtin_wasm_replace_lane_i32x4:\n    case WebAssembly::BI__builtin_wasm_replace_lane_i64x2:\n    case WebAssembly::BI__builtin_wasm_replace_lane_f32x4:\n    case WebAssembly::BI__builtin_wasm_replace_lane_f64x2:\n      return Builder.CreateInsertElement(Vec, Val, Lane);\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n  }\n  case WebAssembly::BI__builtin_wasm_add_saturate_s_i8x16:\n  case WebAssembly::BI__builtin_wasm_add_saturate_u_i8x16:\n  case WebAssembly::BI__builtin_wasm_add_saturate_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_add_saturate_u_i16x8:\n  case WebAssembly::BI__builtin_wasm_sub_saturate_s_i8x16:\n  case WebAssembly::BI__builtin_wasm_sub_saturate_u_i8x16:\n  case WebAssembly::BI__builtin_wasm_sub_saturate_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_sub_saturate_u_i16x8: {\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_add_saturate_s_i8x16:\n    case WebAssembly::BI__builtin_wasm_add_saturate_s_i16x8:\n      IntNo = Intrinsic::sadd_sat;\n      break;\n    case WebAssembly::BI__builtin_wasm_add_saturate_u_i8x16:\n    case WebAssembly::BI__builtin_wasm_add_saturate_u_i16x8:\n      IntNo = Intrinsic::uadd_sat;\n      break;\n    case WebAssembly::BI__builtin_wasm_sub_saturate_s_i8x16:\n    case WebAssembly::BI__builtin_wasm_sub_saturate_s_i16x8:\n      IntNo = Intrinsic::wasm_sub_saturate_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_sub_saturate_u_i8x16:\n    case WebAssembly::BI__builtin_wasm_sub_saturate_u_i16x8:\n      IntNo = Intrinsic::wasm_sub_saturate_unsigned;\n      break;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee = CGM.getIntrinsic(IntNo, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_abs_i8x16:\n  case WebAssembly::BI__builtin_wasm_abs_i16x8:\n  case WebAssembly::BI__builtin_wasm_abs_i32x4: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Value *Neg = Builder.CreateNeg(Vec, \"neg\");\n    Constant *Zero = llvm::Constant::getNullValue(Vec->getType());\n    Value *ICmp = Builder.CreateICmpSLT(Vec, Zero, \"abscond\");\n    return Builder.CreateSelect(ICmp, Neg, Vec, \"abs\");\n  }\n  case WebAssembly::BI__builtin_wasm_min_s_i8x16:\n  case WebAssembly::BI__builtin_wasm_min_u_i8x16:\n  case WebAssembly::BI__builtin_wasm_max_s_i8x16:\n  case WebAssembly::BI__builtin_wasm_max_u_i8x16:\n  case WebAssembly::BI__builtin_wasm_min_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_min_u_i16x8:\n  case WebAssembly::BI__builtin_wasm_max_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_max_u_i16x8:\n  case WebAssembly::BI__builtin_wasm_min_s_i32x4:\n  case WebAssembly::BI__builtin_wasm_min_u_i32x4:\n  case WebAssembly::BI__builtin_wasm_max_s_i32x4:\n  case WebAssembly::BI__builtin_wasm_max_u_i32x4: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Value *ICmp;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_min_s_i8x16:\n    case WebAssembly::BI__builtin_wasm_min_s_i16x8:\n    case WebAssembly::BI__builtin_wasm_min_s_i32x4:\n      ICmp = Builder.CreateICmpSLT(LHS, RHS);\n      break;\n    case WebAssembly::BI__builtin_wasm_min_u_i8x16:\n    case WebAssembly::BI__builtin_wasm_min_u_i16x8:\n    case WebAssembly::BI__builtin_wasm_min_u_i32x4:\n      ICmp = Builder.CreateICmpULT(LHS, RHS);\n      break;\n    case WebAssembly::BI__builtin_wasm_max_s_i8x16:\n    case WebAssembly::BI__builtin_wasm_max_s_i16x8:\n    case WebAssembly::BI__builtin_wasm_max_s_i32x4:\n      ICmp = Builder.CreateICmpSGT(LHS, RHS);\n      break;\n    case WebAssembly::BI__builtin_wasm_max_u_i8x16:\n    case WebAssembly::BI__builtin_wasm_max_u_i16x8:\n    case WebAssembly::BI__builtin_wasm_max_u_i32x4:\n      ICmp = Builder.CreateICmpUGT(LHS, RHS);\n      break;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n    return Builder.CreateSelect(ICmp, LHS, RHS);\n  }\n  case WebAssembly::BI__builtin_wasm_avgr_u_i8x16:\n  case WebAssembly::BI__builtin_wasm_avgr_u_i16x8: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_avgr_unsigned,\n                                        ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_q15mulr_saturate_s_i16x8: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_q15mulr_saturate_signed);\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_extmul_low_i8x16_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_extmul_high_i8x16_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_extmul_low_i8x16_u_i16x8:\n  case WebAssembly::BI__builtin_wasm_extmul_high_i8x16_u_i16x8:\n  case WebAssembly::BI__builtin_wasm_extmul_low_i16x8_s_i32x4:\n  case WebAssembly::BI__builtin_wasm_extmul_high_i16x8_s_i32x4:\n  case WebAssembly::BI__builtin_wasm_extmul_low_i16x8_u_i32x4:\n  case WebAssembly::BI__builtin_wasm_extmul_high_i16x8_u_i32x4:\n  case WebAssembly::BI__builtin_wasm_extmul_low_i32x4_s_i64x2:\n  case WebAssembly::BI__builtin_wasm_extmul_high_i32x4_s_i64x2:\n  case WebAssembly::BI__builtin_wasm_extmul_low_i32x4_u_i64x2:\n  case WebAssembly::BI__builtin_wasm_extmul_high_i32x4_u_i64x2: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_extmul_low_i8x16_s_i16x8:\n    case WebAssembly::BI__builtin_wasm_extmul_low_i16x8_s_i32x4:\n    case WebAssembly::BI__builtin_wasm_extmul_low_i32x4_s_i64x2:\n      IntNo = Intrinsic::wasm_extmul_low_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_extmul_low_i8x16_u_i16x8:\n    case WebAssembly::BI__builtin_wasm_extmul_low_i16x8_u_i32x4:\n    case WebAssembly::BI__builtin_wasm_extmul_low_i32x4_u_i64x2:\n      IntNo = Intrinsic::wasm_extmul_low_unsigned;\n      break;\n    case WebAssembly::BI__builtin_wasm_extmul_high_i8x16_s_i16x8:\n    case WebAssembly::BI__builtin_wasm_extmul_high_i16x8_s_i32x4:\n    case WebAssembly::BI__builtin_wasm_extmul_high_i32x4_s_i64x2:\n      IntNo = Intrinsic::wasm_extmul_high_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_extmul_high_i8x16_u_i16x8:\n    case WebAssembly::BI__builtin_wasm_extmul_high_i16x8_u_i32x4:\n    case WebAssembly::BI__builtin_wasm_extmul_high_i32x4_u_i64x2:\n      IntNo = Intrinsic::wasm_extmul_high_unsigned;\n      break;\n    default:\n      llvm_unreachable(\"unexptected builtin ID\");\n    }\n\n    Function *Callee = CGM.getIntrinsic(IntNo, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_extadd_pairwise_i8x16_s_i16x8:\n  case WebAssembly::BI__builtin_wasm_extadd_pairwise_i8x16_u_i16x8:\n  case WebAssembly::BI__builtin_wasm_extadd_pairwise_i16x8_s_i32x4:\n  case WebAssembly::BI__builtin_wasm_extadd_pairwise_i16x8_u_i32x4: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_extadd_pairwise_i8x16_s_i16x8:\n    case WebAssembly::BI__builtin_wasm_extadd_pairwise_i16x8_s_i32x4:\n      IntNo = Intrinsic::wasm_extadd_pairwise_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_extadd_pairwise_i8x16_u_i16x8:\n    case WebAssembly::BI__builtin_wasm_extadd_pairwise_i16x8_u_i32x4:\n      IntNo = Intrinsic::wasm_extadd_pairwise_unsigned;\n      break;\n    default:\n      llvm_unreachable(\"unexptected builtin ID\");\n    }\n\n    Function *Callee = CGM.getIntrinsic(IntNo, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, Vec);\n  }\n  case WebAssembly::BI__builtin_wasm_bitselect: {\n    Value *V1 = EmitScalarExpr(E->getArg(0));\n    Value *V2 = EmitScalarExpr(E->getArg(1));\n    Value *C = EmitScalarExpr(E->getArg(2));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_bitselect, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {V1, V2, C});\n  }\n  case WebAssembly::BI__builtin_wasm_signselect_i8x16:\n  case WebAssembly::BI__builtin_wasm_signselect_i16x8:\n  case WebAssembly::BI__builtin_wasm_signselect_i32x4:\n  case WebAssembly::BI__builtin_wasm_signselect_i64x2: {\n    Value *V1 = EmitScalarExpr(E->getArg(0));\n    Value *V2 = EmitScalarExpr(E->getArg(1));\n    Value *C = EmitScalarExpr(E->getArg(2));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_signselect, ConvertType(E->getType()));\n    return Builder.CreateCall(Callee, {V1, V2, C});\n  }\n  case WebAssembly::BI__builtin_wasm_dot_s_i32x4_i16x8: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_dot);\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_popcnt_i8x16: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_popcnt);\n    return Builder.CreateCall(Callee, {Vec});\n  }\n  case WebAssembly::BI__builtin_wasm_eq_i64x2: {\n    Value *LHS = EmitScalarExpr(E->getArg(0));\n    Value *RHS = EmitScalarExpr(E->getArg(1));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_eq);\n    return Builder.CreateCall(Callee, {LHS, RHS});\n  }\n  case WebAssembly::BI__builtin_wasm_any_true_i8x16:\n  case WebAssembly::BI__builtin_wasm_any_true_i16x8:\n  case WebAssembly::BI__builtin_wasm_any_true_i32x4:\n  case WebAssembly::BI__builtin_wasm_any_true_i64x2:\n  case WebAssembly::BI__builtin_wasm_all_true_i8x16:\n  case WebAssembly::BI__builtin_wasm_all_true_i16x8:\n  case WebAssembly::BI__builtin_wasm_all_true_i32x4:\n  case WebAssembly::BI__builtin_wasm_all_true_i64x2: {\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_any_true_i8x16:\n    case WebAssembly::BI__builtin_wasm_any_true_i16x8:\n    case WebAssembly::BI__builtin_wasm_any_true_i32x4:\n    case WebAssembly::BI__builtin_wasm_any_true_i64x2:\n      IntNo = Intrinsic::wasm_anytrue;\n      break;\n    case WebAssembly::BI__builtin_wasm_all_true_i8x16:\n    case WebAssembly::BI__builtin_wasm_all_true_i16x8:\n    case WebAssembly::BI__builtin_wasm_all_true_i32x4:\n    case WebAssembly::BI__builtin_wasm_all_true_i64x2:\n      IntNo = Intrinsic::wasm_alltrue;\n      break;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(IntNo, Vec->getType());\n    return Builder.CreateCall(Callee, {Vec});\n  }\n  case WebAssembly::BI__builtin_wasm_bitmask_i8x16:\n  case WebAssembly::BI__builtin_wasm_bitmask_i16x8:\n  case WebAssembly::BI__builtin_wasm_bitmask_i32x4:\n  case WebAssembly::BI__builtin_wasm_bitmask_i64x2: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Function *Callee =\n        CGM.getIntrinsic(Intrinsic::wasm_bitmask, Vec->getType());\n    return Builder.CreateCall(Callee, {Vec});\n  }\n  case WebAssembly::BI__builtin_wasm_abs_f32x4:\n  case WebAssembly::BI__builtin_wasm_abs_f64x2: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::fabs, Vec->getType());\n    return Builder.CreateCall(Callee, {Vec});\n  }\n  case WebAssembly::BI__builtin_wasm_sqrt_f32x4:\n  case WebAssembly::BI__builtin_wasm_sqrt_f64x2: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::sqrt, Vec->getType());\n    return Builder.CreateCall(Callee, {Vec});\n  }\n  case WebAssembly::BI__builtin_wasm_qfma_f32x4:\n  case WebAssembly::BI__builtin_wasm_qfms_f32x4:\n  case WebAssembly::BI__builtin_wasm_qfma_f64x2:\n  case WebAssembly::BI__builtin_wasm_qfms_f64x2: {\n    Value *A = EmitScalarExpr(E->getArg(0));\n    Value *B = EmitScalarExpr(E->getArg(1));\n    Value *C = EmitScalarExpr(E->getArg(2));\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_qfma_f32x4:\n    case WebAssembly::BI__builtin_wasm_qfma_f64x2:\n      IntNo = Intrinsic::wasm_qfma;\n      break;\n    case WebAssembly::BI__builtin_wasm_qfms_f32x4:\n    case WebAssembly::BI__builtin_wasm_qfms_f64x2:\n      IntNo = Intrinsic::wasm_qfms;\n      break;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n    Function *Callee = CGM.getIntrinsic(IntNo, A->getType());\n    return Builder.CreateCall(Callee, {A, B, C});\n  }\n  case WebAssembly::BI__builtin_wasm_narrow_s_i8x16_i16x8:\n  case WebAssembly::BI__builtin_wasm_narrow_u_i8x16_i16x8:\n  case WebAssembly::BI__builtin_wasm_narrow_s_i16x8_i32x4:\n  case WebAssembly::BI__builtin_wasm_narrow_u_i16x8_i32x4: {\n    Value *Low = EmitScalarExpr(E->getArg(0));\n    Value *High = EmitScalarExpr(E->getArg(1));\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_narrow_s_i8x16_i16x8:\n    case WebAssembly::BI__builtin_wasm_narrow_s_i16x8_i32x4:\n      IntNo = Intrinsic::wasm_narrow_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_narrow_u_i8x16_i16x8:\n    case WebAssembly::BI__builtin_wasm_narrow_u_i16x8_i32x4:\n      IntNo = Intrinsic::wasm_narrow_unsigned;\n      break;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n    Function *Callee =\n        CGM.getIntrinsic(IntNo, {ConvertType(E->getType()), Low->getType()});\n    return Builder.CreateCall(Callee, {Low, High});\n  }\n  case WebAssembly::BI__builtin_wasm_widen_low_s_i32x4_i64x2:\n  case WebAssembly::BI__builtin_wasm_widen_high_s_i32x4_i64x2:\n  case WebAssembly::BI__builtin_wasm_widen_low_u_i32x4_i64x2:\n  case WebAssembly::BI__builtin_wasm_widen_high_u_i32x4_i64x2: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_widen_low_s_i32x4_i64x2:\n      IntNo = Intrinsic::wasm_widen_low_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_widen_high_s_i32x4_i64x2:\n      IntNo = Intrinsic::wasm_widen_high_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_widen_low_u_i32x4_i64x2:\n      IntNo = Intrinsic::wasm_widen_low_unsigned;\n      break;\n    case WebAssembly::BI__builtin_wasm_widen_high_u_i32x4_i64x2:\n      IntNo = Intrinsic::wasm_widen_high_unsigned;\n      break;\n    }\n    Function *Callee = CGM.getIntrinsic(IntNo);\n    return Builder.CreateCall(Callee, Vec);\n  }\n  case WebAssembly::BI__builtin_wasm_widen_s_i8x16_i32x4:\n  case WebAssembly::BI__builtin_wasm_widen_u_i8x16_i32x4: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    llvm::APSInt SubVecConst =\n        *E->getArg(1)->getIntegerConstantExpr(getContext());\n    Value *SubVec = llvm::ConstantInt::get(getLLVMContext(), SubVecConst);\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_widen_s_i8x16_i32x4:\n      IntNo = Intrinsic::wasm_widen_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_widen_u_i8x16_i32x4:\n      IntNo = Intrinsic::wasm_widen_unsigned;\n      break;\n    }\n    Function *Callee = CGM.getIntrinsic(IntNo);\n    return Builder.CreateCall(Callee, {Vec, SubVec});\n  }\n  case WebAssembly::BI__builtin_wasm_convert_low_s_i32x4_f64x2:\n  case WebAssembly::BI__builtin_wasm_convert_low_u_i32x4_f64x2: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_convert_low_s_i32x4_f64x2:\n      IntNo = Intrinsic::wasm_convert_low_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_convert_low_u_i32x4_f64x2:\n      IntNo = Intrinsic::wasm_convert_low_unsigned;\n      break;\n    }\n    Function *Callee = CGM.getIntrinsic(IntNo);\n    return Builder.CreateCall(Callee, Vec);\n  }\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_zero_s_f64x2_i32x4:\n  case WebAssembly::BI__builtin_wasm_trunc_saturate_zero_u_f64x2_i32x4: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_trunc_saturate_zero_s_f64x2_i32x4:\n      IntNo = Intrinsic::wasm_trunc_saturate_zero_signed;\n      break;\n    case WebAssembly::BI__builtin_wasm_trunc_saturate_zero_u_f64x2_i32x4:\n      IntNo = Intrinsic::wasm_trunc_saturate_zero_unsigned;\n      break;\n    }\n    Function *Callee = CGM.getIntrinsic(IntNo);\n    return Builder.CreateCall(Callee, Vec);\n  }\n  case WebAssembly::BI__builtin_wasm_demote_zero_f64x2_f32x4: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_demote_zero);\n    return Builder.CreateCall(Callee, Vec);\n  }\n  case WebAssembly::BI__builtin_wasm_promote_low_f32x4_f64x2: {\n    Value *Vec = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_promote_low);\n    return Builder.CreateCall(Callee, Vec);\n  }\n  case WebAssembly::BI__builtin_wasm_load32_zero: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_load32_zero);\n    return Builder.CreateCall(Callee, {Ptr});\n  }\n  case WebAssembly::BI__builtin_wasm_load64_zero: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_load64_zero);\n    return Builder.CreateCall(Callee, {Ptr});\n  }\n  case WebAssembly::BI__builtin_wasm_load8_lane:\n  case WebAssembly::BI__builtin_wasm_load16_lane:\n  case WebAssembly::BI__builtin_wasm_load32_lane:\n  case WebAssembly::BI__builtin_wasm_load64_lane:\n  case WebAssembly::BI__builtin_wasm_store8_lane:\n  case WebAssembly::BI__builtin_wasm_store16_lane:\n  case WebAssembly::BI__builtin_wasm_store32_lane:\n  case WebAssembly::BI__builtin_wasm_store64_lane: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Value *Vec = EmitScalarExpr(E->getArg(1));\n    Optional<llvm::APSInt> LaneIdxConst =\n        E->getArg(2)->getIntegerConstantExpr(getContext());\n    assert(LaneIdxConst && \"Constant arg isn't actually constant?\");\n    Value *LaneIdx = llvm::ConstantInt::get(getLLVMContext(), *LaneIdxConst);\n    unsigned IntNo;\n    switch (BuiltinID) {\n    case WebAssembly::BI__builtin_wasm_load8_lane:\n      IntNo = Intrinsic::wasm_load8_lane;\n      break;\n    case WebAssembly::BI__builtin_wasm_load16_lane:\n      IntNo = Intrinsic::wasm_load16_lane;\n      break;\n    case WebAssembly::BI__builtin_wasm_load32_lane:\n      IntNo = Intrinsic::wasm_load32_lane;\n      break;\n    case WebAssembly::BI__builtin_wasm_load64_lane:\n      IntNo = Intrinsic::wasm_load64_lane;\n      break;\n    case WebAssembly::BI__builtin_wasm_store8_lane:\n      IntNo = Intrinsic::wasm_store8_lane;\n      break;\n    case WebAssembly::BI__builtin_wasm_store16_lane:\n      IntNo = Intrinsic::wasm_store16_lane;\n      break;\n    case WebAssembly::BI__builtin_wasm_store32_lane:\n      IntNo = Intrinsic::wasm_store32_lane;\n      break;\n    case WebAssembly::BI__builtin_wasm_store64_lane:\n      IntNo = Intrinsic::wasm_store64_lane;\n      break;\n    default:\n      llvm_unreachable(\"unexpected builtin ID\");\n    }\n    Function *Callee = CGM.getIntrinsic(IntNo);\n    return Builder.CreateCall(Callee, {Ptr, Vec, LaneIdx});\n  }\n  case WebAssembly::BI__builtin_wasm_shuffle_v8x16: {\n    Value *Ops[18];\n    size_t OpIdx = 0;\n    Ops[OpIdx++] = EmitScalarExpr(E->getArg(0));\n    Ops[OpIdx++] = EmitScalarExpr(E->getArg(1));\n    while (OpIdx < 18) {\n      Optional<llvm::APSInt> LaneConst =\n          E->getArg(OpIdx)->getIntegerConstantExpr(getContext());\n      assert(LaneConst && \"Constant arg isn't actually constant?\");\n      Ops[OpIdx++] = llvm::ConstantInt::get(getLLVMContext(), *LaneConst);\n    }\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_shuffle);\n    return Builder.CreateCall(Callee, Ops);\n  }\n  case WebAssembly::BI__builtin_wasm_prefetch_t: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_prefetch_t);\n    return Builder.CreateCall(Callee, Ptr);\n  }\n  case WebAssembly::BI__builtin_wasm_prefetch_nt: {\n    Value *Ptr = EmitScalarExpr(E->getArg(0));\n    Function *Callee = CGM.getIntrinsic(Intrinsic::wasm_prefetch_nt);\n    return Builder.CreateCall(Callee, Ptr);\n  }\n  default:\n    return nullptr;\n  }\n}\n\nstatic std::pair<Intrinsic::ID, unsigned>\ngetIntrinsicForHexagonNonGCCBuiltin(unsigned BuiltinID) {\n  struct Info {\n    unsigned BuiltinID;\n    Intrinsic::ID IntrinsicID;\n    unsigned VecLen;\n  };\n  Info Infos[] = {\n#define CUSTOM_BUILTIN_MAPPING(x,s) \\\n  { Hexagon::BI__builtin_HEXAGON_##x, Intrinsic::hexagon_##x, s },\n    CUSTOM_BUILTIN_MAPPING(L2_loadrub_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadrb_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadruh_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadrh_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadri_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadrd_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadrub_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadrb_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadruh_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadrh_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadri_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(L2_loadrd_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerb_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerh_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerf_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storeri_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerd_pci, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerb_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerh_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerf_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storeri_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(S2_storerd_pcr, 0)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstoreq, 64)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstorenq, 64)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstorentq, 64)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstorentnq, 64)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstoreq_128B, 128)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstorenq_128B, 128)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstorentq_128B, 128)\n    CUSTOM_BUILTIN_MAPPING(V6_vmaskedstorentnq_128B, 128)\n#include \"clang/Basic/BuiltinsHexagonMapCustomDep.def\"\n#undef CUSTOM_BUILTIN_MAPPING\n  };\n\n  auto CmpInfo = [] (Info A, Info B) { return A.BuiltinID < B.BuiltinID; };\n  static const bool SortOnce = (llvm::sort(Infos, CmpInfo), true);\n  (void)SortOnce;\n\n  const Info *F = std::lower_bound(std::begin(Infos), std::end(Infos),\n                                   Info{BuiltinID, 0, 0}, CmpInfo);\n  if (F == std::end(Infos) || F->BuiltinID != BuiltinID)\n    return {Intrinsic::not_intrinsic, 0};\n\n  return {F->IntrinsicID, F->VecLen};\n}\n\nValue *CodeGenFunction::EmitHexagonBuiltinExpr(unsigned BuiltinID,\n                                               const CallExpr *E) {\n  Intrinsic::ID ID;\n  unsigned VecLen;\n  std::tie(ID, VecLen) = getIntrinsicForHexagonNonGCCBuiltin(BuiltinID);\n\n  auto MakeCircOp = [this, E](unsigned IntID, bool IsLoad) {\n    // The base pointer is passed by address, so it needs to be loaded.\n    Address A = EmitPointerWithAlignment(E->getArg(0));\n    Address BP = Address(\n        Builder.CreateBitCast(A.getPointer(), Int8PtrPtrTy), A.getAlignment());\n    llvm::Value *Base = Builder.CreateLoad(BP);\n    // The treatment of both loads and stores is the same: the arguments for\n    // the builtin are the same as the arguments for the intrinsic.\n    // Load:\n    //   builtin(Base, Inc, Mod, Start) -> intr(Base, Inc, Mod, Start)\n    //   builtin(Base, Mod, Start)      -> intr(Base, Mod, Start)\n    // Store:\n    //   builtin(Base, Inc, Mod, Val, Start) -> intr(Base, Inc, Mod, Val, Start)\n    //   builtin(Base, Mod, Val, Start)      -> intr(Base, Mod, Val, Start)\n    SmallVector<llvm::Value*,5> Ops = { Base };\n    for (unsigned i = 1, e = E->getNumArgs(); i != e; ++i)\n      Ops.push_back(EmitScalarExpr(E->getArg(i)));\n\n    llvm::Value *Result = Builder.CreateCall(CGM.getIntrinsic(IntID), Ops);\n    // The load intrinsics generate two results (Value, NewBase), stores\n    // generate one (NewBase). The new base address needs to be stored.\n    llvm::Value *NewBase = IsLoad ? Builder.CreateExtractValue(Result, 1)\n                                  : Result;\n    llvm::Value *LV = Builder.CreateBitCast(\n        EmitScalarExpr(E->getArg(0)), NewBase->getType()->getPointerTo());\n    Address Dest = EmitPointerWithAlignment(E->getArg(0));\n    llvm::Value *RetVal =\n        Builder.CreateAlignedStore(NewBase, LV, Dest.getAlignment());\n    if (IsLoad)\n      RetVal = Builder.CreateExtractValue(Result, 0);\n    return RetVal;\n  };\n\n  // Handle the conversion of bit-reverse load intrinsics to bit code.\n  // The intrinsic call after this function only reads from memory and the\n  // write to memory is dealt by the store instruction.\n  auto MakeBrevLd = [this, E](unsigned IntID, llvm::Type *DestTy) {\n    // The intrinsic generates one result, which is the new value for the base\n    // pointer. It needs to be returned. The result of the load instruction is\n    // passed to intrinsic by address, so the value needs to be stored.\n    llvm::Value *BaseAddress =\n        Builder.CreateBitCast(EmitScalarExpr(E->getArg(0)), Int8PtrTy);\n\n    // Expressions like &(*pt++) will be incremented per evaluation.\n    // EmitPointerWithAlignment and EmitScalarExpr evaluates the expression\n    // per call.\n    Address DestAddr = EmitPointerWithAlignment(E->getArg(1));\n    DestAddr = Address(Builder.CreateBitCast(DestAddr.getPointer(), Int8PtrTy),\n                       DestAddr.getAlignment());\n    llvm::Value *DestAddress = DestAddr.getPointer();\n\n    // Operands are Base, Dest, Modifier.\n    // The intrinsic format in LLVM IR is defined as\n    // { ValueType, i8* } (i8*, i32).\n    llvm::Value *Result = Builder.CreateCall(\n        CGM.getIntrinsic(IntID), {BaseAddress, EmitScalarExpr(E->getArg(2))});\n\n    // The value needs to be stored as the variable is passed by reference.\n    llvm::Value *DestVal = Builder.CreateExtractValue(Result, 0);\n\n    // The store needs to be truncated to fit the destination type.\n    // While i32 and i64 are natively supported on Hexagon, i8 and i16 needs\n    // to be handled with stores of respective destination type.\n    DestVal = Builder.CreateTrunc(DestVal, DestTy);\n\n    llvm::Value *DestForStore =\n        Builder.CreateBitCast(DestAddress, DestVal->getType()->getPointerTo());\n    Builder.CreateAlignedStore(DestVal, DestForStore, DestAddr.getAlignment());\n    // The updated value of the base pointer is returned.\n    return Builder.CreateExtractValue(Result, 1);\n  };\n\n  auto V2Q = [this, VecLen] (llvm::Value *Vec) {\n    Intrinsic::ID ID = VecLen == 128 ? Intrinsic::hexagon_V6_vandvrt_128B\n                                     : Intrinsic::hexagon_V6_vandvrt;\n    return Builder.CreateCall(CGM.getIntrinsic(ID),\n                              {Vec, Builder.getInt32(-1)});\n  };\n  auto Q2V = [this, VecLen] (llvm::Value *Pred) {\n    Intrinsic::ID ID = VecLen == 128 ? Intrinsic::hexagon_V6_vandqrt_128B\n                                     : Intrinsic::hexagon_V6_vandqrt;\n    return Builder.CreateCall(CGM.getIntrinsic(ID),\n                              {Pred, Builder.getInt32(-1)});\n  };\n\n  switch (BuiltinID) {\n  // These intrinsics return a tuple {Vector, VectorPred} in LLVM IR,\n  // and the corresponding C/C++ builtins use loads/stores to update\n  // the predicate.\n  case Hexagon::BI__builtin_HEXAGON_V6_vaddcarry:\n  case Hexagon::BI__builtin_HEXAGON_V6_vaddcarry_128B:\n  case Hexagon::BI__builtin_HEXAGON_V6_vsubcarry:\n  case Hexagon::BI__builtin_HEXAGON_V6_vsubcarry_128B: {\n    // Get the type from the 0-th argument.\n    llvm::Type *VecType = ConvertType(E->getArg(0)->getType());\n    Address PredAddr = Builder.CreateBitCast(\n        EmitPointerWithAlignment(E->getArg(2)), VecType->getPointerTo(0));\n    llvm::Value *PredIn = V2Q(Builder.CreateLoad(PredAddr));\n    llvm::Value *Result = Builder.CreateCall(CGM.getIntrinsic(ID),\n        {EmitScalarExpr(E->getArg(0)), EmitScalarExpr(E->getArg(1)), PredIn});\n\n    llvm::Value *PredOut = Builder.CreateExtractValue(Result, 1);\n    Builder.CreateAlignedStore(Q2V(PredOut), PredAddr.getPointer(),\n        PredAddr.getAlignment());\n    return Builder.CreateExtractValue(Result, 0);\n  }\n\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrub_pci:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrb_pci:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadruh_pci:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrh_pci:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadri_pci:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrd_pci:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrub_pcr:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrb_pcr:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadruh_pcr:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrh_pcr:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadri_pcr:\n  case Hexagon::BI__builtin_HEXAGON_L2_loadrd_pcr:\n    return MakeCircOp(ID, /*IsLoad=*/true);\n  case Hexagon::BI__builtin_HEXAGON_S2_storerb_pci:\n  case Hexagon::BI__builtin_HEXAGON_S2_storerh_pci:\n  case Hexagon::BI__builtin_HEXAGON_S2_storerf_pci:\n  case Hexagon::BI__builtin_HEXAGON_S2_storeri_pci:\n  case Hexagon::BI__builtin_HEXAGON_S2_storerd_pci:\n  case Hexagon::BI__builtin_HEXAGON_S2_storerb_pcr:\n  case Hexagon::BI__builtin_HEXAGON_S2_storerh_pcr:\n  case Hexagon::BI__builtin_HEXAGON_S2_storerf_pcr:\n  case Hexagon::BI__builtin_HEXAGON_S2_storeri_pcr:\n  case Hexagon::BI__builtin_HEXAGON_S2_storerd_pcr:\n    return MakeCircOp(ID, /*IsLoad=*/false);\n  case Hexagon::BI__builtin_brev_ldub:\n    return MakeBrevLd(Intrinsic::hexagon_L2_loadrub_pbr, Int8Ty);\n  case Hexagon::BI__builtin_brev_ldb:\n    return MakeBrevLd(Intrinsic::hexagon_L2_loadrb_pbr, Int8Ty);\n  case Hexagon::BI__builtin_brev_lduh:\n    return MakeBrevLd(Intrinsic::hexagon_L2_loadruh_pbr, Int16Ty);\n  case Hexagon::BI__builtin_brev_ldh:\n    return MakeBrevLd(Intrinsic::hexagon_L2_loadrh_pbr, Int16Ty);\n  case Hexagon::BI__builtin_brev_ldw:\n    return MakeBrevLd(Intrinsic::hexagon_L2_loadri_pbr, Int32Ty);\n  case Hexagon::BI__builtin_brev_ldd:\n    return MakeBrevLd(Intrinsic::hexagon_L2_loadrd_pbr, Int64Ty);\n\n  default: {\n    if (ID == Intrinsic::not_intrinsic)\n      return nullptr;\n\n    auto IsVectorPredTy = [](llvm::Type *T) {\n      return T->isVectorTy() &&\n             cast<llvm::VectorType>(T)->getElementType()->isIntegerTy(1);\n    };\n\n    llvm::Function *IntrFn = CGM.getIntrinsic(ID);\n    llvm::FunctionType *IntrTy = IntrFn->getFunctionType();\n    SmallVector<llvm::Value*,4> Ops;\n    for (unsigned i = 0, e = IntrTy->getNumParams(); i != e; ++i) {\n      llvm::Type *T = IntrTy->getParamType(i);\n      const Expr *A = E->getArg(i);\n      if (IsVectorPredTy(T)) {\n        // There will be an implicit cast to a boolean vector. Strip it.\n        if (auto *Cast = dyn_cast<ImplicitCastExpr>(A)) {\n          if (Cast->getCastKind() == CK_BitCast)\n            A = Cast->getSubExpr();\n        }\n        Ops.push_back(V2Q(EmitScalarExpr(A)));\n      } else {\n        Ops.push_back(EmitScalarExpr(A));\n      }\n    }\n\n    llvm::Value *Call = Builder.CreateCall(IntrFn, Ops);\n    if (IsVectorPredTy(IntrTy->getReturnType()))\n      Call = Q2V(Call);\n\n    return Call;\n  } // default\n  } // switch\n\n  return nullptr;\n}\n"}, "0": {"id": 0, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "content": "//===-- CodeGenFunction.h - Per-Function state for LLVM CodeGen -*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This is the internal per-function state used for llvm translation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CODEGENFUNCTION_H\n#define LLVM_CLANG_LIB_CODEGEN_CODEGENFUNCTION_H\n\n#include \"CGBuilder.h\"\n#include \"CGDebugInfo.h\"\n#include \"CGLoopInfo.h\"\n#include \"CGValue.h\"\n#include \"CodeGenModule.h\"\n#include \"CodeGenPGO.h\"\n#include \"EHScopeStack.h\"\n#include \"VarBypassDetector.h\"\n#include \"clang/AST/CharUnits.h\"\n#include \"clang/AST/CurrentSourceLocExprScope.h\"\n#include \"clang/AST/ExprCXX.h\"\n#include \"clang/AST/ExprObjC.h\"\n#include \"clang/AST/ExprOpenMP.h\"\n#include \"clang/AST/StmtOpenMP.h\"\n#include \"clang/AST/Type.h\"\n#include \"clang/Basic/ABI.h\"\n#include \"clang/Basic/CapturedStmt.h\"\n#include \"clang/Basic/CodeGenOptions.h\"\n#include \"clang/Basic/OpenMPKinds.h\"\n#include \"clang/Basic/TargetInfo.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/MapVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/Frontend/OpenMP/OMPIRBuilder.h\"\n#include \"llvm/IR/ValueHandle.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Transforms/Utils/SanitizerStats.h\"\n\nnamespace llvm {\nclass BasicBlock;\nclass LLVMContext;\nclass MDNode;\nclass Module;\nclass SwitchInst;\nclass Twine;\nclass Value;\n}\n\nnamespace clang {\nclass ASTContext;\nclass BlockDecl;\nclass CXXDestructorDecl;\nclass CXXForRangeStmt;\nclass CXXTryStmt;\nclass Decl;\nclass LabelDecl;\nclass EnumConstantDecl;\nclass FunctionDecl;\nclass FunctionProtoType;\nclass LabelStmt;\nclass ObjCContainerDecl;\nclass ObjCInterfaceDecl;\nclass ObjCIvarDecl;\nclass ObjCMethodDecl;\nclass ObjCImplementationDecl;\nclass ObjCPropertyImplDecl;\nclass TargetInfo;\nclass VarDecl;\nclass ObjCForCollectionStmt;\nclass ObjCAtTryStmt;\nclass ObjCAtThrowStmt;\nclass ObjCAtSynchronizedStmt;\nclass ObjCAutoreleasePoolStmt;\nclass OMPUseDevicePtrClause;\nclass OMPUseDeviceAddrClause;\nclass ReturnsNonNullAttr;\nclass SVETypeFlags;\nclass OMPExecutableDirective;\n\nnamespace analyze_os_log {\nclass OSLogBufferLayout;\n}\n\nnamespace CodeGen {\nclass CodeGenTypes;\nclass CGCallee;\nclass CGFunctionInfo;\nclass CGRecordLayout;\nclass CGBlockInfo;\nclass CGCXXABI;\nclass BlockByrefHelpers;\nclass BlockByrefInfo;\nclass BlockFlags;\nclass BlockFieldFlags;\nclass RegionCodeGenTy;\nclass TargetCodeGenInfo;\nstruct OMPTaskDataTy;\nstruct CGCoroData;\n\n/// The kind of evaluation to perform on values of a particular\n/// type.  Basically, is the code in CGExprScalar, CGExprComplex, or\n/// CGExprAgg?\n///\n/// TODO: should vectors maybe be split out into their own thing?\nenum TypeEvaluationKind {\n  TEK_Scalar,\n  TEK_Complex,\n  TEK_Aggregate\n};\n\n#define LIST_SANITIZER_CHECKS                                                  \\\n  SANITIZER_CHECK(AddOverflow, add_overflow, 0)                                \\\n  SANITIZER_CHECK(BuiltinUnreachable, builtin_unreachable, 0)                  \\\n  SANITIZER_CHECK(CFICheckFail, cfi_check_fail, 0)                             \\\n  SANITIZER_CHECK(DivremOverflow, divrem_overflow, 0)                          \\\n  SANITIZER_CHECK(DynamicTypeCacheMiss, dynamic_type_cache_miss, 0)            \\\n  SANITIZER_CHECK(FloatCastOverflow, float_cast_overflow, 0)                   \\\n  SANITIZER_CHECK(FunctionTypeMismatch, function_type_mismatch, 1)             \\\n  SANITIZER_CHECK(ImplicitConversion, implicit_conversion, 0)                  \\\n  SANITIZER_CHECK(InvalidBuiltin, invalid_builtin, 0)                          \\\n  SANITIZER_CHECK(InvalidObjCCast, invalid_objc_cast, 0)                       \\\n  SANITIZER_CHECK(LoadInvalidValue, load_invalid_value, 0)                     \\\n  SANITIZER_CHECK(MissingReturn, missing_return, 0)                            \\\n  SANITIZER_CHECK(MulOverflow, mul_overflow, 0)                                \\\n  SANITIZER_CHECK(NegateOverflow, negate_overflow, 0)                          \\\n  SANITIZER_CHECK(NullabilityArg, nullability_arg, 0)                          \\\n  SANITIZER_CHECK(NullabilityReturn, nullability_return, 1)                    \\\n  SANITIZER_CHECK(NonnullArg, nonnull_arg, 0)                                  \\\n  SANITIZER_CHECK(NonnullReturn, nonnull_return, 1)                            \\\n  SANITIZER_CHECK(OutOfBounds, out_of_bounds, 0)                               \\\n  SANITIZER_CHECK(PointerOverflow, pointer_overflow, 0)                        \\\n  SANITIZER_CHECK(ShiftOutOfBounds, shift_out_of_bounds, 0)                    \\\n  SANITIZER_CHECK(SubOverflow, sub_overflow, 0)                                \\\n  SANITIZER_CHECK(TypeMismatch, type_mismatch, 1)                              \\\n  SANITIZER_CHECK(AlignmentAssumption, alignment_assumption, 0)                \\\n  SANITIZER_CHECK(VLABoundNotPositive, vla_bound_not_positive, 0)\n\nenum SanitizerHandler {\n#define SANITIZER_CHECK(Enum, Name, Version) Enum,\n  LIST_SANITIZER_CHECKS\n#undef SANITIZER_CHECK\n};\n\n/// Helper class with most of the code for saving a value for a\n/// conditional expression cleanup.\nstruct DominatingLLVMValue {\n  typedef llvm::PointerIntPair<llvm::Value*, 1, bool> saved_type;\n\n  /// Answer whether the given value needs extra work to be saved.\n  static bool needsSaving(llvm::Value *value) {\n    // If it's not an instruction, we don't need to save.\n    if (!isa<llvm::Instruction>(value)) return false;\n\n    // If it's an instruction in the entry block, we don't need to save.\n    llvm::BasicBlock *block = cast<llvm::Instruction>(value)->getParent();\n    return (block != &block->getParent()->getEntryBlock());\n  }\n\n  static saved_type save(CodeGenFunction &CGF, llvm::Value *value);\n  static llvm::Value *restore(CodeGenFunction &CGF, saved_type value);\n};\n\n/// A partial specialization of DominatingValue for llvm::Values that\n/// might be llvm::Instructions.\ntemplate <class T> struct DominatingPointer<T,true> : DominatingLLVMValue {\n  typedef T *type;\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return static_cast<T*>(DominatingLLVMValue::restore(CGF, value));\n  }\n};\n\n/// A specialization of DominatingValue for Address.\ntemplate <> struct DominatingValue<Address> {\n  typedef Address type;\n\n  struct saved_type {\n    DominatingLLVMValue::saved_type SavedValue;\n    CharUnits Alignment;\n  };\n\n  static bool needsSaving(type value) {\n    return DominatingLLVMValue::needsSaving(value.getPointer());\n  }\n  static saved_type save(CodeGenFunction &CGF, type value) {\n    return { DominatingLLVMValue::save(CGF, value.getPointer()),\n             value.getAlignment() };\n  }\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return Address(DominatingLLVMValue::restore(CGF, value.SavedValue),\n                   value.Alignment);\n  }\n};\n\n/// A specialization of DominatingValue for RValue.\ntemplate <> struct DominatingValue<RValue> {\n  typedef RValue type;\n  class saved_type {\n    enum Kind { ScalarLiteral, ScalarAddress, AggregateLiteral,\n                AggregateAddress, ComplexAddress };\n\n    llvm::Value *Value;\n    unsigned K : 3;\n    unsigned Align : 29;\n    saved_type(llvm::Value *v, Kind k, unsigned a = 0)\n      : Value(v), K(k), Align(a) {}\n\n  public:\n    static bool needsSaving(RValue value);\n    static saved_type save(CodeGenFunction &CGF, RValue value);\n    RValue restore(CodeGenFunction &CGF);\n\n    // implementations in CGCleanup.cpp\n  };\n\n  static bool needsSaving(type value) {\n    return saved_type::needsSaving(value);\n  }\n  static saved_type save(CodeGenFunction &CGF, type value) {\n    return saved_type::save(CGF, value);\n  }\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return value.restore(CGF);\n  }\n};\n\n/// CodeGenFunction - This class organizes the per-function state that is used\n/// while generating LLVM code.\nclass CodeGenFunction : public CodeGenTypeCache {\n  CodeGenFunction(const CodeGenFunction &) = delete;\n  void operator=(const CodeGenFunction &) = delete;\n\n  friend class CGCXXABI;\npublic:\n  /// A jump destination is an abstract label, branching to which may\n  /// require a jump out through normal cleanups.\n  struct JumpDest {\n    JumpDest() : Block(nullptr), ScopeDepth(), Index(0) {}\n    JumpDest(llvm::BasicBlock *Block,\n             EHScopeStack::stable_iterator Depth,\n             unsigned Index)\n      : Block(Block), ScopeDepth(Depth), Index(Index) {}\n\n    bool isValid() const { return Block != nullptr; }\n    llvm::BasicBlock *getBlock() const { return Block; }\n    EHScopeStack::stable_iterator getScopeDepth() const { return ScopeDepth; }\n    unsigned getDestIndex() const { return Index; }\n\n    // This should be used cautiously.\n    void setScopeDepth(EHScopeStack::stable_iterator depth) {\n      ScopeDepth = depth;\n    }\n\n  private:\n    llvm::BasicBlock *Block;\n    EHScopeStack::stable_iterator ScopeDepth;\n    unsigned Index;\n  };\n\n  CodeGenModule &CGM;  // Per-module state.\n  const TargetInfo &Target;\n\n  // For EH/SEH outlined funclets, this field points to parent's CGF\n  CodeGenFunction *ParentCGF = nullptr;\n\n  typedef std::pair<llvm::Value *, llvm::Value *> ComplexPairTy;\n  LoopInfoStack LoopStack;\n  CGBuilderTy Builder;\n\n  // Stores variables for which we can't generate correct lifetime markers\n  // because of jumps.\n  VarBypassDetector Bypasses;\n\n  // CodeGen lambda for loops and support for ordered clause\n  typedef llvm::function_ref<void(CodeGenFunction &, const OMPLoopDirective &,\n                                  JumpDest)>\n      CodeGenLoopTy;\n  typedef llvm::function_ref<void(CodeGenFunction &, SourceLocation,\n                                  const unsigned, const bool)>\n      CodeGenOrderedTy;\n\n  // Codegen lambda for loop bounds in worksharing loop constructs\n  typedef llvm::function_ref<std::pair<LValue, LValue>(\n      CodeGenFunction &, const OMPExecutableDirective &S)>\n      CodeGenLoopBoundsTy;\n\n  // Codegen lambda for loop bounds in dispatch-based loop implementation\n  typedef llvm::function_ref<std::pair<llvm::Value *, llvm::Value *>(\n      CodeGenFunction &, const OMPExecutableDirective &S, Address LB,\n      Address UB)>\n      CodeGenDispatchBoundsTy;\n\n  /// CGBuilder insert helper. This function is called after an\n  /// instruction is created using Builder.\n  void InsertHelper(llvm::Instruction *I, const llvm::Twine &Name,\n                    llvm::BasicBlock *BB,\n                    llvm::BasicBlock::iterator InsertPt) const;\n\n  /// CurFuncDecl - Holds the Decl for the current outermost\n  /// non-closure context.\n  const Decl *CurFuncDecl;\n  /// CurCodeDecl - This is the inner-most code context, which includes blocks.\n  const Decl *CurCodeDecl;\n  const CGFunctionInfo *CurFnInfo;\n  QualType FnRetTy;\n  llvm::Function *CurFn = nullptr;\n\n  // Holds coroutine data if the current function is a coroutine. We use a\n  // wrapper to manage its lifetime, so that we don't have to define CGCoroData\n  // in this header.\n  struct CGCoroInfo {\n    std::unique_ptr<CGCoroData> Data;\n    CGCoroInfo();\n    ~CGCoroInfo();\n  };\n  CGCoroInfo CurCoro;\n\n  bool isCoroutine() const {\n    return CurCoro.Data != nullptr;\n  }\n\n  /// CurGD - The GlobalDecl for the current function being compiled.\n  GlobalDecl CurGD;\n\n  /// PrologueCleanupDepth - The cleanup depth enclosing all the\n  /// cleanups associated with the parameters.\n  EHScopeStack::stable_iterator PrologueCleanupDepth;\n\n  /// ReturnBlock - Unified return block.\n  JumpDest ReturnBlock;\n\n  /// ReturnValue - The temporary alloca to hold the return\n  /// value. This is invalid iff the function has no return value.\n  Address ReturnValue = Address::invalid();\n\n  /// ReturnValuePointer - The temporary alloca to hold a pointer to sret.\n  /// This is invalid if sret is not in use.\n  Address ReturnValuePointer = Address::invalid();\n\n  /// If a return statement is being visited, this holds the return statment's\n  /// result expression.\n  const Expr *RetExpr = nullptr;\n\n  /// Return true if a label was seen in the current scope.\n  bool hasLabelBeenSeenInCurrentScope() const {\n    if (CurLexicalScope)\n      return CurLexicalScope->hasLabels();\n    return !LabelMap.empty();\n  }\n\n  /// AllocaInsertPoint - This is an instruction in the entry block before which\n  /// we prefer to insert allocas.\n  llvm::AssertingVH<llvm::Instruction> AllocaInsertPt;\n\n  /// API for captured statement code generation.\n  class CGCapturedStmtInfo {\n  public:\n    explicit CGCapturedStmtInfo(CapturedRegionKind K = CR_Default)\n        : Kind(K), ThisValue(nullptr), CXXThisFieldDecl(nullptr) {}\n    explicit CGCapturedStmtInfo(const CapturedStmt &S,\n                                CapturedRegionKind K = CR_Default)\n      : Kind(K), ThisValue(nullptr), CXXThisFieldDecl(nullptr) {\n\n      RecordDecl::field_iterator Field =\n        S.getCapturedRecordDecl()->field_begin();\n      for (CapturedStmt::const_capture_iterator I = S.capture_begin(),\n                                                E = S.capture_end();\n           I != E; ++I, ++Field) {\n        if (I->capturesThis())\n          CXXThisFieldDecl = *Field;\n        else if (I->capturesVariable())\n          CaptureFields[I->getCapturedVar()->getCanonicalDecl()] = *Field;\n        else if (I->capturesVariableByCopy())\n          CaptureFields[I->getCapturedVar()->getCanonicalDecl()] = *Field;\n      }\n    }\n\n    virtual ~CGCapturedStmtInfo();\n\n    CapturedRegionKind getKind() const { return Kind; }\n\n    virtual void setContextValue(llvm::Value *V) { ThisValue = V; }\n    // Retrieve the value of the context parameter.\n    virtual llvm::Value *getContextValue() const { return ThisValue; }\n\n    /// Lookup the captured field decl for a variable.\n    virtual const FieldDecl *lookup(const VarDecl *VD) const {\n      return CaptureFields.lookup(VD->getCanonicalDecl());\n    }\n\n    bool isCXXThisExprCaptured() const { return getThisFieldDecl() != nullptr; }\n    virtual FieldDecl *getThisFieldDecl() const { return CXXThisFieldDecl; }\n\n    static bool classof(const CGCapturedStmtInfo *) {\n      return true;\n    }\n\n    /// Emit the captured statement body.\n    virtual void EmitBody(CodeGenFunction &CGF, const Stmt *S) {\n      CGF.incrementProfileCounter(S);\n      CGF.EmitStmt(S);\n    }\n\n    /// Get the name of the capture helper.\n    virtual StringRef getHelperName() const { return \"__captured_stmt\"; }\n\n  private:\n    /// The kind of captured statement being generated.\n    CapturedRegionKind Kind;\n\n    /// Keep the map between VarDecl and FieldDecl.\n    llvm::SmallDenseMap<const VarDecl *, FieldDecl *> CaptureFields;\n\n    /// The base address of the captured record, passed in as the first\n    /// argument of the parallel region function.\n    llvm::Value *ThisValue;\n\n    /// Captured 'this' type.\n    FieldDecl *CXXThisFieldDecl;\n  };\n  CGCapturedStmtInfo *CapturedStmtInfo = nullptr;\n\n  /// RAII for correct setting/restoring of CapturedStmtInfo.\n  class CGCapturedStmtRAII {\n  private:\n    CodeGenFunction &CGF;\n    CGCapturedStmtInfo *PrevCapturedStmtInfo;\n  public:\n    CGCapturedStmtRAII(CodeGenFunction &CGF,\n                       CGCapturedStmtInfo *NewCapturedStmtInfo)\n        : CGF(CGF), PrevCapturedStmtInfo(CGF.CapturedStmtInfo) {\n      CGF.CapturedStmtInfo = NewCapturedStmtInfo;\n    }\n    ~CGCapturedStmtRAII() { CGF.CapturedStmtInfo = PrevCapturedStmtInfo; }\n  };\n\n  /// An abstract representation of regular/ObjC call/message targets.\n  class AbstractCallee {\n    /// The function declaration of the callee.\n    const Decl *CalleeDecl;\n\n  public:\n    AbstractCallee() : CalleeDecl(nullptr) {}\n    AbstractCallee(const FunctionDecl *FD) : CalleeDecl(FD) {}\n    AbstractCallee(const ObjCMethodDecl *OMD) : CalleeDecl(OMD) {}\n    bool hasFunctionDecl() const {\n      return dyn_cast_or_null<FunctionDecl>(CalleeDecl);\n    }\n    const Decl *getDecl() const { return CalleeDecl; }\n    unsigned getNumParams() const {\n      if (const auto *FD = dyn_cast<FunctionDecl>(CalleeDecl))\n        return FD->getNumParams();\n      return cast<ObjCMethodDecl>(CalleeDecl)->param_size();\n    }\n    const ParmVarDecl *getParamDecl(unsigned I) const {\n      if (const auto *FD = dyn_cast<FunctionDecl>(CalleeDecl))\n        return FD->getParamDecl(I);\n      return *(cast<ObjCMethodDecl>(CalleeDecl)->param_begin() + I);\n    }\n  };\n\n  /// Sanitizers enabled for this function.\n  SanitizerSet SanOpts;\n\n  /// True if CodeGen currently emits code implementing sanitizer checks.\n  bool IsSanitizerScope = false;\n\n  /// RAII object to set/unset CodeGenFunction::IsSanitizerScope.\n  class SanitizerScope {\n    CodeGenFunction *CGF;\n  public:\n    SanitizerScope(CodeGenFunction *CGF);\n    ~SanitizerScope();\n  };\n\n  /// In C++, whether we are code generating a thunk.  This controls whether we\n  /// should emit cleanups.\n  bool CurFuncIsThunk = false;\n\n  /// In ARC, whether we should autorelease the return value.\n  bool AutoreleaseResult = false;\n\n  /// Whether we processed a Microsoft-style asm block during CodeGen. These can\n  /// potentially set the return value.\n  bool SawAsmBlock = false;\n\n  const NamedDecl *CurSEHParent = nullptr;\n\n  /// True if the current function is an outlined SEH helper. This can be a\n  /// finally block or filter expression.\n  bool IsOutlinedSEHHelper = false;\n\n  /// True if CodeGen currently emits code inside presereved access index\n  /// region.\n  bool IsInPreservedAIRegion = false;\n\n  /// True if the current statement has nomerge attribute.\n  bool InNoMergeAttributedStmt = false;\n\n  /// True if the current function should be marked mustprogress.\n  bool FnIsMustProgress = false;\n\n  /// True if the C++ Standard Requires Progress.\n  bool CPlusPlusWithProgress() {\n    return getLangOpts().CPlusPlus11 || getLangOpts().CPlusPlus14 ||\n           getLangOpts().CPlusPlus17 || getLangOpts().CPlusPlus20;\n  }\n\n  /// True if the C Standard Requires Progress.\n  bool CWithProgress() {\n    return getLangOpts().C11 || getLangOpts().C17 || getLangOpts().C2x;\n  }\n\n  /// True if the language standard requires progress in functions or\n  /// in infinite loops with non-constant conditionals.\n  bool LanguageRequiresProgress() {\n    return CWithProgress() || CPlusPlusWithProgress();\n  }\n\n  const CodeGen::CGBlockInfo *BlockInfo = nullptr;\n  llvm::Value *BlockPointer = nullptr;\n\n  llvm::DenseMap<const VarDecl *, FieldDecl *> LambdaCaptureFields;\n  FieldDecl *LambdaThisCaptureField = nullptr;\n\n  /// A mapping from NRVO variables to the flags used to indicate\n  /// when the NRVO has been applied to this variable.\n  llvm::DenseMap<const VarDecl *, llvm::Value *> NRVOFlags;\n\n  EHScopeStack EHStack;\n  llvm::SmallVector<char, 256> LifetimeExtendedCleanupStack;\n  llvm::SmallVector<const JumpDest *, 2> SEHTryEpilogueStack;\n\n  llvm::Instruction *CurrentFuncletPad = nullptr;\n\n  class CallLifetimeEnd final : public EHScopeStack::Cleanup {\n    llvm::Value *Addr;\n    llvm::Value *Size;\n\n  public:\n    CallLifetimeEnd(Address addr, llvm::Value *size)\n        : Addr(addr.getPointer()), Size(size) {}\n\n    void Emit(CodeGenFunction &CGF, Flags flags) override {\n      CGF.EmitLifetimeEnd(Size, Addr);\n    }\n  };\n\n  /// Header for data within LifetimeExtendedCleanupStack.\n  struct LifetimeExtendedCleanupHeader {\n    /// The size of the following cleanup object.\n    unsigned Size;\n    /// The kind of cleanup to push: a value from the CleanupKind enumeration.\n    unsigned Kind : 31;\n    /// Whether this is a conditional cleanup.\n    unsigned IsConditional : 1;\n\n    size_t getSize() const { return Size; }\n    CleanupKind getKind() const { return (CleanupKind)Kind; }\n    bool isConditional() const { return IsConditional; }\n  };\n\n  /// i32s containing the indexes of the cleanup destinations.\n  Address NormalCleanupDest = Address::invalid();\n\n  unsigned NextCleanupDestIndex = 1;\n\n  /// EHResumeBlock - Unified block containing a call to llvm.eh.resume.\n  llvm::BasicBlock *EHResumeBlock = nullptr;\n\n  /// The exception slot.  All landing pads write the current exception pointer\n  /// into this alloca.\n  llvm::Value *ExceptionSlot = nullptr;\n\n  /// The selector slot.  Under the MandatoryCleanup model, all landing pads\n  /// write the current selector value into this alloca.\n  llvm::AllocaInst *EHSelectorSlot = nullptr;\n\n  /// A stack of exception code slots. Entering an __except block pushes a slot\n  /// on the stack and leaving pops one. The __exception_code() intrinsic loads\n  /// a value from the top of the stack.\n  SmallVector<Address, 1> SEHCodeSlotStack;\n\n  /// Value returned by __exception_info intrinsic.\n  llvm::Value *SEHInfo = nullptr;\n\n  /// Emits a landing pad for the current EH stack.\n  llvm::BasicBlock *EmitLandingPad();\n\n  llvm::BasicBlock *getInvokeDestImpl();\n\n  /// Parent loop-based directive for scan directive.\n  const OMPExecutableDirective *OMPParentLoopDirectiveForScan = nullptr;\n  llvm::BasicBlock *OMPBeforeScanBlock = nullptr;\n  llvm::BasicBlock *OMPAfterScanBlock = nullptr;\n  llvm::BasicBlock *OMPScanExitBlock = nullptr;\n  llvm::BasicBlock *OMPScanDispatch = nullptr;\n  bool OMPFirstScanLoop = false;\n\n  /// Manages parent directive for scan directives.\n  class ParentLoopDirectiveForScanRegion {\n    CodeGenFunction &CGF;\n    const OMPExecutableDirective *ParentLoopDirectiveForScan;\n\n  public:\n    ParentLoopDirectiveForScanRegion(\n        CodeGenFunction &CGF,\n        const OMPExecutableDirective &ParentLoopDirectiveForScan)\n        : CGF(CGF),\n          ParentLoopDirectiveForScan(CGF.OMPParentLoopDirectiveForScan) {\n      CGF.OMPParentLoopDirectiveForScan = &ParentLoopDirectiveForScan;\n    }\n    ~ParentLoopDirectiveForScanRegion() {\n      CGF.OMPParentLoopDirectiveForScan = ParentLoopDirectiveForScan;\n    }\n  };\n\n  template <class T>\n  typename DominatingValue<T>::saved_type saveValueInCond(T value) {\n    return DominatingValue<T>::save(*this, value);\n  }\n\n  class CGFPOptionsRAII {\n  public:\n    CGFPOptionsRAII(CodeGenFunction &CGF, FPOptions FPFeatures);\n    CGFPOptionsRAII(CodeGenFunction &CGF, const Expr *E);\n    ~CGFPOptionsRAII();\n\n  private:\n    void ConstructorHelper(FPOptions FPFeatures);\n    CodeGenFunction &CGF;\n    FPOptions OldFPFeatures;\n    llvm::fp::ExceptionBehavior OldExcept;\n    llvm::RoundingMode OldRounding;\n    Optional<CGBuilderTy::FastMathFlagGuard> FMFGuard;\n  };\n  FPOptions CurFPFeatures;\n\npublic:\n  /// ObjCEHValueStack - Stack of Objective-C exception values, used for\n  /// rethrows.\n  SmallVector<llvm::Value*, 8> ObjCEHValueStack;\n\n  /// A class controlling the emission of a finally block.\n  class FinallyInfo {\n    /// Where the catchall's edge through the cleanup should go.\n    JumpDest RethrowDest;\n\n    /// A function to call to enter the catch.\n    llvm::FunctionCallee BeginCatchFn;\n\n    /// An i1 variable indicating whether or not the @finally is\n    /// running for an exception.\n    llvm::AllocaInst *ForEHVar;\n\n    /// An i8* variable into which the exception pointer to rethrow\n    /// has been saved.\n    llvm::AllocaInst *SavedExnVar;\n\n  public:\n    void enter(CodeGenFunction &CGF, const Stmt *Finally,\n               llvm::FunctionCallee beginCatchFn,\n               llvm::FunctionCallee endCatchFn, llvm::FunctionCallee rethrowFn);\n    void exit(CodeGenFunction &CGF);\n  };\n\n  /// Returns true inside SEH __try blocks.\n  bool isSEHTryScope() const { return !SEHTryEpilogueStack.empty(); }\n\n  /// Returns true while emitting a cleanuppad.\n  bool isCleanupPadScope() const {\n    return CurrentFuncletPad && isa<llvm::CleanupPadInst>(CurrentFuncletPad);\n  }\n\n  /// pushFullExprCleanup - Push a cleanup to be run at the end of the\n  /// current full-expression.  Safe against the possibility that\n  /// we're currently inside a conditionally-evaluated expression.\n  template <class T, class... As>\n  void pushFullExprCleanup(CleanupKind kind, As... A) {\n    // If we're not in a conditional branch, or if none of the\n    // arguments requires saving, then use the unconditional cleanup.\n    if (!isInConditionalBranch())\n      return EHStack.pushCleanup<T>(kind, A...);\n\n    // Stash values in a tuple so we can guarantee the order of saves.\n    typedef std::tuple<typename DominatingValue<As>::saved_type...> SavedTuple;\n    SavedTuple Saved{saveValueInCond(A)...};\n\n    typedef EHScopeStack::ConditionalCleanup<T, As...> CleanupType;\n    EHStack.pushCleanupTuple<CleanupType>(kind, Saved);\n    initFullExprCleanup();\n  }\n\n  /// Queue a cleanup to be pushed after finishing the current full-expression,\n  /// potentially with an active flag.\n  template <class T, class... As>\n  void pushCleanupAfterFullExpr(CleanupKind Kind, As... A) {\n    if (!isInConditionalBranch())\n      return pushCleanupAfterFullExprWithActiveFlag<T>(Kind, Address::invalid(),\n                                                       A...);\n\n    Address ActiveFlag = createCleanupActiveFlag();\n    assert(!DominatingValue<Address>::needsSaving(ActiveFlag) &&\n           \"cleanup active flag should never need saving\");\n\n    typedef std::tuple<typename DominatingValue<As>::saved_type...> SavedTuple;\n    SavedTuple Saved{saveValueInCond(A)...};\n\n    typedef EHScopeStack::ConditionalCleanup<T, As...> CleanupType;\n    pushCleanupAfterFullExprWithActiveFlag<CleanupType>(Kind, ActiveFlag, Saved);\n  }\n\n  template <class T, class... As>\n  void pushCleanupAfterFullExprWithActiveFlag(CleanupKind Kind,\n                                              Address ActiveFlag, As... A) {\n    LifetimeExtendedCleanupHeader Header = {sizeof(T), Kind,\n                                            ActiveFlag.isValid()};\n\n    size_t OldSize = LifetimeExtendedCleanupStack.size();\n    LifetimeExtendedCleanupStack.resize(\n        LifetimeExtendedCleanupStack.size() + sizeof(Header) + Header.Size +\n        (Header.IsConditional ? sizeof(ActiveFlag) : 0));\n\n    static_assert(sizeof(Header) % alignof(T) == 0,\n                  \"Cleanup will be allocated on misaligned address\");\n    char *Buffer = &LifetimeExtendedCleanupStack[OldSize];\n    new (Buffer) LifetimeExtendedCleanupHeader(Header);\n    new (Buffer + sizeof(Header)) T(A...);\n    if (Header.IsConditional)\n      new (Buffer + sizeof(Header) + sizeof(T)) Address(ActiveFlag);\n  }\n\n  /// Set up the last cleanup that was pushed as a conditional\n  /// full-expression cleanup.\n  void initFullExprCleanup() {\n    initFullExprCleanupWithFlag(createCleanupActiveFlag());\n  }\n\n  void initFullExprCleanupWithFlag(Address ActiveFlag);\n  Address createCleanupActiveFlag();\n\n  /// PushDestructorCleanup - Push a cleanup to call the\n  /// complete-object destructor of an object of the given type at the\n  /// given address.  Does nothing if T is not a C++ class type with a\n  /// non-trivial destructor.\n  void PushDestructorCleanup(QualType T, Address Addr);\n\n  /// PushDestructorCleanup - Push a cleanup to call the\n  /// complete-object variant of the given destructor on the object at\n  /// the given address.\n  void PushDestructorCleanup(const CXXDestructorDecl *Dtor, QualType T,\n                             Address Addr);\n\n  /// PopCleanupBlock - Will pop the cleanup entry on the stack and\n  /// process all branch fixups.\n  void PopCleanupBlock(bool FallThroughIsBranchThrough = false);\n\n  /// DeactivateCleanupBlock - Deactivates the given cleanup block.\n  /// The block cannot be reactivated.  Pops it if it's the top of the\n  /// stack.\n  ///\n  /// \\param DominatingIP - An instruction which is known to\n  ///   dominate the current IP (if set) and which lies along\n  ///   all paths of execution between the current IP and the\n  ///   the point at which the cleanup comes into scope.\n  void DeactivateCleanupBlock(EHScopeStack::stable_iterator Cleanup,\n                              llvm::Instruction *DominatingIP);\n\n  /// ActivateCleanupBlock - Activates an initially-inactive cleanup.\n  /// Cannot be used to resurrect a deactivated cleanup.\n  ///\n  /// \\param DominatingIP - An instruction which is known to\n  ///   dominate the current IP (if set) and which lies along\n  ///   all paths of execution between the current IP and the\n  ///   the point at which the cleanup comes into scope.\n  void ActivateCleanupBlock(EHScopeStack::stable_iterator Cleanup,\n                            llvm::Instruction *DominatingIP);\n\n  /// Enters a new scope for capturing cleanups, all of which\n  /// will be executed once the scope is exited.\n  class RunCleanupsScope {\n    EHScopeStack::stable_iterator CleanupStackDepth, OldCleanupScopeDepth;\n    size_t LifetimeExtendedCleanupStackSize;\n    bool OldDidCallStackSave;\n  protected:\n    bool PerformCleanup;\n  private:\n\n    RunCleanupsScope(const RunCleanupsScope &) = delete;\n    void operator=(const RunCleanupsScope &) = delete;\n\n  protected:\n    CodeGenFunction& CGF;\n\n  public:\n    /// Enter a new cleanup scope.\n    explicit RunCleanupsScope(CodeGenFunction &CGF)\n      : PerformCleanup(true), CGF(CGF)\n    {\n      CleanupStackDepth = CGF.EHStack.stable_begin();\n      LifetimeExtendedCleanupStackSize =\n          CGF.LifetimeExtendedCleanupStack.size();\n      OldDidCallStackSave = CGF.DidCallStackSave;\n      CGF.DidCallStackSave = false;\n      OldCleanupScopeDepth = CGF.CurrentCleanupScopeDepth;\n      CGF.CurrentCleanupScopeDepth = CleanupStackDepth;\n    }\n\n    /// Exit this cleanup scope, emitting any accumulated cleanups.\n    ~RunCleanupsScope() {\n      if (PerformCleanup)\n        ForceCleanup();\n    }\n\n    /// Determine whether this scope requires any cleanups.\n    bool requiresCleanups() const {\n      return CGF.EHStack.stable_begin() != CleanupStackDepth;\n    }\n\n    /// Force the emission of cleanups now, instead of waiting\n    /// until this object is destroyed.\n    /// \\param ValuesToReload - A list of values that need to be available at\n    /// the insertion point after cleanup emission. If cleanup emission created\n    /// a shared cleanup block, these value pointers will be rewritten.\n    /// Otherwise, they not will be modified.\n    void ForceCleanup(std::initializer_list<llvm::Value**> ValuesToReload = {}) {\n      assert(PerformCleanup && \"Already forced cleanup\");\n      CGF.DidCallStackSave = OldDidCallStackSave;\n      CGF.PopCleanupBlocks(CleanupStackDepth, LifetimeExtendedCleanupStackSize,\n                           ValuesToReload);\n      PerformCleanup = false;\n      CGF.CurrentCleanupScopeDepth = OldCleanupScopeDepth;\n    }\n  };\n\n  // Cleanup stack depth of the RunCleanupsScope that was pushed most recently.\n  EHScopeStack::stable_iterator CurrentCleanupScopeDepth =\n      EHScopeStack::stable_end();\n\n  class LexicalScope : public RunCleanupsScope {\n    SourceRange Range;\n    SmallVector<const LabelDecl*, 4> Labels;\n    LexicalScope *ParentScope;\n\n    LexicalScope(const LexicalScope &) = delete;\n    void operator=(const LexicalScope &) = delete;\n\n  public:\n    /// Enter a new cleanup scope.\n    explicit LexicalScope(CodeGenFunction &CGF, SourceRange Range)\n      : RunCleanupsScope(CGF), Range(Range), ParentScope(CGF.CurLexicalScope) {\n      CGF.CurLexicalScope = this;\n      if (CGDebugInfo *DI = CGF.getDebugInfo())\n        DI->EmitLexicalBlockStart(CGF.Builder, Range.getBegin());\n    }\n\n    void addLabel(const LabelDecl *label) {\n      assert(PerformCleanup && \"adding label to dead scope?\");\n      Labels.push_back(label);\n    }\n\n    /// Exit this cleanup scope, emitting any accumulated\n    /// cleanups.\n    ~LexicalScope() {\n      if (CGDebugInfo *DI = CGF.getDebugInfo())\n        DI->EmitLexicalBlockEnd(CGF.Builder, Range.getEnd());\n\n      // If we should perform a cleanup, force them now.  Note that\n      // this ends the cleanup scope before rescoping any labels.\n      if (PerformCleanup) {\n        ApplyDebugLocation DL(CGF, Range.getEnd());\n        ForceCleanup();\n      }\n    }\n\n    /// Force the emission of cleanups now, instead of waiting\n    /// until this object is destroyed.\n    void ForceCleanup() {\n      CGF.CurLexicalScope = ParentScope;\n      RunCleanupsScope::ForceCleanup();\n\n      if (!Labels.empty())\n        rescopeLabels();\n    }\n\n    bool hasLabels() const {\n      return !Labels.empty();\n    }\n\n    void rescopeLabels();\n  };\n\n  typedef llvm::DenseMap<const Decl *, Address> DeclMapTy;\n\n  /// The class used to assign some variables some temporarily addresses.\n  class OMPMapVars {\n    DeclMapTy SavedLocals;\n    DeclMapTy SavedTempAddresses;\n    OMPMapVars(const OMPMapVars &) = delete;\n    void operator=(const OMPMapVars &) = delete;\n\n  public:\n    explicit OMPMapVars() = default;\n    ~OMPMapVars() {\n      assert(SavedLocals.empty() && \"Did not restored original addresses.\");\n    };\n\n    /// Sets the address of the variable \\p LocalVD to be \\p TempAddr in\n    /// function \\p CGF.\n    /// \\return true if at least one variable was set already, false otherwise.\n    bool setVarAddr(CodeGenFunction &CGF, const VarDecl *LocalVD,\n                    Address TempAddr) {\n      LocalVD = LocalVD->getCanonicalDecl();\n      // Only save it once.\n      if (SavedLocals.count(LocalVD)) return false;\n\n      // Copy the existing local entry to SavedLocals.\n      auto it = CGF.LocalDeclMap.find(LocalVD);\n      if (it != CGF.LocalDeclMap.end())\n        SavedLocals.try_emplace(LocalVD, it->second);\n      else\n        SavedLocals.try_emplace(LocalVD, Address::invalid());\n\n      // Generate the private entry.\n      QualType VarTy = LocalVD->getType();\n      if (VarTy->isReferenceType()) {\n        Address Temp = CGF.CreateMemTemp(VarTy);\n        CGF.Builder.CreateStore(TempAddr.getPointer(), Temp);\n        TempAddr = Temp;\n      }\n      SavedTempAddresses.try_emplace(LocalVD, TempAddr);\n\n      return true;\n    }\n\n    /// Applies new addresses to the list of the variables.\n    /// \\return true if at least one variable is using new address, false\n    /// otherwise.\n    bool apply(CodeGenFunction &CGF) {\n      copyInto(SavedTempAddresses, CGF.LocalDeclMap);\n      SavedTempAddresses.clear();\n      return !SavedLocals.empty();\n    }\n\n    /// Restores original addresses of the variables.\n    void restore(CodeGenFunction &CGF) {\n      if (!SavedLocals.empty()) {\n        copyInto(SavedLocals, CGF.LocalDeclMap);\n        SavedLocals.clear();\n      }\n    }\n\n  private:\n    /// Copy all the entries in the source map over the corresponding\n    /// entries in the destination, which must exist.\n    static void copyInto(const DeclMapTy &Src, DeclMapTy &Dest) {\n      for (auto &Pair : Src) {\n        if (!Pair.second.isValid()) {\n          Dest.erase(Pair.first);\n          continue;\n        }\n\n        auto I = Dest.find(Pair.first);\n        if (I != Dest.end())\n          I->second = Pair.second;\n        else\n          Dest.insert(Pair);\n      }\n    }\n  };\n\n  /// The scope used to remap some variables as private in the OpenMP loop body\n  /// (or other captured region emitted without outlining), and to restore old\n  /// vars back on exit.\n  class OMPPrivateScope : public RunCleanupsScope {\n    OMPMapVars MappedVars;\n    OMPPrivateScope(const OMPPrivateScope &) = delete;\n    void operator=(const OMPPrivateScope &) = delete;\n\n  public:\n    /// Enter a new OpenMP private scope.\n    explicit OMPPrivateScope(CodeGenFunction &CGF) : RunCleanupsScope(CGF) {}\n\n    /// Registers \\p LocalVD variable as a private and apply \\p PrivateGen\n    /// function for it to generate corresponding private variable. \\p\n    /// PrivateGen returns an address of the generated private variable.\n    /// \\return true if the variable is registered as private, false if it has\n    /// been privatized already.\n    bool addPrivate(const VarDecl *LocalVD,\n                    const llvm::function_ref<Address()> PrivateGen) {\n      assert(PerformCleanup && \"adding private to dead scope\");\n      return MappedVars.setVarAddr(CGF, LocalVD, PrivateGen());\n    }\n\n    /// Privatizes local variables previously registered as private.\n    /// Registration is separate from the actual privatization to allow\n    /// initializers use values of the original variables, not the private one.\n    /// This is important, for example, if the private variable is a class\n    /// variable initialized by a constructor that references other private\n    /// variables. But at initialization original variables must be used, not\n    /// private copies.\n    /// \\return true if at least one variable was privatized, false otherwise.\n    bool Privatize() { return MappedVars.apply(CGF); }\n\n    void ForceCleanup() {\n      RunCleanupsScope::ForceCleanup();\n      MappedVars.restore(CGF);\n    }\n\n    /// Exit scope - all the mapped variables are restored.\n    ~OMPPrivateScope() {\n      if (PerformCleanup)\n        ForceCleanup();\n    }\n\n    /// Checks if the global variable is captured in current function.\n    bool isGlobalVarCaptured(const VarDecl *VD) const {\n      VD = VD->getCanonicalDecl();\n      return !VD->isLocalVarDeclOrParm() && CGF.LocalDeclMap.count(VD) > 0;\n    }\n  };\n\n  /// Save/restore original map of previously emitted local vars in case when we\n  /// need to duplicate emission of the same code several times in the same\n  /// function for OpenMP code.\n  class OMPLocalDeclMapRAII {\n    CodeGenFunction &CGF;\n    DeclMapTy SavedMap;\n\n  public:\n    OMPLocalDeclMapRAII(CodeGenFunction &CGF)\n        : CGF(CGF), SavedMap(CGF.LocalDeclMap) {}\n    ~OMPLocalDeclMapRAII() { SavedMap.swap(CGF.LocalDeclMap); }\n  };\n\n  /// Takes the old cleanup stack size and emits the cleanup blocks\n  /// that have been added.\n  void\n  PopCleanupBlocks(EHScopeStack::stable_iterator OldCleanupStackSize,\n                   std::initializer_list<llvm::Value **> ValuesToReload = {});\n\n  /// Takes the old cleanup stack size and emits the cleanup blocks\n  /// that have been added, then adds all lifetime-extended cleanups from\n  /// the given position to the stack.\n  void\n  PopCleanupBlocks(EHScopeStack::stable_iterator OldCleanupStackSize,\n                   size_t OldLifetimeExtendedStackSize,\n                   std::initializer_list<llvm::Value **> ValuesToReload = {});\n\n  void ResolveBranchFixups(llvm::BasicBlock *Target);\n\n  /// The given basic block lies in the current EH scope, but may be a\n  /// target of a potentially scope-crossing jump; get a stable handle\n  /// to which we can perform this jump later.\n  JumpDest getJumpDestInCurrentScope(llvm::BasicBlock *Target) {\n    return JumpDest(Target,\n                    EHStack.getInnermostNormalCleanup(),\n                    NextCleanupDestIndex++);\n  }\n\n  /// The given basic block lies in the current EH scope, but may be a\n  /// target of a potentially scope-crossing jump; get a stable handle\n  /// to which we can perform this jump later.\n  JumpDest getJumpDestInCurrentScope(StringRef Name = StringRef()) {\n    return getJumpDestInCurrentScope(createBasicBlock(Name));\n  }\n\n  /// EmitBranchThroughCleanup - Emit a branch from the current insert\n  /// block through the normal cleanup handling code (if any) and then\n  /// on to \\arg Dest.\n  void EmitBranchThroughCleanup(JumpDest Dest);\n\n  /// isObviouslyBranchWithoutCleanups - Return true if a branch to the\n  /// specified destination obviously has no cleanups to run.  'false' is always\n  /// a conservatively correct answer for this method.\n  bool isObviouslyBranchWithoutCleanups(JumpDest Dest) const;\n\n  /// popCatchScope - Pops the catch scope at the top of the EHScope\n  /// stack, emitting any required code (other than the catch handlers\n  /// themselves).\n  void popCatchScope();\n\n  llvm::BasicBlock *getEHResumeBlock(bool isCleanup);\n  llvm::BasicBlock *getEHDispatchBlock(EHScopeStack::stable_iterator scope);\n  llvm::BasicBlock *\n  getFuncletEHDispatchBlock(EHScopeStack::stable_iterator scope);\n\n  /// An object to manage conditionally-evaluated expressions.\n  class ConditionalEvaluation {\n    llvm::BasicBlock *StartBB;\n\n  public:\n    ConditionalEvaluation(CodeGenFunction &CGF)\n      : StartBB(CGF.Builder.GetInsertBlock()) {}\n\n    void begin(CodeGenFunction &CGF) {\n      assert(CGF.OutermostConditional != this);\n      if (!CGF.OutermostConditional)\n        CGF.OutermostConditional = this;\n    }\n\n    void end(CodeGenFunction &CGF) {\n      assert(CGF.OutermostConditional != nullptr);\n      if (CGF.OutermostConditional == this)\n        CGF.OutermostConditional = nullptr;\n    }\n\n    /// Returns a block which will be executed prior to each\n    /// evaluation of the conditional code.\n    llvm::BasicBlock *getStartingBlock() const {\n      return StartBB;\n    }\n  };\n\n  /// isInConditionalBranch - Return true if we're currently emitting\n  /// one branch or the other of a conditional expression.\n  bool isInConditionalBranch() const { return OutermostConditional != nullptr; }\n\n  void setBeforeOutermostConditional(llvm::Value *value, Address addr) {\n    assert(isInConditionalBranch());\n    llvm::BasicBlock *block = OutermostConditional->getStartingBlock();\n    auto store = new llvm::StoreInst(value, addr.getPointer(), &block->back());\n    store->setAlignment(addr.getAlignment().getAsAlign());\n  }\n\n  /// An RAII object to record that we're evaluating a statement\n  /// expression.\n  class StmtExprEvaluation {\n    CodeGenFunction &CGF;\n\n    /// We have to save the outermost conditional: cleanups in a\n    /// statement expression aren't conditional just because the\n    /// StmtExpr is.\n    ConditionalEvaluation *SavedOutermostConditional;\n\n  public:\n    StmtExprEvaluation(CodeGenFunction &CGF)\n      : CGF(CGF), SavedOutermostConditional(CGF.OutermostConditional) {\n      CGF.OutermostConditional = nullptr;\n    }\n\n    ~StmtExprEvaluation() {\n      CGF.OutermostConditional = SavedOutermostConditional;\n      CGF.EnsureInsertPoint();\n    }\n  };\n\n  /// An object which temporarily prevents a value from being\n  /// destroyed by aggressive peephole optimizations that assume that\n  /// all uses of a value have been realized in the IR.\n  class PeepholeProtection {\n    llvm::Instruction *Inst;\n    friend class CodeGenFunction;\n\n  public:\n    PeepholeProtection() : Inst(nullptr) {}\n  };\n\n  /// A non-RAII class containing all the information about a bound\n  /// opaque value.  OpaqueValueMapping, below, is a RAII wrapper for\n  /// this which makes individual mappings very simple; using this\n  /// class directly is useful when you have a variable number of\n  /// opaque values or don't want the RAII functionality for some\n  /// reason.\n  class OpaqueValueMappingData {\n    const OpaqueValueExpr *OpaqueValue;\n    bool BoundLValue;\n    CodeGenFunction::PeepholeProtection Protection;\n\n    OpaqueValueMappingData(const OpaqueValueExpr *ov,\n                           bool boundLValue)\n      : OpaqueValue(ov), BoundLValue(boundLValue) {}\n  public:\n    OpaqueValueMappingData() : OpaqueValue(nullptr) {}\n\n    static bool shouldBindAsLValue(const Expr *expr) {\n      // gl-values should be bound as l-values for obvious reasons.\n      // Records should be bound as l-values because IR generation\n      // always keeps them in memory.  Expressions of function type\n      // act exactly like l-values but are formally required to be\n      // r-values in C.\n      return expr->isGLValue() ||\n             expr->getType()->isFunctionType() ||\n             hasAggregateEvaluationKind(expr->getType());\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const Expr *e) {\n      if (shouldBindAsLValue(ov))\n        return bind(CGF, ov, CGF.EmitLValue(e));\n      return bind(CGF, ov, CGF.EmitAnyExpr(e));\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const LValue &lv) {\n      assert(shouldBindAsLValue(ov));\n      CGF.OpaqueLValues.insert(std::make_pair(ov, lv));\n      return OpaqueValueMappingData(ov, true);\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const RValue &rv) {\n      assert(!shouldBindAsLValue(ov));\n      CGF.OpaqueRValues.insert(std::make_pair(ov, rv));\n\n      OpaqueValueMappingData data(ov, false);\n\n      // Work around an extremely aggressive peephole optimization in\n      // EmitScalarConversion which assumes that all other uses of a\n      // value are extant.\n      data.Protection = CGF.protectFromPeepholes(rv);\n\n      return data;\n    }\n\n    bool isValid() const { return OpaqueValue != nullptr; }\n    void clear() { OpaqueValue = nullptr; }\n\n    void unbind(CodeGenFunction &CGF) {\n      assert(OpaqueValue && \"no data to unbind!\");\n\n      if (BoundLValue) {\n        CGF.OpaqueLValues.erase(OpaqueValue);\n      } else {\n        CGF.OpaqueRValues.erase(OpaqueValue);\n        CGF.unprotectFromPeepholes(Protection);\n      }\n    }\n  };\n\n  /// An RAII object to set (and then clear) a mapping for an OpaqueValueExpr.\n  class OpaqueValueMapping {\n    CodeGenFunction &CGF;\n    OpaqueValueMappingData Data;\n\n  public:\n    static bool shouldBindAsLValue(const Expr *expr) {\n      return OpaqueValueMappingData::shouldBindAsLValue(expr);\n    }\n\n    /// Build the opaque value mapping for the given conditional\n    /// operator if it's the GNU ?: extension.  This is a common\n    /// enough pattern that the convenience operator is really\n    /// helpful.\n    ///\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const AbstractConditionalOperator *op) : CGF(CGF) {\n      if (isa<ConditionalOperator>(op))\n        // Leave Data empty.\n        return;\n\n      const BinaryConditionalOperator *e = cast<BinaryConditionalOperator>(op);\n      Data = OpaqueValueMappingData::bind(CGF, e->getOpaqueValue(),\n                                          e->getCommon());\n    }\n\n    /// Build the opaque value mapping for an OpaqueValueExpr whose source\n    /// expression is set to the expression the OVE represents.\n    OpaqueValueMapping(CodeGenFunction &CGF, const OpaqueValueExpr *OV)\n        : CGF(CGF) {\n      if (OV) {\n        assert(OV->getSourceExpr() && \"wrong form of OpaqueValueMapping used \"\n                                      \"for OVE with no source expression\");\n        Data = OpaqueValueMappingData::bind(CGF, OV, OV->getSourceExpr());\n      }\n    }\n\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const OpaqueValueExpr *opaqueValue,\n                       LValue lvalue)\n      : CGF(CGF), Data(OpaqueValueMappingData::bind(CGF, opaqueValue, lvalue)) {\n    }\n\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const OpaqueValueExpr *opaqueValue,\n                       RValue rvalue)\n      : CGF(CGF), Data(OpaqueValueMappingData::bind(CGF, opaqueValue, rvalue)) {\n    }\n\n    void pop() {\n      Data.unbind(CGF);\n      Data.clear();\n    }\n\n    ~OpaqueValueMapping() {\n      if (Data.isValid()) Data.unbind(CGF);\n    }\n  };\n\nprivate:\n  CGDebugInfo *DebugInfo;\n  /// Used to create unique names for artificial VLA size debug info variables.\n  unsigned VLAExprCounter = 0;\n  bool DisableDebugInfo = false;\n\n  /// DidCallStackSave - Whether llvm.stacksave has been called. Used to avoid\n  /// calling llvm.stacksave for multiple VLAs in the same scope.\n  bool DidCallStackSave = false;\n\n  /// IndirectBranch - The first time an indirect goto is seen we create a block\n  /// with an indirect branch.  Every time we see the address of a label taken,\n  /// we add the label to the indirect goto.  Every subsequent indirect goto is\n  /// codegen'd as a jump to the IndirectBranch's basic block.\n  llvm::IndirectBrInst *IndirectBranch = nullptr;\n\n  /// LocalDeclMap - This keeps track of the LLVM allocas or globals for local C\n  /// decls.\n  DeclMapTy LocalDeclMap;\n\n  // Keep track of the cleanups for callee-destructed parameters pushed to the\n  // cleanup stack so that they can be deactivated later.\n  llvm::DenseMap<const ParmVarDecl *, EHScopeStack::stable_iterator>\n      CalleeDestructedParamCleanups;\n\n  /// SizeArguments - If a ParmVarDecl had the pass_object_size attribute, this\n  /// will contain a mapping from said ParmVarDecl to its implicit \"object_size\"\n  /// parameter.\n  llvm::SmallDenseMap<const ParmVarDecl *, const ImplicitParamDecl *, 2>\n      SizeArguments;\n\n  /// Track escaped local variables with auto storage. Used during SEH\n  /// outlining to produce a call to llvm.localescape.\n  llvm::DenseMap<llvm::AllocaInst *, int> EscapedLocals;\n\n  /// LabelMap - This keeps track of the LLVM basic block for each C label.\n  llvm::DenseMap<const LabelDecl*, JumpDest> LabelMap;\n\n  // BreakContinueStack - This keeps track of where break and continue\n  // statements should jump to.\n  struct BreakContinue {\n    BreakContinue(JumpDest Break, JumpDest Continue)\n      : BreakBlock(Break), ContinueBlock(Continue) {}\n\n    JumpDest BreakBlock;\n    JumpDest ContinueBlock;\n  };\n  SmallVector<BreakContinue, 8> BreakContinueStack;\n\n  /// Handles cancellation exit points in OpenMP-related constructs.\n  class OpenMPCancelExitStack {\n    /// Tracks cancellation exit point and join point for cancel-related exit\n    /// and normal exit.\n    struct CancelExit {\n      CancelExit() = default;\n      CancelExit(OpenMPDirectiveKind Kind, JumpDest ExitBlock,\n                 JumpDest ContBlock)\n          : Kind(Kind), ExitBlock(ExitBlock), ContBlock(ContBlock) {}\n      OpenMPDirectiveKind Kind = llvm::omp::OMPD_unknown;\n      /// true if the exit block has been emitted already by the special\n      /// emitExit() call, false if the default codegen is used.\n      bool HasBeenEmitted = false;\n      JumpDest ExitBlock;\n      JumpDest ContBlock;\n    };\n\n    SmallVector<CancelExit, 8> Stack;\n\n  public:\n    OpenMPCancelExitStack() : Stack(1) {}\n    ~OpenMPCancelExitStack() = default;\n    /// Fetches the exit block for the current OpenMP construct.\n    JumpDest getExitBlock() const { return Stack.back().ExitBlock; }\n    /// Emits exit block with special codegen procedure specific for the related\n    /// OpenMP construct + emits code for normal construct cleanup.\n    void emitExit(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,\n                  const llvm::function_ref<void(CodeGenFunction &)> CodeGen) {\n      if (Stack.back().Kind == Kind && getExitBlock().isValid()) {\n        assert(CGF.getOMPCancelDestination(Kind).isValid());\n        assert(CGF.HaveInsertPoint());\n        assert(!Stack.back().HasBeenEmitted);\n        auto IP = CGF.Builder.saveAndClearIP();\n        CGF.EmitBlock(Stack.back().ExitBlock.getBlock());\n        CodeGen(CGF);\n        CGF.EmitBranch(Stack.back().ContBlock.getBlock());\n        CGF.Builder.restoreIP(IP);\n        Stack.back().HasBeenEmitted = true;\n      }\n      CodeGen(CGF);\n    }\n    /// Enter the cancel supporting \\a Kind construct.\n    /// \\param Kind OpenMP directive that supports cancel constructs.\n    /// \\param HasCancel true, if the construct has inner cancel directive,\n    /// false otherwise.\n    void enter(CodeGenFunction &CGF, OpenMPDirectiveKind Kind, bool HasCancel) {\n      Stack.push_back({Kind,\n                       HasCancel ? CGF.getJumpDestInCurrentScope(\"cancel.exit\")\n                                 : JumpDest(),\n                       HasCancel ? CGF.getJumpDestInCurrentScope(\"cancel.cont\")\n                                 : JumpDest()});\n    }\n    /// Emits default exit point for the cancel construct (if the special one\n    /// has not be used) + join point for cancel/normal exits.\n    void exit(CodeGenFunction &CGF) {\n      if (getExitBlock().isValid()) {\n        assert(CGF.getOMPCancelDestination(Stack.back().Kind).isValid());\n        bool HaveIP = CGF.HaveInsertPoint();\n        if (!Stack.back().HasBeenEmitted) {\n          if (HaveIP)\n            CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);\n          CGF.EmitBlock(Stack.back().ExitBlock.getBlock());\n          CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);\n        }\n        CGF.EmitBlock(Stack.back().ContBlock.getBlock());\n        if (!HaveIP) {\n          CGF.Builder.CreateUnreachable();\n          CGF.Builder.ClearInsertionPoint();\n        }\n      }\n      Stack.pop_back();\n    }\n  };\n  OpenMPCancelExitStack OMPCancelStack;\n\n  /// Calculate branch weights for the likelihood attribute\n  llvm::MDNode *createBranchWeights(Stmt::Likelihood LH) const;\n\n  CodeGenPGO PGO;\n\n  /// Calculate branch weights appropriate for PGO data\n  llvm::MDNode *createProfileWeights(uint64_t TrueCount,\n                                     uint64_t FalseCount) const;\n  llvm::MDNode *createProfileWeights(ArrayRef<uint64_t> Weights) const;\n  llvm::MDNode *createProfileWeightsForLoop(const Stmt *Cond,\n                                            uint64_t LoopCount) const;\n\n  /// Calculate the branch weight for PGO data or the likelihood attribute.\n  /// The function tries to get the weight of \\ref createProfileWeightsForLoop.\n  /// If that fails it gets the weight of \\ref createBranchWeights.\n  llvm::MDNode *createProfileOrBranchWeightsForLoop(const Stmt *Cond,\n                                                    uint64_t LoopCount,\n                                                    const Stmt *Body) const;\n\npublic:\n  /// Increment the profiler's counter for the given statement by \\p StepV.\n  /// If \\p StepV is null, the default increment is 1.\n  void incrementProfileCounter(const Stmt *S, llvm::Value *StepV = nullptr) {\n    if (CGM.getCodeGenOpts().hasProfileClangInstr() &&\n        !CurFn->hasFnAttribute(llvm::Attribute::NoProfile))\n      PGO.emitCounterIncrement(Builder, S, StepV);\n    PGO.setCurrentStmt(S);\n  }\n\n  /// Get the profiler's count for the given statement.\n  uint64_t getProfileCount(const Stmt *S) {\n    Optional<uint64_t> Count = PGO.getStmtCount(S);\n    if (!Count.hasValue())\n      return 0;\n    return *Count;\n  }\n\n  /// Set the profiler's current count.\n  void setCurrentProfileCount(uint64_t Count) {\n    PGO.setCurrentRegionCount(Count);\n  }\n\n  /// Get the profiler's current count. This is generally the count for the most\n  /// recently incremented counter.\n  uint64_t getCurrentProfileCount() {\n    return PGO.getCurrentRegionCount();\n  }\n\nprivate:\n\n  /// SwitchInsn - This is nearest current switch instruction. It is null if\n  /// current context is not in a switch.\n  llvm::SwitchInst *SwitchInsn = nullptr;\n  /// The branch weights of SwitchInsn when doing instrumentation based PGO.\n  SmallVector<uint64_t, 16> *SwitchWeights = nullptr;\n\n  /// The likelihood attributes of the SwitchCase.\n  SmallVector<Stmt::Likelihood, 16> *SwitchLikelihood = nullptr;\n\n  /// CaseRangeBlock - This block holds if condition check for last case\n  /// statement range in current switch instruction.\n  llvm::BasicBlock *CaseRangeBlock = nullptr;\n\n  /// OpaqueLValues - Keeps track of the current set of opaque value\n  /// expressions.\n  llvm::DenseMap<const OpaqueValueExpr *, LValue> OpaqueLValues;\n  llvm::DenseMap<const OpaqueValueExpr *, RValue> OpaqueRValues;\n\n  // VLASizeMap - This keeps track of the associated size for each VLA type.\n  // We track this by the size expression rather than the type itself because\n  // in certain situations, like a const qualifier applied to an VLA typedef,\n  // multiple VLA types can share the same size expression.\n  // FIXME: Maybe this could be a stack of maps that is pushed/popped as we\n  // enter/leave scopes.\n  llvm::DenseMap<const Expr*, llvm::Value*> VLASizeMap;\n\n  /// A block containing a single 'unreachable' instruction.  Created\n  /// lazily by getUnreachableBlock().\n  llvm::BasicBlock *UnreachableBlock = nullptr;\n\n  /// Counts of the number return expressions in the function.\n  unsigned NumReturnExprs = 0;\n\n  /// Count the number of simple (constant) return expressions in the function.\n  unsigned NumSimpleReturnExprs = 0;\n\n  /// The last regular (non-return) debug location (breakpoint) in the function.\n  SourceLocation LastStopPoint;\n\npublic:\n  /// Source location information about the default argument or member\n  /// initializer expression we're evaluating, if any.\n  CurrentSourceLocExprScope CurSourceLocExprScope;\n  using SourceLocExprScopeGuard =\n      CurrentSourceLocExprScope::SourceLocExprScopeGuard;\n\n  /// A scope within which we are constructing the fields of an object which\n  /// might use a CXXDefaultInitExpr. This stashes away a 'this' value to use\n  /// if we need to evaluate a CXXDefaultInitExpr within the evaluation.\n  class FieldConstructionScope {\n  public:\n    FieldConstructionScope(CodeGenFunction &CGF, Address This)\n        : CGF(CGF), OldCXXDefaultInitExprThis(CGF.CXXDefaultInitExprThis) {\n      CGF.CXXDefaultInitExprThis = This;\n    }\n    ~FieldConstructionScope() {\n      CGF.CXXDefaultInitExprThis = OldCXXDefaultInitExprThis;\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    Address OldCXXDefaultInitExprThis;\n  };\n\n  /// The scope of a CXXDefaultInitExpr. Within this scope, the value of 'this'\n  /// is overridden to be the object under construction.\n  class CXXDefaultInitExprScope  {\n  public:\n    CXXDefaultInitExprScope(CodeGenFunction &CGF, const CXXDefaultInitExpr *E)\n        : CGF(CGF), OldCXXThisValue(CGF.CXXThisValue),\n          OldCXXThisAlignment(CGF.CXXThisAlignment),\n          SourceLocScope(E, CGF.CurSourceLocExprScope) {\n      CGF.CXXThisValue = CGF.CXXDefaultInitExprThis.getPointer();\n      CGF.CXXThisAlignment = CGF.CXXDefaultInitExprThis.getAlignment();\n    }\n    ~CXXDefaultInitExprScope() {\n      CGF.CXXThisValue = OldCXXThisValue;\n      CGF.CXXThisAlignment = OldCXXThisAlignment;\n    }\n\n  public:\n    CodeGenFunction &CGF;\n    llvm::Value *OldCXXThisValue;\n    CharUnits OldCXXThisAlignment;\n    SourceLocExprScopeGuard SourceLocScope;\n  };\n\n  struct CXXDefaultArgExprScope : SourceLocExprScopeGuard {\n    CXXDefaultArgExprScope(CodeGenFunction &CGF, const CXXDefaultArgExpr *E)\n        : SourceLocExprScopeGuard(E, CGF.CurSourceLocExprScope) {}\n  };\n\n  /// The scope of an ArrayInitLoopExpr. Within this scope, the value of the\n  /// current loop index is overridden.\n  class ArrayInitLoopExprScope {\n  public:\n    ArrayInitLoopExprScope(CodeGenFunction &CGF, llvm::Value *Index)\n      : CGF(CGF), OldArrayInitIndex(CGF.ArrayInitIndex) {\n      CGF.ArrayInitIndex = Index;\n    }\n    ~ArrayInitLoopExprScope() {\n      CGF.ArrayInitIndex = OldArrayInitIndex;\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    llvm::Value *OldArrayInitIndex;\n  };\n\n  class InlinedInheritingConstructorScope {\n  public:\n    InlinedInheritingConstructorScope(CodeGenFunction &CGF, GlobalDecl GD)\n        : CGF(CGF), OldCurGD(CGF.CurGD), OldCurFuncDecl(CGF.CurFuncDecl),\n          OldCurCodeDecl(CGF.CurCodeDecl),\n          OldCXXABIThisDecl(CGF.CXXABIThisDecl),\n          OldCXXABIThisValue(CGF.CXXABIThisValue),\n          OldCXXThisValue(CGF.CXXThisValue),\n          OldCXXABIThisAlignment(CGF.CXXABIThisAlignment),\n          OldCXXThisAlignment(CGF.CXXThisAlignment),\n          OldReturnValue(CGF.ReturnValue), OldFnRetTy(CGF.FnRetTy),\n          OldCXXInheritedCtorInitExprArgs(\n              std::move(CGF.CXXInheritedCtorInitExprArgs)) {\n      CGF.CurGD = GD;\n      CGF.CurFuncDecl = CGF.CurCodeDecl =\n          cast<CXXConstructorDecl>(GD.getDecl());\n      CGF.CXXABIThisDecl = nullptr;\n      CGF.CXXABIThisValue = nullptr;\n      CGF.CXXThisValue = nullptr;\n      CGF.CXXABIThisAlignment = CharUnits();\n      CGF.CXXThisAlignment = CharUnits();\n      CGF.ReturnValue = Address::invalid();\n      CGF.FnRetTy = QualType();\n      CGF.CXXInheritedCtorInitExprArgs.clear();\n    }\n    ~InlinedInheritingConstructorScope() {\n      CGF.CurGD = OldCurGD;\n      CGF.CurFuncDecl = OldCurFuncDecl;\n      CGF.CurCodeDecl = OldCurCodeDecl;\n      CGF.CXXABIThisDecl = OldCXXABIThisDecl;\n      CGF.CXXABIThisValue = OldCXXABIThisValue;\n      CGF.CXXThisValue = OldCXXThisValue;\n      CGF.CXXABIThisAlignment = OldCXXABIThisAlignment;\n      CGF.CXXThisAlignment = OldCXXThisAlignment;\n      CGF.ReturnValue = OldReturnValue;\n      CGF.FnRetTy = OldFnRetTy;\n      CGF.CXXInheritedCtorInitExprArgs =\n          std::move(OldCXXInheritedCtorInitExprArgs);\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    GlobalDecl OldCurGD;\n    const Decl *OldCurFuncDecl;\n    const Decl *OldCurCodeDecl;\n    ImplicitParamDecl *OldCXXABIThisDecl;\n    llvm::Value *OldCXXABIThisValue;\n    llvm::Value *OldCXXThisValue;\n    CharUnits OldCXXABIThisAlignment;\n    CharUnits OldCXXThisAlignment;\n    Address OldReturnValue;\n    QualType OldFnRetTy;\n    CallArgList OldCXXInheritedCtorInitExprArgs;\n  };\n\n  // Helper class for the OpenMP IR Builder. Allows reusability of code used for\n  // region body, and finalization codegen callbacks. This will class will also\n  // contain privatization functions used by the privatization call backs\n  //\n  // TODO: this is temporary class for things that are being moved out of\n  // CGOpenMPRuntime, new versions of current CodeGenFunction methods, or\n  // utility function for use with the OMPBuilder. Once that move to use the\n  // OMPBuilder is done, everything here will either become part of CodeGenFunc.\n  // directly, or a new helper class that will contain functions used by both\n  // this and the OMPBuilder\n\n  struct OMPBuilderCBHelpers {\n\n    OMPBuilderCBHelpers() = delete;\n    OMPBuilderCBHelpers(const OMPBuilderCBHelpers &) = delete;\n    OMPBuilderCBHelpers &operator=(const OMPBuilderCBHelpers &) = delete;\n\n    using InsertPointTy = llvm::OpenMPIRBuilder::InsertPointTy;\n\n    /// Cleanup action for allocate support.\n    class OMPAllocateCleanupTy final : public EHScopeStack::Cleanup {\n\n    private:\n      llvm::CallInst *RTLFnCI;\n\n    public:\n      OMPAllocateCleanupTy(llvm::CallInst *RLFnCI) : RTLFnCI(RLFnCI) {\n        RLFnCI->removeFromParent();\n      }\n\n      void Emit(CodeGenFunction &CGF, Flags /*flags*/) override {\n        if (!CGF.HaveInsertPoint())\n          return;\n        CGF.Builder.Insert(RTLFnCI);\n      }\n    };\n\n    /// Returns address of the threadprivate variable for the current\n    /// thread. This Also create any necessary OMP runtime calls.\n    ///\n    /// \\param VD VarDecl for Threadprivate variable.\n    /// \\param VDAddr Address of the Vardecl\n    /// \\param Loc  The location where the barrier directive was encountered\n    static Address getAddrOfThreadPrivate(CodeGenFunction &CGF,\n                                          const VarDecl *VD, Address VDAddr,\n                                          SourceLocation Loc);\n\n    /// Gets the OpenMP-specific address of the local variable /p VD.\n    static Address getAddressOfLocalVariable(CodeGenFunction &CGF,\n                                             const VarDecl *VD);\n    /// Get the platform-specific name separator.\n    /// \\param Parts different parts of the final name that needs separation\n    /// \\param FirstSeparator First separator used between the initial two\n    ///        parts of the name.\n    /// \\param Separator separator used between all of the rest consecutinve\n    ///        parts of the name\n    static std::string getNameWithSeparators(ArrayRef<StringRef> Parts,\n                                             StringRef FirstSeparator = \".\",\n                                             StringRef Separator = \".\");\n    /// Emit the Finalization for an OMP region\n    /// \\param CGF\tThe Codegen function this belongs to\n    /// \\param IP\tInsertion point for generating the finalization code.\n    static void FinalizeOMPRegion(CodeGenFunction &CGF, InsertPointTy IP) {\n      CGBuilderTy::InsertPointGuard IPG(CGF.Builder);\n      assert(IP.getBlock()->end() != IP.getPoint() &&\n             \"OpenMP IR Builder should cause terminated block!\");\n\n      llvm::BasicBlock *IPBB = IP.getBlock();\n      llvm::BasicBlock *DestBB = IPBB->getUniqueSuccessor();\n      assert(DestBB && \"Finalization block should have one successor!\");\n\n      // erase and replace with cleanup branch.\n      IPBB->getTerminator()->eraseFromParent();\n      CGF.Builder.SetInsertPoint(IPBB);\n      CodeGenFunction::JumpDest Dest = CGF.getJumpDestInCurrentScope(DestBB);\n      CGF.EmitBranchThroughCleanup(Dest);\n    }\n\n    /// Emit the body of an OMP region\n    /// \\param CGF\tThe Codegen function this belongs to\n    /// \\param RegionBodyStmt\tThe body statement for the OpenMP region being\n    /// \t\t\t generated\n    /// \\param CodeGenIP\tInsertion point for generating the body code.\n    /// \\param FiniBB\tThe finalization basic block\n    static void EmitOMPRegionBody(CodeGenFunction &CGF,\n                                  const Stmt *RegionBodyStmt,\n                                  InsertPointTy CodeGenIP,\n                                  llvm::BasicBlock &FiniBB) {\n      llvm::BasicBlock *CodeGenIPBB = CodeGenIP.getBlock();\n      if (llvm::Instruction *CodeGenIPBBTI = CodeGenIPBB->getTerminator())\n        CodeGenIPBBTI->eraseFromParent();\n\n      CGF.Builder.SetInsertPoint(CodeGenIPBB);\n\n      CGF.EmitStmt(RegionBodyStmt);\n\n      if (CGF.Builder.saveIP().isSet())\n        CGF.Builder.CreateBr(&FiniBB);\n    }\n\n    /// RAII for preserving necessary info during Outlined region body codegen.\n    class OutlinedRegionBodyRAII {\n\n      llvm::AssertingVH<llvm::Instruction> OldAllocaIP;\n      CodeGenFunction::JumpDest OldReturnBlock;\n      CGBuilderTy::InsertPoint IP;\n      CodeGenFunction &CGF;\n\n    public:\n      OutlinedRegionBodyRAII(CodeGenFunction &cgf, InsertPointTy &AllocaIP,\n                             llvm::BasicBlock &RetBB)\n          : CGF(cgf) {\n        assert(AllocaIP.isSet() &&\n               \"Must specify Insertion point for allocas of outlined function\");\n        OldAllocaIP = CGF.AllocaInsertPt;\n        CGF.AllocaInsertPt = &*AllocaIP.getPoint();\n        IP = CGF.Builder.saveIP();\n\n        OldReturnBlock = CGF.ReturnBlock;\n        CGF.ReturnBlock = CGF.getJumpDestInCurrentScope(&RetBB);\n      }\n\n      ~OutlinedRegionBodyRAII() {\n        CGF.AllocaInsertPt = OldAllocaIP;\n        CGF.ReturnBlock = OldReturnBlock;\n        CGF.Builder.restoreIP(IP);\n      }\n    };\n\n    /// RAII for preserving necessary info during inlined region body codegen.\n    class InlinedRegionBodyRAII {\n\n      llvm::AssertingVH<llvm::Instruction> OldAllocaIP;\n      CodeGenFunction &CGF;\n\n    public:\n      InlinedRegionBodyRAII(CodeGenFunction &cgf, InsertPointTy &AllocaIP,\n                            llvm::BasicBlock &FiniBB)\n          : CGF(cgf) {\n        // Alloca insertion block should be in the entry block of the containing\n        // function so it expects an empty AllocaIP in which case will reuse the\n        // old alloca insertion point, or a new AllocaIP in the same block as\n        // the old one\n        assert((!AllocaIP.isSet() ||\n                CGF.AllocaInsertPt->getParent() == AllocaIP.getBlock()) &&\n               \"Insertion point should be in the entry block of containing \"\n               \"function!\");\n        OldAllocaIP = CGF.AllocaInsertPt;\n        if (AllocaIP.isSet())\n          CGF.AllocaInsertPt = &*AllocaIP.getPoint();\n\n        // TODO: Remove the call, after making sure the counter is not used by\n        //       the EHStack.\n        // Since this is an inlined region, it should not modify the\n        // ReturnBlock, and should reuse the one for the enclosing outlined\n        // region. So, the JumpDest being return by the function is discarded\n        (void)CGF.getJumpDestInCurrentScope(&FiniBB);\n      }\n\n      ~InlinedRegionBodyRAII() { CGF.AllocaInsertPt = OldAllocaIP; }\n    };\n  };\n\nprivate:\n  /// CXXThisDecl - When generating code for a C++ member function,\n  /// this will hold the implicit 'this' declaration.\n  ImplicitParamDecl *CXXABIThisDecl = nullptr;\n  llvm::Value *CXXABIThisValue = nullptr;\n  llvm::Value *CXXThisValue = nullptr;\n  CharUnits CXXABIThisAlignment;\n  CharUnits CXXThisAlignment;\n\n  /// The value of 'this' to use when evaluating CXXDefaultInitExprs within\n  /// this expression.\n  Address CXXDefaultInitExprThis = Address::invalid();\n\n  /// The current array initialization index when evaluating an\n  /// ArrayInitIndexExpr within an ArrayInitLoopExpr.\n  llvm::Value *ArrayInitIndex = nullptr;\n\n  /// The values of function arguments to use when evaluating\n  /// CXXInheritedCtorInitExprs within this context.\n  CallArgList CXXInheritedCtorInitExprArgs;\n\n  /// CXXStructorImplicitParamDecl - When generating code for a constructor or\n  /// destructor, this will hold the implicit argument (e.g. VTT).\n  ImplicitParamDecl *CXXStructorImplicitParamDecl = nullptr;\n  llvm::Value *CXXStructorImplicitParamValue = nullptr;\n\n  /// OutermostConditional - Points to the outermost active\n  /// conditional control.  This is used so that we know if a\n  /// temporary should be destroyed conditionally.\n  ConditionalEvaluation *OutermostConditional = nullptr;\n\n  /// The current lexical scope.\n  LexicalScope *CurLexicalScope = nullptr;\n\n  /// The current source location that should be used for exception\n  /// handling code.\n  SourceLocation CurEHLocation;\n\n  /// BlockByrefInfos - For each __block variable, contains\n  /// information about the layout of the variable.\n  llvm::DenseMap<const ValueDecl *, BlockByrefInfo> BlockByrefInfos;\n\n  /// Used by -fsanitize=nullability-return to determine whether the return\n  /// value can be checked.\n  llvm::Value *RetValNullabilityPrecondition = nullptr;\n\n  /// Check if -fsanitize=nullability-return instrumentation is required for\n  /// this function.\n  bool requiresReturnValueNullabilityCheck() const {\n    return RetValNullabilityPrecondition;\n  }\n\n  /// Used to store precise source locations for return statements by the\n  /// runtime return value checks.\n  Address ReturnLocation = Address::invalid();\n\n  /// Check if the return value of this function requires sanitization.\n  bool requiresReturnValueCheck() const;\n\n  llvm::BasicBlock *TerminateLandingPad = nullptr;\n  llvm::BasicBlock *TerminateHandler = nullptr;\n  llvm::SmallVector<llvm::BasicBlock *, 2> TrapBBs;\n\n  /// Terminate funclets keyed by parent funclet pad.\n  llvm::MapVector<llvm::Value *, llvm::BasicBlock *> TerminateFunclets;\n\n  /// Largest vector width used in ths function. Will be used to create a\n  /// function attribute.\n  unsigned LargestVectorWidth = 0;\n\n  /// True if we need emit the life-time markers.\n  const bool ShouldEmitLifetimeMarkers;\n\n  /// Add OpenCL kernel arg metadata and the kernel attribute metadata to\n  /// the function metadata.\n  void EmitOpenCLKernelMetadata(const FunctionDecl *FD,\n                                llvm::Function *Fn);\n\npublic:\n  CodeGenFunction(CodeGenModule &cgm, bool suppressNewContext=false);\n  ~CodeGenFunction();\n\n  CodeGenTypes &getTypes() const { return CGM.getTypes(); }\n  ASTContext &getContext() const { return CGM.getContext(); }\n  CGDebugInfo *getDebugInfo() {\n    if (DisableDebugInfo)\n      return nullptr;\n    return DebugInfo;\n  }\n  void disableDebugInfo() { DisableDebugInfo = true; }\n  void enableDebugInfo() { DisableDebugInfo = false; }\n\n  bool shouldUseFusedARCCalls() {\n    return CGM.getCodeGenOpts().OptimizationLevel == 0;\n  }\n\n  const LangOptions &getLangOpts() const { return CGM.getLangOpts(); }\n\n  /// Returns a pointer to the function's exception object and selector slot,\n  /// which is assigned in every landing pad.\n  Address getExceptionSlot();\n  Address getEHSelectorSlot();\n\n  /// Returns the contents of the function's exception object and selector\n  /// slots.\n  llvm::Value *getExceptionFromSlot();\n  llvm::Value *getSelectorFromSlot();\n\n  Address getNormalCleanupDestSlot();\n\n  llvm::BasicBlock *getUnreachableBlock() {\n    if (!UnreachableBlock) {\n      UnreachableBlock = createBasicBlock(\"unreachable\");\n      new llvm::UnreachableInst(getLLVMContext(), UnreachableBlock);\n    }\n    return UnreachableBlock;\n  }\n\n  llvm::BasicBlock *getInvokeDest() {\n    if (!EHStack.requiresLandingPad()) return nullptr;\n    return getInvokeDestImpl();\n  }\n\n  bool currentFunctionUsesSEHTry() const { return CurSEHParent != nullptr; }\n\n  const TargetInfo &getTarget() const { return Target; }\n  llvm::LLVMContext &getLLVMContext() { return CGM.getLLVMContext(); }\n  const TargetCodeGenInfo &getTargetHooks() const {\n    return CGM.getTargetCodeGenInfo();\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                                  Cleanups\n  //===--------------------------------------------------------------------===//\n\n  typedef void Destroyer(CodeGenFunction &CGF, Address addr, QualType ty);\n\n  void pushIrregularPartialArrayCleanup(llvm::Value *arrayBegin,\n                                        Address arrayEndPointer,\n                                        QualType elementType,\n                                        CharUnits elementAlignment,\n                                        Destroyer *destroyer);\n  void pushRegularPartialArrayCleanup(llvm::Value *arrayBegin,\n                                      llvm::Value *arrayEnd,\n                                      QualType elementType,\n                                      CharUnits elementAlignment,\n                                      Destroyer *destroyer);\n\n  void pushDestroy(QualType::DestructionKind dtorKind,\n                   Address addr, QualType type);\n  void pushEHDestroy(QualType::DestructionKind dtorKind,\n                     Address addr, QualType type);\n  void pushDestroy(CleanupKind kind, Address addr, QualType type,\n                   Destroyer *destroyer, bool useEHCleanupForArray);\n  void pushLifetimeExtendedDestroy(CleanupKind kind, Address addr,\n                                   QualType type, Destroyer *destroyer,\n                                   bool useEHCleanupForArray);\n  void pushCallObjectDeleteCleanup(const FunctionDecl *OperatorDelete,\n                                   llvm::Value *CompletePtr,\n                                   QualType ElementType);\n  void pushStackRestore(CleanupKind kind, Address SPMem);\n  void emitDestroy(Address addr, QualType type, Destroyer *destroyer,\n                   bool useEHCleanupForArray);\n  llvm::Function *generateDestroyHelper(Address addr, QualType type,\n                                        Destroyer *destroyer,\n                                        bool useEHCleanupForArray,\n                                        const VarDecl *VD);\n  void emitArrayDestroy(llvm::Value *begin, llvm::Value *end,\n                        QualType elementType, CharUnits elementAlign,\n                        Destroyer *destroyer,\n                        bool checkZeroLength, bool useEHCleanup);\n\n  Destroyer *getDestroyer(QualType::DestructionKind destructionKind);\n\n  /// Determines whether an EH cleanup is required to destroy a type\n  /// with the given destruction kind.\n  bool needsEHCleanup(QualType::DestructionKind kind) {\n    switch (kind) {\n    case QualType::DK_none:\n      return false;\n    case QualType::DK_cxx_destructor:\n    case QualType::DK_objc_weak_lifetime:\n    case QualType::DK_nontrivial_c_struct:\n      return getLangOpts().Exceptions;\n    case QualType::DK_objc_strong_lifetime:\n      return getLangOpts().Exceptions &&\n             CGM.getCodeGenOpts().ObjCAutoRefCountExceptions;\n    }\n    llvm_unreachable(\"bad destruction kind\");\n  }\n\n  CleanupKind getCleanupKind(QualType::DestructionKind kind) {\n    return (needsEHCleanup(kind) ? NormalAndEHCleanup : NormalCleanup);\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                                  Objective-C\n  //===--------------------------------------------------------------------===//\n\n  void GenerateObjCMethod(const ObjCMethodDecl *OMD);\n\n  void StartObjCMethod(const ObjCMethodDecl *MD, const ObjCContainerDecl *CD);\n\n  /// GenerateObjCGetter - Synthesize an Objective-C property getter function.\n  void GenerateObjCGetter(ObjCImplementationDecl *IMP,\n                          const ObjCPropertyImplDecl *PID);\n  void generateObjCGetterBody(const ObjCImplementationDecl *classImpl,\n                              const ObjCPropertyImplDecl *propImpl,\n                              const ObjCMethodDecl *GetterMothodDecl,\n                              llvm::Constant *AtomicHelperFn);\n\n  void GenerateObjCCtorDtorMethod(ObjCImplementationDecl *IMP,\n                                  ObjCMethodDecl *MD, bool ctor);\n\n  /// GenerateObjCSetter - Synthesize an Objective-C property setter function\n  /// for the given property.\n  void GenerateObjCSetter(ObjCImplementationDecl *IMP,\n                          const ObjCPropertyImplDecl *PID);\n  void generateObjCSetterBody(const ObjCImplementationDecl *classImpl,\n                              const ObjCPropertyImplDecl *propImpl,\n                              llvm::Constant *AtomicHelperFn);\n\n  //===--------------------------------------------------------------------===//\n  //                                  Block Bits\n  //===--------------------------------------------------------------------===//\n\n  /// Emit block literal.\n  /// \\return an LLVM value which is a pointer to a struct which contains\n  /// information about the block, including the block invoke function, the\n  /// captured variables, etc.\n  llvm::Value *EmitBlockLiteral(const BlockExpr *);\n\n  llvm::Function *GenerateBlockFunction(GlobalDecl GD,\n                                        const CGBlockInfo &Info,\n                                        const DeclMapTy &ldm,\n                                        bool IsLambdaConversionToBlock,\n                                        bool BuildGlobalBlock);\n\n  /// Check if \\p T is a C++ class that has a destructor that can throw.\n  static bool cxxDestructorCanThrow(QualType T);\n\n  llvm::Constant *GenerateCopyHelperFunction(const CGBlockInfo &blockInfo);\n  llvm::Constant *GenerateDestroyHelperFunction(const CGBlockInfo &blockInfo);\n  llvm::Constant *GenerateObjCAtomicSetterCopyHelperFunction(\n                                             const ObjCPropertyImplDecl *PID);\n  llvm::Constant *GenerateObjCAtomicGetterCopyHelperFunction(\n                                             const ObjCPropertyImplDecl *PID);\n  llvm::Value *EmitBlockCopyAndAutorelease(llvm::Value *Block, QualType Ty);\n\n  void BuildBlockRelease(llvm::Value *DeclPtr, BlockFieldFlags flags,\n                         bool CanThrow);\n\n  class AutoVarEmission;\n\n  void emitByrefStructureInit(const AutoVarEmission &emission);\n\n  /// Enter a cleanup to destroy a __block variable.  Note that this\n  /// cleanup should be a no-op if the variable hasn't left the stack\n  /// yet; if a cleanup is required for the variable itself, that needs\n  /// to be done externally.\n  ///\n  /// \\param Kind Cleanup kind.\n  ///\n  /// \\param Addr When \\p LoadBlockVarAddr is false, the address of the __block\n  /// structure that will be passed to _Block_object_dispose. When\n  /// \\p LoadBlockVarAddr is true, the address of the field of the block\n  /// structure that holds the address of the __block structure.\n  ///\n  /// \\param Flags The flag that will be passed to _Block_object_dispose.\n  ///\n  /// \\param LoadBlockVarAddr Indicates whether we need to emit a load from\n  /// \\p Addr to get the address of the __block structure.\n  void enterByrefCleanup(CleanupKind Kind, Address Addr, BlockFieldFlags Flags,\n                         bool LoadBlockVarAddr, bool CanThrow);\n\n  void setBlockContextParameter(const ImplicitParamDecl *D, unsigned argNum,\n                                llvm::Value *ptr);\n\n  Address LoadBlockStruct();\n  Address GetAddrOfBlockDecl(const VarDecl *var);\n\n  /// BuildBlockByrefAddress - Computes the location of the\n  /// data in a variable which is declared as __block.\n  Address emitBlockByrefAddress(Address baseAddr, const VarDecl *V,\n                                bool followForward = true);\n  Address emitBlockByrefAddress(Address baseAddr,\n                                const BlockByrefInfo &info,\n                                bool followForward,\n                                const llvm::Twine &name);\n\n  const BlockByrefInfo &getBlockByrefInfo(const VarDecl *var);\n\n  QualType BuildFunctionArgList(GlobalDecl GD, FunctionArgList &Args);\n\n  void GenerateCode(GlobalDecl GD, llvm::Function *Fn,\n                    const CGFunctionInfo &FnInfo);\n\n  /// Annotate the function with an attribute that disables TSan checking at\n  /// runtime.\n  void markAsIgnoreThreadCheckingAtRuntime(llvm::Function *Fn);\n\n  /// Emit code for the start of a function.\n  /// \\param Loc       The location to be associated with the function.\n  /// \\param StartLoc  The location of the function body.\n  void StartFunction(GlobalDecl GD,\n                     QualType RetTy,\n                     llvm::Function *Fn,\n                     const CGFunctionInfo &FnInfo,\n                     const FunctionArgList &Args,\n                     SourceLocation Loc = SourceLocation(),\n                     SourceLocation StartLoc = SourceLocation());\n\n  static bool IsConstructorDelegationValid(const CXXConstructorDecl *Ctor);\n\n  void EmitConstructorBody(FunctionArgList &Args);\n  void EmitDestructorBody(FunctionArgList &Args);\n  void emitImplicitAssignmentOperatorBody(FunctionArgList &Args);\n  void EmitFunctionBody(const Stmt *Body);\n  void EmitBlockWithFallThrough(llvm::BasicBlock *BB, const Stmt *S);\n\n  void EmitForwardingCallToLambda(const CXXMethodDecl *LambdaCallOperator,\n                                  CallArgList &CallArgs);\n  void EmitLambdaBlockInvokeBody();\n  void EmitLambdaDelegatingInvokeBody(const CXXMethodDecl *MD);\n  void EmitLambdaStaticInvokeBody(const CXXMethodDecl *MD);\n  void EmitLambdaVLACapture(const VariableArrayType *VAT, LValue LV) {\n    EmitStoreThroughLValue(RValue::get(VLASizeMap[VAT->getSizeExpr()]), LV);\n  }\n  void EmitAsanPrologueOrEpilogue(bool Prologue);\n\n  /// Emit the unified return block, trying to avoid its emission when\n  /// possible.\n  /// \\return The debug location of the user written return statement if the\n  /// return block is is avoided.\n  llvm::DebugLoc EmitReturnBlock();\n\n  /// FinishFunction - Complete IR generation of the current function. It is\n  /// legal to call this function even if there is no current insertion point.\n  void FinishFunction(SourceLocation EndLoc=SourceLocation());\n\n  void StartThunk(llvm::Function *Fn, GlobalDecl GD,\n                  const CGFunctionInfo &FnInfo, bool IsUnprototyped);\n\n  void EmitCallAndReturnForThunk(llvm::FunctionCallee Callee,\n                                 const ThunkInfo *Thunk, bool IsUnprototyped);\n\n  void FinishThunk();\n\n  /// Emit a musttail call for a thunk with a potentially adjusted this pointer.\n  void EmitMustTailThunk(GlobalDecl GD, llvm::Value *AdjustedThisPtr,\n                         llvm::FunctionCallee Callee);\n\n  /// Generate a thunk for the given method.\n  void generateThunk(llvm::Function *Fn, const CGFunctionInfo &FnInfo,\n                     GlobalDecl GD, const ThunkInfo &Thunk,\n                     bool IsUnprototyped);\n\n  llvm::Function *GenerateVarArgsThunk(llvm::Function *Fn,\n                                       const CGFunctionInfo &FnInfo,\n                                       GlobalDecl GD, const ThunkInfo &Thunk);\n\n  void EmitCtorPrologue(const CXXConstructorDecl *CD, CXXCtorType Type,\n                        FunctionArgList &Args);\n\n  void EmitInitializerForField(FieldDecl *Field, LValue LHS, Expr *Init);\n\n  /// Struct with all information about dynamic [sub]class needed to set vptr.\n  struct VPtr {\n    BaseSubobject Base;\n    const CXXRecordDecl *NearestVBase;\n    CharUnits OffsetFromNearestVBase;\n    const CXXRecordDecl *VTableClass;\n  };\n\n  /// Initialize the vtable pointer of the given subobject.\n  void InitializeVTablePointer(const VPtr &vptr);\n\n  typedef llvm::SmallVector<VPtr, 4> VPtrsVector;\n\n  typedef llvm::SmallPtrSet<const CXXRecordDecl *, 4> VisitedVirtualBasesSetTy;\n  VPtrsVector getVTablePointers(const CXXRecordDecl *VTableClass);\n\n  void getVTablePointers(BaseSubobject Base, const CXXRecordDecl *NearestVBase,\n                         CharUnits OffsetFromNearestVBase,\n                         bool BaseIsNonVirtualPrimaryBase,\n                         const CXXRecordDecl *VTableClass,\n                         VisitedVirtualBasesSetTy &VBases, VPtrsVector &vptrs);\n\n  void InitializeVTablePointers(const CXXRecordDecl *ClassDecl);\n\n  /// GetVTablePtr - Return the Value of the vtable pointer member pointed\n  /// to by This.\n  llvm::Value *GetVTablePtr(Address This, llvm::Type *VTableTy,\n                            const CXXRecordDecl *VTableClass);\n\n  enum CFITypeCheckKind {\n    CFITCK_VCall,\n    CFITCK_NVCall,\n    CFITCK_DerivedCast,\n    CFITCK_UnrelatedCast,\n    CFITCK_ICall,\n    CFITCK_NVMFCall,\n    CFITCK_VMFCall,\n  };\n\n  /// Derived is the presumed address of an object of type T after a\n  /// cast. If T is a polymorphic class type, emit a check that the virtual\n  /// table for Derived belongs to a class derived from T.\n  void EmitVTablePtrCheckForCast(QualType T, llvm::Value *Derived,\n                                 bool MayBeNull, CFITypeCheckKind TCK,\n                                 SourceLocation Loc);\n\n  /// EmitVTablePtrCheckForCall - Virtual method MD is being called via VTable.\n  /// If vptr CFI is enabled, emit a check that VTable is valid.\n  void EmitVTablePtrCheckForCall(const CXXRecordDecl *RD, llvm::Value *VTable,\n                                 CFITypeCheckKind TCK, SourceLocation Loc);\n\n  /// EmitVTablePtrCheck - Emit a check that VTable is a valid virtual table for\n  /// RD using llvm.type.test.\n  void EmitVTablePtrCheck(const CXXRecordDecl *RD, llvm::Value *VTable,\n                          CFITypeCheckKind TCK, SourceLocation Loc);\n\n  /// If whole-program virtual table optimization is enabled, emit an assumption\n  /// that VTable is a member of RD's type identifier. Or, if vptr CFI is\n  /// enabled, emit a check that VTable is a member of RD's type identifier.\n  void EmitTypeMetadataCodeForVCall(const CXXRecordDecl *RD,\n                                    llvm::Value *VTable, SourceLocation Loc);\n\n  /// Returns whether we should perform a type checked load when loading a\n  /// virtual function for virtual calls to members of RD. This is generally\n  /// true when both vcall CFI and whole-program-vtables are enabled.\n  bool ShouldEmitVTableTypeCheckedLoad(const CXXRecordDecl *RD);\n\n  /// Emit a type checked load from the given vtable.\n  llvm::Value *EmitVTableTypeCheckedLoad(const CXXRecordDecl *RD, llvm::Value *VTable,\n                                         uint64_t VTableByteOffset);\n\n  /// EnterDtorCleanups - Enter the cleanups necessary to complete the\n  /// given phase of destruction for a destructor.  The end result\n  /// should call destructors on members and base classes in reverse\n  /// order of their construction.\n  void EnterDtorCleanups(const CXXDestructorDecl *Dtor, CXXDtorType Type);\n\n  /// ShouldInstrumentFunction - Return true if the current function should be\n  /// instrumented with __cyg_profile_func_* calls\n  bool ShouldInstrumentFunction();\n\n  /// ShouldXRayInstrument - Return true if the current function should be\n  /// instrumented with XRay nop sleds.\n  bool ShouldXRayInstrumentFunction() const;\n\n  /// AlwaysEmitXRayCustomEvents - Return true if we must unconditionally emit\n  /// XRay custom event handling calls.\n  bool AlwaysEmitXRayCustomEvents() const;\n\n  /// AlwaysEmitXRayTypedEvents - Return true if clang must unconditionally emit\n  /// XRay typed event handling calls.\n  bool AlwaysEmitXRayTypedEvents() const;\n\n  /// Encode an address into a form suitable for use in a function prologue.\n  llvm::Constant *EncodeAddrForUseInPrologue(llvm::Function *F,\n                                             llvm::Constant *Addr);\n\n  /// Decode an address used in a function prologue, encoded by \\c\n  /// EncodeAddrForUseInPrologue.\n  llvm::Value *DecodeAddrUsedInPrologue(llvm::Value *F,\n                                        llvm::Value *EncodedAddr);\n\n  /// EmitFunctionProlog - Emit the target specific LLVM code to load the\n  /// arguments for the given function. This is also responsible for naming the\n  /// LLVM function arguments.\n  void EmitFunctionProlog(const CGFunctionInfo &FI,\n                          llvm::Function *Fn,\n                          const FunctionArgList &Args);\n\n  /// EmitFunctionEpilog - Emit the target specific LLVM code to return the\n  /// given temporary.\n  void EmitFunctionEpilog(const CGFunctionInfo &FI, bool EmitRetDbgLoc,\n                          SourceLocation EndLoc);\n\n  /// Emit a test that checks if the return value \\p RV is nonnull.\n  void EmitReturnValueCheck(llvm::Value *RV);\n\n  /// EmitStartEHSpec - Emit the start of the exception spec.\n  void EmitStartEHSpec(const Decl *D);\n\n  /// EmitEndEHSpec - Emit the end of the exception spec.\n  void EmitEndEHSpec(const Decl *D);\n\n  /// getTerminateLandingPad - Return a landing pad that just calls terminate.\n  llvm::BasicBlock *getTerminateLandingPad();\n\n  /// getTerminateLandingPad - Return a cleanup funclet that just calls\n  /// terminate.\n  llvm::BasicBlock *getTerminateFunclet();\n\n  /// getTerminateHandler - Return a handler (not a landing pad, just\n  /// a catch handler) that just calls terminate.  This is used when\n  /// a terminate scope encloses a try.\n  llvm::BasicBlock *getTerminateHandler();\n\n  llvm::Type *ConvertTypeForMem(QualType T);\n  llvm::Type *ConvertType(QualType T);\n  llvm::Type *ConvertType(const TypeDecl *T) {\n    return ConvertType(getContext().getTypeDeclType(T));\n  }\n\n  /// LoadObjCSelf - Load the value of self. This function is only valid while\n  /// generating code for an Objective-C method.\n  llvm::Value *LoadObjCSelf();\n\n  /// TypeOfSelfObject - Return type of object that this self represents.\n  QualType TypeOfSelfObject();\n\n  /// getEvaluationKind - Return the TypeEvaluationKind of QualType \\c T.\n  static TypeEvaluationKind getEvaluationKind(QualType T);\n\n  static bool hasScalarEvaluationKind(QualType T) {\n    return getEvaluationKind(T) == TEK_Scalar;\n  }\n\n  static bool hasAggregateEvaluationKind(QualType T) {\n    return getEvaluationKind(T) == TEK_Aggregate;\n  }\n\n  /// createBasicBlock - Create an LLVM basic block.\n  llvm::BasicBlock *createBasicBlock(const Twine &name = \"\",\n                                     llvm::Function *parent = nullptr,\n                                     llvm::BasicBlock *before = nullptr) {\n    return llvm::BasicBlock::Create(getLLVMContext(), name, parent, before);\n  }\n\n  /// getBasicBlockForLabel - Return the LLVM basicblock that the specified\n  /// label maps to.\n  JumpDest getJumpDestForLabel(const LabelDecl *S);\n\n  /// SimplifyForwardingBlocks - If the given basic block is only a branch to\n  /// another basic block, simplify it. This assumes that no other code could\n  /// potentially reference the basic block.\n  void SimplifyForwardingBlocks(llvm::BasicBlock *BB);\n\n  /// EmitBlock - Emit the given block \\arg BB and set it as the insert point,\n  /// adding a fall-through branch from the current insert block if\n  /// necessary. It is legal to call this function even if there is no current\n  /// insertion point.\n  ///\n  /// IsFinished - If true, indicates that the caller has finished emitting\n  /// branches to the given block and does not expect to emit code into it. This\n  /// means the block can be ignored if it is unreachable.\n  void EmitBlock(llvm::BasicBlock *BB, bool IsFinished=false);\n\n  /// EmitBlockAfterUses - Emit the given block somewhere hopefully\n  /// near its uses, and leave the insertion point in it.\n  void EmitBlockAfterUses(llvm::BasicBlock *BB);\n\n  /// EmitBranch - Emit a branch to the specified basic block from the current\n  /// insert block, taking care to avoid creation of branches from dummy\n  /// blocks. It is legal to call this function even if there is no current\n  /// insertion point.\n  ///\n  /// This function clears the current insertion point. The caller should follow\n  /// calls to this function with calls to Emit*Block prior to generation new\n  /// code.\n  void EmitBranch(llvm::BasicBlock *Block);\n\n  /// HaveInsertPoint - True if an insertion point is defined. If not, this\n  /// indicates that the current code being emitted is unreachable.\n  bool HaveInsertPoint() const {\n    return Builder.GetInsertBlock() != nullptr;\n  }\n\n  /// EnsureInsertPoint - Ensure that an insertion point is defined so that\n  /// emitted IR has a place to go. Note that by definition, if this function\n  /// creates a block then that block is unreachable; callers may do better to\n  /// detect when no insertion point is defined and simply skip IR generation.\n  void EnsureInsertPoint() {\n    if (!HaveInsertPoint())\n      EmitBlock(createBasicBlock());\n  }\n\n  /// ErrorUnsupported - Print out an error that codegen doesn't support the\n  /// specified stmt yet.\n  void ErrorUnsupported(const Stmt *S, const char *Type);\n\n  //===--------------------------------------------------------------------===//\n  //                                  Helpers\n  //===--------------------------------------------------------------------===//\n\n  LValue MakeAddrLValue(Address Addr, QualType T,\n                        AlignmentSource Source = AlignmentSource::Type) {\n    return LValue::MakeAddr(Addr, T, getContext(), LValueBaseInfo(Source),\n                            CGM.getTBAAAccessInfo(T));\n  }\n\n  LValue MakeAddrLValue(Address Addr, QualType T, LValueBaseInfo BaseInfo,\n                        TBAAAccessInfo TBAAInfo) {\n    return LValue::MakeAddr(Addr, T, getContext(), BaseInfo, TBAAInfo);\n  }\n\n  LValue MakeAddrLValue(llvm::Value *V, QualType T, CharUnits Alignment,\n                        AlignmentSource Source = AlignmentSource::Type) {\n    return LValue::MakeAddr(Address(V, Alignment), T, getContext(),\n                            LValueBaseInfo(Source), CGM.getTBAAAccessInfo(T));\n  }\n\n  LValue MakeAddrLValue(llvm::Value *V, QualType T, CharUnits Alignment,\n                        LValueBaseInfo BaseInfo, TBAAAccessInfo TBAAInfo) {\n    return LValue::MakeAddr(Address(V, Alignment), T, getContext(),\n                            BaseInfo, TBAAInfo);\n  }\n\n  LValue MakeNaturalAlignPointeeAddrLValue(llvm::Value *V, QualType T);\n  LValue MakeNaturalAlignAddrLValue(llvm::Value *V, QualType T);\n\n  Address EmitLoadOfReference(LValue RefLVal,\n                              LValueBaseInfo *PointeeBaseInfo = nullptr,\n                              TBAAAccessInfo *PointeeTBAAInfo = nullptr);\n  LValue EmitLoadOfReferenceLValue(LValue RefLVal);\n  LValue EmitLoadOfReferenceLValue(Address RefAddr, QualType RefTy,\n                                   AlignmentSource Source =\n                                       AlignmentSource::Type) {\n    LValue RefLVal = MakeAddrLValue(RefAddr, RefTy, LValueBaseInfo(Source),\n                                    CGM.getTBAAAccessInfo(RefTy));\n    return EmitLoadOfReferenceLValue(RefLVal);\n  }\n\n  Address EmitLoadOfPointer(Address Ptr, const PointerType *PtrTy,\n                            LValueBaseInfo *BaseInfo = nullptr,\n                            TBAAAccessInfo *TBAAInfo = nullptr);\n  LValue EmitLoadOfPointerLValue(Address Ptr, const PointerType *PtrTy);\n\n  /// CreateTempAlloca - This creates an alloca and inserts it into the entry\n  /// block if \\p ArraySize is nullptr, otherwise inserts it at the current\n  /// insertion point of the builder. The caller is responsible for setting an\n  /// appropriate alignment on\n  /// the alloca.\n  ///\n  /// \\p ArraySize is the number of array elements to be allocated if it\n  ///    is not nullptr.\n  ///\n  /// LangAS::Default is the address space of pointers to local variables and\n  /// temporaries, as exposed in the source language. In certain\n  /// configurations, this is not the same as the alloca address space, and a\n  /// cast is needed to lift the pointer from the alloca AS into\n  /// LangAS::Default. This can happen when the target uses a restricted\n  /// address space for the stack but the source language requires\n  /// LangAS::Default to be a generic address space. The latter condition is\n  /// common for most programming languages; OpenCL is an exception in that\n  /// LangAS::Default is the private address space, which naturally maps\n  /// to the stack.\n  ///\n  /// Because the address of a temporary is often exposed to the program in\n  /// various ways, this function will perform the cast. The original alloca\n  /// instruction is returned through \\p Alloca if it is not nullptr.\n  ///\n  /// The cast is not performaed in CreateTempAllocaWithoutCast. This is\n  /// more efficient if the caller knows that the address will not be exposed.\n  llvm::AllocaInst *CreateTempAlloca(llvm::Type *Ty, const Twine &Name = \"tmp\",\n                                     llvm::Value *ArraySize = nullptr);\n  Address CreateTempAlloca(llvm::Type *Ty, CharUnits align,\n                           const Twine &Name = \"tmp\",\n                           llvm::Value *ArraySize = nullptr,\n                           Address *Alloca = nullptr);\n  Address CreateTempAllocaWithoutCast(llvm::Type *Ty, CharUnits align,\n                                      const Twine &Name = \"tmp\",\n                                      llvm::Value *ArraySize = nullptr);\n\n  /// CreateDefaultAlignedTempAlloca - This creates an alloca with the\n  /// default ABI alignment of the given LLVM type.\n  ///\n  /// IMPORTANT NOTE: This is *not* generally the right alignment for\n  /// any given AST type that happens to have been lowered to the\n  /// given IR type.  This should only ever be used for function-local,\n  /// IR-driven manipulations like saving and restoring a value.  Do\n  /// not hand this address off to arbitrary IRGen routines, and especially\n  /// do not pass it as an argument to a function that might expect a\n  /// properly ABI-aligned value.\n  Address CreateDefaultAlignTempAlloca(llvm::Type *Ty,\n                                       const Twine &Name = \"tmp\");\n\n  /// InitTempAlloca - Provide an initial value for the given alloca which\n  /// will be observable at all locations in the function.\n  ///\n  /// The address should be something that was returned from one of\n  /// the CreateTempAlloca or CreateMemTemp routines, and the\n  /// initializer must be valid in the entry block (i.e. it must\n  /// either be a constant or an argument value).\n  void InitTempAlloca(Address Alloca, llvm::Value *Value);\n\n  /// CreateIRTemp - Create a temporary IR object of the given type, with\n  /// appropriate alignment. This routine should only be used when an temporary\n  /// value needs to be stored into an alloca (for example, to avoid explicit\n  /// PHI construction), but the type is the IR type, not the type appropriate\n  /// for storing in memory.\n  ///\n  /// That is, this is exactly equivalent to CreateMemTemp, but calling\n  /// ConvertType instead of ConvertTypeForMem.\n  Address CreateIRTemp(QualType T, const Twine &Name = \"tmp\");\n\n  /// CreateMemTemp - Create a temporary memory object of the given type, with\n  /// appropriate alignmen and cast it to the default address space. Returns\n  /// the original alloca instruction by \\p Alloca if it is not nullptr.\n  Address CreateMemTemp(QualType T, const Twine &Name = \"tmp\",\n                        Address *Alloca = nullptr);\n  Address CreateMemTemp(QualType T, CharUnits Align, const Twine &Name = \"tmp\",\n                        Address *Alloca = nullptr);\n\n  /// CreateMemTemp - Create a temporary memory object of the given type, with\n  /// appropriate alignmen without casting it to the default address space.\n  Address CreateMemTempWithoutCast(QualType T, const Twine &Name = \"tmp\");\n  Address CreateMemTempWithoutCast(QualType T, CharUnits Align,\n                                   const Twine &Name = \"tmp\");\n\n  /// CreateAggTemp - Create a temporary memory object for the given\n  /// aggregate type.\n  AggValueSlot CreateAggTemp(QualType T, const Twine &Name = \"tmp\",\n                             Address *Alloca = nullptr) {\n    return AggValueSlot::forAddr(CreateMemTemp(T, Name, Alloca),\n                                 T.getQualifiers(),\n                                 AggValueSlot::IsNotDestructed,\n                                 AggValueSlot::DoesNotNeedGCBarriers,\n                                 AggValueSlot::IsNotAliased,\n                                 AggValueSlot::DoesNotOverlap);\n  }\n\n  /// Emit a cast to void* in the appropriate address space.\n  llvm::Value *EmitCastToVoidPtr(llvm::Value *value);\n\n  /// EvaluateExprAsBool - Perform the usual unary conversions on the specified\n  /// expression and compare the result against zero, returning an Int1Ty value.\n  llvm::Value *EvaluateExprAsBool(const Expr *E);\n\n  /// EmitIgnoredExpr - Emit an expression in a context which ignores the result.\n  void EmitIgnoredExpr(const Expr *E);\n\n  /// EmitAnyExpr - Emit code to compute the specified expression which can have\n  /// any type.  The result is returned as an RValue struct.  If this is an\n  /// aggregate expression, the aggloc/agglocvolatile arguments indicate where\n  /// the result should be returned.\n  ///\n  /// \\param ignoreResult True if the resulting value isn't used.\n  RValue EmitAnyExpr(const Expr *E,\n                     AggValueSlot aggSlot = AggValueSlot::ignored(),\n                     bool ignoreResult = false);\n\n  // EmitVAListRef - Emit a \"reference\" to a va_list; this is either the address\n  // or the value of the expression, depending on how va_list is defined.\n  Address EmitVAListRef(const Expr *E);\n\n  /// Emit a \"reference\" to a __builtin_ms_va_list; this is\n  /// always the value of the expression, because a __builtin_ms_va_list is a\n  /// pointer to a char.\n  Address EmitMSVAListRef(const Expr *E);\n\n  /// EmitAnyExprToTemp - Similarly to EmitAnyExpr(), however, the result will\n  /// always be accessible even if no aggregate location is provided.\n  RValue EmitAnyExprToTemp(const Expr *E);\n\n  /// EmitAnyExprToMem - Emits the code necessary to evaluate an\n  /// arbitrary expression into the given memory location.\n  void EmitAnyExprToMem(const Expr *E, Address Location,\n                        Qualifiers Quals, bool IsInitializer);\n\n  void EmitAnyExprToExn(const Expr *E, Address Addr);\n\n  /// EmitExprAsInit - Emits the code necessary to initialize a\n  /// location in memory with the given initializer.\n  void EmitExprAsInit(const Expr *init, const ValueDecl *D, LValue lvalue,\n                      bool capturedByInit);\n\n  /// hasVolatileMember - returns true if aggregate type has a volatile\n  /// member.\n  bool hasVolatileMember(QualType T) {\n    if (const RecordType *RT = T->getAs<RecordType>()) {\n      const RecordDecl *RD = cast<RecordDecl>(RT->getDecl());\n      return RD->hasVolatileMember();\n    }\n    return false;\n  }\n\n  /// Determine whether a return value slot may overlap some other object.\n  AggValueSlot::Overlap_t getOverlapForReturnValue() {\n    // FIXME: Assuming no overlap here breaks guaranteed copy elision for base\n    // class subobjects. These cases may need to be revisited depending on the\n    // resolution of the relevant core issue.\n    return AggValueSlot::DoesNotOverlap;\n  }\n\n  /// Determine whether a field initialization may overlap some other object.\n  AggValueSlot::Overlap_t getOverlapForFieldInit(const FieldDecl *FD);\n\n  /// Determine whether a base class initialization may overlap some other\n  /// object.\n  AggValueSlot::Overlap_t getOverlapForBaseInit(const CXXRecordDecl *RD,\n                                                const CXXRecordDecl *BaseRD,\n                                                bool IsVirtual);\n\n  /// Emit an aggregate assignment.\n  void EmitAggregateAssign(LValue Dest, LValue Src, QualType EltTy) {\n    bool IsVolatile = hasVolatileMember(EltTy);\n    EmitAggregateCopy(Dest, Src, EltTy, AggValueSlot::MayOverlap, IsVolatile);\n  }\n\n  void EmitAggregateCopyCtor(LValue Dest, LValue Src,\n                             AggValueSlot::Overlap_t MayOverlap) {\n    EmitAggregateCopy(Dest, Src, Src.getType(), MayOverlap);\n  }\n\n  /// EmitAggregateCopy - Emit an aggregate copy.\n  ///\n  /// \\param isVolatile \\c true iff either the source or the destination is\n  ///        volatile.\n  /// \\param MayOverlap Whether the tail padding of the destination might be\n  ///        occupied by some other object. More efficient code can often be\n  ///        generated if not.\n  void EmitAggregateCopy(LValue Dest, LValue Src, QualType EltTy,\n                         AggValueSlot::Overlap_t MayOverlap,\n                         bool isVolatile = false);\n\n  /// GetAddrOfLocalVar - Return the address of a local variable.\n  Address GetAddrOfLocalVar(const VarDecl *VD) {\n    auto it = LocalDeclMap.find(VD);\n    assert(it != LocalDeclMap.end() &&\n           \"Invalid argument to GetAddrOfLocalVar(), no decl!\");\n    return it->second;\n  }\n\n  /// Given an opaque value expression, return its LValue mapping if it exists,\n  /// otherwise create one.\n  LValue getOrCreateOpaqueLValueMapping(const OpaqueValueExpr *e);\n\n  /// Given an opaque value expression, return its RValue mapping if it exists,\n  /// otherwise create one.\n  RValue getOrCreateOpaqueRValueMapping(const OpaqueValueExpr *e);\n\n  /// Get the index of the current ArrayInitLoopExpr, if any.\n  llvm::Value *getArrayInitIndex() { return ArrayInitIndex; }\n\n  /// getAccessedFieldNo - Given an encoded value and a result number, return\n  /// the input field number being accessed.\n  static unsigned getAccessedFieldNo(unsigned Idx, const llvm::Constant *Elts);\n\n  llvm::BlockAddress *GetAddrOfLabel(const LabelDecl *L);\n  llvm::BasicBlock *GetIndirectGotoBlock();\n\n  /// Check if \\p E is a C++ \"this\" pointer wrapped in value-preserving casts.\n  static bool IsWrappedCXXThis(const Expr *E);\n\n  /// EmitNullInitialization - Generate code to set a value of the given type to\n  /// null, If the type contains data member pointers, they will be initialized\n  /// to -1 in accordance with the Itanium C++ ABI.\n  void EmitNullInitialization(Address DestPtr, QualType Ty);\n\n  /// Emits a call to an LLVM variable-argument intrinsic, either\n  /// \\c llvm.va_start or \\c llvm.va_end.\n  /// \\param ArgValue A reference to the \\c va_list as emitted by either\n  /// \\c EmitVAListRef or \\c EmitMSVAListRef.\n  /// \\param IsStart If \\c true, emits a call to \\c llvm.va_start; otherwise,\n  /// calls \\c llvm.va_end.\n  llvm::Value *EmitVAStartEnd(llvm::Value *ArgValue, bool IsStart);\n\n  /// Generate code to get an argument from the passed in pointer\n  /// and update it accordingly.\n  /// \\param VE The \\c VAArgExpr for which to generate code.\n  /// \\param VAListAddr Receives a reference to the \\c va_list as emitted by\n  /// either \\c EmitVAListRef or \\c EmitMSVAListRef.\n  /// \\returns A pointer to the argument.\n  // FIXME: We should be able to get rid of this method and use the va_arg\n  // instruction in LLVM instead once it works well enough.\n  Address EmitVAArg(VAArgExpr *VE, Address &VAListAddr);\n\n  /// emitArrayLength - Compute the length of an array, even if it's a\n  /// VLA, and drill down to the base element type.\n  llvm::Value *emitArrayLength(const ArrayType *arrayType,\n                               QualType &baseType,\n                               Address &addr);\n\n  /// EmitVLASize - Capture all the sizes for the VLA expressions in\n  /// the given variably-modified type and store them in the VLASizeMap.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitVariablyModifiedType(QualType Ty);\n\n  struct VlaSizePair {\n    llvm::Value *NumElts;\n    QualType Type;\n\n    VlaSizePair(llvm::Value *NE, QualType T) : NumElts(NE), Type(T) {}\n  };\n\n  /// Return the number of elements for a single dimension\n  /// for the given array type.\n  VlaSizePair getVLAElements1D(const VariableArrayType *vla);\n  VlaSizePair getVLAElements1D(QualType vla);\n\n  /// Returns an LLVM value that corresponds to the size,\n  /// in non-variably-sized elements, of a variable length array type,\n  /// plus that largest non-variably-sized element type.  Assumes that\n  /// the type has already been emitted with EmitVariablyModifiedType.\n  VlaSizePair getVLASize(const VariableArrayType *vla);\n  VlaSizePair getVLASize(QualType vla);\n\n  /// LoadCXXThis - Load the value of 'this'. This function is only valid while\n  /// generating code for an C++ member function.\n  llvm::Value *LoadCXXThis() {\n    assert(CXXThisValue && \"no 'this' value for this function\");\n    return CXXThisValue;\n  }\n  Address LoadCXXThisAddress();\n\n  /// LoadCXXVTT - Load the VTT parameter to base constructors/destructors have\n  /// virtual bases.\n  // FIXME: Every place that calls LoadCXXVTT is something\n  // that needs to be abstracted properly.\n  llvm::Value *LoadCXXVTT() {\n    assert(CXXStructorImplicitParamValue && \"no VTT value for this function\");\n    return CXXStructorImplicitParamValue;\n  }\n\n  /// GetAddressOfBaseOfCompleteClass - Convert the given pointer to a\n  /// complete class to the given direct base.\n  Address\n  GetAddressOfDirectBaseInCompleteClass(Address Value,\n                                        const CXXRecordDecl *Derived,\n                                        const CXXRecordDecl *Base,\n                                        bool BaseIsVirtual);\n\n  static bool ShouldNullCheckClassCastValue(const CastExpr *Cast);\n\n  /// GetAddressOfBaseClass - This function will add the necessary delta to the\n  /// load of 'this' and returns address of the base class.\n  Address GetAddressOfBaseClass(Address Value,\n                                const CXXRecordDecl *Derived,\n                                CastExpr::path_const_iterator PathBegin,\n                                CastExpr::path_const_iterator PathEnd,\n                                bool NullCheckValue, SourceLocation Loc);\n\n  Address GetAddressOfDerivedClass(Address Value,\n                                   const CXXRecordDecl *Derived,\n                                   CastExpr::path_const_iterator PathBegin,\n                                   CastExpr::path_const_iterator PathEnd,\n                                   bool NullCheckValue);\n\n  /// GetVTTParameter - Return the VTT parameter that should be passed to a\n  /// base constructor/destructor with virtual bases.\n  /// FIXME: VTTs are Itanium ABI-specific, so the definition should move\n  /// to ItaniumCXXABI.cpp together with all the references to VTT.\n  llvm::Value *GetVTTParameter(GlobalDecl GD, bool ForVirtualBase,\n                               bool Delegating);\n\n  void EmitDelegateCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                      CXXCtorType CtorType,\n                                      const FunctionArgList &Args,\n                                      SourceLocation Loc);\n  // It's important not to confuse this and the previous function. Delegating\n  // constructors are the C++0x feature. The constructor delegate optimization\n  // is used to reduce duplication in the base and complete consturctors where\n  // they are substantially the same.\n  void EmitDelegatingCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                        const FunctionArgList &Args);\n\n  /// Emit a call to an inheriting constructor (that is, one that invokes a\n  /// constructor inherited from a base class) by inlining its definition. This\n  /// is necessary if the ABI does not support forwarding the arguments to the\n  /// base class constructor (because they're variadic or similar).\n  void EmitInlinedInheritingCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                               CXXCtorType CtorType,\n                                               bool ForVirtualBase,\n                                               bool Delegating,\n                                               CallArgList &Args);\n\n  /// Emit a call to a constructor inherited from a base class, passing the\n  /// current constructor's arguments along unmodified (without even making\n  /// a copy).\n  void EmitInheritedCXXConstructorCall(const CXXConstructorDecl *D,\n                                       bool ForVirtualBase, Address This,\n                                       bool InheritedFromVBase,\n                                       const CXXInheritedCtorInitExpr *E);\n\n  void EmitCXXConstructorCall(const CXXConstructorDecl *D, CXXCtorType Type,\n                              bool ForVirtualBase, bool Delegating,\n                              AggValueSlot ThisAVS, const CXXConstructExpr *E);\n\n  void EmitCXXConstructorCall(const CXXConstructorDecl *D, CXXCtorType Type,\n                              bool ForVirtualBase, bool Delegating,\n                              Address This, CallArgList &Args,\n                              AggValueSlot::Overlap_t Overlap,\n                              SourceLocation Loc, bool NewPointerIsChecked);\n\n  /// Emit assumption load for all bases. Requires to be be called only on\n  /// most-derived class and not under construction of the object.\n  void EmitVTableAssumptionLoads(const CXXRecordDecl *ClassDecl, Address This);\n\n  /// Emit assumption that vptr load == global vtable.\n  void EmitVTableAssumptionLoad(const VPtr &vptr, Address This);\n\n  void EmitSynthesizedCXXCopyCtorCall(const CXXConstructorDecl *D,\n                                      Address This, Address Src,\n                                      const CXXConstructExpr *E);\n\n  void EmitCXXAggrConstructorCall(const CXXConstructorDecl *D,\n                                  const ArrayType *ArrayTy,\n                                  Address ArrayPtr,\n                                  const CXXConstructExpr *E,\n                                  bool NewPointerIsChecked,\n                                  bool ZeroInitialization = false);\n\n  void EmitCXXAggrConstructorCall(const CXXConstructorDecl *D,\n                                  llvm::Value *NumElements,\n                                  Address ArrayPtr,\n                                  const CXXConstructExpr *E,\n                                  bool NewPointerIsChecked,\n                                  bool ZeroInitialization = false);\n\n  static Destroyer destroyCXXObject;\n\n  void EmitCXXDestructorCall(const CXXDestructorDecl *D, CXXDtorType Type,\n                             bool ForVirtualBase, bool Delegating, Address This,\n                             QualType ThisTy);\n\n  void EmitNewArrayInitializer(const CXXNewExpr *E, QualType elementType,\n                               llvm::Type *ElementTy, Address NewPtr,\n                               llvm::Value *NumElements,\n                               llvm::Value *AllocSizeWithoutCookie);\n\n  void EmitCXXTemporary(const CXXTemporary *Temporary, QualType TempType,\n                        Address Ptr);\n\n  llvm::Value *EmitLifetimeStart(uint64_t Size, llvm::Value *Addr);\n  void EmitLifetimeEnd(llvm::Value *Size, llvm::Value *Addr);\n\n  llvm::Value *EmitCXXNewExpr(const CXXNewExpr *E);\n  void EmitCXXDeleteExpr(const CXXDeleteExpr *E);\n\n  void EmitDeleteCall(const FunctionDecl *DeleteFD, llvm::Value *Ptr,\n                      QualType DeleteTy, llvm::Value *NumElements = nullptr,\n                      CharUnits CookieSize = CharUnits());\n\n  RValue EmitBuiltinNewDeleteCall(const FunctionProtoType *Type,\n                                  const CallExpr *TheCallExpr, bool IsDelete);\n\n  llvm::Value *EmitCXXTypeidExpr(const CXXTypeidExpr *E);\n  llvm::Value *EmitDynamicCast(Address V, const CXXDynamicCastExpr *DCE);\n  Address EmitCXXUuidofExpr(const CXXUuidofExpr *E);\n\n  /// Situations in which we might emit a check for the suitability of a\n  /// pointer or glvalue. Needs to be kept in sync with ubsan_handlers.cpp in\n  /// compiler-rt.\n  enum TypeCheckKind {\n    /// Checking the operand of a load. Must be suitably sized and aligned.\n    TCK_Load,\n    /// Checking the destination of a store. Must be suitably sized and aligned.\n    TCK_Store,\n    /// Checking the bound value in a reference binding. Must be suitably sized\n    /// and aligned, but is not required to refer to an object (until the\n    /// reference is used), per core issue 453.\n    TCK_ReferenceBinding,\n    /// Checking the object expression in a non-static data member access. Must\n    /// be an object within its lifetime.\n    TCK_MemberAccess,\n    /// Checking the 'this' pointer for a call to a non-static member function.\n    /// Must be an object within its lifetime.\n    TCK_MemberCall,\n    /// Checking the 'this' pointer for a constructor call.\n    TCK_ConstructorCall,\n    /// Checking the operand of a static_cast to a derived pointer type. Must be\n    /// null or an object within its lifetime.\n    TCK_DowncastPointer,\n    /// Checking the operand of a static_cast to a derived reference type. Must\n    /// be an object within its lifetime.\n    TCK_DowncastReference,\n    /// Checking the operand of a cast to a base object. Must be suitably sized\n    /// and aligned.\n    TCK_Upcast,\n    /// Checking the operand of a cast to a virtual base object. Must be an\n    /// object within its lifetime.\n    TCK_UpcastToVirtualBase,\n    /// Checking the value assigned to a _Nonnull pointer. Must not be null.\n    TCK_NonnullAssign,\n    /// Checking the operand of a dynamic_cast or a typeid expression.  Must be\n    /// null or an object within its lifetime.\n    TCK_DynamicOperation\n  };\n\n  /// Determine whether the pointer type check \\p TCK permits null pointers.\n  static bool isNullPointerAllowed(TypeCheckKind TCK);\n\n  /// Determine whether the pointer type check \\p TCK requires a vptr check.\n  static bool isVptrCheckRequired(TypeCheckKind TCK, QualType Ty);\n\n  /// Whether any type-checking sanitizers are enabled. If \\c false,\n  /// calls to EmitTypeCheck can be skipped.\n  bool sanitizePerformTypeCheck() const;\n\n  /// Emit a check that \\p V is the address of storage of the\n  /// appropriate size and alignment for an object of type \\p Type\n  /// (or if ArraySize is provided, for an array of that bound).\n  void EmitTypeCheck(TypeCheckKind TCK, SourceLocation Loc, llvm::Value *V,\n                     QualType Type, CharUnits Alignment = CharUnits::Zero(),\n                     SanitizerSet SkippedChecks = SanitizerSet(),\n                     llvm::Value *ArraySize = nullptr);\n\n  /// Emit a check that \\p Base points into an array object, which\n  /// we can access at index \\p Index. \\p Accessed should be \\c false if we\n  /// this expression is used as an lvalue, for instance in \"&Arr[Idx]\".\n  void EmitBoundsCheck(const Expr *E, const Expr *Base, llvm::Value *Index,\n                       QualType IndexType, bool Accessed);\n\n  llvm::Value *EmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                       bool isInc, bool isPre);\n  ComplexPairTy EmitComplexPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                         bool isInc, bool isPre);\n\n  /// Converts Location to a DebugLoc, if debug information is enabled.\n  llvm::DebugLoc SourceLocToDebugLoc(SourceLocation Location);\n\n  /// Get the record field index as represented in debug info.\n  unsigned getDebugInfoFIndex(const RecordDecl *Rec, unsigned FieldIndex);\n\n\n  //===--------------------------------------------------------------------===//\n  //                            Declaration Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitDecl - Emit a declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitDecl(const Decl &D);\n\n  /// EmitVarDecl - Emit a local variable declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitVarDecl(const VarDecl &D);\n\n  void EmitScalarInit(const Expr *init, const ValueDecl *D, LValue lvalue,\n                      bool capturedByInit);\n\n  typedef void SpecialInitFn(CodeGenFunction &Init, const VarDecl &D,\n                             llvm::Value *Address);\n\n  /// Determine whether the given initializer is trivial in the sense\n  /// that it requires no code to be generated.\n  bool isTrivialInitializer(const Expr *Init);\n\n  /// EmitAutoVarDecl - Emit an auto variable declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitAutoVarDecl(const VarDecl &D);\n\n  class AutoVarEmission {\n    friend class CodeGenFunction;\n\n    const VarDecl *Variable;\n\n    /// The address of the alloca for languages with explicit address space\n    /// (e.g. OpenCL) or alloca casted to generic pointer for address space\n    /// agnostic languages (e.g. C++). Invalid if the variable was emitted\n    /// as a global constant.\n    Address Addr;\n\n    llvm::Value *NRVOFlag;\n\n    /// True if the variable is a __block variable that is captured by an\n    /// escaping block.\n    bool IsEscapingByRef;\n\n    /// True if the variable is of aggregate type and has a constant\n    /// initializer.\n    bool IsConstantAggregate;\n\n    /// Non-null if we should use lifetime annotations.\n    llvm::Value *SizeForLifetimeMarkers;\n\n    /// Address with original alloca instruction. Invalid if the variable was\n    /// emitted as a global constant.\n    Address AllocaAddr;\n\n    struct Invalid {};\n    AutoVarEmission(Invalid)\n        : Variable(nullptr), Addr(Address::invalid()),\n          AllocaAddr(Address::invalid()) {}\n\n    AutoVarEmission(const VarDecl &variable)\n        : Variable(&variable), Addr(Address::invalid()), NRVOFlag(nullptr),\n          IsEscapingByRef(false), IsConstantAggregate(false),\n          SizeForLifetimeMarkers(nullptr), AllocaAddr(Address::invalid()) {}\n\n    bool wasEmittedAsGlobal() const { return !Addr.isValid(); }\n\n  public:\n    static AutoVarEmission invalid() { return AutoVarEmission(Invalid()); }\n\n    bool useLifetimeMarkers() const {\n      return SizeForLifetimeMarkers != nullptr;\n    }\n    llvm::Value *getSizeForLifetimeMarkers() const {\n      assert(useLifetimeMarkers());\n      return SizeForLifetimeMarkers;\n    }\n\n    /// Returns the raw, allocated address, which is not necessarily\n    /// the address of the object itself. It is casted to default\n    /// address space for address space agnostic languages.\n    Address getAllocatedAddress() const {\n      return Addr;\n    }\n\n    /// Returns the address for the original alloca instruction.\n    Address getOriginalAllocatedAddress() const { return AllocaAddr; }\n\n    /// Returns the address of the object within this declaration.\n    /// Note that this does not chase the forwarding pointer for\n    /// __block decls.\n    Address getObjectAddress(CodeGenFunction &CGF) const {\n      if (!IsEscapingByRef) return Addr;\n\n      return CGF.emitBlockByrefAddress(Addr, Variable, /*forward*/ false);\n    }\n  };\n  AutoVarEmission EmitAutoVarAlloca(const VarDecl &var);\n  void EmitAutoVarInit(const AutoVarEmission &emission);\n  void EmitAutoVarCleanups(const AutoVarEmission &emission);\n  void emitAutoVarTypeCleanup(const AutoVarEmission &emission,\n                              QualType::DestructionKind dtorKind);\n\n  /// Emits the alloca and debug information for the size expressions for each\n  /// dimension of an array. It registers the association of its (1-dimensional)\n  /// QualTypes and size expression's debug node, so that CGDebugInfo can\n  /// reference this node when creating the DISubrange object to describe the\n  /// array types.\n  void EmitAndRegisterVariableArrayDimensions(CGDebugInfo *DI,\n                                              const VarDecl &D,\n                                              bool EmitDebugInfo);\n\n  void EmitStaticVarDecl(const VarDecl &D,\n                         llvm::GlobalValue::LinkageTypes Linkage);\n\n  class ParamValue {\n    llvm::Value *Value;\n    unsigned Alignment;\n    ParamValue(llvm::Value *V, unsigned A) : Value(V), Alignment(A) {}\n  public:\n    static ParamValue forDirect(llvm::Value *value) {\n      return ParamValue(value, 0);\n    }\n    static ParamValue forIndirect(Address addr) {\n      assert(!addr.getAlignment().isZero());\n      return ParamValue(addr.getPointer(), addr.getAlignment().getQuantity());\n    }\n\n    bool isIndirect() const { return Alignment != 0; }\n    llvm::Value *getAnyValue() const { return Value; }\n\n    llvm::Value *getDirectValue() const {\n      assert(!isIndirect());\n      return Value;\n    }\n\n    Address getIndirectAddress() const {\n      assert(isIndirect());\n      return Address(Value, CharUnits::fromQuantity(Alignment));\n    }\n  };\n\n  /// EmitParmDecl - Emit a ParmVarDecl or an ImplicitParamDecl.\n  void EmitParmDecl(const VarDecl &D, ParamValue Arg, unsigned ArgNo);\n\n  /// protectFromPeepholes - Protect a value that we're intending to\n  /// store to the side, but which will probably be used later, from\n  /// aggressive peepholing optimizations that might delete it.\n  ///\n  /// Pass the result to unprotectFromPeepholes to declare that\n  /// protection is no longer required.\n  ///\n  /// There's no particular reason why this shouldn't apply to\n  /// l-values, it's just that no existing peepholes work on pointers.\n  PeepholeProtection protectFromPeepholes(RValue rvalue);\n  void unprotectFromPeepholes(PeepholeProtection protection);\n\n  void emitAlignmentAssumptionCheck(llvm::Value *Ptr, QualType Ty,\n                                    SourceLocation Loc,\n                                    SourceLocation AssumptionLoc,\n                                    llvm::Value *Alignment,\n                                    llvm::Value *OffsetValue,\n                                    llvm::Value *TheCheck,\n                                    llvm::Instruction *Assumption);\n\n  void emitAlignmentAssumption(llvm::Value *PtrValue, QualType Ty,\n                               SourceLocation Loc, SourceLocation AssumptionLoc,\n                               llvm::Value *Alignment,\n                               llvm::Value *OffsetValue = nullptr);\n\n  void emitAlignmentAssumption(llvm::Value *PtrValue, const Expr *E,\n                               SourceLocation AssumptionLoc,\n                               llvm::Value *Alignment,\n                               llvm::Value *OffsetValue = nullptr);\n\n  //===--------------------------------------------------------------------===//\n  //                             Statement Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitStopPoint - Emit a debug stoppoint if we are emitting debug info.\n  void EmitStopPoint(const Stmt *S);\n\n  /// EmitStmt - Emit the code for the statement \\arg S. It is legal to call\n  /// this function even if there is no current insertion point.\n  ///\n  /// This function may clear the current insertion point; callers should use\n  /// EnsureInsertPoint if they wish to subsequently generate code without first\n  /// calling EmitBlock, EmitBranch, or EmitStmt.\n  void EmitStmt(const Stmt *S, ArrayRef<const Attr *> Attrs = None);\n\n  /// EmitSimpleStmt - Try to emit a \"simple\" statement which does not\n  /// necessarily require an insertion point or debug information; typically\n  /// because the statement amounts to a jump or a container of other\n  /// statements.\n  ///\n  /// \\return True if the statement was handled.\n  bool EmitSimpleStmt(const Stmt *S, ArrayRef<const Attr *> Attrs);\n\n  Address EmitCompoundStmt(const CompoundStmt &S, bool GetLast = false,\n                           AggValueSlot AVS = AggValueSlot::ignored());\n  Address EmitCompoundStmtWithoutScope(const CompoundStmt &S,\n                                       bool GetLast = false,\n                                       AggValueSlot AVS =\n                                                AggValueSlot::ignored());\n\n  /// EmitLabel - Emit the block for the given label. It is legal to call this\n  /// function even if there is no current insertion point.\n  void EmitLabel(const LabelDecl *D); // helper for EmitLabelStmt.\n\n  void EmitLabelStmt(const LabelStmt &S);\n  void EmitAttributedStmt(const AttributedStmt &S);\n  void EmitGotoStmt(const GotoStmt &S);\n  void EmitIndirectGotoStmt(const IndirectGotoStmt &S);\n  void EmitIfStmt(const IfStmt &S);\n\n  void EmitWhileStmt(const WhileStmt &S,\n                     ArrayRef<const Attr *> Attrs = None);\n  void EmitDoStmt(const DoStmt &S, ArrayRef<const Attr *> Attrs = None);\n  void EmitForStmt(const ForStmt &S,\n                   ArrayRef<const Attr *> Attrs = None);\n  void EmitReturnStmt(const ReturnStmt &S);\n  void EmitDeclStmt(const DeclStmt &S);\n  void EmitBreakStmt(const BreakStmt &S);\n  void EmitContinueStmt(const ContinueStmt &S);\n  void EmitSwitchStmt(const SwitchStmt &S);\n  void EmitDefaultStmt(const DefaultStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitCaseStmt(const CaseStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitCaseStmtRange(const CaseStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitAsmStmt(const AsmStmt &S);\n\n  void EmitObjCForCollectionStmt(const ObjCForCollectionStmt &S);\n  void EmitObjCAtTryStmt(const ObjCAtTryStmt &S);\n  void EmitObjCAtThrowStmt(const ObjCAtThrowStmt &S);\n  void EmitObjCAtSynchronizedStmt(const ObjCAtSynchronizedStmt &S);\n  void EmitObjCAutoreleasePoolStmt(const ObjCAutoreleasePoolStmt &S);\n\n  void EmitCoroutineBody(const CoroutineBodyStmt &S);\n  void EmitCoreturnStmt(const CoreturnStmt &S);\n  RValue EmitCoawaitExpr(const CoawaitExpr &E,\n                         AggValueSlot aggSlot = AggValueSlot::ignored(),\n                         bool ignoreResult = false);\n  LValue EmitCoawaitLValue(const CoawaitExpr *E);\n  RValue EmitCoyieldExpr(const CoyieldExpr &E,\n                         AggValueSlot aggSlot = AggValueSlot::ignored(),\n                         bool ignoreResult = false);\n  LValue EmitCoyieldLValue(const CoyieldExpr *E);\n  RValue EmitCoroutineIntrinsic(const CallExpr *E, unsigned int IID);\n\n  void EnterCXXTryStmt(const CXXTryStmt &S, bool IsFnTryBlock = false);\n  void ExitCXXTryStmt(const CXXTryStmt &S, bool IsFnTryBlock = false);\n\n  void EmitCXXTryStmt(const CXXTryStmt &S);\n  void EmitSEHTryStmt(const SEHTryStmt &S);\n  void EmitSEHLeaveStmt(const SEHLeaveStmt &S);\n  void EnterSEHTryStmt(const SEHTryStmt &S);\n  void ExitSEHTryStmt(const SEHTryStmt &S);\n\n  void pushSEHCleanup(CleanupKind kind,\n                      llvm::Function *FinallyFunc);\n  void startOutlinedSEHHelper(CodeGenFunction &ParentCGF, bool IsFilter,\n                              const Stmt *OutlinedStmt);\n\n  llvm::Function *GenerateSEHFilterFunction(CodeGenFunction &ParentCGF,\n                                            const SEHExceptStmt &Except);\n\n  llvm::Function *GenerateSEHFinallyFunction(CodeGenFunction &ParentCGF,\n                                             const SEHFinallyStmt &Finally);\n\n  void EmitSEHExceptionCodeSave(CodeGenFunction &ParentCGF,\n                                llvm::Value *ParentFP,\n                                llvm::Value *EntryEBP);\n  llvm::Value *EmitSEHExceptionCode();\n  llvm::Value *EmitSEHExceptionInfo();\n  llvm::Value *EmitSEHAbnormalTermination();\n\n  /// Emit simple code for OpenMP directives in Simd-only mode.\n  void EmitSimpleOMPExecutableDirective(const OMPExecutableDirective &D);\n\n  /// Scan the outlined statement for captures from the parent function. For\n  /// each capture, mark the capture as escaped and emit a call to\n  /// llvm.localrecover. Insert the localrecover result into the LocalDeclMap.\n  void EmitCapturedLocals(CodeGenFunction &ParentCGF, const Stmt *OutlinedStmt,\n                          bool IsFilter);\n\n  /// Recovers the address of a local in a parent function. ParentVar is the\n  /// address of the variable used in the immediate parent function. It can\n  /// either be an alloca or a call to llvm.localrecover if there are nested\n  /// outlined functions. ParentFP is the frame pointer of the outermost parent\n  /// frame.\n  Address recoverAddrOfEscapedLocal(CodeGenFunction &ParentCGF,\n                                    Address ParentVar,\n                                    llvm::Value *ParentFP);\n\n  void EmitCXXForRangeStmt(const CXXForRangeStmt &S,\n                           ArrayRef<const Attr *> Attrs = None);\n\n  /// Controls insertion of cancellation exit blocks in worksharing constructs.\n  class OMPCancelStackRAII {\n    CodeGenFunction &CGF;\n\n  public:\n    OMPCancelStackRAII(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,\n                       bool HasCancel)\n        : CGF(CGF) {\n      CGF.OMPCancelStack.enter(CGF, Kind, HasCancel);\n    }\n    ~OMPCancelStackRAII() { CGF.OMPCancelStack.exit(CGF); }\n  };\n\n  /// Returns calculated size of the specified type.\n  llvm::Value *getTypeSize(QualType Ty);\n  LValue InitCapturedStruct(const CapturedStmt &S);\n  llvm::Function *EmitCapturedStmt(const CapturedStmt &S, CapturedRegionKind K);\n  llvm::Function *GenerateCapturedStmtFunction(const CapturedStmt &S);\n  Address GenerateCapturedStmtArgument(const CapturedStmt &S);\n  llvm::Function *GenerateOpenMPCapturedStmtFunction(const CapturedStmt &S,\n                                                     SourceLocation Loc);\n  void GenerateOpenMPCapturedVars(const CapturedStmt &S,\n                                  SmallVectorImpl<llvm::Value *> &CapturedVars);\n  void emitOMPSimpleStore(LValue LVal, RValue RVal, QualType RValTy,\n                          SourceLocation Loc);\n  /// Perform element by element copying of arrays with type \\a\n  /// OriginalType from \\a SrcAddr to \\a DestAddr using copying procedure\n  /// generated by \\a CopyGen.\n  ///\n  /// \\param DestAddr Address of the destination array.\n  /// \\param SrcAddr Address of the source array.\n  /// \\param OriginalType Type of destination and source arrays.\n  /// \\param CopyGen Copying procedure that copies value of single array element\n  /// to another single array element.\n  void EmitOMPAggregateAssign(\n      Address DestAddr, Address SrcAddr, QualType OriginalType,\n      const llvm::function_ref<void(Address, Address)> CopyGen);\n  /// Emit proper copying of data from one variable to another.\n  ///\n  /// \\param OriginalType Original type of the copied variables.\n  /// \\param DestAddr Destination address.\n  /// \\param SrcAddr Source address.\n  /// \\param DestVD Destination variable used in \\a CopyExpr (for arrays, has\n  /// type of the base array element).\n  /// \\param SrcVD Source variable used in \\a CopyExpr (for arrays, has type of\n  /// the base array element).\n  /// \\param Copy Actual copygin expression for copying data from \\a SrcVD to \\a\n  /// DestVD.\n  void EmitOMPCopy(QualType OriginalType,\n                   Address DestAddr, Address SrcAddr,\n                   const VarDecl *DestVD, const VarDecl *SrcVD,\n                   const Expr *Copy);\n  /// Emit atomic update code for constructs: \\a X = \\a X \\a BO \\a E or\n  /// \\a X = \\a E \\a BO \\a E.\n  ///\n  /// \\param X Value to be updated.\n  /// \\param E Update value.\n  /// \\param BO Binary operation for update operation.\n  /// \\param IsXLHSInRHSPart true if \\a X is LHS in RHS part of the update\n  /// expression, false otherwise.\n  /// \\param AO Atomic ordering of the generated atomic instructions.\n  /// \\param CommonGen Code generator for complex expressions that cannot be\n  /// expressed through atomicrmw instruction.\n  /// \\returns <true, OldAtomicValue> if simple 'atomicrmw' instruction was\n  /// generated, <false, RValue::get(nullptr)> otherwise.\n  std::pair<bool, RValue> EmitOMPAtomicSimpleUpdateExpr(\n      LValue X, RValue E, BinaryOperatorKind BO, bool IsXLHSInRHSPart,\n      llvm::AtomicOrdering AO, SourceLocation Loc,\n      const llvm::function_ref<RValue(RValue)> CommonGen);\n  bool EmitOMPFirstprivateClause(const OMPExecutableDirective &D,\n                                 OMPPrivateScope &PrivateScope);\n  void EmitOMPPrivateClause(const OMPExecutableDirective &D,\n                            OMPPrivateScope &PrivateScope);\n  void EmitOMPUseDevicePtrClause(\n      const OMPUseDevicePtrClause &C, OMPPrivateScope &PrivateScope,\n      const llvm::DenseMap<const ValueDecl *, Address> &CaptureDeviceAddrMap);\n  void EmitOMPUseDeviceAddrClause(\n      const OMPUseDeviceAddrClause &C, OMPPrivateScope &PrivateScope,\n      const llvm::DenseMap<const ValueDecl *, Address> &CaptureDeviceAddrMap);\n  /// Emit code for copyin clause in \\a D directive. The next code is\n  /// generated at the start of outlined functions for directives:\n  /// \\code\n  /// threadprivate_var1 = master_threadprivate_var1;\n  /// operator=(threadprivate_var2, master_threadprivate_var2);\n  /// ...\n  /// __kmpc_barrier(&loc, global_tid);\n  /// \\endcode\n  ///\n  /// \\param D OpenMP directive possibly with 'copyin' clause(s).\n  /// \\returns true if at least one copyin variable is found, false otherwise.\n  bool EmitOMPCopyinClause(const OMPExecutableDirective &D);\n  /// Emit initial code for lastprivate variables. If some variable is\n  /// not also firstprivate, then the default initialization is used. Otherwise\n  /// initialization of this variable is performed by EmitOMPFirstprivateClause\n  /// method.\n  ///\n  /// \\param D Directive that may have 'lastprivate' directives.\n  /// \\param PrivateScope Private scope for capturing lastprivate variables for\n  /// proper codegen in internal captured statement.\n  ///\n  /// \\returns true if there is at least one lastprivate variable, false\n  /// otherwise.\n  bool EmitOMPLastprivateClauseInit(const OMPExecutableDirective &D,\n                                    OMPPrivateScope &PrivateScope);\n  /// Emit final copying of lastprivate values to original variables at\n  /// the end of the worksharing or simd directive.\n  ///\n  /// \\param D Directive that has at least one 'lastprivate' directives.\n  /// \\param IsLastIterCond Boolean condition that must be set to 'i1 true' if\n  /// it is the last iteration of the loop code in associated directive, or to\n  /// 'i1 false' otherwise. If this item is nullptr, no final check is required.\n  void EmitOMPLastprivateClauseFinal(const OMPExecutableDirective &D,\n                                     bool NoFinals,\n                                     llvm::Value *IsLastIterCond = nullptr);\n  /// Emit initial code for linear clauses.\n  void EmitOMPLinearClause(const OMPLoopDirective &D,\n                           CodeGenFunction::OMPPrivateScope &PrivateScope);\n  /// Emit final code for linear clauses.\n  /// \\param CondGen Optional conditional code for final part of codegen for\n  /// linear clause.\n  void EmitOMPLinearClauseFinal(\n      const OMPLoopDirective &D,\n      const llvm::function_ref<llvm::Value *(CodeGenFunction &)> CondGen);\n  /// Emit initial code for reduction variables. Creates reduction copies\n  /// and initializes them with the values according to OpenMP standard.\n  ///\n  /// \\param D Directive (possibly) with the 'reduction' clause.\n  /// \\param PrivateScope Private scope for capturing reduction variables for\n  /// proper codegen in internal captured statement.\n  ///\n  void EmitOMPReductionClauseInit(const OMPExecutableDirective &D,\n                                  OMPPrivateScope &PrivateScope,\n                                  bool ForInscan = false);\n  /// Emit final update of reduction values to original variables at\n  /// the end of the directive.\n  ///\n  /// \\param D Directive that has at least one 'reduction' directives.\n  /// \\param ReductionKind The kind of reduction to perform.\n  void EmitOMPReductionClauseFinal(const OMPExecutableDirective &D,\n                                   const OpenMPDirectiveKind ReductionKind);\n  /// Emit initial code for linear variables. Creates private copies\n  /// and initializes them with the values according to OpenMP standard.\n  ///\n  /// \\param D Directive (possibly) with the 'linear' clause.\n  /// \\return true if at least one linear variable is found that should be\n  /// initialized with the value of the original variable, false otherwise.\n  bool EmitOMPLinearClauseInit(const OMPLoopDirective &D);\n\n  typedef const llvm::function_ref<void(CodeGenFunction & /*CGF*/,\n                                        llvm::Function * /*OutlinedFn*/,\n                                        const OMPTaskDataTy & /*Data*/)>\n      TaskGenTy;\n  void EmitOMPTaskBasedDirective(const OMPExecutableDirective &S,\n                                 const OpenMPDirectiveKind CapturedRegion,\n                                 const RegionCodeGenTy &BodyGen,\n                                 const TaskGenTy &TaskGen, OMPTaskDataTy &Data);\n  struct OMPTargetDataInfo {\n    Address BasePointersArray = Address::invalid();\n    Address PointersArray = Address::invalid();\n    Address SizesArray = Address::invalid();\n    Address MappersArray = Address::invalid();\n    unsigned NumberOfTargetItems = 0;\n    explicit OMPTargetDataInfo() = default;\n    OMPTargetDataInfo(Address BasePointersArray, Address PointersArray,\n                      Address SizesArray, Address MappersArray,\n                      unsigned NumberOfTargetItems)\n        : BasePointersArray(BasePointersArray), PointersArray(PointersArray),\n          SizesArray(SizesArray), MappersArray(MappersArray),\n          NumberOfTargetItems(NumberOfTargetItems) {}\n  };\n  void EmitOMPTargetTaskBasedDirective(const OMPExecutableDirective &S,\n                                       const RegionCodeGenTy &BodyGen,\n                                       OMPTargetDataInfo &InputInfo);\n\n  void EmitOMPParallelDirective(const OMPParallelDirective &S);\n  void EmitOMPSimdDirective(const OMPSimdDirective &S);\n  void EmitOMPForDirective(const OMPForDirective &S);\n  void EmitOMPForSimdDirective(const OMPForSimdDirective &S);\n  void EmitOMPSectionsDirective(const OMPSectionsDirective &S);\n  void EmitOMPSectionDirective(const OMPSectionDirective &S);\n  void EmitOMPSingleDirective(const OMPSingleDirective &S);\n  void EmitOMPMasterDirective(const OMPMasterDirective &S);\n  void EmitOMPCriticalDirective(const OMPCriticalDirective &S);\n  void EmitOMPParallelForDirective(const OMPParallelForDirective &S);\n  void EmitOMPParallelForSimdDirective(const OMPParallelForSimdDirective &S);\n  void EmitOMPParallelSectionsDirective(const OMPParallelSectionsDirective &S);\n  void EmitOMPParallelMasterDirective(const OMPParallelMasterDirective &S);\n  void EmitOMPTaskDirective(const OMPTaskDirective &S);\n  void EmitOMPTaskyieldDirective(const OMPTaskyieldDirective &S);\n  void EmitOMPBarrierDirective(const OMPBarrierDirective &S);\n  void EmitOMPTaskwaitDirective(const OMPTaskwaitDirective &S);\n  void EmitOMPTaskgroupDirective(const OMPTaskgroupDirective &S);\n  void EmitOMPFlushDirective(const OMPFlushDirective &S);\n  void EmitOMPDepobjDirective(const OMPDepobjDirective &S);\n  void EmitOMPScanDirective(const OMPScanDirective &S);\n  void EmitOMPOrderedDirective(const OMPOrderedDirective &S);\n  void EmitOMPAtomicDirective(const OMPAtomicDirective &S);\n  void EmitOMPTargetDirective(const OMPTargetDirective &S);\n  void EmitOMPTargetDataDirective(const OMPTargetDataDirective &S);\n  void EmitOMPTargetEnterDataDirective(const OMPTargetEnterDataDirective &S);\n  void EmitOMPTargetExitDataDirective(const OMPTargetExitDataDirective &S);\n  void EmitOMPTargetUpdateDirective(const OMPTargetUpdateDirective &S);\n  void EmitOMPTargetParallelDirective(const OMPTargetParallelDirective &S);\n  void\n  EmitOMPTargetParallelForDirective(const OMPTargetParallelForDirective &S);\n  void EmitOMPTeamsDirective(const OMPTeamsDirective &S);\n  void\n  EmitOMPCancellationPointDirective(const OMPCancellationPointDirective &S);\n  void EmitOMPCancelDirective(const OMPCancelDirective &S);\n  void EmitOMPTaskLoopBasedDirective(const OMPLoopDirective &S);\n  void EmitOMPTaskLoopDirective(const OMPTaskLoopDirective &S);\n  void EmitOMPTaskLoopSimdDirective(const OMPTaskLoopSimdDirective &S);\n  void EmitOMPMasterTaskLoopDirective(const OMPMasterTaskLoopDirective &S);\n  void\n  EmitOMPMasterTaskLoopSimdDirective(const OMPMasterTaskLoopSimdDirective &S);\n  void EmitOMPParallelMasterTaskLoopDirective(\n      const OMPParallelMasterTaskLoopDirective &S);\n  void EmitOMPParallelMasterTaskLoopSimdDirective(\n      const OMPParallelMasterTaskLoopSimdDirective &S);\n  void EmitOMPDistributeDirective(const OMPDistributeDirective &S);\n  void EmitOMPDistributeParallelForDirective(\n      const OMPDistributeParallelForDirective &S);\n  void EmitOMPDistributeParallelForSimdDirective(\n      const OMPDistributeParallelForSimdDirective &S);\n  void EmitOMPDistributeSimdDirective(const OMPDistributeSimdDirective &S);\n  void EmitOMPTargetParallelForSimdDirective(\n      const OMPTargetParallelForSimdDirective &S);\n  void EmitOMPTargetSimdDirective(const OMPTargetSimdDirective &S);\n  void EmitOMPTeamsDistributeDirective(const OMPTeamsDistributeDirective &S);\n  void\n  EmitOMPTeamsDistributeSimdDirective(const OMPTeamsDistributeSimdDirective &S);\n  void EmitOMPTeamsDistributeParallelForSimdDirective(\n      const OMPTeamsDistributeParallelForSimdDirective &S);\n  void EmitOMPTeamsDistributeParallelForDirective(\n      const OMPTeamsDistributeParallelForDirective &S);\n  void EmitOMPTargetTeamsDirective(const OMPTargetTeamsDirective &S);\n  void EmitOMPTargetTeamsDistributeDirective(\n      const OMPTargetTeamsDistributeDirective &S);\n  void EmitOMPTargetTeamsDistributeParallelForDirective(\n      const OMPTargetTeamsDistributeParallelForDirective &S);\n  void EmitOMPTargetTeamsDistributeParallelForSimdDirective(\n      const OMPTargetTeamsDistributeParallelForSimdDirective &S);\n  void EmitOMPTargetTeamsDistributeSimdDirective(\n      const OMPTargetTeamsDistributeSimdDirective &S);\n\n  /// Emit device code for the target directive.\n  static void EmitOMPTargetDeviceFunction(CodeGenModule &CGM,\n                                          StringRef ParentName,\n                                          const OMPTargetDirective &S);\n  static void\n  EmitOMPTargetParallelDeviceFunction(CodeGenModule &CGM, StringRef ParentName,\n                                      const OMPTargetParallelDirective &S);\n  /// Emit device code for the target parallel for directive.\n  static void EmitOMPTargetParallelForDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetParallelForDirective &S);\n  /// Emit device code for the target parallel for simd directive.\n  static void EmitOMPTargetParallelForSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetParallelForSimdDirective &S);\n  /// Emit device code for the target teams directive.\n  static void\n  EmitOMPTargetTeamsDeviceFunction(CodeGenModule &CGM, StringRef ParentName,\n                                   const OMPTargetTeamsDirective &S);\n  /// Emit device code for the target teams distribute directive.\n  static void EmitOMPTargetTeamsDistributeDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeDirective &S);\n  /// Emit device code for the target teams distribute simd directive.\n  static void EmitOMPTargetTeamsDistributeSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeSimdDirective &S);\n  /// Emit device code for the target simd directive.\n  static void EmitOMPTargetSimdDeviceFunction(CodeGenModule &CGM,\n                                              StringRef ParentName,\n                                              const OMPTargetSimdDirective &S);\n  /// Emit device code for the target teams distribute parallel for simd\n  /// directive.\n  static void EmitOMPTargetTeamsDistributeParallelForSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeParallelForSimdDirective &S);\n\n  static void EmitOMPTargetTeamsDistributeParallelForDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeParallelForDirective &S);\n  /// Emit inner loop of the worksharing/simd construct.\n  ///\n  /// \\param S Directive, for which the inner loop must be emitted.\n  /// \\param RequiresCleanup true, if directive has some associated private\n  /// variables.\n  /// \\param LoopCond Bollean condition for loop continuation.\n  /// \\param IncExpr Increment expression for loop control variable.\n  /// \\param BodyGen Generator for the inner body of the inner loop.\n  /// \\param PostIncGen Genrator for post-increment code (required for ordered\n  /// loop directvies).\n  void EmitOMPInnerLoop(\n      const OMPExecutableDirective &S, bool RequiresCleanup,\n      const Expr *LoopCond, const Expr *IncExpr,\n      const llvm::function_ref<void(CodeGenFunction &)> BodyGen,\n      const llvm::function_ref<void(CodeGenFunction &)> PostIncGen);\n\n  JumpDest getOMPCancelDestination(OpenMPDirectiveKind Kind);\n  /// Emit initial code for loop counters of loop-based directives.\n  void EmitOMPPrivateLoopCounters(const OMPLoopDirective &S,\n                                  OMPPrivateScope &LoopScope);\n\n  /// Helper for the OpenMP loop directives.\n  void EmitOMPLoopBody(const OMPLoopDirective &D, JumpDest LoopExit);\n\n  /// Emit code for the worksharing loop-based directive.\n  /// \\return true, if this construct has any lastprivate clause, false -\n  /// otherwise.\n  bool EmitOMPWorksharingLoop(const OMPLoopDirective &S, Expr *EUB,\n                              const CodeGenLoopBoundsTy &CodeGenLoopBounds,\n                              const CodeGenDispatchBoundsTy &CGDispatchBounds);\n\n  /// Emit code for the distribute loop-based directive.\n  void EmitOMPDistributeLoop(const OMPLoopDirective &S,\n                             const CodeGenLoopTy &CodeGenLoop, Expr *IncExpr);\n\n  /// Helpers for the OpenMP loop directives.\n  void EmitOMPSimdInit(const OMPLoopDirective &D, bool IsMonotonic = false);\n  void EmitOMPSimdFinal(\n      const OMPLoopDirective &D,\n      const llvm::function_ref<llvm::Value *(CodeGenFunction &)> CondGen);\n\n  /// Emits the lvalue for the expression with possibly captured variable.\n  LValue EmitOMPSharedLValue(const Expr *E);\n\nprivate:\n  /// Helpers for blocks.\n  llvm::Value *EmitBlockLiteral(const CGBlockInfo &Info);\n\n  /// struct with the values to be passed to the OpenMP loop-related functions\n  struct OMPLoopArguments {\n    /// loop lower bound\n    Address LB = Address::invalid();\n    /// loop upper bound\n    Address UB = Address::invalid();\n    /// loop stride\n    Address ST = Address::invalid();\n    /// isLastIteration argument for runtime functions\n    Address IL = Address::invalid();\n    /// Chunk value generated by sema\n    llvm::Value *Chunk = nullptr;\n    /// EnsureUpperBound\n    Expr *EUB = nullptr;\n    /// IncrementExpression\n    Expr *IncExpr = nullptr;\n    /// Loop initialization\n    Expr *Init = nullptr;\n    /// Loop exit condition\n    Expr *Cond = nullptr;\n    /// Update of LB after a whole chunk has been executed\n    Expr *NextLB = nullptr;\n    /// Update of UB after a whole chunk has been executed\n    Expr *NextUB = nullptr;\n    OMPLoopArguments() = default;\n    OMPLoopArguments(Address LB, Address UB, Address ST, Address IL,\n                     llvm::Value *Chunk = nullptr, Expr *EUB = nullptr,\n                     Expr *IncExpr = nullptr, Expr *Init = nullptr,\n                     Expr *Cond = nullptr, Expr *NextLB = nullptr,\n                     Expr *NextUB = nullptr)\n        : LB(LB), UB(UB), ST(ST), IL(IL), Chunk(Chunk), EUB(EUB),\n          IncExpr(IncExpr), Init(Init), Cond(Cond), NextLB(NextLB),\n          NextUB(NextUB) {}\n  };\n  void EmitOMPOuterLoop(bool DynamicOrOrdered, bool IsMonotonic,\n                        const OMPLoopDirective &S, OMPPrivateScope &LoopScope,\n                        const OMPLoopArguments &LoopArgs,\n                        const CodeGenLoopTy &CodeGenLoop,\n                        const CodeGenOrderedTy &CodeGenOrdered);\n  void EmitOMPForOuterLoop(const OpenMPScheduleTy &ScheduleKind,\n                           bool IsMonotonic, const OMPLoopDirective &S,\n                           OMPPrivateScope &LoopScope, bool Ordered,\n                           const OMPLoopArguments &LoopArgs,\n                           const CodeGenDispatchBoundsTy &CGDispatchBounds);\n  void EmitOMPDistributeOuterLoop(OpenMPDistScheduleClauseKind ScheduleKind,\n                                  const OMPLoopDirective &S,\n                                  OMPPrivateScope &LoopScope,\n                                  const OMPLoopArguments &LoopArgs,\n                                  const CodeGenLoopTy &CodeGenLoopContent);\n  /// Emit code for sections directive.\n  void EmitSections(const OMPExecutableDirective &S);\n\npublic:\n\n  //===--------------------------------------------------------------------===//\n  //                         LValue Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  /// Create a check that a scalar RValue is non-null.\n  llvm::Value *EmitNonNullRValueCheck(RValue RV, QualType T);\n\n  /// GetUndefRValue - Get an appropriate 'undef' rvalue for the given type.\n  RValue GetUndefRValue(QualType Ty);\n\n  /// EmitUnsupportedRValue - Emit a dummy r-value using the type of E\n  /// and issue an ErrorUnsupported style diagnostic (using the\n  /// provided Name).\n  RValue EmitUnsupportedRValue(const Expr *E,\n                               const char *Name);\n\n  /// EmitUnsupportedLValue - Emit a dummy l-value using the type of E and issue\n  /// an ErrorUnsupported style diagnostic (using the provided Name).\n  LValue EmitUnsupportedLValue(const Expr *E,\n                               const char *Name);\n\n  /// EmitLValue - Emit code to compute a designator that specifies the location\n  /// of the expression.\n  ///\n  /// This can return one of two things: a simple address or a bitfield\n  /// reference.  In either case, the LLVM Value* in the LValue structure is\n  /// guaranteed to be an LLVM pointer type.\n  ///\n  /// If this returns a bitfield reference, nothing about the pointee type of\n  /// the LLVM value is known: For example, it may not be a pointer to an\n  /// integer.\n  ///\n  /// If this returns a normal address, and if the lvalue's C type is fixed\n  /// size, this method guarantees that the returned pointer type will point to\n  /// an LLVM type of the same size of the lvalue's type.  If the lvalue has a\n  /// variable length type, this is not possible.\n  ///\n  LValue EmitLValue(const Expr *E);\n\n  /// Same as EmitLValue but additionally we generate checking code to\n  /// guard against undefined behavior.  This is only suitable when we know\n  /// that the address will be used to access the object.\n  LValue EmitCheckedLValue(const Expr *E, TypeCheckKind TCK);\n\n  RValue convertTempToRValue(Address addr, QualType type,\n                             SourceLocation Loc);\n\n  void EmitAtomicInit(Expr *E, LValue lvalue);\n\n  bool LValueIsSuitableForInlineAtomic(LValue Src);\n\n  RValue EmitAtomicLoad(LValue LV, SourceLocation SL,\n                        AggValueSlot Slot = AggValueSlot::ignored());\n\n  RValue EmitAtomicLoad(LValue lvalue, SourceLocation loc,\n                        llvm::AtomicOrdering AO, bool IsVolatile = false,\n                        AggValueSlot slot = AggValueSlot::ignored());\n\n  void EmitAtomicStore(RValue rvalue, LValue lvalue, bool isInit);\n\n  void EmitAtomicStore(RValue rvalue, LValue lvalue, llvm::AtomicOrdering AO,\n                       bool IsVolatile, bool isInit);\n\n  std::pair<RValue, llvm::Value *> EmitAtomicCompareExchange(\n      LValue Obj, RValue Expected, RValue Desired, SourceLocation Loc,\n      llvm::AtomicOrdering Success =\n          llvm::AtomicOrdering::SequentiallyConsistent,\n      llvm::AtomicOrdering Failure =\n          llvm::AtomicOrdering::SequentiallyConsistent,\n      bool IsWeak = false, AggValueSlot Slot = AggValueSlot::ignored());\n\n  void EmitAtomicUpdate(LValue LVal, llvm::AtomicOrdering AO,\n                        const llvm::function_ref<RValue(RValue)> &UpdateOp,\n                        bool IsVolatile);\n\n  /// EmitToMemory - Change a scalar value from its value\n  /// representation to its in-memory representation.\n  llvm::Value *EmitToMemory(llvm::Value *Value, QualType Ty);\n\n  /// EmitFromMemory - Change a scalar value from its memory\n  /// representation to its value representation.\n  llvm::Value *EmitFromMemory(llvm::Value *Value, QualType Ty);\n\n  /// Check if the scalar \\p Value is within the valid range for the given\n  /// type \\p Ty.\n  ///\n  /// Returns true if a check is needed (even if the range is unknown).\n  bool EmitScalarRangeCheck(llvm::Value *Value, QualType Ty,\n                            SourceLocation Loc);\n\n  /// EmitLoadOfScalar - Load a scalar value from an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.\n  llvm::Value *EmitLoadOfScalar(Address Addr, bool Volatile, QualType Ty,\n                                SourceLocation Loc,\n                                AlignmentSource Source = AlignmentSource::Type,\n                                bool isNontemporal = false) {\n    return EmitLoadOfScalar(Addr, Volatile, Ty, Loc, LValueBaseInfo(Source),\n                            CGM.getTBAAAccessInfo(Ty), isNontemporal);\n  }\n\n  llvm::Value *EmitLoadOfScalar(Address Addr, bool Volatile, QualType Ty,\n                                SourceLocation Loc, LValueBaseInfo BaseInfo,\n                                TBAAAccessInfo TBAAInfo,\n                                bool isNontemporal = false);\n\n  /// EmitLoadOfScalar - Load a scalar value from an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.  The l-value must be a simple\n  /// l-value.\n  llvm::Value *EmitLoadOfScalar(LValue lvalue, SourceLocation Loc);\n\n  /// EmitStoreOfScalar - Store a scalar value to an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.\n  void EmitStoreOfScalar(llvm::Value *Value, Address Addr,\n                         bool Volatile, QualType Ty,\n                         AlignmentSource Source = AlignmentSource::Type,\n                         bool isInit = false, bool isNontemporal = false) {\n    EmitStoreOfScalar(Value, Addr, Volatile, Ty, LValueBaseInfo(Source),\n                      CGM.getTBAAAccessInfo(Ty), isInit, isNontemporal);\n  }\n\n  void EmitStoreOfScalar(llvm::Value *Value, Address Addr,\n                         bool Volatile, QualType Ty,\n                         LValueBaseInfo BaseInfo, TBAAAccessInfo TBAAInfo,\n                         bool isInit = false, bool isNontemporal = false);\n\n  /// EmitStoreOfScalar - Store a scalar value to an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.  The l-value must be a simple\n  /// l-value.  The isInit flag indicates whether this is an initialization.\n  /// If so, atomic qualifiers are ignored and the store is always non-atomic.\n  void EmitStoreOfScalar(llvm::Value *value, LValue lvalue, bool isInit=false);\n\n  /// EmitLoadOfLValue - Given an expression that represents a value lvalue,\n  /// this method emits the address of the lvalue, then loads the result as an\n  /// rvalue, returning the rvalue.\n  RValue EmitLoadOfLValue(LValue V, SourceLocation Loc);\n  RValue EmitLoadOfExtVectorElementLValue(LValue V);\n  RValue EmitLoadOfBitfieldLValue(LValue LV, SourceLocation Loc);\n  RValue EmitLoadOfGlobalRegLValue(LValue LV);\n\n  /// EmitStoreThroughLValue - Store the specified rvalue into the specified\n  /// lvalue, where both are guaranteed to the have the same type, and that type\n  /// is 'Ty'.\n  void EmitStoreThroughLValue(RValue Src, LValue Dst, bool isInit = false);\n  void EmitStoreThroughExtVectorComponentLValue(RValue Src, LValue Dst);\n  void EmitStoreThroughGlobalRegLValue(RValue Src, LValue Dst);\n\n  /// EmitStoreThroughBitfieldLValue - Store Src into Dst with same constraints\n  /// as EmitStoreThroughLValue.\n  ///\n  /// \\param Result [out] - If non-null, this will be set to a Value* for the\n  /// bit-field contents after the store, appropriate for use as the result of\n  /// an assignment to the bit-field.\n  void EmitStoreThroughBitfieldLValue(RValue Src, LValue Dst,\n                                      llvm::Value **Result=nullptr);\n\n  /// Emit an l-value for an assignment (simple or compound) of complex type.\n  LValue EmitComplexAssignmentLValue(const BinaryOperator *E);\n  LValue EmitComplexCompoundAssignmentLValue(const CompoundAssignOperator *E);\n  LValue EmitScalarCompoundAssignWithComplex(const CompoundAssignOperator *E,\n                                             llvm::Value *&Result);\n\n  // Note: only available for agg return types\n  LValue EmitBinaryOperatorLValue(const BinaryOperator *E);\n  LValue EmitCompoundAssignmentLValue(const CompoundAssignOperator *E);\n  // Note: only available for agg return types\n  LValue EmitCallExprLValue(const CallExpr *E);\n  // Note: only available for agg return types\n  LValue EmitVAArgExprLValue(const VAArgExpr *E);\n  LValue EmitDeclRefLValue(const DeclRefExpr *E);\n  LValue EmitStringLiteralLValue(const StringLiteral *E);\n  LValue EmitObjCEncodeExprLValue(const ObjCEncodeExpr *E);\n  LValue EmitPredefinedLValue(const PredefinedExpr *E);\n  LValue EmitUnaryOpLValue(const UnaryOperator *E);\n  LValue EmitArraySubscriptExpr(const ArraySubscriptExpr *E,\n                                bool Accessed = false);\n  LValue EmitMatrixSubscriptExpr(const MatrixSubscriptExpr *E);\n  LValue EmitOMPArraySectionExpr(const OMPArraySectionExpr *E,\n                                 bool IsLowerBound = true);\n  LValue EmitExtVectorElementExpr(const ExtVectorElementExpr *E);\n  LValue EmitMemberExpr(const MemberExpr *E);\n  LValue EmitObjCIsaExpr(const ObjCIsaExpr *E);\n  LValue EmitCompoundLiteralLValue(const CompoundLiteralExpr *E);\n  LValue EmitInitListLValue(const InitListExpr *E);\n  LValue EmitConditionalOperatorLValue(const AbstractConditionalOperator *E);\n  LValue EmitCastLValue(const CastExpr *E);\n  LValue EmitMaterializeTemporaryExpr(const MaterializeTemporaryExpr *E);\n  LValue EmitOpaqueValueLValue(const OpaqueValueExpr *e);\n\n  Address EmitExtVectorElementLValue(LValue V);\n\n  RValue EmitRValueForField(LValue LV, const FieldDecl *FD, SourceLocation Loc);\n\n  Address EmitArrayToPointerDecay(const Expr *Array,\n                                  LValueBaseInfo *BaseInfo = nullptr,\n                                  TBAAAccessInfo *TBAAInfo = nullptr);\n\n  class ConstantEmission {\n    llvm::PointerIntPair<llvm::Constant*, 1, bool> ValueAndIsReference;\n    ConstantEmission(llvm::Constant *C, bool isReference)\n      : ValueAndIsReference(C, isReference) {}\n  public:\n    ConstantEmission() {}\n    static ConstantEmission forReference(llvm::Constant *C) {\n      return ConstantEmission(C, true);\n    }\n    static ConstantEmission forValue(llvm::Constant *C) {\n      return ConstantEmission(C, false);\n    }\n\n    explicit operator bool() const {\n      return ValueAndIsReference.getOpaqueValue() != nullptr;\n    }\n\n    bool isReference() const { return ValueAndIsReference.getInt(); }\n    LValue getReferenceLValue(CodeGenFunction &CGF, Expr *refExpr) const {\n      assert(isReference());\n      return CGF.MakeNaturalAlignAddrLValue(ValueAndIsReference.getPointer(),\n                                            refExpr->getType());\n    }\n\n    llvm::Constant *getValue() const {\n      assert(!isReference());\n      return ValueAndIsReference.getPointer();\n    }\n  };\n\n  ConstantEmission tryEmitAsConstant(DeclRefExpr *refExpr);\n  ConstantEmission tryEmitAsConstant(const MemberExpr *ME);\n  llvm::Value *emitScalarConstant(const ConstantEmission &Constant, Expr *E);\n\n  RValue EmitPseudoObjectRValue(const PseudoObjectExpr *e,\n                                AggValueSlot slot = AggValueSlot::ignored());\n  LValue EmitPseudoObjectLValue(const PseudoObjectExpr *e);\n\n  llvm::Value *EmitIvarOffset(const ObjCInterfaceDecl *Interface,\n                              const ObjCIvarDecl *Ivar);\n  LValue EmitLValueForField(LValue Base, const FieldDecl* Field);\n  LValue EmitLValueForLambdaField(const FieldDecl *Field);\n\n  /// EmitLValueForFieldInitialization - Like EmitLValueForField, except that\n  /// if the Field is a reference, this will return the address of the reference\n  /// and not the address of the value stored in the reference.\n  LValue EmitLValueForFieldInitialization(LValue Base,\n                                          const FieldDecl* Field);\n\n  LValue EmitLValueForIvar(QualType ObjectTy,\n                           llvm::Value* Base, const ObjCIvarDecl *Ivar,\n                           unsigned CVRQualifiers);\n\n  LValue EmitCXXConstructLValue(const CXXConstructExpr *E);\n  LValue EmitCXXBindTemporaryLValue(const CXXBindTemporaryExpr *E);\n  LValue EmitCXXTypeidLValue(const CXXTypeidExpr *E);\n  LValue EmitCXXUuidofLValue(const CXXUuidofExpr *E);\n\n  LValue EmitObjCMessageExprLValue(const ObjCMessageExpr *E);\n  LValue EmitObjCIvarRefLValue(const ObjCIvarRefExpr *E);\n  LValue EmitStmtExprLValue(const StmtExpr *E);\n  LValue EmitPointerToDataMemberBinaryExpr(const BinaryOperator *E);\n  LValue EmitObjCSelectorLValue(const ObjCSelectorExpr *E);\n  void   EmitDeclRefExprDbgValue(const DeclRefExpr *E, const APValue &Init);\n\n  //===--------------------------------------------------------------------===//\n  //                         Scalar Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitCall - Generate a call of the given function, expecting the given\n  /// result type, and using the given argument list which specifies both the\n  /// LLVM arguments and the types they were derived from.\n  RValue EmitCall(const CGFunctionInfo &CallInfo, const CGCallee &Callee,\n                  ReturnValueSlot ReturnValue, const CallArgList &Args,\n                  llvm::CallBase **callOrInvoke, SourceLocation Loc);\n  RValue EmitCall(const CGFunctionInfo &CallInfo, const CGCallee &Callee,\n                  ReturnValueSlot ReturnValue, const CallArgList &Args,\n                  llvm::CallBase **callOrInvoke = nullptr) {\n    return EmitCall(CallInfo, Callee, ReturnValue, Args, callOrInvoke,\n                    SourceLocation());\n  }\n  RValue EmitCall(QualType FnType, const CGCallee &Callee, const CallExpr *E,\n                  ReturnValueSlot ReturnValue, llvm::Value *Chain = nullptr);\n  RValue EmitCallExpr(const CallExpr *E,\n                      ReturnValueSlot ReturnValue = ReturnValueSlot());\n  RValue EmitSimpleCallExpr(const CallExpr *E, ReturnValueSlot ReturnValue);\n  CGCallee EmitCallee(const Expr *E);\n\n  void checkTargetFeatures(const CallExpr *E, const FunctionDecl *TargetDecl);\n  void checkTargetFeatures(SourceLocation Loc, const FunctionDecl *TargetDecl);\n\n  llvm::CallInst *EmitRuntimeCall(llvm::FunctionCallee callee,\n                                  const Twine &name = \"\");\n  llvm::CallInst *EmitRuntimeCall(llvm::FunctionCallee callee,\n                                  ArrayRef<llvm::Value *> args,\n                                  const Twine &name = \"\");\n  llvm::CallInst *EmitNounwindRuntimeCall(llvm::FunctionCallee callee,\n                                          const Twine &name = \"\");\n  llvm::CallInst *EmitNounwindRuntimeCall(llvm::FunctionCallee callee,\n                                          ArrayRef<llvm::Value *> args,\n                                          const Twine &name = \"\");\n\n  SmallVector<llvm::OperandBundleDef, 1>\n  getBundlesForFunclet(llvm::Value *Callee);\n\n  llvm::CallBase *EmitCallOrInvoke(llvm::FunctionCallee Callee,\n                                   ArrayRef<llvm::Value *> Args,\n                                   const Twine &Name = \"\");\n  llvm::CallBase *EmitRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                          ArrayRef<llvm::Value *> args,\n                                          const Twine &name = \"\");\n  llvm::CallBase *EmitRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                          const Twine &name = \"\");\n  void EmitNoreturnRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                       ArrayRef<llvm::Value *> args);\n\n  CGCallee BuildAppleKextVirtualCall(const CXXMethodDecl *MD,\n                                     NestedNameSpecifier *Qual,\n                                     llvm::Type *Ty);\n\n  CGCallee BuildAppleKextVirtualDestructorCall(const CXXDestructorDecl *DD,\n                                               CXXDtorType Type,\n                                               const CXXRecordDecl *RD);\n\n  // Return the copy constructor name with the prefix \"__copy_constructor_\"\n  // removed.\n  static std::string getNonTrivialCopyConstructorStr(QualType QT,\n                                                     CharUnits Alignment,\n                                                     bool IsVolatile,\n                                                     ASTContext &Ctx);\n\n  // Return the destructor name with the prefix \"__destructor_\" removed.\n  static std::string getNonTrivialDestructorStr(QualType QT,\n                                                CharUnits Alignment,\n                                                bool IsVolatile,\n                                                ASTContext &Ctx);\n\n  // These functions emit calls to the special functions of non-trivial C\n  // structs.\n  void defaultInitNonTrivialCStructVar(LValue Dst);\n  void callCStructDefaultConstructor(LValue Dst);\n  void callCStructDestructor(LValue Dst);\n  void callCStructCopyConstructor(LValue Dst, LValue Src);\n  void callCStructMoveConstructor(LValue Dst, LValue Src);\n  void callCStructCopyAssignmentOperator(LValue Dst, LValue Src);\n  void callCStructMoveAssignmentOperator(LValue Dst, LValue Src);\n\n  RValue\n  EmitCXXMemberOrOperatorCall(const CXXMethodDecl *Method,\n                              const CGCallee &Callee,\n                              ReturnValueSlot ReturnValue, llvm::Value *This,\n                              llvm::Value *ImplicitParam,\n                              QualType ImplicitParamTy, const CallExpr *E,\n                              CallArgList *RtlArgs);\n  RValue EmitCXXDestructorCall(GlobalDecl Dtor, const CGCallee &Callee,\n                               llvm::Value *This, QualType ThisTy,\n                               llvm::Value *ImplicitParam,\n                               QualType ImplicitParamTy, const CallExpr *E);\n  RValue EmitCXXMemberCallExpr(const CXXMemberCallExpr *E,\n                               ReturnValueSlot ReturnValue);\n  RValue EmitCXXMemberOrOperatorMemberCallExpr(const CallExpr *CE,\n                                               const CXXMethodDecl *MD,\n                                               ReturnValueSlot ReturnValue,\n                                               bool HasQualifier,\n                                               NestedNameSpecifier *Qualifier,\n                                               bool IsArrow, const Expr *Base);\n  // Compute the object pointer.\n  Address EmitCXXMemberDataPointerAddress(const Expr *E, Address base,\n                                          llvm::Value *memberPtr,\n                                          const MemberPointerType *memberPtrType,\n                                          LValueBaseInfo *BaseInfo = nullptr,\n                                          TBAAAccessInfo *TBAAInfo = nullptr);\n  RValue EmitCXXMemberPointerCallExpr(const CXXMemberCallExpr *E,\n                                      ReturnValueSlot ReturnValue);\n\n  RValue EmitCXXOperatorMemberCallExpr(const CXXOperatorCallExpr *E,\n                                       const CXXMethodDecl *MD,\n                                       ReturnValueSlot ReturnValue);\n  RValue EmitCXXPseudoDestructorExpr(const CXXPseudoDestructorExpr *E);\n\n  RValue EmitCUDAKernelCallExpr(const CUDAKernelCallExpr *E,\n                                ReturnValueSlot ReturnValue);\n\n  RValue EmitNVPTXDevicePrintfCallExpr(const CallExpr *E,\n                                       ReturnValueSlot ReturnValue);\n  RValue EmitAMDGPUDevicePrintfCallExpr(const CallExpr *E,\n                                        ReturnValueSlot ReturnValue);\n\n  RValue EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,\n                         const CallExpr *E, ReturnValueSlot ReturnValue);\n\n  RValue emitRotate(const CallExpr *E, bool IsRotateRight);\n\n  /// Emit IR for __builtin_os_log_format.\n  RValue emitBuiltinOSLogFormat(const CallExpr &E);\n\n  /// Emit IR for __builtin_is_aligned.\n  RValue EmitBuiltinIsAligned(const CallExpr *E);\n  /// Emit IR for __builtin_align_up/__builtin_align_down.\n  RValue EmitBuiltinAlignTo(const CallExpr *E, bool AlignUp);\n\n  llvm::Function *generateBuiltinOSLogHelperFunction(\n      const analyze_os_log::OSLogBufferLayout &Layout,\n      CharUnits BufferAlignment);\n\n  RValue EmitBlockCallExpr(const CallExpr *E, ReturnValueSlot ReturnValue);\n\n  /// EmitTargetBuiltinExpr - Emit the given builtin call. Returns 0 if the call\n  /// is unhandled by the current target.\n  llvm::Value *EmitTargetBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue);\n\n  llvm::Value *EmitAArch64CompareBuiltinExpr(llvm::Value *Op, llvm::Type *Ty,\n                                             const llvm::CmpInst::Predicate Fp,\n                                             const llvm::CmpInst::Predicate Ip,\n                                             const llvm::Twine &Name = \"\");\n  llvm::Value *EmitARMBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                  ReturnValueSlot ReturnValue,\n                                  llvm::Triple::ArchType Arch);\n  llvm::Value *EmitARMMVEBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue,\n                                     llvm::Triple::ArchType Arch);\n  llvm::Value *EmitARMCDEBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue,\n                                     llvm::Triple::ArchType Arch);\n  llvm::Value *EmitCMSEClearRecord(llvm::Value *V, llvm::IntegerType *ITy,\n                                   QualType RTy);\n  llvm::Value *EmitCMSEClearRecord(llvm::Value *V, llvm::ArrayType *ATy,\n                                   QualType RTy);\n\n  llvm::Value *EmitCommonNeonBuiltinExpr(unsigned BuiltinID,\n                                         unsigned LLVMIntrinsic,\n                                         unsigned AltLLVMIntrinsic,\n                                         const char *NameHint,\n                                         unsigned Modifier,\n                                         const CallExpr *E,\n                                         SmallVectorImpl<llvm::Value *> &Ops,\n                                         Address PtrOp0, Address PtrOp1,\n                                         llvm::Triple::ArchType Arch);\n\n  llvm::Function *LookupNeonLLVMIntrinsic(unsigned IntrinsicID,\n                                          unsigned Modifier, llvm::Type *ArgTy,\n                                          const CallExpr *E);\n  llvm::Value *EmitNeonCall(llvm::Function *F,\n                            SmallVectorImpl<llvm::Value*> &O,\n                            const char *name,\n                            unsigned shift = 0, bool rightshift = false);\n  llvm::Value *EmitNeonSplat(llvm::Value *V, llvm::Constant *Idx,\n                             const llvm::ElementCount &Count);\n  llvm::Value *EmitNeonSplat(llvm::Value *V, llvm::Constant *Idx);\n  llvm::Value *EmitNeonShiftVector(llvm::Value *V, llvm::Type *Ty,\n                                   bool negateForRightShift);\n  llvm::Value *EmitNeonRShiftImm(llvm::Value *Vec, llvm::Value *Amt,\n                                 llvm::Type *Ty, bool usgn, const char *name);\n  llvm::Value *vectorWrapScalar16(llvm::Value *Op);\n  /// SVEBuiltinMemEltTy - Returns the memory element type for this memory\n  /// access builtin.  Only required if it can't be inferred from the base\n  /// pointer operand.\n  llvm::Type *SVEBuiltinMemEltTy(SVETypeFlags TypeFlags);\n\n  SmallVector<llvm::Type *, 2> getSVEOverloadTypes(SVETypeFlags TypeFlags,\n                                                   llvm::Type *ReturnType,\n                                                   ArrayRef<llvm::Value *> Ops);\n  llvm::Type *getEltType(SVETypeFlags TypeFlags);\n  llvm::ScalableVectorType *getSVEType(const SVETypeFlags &TypeFlags);\n  llvm::ScalableVectorType *getSVEPredType(SVETypeFlags TypeFlags);\n  llvm::Value *EmitSVEAllTruePred(SVETypeFlags TypeFlags);\n  llvm::Value *EmitSVEDupX(llvm::Value *Scalar);\n  llvm::Value *EmitSVEDupX(llvm::Value *Scalar, llvm::Type *Ty);\n  llvm::Value *EmitSVEReinterpret(llvm::Value *Val, llvm::Type *Ty);\n  llvm::Value *EmitSVEPMull(SVETypeFlags TypeFlags,\n                            llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                            unsigned BuiltinID);\n  llvm::Value *EmitSVEMovl(SVETypeFlags TypeFlags,\n                           llvm::ArrayRef<llvm::Value *> Ops,\n                           unsigned BuiltinID);\n  llvm::Value *EmitSVEPredicateCast(llvm::Value *Pred,\n                                    llvm::ScalableVectorType *VTy);\n  llvm::Value *EmitSVEGatherLoad(SVETypeFlags TypeFlags,\n                                 llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                                 unsigned IntID);\n  llvm::Value *EmitSVEScatterStore(SVETypeFlags TypeFlags,\n                                   llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                                   unsigned IntID);\n  llvm::Value *EmitSVEMaskedLoad(const CallExpr *, llvm::Type *ReturnTy,\n                                 SmallVectorImpl<llvm::Value *> &Ops,\n                                 unsigned BuiltinID, bool IsZExtReturn);\n  llvm::Value *EmitSVEMaskedStore(const CallExpr *,\n                                  SmallVectorImpl<llvm::Value *> &Ops,\n                                  unsigned BuiltinID);\n  llvm::Value *EmitSVEPrefetchLoad(SVETypeFlags TypeFlags,\n                                   SmallVectorImpl<llvm::Value *> &Ops,\n                                   unsigned BuiltinID);\n  llvm::Value *EmitSVEGatherPrefetch(SVETypeFlags TypeFlags,\n                                     SmallVectorImpl<llvm::Value *> &Ops,\n                                     unsigned IntID);\n  llvm::Value *EmitSVEStructLoad(SVETypeFlags TypeFlags,\n                                 SmallVectorImpl<llvm::Value *> &Ops, unsigned IntID);\n  llvm::Value *EmitSVEStructStore(SVETypeFlags TypeFlags,\n                                  SmallVectorImpl<llvm::Value *> &Ops,\n                                  unsigned IntID);\n  llvm::Value *EmitAArch64SVEBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n\n  llvm::Value *EmitAArch64BuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                      llvm::Triple::ArchType Arch);\n  llvm::Value *EmitBPFBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n\n  llvm::Value *BuildVector(ArrayRef<llvm::Value*> Ops);\n  llvm::Value *EmitX86BuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitPPCBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitAMDGPUBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitSystemZBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitNVPTXBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitWebAssemblyBuiltinExpr(unsigned BuiltinID,\n                                          const CallExpr *E);\n  llvm::Value *EmitHexagonBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  bool ProcessOrderScopeAMDGCN(llvm::Value *Order, llvm::Value *Scope,\n                               llvm::AtomicOrdering &AO,\n                               llvm::SyncScope::ID &SSID);\n\n  enum class MSVCIntrin;\n  llvm::Value *EmitMSVCBuiltinExpr(MSVCIntrin BuiltinID, const CallExpr *E);\n\n  llvm::Value *EmitBuiltinAvailable(const VersionTuple &Version);\n\n  llvm::Value *EmitObjCProtocolExpr(const ObjCProtocolExpr *E);\n  llvm::Value *EmitObjCStringLiteral(const ObjCStringLiteral *E);\n  llvm::Value *EmitObjCBoxedExpr(const ObjCBoxedExpr *E);\n  llvm::Value *EmitObjCArrayLiteral(const ObjCArrayLiteral *E);\n  llvm::Value *EmitObjCDictionaryLiteral(const ObjCDictionaryLiteral *E);\n  llvm::Value *EmitObjCCollectionLiteral(const Expr *E,\n                                const ObjCMethodDecl *MethodWithObjects);\n  llvm::Value *EmitObjCSelectorExpr(const ObjCSelectorExpr *E);\n  RValue EmitObjCMessageExpr(const ObjCMessageExpr *E,\n                             ReturnValueSlot Return = ReturnValueSlot());\n\n  /// Retrieves the default cleanup kind for an ARC cleanup.\n  /// Except under -fobjc-arc-eh, ARC cleanups are normal-only.\n  CleanupKind getARCCleanupKind() {\n    return CGM.getCodeGenOpts().ObjCAutoRefCountExceptions\n             ? NormalAndEHCleanup : NormalCleanup;\n  }\n\n  // ARC primitives.\n  void EmitARCInitWeak(Address addr, llvm::Value *value);\n  void EmitARCDestroyWeak(Address addr);\n  llvm::Value *EmitARCLoadWeak(Address addr);\n  llvm::Value *EmitARCLoadWeakRetained(Address addr);\n  llvm::Value *EmitARCStoreWeak(Address addr, llvm::Value *value, bool ignored);\n  void emitARCCopyAssignWeak(QualType Ty, Address DstAddr, Address SrcAddr);\n  void emitARCMoveAssignWeak(QualType Ty, Address DstAddr, Address SrcAddr);\n  void EmitARCCopyWeak(Address dst, Address src);\n  void EmitARCMoveWeak(Address dst, Address src);\n  llvm::Value *EmitARCRetainAutorelease(QualType type, llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleaseNonBlock(llvm::Value *value);\n  llvm::Value *EmitARCStoreStrong(LValue lvalue, llvm::Value *value,\n                                  bool resultIgnored);\n  llvm::Value *EmitARCStoreStrongCall(Address addr, llvm::Value *value,\n                                      bool resultIgnored);\n  llvm::Value *EmitARCRetain(QualType type, llvm::Value *value);\n  llvm::Value *EmitARCRetainNonBlock(llvm::Value *value);\n  llvm::Value *EmitARCRetainBlock(llvm::Value *value, bool mandatory);\n  void EmitARCDestroyStrong(Address addr, ARCPreciseLifetime_t precise);\n  void EmitARCRelease(llvm::Value *value, ARCPreciseLifetime_t precise);\n  llvm::Value *EmitARCAutorelease(llvm::Value *value);\n  llvm::Value *EmitARCAutoreleaseReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleaseReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleasedReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCUnsafeClaimAutoreleasedReturnValue(llvm::Value *value);\n\n  llvm::Value *EmitObjCAutorelease(llvm::Value *value, llvm::Type *returnType);\n  llvm::Value *EmitObjCRetainNonBlock(llvm::Value *value,\n                                      llvm::Type *returnType);\n  void EmitObjCRelease(llvm::Value *value, ARCPreciseLifetime_t precise);\n\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreAutoreleasing(const BinaryOperator *e);\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreStrong(const BinaryOperator *e, bool ignored);\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreUnsafeUnretained(const BinaryOperator *e, bool ignored);\n\n  llvm::Value *EmitObjCAlloc(llvm::Value *value,\n                             llvm::Type *returnType);\n  llvm::Value *EmitObjCAllocWithZone(llvm::Value *value,\n                                     llvm::Type *returnType);\n  llvm::Value *EmitObjCAllocInit(llvm::Value *value, llvm::Type *resultType);\n\n  llvm::Value *EmitObjCThrowOperand(const Expr *expr);\n  llvm::Value *EmitObjCConsumeObject(QualType T, llvm::Value *Ptr);\n  llvm::Value *EmitObjCExtendObjectLifetime(QualType T, llvm::Value *Ptr);\n\n  llvm::Value *EmitARCExtendBlockObject(const Expr *expr);\n  llvm::Value *EmitARCReclaimReturnedObject(const Expr *e,\n                                            bool allowUnsafeClaim);\n  llvm::Value *EmitARCRetainScalarExpr(const Expr *expr);\n  llvm::Value *EmitARCRetainAutoreleaseScalarExpr(const Expr *expr);\n  llvm::Value *EmitARCUnsafeUnretainedScalarExpr(const Expr *expr);\n\n  void EmitARCIntrinsicUse(ArrayRef<llvm::Value*> values);\n\n  static Destroyer destroyARCStrongImprecise;\n  static Destroyer destroyARCStrongPrecise;\n  static Destroyer destroyARCWeak;\n  static Destroyer emitARCIntrinsicUse;\n  static Destroyer destroyNonTrivialCStruct;\n\n  void EmitObjCAutoreleasePoolPop(llvm::Value *Ptr);\n  llvm::Value *EmitObjCAutoreleasePoolPush();\n  llvm::Value *EmitObjCMRRAutoreleasePoolPush();\n  void EmitObjCAutoreleasePoolCleanup(llvm::Value *Ptr);\n  void EmitObjCMRRAutoreleasePoolPop(llvm::Value *Ptr);\n\n  /// Emits a reference binding to the passed in expression.\n  RValue EmitReferenceBindingToExpr(const Expr *E);\n\n  //===--------------------------------------------------------------------===//\n  //                           Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  // Expressions are broken into three classes: scalar, complex, aggregate.\n\n  /// EmitScalarExpr - Emit the computation of the specified expression of LLVM\n  /// scalar type, returning the result.\n  llvm::Value *EmitScalarExpr(const Expr *E , bool IgnoreResultAssign = false);\n\n  /// Emit a conversion from the specified type to the specified destination\n  /// type, both of which are LLVM scalar types.\n  llvm::Value *EmitScalarConversion(llvm::Value *Src, QualType SrcTy,\n                                    QualType DstTy, SourceLocation Loc);\n\n  /// Emit a conversion from the specified complex type to the specified\n  /// destination type, where the destination type is an LLVM scalar type.\n  llvm::Value *EmitComplexToScalarConversion(ComplexPairTy Src, QualType SrcTy,\n                                             QualType DstTy,\n                                             SourceLocation Loc);\n\n  /// EmitAggExpr - Emit the computation of the specified expression\n  /// of aggregate type.  The result is computed into the given slot,\n  /// which may be null to indicate that the value is not needed.\n  void EmitAggExpr(const Expr *E, AggValueSlot AS);\n\n  /// EmitAggExprToLValue - Emit the computation of the specified expression of\n  /// aggregate type into a temporary LValue.\n  LValue EmitAggExprToLValue(const Expr *E);\n\n  /// Build all the stores needed to initialize an aggregate at Dest with the\n  /// value Val.\n  void EmitAggregateStore(llvm::Value *Val, Address Dest, bool DestIsVolatile);\n\n  /// EmitExtendGCLifetime - Given a pointer to an Objective-C object,\n  /// make sure it survives garbage collection until this point.\n  void EmitExtendGCLifetime(llvm::Value *object);\n\n  /// EmitComplexExpr - Emit the computation of the specified expression of\n  /// complex type, returning the result.\n  ComplexPairTy EmitComplexExpr(const Expr *E,\n                                bool IgnoreReal = false,\n                                bool IgnoreImag = false);\n\n  /// EmitComplexExprIntoLValue - Emit the given expression of complex\n  /// type and place its result into the specified l-value.\n  void EmitComplexExprIntoLValue(const Expr *E, LValue dest, bool isInit);\n\n  /// EmitStoreOfComplex - Store a complex number into the specified l-value.\n  void EmitStoreOfComplex(ComplexPairTy V, LValue dest, bool isInit);\n\n  /// EmitLoadOfComplex - Load a complex number from the specified l-value.\n  ComplexPairTy EmitLoadOfComplex(LValue src, SourceLocation loc);\n\n  Address emitAddrOfRealComponent(Address complex, QualType complexType);\n  Address emitAddrOfImagComponent(Address complex, QualType complexType);\n\n  /// AddInitializerToStaticVarDecl - Add the initializer for 'D' to the\n  /// global variable that has already been created for it.  If the initializer\n  /// has a different type than GV does, this may free GV and return a different\n  /// one.  Otherwise it just returns GV.\n  llvm::GlobalVariable *\n  AddInitializerToStaticVarDecl(const VarDecl &D,\n                                llvm::GlobalVariable *GV);\n\n  // Emit an @llvm.invariant.start call for the given memory region.\n  void EmitInvariantStart(llvm::Constant *Addr, CharUnits Size);\n\n  /// EmitCXXGlobalVarDeclInit - Create the initializer for a C++\n  /// variable with global storage.\n  void EmitCXXGlobalVarDeclInit(const VarDecl &D, llvm::Constant *DeclPtr,\n                                bool PerformInit);\n\n  llvm::Function *createAtExitStub(const VarDecl &VD, llvm::FunctionCallee Dtor,\n                                   llvm::Constant *Addr);\n\n  /// Call atexit() with a function that passes the given argument to\n  /// the given function.\n  void registerGlobalDtorWithAtExit(const VarDecl &D, llvm::FunctionCallee fn,\n                                    llvm::Constant *addr);\n\n  /// Call atexit() with function dtorStub.\n  void registerGlobalDtorWithAtExit(llvm::Constant *dtorStub);\n\n  /// Call unatexit() with function dtorStub.\n  llvm::Value *unregisterGlobalDtorWithUnAtExit(llvm::Constant *dtorStub);\n\n  /// Emit code in this function to perform a guarded variable\n  /// initialization.  Guarded initializations are used when it's not\n  /// possible to prove that an initialization will be done exactly\n  /// once, e.g. with a static local variable or a static data member\n  /// of a class template.\n  void EmitCXXGuardedInit(const VarDecl &D, llvm::GlobalVariable *DeclPtr,\n                          bool PerformInit);\n\n  enum class GuardKind { VariableGuard, TlsGuard };\n\n  /// Emit a branch to select whether or not to perform guarded initialization.\n  void EmitCXXGuardedInitBranch(llvm::Value *NeedsInit,\n                                llvm::BasicBlock *InitBlock,\n                                llvm::BasicBlock *NoInitBlock,\n                                GuardKind Kind, const VarDecl *D);\n\n  /// GenerateCXXGlobalInitFunc - Generates code for initializing global\n  /// variables.\n  void\n  GenerateCXXGlobalInitFunc(llvm::Function *Fn,\n                            ArrayRef<llvm::Function *> CXXThreadLocals,\n                            ConstantAddress Guard = ConstantAddress::invalid());\n\n  /// GenerateCXXGlobalCleanUpFunc - Generates code for cleaning up global\n  /// variables.\n  void GenerateCXXGlobalCleanUpFunc(\n      llvm::Function *Fn,\n      const std::vector<std::tuple<llvm::FunctionType *, llvm::WeakTrackingVH,\n                                   llvm::Constant *>> &DtorsOrStermFinalizers);\n\n  void GenerateCXXGlobalVarDeclInitFunc(llvm::Function *Fn,\n                                        const VarDecl *D,\n                                        llvm::GlobalVariable *Addr,\n                                        bool PerformInit);\n\n  void EmitCXXConstructExpr(const CXXConstructExpr *E, AggValueSlot Dest);\n\n  void EmitSynthesizedCXXCopyCtor(Address Dest, Address Src, const Expr *Exp);\n\n  void EmitCXXThrowExpr(const CXXThrowExpr *E, bool KeepInsertionPoint = true);\n\n  RValue EmitAtomicExpr(AtomicExpr *E);\n\n  //===--------------------------------------------------------------------===//\n  //                         Annotations Emission\n  //===--------------------------------------------------------------------===//\n\n  /// Emit an annotation call (intrinsic).\n  llvm::Value *EmitAnnotationCall(llvm::Function *AnnotationFn,\n                                  llvm::Value *AnnotatedVal,\n                                  StringRef AnnotationStr,\n                                  SourceLocation Location,\n                                  const AnnotateAttr *Attr);\n\n  /// Emit local annotations for the local variable V, declared by D.\n  void EmitVarAnnotations(const VarDecl *D, llvm::Value *V);\n\n  /// Emit field annotations for the given field & value. Returns the\n  /// annotation result.\n  Address EmitFieldAnnotations(const FieldDecl *D, Address V);\n\n  //===--------------------------------------------------------------------===//\n  //                             Internal Helpers\n  //===--------------------------------------------------------------------===//\n\n  /// ContainsLabel - Return true if the statement contains a label in it.  If\n  /// this statement is not executed normally, it not containing a label means\n  /// that we can just remove the code.\n  static bool ContainsLabel(const Stmt *S, bool IgnoreCaseStmts = false);\n\n  /// containsBreak - Return true if the statement contains a break out of it.\n  /// If the statement (recursively) contains a switch or loop with a break\n  /// inside of it, this is fine.\n  static bool containsBreak(const Stmt *S);\n\n  /// Determine if the given statement might introduce a declaration into the\n  /// current scope, by being a (possibly-labelled) DeclStmt.\n  static bool mightAddDeclToScope(const Stmt *S);\n\n  /// ConstantFoldsToSimpleInteger - If the specified expression does not fold\n  /// to a constant, or if it does but contains a label, return false.  If it\n  /// constant folds return true and set the boolean result in Result.\n  bool ConstantFoldsToSimpleInteger(const Expr *Cond, bool &Result,\n                                    bool AllowLabels = false);\n\n  /// ConstantFoldsToSimpleInteger - If the specified expression does not fold\n  /// to a constant, or if it does but contains a label, return false.  If it\n  /// constant folds return true and set the folded value.\n  bool ConstantFoldsToSimpleInteger(const Expr *Cond, llvm::APSInt &Result,\n                                    bool AllowLabels = false);\n\n  /// isInstrumentedCondition - Determine whether the given condition is an\n  /// instrumentable condition (i.e. no \"&&\" or \"||\").\n  static bool isInstrumentedCondition(const Expr *C);\n\n  /// EmitBranchToCounterBlock - Emit a conditional branch to a new block that\n  /// increments a profile counter based on the semantics of the given logical\n  /// operator opcode.  This is used to instrument branch condition coverage\n  /// for logical operators.\n  void EmitBranchToCounterBlock(const Expr *Cond, BinaryOperator::Opcode LOp,\n                                llvm::BasicBlock *TrueBlock,\n                                llvm::BasicBlock *FalseBlock,\n                                uint64_t TrueCount = 0,\n                                Stmt::Likelihood LH = Stmt::LH_None,\n                                const Expr *CntrIdx = nullptr);\n\n  /// EmitBranchOnBoolExpr - Emit a branch on a boolean condition (e.g. for an\n  /// if statement) to the specified blocks.  Based on the condition, this might\n  /// try to simplify the codegen of the conditional based on the branch.\n  /// TrueCount should be the number of times we expect the condition to\n  /// evaluate to true based on PGO data.\n  void EmitBranchOnBoolExpr(const Expr *Cond, llvm::BasicBlock *TrueBlock,\n                            llvm::BasicBlock *FalseBlock, uint64_t TrueCount,\n                            Stmt::Likelihood LH = Stmt::LH_None);\n\n  /// Given an assignment `*LHS = RHS`, emit a test that checks if \\p RHS is\n  /// nonnull, if \\p LHS is marked _Nonnull.\n  void EmitNullabilityCheck(LValue LHS, llvm::Value *RHS, SourceLocation Loc);\n\n  /// An enumeration which makes it easier to specify whether or not an\n  /// operation is a subtraction.\n  enum { NotSubtraction = false, IsSubtraction = true };\n\n  /// Same as IRBuilder::CreateInBoundsGEP, but additionally emits a check to\n  /// detect undefined behavior when the pointer overflow sanitizer is enabled.\n  /// \\p SignedIndices indicates whether any of the GEP indices are signed.\n  /// \\p IsSubtraction indicates whether the expression used to form the GEP\n  /// is a subtraction.\n  llvm::Value *EmitCheckedInBoundsGEP(llvm::Value *Ptr,\n                                      ArrayRef<llvm::Value *> IdxList,\n                                      bool SignedIndices,\n                                      bool IsSubtraction,\n                                      SourceLocation Loc,\n                                      const Twine &Name = \"\");\n\n  /// Specifies which type of sanitizer check to apply when handling a\n  /// particular builtin.\n  enum BuiltinCheckKind {\n    BCK_CTZPassedZero,\n    BCK_CLZPassedZero,\n  };\n\n  /// Emits an argument for a call to a builtin. If the builtin sanitizer is\n  /// enabled, a runtime check specified by \\p Kind is also emitted.\n  llvm::Value *EmitCheckedArgForBuiltin(const Expr *E, BuiltinCheckKind Kind);\n\n  /// Emit a description of a type in a format suitable for passing to\n  /// a runtime sanitizer handler.\n  llvm::Constant *EmitCheckTypeDescriptor(QualType T);\n\n  /// Convert a value into a format suitable for passing to a runtime\n  /// sanitizer handler.\n  llvm::Value *EmitCheckValue(llvm::Value *V);\n\n  /// Emit a description of a source location in a format suitable for\n  /// passing to a runtime sanitizer handler.\n  llvm::Constant *EmitCheckSourceLocation(SourceLocation Loc);\n\n  /// Create a basic block that will either trap or call a handler function in\n  /// the UBSan runtime with the provided arguments, and create a conditional\n  /// branch to it.\n  void EmitCheck(ArrayRef<std::pair<llvm::Value *, SanitizerMask>> Checked,\n                 SanitizerHandler Check, ArrayRef<llvm::Constant *> StaticArgs,\n                 ArrayRef<llvm::Value *> DynamicArgs);\n\n  /// Emit a slow path cross-DSO CFI check which calls __cfi_slowpath\n  /// if Cond if false.\n  void EmitCfiSlowPathCheck(SanitizerMask Kind, llvm::Value *Cond,\n                            llvm::ConstantInt *TypeId, llvm::Value *Ptr,\n                            ArrayRef<llvm::Constant *> StaticArgs);\n\n  /// Emit a reached-unreachable diagnostic if \\p Loc is valid and runtime\n  /// checking is enabled. Otherwise, just emit an unreachable instruction.\n  void EmitUnreachable(SourceLocation Loc);\n\n  /// Create a basic block that will call the trap intrinsic, and emit a\n  /// conditional branch to it, for the -ftrapv checks.\n  void EmitTrapCheck(llvm::Value *Checked, SanitizerHandler CheckHandlerID);\n\n  /// Emit a call to trap or debugtrap and attach function attribute\n  /// \"trap-func-name\" if specified.\n  llvm::CallInst *EmitTrapCall(llvm::Intrinsic::ID IntrID);\n\n  /// Emit a stub for the cross-DSO CFI check function.\n  void EmitCfiCheckStub();\n\n  /// Emit a cross-DSO CFI failure handling function.\n  void EmitCfiCheckFail();\n\n  /// Create a check for a function parameter that may potentially be\n  /// declared as non-null.\n  void EmitNonNullArgCheck(RValue RV, QualType ArgType, SourceLocation ArgLoc,\n                           AbstractCallee AC, unsigned ParmNum);\n\n  /// EmitCallArg - Emit a single call argument.\n  void EmitCallArg(CallArgList &args, const Expr *E, QualType ArgType);\n\n  /// EmitDelegateCallArg - We are performing a delegate call; that\n  /// is, the current function is delegating to another one.  Produce\n  /// a r-value suitable for passing the given parameter.\n  void EmitDelegateCallArg(CallArgList &args, const VarDecl *param,\n                           SourceLocation loc);\n\n  /// SetFPAccuracy - Set the minimum required accuracy of the given floating\n  /// point operation, expressed as the maximum relative error in ulp.\n  void SetFPAccuracy(llvm::Value *Val, float Accuracy);\n\n  /// SetFPModel - Control floating point behavior via fp-model settings.\n  void SetFPModel();\n\n  /// Set the codegen fast-math flags.\n  void SetFastMathFlags(FPOptions FPFeatures);\n\nprivate:\n  llvm::MDNode *getRangeForLoadFromType(QualType Ty);\n  void EmitReturnOfRValue(RValue RV, QualType Ty);\n\n  void deferPlaceholderReplacement(llvm::Instruction *Old, llvm::Value *New);\n\n  llvm::SmallVector<std::pair<llvm::Instruction *, llvm::Value *>, 4>\n  DeferredReplacements;\n\n  /// Set the address of a local variable.\n  void setAddrOfLocalVar(const VarDecl *VD, Address Addr) {\n    assert(!LocalDeclMap.count(VD) && \"Decl already exists in LocalDeclMap!\");\n    LocalDeclMap.insert({VD, Addr});\n  }\n\n  /// ExpandTypeFromArgs - Reconstruct a structure of type \\arg Ty\n  /// from function arguments into \\arg Dst. See ABIArgInfo::Expand.\n  ///\n  /// \\param AI - The first function argument of the expansion.\n  void ExpandTypeFromArgs(QualType Ty, LValue Dst,\n                          llvm::Function::arg_iterator &AI);\n\n  /// ExpandTypeToArgs - Expand an CallArg \\arg Arg, with the LLVM type for \\arg\n  /// Ty, into individual arguments on the provided vector \\arg IRCallArgs,\n  /// starting at index \\arg IRCallArgPos. See ABIArgInfo::Expand.\n  void ExpandTypeToArgs(QualType Ty, CallArg Arg, llvm::FunctionType *IRFuncTy,\n                        SmallVectorImpl<llvm::Value *> &IRCallArgs,\n                        unsigned &IRCallArgPos);\n\n  llvm::Value* EmitAsmInput(const TargetInfo::ConstraintInfo &Info,\n                            const Expr *InputExpr, std::string &ConstraintStr);\n\n  llvm::Value* EmitAsmInputLValue(const TargetInfo::ConstraintInfo &Info,\n                                  LValue InputValue, QualType InputType,\n                                  std::string &ConstraintStr,\n                                  SourceLocation Loc);\n\n  /// Attempts to statically evaluate the object size of E. If that\n  /// fails, emits code to figure the size of E out for us. This is\n  /// pass_object_size aware.\n  ///\n  /// If EmittedExpr is non-null, this will use that instead of re-emitting E.\n  llvm::Value *evaluateOrEmitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                               llvm::IntegerType *ResType,\n                                               llvm::Value *EmittedE,\n                                               bool IsDynamic);\n\n  /// Emits the size of E, as required by __builtin_object_size. This\n  /// function is aware of pass_object_size parameters, and will act accordingly\n  /// if E is a parameter with the pass_object_size attribute.\n  llvm::Value *emitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                     llvm::IntegerType *ResType,\n                                     llvm::Value *EmittedE,\n                                     bool IsDynamic);\n\n  void emitZeroOrPatternForAutoVarInit(QualType type, const VarDecl &D,\n                                       Address Loc);\n\npublic:\n  enum class EvaluationOrder {\n    ///! No language constraints on evaluation order.\n    Default,\n    ///! Language semantics require left-to-right evaluation.\n    ForceLeftToRight,\n    ///! Language semantics require right-to-left evaluation.\n    ForceRightToLeft\n  };\n\n  // Wrapper for function prototype sources. Wraps either a FunctionProtoType or\n  // an ObjCMethodDecl.\n  struct PrototypeWrapper {\n    llvm::PointerUnion<const FunctionProtoType *, const ObjCMethodDecl *> P;\n\n    PrototypeWrapper(const FunctionProtoType *FT) : P(FT) {}\n    PrototypeWrapper(const ObjCMethodDecl *MD) : P(MD) {}\n  };\n\n  void EmitCallArgs(CallArgList &Args, PrototypeWrapper Prototype,\n                    llvm::iterator_range<CallExpr::const_arg_iterator> ArgRange,\n                    AbstractCallee AC = AbstractCallee(),\n                    unsigned ParamsToSkip = 0,\n                    EvaluationOrder Order = EvaluationOrder::Default);\n\n  /// EmitPointerWithAlignment - Given an expression with a pointer type,\n  /// emit the value and compute our best estimate of the alignment of the\n  /// pointee.\n  ///\n  /// \\param BaseInfo - If non-null, this will be initialized with\n  /// information about the source of the alignment and the may-alias\n  /// attribute.  Note that this function will conservatively fall back on\n  /// the type when it doesn't recognize the expression and may-alias will\n  /// be set to false.\n  ///\n  /// One reasonable way to use this information is when there's a language\n  /// guarantee that the pointer must be aligned to some stricter value, and\n  /// we're simply trying to ensure that sufficiently obvious uses of under-\n  /// aligned objects don't get miscompiled; for example, a placement new\n  /// into the address of a local variable.  In such a case, it's quite\n  /// reasonable to just ignore the returned alignment when it isn't from an\n  /// explicit source.\n  Address EmitPointerWithAlignment(const Expr *Addr,\n                                   LValueBaseInfo *BaseInfo = nullptr,\n                                   TBAAAccessInfo *TBAAInfo = nullptr);\n\n  /// If \\p E references a parameter with pass_object_size info or a constant\n  /// array size modifier, emit the object size divided by the size of \\p EltTy.\n  /// Otherwise return null.\n  llvm::Value *LoadPassedObjectSize(const Expr *E, QualType EltTy);\n\n  void EmitSanitizerStatReport(llvm::SanitizerStatKind SSK);\n\n  struct MultiVersionResolverOption {\n    llvm::Function *Function;\n    FunctionDecl *FD;\n    struct Conds {\n      StringRef Architecture;\n      llvm::SmallVector<StringRef, 8> Features;\n\n      Conds(StringRef Arch, ArrayRef<StringRef> Feats)\n          : Architecture(Arch), Features(Feats.begin(), Feats.end()) {}\n    } Conditions;\n\n    MultiVersionResolverOption(llvm::Function *F, StringRef Arch,\n                               ArrayRef<StringRef> Feats)\n        : Function(F), Conditions(Arch, Feats) {}\n  };\n\n  // Emits the body of a multiversion function's resolver. Assumes that the\n  // options are already sorted in the proper order, with the 'default' option\n  // last (if it exists).\n  void EmitMultiVersionResolver(llvm::Function *Resolver,\n                                ArrayRef<MultiVersionResolverOption> Options);\n\n  static uint64_t GetX86CpuSupportsMask(ArrayRef<StringRef> FeatureStrs);\n\nprivate:\n  QualType getVarArgType(const Expr *Arg);\n\n  void EmitDeclMetadata();\n\n  BlockByrefHelpers *buildByrefHelpers(llvm::StructType &byrefType,\n                                  const AutoVarEmission &emission);\n\n  void AddObjCARCExceptionMetadata(llvm::Instruction *Inst);\n\n  llvm::Value *GetValueForARMHint(unsigned BuiltinID);\n  llvm::Value *EmitX86CpuIs(const CallExpr *E);\n  llvm::Value *EmitX86CpuIs(StringRef CPUStr);\n  llvm::Value *EmitX86CpuSupports(const CallExpr *E);\n  llvm::Value *EmitX86CpuSupports(ArrayRef<StringRef> FeatureStrs);\n  llvm::Value *EmitX86CpuSupports(uint64_t Mask);\n  llvm::Value *EmitX86CpuInit();\n  llvm::Value *FormResolverCondition(const MultiVersionResolverOption &RO);\n};\n\n/// TargetFeatures - This class is used to check whether the builtin function\n/// has the required tagert specific features. It is able to support the\n/// combination of ','(and), '|'(or), and '()'. By default, the priority of\n/// ',' is higher than that of '|' .\n/// E.g:\n/// A,B|C means the builtin function requires both A and B, or C.\n/// If we want the builtin function requires both A and B, or both A and C,\n/// there are two ways: A,B|A,C or A,(B|C).\n/// The FeaturesList should not contain spaces, and brackets must appear in\n/// pairs.\nclass TargetFeatures {\n  struct FeatureListStatus {\n    bool HasFeatures;\n    StringRef CurFeaturesList;\n  };\n\n  const llvm::StringMap<bool> &CallerFeatureMap;\n\n  FeatureListStatus getAndFeatures(StringRef FeatureList) {\n    int InParentheses = 0;\n    bool HasFeatures = true;\n    size_t SubexpressionStart = 0;\n    for (size_t i = 0, e = FeatureList.size(); i < e; ++i) {\n      char CurrentToken = FeatureList[i];\n      switch (CurrentToken) {\n      default:\n        break;\n      case '(':\n        if (InParentheses == 0)\n          SubexpressionStart = i + 1;\n        ++InParentheses;\n        break;\n      case ')':\n        --InParentheses;\n        assert(InParentheses >= 0 && \"Parentheses are not in pair\");\n        LLVM_FALLTHROUGH;\n      case '|':\n      case ',':\n        if (InParentheses == 0) {\n          if (HasFeatures && i != SubexpressionStart) {\n            StringRef F = FeatureList.slice(SubexpressionStart, i);\n            HasFeatures = CurrentToken == ')' ? hasRequiredFeatures(F)\n                                              : CallerFeatureMap.lookup(F);\n          }\n          SubexpressionStart = i + 1;\n          if (CurrentToken == '|') {\n            return {HasFeatures, FeatureList.substr(SubexpressionStart)};\n          }\n        }\n        break;\n      }\n    }\n    assert(InParentheses == 0 && \"Parentheses are not in pair\");\n    if (HasFeatures && SubexpressionStart != FeatureList.size())\n      HasFeatures =\n          CallerFeatureMap.lookup(FeatureList.substr(SubexpressionStart));\n    return {HasFeatures, StringRef()};\n  }\n\npublic:\n  bool hasRequiredFeatures(StringRef FeatureList) {\n    FeatureListStatus FS = {false, FeatureList};\n    while (!FS.HasFeatures && !FS.CurFeaturesList.empty())\n      FS = getAndFeatures(FS.CurFeaturesList);\n    return FS.HasFeatures;\n  }\n\n  TargetFeatures(const llvm::StringMap<bool> &CallerFeatureMap)\n      : CallerFeatureMap(CallerFeatureMap) {}\n};\n\ninline DominatingLLVMValue::saved_type\nDominatingLLVMValue::save(CodeGenFunction &CGF, llvm::Value *value) {\n  if (!needsSaving(value)) return saved_type(value, false);\n\n  // Otherwise, we need an alloca.\n  auto align = CharUnits::fromQuantity(\n            CGF.CGM.getDataLayout().getPrefTypeAlignment(value->getType()));\n  Address alloca =\n    CGF.CreateTempAlloca(value->getType(), align, \"cond-cleanup.save\");\n  CGF.Builder.CreateStore(value, alloca);\n\n  return saved_type(alloca.getPointer(), true);\n}\n\ninline llvm::Value *DominatingLLVMValue::restore(CodeGenFunction &CGF,\n                                                 saved_type value) {\n  // If the value says it wasn't saved, trust that it's still dominating.\n  if (!value.getInt()) return value.getPointer();\n\n  // Otherwise, it should be an alloca instruction, as set up in save().\n  auto alloca = cast<llvm::AllocaInst>(value.getPointer());\n  return CGF.Builder.CreateAlignedLoad(alloca, alloca->getAlign());\n}\n\n}  // end namespace CodeGen\n\n// Map the LangOption for floating point exception behavior into\n// the corresponding enum in the IR.\nllvm::fp::ExceptionBehavior\nToConstrainedExceptMD(LangOptions::FPExceptionModeKind Kind);\n}  // end namespace clang\n\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 25, "file": 1, "line": 5216}, "message": "the definition seen here"}, {"location": {"col": 16, "file": 0, "line": 4051}, "message": "differing parameters are named here: ('Idx'), in definition: ('C')"}, {"location": {"col": 16, "file": 0, "line": 4051}, "message": "function 'clang::CodeGen::CodeGenFunction::EmitNeonSplat' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "8ca00c958a859ed6a855e1fb5a7c8fe6", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 25, "file": 1, "line": 5222}, "message": "the definition seen here"}, {"location": {"col": 16, "file": 0, "line": 4053}, "message": "differing parameters are named here: ('Idx'), in definition: ('C')"}, {"location": {"col": 16, "file": 0, "line": 4053}, "message": "function 'clang::CodeGen::CodeGenFunction::EmitNeonSplat' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "1fb3fda4a76ac1ad35864e954d4c5a85", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 25, "file": 1, "line": 5255}, "message": "the definition seen here"}, {"location": {"col": 16, "file": 0, "line": 4056}, "message": "differing parameters are named here: ('Amt'), in definition: ('Shift')"}, {"location": {"col": 16, "file": 0, "line": 4056}, "message": "function 'clang::CodeGen::CodeGenFunction::EmitNeonRShiftImm' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "5f85df80c38768299114f8a90bc3ba53", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 47, "file": 1, "line": 8660}, "message": "the definition seen here"}, {"location": {"col": 32, "file": 0, "line": 4064}, "message": "differing parameters are named here: ('ReturnType'), in definition: ('ResultType')"}, {"location": {"col": 32, "file": 0, "line": 4064}, "message": "function 'clang::CodeGen::CodeGenFunction::getSVEOverloadTypes' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "fe79441b536729d880f68b539f06ab3c", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
