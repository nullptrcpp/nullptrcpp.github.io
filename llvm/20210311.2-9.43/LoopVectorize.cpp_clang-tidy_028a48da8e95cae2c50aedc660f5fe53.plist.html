<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"24": {"id": 24, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h", "content": "//===- TargetTransformInfo.h ------------------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n/// \\file\n/// This pass exposes codegen information to IR-level passes. Every\n/// transformation that uses codegen information is broken into three parts:\n/// 1. The IR-level analysis pass.\n/// 2. The IR-level transformation interface which provides the needed\n///    information.\n/// 3. Codegen-level implementation which uses target-specific hooks.\n///\n/// This file defines #2, which is the interface that IR-level transformations\n/// use for querying the codegen.\n///\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ANALYSIS_TARGETTRANSFORMINFO_H\n#define LLVM_ANALYSIS_TARGETTRANSFORMINFO_H\n\n#include \"llvm/Analysis/IVDescriptors.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Operator.h\"\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/Pass.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/DataTypes.h\"\n#include \"llvm/Support/InstructionCost.h\"\n#include <functional>\n\nnamespace llvm {\n\nnamespace Intrinsic {\ntypedef unsigned ID;\n}\n\nclass AssumptionCache;\nclass BlockFrequencyInfo;\nclass DominatorTree;\nclass BranchInst;\nclass CallBase;\nclass ExtractElementInst;\nclass Function;\nclass GlobalValue;\nclass InstCombiner;\nclass IntrinsicInst;\nclass LoadInst;\nclass LoopAccessInfo;\nclass Loop;\nclass LoopInfo;\nclass ProfileSummaryInfo;\nclass SCEV;\nclass ScalarEvolution;\nclass StoreInst;\nclass SwitchInst;\nclass TargetLibraryInfo;\nclass Type;\nclass User;\nclass Value;\nstruct KnownBits;\ntemplate <typename T> class Optional;\n\n/// Information about a load/store intrinsic defined by the target.\nstruct MemIntrinsicInfo {\n  /// This is the pointer that the intrinsic is loading from or storing to.\n  /// If this is non-null, then analysis/optimization passes can assume that\n  /// this intrinsic is functionally equivalent to a load/store from this\n  /// pointer.\n  Value *PtrVal = nullptr;\n\n  // Ordering for atomic operations.\n  AtomicOrdering Ordering = AtomicOrdering::NotAtomic;\n\n  // Same Id is set by the target for corresponding load/store intrinsics.\n  unsigned short MatchingId = 0;\n\n  bool ReadMem = false;\n  bool WriteMem = false;\n  bool IsVolatile = false;\n\n  bool isUnordered() const {\n    return (Ordering == AtomicOrdering::NotAtomic ||\n            Ordering == AtomicOrdering::Unordered) &&\n           !IsVolatile;\n  }\n};\n\n/// Attributes of a target dependent hardware loop.\nstruct HardwareLoopInfo {\n  HardwareLoopInfo() = delete;\n  HardwareLoopInfo(Loop *L) : L(L) {}\n  Loop *L = nullptr;\n  BasicBlock *ExitBlock = nullptr;\n  BranchInst *ExitBranch = nullptr;\n  const SCEV *TripCount = nullptr;\n  IntegerType *CountType = nullptr;\n  Value *LoopDecrement = nullptr; // Decrement the loop counter by this\n                                  // value in every iteration.\n  bool IsNestingLegal = false;    // Can a hardware loop be a parent to\n                                  // another hardware loop?\n  bool CounterInReg = false;      // Should loop counter be updated in\n                                  // the loop via a phi?\n  bool PerformEntryTest = false;  // Generate the intrinsic which also performs\n                                  // icmp ne zero on the loop counter value and\n                                  // produces an i1 to guard the loop entry.\n  bool isHardwareLoopCandidate(ScalarEvolution &SE, LoopInfo &LI,\n                               DominatorTree &DT, bool ForceNestedLoop = false,\n                               bool ForceHardwareLoopPHI = false);\n  bool canAnalyze(LoopInfo &LI);\n};\n\nclass IntrinsicCostAttributes {\n  const IntrinsicInst *II = nullptr;\n  Type *RetTy = nullptr;\n  Intrinsic::ID IID;\n  SmallVector<Type *, 4> ParamTys;\n  SmallVector<const Value *, 4> Arguments;\n  FastMathFlags FMF;\n  // If ScalarizationCost is UINT_MAX, the cost of scalarizing the\n  // arguments and the return value will be computed based on types.\n  unsigned ScalarizationCost = std::numeric_limits<unsigned>::max();\n\npublic:\n  IntrinsicCostAttributes(\n      Intrinsic::ID Id, const CallBase &CI,\n      unsigned ScalarizationCost = std::numeric_limits<unsigned>::max());\n\n  IntrinsicCostAttributes(\n      Intrinsic::ID Id, Type *RTy, ArrayRef<Type *> Tys,\n      FastMathFlags Flags = FastMathFlags(), const IntrinsicInst *I = nullptr,\n      unsigned ScalarCost = std::numeric_limits<unsigned>::max());\n\n  IntrinsicCostAttributes(Intrinsic::ID Id, Type *RTy,\n                          ArrayRef<const Value *> Args);\n\n  IntrinsicCostAttributes(\n      Intrinsic::ID Id, Type *RTy, ArrayRef<const Value *> Args,\n      ArrayRef<Type *> Tys, FastMathFlags Flags = FastMathFlags(),\n      const IntrinsicInst *I = nullptr,\n      unsigned ScalarCost = std::numeric_limits<unsigned>::max());\n\n  Intrinsic::ID getID() const { return IID; }\n  const IntrinsicInst *getInst() const { return II; }\n  Type *getReturnType() const { return RetTy; }\n  FastMathFlags getFlags() const { return FMF; }\n  unsigned getScalarizationCost() const { return ScalarizationCost; }\n  const SmallVectorImpl<const Value *> &getArgs() const { return Arguments; }\n  const SmallVectorImpl<Type *> &getArgTypes() const { return ParamTys; }\n\n  bool isTypeBasedOnly() const {\n    return Arguments.empty();\n  }\n\n  bool skipScalarizationCost() const {\n    return ScalarizationCost != std::numeric_limits<unsigned>::max();\n  }\n};\n\nclass TargetTransformInfo;\ntypedef TargetTransformInfo TTI;\n\n/// This pass provides access to the codegen interfaces that are needed\n/// for IR-level transformations.\nclass TargetTransformInfo {\npublic:\n  /// Construct a TTI object using a type implementing the \\c Concept\n  /// API below.\n  ///\n  /// This is used by targets to construct a TTI wrapping their target-specific\n  /// implementation that encodes appropriate costs for their target.\n  template <typename T> TargetTransformInfo(T Impl);\n\n  /// Construct a baseline TTI object using a minimal implementation of\n  /// the \\c Concept API below.\n  ///\n  /// The TTI implementation will reflect the information in the DataLayout\n  /// provided if non-null.\n  explicit TargetTransformInfo(const DataLayout &DL);\n\n  // Provide move semantics.\n  TargetTransformInfo(TargetTransformInfo &&Arg);\n  TargetTransformInfo &operator=(TargetTransformInfo &&RHS);\n\n  // We need to define the destructor out-of-line to define our sub-classes\n  // out-of-line.\n  ~TargetTransformInfo();\n\n  /// Handle the invalidation of this information.\n  ///\n  /// When used as a result of \\c TargetIRAnalysis this method will be called\n  /// when the function this was computed for changes. When it returns false,\n  /// the information is preserved across those changes.\n  bool invalidate(Function &, const PreservedAnalyses &,\n                  FunctionAnalysisManager::Invalidator &) {\n    // FIXME: We should probably in some way ensure that the subtarget\n    // information for a function hasn't changed.\n    return false;\n  }\n\n  /// \\name Generic Target Information\n  /// @{\n\n  /// The kind of cost model.\n  ///\n  /// There are several different cost models that can be customized by the\n  /// target. The normalization of each cost model may be target specific.\n  enum TargetCostKind {\n    TCK_RecipThroughput, ///< Reciprocal throughput.\n    TCK_Latency,         ///< The latency of instruction.\n    TCK_CodeSize,        ///< Instruction code size.\n    TCK_SizeAndLatency   ///< The weighted sum of size and latency.\n  };\n\n  /// Query the cost of a specified instruction.\n  ///\n  /// Clients should use this interface to query the cost of an existing\n  /// instruction. The instruction must have a valid parent (basic block).\n  ///\n  /// Note, this method does not cache the cost calculation and it\n  /// can be expensive in some cases.\n  InstructionCost getInstructionCost(const Instruction *I,\n                                     enum TargetCostKind kind) const {\n    InstructionCost Cost;\n    switch (kind) {\n    case TCK_RecipThroughput:\n      Cost = getInstructionThroughput(I);\n      break;\n    case TCK_Latency:\n      Cost = getInstructionLatency(I);\n      break;\n    case TCK_CodeSize:\n    case TCK_SizeAndLatency:\n      Cost = getUserCost(I, kind);\n      break;\n    }\n    if (Cost == -1)\n      Cost.setInvalid();\n    return Cost;\n  }\n\n  /// Underlying constants for 'cost' values in this interface.\n  ///\n  /// Many APIs in this interface return a cost. This enum defines the\n  /// fundamental values that should be used to interpret (and produce) those\n  /// costs. The costs are returned as an int rather than a member of this\n  /// enumeration because it is expected that the cost of one IR instruction\n  /// may have a multiplicative factor to it or otherwise won't fit directly\n  /// into the enum. Moreover, it is common to sum or average costs which works\n  /// better as simple integral values. Thus this enum only provides constants.\n  /// Also note that the returned costs are signed integers to make it natural\n  /// to add, subtract, and test with zero (a common boundary condition). It is\n  /// not expected that 2^32 is a realistic cost to be modeling at any point.\n  ///\n  /// Note that these costs should usually reflect the intersection of code-size\n  /// cost and execution cost. A free instruction is typically one that folds\n  /// into another instruction. For example, reg-to-reg moves can often be\n  /// skipped by renaming the registers in the CPU, but they still are encoded\n  /// and thus wouldn't be considered 'free' here.\n  enum TargetCostConstants {\n    TCC_Free = 0,     ///< Expected to fold away in lowering.\n    TCC_Basic = 1,    ///< The cost of a typical 'add' instruction.\n    TCC_Expensive = 4 ///< The cost of a 'div' instruction on x86.\n  };\n\n  /// Estimate the cost of a GEP operation when lowered.\n  int getGEPCost(Type *PointeeType, const Value *Ptr,\n                 ArrayRef<const Value *> Operands,\n                 TargetCostKind CostKind = TCK_SizeAndLatency) const;\n\n  /// \\returns A value by which our inlining threshold should be multiplied.\n  /// This is primarily used to bump up the inlining threshold wholesale on\n  /// targets where calls are unusually expensive.\n  ///\n  /// TODO: This is a rather blunt instrument.  Perhaps altering the costs of\n  /// individual classes of instructions would be better.\n  unsigned getInliningThresholdMultiplier() const;\n\n  /// \\returns A value to be added to the inlining threshold.\n  unsigned adjustInliningThreshold(const CallBase *CB) const;\n\n  /// \\returns Vector bonus in percent.\n  ///\n  /// Vector bonuses: We want to more aggressively inline vector-dense kernels\n  /// and apply this bonus based on the percentage of vector instructions. A\n  /// bonus is applied if the vector instructions exceed 50% and half that\n  /// amount is applied if it exceeds 10%. Note that these bonuses are some what\n  /// arbitrary and evolved over time by accident as much as because they are\n  /// principled bonuses.\n  /// FIXME: It would be nice to base the bonus values on something more\n  /// scientific. A target may has no bonus on vector instructions.\n  int getInlinerVectorBonusPercent() const;\n\n  /// \\return the expected cost of a memcpy, which could e.g. depend on the\n  /// source/destination type and alignment and the number of bytes copied.\n  int getMemcpyCost(const Instruction *I) const;\n\n  /// \\return The estimated number of case clusters when lowering \\p 'SI'.\n  /// \\p JTSize Set a jump table size only when \\p SI is suitable for a jump\n  /// table.\n  unsigned getEstimatedNumberOfCaseClusters(const SwitchInst &SI,\n                                            unsigned &JTSize,\n                                            ProfileSummaryInfo *PSI,\n                                            BlockFrequencyInfo *BFI) const;\n\n  /// Estimate the cost of a given IR user when lowered.\n  ///\n  /// This can estimate the cost of either a ConstantExpr or Instruction when\n  /// lowered.\n  ///\n  /// \\p Operands is a list of operands which can be a result of transformations\n  /// of the current operands. The number of the operands on the list must equal\n  /// to the number of the current operands the IR user has. Their order on the\n  /// list must be the same as the order of the current operands the IR user\n  /// has.\n  ///\n  /// The returned cost is defined in terms of \\c TargetCostConstants, see its\n  /// comments for a detailed explanation of the cost values.\n  int getUserCost(const User *U, ArrayRef<const Value *> Operands,\n                  TargetCostKind CostKind) const;\n\n  /// This is a helper function which calls the two-argument getUserCost\n  /// with \\p Operands which are the current operands U has.\n  int getUserCost(const User *U, TargetCostKind CostKind) const {\n    SmallVector<const Value *, 4> Operands(U->operand_values());\n    return getUserCost(U, Operands, CostKind);\n  }\n\n  /// Return true if branch divergence exists.\n  ///\n  /// Branch divergence has a significantly negative impact on GPU performance\n  /// when threads in the same wavefront take different paths due to conditional\n  /// branches.\n  bool hasBranchDivergence() const;\n\n  /// Return true if the target prefers to use GPU divergence analysis to\n  /// replace the legacy version.\n  bool useGPUDivergenceAnalysis() const;\n\n  /// Returns whether V is a source of divergence.\n  ///\n  /// This function provides the target-dependent information for\n  /// the target-independent LegacyDivergenceAnalysis. LegacyDivergenceAnalysis\n  /// first builds the dependency graph, and then runs the reachability\n  /// algorithm starting with the sources of divergence.\n  bool isSourceOfDivergence(const Value *V) const;\n\n  // Returns true for the target specific\n  // set of operations which produce uniform result\n  // even taking non-uniform arguments\n  bool isAlwaysUniform(const Value *V) const;\n\n  /// Returns the address space ID for a target's 'flat' address space. Note\n  /// this is not necessarily the same as addrspace(0), which LLVM sometimes\n  /// refers to as the generic address space. The flat address space is a\n  /// generic address space that can be used access multiple segments of memory\n  /// with different address spaces. Access of a memory location through a\n  /// pointer with this address space is expected to be legal but slower\n  /// compared to the same memory location accessed through a pointer with a\n  /// different address space.\n  //\n  /// This is for targets with different pointer representations which can\n  /// be converted with the addrspacecast instruction. If a pointer is converted\n  /// to this address space, optimizations should attempt to replace the access\n  /// with the source address space.\n  ///\n  /// \\returns ~0u if the target does not have such a flat address space to\n  /// optimize away.\n  unsigned getFlatAddressSpace() const;\n\n  /// Return any intrinsic address operand indexes which may be rewritten if\n  /// they use a flat address space pointer.\n  ///\n  /// \\returns true if the intrinsic was handled.\n  bool collectFlatAddressOperands(SmallVectorImpl<int> &OpIndexes,\n                                  Intrinsic::ID IID) const;\n\n  bool isNoopAddrSpaceCast(unsigned FromAS, unsigned ToAS) const;\n\n  unsigned getAssumedAddrSpace(const Value *V) const;\n\n  /// Rewrite intrinsic call \\p II such that \\p OldV will be replaced with \\p\n  /// NewV, which has a different address space. This should happen for every\n  /// operand index that collectFlatAddressOperands returned for the intrinsic.\n  /// \\returns nullptr if the intrinsic was not handled. Otherwise, returns the\n  /// new value (which may be the original \\p II with modified operands).\n  Value *rewriteIntrinsicWithAddressSpace(IntrinsicInst *II, Value *OldV,\n                                          Value *NewV) const;\n\n  /// Test whether calls to a function lower to actual program function\n  /// calls.\n  ///\n  /// The idea is to test whether the program is likely to require a 'call'\n  /// instruction or equivalent in order to call the given function.\n  ///\n  /// FIXME: It's not clear that this is a good or useful query API. Client's\n  /// should probably move to simpler cost metrics using the above.\n  /// Alternatively, we could split the cost interface into distinct code-size\n  /// and execution-speed costs. This would allow modelling the core of this\n  /// query more accurately as a call is a single small instruction, but\n  /// incurs significant execution cost.\n  bool isLoweredToCall(const Function *F) const;\n\n  struct LSRCost {\n    /// TODO: Some of these could be merged. Also, a lexical ordering\n    /// isn't always optimal.\n    unsigned Insns;\n    unsigned NumRegs;\n    unsigned AddRecCost;\n    unsigned NumIVMuls;\n    unsigned NumBaseAdds;\n    unsigned ImmCost;\n    unsigned SetupCost;\n    unsigned ScaleCost;\n  };\n\n  /// Parameters that control the generic loop unrolling transformation.\n  struct UnrollingPreferences {\n    /// The cost threshold for the unrolled loop. Should be relative to the\n    /// getUserCost values returned by this API, and the expectation is that\n    /// the unrolled loop's instructions when run through that interface should\n    /// not exceed this cost. However, this is only an estimate. Also, specific\n    /// loops may be unrolled even with a cost above this threshold if deemed\n    /// profitable. Set this to UINT_MAX to disable the loop body cost\n    /// restriction.\n    unsigned Threshold;\n    /// If complete unrolling will reduce the cost of the loop, we will boost\n    /// the Threshold by a certain percent to allow more aggressive complete\n    /// unrolling. This value provides the maximum boost percentage that we\n    /// can apply to Threshold (The value should be no less than 100).\n    /// BoostedThreshold = Threshold * min(RolledCost / UnrolledCost,\n    ///                                    MaxPercentThresholdBoost / 100)\n    /// E.g. if complete unrolling reduces the loop execution time by 50%\n    /// then we boost the threshold by the factor of 2x. If unrolling is not\n    /// expected to reduce the running time, then we do not increase the\n    /// threshold.\n    unsigned MaxPercentThresholdBoost;\n    /// The cost threshold for the unrolled loop when optimizing for size (set\n    /// to UINT_MAX to disable).\n    unsigned OptSizeThreshold;\n    /// The cost threshold for the unrolled loop, like Threshold, but used\n    /// for partial/runtime unrolling (set to UINT_MAX to disable).\n    unsigned PartialThreshold;\n    /// The cost threshold for the unrolled loop when optimizing for size, like\n    /// OptSizeThreshold, but used for partial/runtime unrolling (set to\n    /// UINT_MAX to disable).\n    unsigned PartialOptSizeThreshold;\n    /// A forced unrolling factor (the number of concatenated bodies of the\n    /// original loop in the unrolled loop body). When set to 0, the unrolling\n    /// transformation will select an unrolling factor based on the current cost\n    /// threshold and other factors.\n    unsigned Count;\n    /// Default unroll count for loops with run-time trip count.\n    unsigned DefaultUnrollRuntimeCount;\n    // Set the maximum unrolling factor. The unrolling factor may be selected\n    // using the appropriate cost threshold, but may not exceed this number\n    // (set to UINT_MAX to disable). This does not apply in cases where the\n    // loop is being fully unrolled.\n    unsigned MaxCount;\n    /// Set the maximum unrolling factor for full unrolling. Like MaxCount, but\n    /// applies even if full unrolling is selected. This allows a target to fall\n    /// back to Partial unrolling if full unrolling is above FullUnrollMaxCount.\n    unsigned FullUnrollMaxCount;\n    // Represents number of instructions optimized when \"back edge\"\n    // becomes \"fall through\" in unrolled loop.\n    // For now we count a conditional branch on a backedge and a comparison\n    // feeding it.\n    unsigned BEInsns;\n    /// Allow partial unrolling (unrolling of loops to expand the size of the\n    /// loop body, not only to eliminate small constant-trip-count loops).\n    bool Partial;\n    /// Allow runtime unrolling (unrolling of loops to expand the size of the\n    /// loop body even when the number of loop iterations is not known at\n    /// compile time).\n    bool Runtime;\n    /// Allow generation of a loop remainder (extra iterations after unroll).\n    bool AllowRemainder;\n    /// Allow emitting expensive instructions (such as divisions) when computing\n    /// the trip count of a loop for runtime unrolling.\n    bool AllowExpensiveTripCount;\n    /// Apply loop unroll on any kind of loop\n    /// (mainly to loops that fail runtime unrolling).\n    bool Force;\n    /// Allow using trip count upper bound to unroll loops.\n    bool UpperBound;\n    /// Allow unrolling of all the iterations of the runtime loop remainder.\n    bool UnrollRemainder;\n    /// Allow unroll and jam. Used to enable unroll and jam for the target.\n    bool UnrollAndJam;\n    /// Threshold for unroll and jam, for inner loop size. The 'Threshold'\n    /// value above is used during unroll and jam for the outer loop size.\n    /// This value is used in the same manner to limit the size of the inner\n    /// loop.\n    unsigned UnrollAndJamInnerLoopThreshold;\n    /// Don't allow loop unrolling to simulate more than this number of\n    /// iterations when checking full unroll profitability\n    unsigned MaxIterationsCountToAnalyze;\n  };\n\n  /// Get target-customized preferences for the generic loop unrolling\n  /// transformation. The caller will initialize UP with the current\n  /// target-independent defaults.\n  void getUnrollingPreferences(Loop *L, ScalarEvolution &,\n                               UnrollingPreferences &UP) const;\n\n  /// Query the target whether it would be profitable to convert the given loop\n  /// into a hardware loop.\n  bool isHardwareLoopProfitable(Loop *L, ScalarEvolution &SE,\n                                AssumptionCache &AC, TargetLibraryInfo *LibInfo,\n                                HardwareLoopInfo &HWLoopInfo) const;\n\n  /// Query the target whether it would be prefered to create a predicated\n  /// vector loop, which can avoid the need to emit a scalar epilogue loop.\n  bool preferPredicateOverEpilogue(Loop *L, LoopInfo *LI, ScalarEvolution &SE,\n                                   AssumptionCache &AC, TargetLibraryInfo *TLI,\n                                   DominatorTree *DT,\n                                   const LoopAccessInfo *LAI) const;\n\n  /// Query the target whether lowering of the llvm.get.active.lane.mask\n  /// intrinsic is supported.\n  bool emitGetActiveLaneMask() const;\n\n  // Parameters that control the loop peeling transformation\n  struct PeelingPreferences {\n    /// A forced peeling factor (the number of bodied of the original loop\n    /// that should be peeled off before the loop body). When set to 0, the\n    /// a peeling factor based on profile information and other factors.\n    unsigned PeelCount;\n    /// Allow peeling off loop iterations.\n    bool AllowPeeling;\n    /// Allow peeling off loop iterations for loop nests.\n    bool AllowLoopNestsPeeling;\n    /// Allow peeling basing on profile. Uses to enable peeling off all\n    /// iterations basing on provided profile.\n    /// If the value is true the peeling cost model can decide to peel only\n    /// some iterations and in this case it will set this to false.\n    bool PeelProfiledIterations;\n  };\n\n  /// Get target-customized preferences for the generic loop peeling\n  /// transformation. The caller will initialize \\p PP with the current\n  /// target-independent defaults with information from \\p L and \\p SE.\n  void getPeelingPreferences(Loop *L, ScalarEvolution &SE,\n                             PeelingPreferences &PP) const;\n\n  /// Targets can implement their own combinations for target-specific\n  /// intrinsics. This function will be called from the InstCombine pass every\n  /// time a target-specific intrinsic is encountered.\n  ///\n  /// \\returns None to not do anything target specific or a value that will be\n  /// returned from the InstCombiner. It is possible to return null and stop\n  /// further processing of the intrinsic by returning nullptr.\n  Optional<Instruction *> instCombineIntrinsic(InstCombiner &IC,\n                                               IntrinsicInst &II) const;\n  /// Can be used to implement target-specific instruction combining.\n  /// \\see instCombineIntrinsic\n  Optional<Value *>\n  simplifyDemandedUseBitsIntrinsic(InstCombiner &IC, IntrinsicInst &II,\n                                   APInt DemandedMask, KnownBits &Known,\n                                   bool &KnownBitsComputed) const;\n  /// Can be used to implement target-specific instruction combining.\n  /// \\see instCombineIntrinsic\n  Optional<Value *> simplifyDemandedVectorEltsIntrinsic(\n      InstCombiner &IC, IntrinsicInst &II, APInt DemandedElts, APInt &UndefElts,\n      APInt &UndefElts2, APInt &UndefElts3,\n      std::function<void(Instruction *, unsigned, APInt, APInt &)>\n          SimplifyAndSetOp) const;\n  /// @}\n\n  /// \\name Scalar Target Information\n  /// @{\n\n  /// Flags indicating the kind of support for population count.\n  ///\n  /// Compared to the SW implementation, HW support is supposed to\n  /// significantly boost the performance when the population is dense, and it\n  /// may or may not degrade performance if the population is sparse. A HW\n  /// support is considered as \"Fast\" if it can outperform, or is on a par\n  /// with, SW implementation when the population is sparse; otherwise, it is\n  /// considered as \"Slow\".\n  enum PopcntSupportKind { PSK_Software, PSK_SlowHardware, PSK_FastHardware };\n\n  /// Return true if the specified immediate is legal add immediate, that\n  /// is the target has add instructions which can add a register with the\n  /// immediate without having to materialize the immediate into a register.\n  bool isLegalAddImmediate(int64_t Imm) const;\n\n  /// Return true if the specified immediate is legal icmp immediate,\n  /// that is the target has icmp instructions which can compare a register\n  /// against the immediate without having to materialize the immediate into a\n  /// register.\n  bool isLegalICmpImmediate(int64_t Imm) const;\n\n  /// Return true if the addressing mode represented by AM is legal for\n  /// this target, for a load/store of the specified type.\n  /// The type may be VoidTy, in which case only return true if the addressing\n  /// mode is legal for a load/store of any legal type.\n  /// If target returns true in LSRWithInstrQueries(), I may be valid.\n  /// TODO: Handle pre/postinc as well.\n  bool isLegalAddressingMode(Type *Ty, GlobalValue *BaseGV, int64_t BaseOffset,\n                             bool HasBaseReg, int64_t Scale,\n                             unsigned AddrSpace = 0,\n                             Instruction *I = nullptr) const;\n\n  /// Return true if LSR cost of C1 is lower than C1.\n  bool isLSRCostLess(TargetTransformInfo::LSRCost &C1,\n                     TargetTransformInfo::LSRCost &C2) const;\n\n  /// Return true if LSR major cost is number of registers. Targets which\n  /// implement their own isLSRCostLess and unset number of registers as major\n  /// cost should return false, otherwise return true.\n  bool isNumRegsMajorCostOfLSR() const;\n\n  /// \\returns true if LSR should not optimize a chain that includes \\p I.\n  bool isProfitableLSRChainElement(Instruction *I) const;\n\n  /// Return true if the target can fuse a compare and branch.\n  /// Loop-strength-reduction (LSR) uses that knowledge to adjust its cost\n  /// calculation for the instructions in a loop.\n  bool canMacroFuseCmp() const;\n\n  /// Return true if the target can save a compare for loop count, for example\n  /// hardware loop saves a compare.\n  bool canSaveCmp(Loop *L, BranchInst **BI, ScalarEvolution *SE, LoopInfo *LI,\n                  DominatorTree *DT, AssumptionCache *AC,\n                  TargetLibraryInfo *LibInfo) const;\n\n  enum AddressingModeKind {\n    AMK_PreIndexed,\n    AMK_PostIndexed,\n    AMK_None\n  };\n\n  /// Return the preferred addressing mode LSR should make efforts to generate.\n  AddressingModeKind getPreferredAddressingMode(const Loop *L,\n                                                ScalarEvolution *SE) const;\n\n  /// Return true if the target supports masked store.\n  bool isLegalMaskedStore(Type *DataType, Align Alignment) const;\n  /// Return true if the target supports masked load.\n  bool isLegalMaskedLoad(Type *DataType, Align Alignment) const;\n\n  /// Return true if the target supports nontemporal store.\n  bool isLegalNTStore(Type *DataType, Align Alignment) const;\n  /// Return true if the target supports nontemporal load.\n  bool isLegalNTLoad(Type *DataType, Align Alignment) const;\n\n  /// Return true if the target supports masked scatter.\n  bool isLegalMaskedScatter(Type *DataType, Align Alignment) const;\n  /// Return true if the target supports masked gather.\n  bool isLegalMaskedGather(Type *DataType, Align Alignment) const;\n\n  /// Return true if the target supports masked compress store.\n  bool isLegalMaskedCompressStore(Type *DataType) const;\n  /// Return true if the target supports masked expand load.\n  bool isLegalMaskedExpandLoad(Type *DataType) const;\n\n  /// Return true if the target has a unified operation to calculate division\n  /// and remainder. If so, the additional implicit multiplication and\n  /// subtraction required to calculate a remainder from division are free. This\n  /// can enable more aggressive transformations for division and remainder than\n  /// would typically be allowed using throughput or size cost models.\n  bool hasDivRemOp(Type *DataType, bool IsSigned) const;\n\n  /// Return true if the given instruction (assumed to be a memory access\n  /// instruction) has a volatile variant. If that's the case then we can avoid\n  /// addrspacecast to generic AS for volatile loads/stores. Default\n  /// implementation returns false, which prevents address space inference for\n  /// volatile loads/stores.\n  bool hasVolatileVariant(Instruction *I, unsigned AddrSpace) const;\n\n  /// Return true if target doesn't mind addresses in vectors.\n  bool prefersVectorizedAddressing() const;\n\n  /// Return the cost of the scaling factor used in the addressing\n  /// mode represented by AM for this target, for a load/store\n  /// of the specified type.\n  /// If the AM is supported, the return value must be >= 0.\n  /// If the AM is not supported, it returns a negative value.\n  /// TODO: Handle pre/postinc as well.\n  int getScalingFactorCost(Type *Ty, GlobalValue *BaseGV, int64_t BaseOffset,\n                           bool HasBaseReg, int64_t Scale,\n                           unsigned AddrSpace = 0) const;\n\n  /// Return true if the loop strength reduce pass should make\n  /// Instruction* based TTI queries to isLegalAddressingMode(). This is\n  /// needed on SystemZ, where e.g. a memcpy can only have a 12 bit unsigned\n  /// immediate offset and no index register.\n  bool LSRWithInstrQueries() const;\n\n  /// Return true if it's free to truncate a value of type Ty1 to type\n  /// Ty2. e.g. On x86 it's free to truncate a i32 value in register EAX to i16\n  /// by referencing its sub-register AX.\n  bool isTruncateFree(Type *Ty1, Type *Ty2) const;\n\n  /// Return true if it is profitable to hoist instruction in the\n  /// then/else to before if.\n  bool isProfitableToHoist(Instruction *I) const;\n\n  bool useAA() const;\n\n  /// Return true if this type is legal.\n  bool isTypeLegal(Type *Ty) const;\n\n  /// Returns the estimated number of registers required to represent \\p Ty.\n  unsigned getRegUsageForType(Type *Ty) const;\n\n  /// Return true if switches should be turned into lookup tables for the\n  /// target.\n  bool shouldBuildLookupTables() const;\n\n  /// Return true if switches should be turned into lookup tables\n  /// containing this constant value for the target.\n  bool shouldBuildLookupTablesForConstant(Constant *C) const;\n\n  /// Return true if the input function which is cold at all call sites,\n  ///  should use coldcc calling convention.\n  bool useColdCCForColdCall(Function &F) const;\n\n  /// Estimate the overhead of scalarizing an instruction. Insert and Extract\n  /// are set if the demanded result elements need to be inserted and/or\n  /// extracted from vectors.\n  unsigned getScalarizationOverhead(VectorType *Ty, const APInt &DemandedElts,\n                                    bool Insert, bool Extract) const;\n\n  /// Estimate the overhead of scalarizing an instructions unique\n  /// non-constant operands. The (potentially vector) types to use for each of\n  /// argument are passes via Tys.\n  unsigned getOperandsScalarizationOverhead(ArrayRef<const Value *> Args,\n                                            ArrayRef<Type *> Tys) const;\n\n  /// If target has efficient vector element load/store instructions, it can\n  /// return true here so that insertion/extraction costs are not added to\n  /// the scalarization cost of a load/store.\n  bool supportsEfficientVectorElementLoadStore() const;\n\n  /// Don't restrict interleaved unrolling to small loops.\n  bool enableAggressiveInterleaving(bool LoopHasReductions) const;\n\n  /// Returns options for expansion of memcmp. IsZeroCmp is\n  // true if this is the expansion of memcmp(p1, p2, s) == 0.\n  struct MemCmpExpansionOptions {\n    // Return true if memcmp expansion is enabled.\n    operator bool() const { return MaxNumLoads > 0; }\n\n    // Maximum number of load operations.\n    unsigned MaxNumLoads = 0;\n\n    // The list of available load sizes (in bytes), sorted in decreasing order.\n    SmallVector<unsigned, 8> LoadSizes;\n\n    // For memcmp expansion when the memcmp result is only compared equal or\n    // not-equal to 0, allow up to this number of load pairs per block. As an\n    // example, this may allow 'memcmp(a, b, 3) == 0' in a single block:\n    //   a0 = load2bytes &a[0]\n    //   b0 = load2bytes &b[0]\n    //   a2 = load1byte  &a[2]\n    //   b2 = load1byte  &b[2]\n    //   r  = cmp eq (a0 ^ b0 | a2 ^ b2), 0\n    unsigned NumLoadsPerBlock = 1;\n\n    // Set to true to allow overlapping loads. For example, 7-byte compares can\n    // be done with two 4-byte compares instead of 4+2+1-byte compares. This\n    // requires all loads in LoadSizes to be doable in an unaligned way.\n    bool AllowOverlappingLoads = false;\n  };\n  MemCmpExpansionOptions enableMemCmpExpansion(bool OptSize,\n                                               bool IsZeroCmp) const;\n\n  /// Enable matching of interleaved access groups.\n  bool enableInterleavedAccessVectorization() const;\n\n  /// Enable matching of interleaved access groups that contain predicated\n  /// accesses or gaps and therefore vectorized using masked\n  /// vector loads/stores.\n  bool enableMaskedInterleavedAccessVectorization() const;\n\n  /// Indicate that it is potentially unsafe to automatically vectorize\n  /// floating-point operations because the semantics of vector and scalar\n  /// floating-point semantics may differ. For example, ARM NEON v7 SIMD math\n  /// does not support IEEE-754 denormal numbers, while depending on the\n  /// platform, scalar floating-point math does.\n  /// This applies to floating-point math operations and calls, not memory\n  /// operations, shuffles, or casts.\n  bool isFPVectorizationPotentiallyUnsafe() const;\n\n  /// Determine if the target supports unaligned memory accesses.\n  bool allowsMisalignedMemoryAccesses(LLVMContext &Context, unsigned BitWidth,\n                                      unsigned AddressSpace = 0,\n                                      Align Alignment = Align(1),\n                                      bool *Fast = nullptr) const;\n\n  /// Return hardware support for population count.\n  PopcntSupportKind getPopcntSupport(unsigned IntTyWidthInBit) const;\n\n  /// Return true if the hardware has a fast square-root instruction.\n  bool haveFastSqrt(Type *Ty) const;\n\n  /// Return true if it is faster to check if a floating-point value is NaN\n  /// (or not-NaN) versus a comparison against a constant FP zero value.\n  /// Targets should override this if materializing a 0.0 for comparison is\n  /// generally as cheap as checking for ordered/unordered.\n  bool isFCmpOrdCheaperThanFCmpZero(Type *Ty) const;\n\n  /// Return the expected cost of supporting the floating point operation\n  /// of the specified type.\n  int getFPOpCost(Type *Ty) const;\n\n  /// Return the expected cost of materializing for the given integer\n  /// immediate of the specified type.\n  int getIntImmCost(const APInt &Imm, Type *Ty, TargetCostKind CostKind) const;\n\n  /// Return the expected cost of materialization for the given integer\n  /// immediate of the specified type for a given instruction. The cost can be\n  /// zero if the immediate can be folded into the specified instruction.\n  int getIntImmCostInst(unsigned Opc, unsigned Idx, const APInt &Imm, Type *Ty,\n                        TargetCostKind CostKind,\n                        Instruction *Inst = nullptr) const;\n  int getIntImmCostIntrin(Intrinsic::ID IID, unsigned Idx, const APInt &Imm,\n                          Type *Ty, TargetCostKind CostKind) const;\n\n  /// Return the expected cost for the given integer when optimising\n  /// for size. This is different than the other integer immediate cost\n  /// functions in that it is subtarget agnostic. This is useful when you e.g.\n  /// target one ISA such as Aarch32 but smaller encodings could be possible\n  /// with another such as Thumb. This return value is used as a penalty when\n  /// the total costs for a constant is calculated (the bigger the cost, the\n  /// more beneficial constant hoisting is).\n  int getIntImmCodeSizeCost(unsigned Opc, unsigned Idx, const APInt &Imm,\n                            Type *Ty) const;\n  /// @}\n\n  /// \\name Vector Target Information\n  /// @{\n\n  /// The various kinds of shuffle patterns for vector queries.\n  enum ShuffleKind {\n    SK_Broadcast,        ///< Broadcast element 0 to all other elements.\n    SK_Reverse,          ///< Reverse the order of the vector.\n    SK_Select,           ///< Selects elements from the corresponding lane of\n                         ///< either source operand. This is equivalent to a\n                         ///< vector select with a constant condition operand.\n    SK_Transpose,        ///< Transpose two vectors.\n    SK_InsertSubvector,  ///< InsertSubvector. Index indicates start offset.\n    SK_ExtractSubvector, ///< ExtractSubvector Index indicates start offset.\n    SK_PermuteTwoSrc,    ///< Merge elements from two source vectors into one\n                         ///< with any shuffle mask.\n    SK_PermuteSingleSrc  ///< Shuffle elements of single source vector with any\n                         ///< shuffle mask.\n  };\n\n  /// Kind of the reduction data.\n  enum ReductionKind {\n    RK_None,           /// Not a reduction.\n    RK_Arithmetic,     /// Binary reduction data.\n    RK_MinMax,         /// Min/max reduction data.\n    RK_UnsignedMinMax, /// Unsigned min/max reduction data.\n  };\n\n  /// Contains opcode + LHS/RHS parts of the reduction operations.\n  struct ReductionData {\n    ReductionData() = delete;\n    ReductionData(ReductionKind Kind, unsigned Opcode, Value *LHS, Value *RHS)\n        : Opcode(Opcode), LHS(LHS), RHS(RHS), Kind(Kind) {\n      assert(Kind != RK_None && \"expected binary or min/max reduction only.\");\n    }\n    unsigned Opcode = 0;\n    Value *LHS = nullptr;\n    Value *RHS = nullptr;\n    ReductionKind Kind = RK_None;\n    bool hasSameData(ReductionData &RD) const {\n      return Kind == RD.Kind && Opcode == RD.Opcode;\n    }\n  };\n\n  static ReductionKind matchPairwiseReduction(\n    const ExtractElementInst *ReduxRoot, unsigned &Opcode, VectorType *&Ty);\n\n  static ReductionKind matchVectorSplittingReduction(\n    const ExtractElementInst *ReduxRoot, unsigned &Opcode, VectorType *&Ty);\n\n  static ReductionKind matchVectorReduction(const ExtractElementInst *ReduxRoot,\n                                            unsigned &Opcode, VectorType *&Ty,\n                                            bool &IsPairwise);\n\n  /// Additional information about an operand's possible values.\n  enum OperandValueKind {\n    OK_AnyValue,               // Operand can have any value.\n    OK_UniformValue,           // Operand is uniform (splat of a value).\n    OK_UniformConstantValue,   // Operand is uniform constant.\n    OK_NonUniformConstantValue // Operand is a non uniform constant value.\n  };\n\n  /// Additional properties of an operand's values.\n  enum OperandValueProperties { OP_None = 0, OP_PowerOf2 = 1 };\n\n  /// \\return the number of registers in the target-provided register class.\n  unsigned getNumberOfRegisters(unsigned ClassID) const;\n\n  /// \\return the target-provided register class ID for the provided type,\n  /// accounting for type promotion and other type-legalization techniques that\n  /// the target might apply. However, it specifically does not account for the\n  /// scalarization or splitting of vector types. Should a vector type require\n  /// scalarization or splitting into multiple underlying vector registers, that\n  /// type should be mapped to a register class containing no registers.\n  /// Specifically, this is designed to provide a simple, high-level view of the\n  /// register allocation later performed by the backend. These register classes\n  /// don't necessarily map onto the register classes used by the backend.\n  /// FIXME: It's not currently possible to determine how many registers\n  /// are used by the provided type.\n  unsigned getRegisterClassForType(bool Vector, Type *Ty = nullptr) const;\n\n  /// \\return the target-provided register class name\n  const char *getRegisterClassName(unsigned ClassID) const;\n\n  /// \\return The width of the largest scalar or vector register type.\n  unsigned getRegisterBitWidth(bool Vector) const;\n\n  /// \\return The width of the smallest vector register type.\n  unsigned getMinVectorRegisterBitWidth() const;\n\n  /// \\return The maximum value of vscale if the target specifies an\n  ///  architectural maximum vector length, and None otherwise.\n  Optional<unsigned> getMaxVScale() const;\n\n  /// \\return True if the vectorization factor should be chosen to\n  /// make the vector of the smallest element type match the size of a\n  /// vector register. For wider element types, this could result in\n  /// creating vectors that span multiple vector registers.\n  /// If false, the vectorization factor will be chosen based on the\n  /// size of the widest element type.\n  bool shouldMaximizeVectorBandwidth(bool OptSize) const;\n\n  /// \\return The minimum vectorization factor for types of given element\n  /// bit width, or 0 if there is no minimum VF. The returned value only\n  /// applies when shouldMaximizeVectorBandwidth returns true.\n  /// If IsScalable is true, the returned ElementCount must be a scalable VF.\n  ElementCount getMinimumVF(unsigned ElemWidth, bool IsScalable) const;\n\n  /// \\return The maximum vectorization factor for types of given element\n  /// bit width and opcode, or 0 if there is no maximum VF.\n  /// Currently only used by the SLP vectorizer.\n  unsigned getMaximumVF(unsigned ElemWidth, unsigned Opcode) const;\n\n  /// \\return True if it should be considered for address type promotion.\n  /// \\p AllowPromotionWithoutCommonHeader Set true if promoting \\p I is\n  /// profitable without finding other extensions fed by the same input.\n  bool shouldConsiderAddressTypePromotion(\n      const Instruction &I, bool &AllowPromotionWithoutCommonHeader) const;\n\n  /// \\return The size of a cache line in bytes.\n  unsigned getCacheLineSize() const;\n\n  /// The possible cache levels\n  enum class CacheLevel {\n    L1D, // The L1 data cache\n    L2D, // The L2 data cache\n\n    // We currently do not model L3 caches, as their sizes differ widely between\n    // microarchitectures. Also, we currently do not have a use for L3 cache\n    // size modeling yet.\n  };\n\n  /// \\return The size of the cache level in bytes, if available.\n  Optional<unsigned> getCacheSize(CacheLevel Level) const;\n\n  /// \\return The associativity of the cache level, if available.\n  Optional<unsigned> getCacheAssociativity(CacheLevel Level) const;\n\n  /// \\return How much before a load we should place the prefetch\n  /// instruction.  This is currently measured in number of\n  /// instructions.\n  unsigned getPrefetchDistance() const;\n\n  /// Some HW prefetchers can handle accesses up to a certain constant stride.\n  /// Sometimes prefetching is beneficial even below the HW prefetcher limit,\n  /// and the arguments provided are meant to serve as a basis for deciding this\n  /// for a particular loop.\n  ///\n  /// \\param NumMemAccesses        Number of memory accesses in the loop.\n  /// \\param NumStridedMemAccesses Number of the memory accesses that\n  ///                              ScalarEvolution could find a known stride\n  ///                              for.\n  /// \\param NumPrefetches         Number of software prefetches that will be\n  ///                              emitted as determined by the addresses\n  ///                              involved and the cache line size.\n  /// \\param HasCall               True if the loop contains a call.\n  ///\n  /// \\return This is the minimum stride in bytes where it makes sense to start\n  ///         adding SW prefetches. The default is 1, i.e. prefetch with any\n  ///         stride.\n  unsigned getMinPrefetchStride(unsigned NumMemAccesses,\n                                unsigned NumStridedMemAccesses,\n                                unsigned NumPrefetches, bool HasCall) const;\n\n  /// \\return The maximum number of iterations to prefetch ahead.  If\n  /// the required number of iterations is more than this number, no\n  /// prefetching is performed.\n  unsigned getMaxPrefetchIterationsAhead() const;\n\n  /// \\return True if prefetching should also be done for writes.\n  bool enableWritePrefetching() const;\n\n  /// \\return The maximum interleave factor that any transform should try to\n  /// perform for this target. This number depends on the level of parallelism\n  /// and the number of execution units in the CPU.\n  unsigned getMaxInterleaveFactor(unsigned VF) const;\n\n  /// Collect properties of V used in cost analysis, e.g. OP_PowerOf2.\n  static OperandValueKind getOperandInfo(const Value *V,\n                                         OperandValueProperties &OpProps);\n\n  /// This is an approximation of reciprocal throughput of a math/logic op.\n  /// A higher cost indicates less expected throughput.\n  /// From Agner Fog's guides, reciprocal throughput is \"the average number of\n  /// clock cycles per instruction when the instructions are not part of a\n  /// limiting dependency chain.\"\n  /// Therefore, costs should be scaled to account for multiple execution units\n  /// on the target that can process this type of instruction. For example, if\n  /// there are 5 scalar integer units and 2 vector integer units that can\n  /// calculate an 'add' in a single cycle, this model should indicate that the\n  /// cost of the vector add instruction is 2.5 times the cost of the scalar\n  /// add instruction.\n  /// \\p Args is an optional argument which holds the instruction operands\n  /// values so the TTI can analyze those values searching for special\n  /// cases or optimizations based on those values.\n  /// \\p CxtI is the optional original context instruction, if one exists, to\n  /// provide even more information.\n  int getArithmeticInstrCost(\n      unsigned Opcode, Type *Ty,\n      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput,\n      OperandValueKind Opd1Info = OK_AnyValue,\n      OperandValueKind Opd2Info = OK_AnyValue,\n      OperandValueProperties Opd1PropInfo = OP_None,\n      OperandValueProperties Opd2PropInfo = OP_None,\n      ArrayRef<const Value *> Args = ArrayRef<const Value *>(),\n      const Instruction *CxtI = nullptr) const;\n\n  /// \\return The cost of a shuffle instruction of kind Kind and of type Tp.\n  /// The index and subtype parameters are used by the subvector insertion and\n  /// extraction shuffle kinds to show the insert/extract point and the type of\n  /// the subvector being inserted/extracted.\n  /// NOTE: For subvector extractions Tp represents the source type.\n  int getShuffleCost(ShuffleKind Kind, VectorType *Tp, int Index = 0,\n                     VectorType *SubTp = nullptr) const;\n\n  /// Represents a hint about the context in which a cast is used.\n  ///\n  /// For zext/sext, the context of the cast is the operand, which must be a\n  /// load of some kind. For trunc, the context is of the cast is the single\n  /// user of the instruction, which must be a store of some kind.\n  ///\n  /// This enum allows the vectorizer to give getCastInstrCost an idea of the\n  /// type of cast it's dealing with, as not every cast is equal. For instance,\n  /// the zext of a load may be free, but the zext of an interleaving load can\n  //// be (very) expensive!\n  ///\n  /// See \\c getCastContextHint to compute a CastContextHint from a cast\n  /// Instruction*. Callers can use it if they don't need to override the\n  /// context and just want it to be calculated from the instruction.\n  ///\n  /// FIXME: This handles the types of load/store that the vectorizer can\n  /// produce, which are the cases where the context instruction is most\n  /// likely to be incorrect. There are other situations where that can happen\n  /// too, which might be handled here but in the long run a more general\n  /// solution of costing multiple instructions at the same times may be better.\n  enum class CastContextHint : uint8_t {\n    None,          ///< The cast is not used with a load/store of any kind.\n    Normal,        ///< The cast is used with a normal load/store.\n    Masked,        ///< The cast is used with a masked load/store.\n    GatherScatter, ///< The cast is used with a gather/scatter.\n    Interleave,    ///< The cast is used with an interleaved load/store.\n    Reversed,      ///< The cast is used with a reversed load/store.\n  };\n\n  /// Calculates a CastContextHint from \\p I.\n  /// This should be used by callers of getCastInstrCost if they wish to\n  /// determine the context from some instruction.\n  /// \\returns the CastContextHint for ZExt/SExt/Trunc, None if \\p I is nullptr,\n  /// or if it's another type of cast.\n  static CastContextHint getCastContextHint(const Instruction *I);\n\n  /// \\return The expected cost of cast instructions, such as bitcast, trunc,\n  /// zext, etc. If there is an existing instruction that holds Opcode, it\n  /// may be passed in the 'I' parameter.\n  int getCastInstrCost(unsigned Opcode, Type *Dst, Type *Src,\n                       TTI::CastContextHint CCH,\n                       TTI::TargetCostKind CostKind = TTI::TCK_SizeAndLatency,\n                       const Instruction *I = nullptr) const;\n\n  /// \\return The expected cost of a sign- or zero-extended vector extract. Use\n  /// -1 to indicate that there is no information about the index value.\n  int getExtractWithExtendCost(unsigned Opcode, Type *Dst, VectorType *VecTy,\n                               unsigned Index = -1) const;\n\n  /// \\return The expected cost of control-flow related instructions such as\n  /// Phi, Ret, Br.\n  int getCFInstrCost(unsigned Opcode,\n                     TTI::TargetCostKind CostKind = TTI::TCK_SizeAndLatency) const;\n\n  /// \\returns The expected cost of compare and select instructions. If there\n  /// is an existing instruction that holds Opcode, it may be passed in the\n  /// 'I' parameter. The \\p VecPred parameter can be used to indicate the select\n  /// is using a compare with the specified predicate as condition. When vector\n  /// types are passed, \\p VecPred must be used for all lanes.\n  int getCmpSelInstrCost(\n      unsigned Opcode, Type *ValTy, Type *CondTy = nullptr,\n      CmpInst::Predicate VecPred = CmpInst::BAD_ICMP_PREDICATE,\n      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput,\n      const Instruction *I = nullptr) const;\n\n  /// \\return The expected cost of vector Insert and Extract.\n  /// Use -1 to indicate that there is no information on the index value.\n  int getVectorInstrCost(unsigned Opcode, Type *Val, unsigned Index = -1) const;\n\n  /// \\return The cost of Load and Store instructions.\n  int getMemoryOpCost(unsigned Opcode, Type *Src, Align Alignment,\n                      unsigned AddressSpace,\n                      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput,\n                      const Instruction *I = nullptr) const;\n\n  /// \\return The cost of masked Load and Store instructions.\n  int getMaskedMemoryOpCost(\n      unsigned Opcode, Type *Src, Align Alignment, unsigned AddressSpace,\n      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput) const;\n\n  /// \\return The cost of Gather or Scatter operation\n  /// \\p Opcode - is a type of memory access Load or Store\n  /// \\p DataTy - a vector type of the data to be loaded or stored\n  /// \\p Ptr - pointer [or vector of pointers] - address[es] in memory\n  /// \\p VariableMask - true when the memory access is predicated with a mask\n  ///                   that is not a compile-time constant\n  /// \\p Alignment - alignment of single element\n  /// \\p I - the optional original context instruction, if one exists, e.g. the\n  ///        load/store to transform or the call to the gather/scatter intrinsic\n  int getGatherScatterOpCost(\n      unsigned Opcode, Type *DataTy, const Value *Ptr, bool VariableMask,\n      Align Alignment, TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput,\n      const Instruction *I = nullptr) const;\n\n  /// \\return The cost of the interleaved memory operation.\n  /// \\p Opcode is the memory operation code\n  /// \\p VecTy is the vector type of the interleaved access.\n  /// \\p Factor is the interleave factor\n  /// \\p Indices is the indices for interleaved load members (as interleaved\n  ///    load allows gaps)\n  /// \\p Alignment is the alignment of the memory operation\n  /// \\p AddressSpace is address space of the pointer.\n  /// \\p UseMaskForCond indicates if the memory access is predicated.\n  /// \\p UseMaskForGaps indicates if gaps should be masked.\n  int getInterleavedMemoryOpCost(\n      unsigned Opcode, Type *VecTy, unsigned Factor, ArrayRef<unsigned> Indices,\n      Align Alignment, unsigned AddressSpace,\n      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput,\n      bool UseMaskForCond = false, bool UseMaskForGaps = false) const;\n\n  /// Calculate the cost of performing a vector reduction.\n  ///\n  /// This is the cost of reducing the vector value of type \\p Ty to a scalar\n  /// value using the operation denoted by \\p Opcode. The form of the reduction\n  /// can either be a pairwise reduction or a reduction that splits the vector\n  /// at every reduction level.\n  ///\n  /// Pairwise:\n  ///  (v0, v1, v2, v3)\n  ///  ((v0+v1), (v2+v3), undef, undef)\n  /// Split:\n  ///  (v0, v1, v2, v3)\n  ///  ((v0+v2), (v1+v3), undef, undef)\n  int getArithmeticReductionCost(\n    unsigned Opcode, VectorType *Ty, bool IsPairwiseForm,\n    TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput) const;\n\n  int getMinMaxReductionCost(\n    VectorType *Ty, VectorType *CondTy, bool IsPairwiseForm, bool IsUnsigned,\n    TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput) const;\n\n  /// Calculate the cost of an extended reduction pattern, similar to\n  /// getArithmeticReductionCost of an Add reduction with an extension and\n  /// optional multiply. This is the cost of as:\n  /// ResTy vecreduce.add(ext(Ty A)), or if IsMLA flag is set then:\n  /// ResTy vecreduce.add(mul(ext(Ty A), ext(Ty B)). The reduction happens\n  /// on a VectorType with ResTy elements and Ty lanes.\n  InstructionCost getExtendedAddReductionCost(\n      bool IsMLA, bool IsUnsigned, Type *ResTy, VectorType *Ty,\n      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput) const;\n\n  /// \\returns The cost of Intrinsic instructions. Analyses the real arguments.\n  /// Three cases are handled: 1. scalar instruction 2. vector instruction\n  /// 3. scalar instruction which is to be vectorized.\n  int getIntrinsicInstrCost(const IntrinsicCostAttributes &ICA,\n                            TTI::TargetCostKind CostKind) const;\n\n  /// \\returns The cost of Call instructions.\n  int getCallInstrCost(Function *F, Type *RetTy, ArrayRef<Type *> Tys,\n                 TTI::TargetCostKind CostKind = TTI::TCK_SizeAndLatency) const;\n\n  /// \\returns The number of pieces into which the provided type must be\n  /// split during legalization. Zero is returned when the answer is unknown.\n  unsigned getNumberOfParts(Type *Tp) const;\n\n  /// \\returns The cost of the address computation. For most targets this can be\n  /// merged into the instruction indexing mode. Some targets might want to\n  /// distinguish between address computation for memory operations on vector\n  /// types and scalar types. Such targets should override this function.\n  /// The 'SE' parameter holds pointer for the scalar evolution object which\n  /// is used in order to get the Ptr step value in case of constant stride.\n  /// The 'Ptr' parameter holds SCEV of the access pointer.\n  int getAddressComputationCost(Type *Ty, ScalarEvolution *SE = nullptr,\n                                const SCEV *Ptr = nullptr) const;\n\n  /// \\returns The cost, if any, of keeping values of the given types alive\n  /// over a callsite.\n  ///\n  /// Some types may require the use of register classes that do not have\n  /// any callee-saved registers, so would require a spill and fill.\n  unsigned getCostOfKeepingLiveOverCall(ArrayRef<Type *> Tys) const;\n\n  /// \\returns True if the intrinsic is a supported memory intrinsic.  Info\n  /// will contain additional information - whether the intrinsic may write\n  /// or read to memory, volatility and the pointer.  Info is undefined\n  /// if false is returned.\n  bool getTgtMemIntrinsic(IntrinsicInst *Inst, MemIntrinsicInfo &Info) const;\n\n  /// \\returns The maximum element size, in bytes, for an element\n  /// unordered-atomic memory intrinsic.\n  unsigned getAtomicMemIntrinsicMaxElementSize() const;\n\n  /// \\returns A value which is the result of the given memory intrinsic.  New\n  /// instructions may be created to extract the result from the given intrinsic\n  /// memory operation.  Returns nullptr if the target cannot create a result\n  /// from the given intrinsic.\n  Value *getOrCreateResultFromMemIntrinsic(IntrinsicInst *Inst,\n                                           Type *ExpectedType) const;\n\n  /// \\returns The type to use in a loop expansion of a memcpy call.\n  Type *getMemcpyLoopLoweringType(LLVMContext &Context, Value *Length,\n                                  unsigned SrcAddrSpace, unsigned DestAddrSpace,\n                                  unsigned SrcAlign, unsigned DestAlign) const;\n\n  /// \\param[out] OpsOut The operand types to copy RemainingBytes of memory.\n  /// \\param RemainingBytes The number of bytes to copy.\n  ///\n  /// Calculates the operand types to use when copying \\p RemainingBytes of\n  /// memory, where source and destination alignments are \\p SrcAlign and\n  /// \\p DestAlign respectively.\n  void getMemcpyLoopResidualLoweringType(\n      SmallVectorImpl<Type *> &OpsOut, LLVMContext &Context,\n      unsigned RemainingBytes, unsigned SrcAddrSpace, unsigned DestAddrSpace,\n      unsigned SrcAlign, unsigned DestAlign) const;\n\n  /// \\returns True if the two functions have compatible attributes for inlining\n  /// purposes.\n  bool areInlineCompatible(const Function *Caller,\n                           const Function *Callee) const;\n\n  /// \\returns True if the caller and callee agree on how \\p Args will be passed\n  /// to the callee.\n  /// \\param[out] Args The list of compatible arguments.  The implementation may\n  /// filter out any incompatible args from this list.\n  bool areFunctionArgsABICompatible(const Function *Caller,\n                                    const Function *Callee,\n                                    SmallPtrSetImpl<Argument *> &Args) const;\n\n  /// The type of load/store indexing.\n  enum MemIndexedMode {\n    MIM_Unindexed, ///< No indexing.\n    MIM_PreInc,    ///< Pre-incrementing.\n    MIM_PreDec,    ///< Pre-decrementing.\n    MIM_PostInc,   ///< Post-incrementing.\n    MIM_PostDec    ///< Post-decrementing.\n  };\n\n  /// \\returns True if the specified indexed load for the given type is legal.\n  bool isIndexedLoadLegal(enum MemIndexedMode Mode, Type *Ty) const;\n\n  /// \\returns True if the specified indexed store for the given type is legal.\n  bool isIndexedStoreLegal(enum MemIndexedMode Mode, Type *Ty) const;\n\n  /// \\returns The bitwidth of the largest vector type that should be used to\n  /// load/store in the given address space.\n  unsigned getLoadStoreVecRegBitWidth(unsigned AddrSpace) const;\n\n  /// \\returns True if the load instruction is legal to vectorize.\n  bool isLegalToVectorizeLoad(LoadInst *LI) const;\n\n  /// \\returns True if the store instruction is legal to vectorize.\n  bool isLegalToVectorizeStore(StoreInst *SI) const;\n\n  /// \\returns True if it is legal to vectorize the given load chain.\n  bool isLegalToVectorizeLoadChain(unsigned ChainSizeInBytes, Align Alignment,\n                                   unsigned AddrSpace) const;\n\n  /// \\returns True if it is legal to vectorize the given store chain.\n  bool isLegalToVectorizeStoreChain(unsigned ChainSizeInBytes, Align Alignment,\n                                    unsigned AddrSpace) const;\n\n  /// \\returns True if it is legal to vectorize the given reduction kind.\n  bool isLegalToVectorizeReduction(RecurrenceDescriptor RdxDesc,\n                                   ElementCount VF) const;\n\n  /// \\returns The new vector factor value if the target doesn't support \\p\n  /// SizeInBytes loads or has a better vector factor.\n  unsigned getLoadVectorFactor(unsigned VF, unsigned LoadSize,\n                               unsigned ChainSizeInBytes,\n                               VectorType *VecTy) const;\n\n  /// \\returns The new vector factor value if the target doesn't support \\p\n  /// SizeInBytes stores or has a better vector factor.\n  unsigned getStoreVectorFactor(unsigned VF, unsigned StoreSize,\n                                unsigned ChainSizeInBytes,\n                                VectorType *VecTy) const;\n\n  /// Flags describing the kind of vector reduction.\n  struct ReductionFlags {\n    ReductionFlags() : IsMaxOp(false), IsSigned(false), NoNaN(false) {}\n    bool IsMaxOp;  ///< If the op a min/max kind, true if it's a max operation.\n    bool IsSigned; ///< Whether the operation is a signed int reduction.\n    bool NoNaN;    ///< If op is an fp min/max, whether NaNs may be present.\n  };\n\n  /// \\returns True if the target prefers reductions in loop.\n  bool preferInLoopReduction(unsigned Opcode, Type *Ty,\n                             ReductionFlags Flags) const;\n\n  /// \\returns True if the target prefers reductions select kept in the loop\n  /// when tail folding. i.e.\n  /// loop:\n  ///   p = phi (0, s)\n  ///   a = add (p, x)\n  ///   s = select (mask, a, p)\n  /// vecreduce.add(s)\n  ///\n  /// As opposed to the normal scheme of p = phi (0, a) which allows the select\n  /// to be pulled out of the loop. If the select(.., add, ..) can be predicated\n  /// by the target, this can lead to cleaner code generation.\n  bool preferPredicatedReductionSelect(unsigned Opcode, Type *Ty,\n                                       ReductionFlags Flags) const;\n\n  /// \\returns True if the target wants to expand the given reduction intrinsic\n  /// into a shuffle sequence.\n  bool shouldExpandReduction(const IntrinsicInst *II) const;\n\n  /// \\returns the size cost of rematerializing a GlobalValue address relative\n  /// to a stack reload.\n  unsigned getGISelRematGlobalCost() const;\n\n  /// \\returns True if the target supports scalable vectors.\n  bool supportsScalableVectors() const;\n\n  /// \\name Vector Predication Information\n  /// @{\n  /// Whether the target supports the %evl parameter of VP intrinsic efficiently\n  /// in hardware. (see LLVM Language Reference - \"Vector Predication\n  /// Intrinsics\") Use of %evl is discouraged when that is not the case.\n  bool hasActiveVectorLength() const;\n\n  /// @}\n\n  /// @}\n\nprivate:\n  /// Estimate the latency of specified instruction.\n  /// Returns 1 as the default value.\n  int getInstructionLatency(const Instruction *I) const;\n\n  /// Returns the expected throughput cost of the instruction.\n  /// Returns -1 if the cost is unknown.\n  int getInstructionThroughput(const Instruction *I) const;\n\n  /// The abstract base class used to type erase specific TTI\n  /// implementations.\n  class Concept;\n\n  /// The template model for the base class which wraps a concrete\n  /// implementation in a type erased interface.\n  template <typename T> class Model;\n\n  std::unique_ptr<Concept> TTIImpl;\n};\n\nclass TargetTransformInfo::Concept {\npublic:\n  virtual ~Concept() = 0;\n  virtual const DataLayout &getDataLayout() const = 0;\n  virtual int getGEPCost(Type *PointeeType, const Value *Ptr,\n                         ArrayRef<const Value *> Operands,\n                         TTI::TargetCostKind CostKind) = 0;\n  virtual unsigned getInliningThresholdMultiplier() = 0;\n  virtual unsigned adjustInliningThreshold(const CallBase *CB) = 0;\n  virtual int getInlinerVectorBonusPercent() = 0;\n  virtual int getMemcpyCost(const Instruction *I) = 0;\n  virtual unsigned\n  getEstimatedNumberOfCaseClusters(const SwitchInst &SI, unsigned &JTSize,\n                                   ProfileSummaryInfo *PSI,\n                                   BlockFrequencyInfo *BFI) = 0;\n  virtual int getUserCost(const User *U, ArrayRef<const Value *> Operands,\n                          TargetCostKind CostKind) = 0;\n  virtual bool hasBranchDivergence() = 0;\n  virtual bool useGPUDivergenceAnalysis() = 0;\n  virtual bool isSourceOfDivergence(const Value *V) = 0;\n  virtual bool isAlwaysUniform(const Value *V) = 0;\n  virtual unsigned getFlatAddressSpace() = 0;\n  virtual bool collectFlatAddressOperands(SmallVectorImpl<int> &OpIndexes,\n                                          Intrinsic::ID IID) const = 0;\n  virtual bool isNoopAddrSpaceCast(unsigned FromAS, unsigned ToAS) const = 0;\n  virtual unsigned getAssumedAddrSpace(const Value *V) const = 0;\n  virtual Value *rewriteIntrinsicWithAddressSpace(IntrinsicInst *II,\n                                                  Value *OldV,\n                                                  Value *NewV) const = 0;\n  virtual bool isLoweredToCall(const Function *F) = 0;\n  virtual void getUnrollingPreferences(Loop *L, ScalarEvolution &,\n                                       UnrollingPreferences &UP) = 0;\n  virtual void getPeelingPreferences(Loop *L, ScalarEvolution &SE,\n                                     PeelingPreferences &PP) = 0;\n  virtual bool isHardwareLoopProfitable(Loop *L, ScalarEvolution &SE,\n                                        AssumptionCache &AC,\n                                        TargetLibraryInfo *LibInfo,\n                                        HardwareLoopInfo &HWLoopInfo) = 0;\n  virtual bool\n  preferPredicateOverEpilogue(Loop *L, LoopInfo *LI, ScalarEvolution &SE,\n                              AssumptionCache &AC, TargetLibraryInfo *TLI,\n                              DominatorTree *DT, const LoopAccessInfo *LAI) = 0;\n  virtual bool emitGetActiveLaneMask() = 0;\n  virtual Optional<Instruction *> instCombineIntrinsic(InstCombiner &IC,\n                                                       IntrinsicInst &II) = 0;\n  virtual Optional<Value *>\n  simplifyDemandedUseBitsIntrinsic(InstCombiner &IC, IntrinsicInst &II,\n                                   APInt DemandedMask, KnownBits &Known,\n                                   bool &KnownBitsComputed) = 0;\n  virtual Optional<Value *> simplifyDemandedVectorEltsIntrinsic(\n      InstCombiner &IC, IntrinsicInst &II, APInt DemandedElts, APInt &UndefElts,\n      APInt &UndefElts2, APInt &UndefElts3,\n      std::function<void(Instruction *, unsigned, APInt, APInt &)>\n          SimplifyAndSetOp) = 0;\n  virtual bool isLegalAddImmediate(int64_t Imm) = 0;\n  virtual bool isLegalICmpImmediate(int64_t Imm) = 0;\n  virtual bool isLegalAddressingMode(Type *Ty, GlobalValue *BaseGV,\n                                     int64_t BaseOffset, bool HasBaseReg,\n                                     int64_t Scale, unsigned AddrSpace,\n                                     Instruction *I) = 0;\n  virtual bool isLSRCostLess(TargetTransformInfo::LSRCost &C1,\n                             TargetTransformInfo::LSRCost &C2) = 0;\n  virtual bool isNumRegsMajorCostOfLSR() = 0;\n  virtual bool isProfitableLSRChainElement(Instruction *I) = 0;\n  virtual bool canMacroFuseCmp() = 0;\n  virtual bool canSaveCmp(Loop *L, BranchInst **BI, ScalarEvolution *SE,\n                          LoopInfo *LI, DominatorTree *DT, AssumptionCache *AC,\n                          TargetLibraryInfo *LibInfo) = 0;\n  virtual AddressingModeKind\n    getPreferredAddressingMode(const Loop *L, ScalarEvolution *SE) const = 0;\n  virtual bool isLegalMaskedStore(Type *DataType, Align Alignment) = 0;\n  virtual bool isLegalMaskedLoad(Type *DataType, Align Alignment) = 0;\n  virtual bool isLegalNTStore(Type *DataType, Align Alignment) = 0;\n  virtual bool isLegalNTLoad(Type *DataType, Align Alignment) = 0;\n  virtual bool isLegalMaskedScatter(Type *DataType, Align Alignment) = 0;\n  virtual bool isLegalMaskedGather(Type *DataType, Align Alignment) = 0;\n  virtual bool isLegalMaskedCompressStore(Type *DataType) = 0;\n  virtual bool isLegalMaskedExpandLoad(Type *DataType) = 0;\n  virtual bool hasDivRemOp(Type *DataType, bool IsSigned) = 0;\n  virtual bool hasVolatileVariant(Instruction *I, unsigned AddrSpace) = 0;\n  virtual bool prefersVectorizedAddressing() = 0;\n  virtual int getScalingFactorCost(Type *Ty, GlobalValue *BaseGV,\n                                   int64_t BaseOffset, bool HasBaseReg,\n                                   int64_t Scale, unsigned AddrSpace) = 0;\n  virtual bool LSRWithInstrQueries() = 0;\n  virtual bool isTruncateFree(Type *Ty1, Type *Ty2) = 0;\n  virtual bool isProfitableToHoist(Instruction *I) = 0;\n  virtual bool useAA() = 0;\n  virtual bool isTypeLegal(Type *Ty) = 0;\n  virtual unsigned getRegUsageForType(Type *Ty) = 0;\n  virtual bool shouldBuildLookupTables() = 0;\n  virtual bool shouldBuildLookupTablesForConstant(Constant *C) = 0;\n  virtual bool useColdCCForColdCall(Function &F) = 0;\n  virtual unsigned getScalarizationOverhead(VectorType *Ty,\n                                            const APInt &DemandedElts,\n                                            bool Insert, bool Extract) = 0;\n  virtual unsigned\n  getOperandsScalarizationOverhead(ArrayRef<const Value *> Args,\n                                   ArrayRef<Type *> Tys) = 0;\n  virtual bool supportsEfficientVectorElementLoadStore() = 0;\n  virtual bool enableAggressiveInterleaving(bool LoopHasReductions) = 0;\n  virtual MemCmpExpansionOptions\n  enableMemCmpExpansion(bool OptSize, bool IsZeroCmp) const = 0;\n  virtual bool enableInterleavedAccessVectorization() = 0;\n  virtual bool enableMaskedInterleavedAccessVectorization() = 0;\n  virtual bool isFPVectorizationPotentiallyUnsafe() = 0;\n  virtual bool allowsMisalignedMemoryAccesses(LLVMContext &Context,\n                                              unsigned BitWidth,\n                                              unsigned AddressSpace,\n                                              Align Alignment,\n                                              bool *Fast) = 0;\n  virtual PopcntSupportKind getPopcntSupport(unsigned IntTyWidthInBit) = 0;\n  virtual bool haveFastSqrt(Type *Ty) = 0;\n  virtual bool isFCmpOrdCheaperThanFCmpZero(Type *Ty) = 0;\n  virtual int getFPOpCost(Type *Ty) = 0;\n  virtual int getIntImmCodeSizeCost(unsigned Opc, unsigned Idx,\n                                    const APInt &Imm, Type *Ty) = 0;\n  virtual int getIntImmCost(const APInt &Imm, Type *Ty,\n                            TargetCostKind CostKind) = 0;\n  virtual int getIntImmCostInst(unsigned Opc, unsigned Idx, const APInt &Imm,\n                                Type *Ty, TargetCostKind CostKind,\n                                Instruction *Inst = nullptr) = 0;\n  virtual int getIntImmCostIntrin(Intrinsic::ID IID, unsigned Idx,\n                                  const APInt &Imm, Type *Ty,\n                                  TargetCostKind CostKind) = 0;\n  virtual unsigned getNumberOfRegisters(unsigned ClassID) const = 0;\n  virtual unsigned getRegisterClassForType(bool Vector,\n                                           Type *Ty = nullptr) const = 0;\n  virtual const char *getRegisterClassName(unsigned ClassID) const = 0;\n  virtual unsigned getRegisterBitWidth(bool Vector) const = 0;\n  virtual unsigned getMinVectorRegisterBitWidth() = 0;\n  virtual Optional<unsigned> getMaxVScale() const = 0;\n  virtual bool shouldMaximizeVectorBandwidth(bool OptSize) const = 0;\n  virtual ElementCount getMinimumVF(unsigned ElemWidth,\n                                    bool IsScalable) const = 0;\n  virtual unsigned getMaximumVF(unsigned ElemWidth, unsigned Opcode) const = 0;\n  virtual bool shouldConsiderAddressTypePromotion(\n      const Instruction &I, bool &AllowPromotionWithoutCommonHeader) = 0;\n  virtual unsigned getCacheLineSize() const = 0;\n  virtual Optional<unsigned> getCacheSize(CacheLevel Level) const = 0;\n  virtual Optional<unsigned> getCacheAssociativity(CacheLevel Level) const = 0;\n\n  /// \\return How much before a load we should place the prefetch\n  /// instruction.  This is currently measured in number of\n  /// instructions.\n  virtual unsigned getPrefetchDistance() const = 0;\n\n  /// \\return Some HW prefetchers can handle accesses up to a certain\n  /// constant stride.  This is the minimum stride in bytes where it\n  /// makes sense to start adding SW prefetches.  The default is 1,\n  /// i.e. prefetch with any stride.  Sometimes prefetching is beneficial\n  /// even below the HW prefetcher limit, and the arguments provided are\n  /// meant to serve as a basis for deciding this for a particular loop.\n  virtual unsigned getMinPrefetchStride(unsigned NumMemAccesses,\n                                        unsigned NumStridedMemAccesses,\n                                        unsigned NumPrefetches,\n                                        bool HasCall) const = 0;\n\n  /// \\return The maximum number of iterations to prefetch ahead.  If\n  /// the required number of iterations is more than this number, no\n  /// prefetching is performed.\n  virtual unsigned getMaxPrefetchIterationsAhead() const = 0;\n\n  /// \\return True if prefetching should also be done for writes.\n  virtual bool enableWritePrefetching() const = 0;\n\n  virtual unsigned getMaxInterleaveFactor(unsigned VF) = 0;\n  virtual unsigned getArithmeticInstrCost(\n      unsigned Opcode, Type *Ty,\n      TTI::TargetCostKind CostKind,\n      OperandValueKind Opd1Info,\n      OperandValueKind Opd2Info, OperandValueProperties Opd1PropInfo,\n      OperandValueProperties Opd2PropInfo, ArrayRef<const Value *> Args,\n      const Instruction *CxtI = nullptr) = 0;\n  virtual int getShuffleCost(ShuffleKind Kind, VectorType *Tp, int Index,\n                             VectorType *SubTp) = 0;\n  virtual int getCastInstrCost(unsigned Opcode, Type *Dst, Type *Src,\n                               CastContextHint CCH,\n                               TTI::TargetCostKind CostKind,\n                               const Instruction *I) = 0;\n  virtual int getExtractWithExtendCost(unsigned Opcode, Type *Dst,\n                                       VectorType *VecTy, unsigned Index) = 0;\n  virtual int getCFInstrCost(unsigned Opcode,\n                             TTI::TargetCostKind CostKind) = 0;\n  virtual int getCmpSelInstrCost(unsigned Opcode, Type *ValTy, Type *CondTy,\n                                 CmpInst::Predicate VecPred,\n                                 TTI::TargetCostKind CostKind,\n                                 const Instruction *I) = 0;\n  virtual int getVectorInstrCost(unsigned Opcode, Type *Val,\n                                 unsigned Index) = 0;\n  virtual int getMemoryOpCost(unsigned Opcode, Type *Src, Align Alignment,\n                              unsigned AddressSpace,\n                              TTI::TargetCostKind CostKind,\n                              const Instruction *I) = 0;\n  virtual int getMaskedMemoryOpCost(unsigned Opcode, Type *Src, Align Alignment,\n                                    unsigned AddressSpace,\n                                    TTI::TargetCostKind CostKind) = 0;\n  virtual int getGatherScatterOpCost(unsigned Opcode, Type *DataTy,\n                                     const Value *Ptr, bool VariableMask,\n                                     Align Alignment,\n                                     TTI::TargetCostKind CostKind,\n                                     const Instruction *I = nullptr) = 0;\n\n  virtual int getInterleavedMemoryOpCost(\n      unsigned Opcode, Type *VecTy, unsigned Factor, ArrayRef<unsigned> Indices,\n      Align Alignment, unsigned AddressSpace, TTI::TargetCostKind CostKind,\n      bool UseMaskForCond = false, bool UseMaskForGaps = false) = 0;\n  virtual int getArithmeticReductionCost(unsigned Opcode, VectorType *Ty,\n                                         bool IsPairwiseForm,\n                                         TTI::TargetCostKind CostKind) = 0;\n  virtual int getMinMaxReductionCost(VectorType *Ty, VectorType *CondTy,\n                                     bool IsPairwiseForm, bool IsUnsigned,\n                                     TTI::TargetCostKind CostKind) = 0;\n  virtual InstructionCost getExtendedAddReductionCost(\n      bool IsMLA, bool IsUnsigned, Type *ResTy, VectorType *Ty,\n      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput) = 0;\n  virtual int getIntrinsicInstrCost(const IntrinsicCostAttributes &ICA,\n                                    TTI::TargetCostKind CostKind) = 0;\n  virtual int getCallInstrCost(Function *F, Type *RetTy,\n                               ArrayRef<Type *> Tys,\n                               TTI::TargetCostKind CostKind) = 0;\n  virtual unsigned getNumberOfParts(Type *Tp) = 0;\n  virtual int getAddressComputationCost(Type *Ty, ScalarEvolution *SE,\n                                        const SCEV *Ptr) = 0;\n  virtual unsigned getCostOfKeepingLiveOverCall(ArrayRef<Type *> Tys) = 0;\n  virtual bool getTgtMemIntrinsic(IntrinsicInst *Inst,\n                                  MemIntrinsicInfo &Info) = 0;\n  virtual unsigned getAtomicMemIntrinsicMaxElementSize() const = 0;\n  virtual Value *getOrCreateResultFromMemIntrinsic(IntrinsicInst *Inst,\n                                                   Type *ExpectedType) = 0;\n  virtual Type *getMemcpyLoopLoweringType(LLVMContext &Context, Value *Length,\n                                          unsigned SrcAddrSpace,\n                                          unsigned DestAddrSpace,\n                                          unsigned SrcAlign,\n                                          unsigned DestAlign) const = 0;\n  virtual void getMemcpyLoopResidualLoweringType(\n      SmallVectorImpl<Type *> &OpsOut, LLVMContext &Context,\n      unsigned RemainingBytes, unsigned SrcAddrSpace, unsigned DestAddrSpace,\n      unsigned SrcAlign, unsigned DestAlign) const = 0;\n  virtual bool areInlineCompatible(const Function *Caller,\n                                   const Function *Callee) const = 0;\n  virtual bool\n  areFunctionArgsABICompatible(const Function *Caller, const Function *Callee,\n                               SmallPtrSetImpl<Argument *> &Args) const = 0;\n  virtual bool isIndexedLoadLegal(MemIndexedMode Mode, Type *Ty) const = 0;\n  virtual bool isIndexedStoreLegal(MemIndexedMode Mode, Type *Ty) const = 0;\n  virtual unsigned getLoadStoreVecRegBitWidth(unsigned AddrSpace) const = 0;\n  virtual bool isLegalToVectorizeLoad(LoadInst *LI) const = 0;\n  virtual bool isLegalToVectorizeStore(StoreInst *SI) const = 0;\n  virtual bool isLegalToVectorizeLoadChain(unsigned ChainSizeInBytes,\n                                           Align Alignment,\n                                           unsigned AddrSpace) const = 0;\n  virtual bool isLegalToVectorizeStoreChain(unsigned ChainSizeInBytes,\n                                            Align Alignment,\n                                            unsigned AddrSpace) const = 0;\n  virtual bool isLegalToVectorizeReduction(RecurrenceDescriptor RdxDesc,\n                                           ElementCount VF) const = 0;\n  virtual unsigned getLoadVectorFactor(unsigned VF, unsigned LoadSize,\n                                       unsigned ChainSizeInBytes,\n                                       VectorType *VecTy) const = 0;\n  virtual unsigned getStoreVectorFactor(unsigned VF, unsigned StoreSize,\n                                        unsigned ChainSizeInBytes,\n                                        VectorType *VecTy) const = 0;\n  virtual bool preferInLoopReduction(unsigned Opcode, Type *Ty,\n                                     ReductionFlags) const = 0;\n  virtual bool preferPredicatedReductionSelect(unsigned Opcode, Type *Ty,\n                                               ReductionFlags) const = 0;\n  virtual bool shouldExpandReduction(const IntrinsicInst *II) const = 0;\n  virtual unsigned getGISelRematGlobalCost() const = 0;\n  virtual bool supportsScalableVectors() const = 0;\n  virtual bool hasActiveVectorLength() const = 0;\n  virtual int getInstructionLatency(const Instruction *I) = 0;\n};\n\ntemplate <typename T>\nclass TargetTransformInfo::Model final : public TargetTransformInfo::Concept {\n  T Impl;\n\npublic:\n  Model(T Impl) : Impl(std::move(Impl)) {}\n  ~Model() override {}\n\n  const DataLayout &getDataLayout() const override {\n    return Impl.getDataLayout();\n  }\n\n  int getGEPCost(Type *PointeeType, const Value *Ptr,\n                 ArrayRef<const Value *> Operands,\n                 enum TargetTransformInfo::TargetCostKind CostKind) override {\n    return Impl.getGEPCost(PointeeType, Ptr, Operands);\n  }\n  unsigned getInliningThresholdMultiplier() override {\n    return Impl.getInliningThresholdMultiplier();\n  }\n  unsigned adjustInliningThreshold(const CallBase *CB) override {\n    return Impl.adjustInliningThreshold(CB);\n  }\n  int getInlinerVectorBonusPercent() override {\n    return Impl.getInlinerVectorBonusPercent();\n  }\n  int getMemcpyCost(const Instruction *I) override {\n    return Impl.getMemcpyCost(I);\n  }\n  int getUserCost(const User *U, ArrayRef<const Value *> Operands,\n                  TargetCostKind CostKind) override {\n    return Impl.getUserCost(U, Operands, CostKind);\n  }\n  bool hasBranchDivergence() override { return Impl.hasBranchDivergence(); }\n  bool useGPUDivergenceAnalysis() override {\n    return Impl.useGPUDivergenceAnalysis();\n  }\n  bool isSourceOfDivergence(const Value *V) override {\n    return Impl.isSourceOfDivergence(V);\n  }\n\n  bool isAlwaysUniform(const Value *V) override {\n    return Impl.isAlwaysUniform(V);\n  }\n\n  unsigned getFlatAddressSpace() override { return Impl.getFlatAddressSpace(); }\n\n  bool collectFlatAddressOperands(SmallVectorImpl<int> &OpIndexes,\n                                  Intrinsic::ID IID) const override {\n    return Impl.collectFlatAddressOperands(OpIndexes, IID);\n  }\n\n  bool isNoopAddrSpaceCast(unsigned FromAS, unsigned ToAS) const override {\n    return Impl.isNoopAddrSpaceCast(FromAS, ToAS);\n  }\n\n  unsigned getAssumedAddrSpace(const Value *V) const override {\n    return Impl.getAssumedAddrSpace(V);\n  }\n\n  Value *rewriteIntrinsicWithAddressSpace(IntrinsicInst *II, Value *OldV,\n                                          Value *NewV) const override {\n    return Impl.rewriteIntrinsicWithAddressSpace(II, OldV, NewV);\n  }\n\n  bool isLoweredToCall(const Function *F) override {\n    return Impl.isLoweredToCall(F);\n  }\n  void getUnrollingPreferences(Loop *L, ScalarEvolution &SE,\n                               UnrollingPreferences &UP) override {\n    return Impl.getUnrollingPreferences(L, SE, UP);\n  }\n  void getPeelingPreferences(Loop *L, ScalarEvolution &SE,\n                             PeelingPreferences &PP) override {\n    return Impl.getPeelingPreferences(L, SE, PP);\n  }\n  bool isHardwareLoopProfitable(Loop *L, ScalarEvolution &SE,\n                                AssumptionCache &AC, TargetLibraryInfo *LibInfo,\n                                HardwareLoopInfo &HWLoopInfo) override {\n    return Impl.isHardwareLoopProfitable(L, SE, AC, LibInfo, HWLoopInfo);\n  }\n  bool preferPredicateOverEpilogue(Loop *L, LoopInfo *LI, ScalarEvolution &SE,\n                                   AssumptionCache &AC, TargetLibraryInfo *TLI,\n                                   DominatorTree *DT,\n                                   const LoopAccessInfo *LAI) override {\n    return Impl.preferPredicateOverEpilogue(L, LI, SE, AC, TLI, DT, LAI);\n  }\n  bool emitGetActiveLaneMask() override {\n    return Impl.emitGetActiveLaneMask();\n  }\n  Optional<Instruction *> instCombineIntrinsic(InstCombiner &IC,\n                                               IntrinsicInst &II) override {\n    return Impl.instCombineIntrinsic(IC, II);\n  }\n  Optional<Value *>\n  simplifyDemandedUseBitsIntrinsic(InstCombiner &IC, IntrinsicInst &II,\n                                   APInt DemandedMask, KnownBits &Known,\n                                   bool &KnownBitsComputed) override {\n    return Impl.simplifyDemandedUseBitsIntrinsic(IC, II, DemandedMask, Known,\n                                                 KnownBitsComputed);\n  }\n  Optional<Value *> simplifyDemandedVectorEltsIntrinsic(\n      InstCombiner &IC, IntrinsicInst &II, APInt DemandedElts, APInt &UndefElts,\n      APInt &UndefElts2, APInt &UndefElts3,\n      std::function<void(Instruction *, unsigned, APInt, APInt &)>\n          SimplifyAndSetOp) override {\n    return Impl.simplifyDemandedVectorEltsIntrinsic(\n        IC, II, DemandedElts, UndefElts, UndefElts2, UndefElts3,\n        SimplifyAndSetOp);\n  }\n  bool isLegalAddImmediate(int64_t Imm) override {\n    return Impl.isLegalAddImmediate(Imm);\n  }\n  bool isLegalICmpImmediate(int64_t Imm) override {\n    return Impl.isLegalICmpImmediate(Imm);\n  }\n  bool isLegalAddressingMode(Type *Ty, GlobalValue *BaseGV, int64_t BaseOffset,\n                             bool HasBaseReg, int64_t Scale, unsigned AddrSpace,\n                             Instruction *I) override {\n    return Impl.isLegalAddressingMode(Ty, BaseGV, BaseOffset, HasBaseReg, Scale,\n                                      AddrSpace, I);\n  }\n  bool isLSRCostLess(TargetTransformInfo::LSRCost &C1,\n                     TargetTransformInfo::LSRCost &C2) override {\n    return Impl.isLSRCostLess(C1, C2);\n  }\n  bool isNumRegsMajorCostOfLSR() override {\n    return Impl.isNumRegsMajorCostOfLSR();\n  }\n  bool isProfitableLSRChainElement(Instruction *I) override {\n    return Impl.isProfitableLSRChainElement(I);\n  }\n  bool canMacroFuseCmp() override { return Impl.canMacroFuseCmp(); }\n  bool canSaveCmp(Loop *L, BranchInst **BI, ScalarEvolution *SE, LoopInfo *LI,\n                  DominatorTree *DT, AssumptionCache *AC,\n                  TargetLibraryInfo *LibInfo) override {\n    return Impl.canSaveCmp(L, BI, SE, LI, DT, AC, LibInfo);\n  }\n  AddressingModeKind\n    getPreferredAddressingMode(const Loop *L,\n                               ScalarEvolution *SE) const override {\n    return Impl.getPreferredAddressingMode(L, SE);\n  }\n  bool isLegalMaskedStore(Type *DataType, Align Alignment) override {\n    return Impl.isLegalMaskedStore(DataType, Alignment);\n  }\n  bool isLegalMaskedLoad(Type *DataType, Align Alignment) override {\n    return Impl.isLegalMaskedLoad(DataType, Alignment);\n  }\n  bool isLegalNTStore(Type *DataType, Align Alignment) override {\n    return Impl.isLegalNTStore(DataType, Alignment);\n  }\n  bool isLegalNTLoad(Type *DataType, Align Alignment) override {\n    return Impl.isLegalNTLoad(DataType, Alignment);\n  }\n  bool isLegalMaskedScatter(Type *DataType, Align Alignment) override {\n    return Impl.isLegalMaskedScatter(DataType, Alignment);\n  }\n  bool isLegalMaskedGather(Type *DataType, Align Alignment) override {\n    return Impl.isLegalMaskedGather(DataType, Alignment);\n  }\n  bool isLegalMaskedCompressStore(Type *DataType) override {\n    return Impl.isLegalMaskedCompressStore(DataType);\n  }\n  bool isLegalMaskedExpandLoad(Type *DataType) override {\n    return Impl.isLegalMaskedExpandLoad(DataType);\n  }\n  bool hasDivRemOp(Type *DataType, bool IsSigned) override {\n    return Impl.hasDivRemOp(DataType, IsSigned);\n  }\n  bool hasVolatileVariant(Instruction *I, unsigned AddrSpace) override {\n    return Impl.hasVolatileVariant(I, AddrSpace);\n  }\n  bool prefersVectorizedAddressing() override {\n    return Impl.prefersVectorizedAddressing();\n  }\n  int getScalingFactorCost(Type *Ty, GlobalValue *BaseGV, int64_t BaseOffset,\n                           bool HasBaseReg, int64_t Scale,\n                           unsigned AddrSpace) override {\n    return Impl.getScalingFactorCost(Ty, BaseGV, BaseOffset, HasBaseReg, Scale,\n                                     AddrSpace);\n  }\n  bool LSRWithInstrQueries() override { return Impl.LSRWithInstrQueries(); }\n  bool isTruncateFree(Type *Ty1, Type *Ty2) override {\n    return Impl.isTruncateFree(Ty1, Ty2);\n  }\n  bool isProfitableToHoist(Instruction *I) override {\n    return Impl.isProfitableToHoist(I);\n  }\n  bool useAA() override { return Impl.useAA(); }\n  bool isTypeLegal(Type *Ty) override { return Impl.isTypeLegal(Ty); }\n  unsigned getRegUsageForType(Type *Ty) override {\n    return Impl.getRegUsageForType(Ty);\n  }\n  bool shouldBuildLookupTables() override {\n    return Impl.shouldBuildLookupTables();\n  }\n  bool shouldBuildLookupTablesForConstant(Constant *C) override {\n    return Impl.shouldBuildLookupTablesForConstant(C);\n  }\n  bool useColdCCForColdCall(Function &F) override {\n    return Impl.useColdCCForColdCall(F);\n  }\n\n  unsigned getScalarizationOverhead(VectorType *Ty, const APInt &DemandedElts,\n                                    bool Insert, bool Extract) override {\n    return Impl.getScalarizationOverhead(Ty, DemandedElts, Insert, Extract);\n  }\n  unsigned getOperandsScalarizationOverhead(ArrayRef<const Value *> Args,\n                                            ArrayRef<Type *> Tys) override {\n    return Impl.getOperandsScalarizationOverhead(Args, Tys);\n  }\n\n  bool supportsEfficientVectorElementLoadStore() override {\n    return Impl.supportsEfficientVectorElementLoadStore();\n  }\n\n  bool enableAggressiveInterleaving(bool LoopHasReductions) override {\n    return Impl.enableAggressiveInterleaving(LoopHasReductions);\n  }\n  MemCmpExpansionOptions enableMemCmpExpansion(bool OptSize,\n                                               bool IsZeroCmp) const override {\n    return Impl.enableMemCmpExpansion(OptSize, IsZeroCmp);\n  }\n  bool enableInterleavedAccessVectorization() override {\n    return Impl.enableInterleavedAccessVectorization();\n  }\n  bool enableMaskedInterleavedAccessVectorization() override {\n    return Impl.enableMaskedInterleavedAccessVectorization();\n  }\n  bool isFPVectorizationPotentiallyUnsafe() override {\n    return Impl.isFPVectorizationPotentiallyUnsafe();\n  }\n  bool allowsMisalignedMemoryAccesses(LLVMContext &Context, unsigned BitWidth,\n                                      unsigned AddressSpace, Align Alignment,\n                                      bool *Fast) override {\n    return Impl.allowsMisalignedMemoryAccesses(Context, BitWidth, AddressSpace,\n                                               Alignment, Fast);\n  }\n  PopcntSupportKind getPopcntSupport(unsigned IntTyWidthInBit) override {\n    return Impl.getPopcntSupport(IntTyWidthInBit);\n  }\n  bool haveFastSqrt(Type *Ty) override { return Impl.haveFastSqrt(Ty); }\n\n  bool isFCmpOrdCheaperThanFCmpZero(Type *Ty) override {\n    return Impl.isFCmpOrdCheaperThanFCmpZero(Ty);\n  }\n\n  int getFPOpCost(Type *Ty) override { return Impl.getFPOpCost(Ty); }\n\n  int getIntImmCodeSizeCost(unsigned Opc, unsigned Idx, const APInt &Imm,\n                            Type *Ty) override {\n    return Impl.getIntImmCodeSizeCost(Opc, Idx, Imm, Ty);\n  }\n  int getIntImmCost(const APInt &Imm, Type *Ty,\n                    TargetCostKind CostKind) override {\n    return Impl.getIntImmCost(Imm, Ty, CostKind);\n  }\n  int getIntImmCostInst(unsigned Opc, unsigned Idx, const APInt &Imm, Type *Ty,\n                        TargetCostKind CostKind,\n                        Instruction *Inst = nullptr) override {\n    return Impl.getIntImmCostInst(Opc, Idx, Imm, Ty, CostKind, Inst);\n  }\n  int getIntImmCostIntrin(Intrinsic::ID IID, unsigned Idx, const APInt &Imm,\n                          Type *Ty, TargetCostKind CostKind) override {\n    return Impl.getIntImmCostIntrin(IID, Idx, Imm, Ty, CostKind);\n  }\n  unsigned getNumberOfRegisters(unsigned ClassID) const override {\n    return Impl.getNumberOfRegisters(ClassID);\n  }\n  unsigned getRegisterClassForType(bool Vector,\n                                   Type *Ty = nullptr) const override {\n    return Impl.getRegisterClassForType(Vector, Ty);\n  }\n  const char *getRegisterClassName(unsigned ClassID) const override {\n    return Impl.getRegisterClassName(ClassID);\n  }\n  unsigned getRegisterBitWidth(bool Vector) const override {\n    return Impl.getRegisterBitWidth(Vector);\n  }\n  unsigned getMinVectorRegisterBitWidth() override {\n    return Impl.getMinVectorRegisterBitWidth();\n  }\n  Optional<unsigned> getMaxVScale() const override {\n    return Impl.getMaxVScale();\n  }\n  bool shouldMaximizeVectorBandwidth(bool OptSize) const override {\n    return Impl.shouldMaximizeVectorBandwidth(OptSize);\n  }\n  ElementCount getMinimumVF(unsigned ElemWidth,\n                            bool IsScalable) const override {\n    return Impl.getMinimumVF(ElemWidth, IsScalable);\n  }\n  unsigned getMaximumVF(unsigned ElemWidth, unsigned Opcode) const override {\n    return Impl.getMaximumVF(ElemWidth, Opcode);\n  }\n  bool shouldConsiderAddressTypePromotion(\n      const Instruction &I, bool &AllowPromotionWithoutCommonHeader) override {\n    return Impl.shouldConsiderAddressTypePromotion(\n        I, AllowPromotionWithoutCommonHeader);\n  }\n  unsigned getCacheLineSize() const override { return Impl.getCacheLineSize(); }\n  Optional<unsigned> getCacheSize(CacheLevel Level) const override {\n    return Impl.getCacheSize(Level);\n  }\n  Optional<unsigned> getCacheAssociativity(CacheLevel Level) const override {\n    return Impl.getCacheAssociativity(Level);\n  }\n\n  /// Return the preferred prefetch distance in terms of instructions.\n  ///\n  unsigned getPrefetchDistance() const override {\n    return Impl.getPrefetchDistance();\n  }\n\n  /// Return the minimum stride necessary to trigger software\n  /// prefetching.\n  ///\n  unsigned getMinPrefetchStride(unsigned NumMemAccesses,\n                                unsigned NumStridedMemAccesses,\n                                unsigned NumPrefetches,\n                                bool HasCall) const override {\n    return Impl.getMinPrefetchStride(NumMemAccesses, NumStridedMemAccesses,\n                                     NumPrefetches, HasCall);\n  }\n\n  /// Return the maximum prefetch distance in terms of loop\n  /// iterations.\n  ///\n  unsigned getMaxPrefetchIterationsAhead() const override {\n    return Impl.getMaxPrefetchIterationsAhead();\n  }\n\n  /// \\return True if prefetching should also be done for writes.\n  bool enableWritePrefetching() const override {\n    return Impl.enableWritePrefetching();\n  }\n\n  unsigned getMaxInterleaveFactor(unsigned VF) override {\n    return Impl.getMaxInterleaveFactor(VF);\n  }\n  unsigned getEstimatedNumberOfCaseClusters(const SwitchInst &SI,\n                                            unsigned &JTSize,\n                                            ProfileSummaryInfo *PSI,\n                                            BlockFrequencyInfo *BFI) override {\n    return Impl.getEstimatedNumberOfCaseClusters(SI, JTSize, PSI, BFI);\n  }\n  unsigned getArithmeticInstrCost(unsigned Opcode, Type *Ty,\n                                  TTI::TargetCostKind CostKind,\n                                  OperandValueKind Opd1Info,\n                                  OperandValueKind Opd2Info,\n                                  OperandValueProperties Opd1PropInfo,\n                                  OperandValueProperties Opd2PropInfo,\n                                  ArrayRef<const Value *> Args,\n                                  const Instruction *CxtI = nullptr) override {\n    return Impl.getArithmeticInstrCost(Opcode, Ty, CostKind, Opd1Info, Opd2Info,\n                                       Opd1PropInfo, Opd2PropInfo, Args, CxtI);\n  }\n  int getShuffleCost(ShuffleKind Kind, VectorType *Tp, int Index,\n                     VectorType *SubTp) override {\n    return Impl.getShuffleCost(Kind, Tp, Index, SubTp);\n  }\n  int getCastInstrCost(unsigned Opcode, Type *Dst, Type *Src,\n                       CastContextHint CCH, TTI::TargetCostKind CostKind,\n                       const Instruction *I) override {\n    return Impl.getCastInstrCost(Opcode, Dst, Src, CCH, CostKind, I);\n  }\n  int getExtractWithExtendCost(unsigned Opcode, Type *Dst, VectorType *VecTy,\n                               unsigned Index) override {\n    return Impl.getExtractWithExtendCost(Opcode, Dst, VecTy, Index);\n  }\n  int getCFInstrCost(unsigned Opcode, TTI::TargetCostKind CostKind) override {\n    return Impl.getCFInstrCost(Opcode, CostKind);\n  }\n  int getCmpSelInstrCost(unsigned Opcode, Type *ValTy, Type *CondTy,\n                         CmpInst::Predicate VecPred,\n                         TTI::TargetCostKind CostKind,\n                         const Instruction *I) override {\n    return Impl.getCmpSelInstrCost(Opcode, ValTy, CondTy, VecPred, CostKind, I);\n  }\n  int getVectorInstrCost(unsigned Opcode, Type *Val, unsigned Index) override {\n    return Impl.getVectorInstrCost(Opcode, Val, Index);\n  }\n  int getMemoryOpCost(unsigned Opcode, Type *Src, Align Alignment,\n                      unsigned AddressSpace, TTI::TargetCostKind CostKind,\n                      const Instruction *I) override {\n    return Impl.getMemoryOpCost(Opcode, Src, Alignment, AddressSpace,\n                                CostKind, I);\n  }\n  int getMaskedMemoryOpCost(unsigned Opcode, Type *Src, Align Alignment,\n                            unsigned AddressSpace,\n                            TTI::TargetCostKind CostKind) override {\n    return Impl.getMaskedMemoryOpCost(Opcode, Src, Alignment, AddressSpace,\n                                      CostKind);\n  }\n  int getGatherScatterOpCost(unsigned Opcode, Type *DataTy, const Value *Ptr,\n                             bool VariableMask, Align Alignment,\n                             TTI::TargetCostKind CostKind,\n                             const Instruction *I = nullptr) override {\n    return Impl.getGatherScatterOpCost(Opcode, DataTy, Ptr, VariableMask,\n                                       Alignment, CostKind, I);\n  }\n  int getInterleavedMemoryOpCost(unsigned Opcode, Type *VecTy, unsigned Factor,\n                                 ArrayRef<unsigned> Indices, Align Alignment,\n                                 unsigned AddressSpace,\n                                 TTI::TargetCostKind CostKind,\n                                 bool UseMaskForCond,\n                                 bool UseMaskForGaps) override {\n    return Impl.getInterleavedMemoryOpCost(Opcode, VecTy, Factor, Indices,\n                                           Alignment, AddressSpace, CostKind,\n                                           UseMaskForCond, UseMaskForGaps);\n  }\n  int getArithmeticReductionCost(unsigned Opcode, VectorType *Ty,\n                                 bool IsPairwiseForm,\n                                 TTI::TargetCostKind CostKind) override {\n    return Impl.getArithmeticReductionCost(Opcode, Ty, IsPairwiseForm,\n                                           CostKind);\n  }\n  int getMinMaxReductionCost(VectorType *Ty, VectorType *CondTy,\n                             bool IsPairwiseForm, bool IsUnsigned,\n                             TTI::TargetCostKind CostKind) override {\n    return Impl.getMinMaxReductionCost(Ty, CondTy, IsPairwiseForm, IsUnsigned,\n                                       CostKind);\n  }\n  InstructionCost getExtendedAddReductionCost(\n      bool IsMLA, bool IsUnsigned, Type *ResTy, VectorType *Ty,\n      TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput) override {\n    return Impl.getExtendedAddReductionCost(IsMLA, IsUnsigned, ResTy, Ty,\n                                            CostKind);\n  }\n  int getIntrinsicInstrCost(const IntrinsicCostAttributes &ICA,\n                            TTI::TargetCostKind CostKind) override {\n    return Impl.getIntrinsicInstrCost(ICA, CostKind);\n  }\n  int getCallInstrCost(Function *F, Type *RetTy,\n                       ArrayRef<Type *> Tys,\n                       TTI::TargetCostKind CostKind) override {\n    return Impl.getCallInstrCost(F, RetTy, Tys, CostKind);\n  }\n  unsigned getNumberOfParts(Type *Tp) override {\n    return Impl.getNumberOfParts(Tp);\n  }\n  int getAddressComputationCost(Type *Ty, ScalarEvolution *SE,\n                                const SCEV *Ptr) override {\n    return Impl.getAddressComputationCost(Ty, SE, Ptr);\n  }\n  unsigned getCostOfKeepingLiveOverCall(ArrayRef<Type *> Tys) override {\n    return Impl.getCostOfKeepingLiveOverCall(Tys);\n  }\n  bool getTgtMemIntrinsic(IntrinsicInst *Inst,\n                          MemIntrinsicInfo &Info) override {\n    return Impl.getTgtMemIntrinsic(Inst, Info);\n  }\n  unsigned getAtomicMemIntrinsicMaxElementSize() const override {\n    return Impl.getAtomicMemIntrinsicMaxElementSize();\n  }\n  Value *getOrCreateResultFromMemIntrinsic(IntrinsicInst *Inst,\n                                           Type *ExpectedType) override {\n    return Impl.getOrCreateResultFromMemIntrinsic(Inst, ExpectedType);\n  }\n  Type *getMemcpyLoopLoweringType(LLVMContext &Context, Value *Length,\n                                  unsigned SrcAddrSpace, unsigned DestAddrSpace,\n                                  unsigned SrcAlign,\n                                  unsigned DestAlign) const override {\n    return Impl.getMemcpyLoopLoweringType(Context, Length, SrcAddrSpace,\n                                          DestAddrSpace, SrcAlign, DestAlign);\n  }\n  void getMemcpyLoopResidualLoweringType(\n      SmallVectorImpl<Type *> &OpsOut, LLVMContext &Context,\n      unsigned RemainingBytes, unsigned SrcAddrSpace, unsigned DestAddrSpace,\n      unsigned SrcAlign, unsigned DestAlign) const override {\n    Impl.getMemcpyLoopResidualLoweringType(OpsOut, Context, RemainingBytes,\n                                           SrcAddrSpace, DestAddrSpace,\n                                           SrcAlign, DestAlign);\n  }\n  bool areInlineCompatible(const Function *Caller,\n                           const Function *Callee) const override {\n    return Impl.areInlineCompatible(Caller, Callee);\n  }\n  bool areFunctionArgsABICompatible(\n      const Function *Caller, const Function *Callee,\n      SmallPtrSetImpl<Argument *> &Args) const override {\n    return Impl.areFunctionArgsABICompatible(Caller, Callee, Args);\n  }\n  bool isIndexedLoadLegal(MemIndexedMode Mode, Type *Ty) const override {\n    return Impl.isIndexedLoadLegal(Mode, Ty, getDataLayout());\n  }\n  bool isIndexedStoreLegal(MemIndexedMode Mode, Type *Ty) const override {\n    return Impl.isIndexedStoreLegal(Mode, Ty, getDataLayout());\n  }\n  unsigned getLoadStoreVecRegBitWidth(unsigned AddrSpace) const override {\n    return Impl.getLoadStoreVecRegBitWidth(AddrSpace);\n  }\n  bool isLegalToVectorizeLoad(LoadInst *LI) const override {\n    return Impl.isLegalToVectorizeLoad(LI);\n  }\n  bool isLegalToVectorizeStore(StoreInst *SI) const override {\n    return Impl.isLegalToVectorizeStore(SI);\n  }\n  bool isLegalToVectorizeLoadChain(unsigned ChainSizeInBytes, Align Alignment,\n                                   unsigned AddrSpace) const override {\n    return Impl.isLegalToVectorizeLoadChain(ChainSizeInBytes, Alignment,\n                                            AddrSpace);\n  }\n  bool isLegalToVectorizeStoreChain(unsigned ChainSizeInBytes, Align Alignment,\n                                    unsigned AddrSpace) const override {\n    return Impl.isLegalToVectorizeStoreChain(ChainSizeInBytes, Alignment,\n                                             AddrSpace);\n  }\n  bool isLegalToVectorizeReduction(RecurrenceDescriptor RdxDesc,\n                                   ElementCount VF) const override {\n    return Impl.isLegalToVectorizeReduction(RdxDesc, VF);\n  }\n  unsigned getLoadVectorFactor(unsigned VF, unsigned LoadSize,\n                               unsigned ChainSizeInBytes,\n                               VectorType *VecTy) const override {\n    return Impl.getLoadVectorFactor(VF, LoadSize, ChainSizeInBytes, VecTy);\n  }\n  unsigned getStoreVectorFactor(unsigned VF, unsigned StoreSize,\n                                unsigned ChainSizeInBytes,\n                                VectorType *VecTy) const override {\n    return Impl.getStoreVectorFactor(VF, StoreSize, ChainSizeInBytes, VecTy);\n  }\n  bool preferInLoopReduction(unsigned Opcode, Type *Ty,\n                             ReductionFlags Flags) const override {\n    return Impl.preferInLoopReduction(Opcode, Ty, Flags);\n  }\n  bool preferPredicatedReductionSelect(unsigned Opcode, Type *Ty,\n                                       ReductionFlags Flags) const override {\n    return Impl.preferPredicatedReductionSelect(Opcode, Ty, Flags);\n  }\n  bool shouldExpandReduction(const IntrinsicInst *II) const override {\n    return Impl.shouldExpandReduction(II);\n  }\n\n  unsigned getGISelRematGlobalCost() const override {\n    return Impl.getGISelRematGlobalCost();\n  }\n\n  bool supportsScalableVectors() const override {\n    return Impl.supportsScalableVectors();\n  }\n\n  bool hasActiveVectorLength() const override {\n    return Impl.hasActiveVectorLength();\n  }\n\n  int getInstructionLatency(const Instruction *I) override {\n    return Impl.getInstructionLatency(I);\n  }\n};\n\ntemplate <typename T>\nTargetTransformInfo::TargetTransformInfo(T Impl)\n    : TTIImpl(new Model<T>(Impl)) {}\n\n/// Analysis pass providing the \\c TargetTransformInfo.\n///\n/// The core idea of the TargetIRAnalysis is to expose an interface through\n/// which LLVM targets can analyze and provide information about the middle\n/// end's target-independent IR. This supports use cases such as target-aware\n/// cost modeling of IR constructs.\n///\n/// This is a function analysis because much of the cost modeling for targets\n/// is done in a subtarget specific way and LLVM supports compiling different\n/// functions targeting different subtargets in order to support runtime\n/// dispatch according to the observed subtarget.\nclass TargetIRAnalysis : public AnalysisInfoMixin<TargetIRAnalysis> {\npublic:\n  typedef TargetTransformInfo Result;\n\n  /// Default construct a target IR analysis.\n  ///\n  /// This will use the module's datalayout to construct a baseline\n  /// conservative TTI result.\n  TargetIRAnalysis();\n\n  /// Construct an IR analysis pass around a target-provide callback.\n  ///\n  /// The callback will be called with a particular function for which the TTI\n  /// is needed and must return a TTI object for that function.\n  TargetIRAnalysis(std::function<Result(const Function &)> TTICallback);\n\n  // Value semantics. We spell out the constructors for MSVC.\n  TargetIRAnalysis(const TargetIRAnalysis &Arg)\n      : TTICallback(Arg.TTICallback) {}\n  TargetIRAnalysis(TargetIRAnalysis &&Arg)\n      : TTICallback(std::move(Arg.TTICallback)) {}\n  TargetIRAnalysis &operator=(const TargetIRAnalysis &RHS) {\n    TTICallback = RHS.TTICallback;\n    return *this;\n  }\n  TargetIRAnalysis &operator=(TargetIRAnalysis &&RHS) {\n    TTICallback = std::move(RHS.TTICallback);\n    return *this;\n  }\n\n  Result run(const Function &F, FunctionAnalysisManager &);\n\nprivate:\n  friend AnalysisInfoMixin<TargetIRAnalysis>;\n  static AnalysisKey Key;\n\n  /// The callback used to produce a result.\n  ///\n  /// We use a completely opaque callback so that targets can provide whatever\n  /// mechanism they desire for constructing the TTI for a given function.\n  ///\n  /// FIXME: Should we really use std::function? It's relatively inefficient.\n  /// It might be possible to arrange for even stateful callbacks to outlive\n  /// the analysis and thus use a function_ref which would be lighter weight.\n  /// This may also be less error prone as the callback is likely to reference\n  /// the external TargetMachine, and that reference needs to never dangle.\n  std::function<Result(const Function &)> TTICallback;\n\n  /// Helper function used as the callback in the default constructor.\n  static Result getDefaultTTI(const Function &F);\n};\n\n/// Wrapper pass for TargetTransformInfo.\n///\n/// This pass can be constructed from a TTI object which it stores internally\n/// and is queried by passes.\nclass TargetTransformInfoWrapperPass : public ImmutablePass {\n  TargetIRAnalysis TIRA;\n  Optional<TargetTransformInfo> TTI;\n\n  virtual void anchor();\n\npublic:\n  static char ID;\n\n  /// We must provide a default constructor for the pass but it should\n  /// never be used.\n  ///\n  /// Use the constructor below or call one of the creation routines.\n  TargetTransformInfoWrapperPass();\n\n  explicit TargetTransformInfoWrapperPass(TargetIRAnalysis TIRA);\n\n  TargetTransformInfo &getTTI(const Function &F);\n};\n\n/// Create an analysis pass wrapper around a TTI object.\n///\n/// This analysis pass just holds the TTI instance and makes it available to\n/// clients.\nImmutablePass *createTargetTransformInfoWrapperPass(TargetIRAnalysis TIRA);\n\n} // namespace llvm\n\n#endif\n"}, "68": {"id": 68, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Transforms/Vectorize/LoopVectorize.h", "content": "//===- LoopVectorize.h ------------------------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This is the LLVM loop vectorizer. This pass modifies 'vectorizable' loops\n// and generates target-independent LLVM-IR.\n// The vectorizer uses the TargetTransformInfo analysis to estimate the costs\n// of instructions in order to estimate the profitability of vectorization.\n//\n// The loop vectorizer combines consecutive loop iterations into a single\n// 'wide' iteration. After this transformation the index is incremented\n// by the SIMD vector width, and not by one.\n//\n// This pass has three parts:\n// 1. The main loop pass that drives the different parts.\n// 2. LoopVectorizationLegality - A unit that checks for the legality\n//    of the vectorization.\n// 3. InnerLoopVectorizer - A unit that performs the actual\n//    widening of instructions.\n// 4. LoopVectorizationCostModel - A unit that checks for the profitability\n//    of vectorization. It decides on the optimal vector width, which\n//    can be one, if vectorization is not profitable.\n//\n// There is a development effort going on to migrate loop vectorizer to the\n// VPlan infrastructure and to introduce outer loop vectorization support (see\n// docs/Proposal/VectorizationPlan.rst and\n// http://lists.llvm.org/pipermail/llvm-dev/2017-December/119523.html). For this\n// purpose, we temporarily introduced the VPlan-native vectorization path: an\n// alternative vectorization path that is natively implemented on top of the\n// VPlan infrastructure. See EnableVPlanNativePath for enabling.\n//\n//===----------------------------------------------------------------------===//\n//\n// The reduction-variable vectorization is based on the paper:\n//  D. Nuzman and R. Henderson. Multi-platform Auto-vectorization.\n//\n// Variable uniformity checks are inspired by:\n//  Karrenberg, R. and Hack, S. Whole Function Vectorization.\n//\n// The interleaved access vectorization is based on the paper:\n//  Dorit Nuzman, Ira Rosen and Ayal Zaks.  Auto-Vectorization of Interleaved\n//  Data for SIMD\n//\n// Other ideas/concepts are from:\n//  A. Zaks and D. Nuzman. Autovectorization in GCC-two years later.\n//\n//  S. Maleki, Y. Gao, M. Garzaran, T. Wong and D. Padua.  An Evaluation of\n//  Vectorizing Compilers.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_TRANSFORMS_VECTORIZE_LOOPVECTORIZE_H\n#define LLVM_TRANSFORMS_VECTORIZE_LOOPVECTORIZE_H\n\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/Support/CommandLine.h\"\n#include <functional>\n\nnamespace llvm {\n\nclass AAResults;\nclass AssumptionCache;\nclass BlockFrequencyInfo;\nclass DemandedBits;\nclass DominatorTree;\nclass Function;\nclass Loop;\nclass LoopAccessInfo;\nclass LoopInfo;\nclass OptimizationRemarkEmitter;\nclass ProfileSummaryInfo;\nclass ScalarEvolution;\nclass TargetLibraryInfo;\nclass TargetTransformInfo;\n\nextern cl::opt<bool> EnableLoopInterleaving;\nextern cl::opt<bool> EnableLoopVectorization;\n\nstruct LoopVectorizeOptions {\n  /// If false, consider all loops for interleaving.\n  /// If true, only loops that explicitly request interleaving are considered.\n  bool InterleaveOnlyWhenForced;\n\n  /// If false, consider all loops for vectorization.\n  /// If true, only loops that explicitly request vectorization are considered.\n  bool VectorizeOnlyWhenForced;\n\n  /// The current defaults when creating the pass with no arguments are:\n  /// EnableLoopInterleaving = true and EnableLoopVectorization = true. This\n  /// means that interleaving default is consistent with the cl::opt flag, while\n  /// vectorization is not.\n  /// FIXME: The default for EnableLoopVectorization in the cl::opt should be\n  /// set to true, and the corresponding change to account for this be made in\n  /// opt.cpp. The initializations below will become:\n  /// InterleaveOnlyWhenForced(!EnableLoopInterleaving)\n  /// VectorizeOnlyWhenForced(!EnableLoopVectorization).\n  LoopVectorizeOptions()\n      : InterleaveOnlyWhenForced(false), VectorizeOnlyWhenForced(false) {}\n  LoopVectorizeOptions(bool InterleaveOnlyWhenForced,\n                       bool VectorizeOnlyWhenForced)\n      : InterleaveOnlyWhenForced(InterleaveOnlyWhenForced),\n        VectorizeOnlyWhenForced(VectorizeOnlyWhenForced) {}\n\n  LoopVectorizeOptions &setInterleaveOnlyWhenForced(bool Value) {\n    InterleaveOnlyWhenForced = Value;\n    return *this;\n  }\n\n  LoopVectorizeOptions &setVectorizeOnlyWhenForced(bool Value) {\n    VectorizeOnlyWhenForced = Value;\n    return *this;\n  }\n};\n\n/// Storage for information about made changes.\nstruct LoopVectorizeResult {\n  bool MadeAnyChange;\n  bool MadeCFGChange;\n\n  LoopVectorizeResult(bool MadeAnyChange, bool MadeCFGChange)\n      : MadeAnyChange(MadeAnyChange), MadeCFGChange(MadeCFGChange) {}\n};\n\n/// The LoopVectorize Pass.\nstruct LoopVectorizePass : public PassInfoMixin<LoopVectorizePass> {\nprivate:\n  /// If false, consider all loops for interleaving.\n  /// If true, only loops that explicitly request interleaving are considered.\n  bool InterleaveOnlyWhenForced;\n\n  /// If false, consider all loops for vectorization.\n  /// If true, only loops that explicitly request vectorization are considered.\n  bool VectorizeOnlyWhenForced;\n\npublic:\n  LoopVectorizePass(LoopVectorizeOptions Opts = {});\n\n  ScalarEvolution *SE;\n  LoopInfo *LI;\n  TargetTransformInfo *TTI;\n  DominatorTree *DT;\n  BlockFrequencyInfo *BFI;\n  TargetLibraryInfo *TLI;\n  DemandedBits *DB;\n  AAResults *AA;\n  AssumptionCache *AC;\n  std::function<const LoopAccessInfo &(Loop &)> *GetLAA;\n  OptimizationRemarkEmitter *ORE;\n  ProfileSummaryInfo *PSI;\n\n  PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);\n\n  // Shim for old PM.\n  LoopVectorizeResult\n  runImpl(Function &F, ScalarEvolution &SE_, LoopInfo &LI_,\n          TargetTransformInfo &TTI_, DominatorTree &DT_,\n          BlockFrequencyInfo &BFI_, TargetLibraryInfo *TLI_, DemandedBits &DB_,\n          AAResults &AA_, AssumptionCache &AC_,\n          std::function<const LoopAccessInfo &(Loop &)> &GetLAA_,\n          OptimizationRemarkEmitter &ORE_, ProfileSummaryInfo *PSI_);\n\n  bool processLoop(Loop *L);\n};\n\n/// Reports a vectorization failure: print \\p DebugMsg for debugging\n/// purposes along with the corresponding optimization remark \\p RemarkName.\n/// If \\p I is passed, it is an instruction that prevents vectorization.\n/// Otherwise, the loop \\p TheLoop is used for the location of the remark.\nvoid reportVectorizationFailure(const StringRef DebugMsg,\n    const StringRef OREMsg, const StringRef ORETag,\n    OptimizationRemarkEmitter *ORE, Loop *TheLoop, Instruction *I = nullptr);\n\n} // end namespace llvm\n\n#endif // LLVM_TRANSFORMS_VECTORIZE_LOOPVECTORIZE_H\n"}, "69": {"id": 69, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationPlanner.h", "content": "//===- LoopVectorizationPlanner.h - Planner for LoopVectorization ---------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n///\n/// \\file\n/// This file provides a LoopVectorizationPlanner class.\n/// InnerLoopVectorizer vectorizes loops which contain only one basic\n/// LoopVectorizationPlanner - drives the vectorization process after having\n/// passed Legality checks.\n/// The planner builds and optimizes the Vectorization Plans which record the\n/// decisions how to vectorize the given loop. In particular, represent the\n/// control-flow of the vectorized version, the replication of instructions that\n/// are to be scalarized, and interleave access groups.\n///\n/// Also provides a VPlan-based builder utility analogous to IRBuilder.\n/// It provides an instruction-level API for generating VPInstructions while\n/// abstracting away the Recipe manipulation details.\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_TRANSFORMS_VECTORIZE_LOOPVECTORIZATIONPLANNER_H\n#define LLVM_TRANSFORMS_VECTORIZE_LOOPVECTORIZATIONPLANNER_H\n\n#include \"VPlan.h\"\n#include \"llvm/Analysis/LoopInfo.h\"\n#include \"llvm/Analysis/TargetLibraryInfo.h\"\n#include \"llvm/Analysis/TargetTransformInfo.h\"\n\nnamespace llvm {\n\nclass LoopVectorizationLegality;\nclass LoopVectorizationCostModel;\nclass PredicatedScalarEvolution;\nclass VPRecipeBuilder;\n\n/// VPlan-based builder utility analogous to IRBuilder.\nclass VPBuilder {\n  VPBasicBlock *BB = nullptr;\n  VPBasicBlock::iterator InsertPt = VPBasicBlock::iterator();\n\n  VPInstruction *createInstruction(unsigned Opcode,\n                                   ArrayRef<VPValue *> Operands) {\n    VPInstruction *Instr = new VPInstruction(Opcode, Operands);\n    if (BB)\n      BB->insert(Instr, InsertPt);\n    return Instr;\n  }\n\n  VPInstruction *createInstruction(unsigned Opcode,\n                                   std::initializer_list<VPValue *> Operands) {\n    return createInstruction(Opcode, ArrayRef<VPValue *>(Operands));\n  }\n\npublic:\n  VPBuilder() {}\n\n  /// Clear the insertion point: created instructions will not be inserted into\n  /// a block.\n  void clearInsertionPoint() {\n    BB = nullptr;\n    InsertPt = VPBasicBlock::iterator();\n  }\n\n  VPBasicBlock *getInsertBlock() const { return BB; }\n  VPBasicBlock::iterator getInsertPoint() const { return InsertPt; }\n\n  /// InsertPoint - A saved insertion point.\n  class VPInsertPoint {\n    VPBasicBlock *Block = nullptr;\n    VPBasicBlock::iterator Point;\n\n  public:\n    /// Creates a new insertion point which doesn't point to anything.\n    VPInsertPoint() = default;\n\n    /// Creates a new insertion point at the given location.\n    VPInsertPoint(VPBasicBlock *InsertBlock, VPBasicBlock::iterator InsertPoint)\n        : Block(InsertBlock), Point(InsertPoint) {}\n\n    /// Returns true if this insert point is set.\n    bool isSet() const { return Block != nullptr; }\n\n    VPBasicBlock *getBlock() const { return Block; }\n    VPBasicBlock::iterator getPoint() const { return Point; }\n  };\n\n  /// Sets the current insert point to a previously-saved location.\n  void restoreIP(VPInsertPoint IP) {\n    if (IP.isSet())\n      setInsertPoint(IP.getBlock(), IP.getPoint());\n    else\n      clearInsertionPoint();\n  }\n\n  /// This specifies that created VPInstructions should be appended to the end\n  /// of the specified block.\n  void setInsertPoint(VPBasicBlock *TheBB) {\n    assert(TheBB && \"Attempting to set a null insert point\");\n    BB = TheBB;\n    InsertPt = BB->end();\n  }\n\n  /// This specifies that created instructions should be inserted at the\n  /// specified point.\n  void setInsertPoint(VPBasicBlock *TheBB, VPBasicBlock::iterator IP) {\n    BB = TheBB;\n    InsertPt = IP;\n  }\n\n  /// Insert and return the specified instruction.\n  VPInstruction *insert(VPInstruction *I) const {\n    BB->insert(I, InsertPt);\n    return I;\n  }\n\n  /// Create an N-ary operation with \\p Opcode, \\p Operands and set \\p Inst as\n  /// its underlying Instruction.\n  VPValue *createNaryOp(unsigned Opcode, ArrayRef<VPValue *> Operands,\n                        Instruction *Inst = nullptr) {\n    VPInstruction *NewVPInst = createInstruction(Opcode, Operands);\n    NewVPInst->setUnderlyingValue(Inst);\n    return NewVPInst;\n  }\n  VPValue *createNaryOp(unsigned Opcode,\n                        std::initializer_list<VPValue *> Operands,\n                        Instruction *Inst = nullptr) {\n    return createNaryOp(Opcode, ArrayRef<VPValue *>(Operands), Inst);\n  }\n\n  VPValue *createNot(VPValue *Operand) {\n    return createInstruction(VPInstruction::Not, {Operand});\n  }\n\n  VPValue *createAnd(VPValue *LHS, VPValue *RHS) {\n    return createInstruction(Instruction::BinaryOps::And, {LHS, RHS});\n  }\n\n  VPValue *createOr(VPValue *LHS, VPValue *RHS) {\n    return createInstruction(Instruction::BinaryOps::Or, {LHS, RHS});\n  }\n\n  VPValue *createSelect(VPValue *Cond, VPValue *TrueVal, VPValue *FalseVal) {\n    return createNaryOp(Instruction::Select, {Cond, TrueVal, FalseVal});\n  }\n\n  //===--------------------------------------------------------------------===//\n  // RAII helpers.\n  //===--------------------------------------------------------------------===//\n\n  /// RAII object that stores the current insertion point and restores it when\n  /// the object is destroyed.\n  class InsertPointGuard {\n    VPBuilder &Builder;\n    VPBasicBlock *Block;\n    VPBasicBlock::iterator Point;\n\n  public:\n    InsertPointGuard(VPBuilder &B)\n        : Builder(B), Block(B.getInsertBlock()), Point(B.getInsertPoint()) {}\n\n    InsertPointGuard(const InsertPointGuard &) = delete;\n    InsertPointGuard &operator=(const InsertPointGuard &) = delete;\n\n    ~InsertPointGuard() { Builder.restoreIP(VPInsertPoint(Block, Point)); }\n  };\n};\n\n/// TODO: The following VectorizationFactor was pulled out of\n/// LoopVectorizationCostModel class. LV also deals with\n/// VectorizerParams::VectorizationFactor and VectorizationCostTy.\n/// We need to streamline them.\n\n/// Information about vectorization costs\nstruct VectorizationFactor {\n  // Vector width with best cost\n  ElementCount Width;\n  // Cost of the loop with that width\n  unsigned Cost;\n\n  // Width 1 means no vectorization, cost 0 means uncomputed cost.\n  static VectorizationFactor Disabled() {\n    return {ElementCount::getFixed(1), 0};\n  }\n\n  bool operator==(const VectorizationFactor &rhs) const {\n    return Width == rhs.Width && Cost == rhs.Cost;\n  }\n\n  bool operator!=(const VectorizationFactor &rhs) const {\n    return !(*this == rhs);\n  }\n};\n\n/// Planner drives the vectorization process after having passed\n/// Legality checks.\nclass LoopVectorizationPlanner {\n  /// The loop that we evaluate.\n  Loop *OrigLoop;\n\n  /// Loop Info analysis.\n  LoopInfo *LI;\n\n  /// Target Library Info.\n  const TargetLibraryInfo *TLI;\n\n  /// Target Transform Info.\n  const TargetTransformInfo *TTI;\n\n  /// The legality analysis.\n  LoopVectorizationLegality *Legal;\n\n  /// The profitability analysis.\n  LoopVectorizationCostModel &CM;\n\n  /// The interleaved access analysis.\n  InterleavedAccessInfo &IAI;\n\n  PredicatedScalarEvolution &PSE;\n\n  SmallVector<VPlanPtr, 4> VPlans;\n\n  /// A builder used to construct the current plan.\n  VPBuilder Builder;\n\n  /// The best number of elements of the vector types used in the\n  /// transformed loop. BestVF = None means that vectorization is\n  /// disabled.\n  Optional<ElementCount> BestVF = None;\n  unsigned BestUF = 0;\n\npublic:\n  LoopVectorizationPlanner(Loop *L, LoopInfo *LI, const TargetLibraryInfo *TLI,\n                           const TargetTransformInfo *TTI,\n                           LoopVectorizationLegality *Legal,\n                           LoopVectorizationCostModel &CM,\n                           InterleavedAccessInfo &IAI,\n                           PredicatedScalarEvolution &PSE)\n      : OrigLoop(L), LI(LI), TLI(TLI), TTI(TTI), Legal(Legal), CM(CM), IAI(IAI),\n        PSE(PSE) {}\n\n  /// Plan how to best vectorize, return the best VF and its cost, or None if\n  /// vectorization and interleaving should be avoided up front.\n  Optional<VectorizationFactor> plan(ElementCount UserVF, unsigned UserIC);\n\n  /// Use the VPlan-native path to plan how to best vectorize, return the best\n  /// VF and its cost.\n  VectorizationFactor planInVPlanNativePath(ElementCount UserVF);\n\n  /// Finalize the best decision and dispose of all other VPlans.\n  void setBestPlan(ElementCount VF, unsigned UF);\n\n  /// Generate the IR code for the body of the vectorized loop according to the\n  /// best selected VPlan.\n  void executePlan(InnerLoopVectorizer &LB, DominatorTree *DT);\n\n  void printPlans(raw_ostream &O) {\n    for (const auto &Plan : VPlans)\n      O << *Plan;\n  }\n\n  /// Look through the existing plans and return true if we have one with all\n  /// the vectorization factors in question.\n  bool hasPlanWithVFs(const ArrayRef<ElementCount> VFs) const {\n    return any_of(VPlans, [&](const VPlanPtr &Plan) {\n      return all_of(VFs, [&](const ElementCount &VF) {\n        return Plan->hasVF(VF);\n      });\n    });\n  }\n\n  /// Test a \\p Predicate on a \\p Range of VF's. Return the value of applying\n  /// \\p Predicate on Range.Start, possibly decreasing Range.End such that the\n  /// returned value holds for the entire \\p Range.\n  static bool\n  getDecisionAndClampRange(const std::function<bool(ElementCount)> &Predicate,\n                           VFRange &Range);\n\nprotected:\n  /// Collect the instructions from the original loop that would be trivially\n  /// dead in the vectorized loop if generated.\n  void collectTriviallyDeadInstructions(\n      SmallPtrSetImpl<Instruction *> &DeadInstructions);\n\n  /// Build VPlans for power-of-2 VF's between \\p MinVF and \\p MaxVF inclusive,\n  /// according to the information gathered by Legal when it checked if it is\n  /// legal to vectorize the loop.\n  void buildVPlans(ElementCount MinVF, ElementCount MaxVF);\n\nprivate:\n  /// Build a VPlan according to the information gathered by Legal. \\return a\n  /// VPlan for vectorization factors \\p Range.Start and up to \\p Range.End\n  /// exclusive, possibly decreasing \\p Range.End.\n  VPlanPtr buildVPlan(VFRange &Range);\n\n  /// Build a VPlan using VPRecipes according to the information gather by\n  /// Legal. This method is only used for the legacy inner loop vectorizer.\n  VPlanPtr buildVPlanWithVPRecipes(\n      VFRange &Range, SmallPtrSetImpl<Instruction *> &DeadInstructions,\n      const DenseMap<Instruction *, Instruction *> &SinkAfter);\n\n  /// Build VPlans for power-of-2 VF's between \\p MinVF and \\p MaxVF inclusive,\n  /// according to the information gathered by Legal when it checked if it is\n  /// legal to vectorize the loop. This method creates VPlans using VPRecipes.\n  void buildVPlansWithVPRecipes(ElementCount MinVF, ElementCount MaxVF);\n\n  /// Adjust the recipes for any inloop reductions. The chain of instructions\n  /// leading from the loop exit instr to the phi need to be converted to\n  /// reductions, with one operand being vector and the other being the scalar\n  /// reduction chain.\n  void adjustRecipesForInLoopReductions(VPlanPtr &Plan,\n                                        VPRecipeBuilder &RecipeBuilder);\n};\n\n} // namespace llvm\n\n#endif // LLVM_TRANSFORMS_VECTORIZE_LOOPVECTORIZATIONPLANNER_H\n"}, "70": {"id": 70, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "content": "//===- LoopVectorize.cpp - A Loop Vectorizer ------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This is the LLVM loop vectorizer. This pass modifies 'vectorizable' loops\n// and generates target-independent LLVM-IR.\n// The vectorizer uses the TargetTransformInfo analysis to estimate the costs\n// of instructions in order to estimate the profitability of vectorization.\n//\n// The loop vectorizer combines consecutive loop iterations into a single\n// 'wide' iteration. After this transformation the index is incremented\n// by the SIMD vector width, and not by one.\n//\n// This pass has three parts:\n// 1. The main loop pass that drives the different parts.\n// 2. LoopVectorizationLegality - A unit that checks for the legality\n//    of the vectorization.\n// 3. InnerLoopVectorizer - A unit that performs the actual\n//    widening of instructions.\n// 4. LoopVectorizationCostModel - A unit that checks for the profitability\n//    of vectorization. It decides on the optimal vector width, which\n//    can be one, if vectorization is not profitable.\n//\n// There is a development effort going on to migrate loop vectorizer to the\n// VPlan infrastructure and to introduce outer loop vectorization support (see\n// docs/Proposal/VectorizationPlan.rst and\n// http://lists.llvm.org/pipermail/llvm-dev/2017-December/119523.html). For this\n// purpose, we temporarily introduced the VPlan-native vectorization path: an\n// alternative vectorization path that is natively implemented on top of the\n// VPlan infrastructure. See EnableVPlanNativePath for enabling.\n//\n//===----------------------------------------------------------------------===//\n//\n// The reduction-variable vectorization is based on the paper:\n//  D. Nuzman and R. Henderson. Multi-platform Auto-vectorization.\n//\n// Variable uniformity checks are inspired by:\n//  Karrenberg, R. and Hack, S. Whole Function Vectorization.\n//\n// The interleaved access vectorization is based on the paper:\n//  Dorit Nuzman, Ira Rosen and Ayal Zaks.  Auto-Vectorization of Interleaved\n//  Data for SIMD\n//\n// Other ideas/concepts are from:\n//  A. Zaks and D. Nuzman. Autovectorization in GCC-two years later.\n//\n//  S. Maleki, Y. Gao, M. Garzaran, T. Wong and D. Padua.  An Evaluation of\n//  Vectorizing Compilers.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"llvm/Transforms/Vectorize/LoopVectorize.h\"\n#include \"LoopVectorizationPlanner.h\"\n#include \"VPRecipeBuilder.h\"\n#include \"VPlan.h\"\n#include \"VPlanHCFGBuilder.h\"\n#include \"VPlanPredicator.h\"\n#include \"VPlanTransforms.h\"\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DenseMapInfo.h\"\n#include \"llvm/ADT/Hashing.h\"\n#include \"llvm/ADT/MapVector.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/Statistic.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Analysis/AssumptionCache.h\"\n#include \"llvm/Analysis/BasicAliasAnalysis.h\"\n#include \"llvm/Analysis/BlockFrequencyInfo.h\"\n#include \"llvm/Analysis/CFG.h\"\n#include \"llvm/Analysis/CodeMetrics.h\"\n#include \"llvm/Analysis/DemandedBits.h\"\n#include \"llvm/Analysis/GlobalsModRef.h\"\n#include \"llvm/Analysis/LoopAccessAnalysis.h\"\n#include \"llvm/Analysis/LoopAnalysisManager.h\"\n#include \"llvm/Analysis/LoopInfo.h\"\n#include \"llvm/Analysis/LoopIterator.h\"\n#include \"llvm/Analysis/MemorySSA.h\"\n#include \"llvm/Analysis/OptimizationRemarkEmitter.h\"\n#include \"llvm/Analysis/ProfileSummaryInfo.h\"\n#include \"llvm/Analysis/ScalarEvolution.h\"\n#include \"llvm/Analysis/ScalarEvolutionExpressions.h\"\n#include \"llvm/Analysis/TargetLibraryInfo.h\"\n#include \"llvm/Analysis/TargetTransformInfo.h\"\n#include \"llvm/Analysis/VectorUtils.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/BasicBlock.h\"\n#include \"llvm/IR/CFG.h\"\n#include \"llvm/IR/Constant.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/DebugInfoMetadata.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/DiagnosticInfo.h\"\n#include \"llvm/IR/Dominators.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/IntrinsicInst.h\"\n#include \"llvm/IR/Intrinsics.h\"\n#include \"llvm/IR/LLVMContext.h\"\n#include \"llvm/IR/Metadata.h\"\n#include \"llvm/IR/Module.h\"\n#include \"llvm/IR/Operator.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/IR/Use.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/IR/ValueHandle.h\"\n#include \"llvm/IR/Verifier.h\"\n#include \"llvm/InitializePasses.h\"\n#include \"llvm/Pass.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/CommandLine.h\"\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/InstructionCost.h\"\n#include \"llvm/Support/MathExtras.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include \"llvm/Transforms/Utils/BasicBlockUtils.h\"\n#include \"llvm/Transforms/Utils/InjectTLIMappings.h\"\n#include \"llvm/Transforms/Utils/LoopSimplify.h\"\n#include \"llvm/Transforms/Utils/LoopUtils.h\"\n#include \"llvm/Transforms/Utils/LoopVersioning.h\"\n#include \"llvm/Transforms/Utils/ScalarEvolutionExpander.h\"\n#include \"llvm/Transforms/Utils/SizeOpts.h\"\n#include \"llvm/Transforms/Vectorize/LoopVectorizationLegality.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstdint>\n#include <cstdlib>\n#include <functional>\n#include <iterator>\n#include <limits>\n#include <memory>\n#include <string>\n#include <tuple>\n#include <utility>\n\nusing namespace llvm;\n\n#define LV_NAME \"loop-vectorize\"\n#define DEBUG_TYPE LV_NAME\n\n#ifndef NDEBUG\nconst char VerboseDebug[] = DEBUG_TYPE \"-verbose\";\n#endif\n\n/// @{\n/// Metadata attribute names\nconst char LLVMLoopVectorizeFollowupAll[] = \"llvm.loop.vectorize.followup_all\";\nconst char LLVMLoopVectorizeFollowupVectorized[] =\n    \"llvm.loop.vectorize.followup_vectorized\";\nconst char LLVMLoopVectorizeFollowupEpilogue[] =\n    \"llvm.loop.vectorize.followup_epilogue\";\n/// @}\n\nSTATISTIC(LoopsVectorized, \"Number of loops vectorized\");\nSTATISTIC(LoopsAnalyzed, \"Number of loops analyzed for vectorization\");\nSTATISTIC(LoopsEpilogueVectorized, \"Number of epilogues vectorized\");\n\nstatic cl::opt<bool> EnableEpilogueVectorization(\n    \"enable-epilogue-vectorization\", cl::init(true), cl::Hidden,\n    cl::desc(\"Enable vectorization of epilogue loops.\"));\n\nstatic cl::opt<unsigned> EpilogueVectorizationForceVF(\n    \"epilogue-vectorization-force-VF\", cl::init(1), cl::Hidden,\n    cl::desc(\"When epilogue vectorization is enabled, and a value greater than \"\n             \"1 is specified, forces the given VF for all applicable epilogue \"\n             \"loops.\"));\n\nstatic cl::opt<unsigned> EpilogueVectorizationMinVF(\n    \"epilogue-vectorization-minimum-VF\", cl::init(16), cl::Hidden,\n    cl::desc(\"Only loops with vectorization factor equal to or larger than \"\n             \"the specified value are considered for epilogue vectorization.\"));\n\n/// Loops with a known constant trip count below this number are vectorized only\n/// if no scalar iteration overheads are incurred.\nstatic cl::opt<unsigned> TinyTripCountVectorThreshold(\n    \"vectorizer-min-trip-count\", cl::init(16), cl::Hidden,\n    cl::desc(\"Loops with a constant trip count that is smaller than this \"\n             \"value are vectorized only if no scalar iteration overheads \"\n             \"are incurred.\"));\n\n// Option prefer-predicate-over-epilogue indicates that an epilogue is undesired,\n// that predication is preferred, and this lists all options. I.e., the\n// vectorizer will try to fold the tail-loop (epilogue) into the vector body\n// and predicate the instructions accordingly. If tail-folding fails, there are\n// different fallback strategies depending on these values:\nnamespace PreferPredicateTy {\n  enum Option {\n    ScalarEpilogue = 0,\n    PredicateElseScalarEpilogue,\n    PredicateOrDontVectorize\n  };\n} // namespace PreferPredicateTy\n\nstatic cl::opt<PreferPredicateTy::Option> PreferPredicateOverEpilogue(\n    \"prefer-predicate-over-epilogue\",\n    cl::init(PreferPredicateTy::ScalarEpilogue),\n    cl::Hidden,\n    cl::desc(\"Tail-folding and predication preferences over creating a scalar \"\n             \"epilogue loop.\"),\n    cl::values(clEnumValN(PreferPredicateTy::ScalarEpilogue,\n                         \"scalar-epilogue\",\n                         \"Don't tail-predicate loops, create scalar epilogue\"),\n              clEnumValN(PreferPredicateTy::PredicateElseScalarEpilogue,\n                         \"predicate-else-scalar-epilogue\",\n                         \"prefer tail-folding, create scalar epilogue if tail \"\n                         \"folding fails.\"),\n              clEnumValN(PreferPredicateTy::PredicateOrDontVectorize,\n                         \"predicate-dont-vectorize\",\n                         \"prefers tail-folding, don't attempt vectorization if \"\n                         \"tail-folding fails.\")));\n\nstatic cl::opt<bool> MaximizeBandwidth(\n    \"vectorizer-maximize-bandwidth\", cl::init(false), cl::Hidden,\n    cl::desc(\"Maximize bandwidth when selecting vectorization factor which \"\n             \"will be determined by the smallest type in loop.\"));\n\nstatic cl::opt<bool> EnableInterleavedMemAccesses(\n    \"enable-interleaved-mem-accesses\", cl::init(false), cl::Hidden,\n    cl::desc(\"Enable vectorization on interleaved memory accesses in a loop\"));\n\n/// An interleave-group may need masking if it resides in a block that needs\n/// predication, or in order to mask away gaps.\nstatic cl::opt<bool> EnableMaskedInterleavedMemAccesses(\n    \"enable-masked-interleaved-mem-accesses\", cl::init(false), cl::Hidden,\n    cl::desc(\"Enable vectorization on masked interleaved memory accesses in a loop\"));\n\nstatic cl::opt<unsigned> TinyTripCountInterleaveThreshold(\n    \"tiny-trip-count-interleave-threshold\", cl::init(128), cl::Hidden,\n    cl::desc(\"We don't interleave loops with a estimated constant trip count \"\n             \"below this number\"));\n\nstatic cl::opt<unsigned> ForceTargetNumScalarRegs(\n    \"force-target-num-scalar-regs\", cl::init(0), cl::Hidden,\n    cl::desc(\"A flag that overrides the target's number of scalar registers.\"));\n\nstatic cl::opt<unsigned> ForceTargetNumVectorRegs(\n    \"force-target-num-vector-regs\", cl::init(0), cl::Hidden,\n    cl::desc(\"A flag that overrides the target's number of vector registers.\"));\n\nstatic cl::opt<unsigned> ForceTargetMaxScalarInterleaveFactor(\n    \"force-target-max-scalar-interleave\", cl::init(0), cl::Hidden,\n    cl::desc(\"A flag that overrides the target's max interleave factor for \"\n             \"scalar loops.\"));\n\nstatic cl::opt<unsigned> ForceTargetMaxVectorInterleaveFactor(\n    \"force-target-max-vector-interleave\", cl::init(0), cl::Hidden,\n    cl::desc(\"A flag that overrides the target's max interleave factor for \"\n             \"vectorized loops.\"));\n\nstatic cl::opt<unsigned> ForceTargetInstructionCost(\n    \"force-target-instruction-cost\", cl::init(0), cl::Hidden,\n    cl::desc(\"A flag that overrides the target's expected cost for \"\n             \"an instruction to a single constant value. Mostly \"\n             \"useful for getting consistent testing.\"));\n\nstatic cl::opt<bool> ForceTargetSupportsScalableVectors(\n    \"force-target-supports-scalable-vectors\", cl::init(false), cl::Hidden,\n    cl::desc(\n        \"Pretend that scalable vectors are supported, even if the target does \"\n        \"not support them. This flag should only be used for testing.\"));\n\nstatic cl::opt<unsigned> SmallLoopCost(\n    \"small-loop-cost\", cl::init(20), cl::Hidden,\n    cl::desc(\n        \"The cost of a loop that is considered 'small' by the interleaver.\"));\n\nstatic cl::opt<bool> LoopVectorizeWithBlockFrequency(\n    \"loop-vectorize-with-block-frequency\", cl::init(true), cl::Hidden,\n    cl::desc(\"Enable the use of the block frequency analysis to access PGO \"\n             \"heuristics minimizing code growth in cold regions and being more \"\n             \"aggressive in hot regions.\"));\n\n// Runtime interleave loops for load/store throughput.\nstatic cl::opt<bool> EnableLoadStoreRuntimeInterleave(\n    \"enable-loadstore-runtime-interleave\", cl::init(true), cl::Hidden,\n    cl::desc(\n        \"Enable runtime interleaving until load/store ports are saturated\"));\n\n/// Interleave small loops with scalar reductions.\nstatic cl::opt<bool> InterleaveSmallLoopScalarReduction(\n    \"interleave-small-loop-scalar-reduction\", cl::init(false), cl::Hidden,\n    cl::desc(\"Enable interleaving for loops with small iteration counts that \"\n             \"contain scalar reductions to expose ILP.\"));\n\n/// The number of stores in a loop that are allowed to need predication.\nstatic cl::opt<unsigned> NumberOfStoresToPredicate(\n    \"vectorize-num-stores-pred\", cl::init(1), cl::Hidden,\n    cl::desc(\"Max number of stores to be predicated behind an if.\"));\n\nstatic cl::opt<bool> EnableIndVarRegisterHeur(\n    \"enable-ind-var-reg-heur\", cl::init(true), cl::Hidden,\n    cl::desc(\"Count the induction variable only once when interleaving\"));\n\nstatic cl::opt<bool> EnableCondStoresVectorization(\n    \"enable-cond-stores-vec\", cl::init(true), cl::Hidden,\n    cl::desc(\"Enable if predication of stores during vectorization.\"));\n\nstatic cl::opt<unsigned> MaxNestedScalarReductionIC(\n    \"max-nested-scalar-reduction-interleave\", cl::init(2), cl::Hidden,\n    cl::desc(\"The maximum interleave count to use when interleaving a scalar \"\n             \"reduction in a nested loop.\"));\n\nstatic cl::opt<bool>\n    PreferInLoopReductions(\"prefer-inloop-reductions\", cl::init(false),\n                           cl::Hidden,\n                           cl::desc(\"Prefer in-loop vector reductions, \"\n                                    \"overriding the targets preference.\"));\n\nstatic cl::opt<bool> PreferPredicatedReductionSelect(\n    \"prefer-predicated-reduction-select\", cl::init(false), cl::Hidden,\n    cl::desc(\n        \"Prefer predicating a reduction operation over an after loop select.\"));\n\ncl::opt<bool> EnableVPlanNativePath(\n    \"enable-vplan-native-path\", cl::init(false), cl::Hidden,\n    cl::desc(\"Enable VPlan-native vectorization path with \"\n             \"support for outer loop vectorization.\"));\n\n// FIXME: Remove this switch once we have divergence analysis. Currently we\n// assume divergent non-backedge branches when this switch is true.\ncl::opt<bool> EnableVPlanPredication(\n    \"enable-vplan-predication\", cl::init(false), cl::Hidden,\n    cl::desc(\"Enable VPlan-native vectorization path predicator with \"\n             \"support for outer loop vectorization.\"));\n\n// This flag enables the stress testing of the VPlan H-CFG construction in the\n// VPlan-native vectorization path. It must be used in conjuction with\n// -enable-vplan-native-path. -vplan-verify-hcfg can also be used to enable the\n// verification of the H-CFGs built.\nstatic cl::opt<bool> VPlanBuildStressTest(\n    \"vplan-build-stress-test\", cl::init(false), cl::Hidden,\n    cl::desc(\n        \"Build VPlan for every supported loop nest in the function and bail \"\n        \"out right after the build (stress test the VPlan H-CFG construction \"\n        \"in the VPlan-native vectorization path).\"));\n\ncl::opt<bool> llvm::EnableLoopInterleaving(\n    \"interleave-loops\", cl::init(true), cl::Hidden,\n    cl::desc(\"Enable loop interleaving in Loop vectorization passes\"));\ncl::opt<bool> llvm::EnableLoopVectorization(\n    \"vectorize-loops\", cl::init(true), cl::Hidden,\n    cl::desc(\"Run the Loop vectorization passes\"));\n\n/// A helper function that returns the type of loaded or stored value.\nstatic Type *getMemInstValueType(Value *I) {\n  assert((isa<LoadInst>(I) || isa<StoreInst>(I)) &&\n         \"Expected Load or Store instruction\");\n  if (auto *LI = dyn_cast<LoadInst>(I))\n    return LI->getType();\n  return cast<StoreInst>(I)->getValueOperand()->getType();\n}\n\n/// A helper function that returns true if the given type is irregular. The\n/// type is irregular if its allocated size doesn't equal the store size of an\n/// element of the corresponding vector type at the given vectorization factor.\nstatic bool hasIrregularType(Type *Ty, const DataLayout &DL, ElementCount VF) {\n  // Determine if an array of VF elements of type Ty is \"bitcast compatible\"\n  // with a <VF x Ty> vector.\n  if (VF.isVector()) {\n    auto *VectorTy = VectorType::get(Ty, VF);\n    return TypeSize::get(VF.getKnownMinValue() *\n                             DL.getTypeAllocSize(Ty).getFixedValue(),\n                         VF.isScalable()) != DL.getTypeStoreSize(VectorTy);\n  }\n\n  // If the vectorization factor is one, we just check if an array of type Ty\n  // requires padding between elements.\n  return DL.getTypeAllocSizeInBits(Ty) != DL.getTypeSizeInBits(Ty);\n}\n\n/// A helper function that returns the reciprocal of the block probability of\n/// predicated blocks. If we return X, we are assuming the predicated block\n/// will execute once for every X iterations of the loop header.\n///\n/// TODO: We should use actual block probability here, if available. Currently,\n///       we always assume predicated blocks have a 50% chance of executing.\nstatic unsigned getReciprocalPredBlockProb() { return 2; }\n\n/// A helper function that returns an integer or floating-point constant with\n/// value C.\nstatic Constant *getSignedIntOrFpConstant(Type *Ty, int64_t C) {\n  return Ty->isIntegerTy() ? ConstantInt::getSigned(Ty, C)\n                           : ConstantFP::get(Ty, C);\n}\n\n/// Returns \"best known\" trip count for the specified loop \\p L as defined by\n/// the following procedure:\n///   1) Returns exact trip count if it is known.\n///   2) Returns expected trip count according to profile data if any.\n///   3) Returns upper bound estimate if it is known.\n///   4) Returns None if all of the above failed.\nstatic Optional<unsigned> getSmallBestKnownTC(ScalarEvolution &SE, Loop *L) {\n  // Check if exact trip count is known.\n  if (unsigned ExpectedTC = SE.getSmallConstantTripCount(L))\n    return ExpectedTC;\n\n  // Check if there is an expected trip count available from profile data.\n  if (LoopVectorizeWithBlockFrequency)\n    if (auto EstimatedTC = getLoopEstimatedTripCount(L))\n      return EstimatedTC;\n\n  // Check if upper bound estimate is known.\n  if (unsigned ExpectedTC = SE.getSmallConstantMaxTripCount(L))\n    return ExpectedTC;\n\n  return None;\n}\n\n// Forward declare GeneratedRTChecks.\nclass GeneratedRTChecks;\n\nnamespace llvm {\n\n/// InnerLoopVectorizer vectorizes loops which contain only one basic\n/// block to a specified vectorization factor (VF).\n/// This class performs the widening of scalars into vectors, or multiple\n/// scalars. This class also implements the following features:\n/// * It inserts an epilogue loop for handling loops that don't have iteration\n///   counts that are known to be a multiple of the vectorization factor.\n/// * It handles the code generation for reduction variables.\n/// * Scalarization (implementation using scalars) of un-vectorizable\n///   instructions.\n/// InnerLoopVectorizer does not perform any vectorization-legality\n/// checks, and relies on the caller to check for the different legality\n/// aspects. The InnerLoopVectorizer relies on the\n/// LoopVectorizationLegality class to provide information about the induction\n/// and reduction variables that were found to a given vectorization factor.\nclass InnerLoopVectorizer {\npublic:\n  InnerLoopVectorizer(Loop *OrigLoop, PredicatedScalarEvolution &PSE,\n                      LoopInfo *LI, DominatorTree *DT,\n                      const TargetLibraryInfo *TLI,\n                      const TargetTransformInfo *TTI, AssumptionCache *AC,\n                      OptimizationRemarkEmitter *ORE, ElementCount VecWidth,\n                      unsigned UnrollFactor, LoopVectorizationLegality *LVL,\n                      LoopVectorizationCostModel *CM, BlockFrequencyInfo *BFI,\n                      ProfileSummaryInfo *PSI, GeneratedRTChecks &RTChecks)\n      : OrigLoop(OrigLoop), PSE(PSE), LI(LI), DT(DT), TLI(TLI), TTI(TTI),\n        AC(AC), ORE(ORE), VF(VecWidth), UF(UnrollFactor),\n        Builder(PSE.getSE()->getContext()), Legal(LVL), Cost(CM), BFI(BFI),\n        PSI(PSI), RTChecks(RTChecks) {\n    // Query this against the original loop and save it here because the profile\n    // of the original loop header may change as the transformation happens.\n    OptForSizeBasedOnProfile = llvm::shouldOptimizeForSize(\n        OrigLoop->getHeader(), PSI, BFI, PGSOQueryType::IRPass);\n  }\n\n  virtual ~InnerLoopVectorizer() = default;\n\n  /// Create a new empty loop that will contain vectorized instructions later\n  /// on, while the old loop will be used as the scalar remainder. Control flow\n  /// is generated around the vectorized (and scalar epilogue) loops consisting\n  /// of various checks and bypasses. Return the pre-header block of the new\n  /// loop.\n  /// In the case of epilogue vectorization, this function is overriden to\n  /// handle the more complex control flow around the loops.\n  virtual BasicBlock *createVectorizedLoopSkeleton();\n\n  /// Widen a single instruction within the innermost loop.\n  void widenInstruction(Instruction &I, VPValue *Def, VPUser &Operands,\n                        VPTransformState &State);\n\n  /// Widen a single call instruction within the innermost loop.\n  void widenCallInstruction(CallInst &I, VPValue *Def, VPUser &ArgOperands,\n                            VPTransformState &State);\n\n  /// Widen a single select instruction within the innermost loop.\n  void widenSelectInstruction(SelectInst &I, VPValue *VPDef, VPUser &Operands,\n                              bool InvariantCond, VPTransformState &State);\n\n  /// Fix the vectorized code, taking care of header phi's, live-outs, and more.\n  void fixVectorizedLoop(VPTransformState &State);\n\n  // Return true if any runtime check is added.\n  bool areSafetyChecksAdded() { return AddedSafetyChecks; }\n\n  /// A type for vectorized values in the new loop. Each value from the\n  /// original loop, when vectorized, is represented by UF vector values in the\n  /// new unrolled loop, where UF is the unroll factor.\n  using VectorParts = SmallVector<Value *, 2>;\n\n  /// Vectorize a single GetElementPtrInst based on information gathered and\n  /// decisions taken during planning.\n  void widenGEP(GetElementPtrInst *GEP, VPValue *VPDef, VPUser &Indices,\n                unsigned UF, ElementCount VF, bool IsPtrLoopInvariant,\n                SmallBitVector &IsIndexLoopInvariant, VPTransformState &State);\n\n  /// Vectorize a single PHINode in a block. This method handles the induction\n  /// variable canonicalization. It supports both VF = 1 for unrolled loops and\n  /// arbitrary length vectors.\n  void widenPHIInstruction(Instruction *PN, RecurrenceDescriptor *RdxDesc,\n                           VPValue *StartV, VPValue *Def,\n                           VPTransformState &State);\n\n  /// A helper function to scalarize a single Instruction in the innermost loop.\n  /// Generates a sequence of scalar instances for each lane between \\p MinLane\n  /// and \\p MaxLane, times each part between \\p MinPart and \\p MaxPart,\n  /// inclusive. Uses the VPValue operands from \\p Operands instead of \\p\n  /// Instr's operands.\n  void scalarizeInstruction(Instruction *Instr, VPValue *Def, VPUser &Operands,\n                            const VPIteration &Instance, bool IfPredicateInstr,\n                            VPTransformState &State);\n\n  /// Widen an integer or floating-point induction variable \\p IV. If \\p Trunc\n  /// is provided, the integer induction variable will first be truncated to\n  /// the corresponding type.\n  void widenIntOrFpInduction(PHINode *IV, Value *Start, TruncInst *Trunc,\n                             VPValue *Def, VPValue *CastDef,\n                             VPTransformState &State);\n\n  /// Construct the vector value of a scalarized value \\p V one lane at a time.\n  void packScalarIntoVectorValue(VPValue *Def, const VPIteration &Instance,\n                                 VPTransformState &State);\n\n  /// Try to vectorize interleaved access group \\p Group with the base address\n  /// given in \\p Addr, optionally masking the vector operations if \\p\n  /// BlockInMask is non-null. Use \\p State to translate given VPValues to IR\n  /// values in the vectorized loop.\n  void vectorizeInterleaveGroup(const InterleaveGroup<Instruction> *Group,\n                                ArrayRef<VPValue *> VPDefs,\n                                VPTransformState &State, VPValue *Addr,\n                                ArrayRef<VPValue *> StoredValues,\n                                VPValue *BlockInMask = nullptr);\n\n  /// Vectorize Load and Store instructions with the base address given in \\p\n  /// Addr, optionally masking the vector operations if \\p BlockInMask is\n  /// non-null. Use \\p State to translate given VPValues to IR values in the\n  /// vectorized loop.\n  void vectorizeMemoryInstruction(Instruction *Instr, VPTransformState &State,\n                                  VPValue *Def, VPValue *Addr,\n                                  VPValue *StoredValue, VPValue *BlockInMask);\n\n  /// Set the debug location in the builder using the debug location in\n  /// the instruction.\n  void setDebugLocFromInst(IRBuilder<> &B, const Value *Ptr);\n\n  /// Fix the non-induction PHIs in the OrigPHIsToFix vector.\n  void fixNonInductionPHIs(VPTransformState &State);\n\n  /// Create a broadcast instruction. This method generates a broadcast\n  /// instruction (shuffle) for loop invariant values and for the induction\n  /// value. If this is the induction variable then we extend it to N, N+1, ...\n  /// this is needed because each iteration in the loop corresponds to a SIMD\n  /// element.\n  virtual Value *getBroadcastInstrs(Value *V);\n\nprotected:\n  friend class LoopVectorizationPlanner;\n\n  /// A small list of PHINodes.\n  using PhiVector = SmallVector<PHINode *, 4>;\n\n  /// A type for scalarized values in the new loop. Each value from the\n  /// original loop, when scalarized, is represented by UF x VF scalar values\n  /// in the new unrolled loop, where UF is the unroll factor and VF is the\n  /// vectorization factor.\n  using ScalarParts = SmallVector<SmallVector<Value *, 4>, 2>;\n\n  /// Set up the values of the IVs correctly when exiting the vector loop.\n  void fixupIVUsers(PHINode *OrigPhi, const InductionDescriptor &II,\n                    Value *CountRoundDown, Value *EndValue,\n                    BasicBlock *MiddleBlock);\n\n  /// Create a new induction variable inside L.\n  PHINode *createInductionVariable(Loop *L, Value *Start, Value *End,\n                                   Value *Step, Instruction *DL);\n\n  /// Handle all cross-iteration phis in the header.\n  void fixCrossIterationPHIs(VPTransformState &State);\n\n  /// Fix a first-order recurrence. This is the second phase of vectorizing\n  /// this phi node.\n  void fixFirstOrderRecurrence(PHINode *Phi, VPTransformState &State);\n\n  /// Fix a reduction cross-iteration phi. This is the second phase of\n  /// vectorizing this phi node.\n  void fixReduction(PHINode *Phi, VPTransformState &State);\n\n  /// Clear NSW/NUW flags from reduction instructions if necessary.\n  void clearReductionWrapFlags(RecurrenceDescriptor &RdxDesc,\n                               VPTransformState &State);\n\n  /// Fixup the LCSSA phi nodes in the unique exit block.  This simply\n  /// means we need to add the appropriate incoming value from the middle\n  /// block as exiting edges from the scalar epilogue loop (if present) are\n  /// already in place, and we exit the vector loop exclusively to the middle\n  /// block.\n  void fixLCSSAPHIs(VPTransformState &State);\n\n  /// Iteratively sink the scalarized operands of a predicated instruction into\n  /// the block that was created for it.\n  void sinkScalarOperands(Instruction *PredInst);\n\n  /// Shrinks vector element sizes to the smallest bitwidth they can be legally\n  /// represented as.\n  void truncateToMinimalBitwidths(VPTransformState &State);\n\n  /// This function adds (StartIdx, StartIdx + Step, StartIdx + 2*Step, ...)\n  /// to each vector element of Val. The sequence starts at StartIndex.\n  /// \\p Opcode is relevant for FP induction variable.\n  virtual Value *getStepVector(Value *Val, int StartIdx, Value *Step,\n                               Instruction::BinaryOps Opcode =\n                               Instruction::BinaryOpsEnd);\n\n  /// Compute scalar induction steps. \\p ScalarIV is the scalar induction\n  /// variable on which to base the steps, \\p Step is the size of the step, and\n  /// \\p EntryVal is the value from the original loop that maps to the steps.\n  /// Note that \\p EntryVal doesn't have to be an induction variable - it\n  /// can also be a truncate instruction.\n  void buildScalarSteps(Value *ScalarIV, Value *Step, Instruction *EntryVal,\n                        const InductionDescriptor &ID, VPValue *Def,\n                        VPValue *CastDef, VPTransformState &State);\n\n  /// Create a vector induction phi node based on an existing scalar one. \\p\n  /// EntryVal is the value from the original loop that maps to the vector phi\n  /// node, and \\p Step is the loop-invariant step. If \\p EntryVal is a\n  /// truncate instruction, instead of widening the original IV, we widen a\n  /// version of the IV truncated to \\p EntryVal's type.\n  void createVectorIntOrFpInductionPHI(const InductionDescriptor &II,\n                                       Value *Step, Value *Start,\n                                       Instruction *EntryVal, VPValue *Def,\n                                       VPValue *CastDef,\n                                       VPTransformState &State);\n\n  /// Returns true if an instruction \\p I should be scalarized instead of\n  /// vectorized for the chosen vectorization factor.\n  bool shouldScalarizeInstruction(Instruction *I) const;\n\n  /// Returns true if we should generate a scalar version of \\p IV.\n  bool needsScalarInduction(Instruction *IV) const;\n\n  /// If there is a cast involved in the induction variable \\p ID, which should\n  /// be ignored in the vectorized loop body, this function records the\n  /// VectorLoopValue of the respective Phi also as the VectorLoopValue of the\n  /// cast. We had already proved that the casted Phi is equal to the uncasted\n  /// Phi in the vectorized loop (under a runtime guard), and therefore\n  /// there is no need to vectorize the cast - the same value can be used in the\n  /// vector loop for both the Phi and the cast.\n  /// If \\p VectorLoopValue is a scalarized value, \\p Lane is also specified,\n  /// Otherwise, \\p VectorLoopValue is a widened/vectorized value.\n  ///\n  /// \\p EntryVal is the value from the original loop that maps to the vector\n  /// phi node and is used to distinguish what is the IV currently being\n  /// processed - original one (if \\p EntryVal is a phi corresponding to the\n  /// original IV) or the \"newly-created\" one based on the proof mentioned above\n  /// (see also buildScalarSteps() and createVectorIntOrFPInductionPHI()). In the\n  /// latter case \\p EntryVal is a TruncInst and we must not record anything for\n  /// that IV, but it's error-prone to expect callers of this routine to care\n  /// about that, hence this explicit parameter.\n  void recordVectorLoopValueForInductionCast(\n      const InductionDescriptor &ID, const Instruction *EntryVal,\n      Value *VectorLoopValue, VPValue *CastDef, VPTransformState &State,\n      unsigned Part, unsigned Lane = UINT_MAX);\n\n  /// Generate a shuffle sequence that will reverse the vector Vec.\n  virtual Value *reverseVector(Value *Vec);\n\n  /// Returns (and creates if needed) the original loop trip count.\n  Value *getOrCreateTripCount(Loop *NewLoop);\n\n  /// Returns (and creates if needed) the trip count of the widened loop.\n  Value *getOrCreateVectorTripCount(Loop *NewLoop);\n\n  /// Returns a bitcasted value to the requested vector type.\n  /// Also handles bitcasts of vector<float> <-> vector<pointer> types.\n  Value *createBitOrPointerCast(Value *V, VectorType *DstVTy,\n                                const DataLayout &DL);\n\n  /// Emit a bypass check to see if the vector trip count is zero, including if\n  /// it overflows.\n  void emitMinimumIterationCountCheck(Loop *L, BasicBlock *Bypass);\n\n  /// Emit a bypass check to see if all of the SCEV assumptions we've\n  /// had to make are correct. Returns the block containing the checks or\n  /// nullptr if no checks have been added.\n  BasicBlock *emitSCEVChecks(Loop *L, BasicBlock *Bypass);\n\n  /// Emit bypass checks to check any memory assumptions we may have made.\n  /// Returns the block containing the checks or nullptr if no checks have been\n  /// added.\n  BasicBlock *emitMemRuntimeChecks(Loop *L, BasicBlock *Bypass);\n\n  /// Compute the transformed value of Index at offset StartValue using step\n  /// StepValue.\n  /// For integer induction, returns StartValue + Index * StepValue.\n  /// For pointer induction, returns StartValue[Index * StepValue].\n  /// FIXME: The newly created binary instructions should contain nsw/nuw\n  /// flags, which can be found from the original scalar operations.\n  Value *emitTransformedIndex(IRBuilder<> &B, Value *Index, ScalarEvolution *SE,\n                              const DataLayout &DL,\n                              const InductionDescriptor &ID) const;\n\n  /// Emit basic blocks (prefixed with \\p Prefix) for the iteration check,\n  /// vector loop preheader, middle block and scalar preheader. Also\n  /// allocate a loop object for the new vector loop and return it.\n  Loop *createVectorLoopSkeleton(StringRef Prefix);\n\n  /// Create new phi nodes for the induction variables to resume iteration count\n  /// in the scalar epilogue, from where the vectorized loop left off (given by\n  /// \\p VectorTripCount).\n  /// In cases where the loop skeleton is more complicated (eg. epilogue\n  /// vectorization) and the resume values can come from an additional bypass\n  /// block, the \\p AdditionalBypass pair provides information about the bypass\n  /// block and the end value on the edge from bypass to this loop.\n  void createInductionResumeValues(\n      Loop *L, Value *VectorTripCount,\n      std::pair<BasicBlock *, Value *> AdditionalBypass = {nullptr, nullptr});\n\n  /// Complete the loop skeleton by adding debug MDs, creating appropriate\n  /// conditional branches in the middle block, preparing the builder and\n  /// running the verifier. Take in the vector loop \\p L as argument, and return\n  /// the preheader of the completed vector loop.\n  BasicBlock *completeLoopSkeleton(Loop *L, MDNode *OrigLoopID);\n\n  /// Add additional metadata to \\p To that was not present on \\p Orig.\n  ///\n  /// Currently this is used to add the noalias annotations based on the\n  /// inserted memchecks.  Use this for instructions that are *cloned* into the\n  /// vector loop.\n  void addNewMetadata(Instruction *To, const Instruction *Orig);\n\n  /// Add metadata from one instruction to another.\n  ///\n  /// This includes both the original MDs from \\p From and additional ones (\\see\n  /// addNewMetadata).  Use this for *newly created* instructions in the vector\n  /// loop.\n  void addMetadata(Instruction *To, Instruction *From);\n\n  /// Similar to the previous function but it adds the metadata to a\n  /// vector of instructions.\n  void addMetadata(ArrayRef<Value *> To, Instruction *From);\n\n  /// Allow subclasses to override and print debug traces before/after vplan\n  /// execution, when trace information is requested.\n  virtual void printDebugTracesAtStart(){};\n  virtual void printDebugTracesAtEnd(){};\n\n  /// The original loop.\n  Loop *OrigLoop;\n\n  /// A wrapper around ScalarEvolution used to add runtime SCEV checks. Applies\n  /// dynamic knowledge to simplify SCEV expressions and converts them to a\n  /// more usable form.\n  PredicatedScalarEvolution &PSE;\n\n  /// Loop Info.\n  LoopInfo *LI;\n\n  /// Dominator Tree.\n  DominatorTree *DT;\n\n  /// Alias Analysis.\n  AAResults *AA;\n\n  /// Target Library Info.\n  const TargetLibraryInfo *TLI;\n\n  /// Target Transform Info.\n  const TargetTransformInfo *TTI;\n\n  /// Assumption Cache.\n  AssumptionCache *AC;\n\n  /// Interface to emit optimization remarks.\n  OptimizationRemarkEmitter *ORE;\n\n  /// LoopVersioning.  It's only set up (non-null) if memchecks were\n  /// used.\n  ///\n  /// This is currently only used to add no-alias metadata based on the\n  /// memchecks.  The actually versioning is performed manually.\n  std::unique_ptr<LoopVersioning> LVer;\n\n  /// The vectorization SIMD factor to use. Each vector will have this many\n  /// vector elements.\n  ElementCount VF;\n\n  /// The vectorization unroll factor to use. Each scalar is vectorized to this\n  /// many different vector instructions.\n  unsigned UF;\n\n  /// The builder that we use\n  IRBuilder<> Builder;\n\n  // --- Vectorization state ---\n\n  /// The vector-loop preheader.\n  BasicBlock *LoopVectorPreHeader;\n\n  /// The scalar-loop preheader.\n  BasicBlock *LoopScalarPreHeader;\n\n  /// Middle Block between the vector and the scalar.\n  BasicBlock *LoopMiddleBlock;\n\n  /// The (unique) ExitBlock of the scalar loop.  Note that\n  /// there can be multiple exiting edges reaching this block.\n  BasicBlock *LoopExitBlock;\n\n  /// The vector loop body.\n  BasicBlock *LoopVectorBody;\n\n  /// The scalar loop body.\n  BasicBlock *LoopScalarBody;\n\n  /// A list of all bypass blocks. The first block is the entry of the loop.\n  SmallVector<BasicBlock *, 4> LoopBypassBlocks;\n\n  /// The new Induction variable which was added to the new block.\n  PHINode *Induction = nullptr;\n\n  /// The induction variable of the old basic block.\n  PHINode *OldInduction = nullptr;\n\n  /// Store instructions that were predicated.\n  SmallVector<Instruction *, 4> PredicatedInstructions;\n\n  /// Trip count of the original loop.\n  Value *TripCount = nullptr;\n\n  /// Trip count of the widened loop (TripCount - TripCount % (VF*UF))\n  Value *VectorTripCount = nullptr;\n\n  /// The legality analysis.\n  LoopVectorizationLegality *Legal;\n\n  /// The profitablity analysis.\n  LoopVectorizationCostModel *Cost;\n\n  // Record whether runtime checks are added.\n  bool AddedSafetyChecks = false;\n\n  // Holds the end values for each induction variable. We save the end values\n  // so we can later fix-up the external users of the induction variables.\n  DenseMap<PHINode *, Value *> IVEndValues;\n\n  // Vector of original scalar PHIs whose corresponding widened PHIs need to be\n  // fixed up at the end of vector code generation.\n  SmallVector<PHINode *, 8> OrigPHIsToFix;\n\n  /// BFI and PSI are used to check for profile guided size optimizations.\n  BlockFrequencyInfo *BFI;\n  ProfileSummaryInfo *PSI;\n\n  // Whether this loop should be optimized for size based on profile guided size\n  // optimizatios.\n  bool OptForSizeBasedOnProfile;\n\n  /// Structure to hold information about generated runtime checks, responsible\n  /// for cleaning the checks, if vectorization turns out unprofitable.\n  GeneratedRTChecks &RTChecks;\n};\n\nclass InnerLoopUnroller : public InnerLoopVectorizer {\npublic:\n  InnerLoopUnroller(Loop *OrigLoop, PredicatedScalarEvolution &PSE,\n                    LoopInfo *LI, DominatorTree *DT,\n                    const TargetLibraryInfo *TLI,\n                    const TargetTransformInfo *TTI, AssumptionCache *AC,\n                    OptimizationRemarkEmitter *ORE, unsigned UnrollFactor,\n                    LoopVectorizationLegality *LVL,\n                    LoopVectorizationCostModel *CM, BlockFrequencyInfo *BFI,\n                    ProfileSummaryInfo *PSI, GeneratedRTChecks &Check)\n      : InnerLoopVectorizer(OrigLoop, PSE, LI, DT, TLI, TTI, AC, ORE,\n                            ElementCount::getFixed(1), UnrollFactor, LVL, CM,\n                            BFI, PSI, Check) {}\n\nprivate:\n  Value *getBroadcastInstrs(Value *V) override;\n  Value *getStepVector(Value *Val, int StartIdx, Value *Step,\n                       Instruction::BinaryOps Opcode =\n                       Instruction::BinaryOpsEnd) override;\n  Value *reverseVector(Value *Vec) override;\n};\n\n/// Encapsulate information regarding vectorization of a loop and its epilogue.\n/// This information is meant to be updated and used across two stages of\n/// epilogue vectorization.\nstruct EpilogueLoopVectorizationInfo {\n  ElementCount MainLoopVF = ElementCount::getFixed(0);\n  unsigned MainLoopUF = 0;\n  ElementCount EpilogueVF = ElementCount::getFixed(0);\n  unsigned EpilogueUF = 0;\n  BasicBlock *MainLoopIterationCountCheck = nullptr;\n  BasicBlock *EpilogueIterationCountCheck = nullptr;\n  BasicBlock *SCEVSafetyCheck = nullptr;\n  BasicBlock *MemSafetyCheck = nullptr;\n  Value *TripCount = nullptr;\n  Value *VectorTripCount = nullptr;\n\n  EpilogueLoopVectorizationInfo(unsigned MVF, unsigned MUF, unsigned EVF,\n                                unsigned EUF)\n      : MainLoopVF(ElementCount::getFixed(MVF)), MainLoopUF(MUF),\n        EpilogueVF(ElementCount::getFixed(EVF)), EpilogueUF(EUF) {\n    assert(EUF == 1 &&\n           \"A high UF for the epilogue loop is likely not beneficial.\");\n  }\n};\n\n/// An extension of the inner loop vectorizer that creates a skeleton for a\n/// vectorized loop that has its epilogue (residual) also vectorized.\n/// The idea is to run the vplan on a given loop twice, firstly to setup the\n/// skeleton and vectorize the main loop, and secondly to complete the skeleton\n/// from the first step and vectorize the epilogue.  This is achieved by\n/// deriving two concrete strategy classes from this base class and invoking\n/// them in succession from the loop vectorizer planner.\nclass InnerLoopAndEpilogueVectorizer : public InnerLoopVectorizer {\npublic:\n  InnerLoopAndEpilogueVectorizer(\n      Loop *OrigLoop, PredicatedScalarEvolution &PSE, LoopInfo *LI,\n      DominatorTree *DT, const TargetLibraryInfo *TLI,\n      const TargetTransformInfo *TTI, AssumptionCache *AC,\n      OptimizationRemarkEmitter *ORE, EpilogueLoopVectorizationInfo &EPI,\n      LoopVectorizationLegality *LVL, llvm::LoopVectorizationCostModel *CM,\n      BlockFrequencyInfo *BFI, ProfileSummaryInfo *PSI,\n      GeneratedRTChecks &Checks)\n      : InnerLoopVectorizer(OrigLoop, PSE, LI, DT, TLI, TTI, AC, ORE,\n                            EPI.MainLoopVF, EPI.MainLoopUF, LVL, CM, BFI, PSI,\n                            Checks),\n        EPI(EPI) {}\n\n  // Override this function to handle the more complex control flow around the\n  // three loops.\n  BasicBlock *createVectorizedLoopSkeleton() final override {\n    return createEpilogueVectorizedLoopSkeleton();\n  }\n\n  /// The interface for creating a vectorized skeleton using one of two\n  /// different strategies, each corresponding to one execution of the vplan\n  /// as described above.\n  virtual BasicBlock *createEpilogueVectorizedLoopSkeleton() = 0;\n\n  /// Holds and updates state information required to vectorize the main loop\n  /// and its epilogue in two separate passes. This setup helps us avoid\n  /// regenerating and recomputing runtime safety checks. It also helps us to\n  /// shorten the iteration-count-check path length for the cases where the\n  /// iteration count of the loop is so small that the main vector loop is\n  /// completely skipped.\n  EpilogueLoopVectorizationInfo &EPI;\n};\n\n/// A specialized derived class of inner loop vectorizer that performs\n/// vectorization of *main* loops in the process of vectorizing loops and their\n/// epilogues.\nclass EpilogueVectorizerMainLoop : public InnerLoopAndEpilogueVectorizer {\npublic:\n  EpilogueVectorizerMainLoop(\n      Loop *OrigLoop, PredicatedScalarEvolution &PSE, LoopInfo *LI,\n      DominatorTree *DT, const TargetLibraryInfo *TLI,\n      const TargetTransformInfo *TTI, AssumptionCache *AC,\n      OptimizationRemarkEmitter *ORE, EpilogueLoopVectorizationInfo &EPI,\n      LoopVectorizationLegality *LVL, llvm::LoopVectorizationCostModel *CM,\n      BlockFrequencyInfo *BFI, ProfileSummaryInfo *PSI,\n      GeneratedRTChecks &Check)\n      : InnerLoopAndEpilogueVectorizer(OrigLoop, PSE, LI, DT, TLI, TTI, AC, ORE,\n                                       EPI, LVL, CM, BFI, PSI, Check) {}\n  /// Implements the interface for creating a vectorized skeleton using the\n  /// *main loop* strategy (ie the first pass of vplan execution).\n  BasicBlock *createEpilogueVectorizedLoopSkeleton() final override;\n\nprotected:\n  /// Emits an iteration count bypass check once for the main loop (when \\p\n  /// ForEpilogue is false) and once for the epilogue loop (when \\p\n  /// ForEpilogue is true).\n  BasicBlock *emitMinimumIterationCountCheck(Loop *L, BasicBlock *Bypass,\n                                             bool ForEpilogue);\n  void printDebugTracesAtStart() override;\n  void printDebugTracesAtEnd() override;\n};\n\n// A specialized derived class of inner loop vectorizer that performs\n// vectorization of *epilogue* loops in the process of vectorizing loops and\n// their epilogues.\nclass EpilogueVectorizerEpilogueLoop : public InnerLoopAndEpilogueVectorizer {\npublic:\n  EpilogueVectorizerEpilogueLoop(\n      Loop *OrigLoop, PredicatedScalarEvolution &PSE, LoopInfo *LI,\n      DominatorTree *DT, const TargetLibraryInfo *TLI,\n      const TargetTransformInfo *TTI, AssumptionCache *AC,\n      OptimizationRemarkEmitter *ORE, EpilogueLoopVectorizationInfo &EPI,\n      LoopVectorizationLegality *LVL, llvm::LoopVectorizationCostModel *CM,\n      BlockFrequencyInfo *BFI, ProfileSummaryInfo *PSI,\n      GeneratedRTChecks &Checks)\n      : InnerLoopAndEpilogueVectorizer(OrigLoop, PSE, LI, DT, TLI, TTI, AC, ORE,\n                                       EPI, LVL, CM, BFI, PSI, Checks) {}\n  /// Implements the interface for creating a vectorized skeleton using the\n  /// *epilogue loop* strategy (ie the second pass of vplan execution).\n  BasicBlock *createEpilogueVectorizedLoopSkeleton() final override;\n\nprotected:\n  /// Emits an iteration count bypass check after the main vector loop has\n  /// finished to see if there are any iterations left to execute by either\n  /// the vector epilogue or the scalar epilogue.\n  BasicBlock *emitMinimumVectorEpilogueIterCountCheck(Loop *L,\n                                                      BasicBlock *Bypass,\n                                                      BasicBlock *Insert);\n  void printDebugTracesAtStart() override;\n  void printDebugTracesAtEnd() override;\n};\n} // end namespace llvm\n\n/// Look for a meaningful debug location on the instruction or it's\n/// operands.\nstatic Instruction *getDebugLocFromInstOrOperands(Instruction *I) {\n  if (!I)\n    return I;\n\n  DebugLoc Empty;\n  if (I->getDebugLoc() != Empty)\n    return I;\n\n  for (Use &Op : I->operands()) {\n    if (Instruction *OpInst = dyn_cast<Instruction>(Op))\n      if (OpInst->getDebugLoc() != Empty)\n        return OpInst;\n  }\n\n  return I;\n}\n\nvoid InnerLoopVectorizer::setDebugLocFromInst(IRBuilder<> &B, const Value *Ptr) {\n  if (const Instruction *Inst = dyn_cast_or_null<Instruction>(Ptr)) {\n    const DILocation *DIL = Inst->getDebugLoc();\n    if (DIL && Inst->getFunction()->isDebugInfoForProfiling() &&\n        !isa<DbgInfoIntrinsic>(Inst)) {\n      assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n      auto NewDIL =\n          DIL->cloneByMultiplyingDuplicationFactor(UF * VF.getKnownMinValue());\n      if (NewDIL)\n        B.SetCurrentDebugLocation(NewDIL.getValue());\n      else\n        LLVM_DEBUG(dbgs()\n                   << \"Failed to create new discriminator: \"\n                   << DIL->getFilename() << \" Line: \" << DIL->getLine());\n    }\n    else\n      B.SetCurrentDebugLocation(DIL);\n  } else\n    B.SetCurrentDebugLocation(DebugLoc());\n}\n\n/// Write a record \\p DebugMsg about vectorization failure to the debug\n/// output stream. If \\p I is passed, it is an instruction that prevents\n/// vectorization.\n#ifndef NDEBUG\nstatic void debugVectorizationFailure(const StringRef DebugMsg,\n    Instruction *I) {\n  dbgs() << \"LV: Not vectorizing: \" << DebugMsg;\n  if (I != nullptr)\n    dbgs() << \" \" << *I;\n  else\n    dbgs() << '.';\n  dbgs() << '\\n';\n}\n#endif\n\n/// Create an analysis remark that explains why vectorization failed\n///\n/// \\p PassName is the name of the pass (e.g. can be AlwaysPrint).  \\p\n/// RemarkName is the identifier for the remark.  If \\p I is passed it is an\n/// instruction that prevents vectorization.  Otherwise \\p TheLoop is used for\n/// the location of the remark.  \\return the remark object that can be\n/// streamed to.\nstatic OptimizationRemarkAnalysis createLVAnalysis(const char *PassName,\n    StringRef RemarkName, Loop *TheLoop, Instruction *I) {\n  Value *CodeRegion = TheLoop->getHeader();\n  DebugLoc DL = TheLoop->getStartLoc();\n\n  if (I) {\n    CodeRegion = I->getParent();\n    // If there is no debug location attached to the instruction, revert back to\n    // using the loop's.\n    if (I->getDebugLoc())\n      DL = I->getDebugLoc();\n  }\n\n  OptimizationRemarkAnalysis R(PassName, RemarkName, DL, CodeRegion);\n  R << \"loop not vectorized: \";\n  return R;\n}\n\n/// Return a value for Step multiplied by VF.\nstatic Value *createStepForVF(IRBuilder<> &B, Constant *Step, ElementCount VF) {\n  assert(isa<ConstantInt>(Step) && \"Expected an integer step\");\n  Constant *StepVal = ConstantInt::get(\n      Step->getType(),\n      cast<ConstantInt>(Step)->getSExtValue() * VF.getKnownMinValue());\n  return VF.isScalable() ? B.CreateVScale(StepVal) : StepVal;\n}\n\nnamespace llvm {\n\n/// Return the runtime value for VF.\nValue *getRuntimeVF(IRBuilder<> &B, Type *Ty, ElementCount VF) {\n  Constant *EC = ConstantInt::get(Ty, VF.getKnownMinValue());\n  return VF.isScalable() ? B.CreateVScale(EC) : EC;\n}\n\nvoid reportVectorizationFailure(const StringRef DebugMsg,\n    const StringRef OREMsg, const StringRef ORETag,\n    OptimizationRemarkEmitter *ORE, Loop *TheLoop, Instruction *I) {\n  LLVM_DEBUG(debugVectorizationFailure(DebugMsg, I));\n  LoopVectorizeHints Hints(TheLoop, true /* doesn't matter */, *ORE);\n  ORE->emit(createLVAnalysis(Hints.vectorizeAnalysisPassName(),\n                ORETag, TheLoop, I) << OREMsg);\n}\n\n} // end namespace llvm\n\n#ifndef NDEBUG\n/// \\return string containing a file name and a line # for the given loop.\nstatic std::string getDebugLocString(const Loop *L) {\n  std::string Result;\n  if (L) {\n    raw_string_ostream OS(Result);\n    if (const DebugLoc LoopDbgLoc = L->getStartLoc())\n      LoopDbgLoc.print(OS);\n    else\n      // Just print the module name.\n      OS << L->getHeader()->getParent()->getParent()->getModuleIdentifier();\n    OS.flush();\n  }\n  return Result;\n}\n#endif\n\nvoid InnerLoopVectorizer::addNewMetadata(Instruction *To,\n                                         const Instruction *Orig) {\n  // If the loop was versioned with memchecks, add the corresponding no-alias\n  // metadata.\n  if (LVer && (isa<LoadInst>(Orig) || isa<StoreInst>(Orig)))\n    LVer->annotateInstWithNoAlias(To, Orig);\n}\n\nvoid InnerLoopVectorizer::addMetadata(Instruction *To,\n                                      Instruction *From) {\n  propagateMetadata(To, From);\n  addNewMetadata(To, From);\n}\n\nvoid InnerLoopVectorizer::addMetadata(ArrayRef<Value *> To,\n                                      Instruction *From) {\n  for (Value *V : To) {\n    if (Instruction *I = dyn_cast<Instruction>(V))\n      addMetadata(I, From);\n  }\n}\n\nnamespace llvm {\n\n// Loop vectorization cost-model hints how the scalar epilogue loop should be\n// lowered.\nenum ScalarEpilogueLowering {\n\n  // The default: allowing scalar epilogues.\n  CM_ScalarEpilogueAllowed,\n\n  // Vectorization with OptForSize: don't allow epilogues.\n  CM_ScalarEpilogueNotAllowedOptSize,\n\n  // A special case of vectorisation with OptForSize: loops with a very small\n  // trip count are considered for vectorization under OptForSize, thereby\n  // making sure the cost of their loop body is dominant, free of runtime\n  // guards and scalar iteration overheads.\n  CM_ScalarEpilogueNotAllowedLowTripLoop,\n\n  // Loop hint predicate indicating an epilogue is undesired.\n  CM_ScalarEpilogueNotNeededUsePredicate,\n\n  // Directive indicating we must either tail fold or not vectorize\n  CM_ScalarEpilogueNotAllowedUsePredicate\n};\n\n/// LoopVectorizationCostModel - estimates the expected speedups due to\n/// vectorization.\n/// In many cases vectorization is not profitable. This can happen because of\n/// a number of reasons. In this class we mainly attempt to predict the\n/// expected speedup/slowdowns due to the supported instruction set. We use the\n/// TargetTransformInfo to query the different backends for the cost of\n/// different operations.\nclass LoopVectorizationCostModel {\npublic:\n  LoopVectorizationCostModel(ScalarEpilogueLowering SEL, Loop *L,\n                             PredicatedScalarEvolution &PSE, LoopInfo *LI,\n                             LoopVectorizationLegality *Legal,\n                             const TargetTransformInfo &TTI,\n                             const TargetLibraryInfo *TLI, DemandedBits *DB,\n                             AssumptionCache *AC,\n                             OptimizationRemarkEmitter *ORE, const Function *F,\n                             const LoopVectorizeHints *Hints,\n                             InterleavedAccessInfo &IAI)\n      : ScalarEpilogueStatus(SEL), TheLoop(L), PSE(PSE), LI(LI), Legal(Legal),\n        TTI(TTI), TLI(TLI), DB(DB), AC(AC), ORE(ORE), TheFunction(F),\n        Hints(Hints), InterleaveInfo(IAI) {}\n\n  /// \\return An upper bound for the vectorization factor, or None if\n  /// vectorization and interleaving should be avoided up front.\n  Optional<ElementCount> computeMaxVF(ElementCount UserVF, unsigned UserIC);\n\n  /// \\return True if runtime checks are required for vectorization, and false\n  /// otherwise.\n  bool runtimeChecksRequired();\n\n  /// \\return The most profitable vectorization factor and the cost of that VF.\n  /// This method checks every power of two up to MaxVF. If UserVF is not ZERO\n  /// then this vectorization factor will be selected if vectorization is\n  /// possible.\n  VectorizationFactor selectVectorizationFactor(ElementCount MaxVF);\n  VectorizationFactor\n  selectEpilogueVectorizationFactor(const ElementCount MaxVF,\n                                    const LoopVectorizationPlanner &LVP);\n\n  /// Setup cost-based decisions for user vectorization factor.\n  void selectUserVectorizationFactor(ElementCount UserVF) {\n    collectUniformsAndScalars(UserVF);\n    collectInstsToScalarize(UserVF);\n  }\n\n  /// \\return The size (in bits) of the smallest and widest types in the code\n  /// that needs to be vectorized. We ignore values that remain scalar such as\n  /// 64 bit loop indices.\n  std::pair<unsigned, unsigned> getSmallestAndWidestTypes();\n\n  /// \\return The desired interleave count.\n  /// If interleave count has been specified by metadata it will be returned.\n  /// Otherwise, the interleave count is computed and returned. VF and LoopCost\n  /// are the selected vectorization factor and the cost of the selected VF.\n  unsigned selectInterleaveCount(ElementCount VF, unsigned LoopCost);\n\n  /// Memory access instruction may be vectorized in more than one way.\n  /// Form of instruction after vectorization depends on cost.\n  /// This function takes cost-based decisions for Load/Store instructions\n  /// and collects them in a map. This decisions map is used for building\n  /// the lists of loop-uniform and loop-scalar instructions.\n  /// The calculated cost is saved with widening decision in order to\n  /// avoid redundant calculations.\n  void setCostBasedWideningDecision(ElementCount VF);\n\n  /// A struct that represents some properties of the register usage\n  /// of a loop.\n  struct RegisterUsage {\n    /// Holds the number of loop invariant values that are used in the loop.\n    /// The key is ClassID of target-provided register class.\n    SmallMapVector<unsigned, unsigned, 4> LoopInvariantRegs;\n    /// Holds the maximum number of concurrent live intervals in the loop.\n    /// The key is ClassID of target-provided register class.\n    SmallMapVector<unsigned, unsigned, 4> MaxLocalUsers;\n  };\n\n  /// \\return Returns information about the register usages of the loop for the\n  /// given vectorization factors.\n  SmallVector<RegisterUsage, 8>\n  calculateRegisterUsage(ArrayRef<ElementCount> VFs);\n\n  /// Collect values we want to ignore in the cost model.\n  void collectValuesToIgnore();\n\n  /// Split reductions into those that happen in the loop, and those that happen\n  /// outside. In loop reductions are collected into InLoopReductionChains.\n  void collectInLoopReductions();\n\n  /// \\returns The smallest bitwidth each instruction can be represented with.\n  /// The vector equivalents of these instructions should be truncated to this\n  /// type.\n  const MapVector<Instruction *, uint64_t> &getMinimalBitwidths() const {\n    return MinBWs;\n  }\n\n  /// \\returns True if it is more profitable to scalarize instruction \\p I for\n  /// vectorization factor \\p VF.\n  bool isProfitableToScalarize(Instruction *I, ElementCount VF) const {\n    assert(VF.isVector() &&\n           \"Profitable to scalarize relevant only for VF > 1.\");\n\n    // Cost model is not run in the VPlan-native path - return conservative\n    // result until this changes.\n    if (EnableVPlanNativePath)\n      return false;\n\n    auto Scalars = InstsToScalarize.find(VF);\n    assert(Scalars != InstsToScalarize.end() &&\n           \"VF not yet analyzed for scalarization profitability\");\n    return Scalars->second.find(I) != Scalars->second.end();\n  }\n\n  /// Returns true if \\p I is known to be uniform after vectorization.\n  bool isUniformAfterVectorization(Instruction *I, ElementCount VF) const {\n    if (VF.isScalar())\n      return true;\n\n    // Cost model is not run in the VPlan-native path - return conservative\n    // result until this changes.\n    if (EnableVPlanNativePath)\n      return false;\n\n    auto UniformsPerVF = Uniforms.find(VF);\n    assert(UniformsPerVF != Uniforms.end() &&\n           \"VF not yet analyzed for uniformity\");\n    return UniformsPerVF->second.count(I);\n  }\n\n  /// Returns true if \\p I is known to be scalar after vectorization.\n  bool isScalarAfterVectorization(Instruction *I, ElementCount VF) const {\n    if (VF.isScalar())\n      return true;\n\n    // Cost model is not run in the VPlan-native path - return conservative\n    // result until this changes.\n    if (EnableVPlanNativePath)\n      return false;\n\n    auto ScalarsPerVF = Scalars.find(VF);\n    assert(ScalarsPerVF != Scalars.end() &&\n           \"Scalar values are not calculated for VF\");\n    return ScalarsPerVF->second.count(I);\n  }\n\n  /// \\returns True if instruction \\p I can be truncated to a smaller bitwidth\n  /// for vectorization factor \\p VF.\n  bool canTruncateToMinimalBitwidth(Instruction *I, ElementCount VF) const {\n    return VF.isVector() && MinBWs.find(I) != MinBWs.end() &&\n           !isProfitableToScalarize(I, VF) &&\n           !isScalarAfterVectorization(I, VF);\n  }\n\n  /// Decision that was taken during cost calculation for memory instruction.\n  enum InstWidening {\n    CM_Unknown,\n    CM_Widen,         // For consecutive accesses with stride +1.\n    CM_Widen_Reverse, // For consecutive accesses with stride -1.\n    CM_Interleave,\n    CM_GatherScatter,\n    CM_Scalarize\n  };\n\n  /// Save vectorization decision \\p W and \\p Cost taken by the cost model for\n  /// instruction \\p I and vector width \\p VF.\n  void setWideningDecision(Instruction *I, ElementCount VF, InstWidening W,\n                           InstructionCost Cost) {\n    assert(VF.isVector() && \"Expected VF >=2\");\n    WideningDecisions[std::make_pair(I, VF)] = std::make_pair(W, Cost);\n  }\n\n  /// Save vectorization decision \\p W and \\p Cost taken by the cost model for\n  /// interleaving group \\p Grp and vector width \\p VF.\n  void setWideningDecision(const InterleaveGroup<Instruction> *Grp,\n                           ElementCount VF, InstWidening W,\n                           InstructionCost Cost) {\n    assert(VF.isVector() && \"Expected VF >=2\");\n    /// Broadcast this decicion to all instructions inside the group.\n    /// But the cost will be assigned to one instruction only.\n    for (unsigned i = 0; i < Grp->getFactor(); ++i) {\n      if (auto *I = Grp->getMember(i)) {\n        if (Grp->getInsertPos() == I)\n          WideningDecisions[std::make_pair(I, VF)] = std::make_pair(W, Cost);\n        else\n          WideningDecisions[std::make_pair(I, VF)] = std::make_pair(W, 0);\n      }\n    }\n  }\n\n  /// Return the cost model decision for the given instruction \\p I and vector\n  /// width \\p VF. Return CM_Unknown if this instruction did not pass\n  /// through the cost modeling.\n  InstWidening getWideningDecision(Instruction *I, ElementCount VF) {\n    assert(VF.isVector() && \"Expected VF to be a vector VF\");\n    // Cost model is not run in the VPlan-native path - return conservative\n    // result until this changes.\n    if (EnableVPlanNativePath)\n      return CM_GatherScatter;\n\n    std::pair<Instruction *, ElementCount> InstOnVF = std::make_pair(I, VF);\n    auto Itr = WideningDecisions.find(InstOnVF);\n    if (Itr == WideningDecisions.end())\n      return CM_Unknown;\n    return Itr->second.first;\n  }\n\n  /// Return the vectorization cost for the given instruction \\p I and vector\n  /// width \\p VF.\n  InstructionCost getWideningCost(Instruction *I, ElementCount VF) {\n    assert(VF.isVector() && \"Expected VF >=2\");\n    std::pair<Instruction *, ElementCount> InstOnVF = std::make_pair(I, VF);\n    assert(WideningDecisions.find(InstOnVF) != WideningDecisions.end() &&\n           \"The cost is not calculated\");\n    return WideningDecisions[InstOnVF].second;\n  }\n\n  /// Return True if instruction \\p I is an optimizable truncate whose operand\n  /// is an induction variable. Such a truncate will be removed by adding a new\n  /// induction variable with the destination type.\n  bool isOptimizableIVTruncate(Instruction *I, ElementCount VF) {\n    // If the instruction is not a truncate, return false.\n    auto *Trunc = dyn_cast<TruncInst>(I);\n    if (!Trunc)\n      return false;\n\n    // Get the source and destination types of the truncate.\n    Type *SrcTy = ToVectorTy(cast<CastInst>(I)->getSrcTy(), VF);\n    Type *DestTy = ToVectorTy(cast<CastInst>(I)->getDestTy(), VF);\n\n    // If the truncate is free for the given types, return false. Replacing a\n    // free truncate with an induction variable would add an induction variable\n    // update instruction to each iteration of the loop. We exclude from this\n    // check the primary induction variable since it will need an update\n    // instruction regardless.\n    Value *Op = Trunc->getOperand(0);\n    if (Op != Legal->getPrimaryInduction() && TTI.isTruncateFree(SrcTy, DestTy))\n      return false;\n\n    // If the truncated value is not an induction variable, return false.\n    return Legal->isInductionPhi(Op);\n  }\n\n  /// Collects the instructions to scalarize for each predicated instruction in\n  /// the loop.\n  void collectInstsToScalarize(ElementCount VF);\n\n  /// Collect Uniform and Scalar values for the given \\p VF.\n  /// The sets depend on CM decision for Load/Store instructions\n  /// that may be vectorized as interleave, gather-scatter or scalarized.\n  void collectUniformsAndScalars(ElementCount VF) {\n    // Do the analysis once.\n    if (VF.isScalar() || Uniforms.find(VF) != Uniforms.end())\n      return;\n    setCostBasedWideningDecision(VF);\n    collectLoopUniforms(VF);\n    collectLoopScalars(VF);\n  }\n\n  /// Returns true if the target machine supports masked store operation\n  /// for the given \\p DataType and kind of access to \\p Ptr.\n  bool isLegalMaskedStore(Type *DataType, Value *Ptr, Align Alignment) {\n    return Legal->isConsecutivePtr(Ptr) &&\n           TTI.isLegalMaskedStore(DataType, Alignment);\n  }\n\n  /// Returns true if the target machine supports masked load operation\n  /// for the given \\p DataType and kind of access to \\p Ptr.\n  bool isLegalMaskedLoad(Type *DataType, Value *Ptr, Align Alignment) {\n    return Legal->isConsecutivePtr(Ptr) &&\n           TTI.isLegalMaskedLoad(DataType, Alignment);\n  }\n\n  /// Returns true if the target machine supports masked scatter operation\n  /// for the given \\p DataType.\n  bool isLegalMaskedScatter(Type *DataType, Align Alignment) {\n    return TTI.isLegalMaskedScatter(DataType, Alignment);\n  }\n\n  /// Returns true if the target machine supports masked gather operation\n  /// for the given \\p DataType.\n  bool isLegalMaskedGather(Type *DataType, Align Alignment) {\n    return TTI.isLegalMaskedGather(DataType, Alignment);\n  }\n\n  /// Returns true if the target machine can represent \\p V as a masked gather\n  /// or scatter operation.\n  bool isLegalGatherOrScatter(Value *V) {\n    bool LI = isa<LoadInst>(V);\n    bool SI = isa<StoreInst>(V);\n    if (!LI && !SI)\n      return false;\n    auto *Ty = getMemInstValueType(V);\n    Align Align = getLoadStoreAlignment(V);\n    return (LI && isLegalMaskedGather(Ty, Align)) ||\n           (SI && isLegalMaskedScatter(Ty, Align));\n  }\n\n  /// Returns true if the target machine supports all of the reduction\n  /// variables found for the given VF.\n  bool canVectorizeReductions(ElementCount VF) {\n    return (all_of(Legal->getReductionVars(), [&](auto &Reduction) -> bool {\n      RecurrenceDescriptor RdxDesc = Reduction.second;\n      return TTI.isLegalToVectorizeReduction(RdxDesc, VF);\n    }));\n  }\n\n  /// Returns true if \\p I is an instruction that will be scalarized with\n  /// predication. Such instructions include conditional stores and\n  /// instructions that may divide by zero.\n  /// If a non-zero VF has been calculated, we check if I will be scalarized\n  /// predication for that VF.\n  bool isScalarWithPredication(Instruction *I,\n                               ElementCount VF = ElementCount::getFixed(1));\n\n  // Returns true if \\p I is an instruction that will be predicated either\n  // through scalar predication or masked load/store or masked gather/scatter.\n  // Superset of instructions that return true for isScalarWithPredication.\n  bool isPredicatedInst(Instruction *I) {\n    if (!blockNeedsPredication(I->getParent()))\n      return false;\n    // Loads and stores that need some form of masked operation are predicated\n    // instructions.\n    if (isa<LoadInst>(I) || isa<StoreInst>(I))\n      return Legal->isMaskRequired(I);\n    return isScalarWithPredication(I);\n  }\n\n  /// Returns true if \\p I is a memory instruction with consecutive memory\n  /// access that can be widened.\n  bool\n  memoryInstructionCanBeWidened(Instruction *I,\n                                ElementCount VF = ElementCount::getFixed(1));\n\n  /// Returns true if \\p I is a memory instruction in an interleaved-group\n  /// of memory accesses that can be vectorized with wide vector loads/stores\n  /// and shuffles.\n  bool\n  interleavedAccessCanBeWidened(Instruction *I,\n                                ElementCount VF = ElementCount::getFixed(1));\n\n  /// Check if \\p Instr belongs to any interleaved access group.\n  bool isAccessInterleaved(Instruction *Instr) {\n    return InterleaveInfo.isInterleaved(Instr);\n  }\n\n  /// Get the interleaved access group that \\p Instr belongs to.\n  const InterleaveGroup<Instruction> *\n  getInterleavedAccessGroup(Instruction *Instr) {\n    return InterleaveInfo.getInterleaveGroup(Instr);\n  }\n\n  /// Returns true if we're required to use a scalar epilogue for at least\n  /// the final iteration of the original loop.\n  bool requiresScalarEpilogue() const {\n    if (!isScalarEpilogueAllowed())\n      return false;\n    // If we might exit from anywhere but the latch, must run the exiting\n    // iteration in scalar form.\n    if (TheLoop->getExitingBlock() != TheLoop->getLoopLatch())\n      return true;\n    return InterleaveInfo.requiresScalarEpilogue();\n  }\n\n  /// Returns true if a scalar epilogue is not allowed due to optsize or a\n  /// loop hint annotation.\n  bool isScalarEpilogueAllowed() const {\n    return ScalarEpilogueStatus == CM_ScalarEpilogueAllowed;\n  }\n\n  /// Returns true if all loop blocks should be masked to fold tail loop.\n  bool foldTailByMasking() const { return FoldTailByMasking; }\n\n  bool blockNeedsPredication(BasicBlock *BB) {\n    return foldTailByMasking() || Legal->blockNeedsPredication(BB);\n  }\n\n  /// A SmallMapVector to store the InLoop reduction op chains, mapping phi\n  /// nodes to the chain of instructions representing the reductions. Uses a\n  /// MapVector to ensure deterministic iteration order.\n  using ReductionChainMap =\n      SmallMapVector<PHINode *, SmallVector<Instruction *, 4>, 4>;\n\n  /// Return the chain of instructions representing an inloop reduction.\n  const ReductionChainMap &getInLoopReductionChains() const {\n    return InLoopReductionChains;\n  }\n\n  /// Returns true if the Phi is part of an inloop reduction.\n  bool isInLoopReduction(PHINode *Phi) const {\n    return InLoopReductionChains.count(Phi);\n  }\n\n  /// Estimate cost of an intrinsic call instruction CI if it were vectorized\n  /// with factor VF.  Return the cost of the instruction, including\n  /// scalarization overhead if it's needed.\n  InstructionCost getVectorIntrinsicCost(CallInst *CI, ElementCount VF);\n\n  /// Estimate cost of a call instruction CI if it were vectorized with factor\n  /// VF. Return the cost of the instruction, including scalarization overhead\n  /// if it's needed. The flag NeedToScalarize shows if the call needs to be\n  /// scalarized -\n  /// i.e. either vector version isn't available, or is too expensive.\n  InstructionCost getVectorCallCost(CallInst *CI, ElementCount VF,\n                                    bool &NeedToScalarize);\n\n  /// Invalidates decisions already taken by the cost model.\n  void invalidateCostModelingDecisions() {\n    WideningDecisions.clear();\n    Uniforms.clear();\n    Scalars.clear();\n  }\n\nprivate:\n  unsigned NumPredStores = 0;\n\n  /// \\return An upper bound for the vectorization factor, a power-of-2 larger\n  /// than zero. One is returned if vectorization should best be avoided due\n  /// to cost.\n  ElementCount computeFeasibleMaxVF(unsigned ConstTripCount,\n                                    ElementCount UserVF);\n\n  /// The vectorization cost is a combination of the cost itself and a boolean\n  /// indicating whether any of the contributing operations will actually\n  /// operate on\n  /// vector values after type legalization in the backend. If this latter value\n  /// is\n  /// false, then all operations will be scalarized (i.e. no vectorization has\n  /// actually taken place).\n  using VectorizationCostTy = std::pair<InstructionCost, bool>;\n\n  /// Returns the expected execution cost. The unit of the cost does\n  /// not matter because we use the 'cost' units to compare different\n  /// vector widths. The cost that is returned is *not* normalized by\n  /// the factor width.\n  VectorizationCostTy expectedCost(ElementCount VF);\n\n  /// Returns the execution time cost of an instruction for a given vector\n  /// width. Vector width of one means scalar.\n  VectorizationCostTy getInstructionCost(Instruction *I, ElementCount VF);\n\n  /// The cost-computation logic from getInstructionCost which provides\n  /// the vector type as an output parameter.\n  InstructionCost getInstructionCost(Instruction *I, ElementCount VF,\n                                     Type *&VectorTy);\n\n  /// Return the cost of instructions in an inloop reduction pattern, if I is\n  /// part of that pattern.\n  InstructionCost getReductionPatternCost(Instruction *I, ElementCount VF,\n                                          Type *VectorTy,\n                                          TTI::TargetCostKind CostKind);\n\n  /// Calculate vectorization cost of memory instruction \\p I.\n  InstructionCost getMemoryInstructionCost(Instruction *I, ElementCount VF);\n\n  /// The cost computation for scalarized memory instruction.\n  InstructionCost getMemInstScalarizationCost(Instruction *I, ElementCount VF);\n\n  /// The cost computation for interleaving group of memory instructions.\n  InstructionCost getInterleaveGroupCost(Instruction *I, ElementCount VF);\n\n  /// The cost computation for Gather/Scatter instruction.\n  InstructionCost getGatherScatterCost(Instruction *I, ElementCount VF);\n\n  /// The cost computation for widening instruction \\p I with consecutive\n  /// memory access.\n  InstructionCost getConsecutiveMemOpCost(Instruction *I, ElementCount VF);\n\n  /// The cost calculation for Load/Store instruction \\p I with uniform pointer -\n  /// Load: scalar load + broadcast.\n  /// Store: scalar store + (loop invariant value stored? 0 : extract of last\n  /// element)\n  InstructionCost getUniformMemOpCost(Instruction *I, ElementCount VF);\n\n  /// Estimate the overhead of scalarizing an instruction. This is a\n  /// convenience wrapper for the type-based getScalarizationOverhead API.\n  InstructionCost getScalarizationOverhead(Instruction *I, ElementCount VF);\n\n  /// Returns whether the instruction is a load or store and will be a emitted\n  /// as a vector operation.\n  bool isConsecutiveLoadOrStore(Instruction *I);\n\n  /// Returns true if an artificially high cost for emulated masked memrefs\n  /// should be used.\n  bool useEmulatedMaskMemRefHack(Instruction *I);\n\n  /// Map of scalar integer values to the smallest bitwidth they can be legally\n  /// represented as. The vector equivalents of these values should be truncated\n  /// to this type.\n  MapVector<Instruction *, uint64_t> MinBWs;\n\n  /// A type representing the costs for instructions if they were to be\n  /// scalarized rather than vectorized. The entries are Instruction-Cost\n  /// pairs.\n  using ScalarCostsTy = DenseMap<Instruction *, InstructionCost>;\n\n  /// A set containing all BasicBlocks that are known to present after\n  /// vectorization as a predicated block.\n  SmallPtrSet<BasicBlock *, 4> PredicatedBBsAfterVectorization;\n\n  /// Records whether it is allowed to have the original scalar loop execute at\n  /// least once. This may be needed as a fallback loop in case runtime\n  /// aliasing/dependence checks fail, or to handle the tail/remainder\n  /// iterations when the trip count is unknown or doesn't divide by the VF,\n  /// or as a peel-loop to handle gaps in interleave-groups.\n  /// Under optsize and when the trip count is very small we don't allow any\n  /// iterations to execute in the scalar loop.\n  ScalarEpilogueLowering ScalarEpilogueStatus = CM_ScalarEpilogueAllowed;\n\n  /// All blocks of loop are to be masked to fold tail of scalar iterations.\n  bool FoldTailByMasking = false;\n\n  /// A map holding scalar costs for different vectorization factors. The\n  /// presence of a cost for an instruction in the mapping indicates that the\n  /// instruction will be scalarized when vectorizing with the associated\n  /// vectorization factor. The entries are VF-ScalarCostTy pairs.\n  DenseMap<ElementCount, ScalarCostsTy> InstsToScalarize;\n\n  /// Holds the instructions known to be uniform after vectorization.\n  /// The data is collected per VF.\n  DenseMap<ElementCount, SmallPtrSet<Instruction *, 4>> Uniforms;\n\n  /// Holds the instructions known to be scalar after vectorization.\n  /// The data is collected per VF.\n  DenseMap<ElementCount, SmallPtrSet<Instruction *, 4>> Scalars;\n\n  /// Holds the instructions (address computations) that are forced to be\n  /// scalarized.\n  DenseMap<ElementCount, SmallPtrSet<Instruction *, 4>> ForcedScalars;\n\n  /// PHINodes of the reductions that should be expanded in-loop along with\n  /// their associated chains of reduction operations, in program order from top\n  /// (PHI) to bottom\n  ReductionChainMap InLoopReductionChains;\n\n  /// A Map of inloop reduction operations and their immediate chain operand.\n  /// FIXME: This can be removed once reductions can be costed correctly in\n  /// vplan. This was added to allow quick lookup to the inloop operations,\n  /// without having to loop through InLoopReductionChains.\n  DenseMap<Instruction *, Instruction *> InLoopReductionImmediateChains;\n\n  /// Returns the expected difference in cost from scalarizing the expression\n  /// feeding a predicated instruction \\p PredInst. The instructions to\n  /// scalarize and their scalar costs are collected in \\p ScalarCosts. A\n  /// non-negative return value implies the expression will be scalarized.\n  /// Currently, only single-use chains are considered for scalarization.\n  int computePredInstDiscount(Instruction *PredInst, ScalarCostsTy &ScalarCosts,\n                              ElementCount VF);\n\n  /// Collect the instructions that are uniform after vectorization. An\n  /// instruction is uniform if we represent it with a single scalar value in\n  /// the vectorized loop corresponding to each vector iteration. Examples of\n  /// uniform instructions include pointer operands of consecutive or\n  /// interleaved memory accesses. Note that although uniformity implies an\n  /// instruction will be scalar, the reverse is not true. In general, a\n  /// scalarized instruction will be represented by VF scalar values in the\n  /// vectorized loop, each corresponding to an iteration of the original\n  /// scalar loop.\n  void collectLoopUniforms(ElementCount VF);\n\n  /// Collect the instructions that are scalar after vectorization. An\n  /// instruction is scalar if it is known to be uniform or will be scalarized\n  /// during vectorization. Non-uniform scalarized instructions will be\n  /// represented by VF values in the vectorized loop, each corresponding to an\n  /// iteration of the original scalar loop.\n  void collectLoopScalars(ElementCount VF);\n\n  /// Keeps cost model vectorization decision and cost for instructions.\n  /// Right now it is used for memory instructions only.\n  using DecisionList = DenseMap<std::pair<Instruction *, ElementCount>,\n                                std::pair<InstWidening, InstructionCost>>;\n\n  DecisionList WideningDecisions;\n\n  /// Returns true if \\p V is expected to be vectorized and it needs to be\n  /// extracted.\n  bool needsExtract(Value *V, ElementCount VF) const {\n    Instruction *I = dyn_cast<Instruction>(V);\n    if (VF.isScalar() || !I || !TheLoop->contains(I) ||\n        TheLoop->isLoopInvariant(I))\n      return false;\n\n    // Assume we can vectorize V (and hence we need extraction) if the\n    // scalars are not computed yet. This can happen, because it is called\n    // via getScalarizationOverhead from setCostBasedWideningDecision, before\n    // the scalars are collected. That should be a safe assumption in most\n    // cases, because we check if the operands have vectorizable types\n    // beforehand in LoopVectorizationLegality.\n    return Scalars.find(VF) == Scalars.end() ||\n           !isScalarAfterVectorization(I, VF);\n  };\n\n  /// Returns a range containing only operands needing to be extracted.\n  SmallVector<Value *, 4> filterExtractingOperands(Instruction::op_range Ops,\n                                                   ElementCount VF) {\n    return SmallVector<Value *, 4>(make_filter_range(\n        Ops, [this, VF](Value *V) { return this->needsExtract(V, VF); }));\n  }\n\n  /// Determines if we have the infrastructure to vectorize loop \\p L and its\n  /// epilogue, assuming the main loop is vectorized by \\p VF.\n  bool isCandidateForEpilogueVectorization(const Loop &L,\n                                           const ElementCount VF) const;\n\n  /// Returns true if epilogue vectorization is considered profitable, and\n  /// false otherwise.\n  /// \\p VF is the vectorization factor chosen for the original loop.\n  bool isEpilogueVectorizationProfitable(const ElementCount VF) const;\n\npublic:\n  /// The loop that we evaluate.\n  Loop *TheLoop;\n\n  /// Predicated scalar evolution analysis.\n  PredicatedScalarEvolution &PSE;\n\n  /// Loop Info analysis.\n  LoopInfo *LI;\n\n  /// Vectorization legality.\n  LoopVectorizationLegality *Legal;\n\n  /// Vector target information.\n  const TargetTransformInfo &TTI;\n\n  /// Target Library Info.\n  const TargetLibraryInfo *TLI;\n\n  /// Demanded bits analysis.\n  DemandedBits *DB;\n\n  /// Assumption cache.\n  AssumptionCache *AC;\n\n  /// Interface to emit optimization remarks.\n  OptimizationRemarkEmitter *ORE;\n\n  const Function *TheFunction;\n\n  /// Loop Vectorize Hint.\n  const LoopVectorizeHints *Hints;\n\n  /// The interleave access information contains groups of interleaved accesses\n  /// with the same stride and close to each other.\n  InterleavedAccessInfo &InterleaveInfo;\n\n  /// Values to ignore in the cost model.\n  SmallPtrSet<const Value *, 16> ValuesToIgnore;\n\n  /// Values to ignore in the cost model when VF > 1.\n  SmallPtrSet<const Value *, 16> VecValuesToIgnore;\n\n  /// Profitable vector factors.\n  SmallVector<VectorizationFactor, 8> ProfitableVFs;\n};\n} // end namespace llvm\n\n/// Helper struct to manage generating runtime checks for vectorization.\n///\n/// The runtime checks are created up-front in temporary blocks to allow better\n/// estimating the cost and un-linked from the existing IR. After deciding to\n/// vectorize, the checks are moved back. If deciding not to vectorize, the\n/// temporary blocks are completely removed.\nclass GeneratedRTChecks {\n  /// Basic block which contains the generated SCEV checks, if any.\n  BasicBlock *SCEVCheckBlock = nullptr;\n\n  /// The value representing the result of the generated SCEV checks. If it is\n  /// nullptr, either no SCEV checks have been generated or they have been used.\n  Value *SCEVCheckCond = nullptr;\n\n  /// Basic block which contains the generated memory runtime checks, if any.\n  BasicBlock *MemCheckBlock = nullptr;\n\n  /// The value representing the result of the generated memory runtime checks.\n  /// If it is nullptr, either no memory runtime checks have been generated or\n  /// they have been used.\n  Instruction *MemRuntimeCheckCond = nullptr;\n\n  DominatorTree *DT;\n  LoopInfo *LI;\n\n  SCEVExpander SCEVExp;\n  SCEVExpander MemCheckExp;\n\npublic:\n  GeneratedRTChecks(ScalarEvolution &SE, DominatorTree *DT, LoopInfo *LI,\n                    const DataLayout &DL)\n      : DT(DT), LI(LI), SCEVExp(SE, DL, \"scev.check\"),\n        MemCheckExp(SE, DL, \"scev.check\") {}\n\n  /// Generate runtime checks in SCEVCheckBlock and MemCheckBlock, so we can\n  /// accurately estimate the cost of the runtime checks. The blocks are\n  /// un-linked from the IR and is added back during vector code generation. If\n  /// there is no vector code generation, the check blocks are removed\n  /// completely.\n  void Create(Loop *L, const LoopAccessInfo &LAI,\n              const SCEVUnionPredicate &UnionPred) {\n\n    BasicBlock *LoopHeader = L->getHeader();\n    BasicBlock *Preheader = L->getLoopPreheader();\n\n    // Use SplitBlock to create blocks for SCEV & memory runtime checks to\n    // ensure the blocks are properly added to LoopInfo & DominatorTree. Those\n    // may be used by SCEVExpander. The blocks will be un-linked from their\n    // predecessors and removed from LI & DT at the end of the function.\n    if (!UnionPred.isAlwaysTrue()) {\n      SCEVCheckBlock = SplitBlock(Preheader, Preheader->getTerminator(), DT, LI,\n                                  nullptr, \"vector.scevcheck\");\n\n      SCEVCheckCond = SCEVExp.expandCodeForPredicate(\n          &UnionPred, SCEVCheckBlock->getTerminator());\n    }\n\n    const auto &RtPtrChecking = *LAI.getRuntimePointerChecking();\n    if (RtPtrChecking.Need) {\n      auto *Pred = SCEVCheckBlock ? SCEVCheckBlock : Preheader;\n      MemCheckBlock = SplitBlock(Pred, Pred->getTerminator(), DT, LI, nullptr,\n                                 \"vector.memcheck\");\n\n      std::tie(std::ignore, MemRuntimeCheckCond) =\n          addRuntimeChecks(MemCheckBlock->getTerminator(), L,\n                           RtPtrChecking.getChecks(), MemCheckExp);\n      assert(MemRuntimeCheckCond &&\n             \"no RT checks generated although RtPtrChecking \"\n             \"claimed checks are required\");\n    }\n\n    if (!MemCheckBlock && !SCEVCheckBlock)\n      return;\n\n    // Unhook the temporary block with the checks, update various places\n    // accordingly.\n    if (SCEVCheckBlock)\n      SCEVCheckBlock->replaceAllUsesWith(Preheader);\n    if (MemCheckBlock)\n      MemCheckBlock->replaceAllUsesWith(Preheader);\n\n    if (SCEVCheckBlock) {\n      SCEVCheckBlock->getTerminator()->moveBefore(Preheader->getTerminator());\n      new UnreachableInst(Preheader->getContext(), SCEVCheckBlock);\n      Preheader->getTerminator()->eraseFromParent();\n    }\n    if (MemCheckBlock) {\n      MemCheckBlock->getTerminator()->moveBefore(Preheader->getTerminator());\n      new UnreachableInst(Preheader->getContext(), MemCheckBlock);\n      Preheader->getTerminator()->eraseFromParent();\n    }\n\n    DT->changeImmediateDominator(LoopHeader, Preheader);\n    if (MemCheckBlock) {\n      DT->eraseNode(MemCheckBlock);\n      LI->removeBlock(MemCheckBlock);\n    }\n    if (SCEVCheckBlock) {\n      DT->eraseNode(SCEVCheckBlock);\n      LI->removeBlock(SCEVCheckBlock);\n    }\n  }\n\n  /// Remove the created SCEV & memory runtime check blocks & instructions, if\n  /// unused.\n  ~GeneratedRTChecks() {\n    SCEVExpanderCleaner SCEVCleaner(SCEVExp, *DT);\n    SCEVExpanderCleaner MemCheckCleaner(MemCheckExp, *DT);\n    if (!SCEVCheckCond)\n      SCEVCleaner.markResultUsed();\n\n    if (!MemRuntimeCheckCond)\n      MemCheckCleaner.markResultUsed();\n\n    if (MemRuntimeCheckCond) {\n      auto &SE = *MemCheckExp.getSE();\n      // Memory runtime check generation creates compares that use expanded\n      // values. Remove them before running the SCEVExpanderCleaners.\n      for (auto &I : make_early_inc_range(reverse(*MemCheckBlock))) {\n        if (MemCheckExp.isInsertedInstruction(&I))\n          continue;\n        SE.forgetValue(&I);\n        SE.eraseValueFromMap(&I);\n        I.eraseFromParent();\n      }\n    }\n    MemCheckCleaner.cleanup();\n    SCEVCleaner.cleanup();\n\n    if (SCEVCheckCond)\n      SCEVCheckBlock->eraseFromParent();\n    if (MemRuntimeCheckCond)\n      MemCheckBlock->eraseFromParent();\n  }\n\n  /// Adds the generated SCEVCheckBlock before \\p LoopVectorPreHeader and\n  /// adjusts the branches to branch to the vector preheader or \\p Bypass,\n  /// depending on the generated condition.\n  BasicBlock *emitSCEVChecks(Loop *L, BasicBlock *Bypass,\n                             BasicBlock *LoopVectorPreHeader,\n                             BasicBlock *LoopExitBlock) {\n    if (!SCEVCheckCond)\n      return nullptr;\n    if (auto *C = dyn_cast<ConstantInt>(SCEVCheckCond))\n      if (C->isZero())\n        return nullptr;\n\n    auto *Pred = LoopVectorPreHeader->getSinglePredecessor();\n\n    BranchInst::Create(LoopVectorPreHeader, SCEVCheckBlock);\n    // Create new preheader for vector loop.\n    if (auto *PL = LI->getLoopFor(LoopVectorPreHeader))\n      PL->addBasicBlockToLoop(SCEVCheckBlock, *LI);\n\n    SCEVCheckBlock->getTerminator()->eraseFromParent();\n    SCEVCheckBlock->moveBefore(LoopVectorPreHeader);\n    Pred->getTerminator()->replaceSuccessorWith(LoopVectorPreHeader,\n                                                SCEVCheckBlock);\n\n    DT->addNewBlock(SCEVCheckBlock, Pred);\n    DT->changeImmediateDominator(LoopVectorPreHeader, SCEVCheckBlock);\n\n    ReplaceInstWithInst(\n        SCEVCheckBlock->getTerminator(),\n        BranchInst::Create(Bypass, LoopVectorPreHeader, SCEVCheckCond));\n    // Mark the check as used, to prevent it from being removed during cleanup.\n    SCEVCheckCond = nullptr;\n    return SCEVCheckBlock;\n  }\n\n  /// Adds the generated MemCheckBlock before \\p LoopVectorPreHeader and adjusts\n  /// the branches to branch to the vector preheader or \\p Bypass, depending on\n  /// the generated condition.\n  BasicBlock *emitMemRuntimeChecks(Loop *L, BasicBlock *Bypass,\n                                   BasicBlock *LoopVectorPreHeader) {\n    // Check if we generated code that checks in runtime if arrays overlap.\n    if (!MemRuntimeCheckCond)\n      return nullptr;\n\n    auto *Pred = LoopVectorPreHeader->getSinglePredecessor();\n    Pred->getTerminator()->replaceSuccessorWith(LoopVectorPreHeader,\n                                                MemCheckBlock);\n\n    DT->addNewBlock(MemCheckBlock, Pred);\n    DT->changeImmediateDominator(LoopVectorPreHeader, MemCheckBlock);\n    MemCheckBlock->moveBefore(LoopVectorPreHeader);\n\n    if (auto *PL = LI->getLoopFor(LoopVectorPreHeader))\n      PL->addBasicBlockToLoop(MemCheckBlock, *LI);\n\n    ReplaceInstWithInst(\n        MemCheckBlock->getTerminator(),\n        BranchInst::Create(Bypass, LoopVectorPreHeader, MemRuntimeCheckCond));\n    MemCheckBlock->getTerminator()->setDebugLoc(\n        Pred->getTerminator()->getDebugLoc());\n\n    // Mark the check as used, to prevent it from being removed during cleanup.\n    MemRuntimeCheckCond = nullptr;\n    return MemCheckBlock;\n  }\n};\n\n// Return true if \\p OuterLp is an outer loop annotated with hints for explicit\n// vectorization. The loop needs to be annotated with #pragma omp simd\n// simdlen(#) or #pragma clang vectorize(enable) vectorize_width(#). If the\n// vector length information is not provided, vectorization is not considered\n// explicit. Interleave hints are not allowed either. These limitations will be\n// relaxed in the future.\n// Please, note that we are currently forced to abuse the pragma 'clang\n// vectorize' semantics. This pragma provides *auto-vectorization hints*\n// (i.e., LV must check that vectorization is legal) whereas pragma 'omp simd'\n// provides *explicit vectorization hints* (LV can bypass legal checks and\n// assume that vectorization is legal). However, both hints are implemented\n// using the same metadata (llvm.loop.vectorize, processed by\n// LoopVectorizeHints). This will be fixed in the future when the native IR\n// representation for pragma 'omp simd' is introduced.\nstatic bool isExplicitVecOuterLoop(Loop *OuterLp,\n                                   OptimizationRemarkEmitter *ORE) {\n  assert(!OuterLp->isInnermost() && \"This is not an outer loop\");\n  LoopVectorizeHints Hints(OuterLp, true /*DisableInterleaving*/, *ORE);\n\n  // Only outer loops with an explicit vectorization hint are supported.\n  // Unannotated outer loops are ignored.\n  if (Hints.getForce() == LoopVectorizeHints::FK_Undefined)\n    return false;\n\n  Function *Fn = OuterLp->getHeader()->getParent();\n  if (!Hints.allowVectorization(Fn, OuterLp,\n                                true /*VectorizeOnlyWhenForced*/)) {\n    LLVM_DEBUG(dbgs() << \"LV: Loop hints prevent outer loop vectorization.\\n\");\n    return false;\n  }\n\n  if (Hints.getInterleave() > 1) {\n    // TODO: Interleave support is future work.\n    LLVM_DEBUG(dbgs() << \"LV: Not vectorizing: Interleave is not supported for \"\n                         \"outer loops.\\n\");\n    Hints.emitRemarkWithHints();\n    return false;\n  }\n\n  return true;\n}\n\nstatic void collectSupportedLoops(Loop &L, LoopInfo *LI,\n                                  OptimizationRemarkEmitter *ORE,\n                                  SmallVectorImpl<Loop *> &V) {\n  // Collect inner loops and outer loops without irreducible control flow. For\n  // now, only collect outer loops that have explicit vectorization hints. If we\n  // are stress testing the VPlan H-CFG construction, we collect the outermost\n  // loop of every loop nest.\n  if (L.isInnermost() || VPlanBuildStressTest ||\n      (EnableVPlanNativePath && isExplicitVecOuterLoop(&L, ORE))) {\n    LoopBlocksRPO RPOT(&L);\n    RPOT.perform(LI);\n    if (!containsIrreducibleCFG<const BasicBlock *>(RPOT, *LI)) {\n      V.push_back(&L);\n      // TODO: Collect inner loops inside marked outer loops in case\n      // vectorization fails for the outer loop. Do not invoke\n      // 'containsIrreducibleCFG' again for inner loops when the outer loop is\n      // already known to be reducible. We can use an inherited attribute for\n      // that.\n      return;\n    }\n  }\n  for (Loop *InnerL : L)\n    collectSupportedLoops(*InnerL, LI, ORE, V);\n}\n\nnamespace {\n\n/// The LoopVectorize Pass.\nstruct LoopVectorize : public FunctionPass {\n  /// Pass identification, replacement for typeid\n  static char ID;\n\n  LoopVectorizePass Impl;\n\n  explicit LoopVectorize(bool InterleaveOnlyWhenForced = false,\n                         bool VectorizeOnlyWhenForced = false)\n      : FunctionPass(ID),\n        Impl({InterleaveOnlyWhenForced, VectorizeOnlyWhenForced}) {\n    initializeLoopVectorizePass(*PassRegistry::getPassRegistry());\n  }\n\n  bool runOnFunction(Function &F) override {\n    if (skipFunction(F))\n      return false;\n\n    auto *SE = &getAnalysis<ScalarEvolutionWrapperPass>().getSE();\n    auto *LI = &getAnalysis<LoopInfoWrapperPass>().getLoopInfo();\n    auto *TTI = &getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);\n    auto *DT = &getAnalysis<DominatorTreeWrapperPass>().getDomTree();\n    auto *BFI = &getAnalysis<BlockFrequencyInfoWrapperPass>().getBFI();\n    auto *TLIP = getAnalysisIfAvailable<TargetLibraryInfoWrapperPass>();\n    auto *TLI = TLIP ? &TLIP->getTLI(F) : nullptr;\n    auto *AA = &getAnalysis<AAResultsWrapperPass>().getAAResults();\n    auto *AC = &getAnalysis<AssumptionCacheTracker>().getAssumptionCache(F);\n    auto *LAA = &getAnalysis<LoopAccessLegacyAnalysis>();\n    auto *DB = &getAnalysis<DemandedBitsWrapperPass>().getDemandedBits();\n    auto *ORE = &getAnalysis<OptimizationRemarkEmitterWrapperPass>().getORE();\n    auto *PSI = &getAnalysis<ProfileSummaryInfoWrapperPass>().getPSI();\n\n    std::function<const LoopAccessInfo &(Loop &)> GetLAA =\n        [&](Loop &L) -> const LoopAccessInfo & { return LAA->getInfo(&L); };\n\n    return Impl.runImpl(F, *SE, *LI, *TTI, *DT, *BFI, TLI, *DB, *AA, *AC,\n                        GetLAA, *ORE, PSI).MadeAnyChange;\n  }\n\n  void getAnalysisUsage(AnalysisUsage &AU) const override {\n    AU.addRequired<AssumptionCacheTracker>();\n    AU.addRequired<BlockFrequencyInfoWrapperPass>();\n    AU.addRequired<DominatorTreeWrapperPass>();\n    AU.addRequired<LoopInfoWrapperPass>();\n    AU.addRequired<ScalarEvolutionWrapperPass>();\n    AU.addRequired<TargetTransformInfoWrapperPass>();\n    AU.addRequired<AAResultsWrapperPass>();\n    AU.addRequired<LoopAccessLegacyAnalysis>();\n    AU.addRequired<DemandedBitsWrapperPass>();\n    AU.addRequired<OptimizationRemarkEmitterWrapperPass>();\n    AU.addRequired<InjectTLIMappingsLegacy>();\n\n    // We currently do not preserve loopinfo/dominator analyses with outer loop\n    // vectorization. Until this is addressed, mark these analyses as preserved\n    // only for non-VPlan-native path.\n    // TODO: Preserve Loop and Dominator analyses for VPlan-native path.\n    if (!EnableVPlanNativePath) {\n      AU.addPreserved<LoopInfoWrapperPass>();\n      AU.addPreserved<DominatorTreeWrapperPass>();\n    }\n\n    AU.addPreserved<BasicAAWrapperPass>();\n    AU.addPreserved<GlobalsAAWrapperPass>();\n    AU.addRequired<ProfileSummaryInfoWrapperPass>();\n  }\n};\n\n} // end anonymous namespace\n\n//===----------------------------------------------------------------------===//\n// Implementation of LoopVectorizationLegality, InnerLoopVectorizer and\n// LoopVectorizationCostModel and LoopVectorizationPlanner.\n//===----------------------------------------------------------------------===//\n\nValue *InnerLoopVectorizer::getBroadcastInstrs(Value *V) {\n  // We need to place the broadcast of invariant variables outside the loop,\n  // but only if it's proven safe to do so. Else, broadcast will be inside\n  // vector loop body.\n  Instruction *Instr = dyn_cast<Instruction>(V);\n  bool SafeToHoist = OrigLoop->isLoopInvariant(V) &&\n                     (!Instr ||\n                      DT->dominates(Instr->getParent(), LoopVectorPreHeader));\n  // Place the code for broadcasting invariant variables in the new preheader.\n  IRBuilder<>::InsertPointGuard Guard(Builder);\n  if (SafeToHoist)\n    Builder.SetInsertPoint(LoopVectorPreHeader->getTerminator());\n\n  // Broadcast the scalar into all locations in the vector.\n  Value *Shuf = Builder.CreateVectorSplat(VF, V, \"broadcast\");\n\n  return Shuf;\n}\n\nvoid InnerLoopVectorizer::createVectorIntOrFpInductionPHI(\n    const InductionDescriptor &II, Value *Step, Value *Start,\n    Instruction *EntryVal, VPValue *Def, VPValue *CastDef,\n    VPTransformState &State) {\n  assert((isa<PHINode>(EntryVal) || isa<TruncInst>(EntryVal)) &&\n         \"Expected either an induction phi-node or a truncate of it!\");\n\n  // Construct the initial value of the vector IV in the vector loop preheader\n  auto CurrIP = Builder.saveIP();\n  Builder.SetInsertPoint(LoopVectorPreHeader->getTerminator());\n  if (isa<TruncInst>(EntryVal)) {\n    assert(Start->getType()->isIntegerTy() &&\n           \"Truncation requires an integer type\");\n    auto *TruncType = cast<IntegerType>(EntryVal->getType());\n    Step = Builder.CreateTrunc(Step, TruncType);\n    Start = Builder.CreateCast(Instruction::Trunc, Start, TruncType);\n  }\n  Value *SplatStart = Builder.CreateVectorSplat(VF, Start);\n  Value *SteppedStart =\n      getStepVector(SplatStart, 0, Step, II.getInductionOpcode());\n\n  // We create vector phi nodes for both integer and floating-point induction\n  // variables. Here, we determine the kind of arithmetic we will perform.\n  Instruction::BinaryOps AddOp;\n  Instruction::BinaryOps MulOp;\n  if (Step->getType()->isIntegerTy()) {\n    AddOp = Instruction::Add;\n    MulOp = Instruction::Mul;\n  } else {\n    AddOp = II.getInductionOpcode();\n    MulOp = Instruction::FMul;\n  }\n\n  // Multiply the vectorization factor by the step using integer or\n  // floating-point arithmetic as appropriate.\n  Value *ConstVF =\n      getSignedIntOrFpConstant(Step->getType(), VF.getKnownMinValue());\n  Value *Mul = Builder.CreateBinOp(MulOp, Step, ConstVF);\n\n  // Create a vector splat to use in the induction update.\n  //\n  // FIXME: If the step is non-constant, we create the vector splat with\n  //        IRBuilder. IRBuilder can constant-fold the multiply, but it doesn't\n  //        handle a constant vector splat.\n  assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n  Value *SplatVF = isa<Constant>(Mul)\n                       ? ConstantVector::getSplat(VF, cast<Constant>(Mul))\n                       : Builder.CreateVectorSplat(VF, Mul);\n  Builder.restoreIP(CurrIP);\n\n  // We may need to add the step a number of times, depending on the unroll\n  // factor. The last of those goes into the PHI.\n  PHINode *VecInd = PHINode::Create(SteppedStart->getType(), 2, \"vec.ind\",\n                                    &*LoopVectorBody->getFirstInsertionPt());\n  VecInd->setDebugLoc(EntryVal->getDebugLoc());\n  Instruction *LastInduction = VecInd;\n  for (unsigned Part = 0; Part < UF; ++Part) {\n    State.set(Def, LastInduction, Part);\n\n    if (isa<TruncInst>(EntryVal))\n      addMetadata(LastInduction, EntryVal);\n    recordVectorLoopValueForInductionCast(II, EntryVal, LastInduction, CastDef,\n                                          State, Part);\n\n    LastInduction = cast<Instruction>(\n        Builder.CreateBinOp(AddOp, LastInduction, SplatVF, \"step.add\"));\n    LastInduction->setDebugLoc(EntryVal->getDebugLoc());\n  }\n\n  // Move the last step to the end of the latch block. This ensures consistent\n  // placement of all induction updates.\n  auto *LoopVectorLatch = LI->getLoopFor(LoopVectorBody)->getLoopLatch();\n  auto *Br = cast<BranchInst>(LoopVectorLatch->getTerminator());\n  auto *ICmp = cast<Instruction>(Br->getCondition());\n  LastInduction->moveBefore(ICmp);\n  LastInduction->setName(\"vec.ind.next\");\n\n  VecInd->addIncoming(SteppedStart, LoopVectorPreHeader);\n  VecInd->addIncoming(LastInduction, LoopVectorLatch);\n}\n\nbool InnerLoopVectorizer::shouldScalarizeInstruction(Instruction *I) const {\n  return Cost->isScalarAfterVectorization(I, VF) ||\n         Cost->isProfitableToScalarize(I, VF);\n}\n\nbool InnerLoopVectorizer::needsScalarInduction(Instruction *IV) const {\n  if (shouldScalarizeInstruction(IV))\n    return true;\n  auto isScalarInst = [&](User *U) -> bool {\n    auto *I = cast<Instruction>(U);\n    return (OrigLoop->contains(I) && shouldScalarizeInstruction(I));\n  };\n  return llvm::any_of(IV->users(), isScalarInst);\n}\n\nvoid InnerLoopVectorizer::recordVectorLoopValueForInductionCast(\n    const InductionDescriptor &ID, const Instruction *EntryVal,\n    Value *VectorLoopVal, VPValue *CastDef, VPTransformState &State,\n    unsigned Part, unsigned Lane) {\n  assert((isa<PHINode>(EntryVal) || isa<TruncInst>(EntryVal)) &&\n         \"Expected either an induction phi-node or a truncate of it!\");\n\n  // This induction variable is not the phi from the original loop but the\n  // newly-created IV based on the proof that casted Phi is equal to the\n  // uncasted Phi in the vectorized loop (under a runtime guard possibly). It\n  // re-uses the same InductionDescriptor that original IV uses but we don't\n  // have to do any recording in this case - that is done when original IV is\n  // processed.\n  if (isa<TruncInst>(EntryVal))\n    return;\n\n  const SmallVectorImpl<Instruction *> &Casts = ID.getCastInsts();\n  if (Casts.empty())\n    return;\n  // Only the first Cast instruction in the Casts vector is of interest.\n  // The rest of the Casts (if exist) have no uses outside the\n  // induction update chain itself.\n  if (Lane < UINT_MAX)\n    State.set(CastDef, VectorLoopVal, VPIteration(Part, Lane));\n  else\n    State.set(CastDef, VectorLoopVal, Part);\n}\n\nvoid InnerLoopVectorizer::widenIntOrFpInduction(PHINode *IV, Value *Start,\n                                                TruncInst *Trunc, VPValue *Def,\n                                                VPValue *CastDef,\n                                                VPTransformState &State) {\n  assert((IV->getType()->isIntegerTy() || IV != OldInduction) &&\n         \"Primary induction variable must have an integer type\");\n\n  auto II = Legal->getInductionVars().find(IV);\n  assert(II != Legal->getInductionVars().end() && \"IV is not an induction\");\n\n  auto ID = II->second;\n  assert(IV->getType() == ID.getStartValue()->getType() && \"Types must match\");\n\n  // The value from the original loop to which we are mapping the new induction\n  // variable.\n  Instruction *EntryVal = Trunc ? cast<Instruction>(Trunc) : IV;\n\n  auto &DL = OrigLoop->getHeader()->getModule()->getDataLayout();\n\n  // Generate code for the induction step. Note that induction steps are\n  // required to be loop-invariant\n  auto CreateStepValue = [&](const SCEV *Step) -> Value * {\n    assert(PSE.getSE()->isLoopInvariant(Step, OrigLoop) &&\n           \"Induction step should be loop invariant\");\n    if (PSE.getSE()->isSCEVable(IV->getType())) {\n      SCEVExpander Exp(*PSE.getSE(), DL, \"induction\");\n      return Exp.expandCodeFor(Step, Step->getType(),\n                               LoopVectorPreHeader->getTerminator());\n    }\n    return cast<SCEVUnknown>(Step)->getValue();\n  };\n\n  // The scalar value to broadcast. This is derived from the canonical\n  // induction variable. If a truncation type is given, truncate the canonical\n  // induction variable and step. Otherwise, derive these values from the\n  // induction descriptor.\n  auto CreateScalarIV = [&](Value *&Step) -> Value * {\n    Value *ScalarIV = Induction;\n    if (IV != OldInduction) {\n      ScalarIV = IV->getType()->isIntegerTy()\n                     ? Builder.CreateSExtOrTrunc(Induction, IV->getType())\n                     : Builder.CreateCast(Instruction::SIToFP, Induction,\n                                          IV->getType());\n      ScalarIV = emitTransformedIndex(Builder, ScalarIV, PSE.getSE(), DL, ID);\n      ScalarIV->setName(\"offset.idx\");\n    }\n    if (Trunc) {\n      auto *TruncType = cast<IntegerType>(Trunc->getType());\n      assert(Step->getType()->isIntegerTy() &&\n             \"Truncation requires an integer step\");\n      ScalarIV = Builder.CreateTrunc(ScalarIV, TruncType);\n      Step = Builder.CreateTrunc(Step, TruncType);\n    }\n    return ScalarIV;\n  };\n\n  // Create the vector values from the scalar IV, in the absence of creating a\n  // vector IV.\n  auto CreateSplatIV = [&](Value *ScalarIV, Value *Step) {\n    Value *Broadcasted = getBroadcastInstrs(ScalarIV);\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n      Value *EntryPart =\n          getStepVector(Broadcasted, VF.getKnownMinValue() * Part, Step,\n                        ID.getInductionOpcode());\n      State.set(Def, EntryPart, Part);\n      if (Trunc)\n        addMetadata(EntryPart, Trunc);\n      recordVectorLoopValueForInductionCast(ID, EntryVal, EntryPart, CastDef,\n                                            State, Part);\n    }\n  };\n\n  // Fast-math-flags propagate from the original induction instruction.\n  IRBuilder<>::FastMathFlagGuard FMFG(Builder);\n  if (ID.getInductionBinOp() && isa<FPMathOperator>(ID.getInductionBinOp()))\n    Builder.setFastMathFlags(ID.getInductionBinOp()->getFastMathFlags());\n\n  // Now do the actual transformations, and start with creating the step value.\n  Value *Step = CreateStepValue(ID.getStep());\n  if (VF.isZero() || VF.isScalar()) {\n    Value *ScalarIV = CreateScalarIV(Step);\n    CreateSplatIV(ScalarIV, Step);\n    return;\n  }\n\n  // Determine if we want a scalar version of the induction variable. This is\n  // true if the induction variable itself is not widened, or if it has at\n  // least one user in the loop that is not widened.\n  auto NeedsScalarIV = needsScalarInduction(EntryVal);\n  if (!NeedsScalarIV) {\n    createVectorIntOrFpInductionPHI(ID, Step, Start, EntryVal, Def, CastDef,\n                                    State);\n    return;\n  }\n\n  // Try to create a new independent vector induction variable. If we can't\n  // create the phi node, we will splat the scalar induction variable in each\n  // loop iteration.\n  if (!shouldScalarizeInstruction(EntryVal)) {\n    createVectorIntOrFpInductionPHI(ID, Step, Start, EntryVal, Def, CastDef,\n                                    State);\n    Value *ScalarIV = CreateScalarIV(Step);\n    // Create scalar steps that can be used by instructions we will later\n    // scalarize. Note that the addition of the scalar steps will not increase\n    // the number of instructions in the loop in the common case prior to\n    // InstCombine. We will be trading one vector extract for each scalar step.\n    buildScalarSteps(ScalarIV, Step, EntryVal, ID, Def, CastDef, State);\n    return;\n  }\n\n  // All IV users are scalar instructions, so only emit a scalar IV, not a\n  // vectorised IV. Except when we tail-fold, then the splat IV feeds the\n  // predicate used by the masked loads/stores.\n  Value *ScalarIV = CreateScalarIV(Step);\n  if (!Cost->isScalarEpilogueAllowed())\n    CreateSplatIV(ScalarIV, Step);\n  buildScalarSteps(ScalarIV, Step, EntryVal, ID, Def, CastDef, State);\n}\n\nValue *InnerLoopVectorizer::getStepVector(Value *Val, int StartIdx, Value *Step,\n                                          Instruction::BinaryOps BinOp) {\n  // Create and check the types.\n  auto *ValVTy = cast<FixedVectorType>(Val->getType());\n  int VLen = ValVTy->getNumElements();\n\n  Type *STy = Val->getType()->getScalarType();\n  assert((STy->isIntegerTy() || STy->isFloatingPointTy()) &&\n         \"Induction Step must be an integer or FP\");\n  assert(Step->getType() == STy && \"Step has wrong type\");\n\n  SmallVector<Constant *, 8> Indices;\n\n  if (STy->isIntegerTy()) {\n    // Create a vector of consecutive numbers from zero to VF.\n    for (int i = 0; i < VLen; ++i)\n      Indices.push_back(ConstantInt::get(STy, StartIdx + i));\n\n    // Add the consecutive indices to the vector value.\n    Constant *Cv = ConstantVector::get(Indices);\n    assert(Cv->getType() == Val->getType() && \"Invalid consecutive vec\");\n    Step = Builder.CreateVectorSplat(VLen, Step);\n    assert(Step->getType() == Val->getType() && \"Invalid step vec\");\n    // FIXME: The newly created binary instructions should contain nsw/nuw flags,\n    // which can be found from the original scalar operations.\n    Step = Builder.CreateMul(Cv, Step);\n    return Builder.CreateAdd(Val, Step, \"induction\");\n  }\n\n  // Floating point induction.\n  assert((BinOp == Instruction::FAdd || BinOp == Instruction::FSub) &&\n         \"Binary Opcode should be specified for FP induction\");\n  // Create a vector of consecutive numbers from zero to VF.\n  for (int i = 0; i < VLen; ++i)\n    Indices.push_back(ConstantFP::get(STy, (double)(StartIdx + i)));\n\n  // Add the consecutive indices to the vector value.\n  // Floating-point operations inherit FMF via the builder's flags.\n  Constant *Cv = ConstantVector::get(Indices);\n  Step = Builder.CreateVectorSplat(VLen, Step);\n  Value *MulOp = Builder.CreateFMul(Cv, Step);\n  return Builder.CreateBinOp(BinOp, Val, MulOp, \"induction\");\n}\n\nvoid InnerLoopVectorizer::buildScalarSteps(Value *ScalarIV, Value *Step,\n                                           Instruction *EntryVal,\n                                           const InductionDescriptor &ID,\n                                           VPValue *Def, VPValue *CastDef,\n                                           VPTransformState &State) {\n  // We shouldn't have to build scalar steps if we aren't vectorizing.\n  assert(VF.isVector() && \"VF should be greater than one\");\n  // Get the value type and ensure it and the step have the same integer type.\n  Type *ScalarIVTy = ScalarIV->getType()->getScalarType();\n  assert(ScalarIVTy == Step->getType() &&\n         \"Val and Step should have the same type\");\n\n  // We build scalar steps for both integer and floating-point induction\n  // variables. Here, we determine the kind of arithmetic we will perform.\n  Instruction::BinaryOps AddOp;\n  Instruction::BinaryOps MulOp;\n  if (ScalarIVTy->isIntegerTy()) {\n    AddOp = Instruction::Add;\n    MulOp = Instruction::Mul;\n  } else {\n    AddOp = ID.getInductionOpcode();\n    MulOp = Instruction::FMul;\n  }\n\n  // Determine the number of scalars we need to generate for each unroll\n  // iteration. If EntryVal is uniform, we only need to generate the first\n  // lane. Otherwise, we generate all VF values.\n  unsigned Lanes =\n      Cost->isUniformAfterVectorization(cast<Instruction>(EntryVal), VF)\n          ? 1\n          : VF.getKnownMinValue();\n  assert((!VF.isScalable() || Lanes == 1) &&\n         \"Should never scalarize a scalable vector\");\n  // Compute the scalar steps and save the results in State.\n  for (unsigned Part = 0; Part < UF; ++Part) {\n    for (unsigned Lane = 0; Lane < Lanes; ++Lane) {\n      auto *IntStepTy = IntegerType::get(ScalarIVTy->getContext(),\n                                         ScalarIVTy->getScalarSizeInBits());\n      Value *StartIdx =\n          createStepForVF(Builder, ConstantInt::get(IntStepTy, Part), VF);\n      if (ScalarIVTy->isFloatingPointTy())\n        StartIdx = Builder.CreateSIToFP(StartIdx, ScalarIVTy);\n      StartIdx = Builder.CreateBinOp(\n          AddOp, StartIdx, getSignedIntOrFpConstant(ScalarIVTy, Lane));\n      // The step returned by `createStepForVF` is a runtime-evaluated value\n      // when VF is scalable. Otherwise, it should be folded into a Constant.\n      assert((VF.isScalable() || isa<Constant>(StartIdx)) &&\n             \"Expected StartIdx to be folded to a constant when VF is not \"\n             \"scalable\");\n      auto *Mul = Builder.CreateBinOp(MulOp, StartIdx, Step);\n      auto *Add = Builder.CreateBinOp(AddOp, ScalarIV, Mul);\n      State.set(Def, Add, VPIteration(Part, Lane));\n      recordVectorLoopValueForInductionCast(ID, EntryVal, Add, CastDef, State,\n                                            Part, Lane);\n    }\n  }\n}\n\nvoid InnerLoopVectorizer::packScalarIntoVectorValue(VPValue *Def,\n                                                    const VPIteration &Instance,\n                                                    VPTransformState &State) {\n  Value *ScalarInst = State.get(Def, Instance);\n  Value *VectorValue = State.get(Def, Instance.Part);\n  VectorValue = Builder.CreateInsertElement(\n      VectorValue, ScalarInst,\n      Instance.Lane.getAsRuntimeExpr(State.Builder, VF));\n  State.set(Def, VectorValue, Instance.Part);\n}\n\nValue *InnerLoopVectorizer::reverseVector(Value *Vec) {\n  assert(Vec->getType()->isVectorTy() && \"Invalid type\");\n  assert(!VF.isScalable() && \"Cannot reverse scalable vectors\");\n  SmallVector<int, 8> ShuffleMask;\n  for (unsigned i = 0; i < VF.getKnownMinValue(); ++i)\n    ShuffleMask.push_back(VF.getKnownMinValue() - i - 1);\n\n  return Builder.CreateShuffleVector(Vec, ShuffleMask, \"reverse\");\n}\n\n// Return whether we allow using masked interleave-groups (for dealing with\n// strided loads/stores that reside in predicated blocks, or for dealing\n// with gaps).\nstatic bool useMaskedInterleavedAccesses(const TargetTransformInfo &TTI) {\n  // If an override option has been passed in for interleaved accesses, use it.\n  if (EnableMaskedInterleavedMemAccesses.getNumOccurrences() > 0)\n    return EnableMaskedInterleavedMemAccesses;\n\n  return TTI.enableMaskedInterleavedAccessVectorization();\n}\n\n// Try to vectorize the interleave group that \\p Instr belongs to.\n//\n// E.g. Translate following interleaved load group (factor = 3):\n//   for (i = 0; i < N; i+=3) {\n//     R = Pic[i];             // Member of index 0\n//     G = Pic[i+1];           // Member of index 1\n//     B = Pic[i+2];           // Member of index 2\n//     ... // do something to R, G, B\n//   }\n// To:\n//   %wide.vec = load <12 x i32>                       ; Read 4 tuples of R,G,B\n//   %R.vec = shuffle %wide.vec, poison, <0, 3, 6, 9>   ; R elements\n//   %G.vec = shuffle %wide.vec, poison, <1, 4, 7, 10>  ; G elements\n//   %B.vec = shuffle %wide.vec, poison, <2, 5, 8, 11>  ; B elements\n//\n// Or translate following interleaved store group (factor = 3):\n//   for (i = 0; i < N; i+=3) {\n//     ... do something to R, G, B\n//     Pic[i]   = R;           // Member of index 0\n//     Pic[i+1] = G;           // Member of index 1\n//     Pic[i+2] = B;           // Member of index 2\n//   }\n// To:\n//   %R_G.vec = shuffle %R.vec, %G.vec, <0, 1, 2, ..., 7>\n//   %B_U.vec = shuffle %B.vec, poison, <0, 1, 2, 3, u, u, u, u>\n//   %interleaved.vec = shuffle %R_G.vec, %B_U.vec,\n//        <0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11>    ; Interleave R,G,B elements\n//   store <12 x i32> %interleaved.vec              ; Write 4 tuples of R,G,B\nvoid InnerLoopVectorizer::vectorizeInterleaveGroup(\n    const InterleaveGroup<Instruction> *Group, ArrayRef<VPValue *> VPDefs,\n    VPTransformState &State, VPValue *Addr, ArrayRef<VPValue *> StoredValues,\n    VPValue *BlockInMask) {\n  Instruction *Instr = Group->getInsertPos();\n  const DataLayout &DL = Instr->getModule()->getDataLayout();\n\n  // Prepare for the vector type of the interleaved load/store.\n  Type *ScalarTy = getMemInstValueType(Instr);\n  unsigned InterleaveFactor = Group->getFactor();\n  assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n  auto *VecTy = VectorType::get(ScalarTy, VF * InterleaveFactor);\n\n  // Prepare for the new pointers.\n  SmallVector<Value *, 2> AddrParts;\n  unsigned Index = Group->getIndex(Instr);\n\n  // TODO: extend the masked interleaved-group support to reversed access.\n  assert((!BlockInMask || !Group->isReverse()) &&\n         \"Reversed masked interleave-group not supported.\");\n\n  // If the group is reverse, adjust the index to refer to the last vector lane\n  // instead of the first. We adjust the index from the first vector lane,\n  // rather than directly getting the pointer for lane VF - 1, because the\n  // pointer operand of the interleaved access is supposed to be uniform. For\n  // uniform instructions, we're only required to generate a value for the\n  // first vector lane in each unroll iteration.\n  assert(!VF.isScalable() &&\n         \"scalable vector reverse operation is not implemented\");\n  if (Group->isReverse())\n    Index += (VF.getKnownMinValue() - 1) * Group->getFactor();\n\n  for (unsigned Part = 0; Part < UF; Part++) {\n    Value *AddrPart = State.get(Addr, VPIteration(Part, 0));\n    setDebugLocFromInst(Builder, AddrPart);\n\n    // Notice current instruction could be any index. Need to adjust the address\n    // to the member of index 0.\n    //\n    // E.g.  a = A[i+1];     // Member of index 1 (Current instruction)\n    //       b = A[i];       // Member of index 0\n    // Current pointer is pointed to A[i+1], adjust it to A[i].\n    //\n    // E.g.  A[i+1] = a;     // Member of index 1\n    //       A[i]   = b;     // Member of index 0\n    //       A[i+2] = c;     // Member of index 2 (Current instruction)\n    // Current pointer is pointed to A[i+2], adjust it to A[i].\n\n    bool InBounds = false;\n    if (auto *gep = dyn_cast<GetElementPtrInst>(AddrPart->stripPointerCasts()))\n      InBounds = gep->isInBounds();\n    AddrPart = Builder.CreateGEP(ScalarTy, AddrPart, Builder.getInt32(-Index));\n    cast<GetElementPtrInst>(AddrPart)->setIsInBounds(InBounds);\n\n    // Cast to the vector pointer type.\n    unsigned AddressSpace = AddrPart->getType()->getPointerAddressSpace();\n    Type *PtrTy = VecTy->getPointerTo(AddressSpace);\n    AddrParts.push_back(Builder.CreateBitCast(AddrPart, PtrTy));\n  }\n\n  setDebugLocFromInst(Builder, Instr);\n  Value *PoisonVec = PoisonValue::get(VecTy);\n\n  Value *MaskForGaps = nullptr;\n  if (Group->requiresScalarEpilogue() && !Cost->isScalarEpilogueAllowed()) {\n    assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n    MaskForGaps = createBitMaskForGaps(Builder, VF.getKnownMinValue(), *Group);\n    assert(MaskForGaps && \"Mask for Gaps is required but it is null\");\n  }\n\n  // Vectorize the interleaved load group.\n  if (isa<LoadInst>(Instr)) {\n    // For each unroll part, create a wide load for the group.\n    SmallVector<Value *, 2> NewLoads;\n    for (unsigned Part = 0; Part < UF; Part++) {\n      Instruction *NewLoad;\n      if (BlockInMask || MaskForGaps) {\n        assert(useMaskedInterleavedAccesses(*TTI) &&\n               \"masked interleaved groups are not allowed.\");\n        Value *GroupMask = MaskForGaps;\n        if (BlockInMask) {\n          Value *BlockInMaskPart = State.get(BlockInMask, Part);\n          assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n          Value *ShuffledMask = Builder.CreateShuffleVector(\n              BlockInMaskPart,\n              createReplicatedMask(InterleaveFactor, VF.getKnownMinValue()),\n              \"interleaved.mask\");\n          GroupMask = MaskForGaps\n                          ? Builder.CreateBinOp(Instruction::And, ShuffledMask,\n                                                MaskForGaps)\n                          : ShuffledMask;\n        }\n        NewLoad =\n            Builder.CreateMaskedLoad(AddrParts[Part], Group->getAlign(),\n                                     GroupMask, PoisonVec, \"wide.masked.vec\");\n      }\n      else\n        NewLoad = Builder.CreateAlignedLoad(VecTy, AddrParts[Part],\n                                            Group->getAlign(), \"wide.vec\");\n      Group->addMetadata(NewLoad);\n      NewLoads.push_back(NewLoad);\n    }\n\n    // For each member in the group, shuffle out the appropriate data from the\n    // wide loads.\n    unsigned J = 0;\n    for (unsigned I = 0; I < InterleaveFactor; ++I) {\n      Instruction *Member = Group->getMember(I);\n\n      // Skip the gaps in the group.\n      if (!Member)\n        continue;\n\n      assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n      auto StrideMask =\n          createStrideMask(I, InterleaveFactor, VF.getKnownMinValue());\n      for (unsigned Part = 0; Part < UF; Part++) {\n        Value *StridedVec = Builder.CreateShuffleVector(\n            NewLoads[Part], StrideMask, \"strided.vec\");\n\n        // If this member has different type, cast the result type.\n        if (Member->getType() != ScalarTy) {\n          assert(!VF.isScalable() && \"VF is assumed to be non scalable.\");\n          VectorType *OtherVTy = VectorType::get(Member->getType(), VF);\n          StridedVec = createBitOrPointerCast(StridedVec, OtherVTy, DL);\n        }\n\n        if (Group->isReverse())\n          StridedVec = reverseVector(StridedVec);\n\n        State.set(VPDefs[J], StridedVec, Part);\n      }\n      ++J;\n    }\n    return;\n  }\n\n  // The sub vector type for current instruction.\n  assert(!VF.isScalable() && \"VF is assumed to be non scalable.\");\n  auto *SubVT = VectorType::get(ScalarTy, VF);\n\n  // Vectorize the interleaved store group.\n  for (unsigned Part = 0; Part < UF; Part++) {\n    // Collect the stored vector from each member.\n    SmallVector<Value *, 4> StoredVecs;\n    for (unsigned i = 0; i < InterleaveFactor; i++) {\n      // Interleaved store group doesn't allow a gap, so each index has a member\n      assert(Group->getMember(i) && \"Fail to get a member from an interleaved store group\");\n\n      Value *StoredVec = State.get(StoredValues[i], Part);\n\n      if (Group->isReverse())\n        StoredVec = reverseVector(StoredVec);\n\n      // If this member has different type, cast it to a unified type.\n\n      if (StoredVec->getType() != SubVT)\n        StoredVec = createBitOrPointerCast(StoredVec, SubVT, DL);\n\n      StoredVecs.push_back(StoredVec);\n    }\n\n    // Concatenate all vectors into a wide vector.\n    Value *WideVec = concatenateVectors(Builder, StoredVecs);\n\n    // Interleave the elements in the wide vector.\n    assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n    Value *IVec = Builder.CreateShuffleVector(\n        WideVec, createInterleaveMask(VF.getKnownMinValue(), InterleaveFactor),\n        \"interleaved.vec\");\n\n    Instruction *NewStoreInstr;\n    if (BlockInMask) {\n      Value *BlockInMaskPart = State.get(BlockInMask, Part);\n      Value *ShuffledMask = Builder.CreateShuffleVector(\n          BlockInMaskPart,\n          createReplicatedMask(InterleaveFactor, VF.getKnownMinValue()),\n          \"interleaved.mask\");\n      NewStoreInstr = Builder.CreateMaskedStore(\n          IVec, AddrParts[Part], Group->getAlign(), ShuffledMask);\n    }\n    else\n      NewStoreInstr =\n          Builder.CreateAlignedStore(IVec, AddrParts[Part], Group->getAlign());\n\n    Group->addMetadata(NewStoreInstr);\n  }\n}\n\nvoid InnerLoopVectorizer::vectorizeMemoryInstruction(\n    Instruction *Instr, VPTransformState &State, VPValue *Def, VPValue *Addr,\n    VPValue *StoredValue, VPValue *BlockInMask) {\n  // Attempt to issue a wide load.\n  LoadInst *LI = dyn_cast<LoadInst>(Instr);\n  StoreInst *SI = dyn_cast<StoreInst>(Instr);\n\n  assert((LI || SI) && \"Invalid Load/Store instruction\");\n  assert((!SI || StoredValue) && \"No stored value provided for widened store\");\n  assert((!LI || !StoredValue) && \"Stored value provided for widened load\");\n\n  LoopVectorizationCostModel::InstWidening Decision =\n      Cost->getWideningDecision(Instr, VF);\n  assert((Decision == LoopVectorizationCostModel::CM_Widen ||\n          Decision == LoopVectorizationCostModel::CM_Widen_Reverse ||\n          Decision == LoopVectorizationCostModel::CM_GatherScatter) &&\n         \"CM decision is not to widen the memory instruction\");\n\n  Type *ScalarDataTy = getMemInstValueType(Instr);\n\n  auto *DataTy = VectorType::get(ScalarDataTy, VF);\n  const Align Alignment = getLoadStoreAlignment(Instr);\n\n  // Determine if the pointer operand of the access is either consecutive or\n  // reverse consecutive.\n  bool Reverse = (Decision == LoopVectorizationCostModel::CM_Widen_Reverse);\n  bool ConsecutiveStride =\n      Reverse || (Decision == LoopVectorizationCostModel::CM_Widen);\n  bool CreateGatherScatter =\n      (Decision == LoopVectorizationCostModel::CM_GatherScatter);\n\n  // Either Ptr feeds a vector load/store, or a vector GEP should feed a vector\n  // gather/scatter. Otherwise Decision should have been to Scalarize.\n  assert((ConsecutiveStride || CreateGatherScatter) &&\n         \"The instruction should be scalarized\");\n  (void)ConsecutiveStride;\n\n  VectorParts BlockInMaskParts(UF);\n  bool isMaskRequired = BlockInMask;\n  if (isMaskRequired)\n    for (unsigned Part = 0; Part < UF; ++Part)\n      BlockInMaskParts[Part] = State.get(BlockInMask, Part);\n\n  const auto CreateVecPtr = [&](unsigned Part, Value *Ptr) -> Value * {\n    // Calculate the pointer for the specific unroll-part.\n    GetElementPtrInst *PartPtr = nullptr;\n\n    bool InBounds = false;\n    if (auto *gep = dyn_cast<GetElementPtrInst>(Ptr->stripPointerCasts()))\n      InBounds = gep->isInBounds();\n\n    if (Reverse) {\n      assert(!VF.isScalable() &&\n             \"Reversing vectors is not yet supported for scalable vectors.\");\n\n      // If the address is consecutive but reversed, then the\n      // wide store needs to start at the last vector element.\n      PartPtr = cast<GetElementPtrInst>(Builder.CreateGEP(\n          ScalarDataTy, Ptr, Builder.getInt32(-Part * VF.getKnownMinValue())));\n      PartPtr->setIsInBounds(InBounds);\n      PartPtr = cast<GetElementPtrInst>(Builder.CreateGEP(\n          ScalarDataTy, PartPtr, Builder.getInt32(1 - VF.getKnownMinValue())));\n      PartPtr->setIsInBounds(InBounds);\n      if (isMaskRequired) // Reverse of a null all-one mask is a null mask.\n        BlockInMaskParts[Part] = reverseVector(BlockInMaskParts[Part]);\n    } else {\n      Value *Increment = createStepForVF(Builder, Builder.getInt32(Part), VF);\n      PartPtr = cast<GetElementPtrInst>(\n          Builder.CreateGEP(ScalarDataTy, Ptr, Increment));\n      PartPtr->setIsInBounds(InBounds);\n    }\n\n    unsigned AddressSpace = Ptr->getType()->getPointerAddressSpace();\n    return Builder.CreateBitCast(PartPtr, DataTy->getPointerTo(AddressSpace));\n  };\n\n  // Handle Stores:\n  if (SI) {\n    setDebugLocFromInst(Builder, SI);\n\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      Instruction *NewSI = nullptr;\n      Value *StoredVal = State.get(StoredValue, Part);\n      if (CreateGatherScatter) {\n        Value *MaskPart = isMaskRequired ? BlockInMaskParts[Part] : nullptr;\n        Value *VectorGep = State.get(Addr, Part);\n        NewSI = Builder.CreateMaskedScatter(StoredVal, VectorGep, Alignment,\n                                            MaskPart);\n      } else {\n        if (Reverse) {\n          // If we store to reverse consecutive memory locations, then we need\n          // to reverse the order of elements in the stored value.\n          StoredVal = reverseVector(StoredVal);\n          // We don't want to update the value in the map as it might be used in\n          // another expression. So don't call resetVectorValue(StoredVal).\n        }\n        auto *VecPtr = CreateVecPtr(Part, State.get(Addr, VPIteration(0, 0)));\n        if (isMaskRequired)\n          NewSI = Builder.CreateMaskedStore(StoredVal, VecPtr, Alignment,\n                                            BlockInMaskParts[Part]);\n        else\n          NewSI = Builder.CreateAlignedStore(StoredVal, VecPtr, Alignment);\n      }\n      addMetadata(NewSI, SI);\n    }\n    return;\n  }\n\n  // Handle loads.\n  assert(LI && \"Must have a load instruction\");\n  setDebugLocFromInst(Builder, LI);\n  for (unsigned Part = 0; Part < UF; ++Part) {\n    Value *NewLI;\n    if (CreateGatherScatter) {\n      Value *MaskPart = isMaskRequired ? BlockInMaskParts[Part] : nullptr;\n      Value *VectorGep = State.get(Addr, Part);\n      NewLI = Builder.CreateMaskedGather(VectorGep, Alignment, MaskPart,\n                                         nullptr, \"wide.masked.gather\");\n      addMetadata(NewLI, LI);\n    } else {\n      auto *VecPtr = CreateVecPtr(Part, State.get(Addr, VPIteration(0, 0)));\n      if (isMaskRequired)\n        NewLI = Builder.CreateMaskedLoad(\n            VecPtr, Alignment, BlockInMaskParts[Part], PoisonValue::get(DataTy),\n            \"wide.masked.load\");\n      else\n        NewLI =\n            Builder.CreateAlignedLoad(DataTy, VecPtr, Alignment, \"wide.load\");\n\n      // Add metadata to the load, but setVectorValue to the reverse shuffle.\n      addMetadata(NewLI, LI);\n      if (Reverse)\n        NewLI = reverseVector(NewLI);\n    }\n\n    State.set(Def, NewLI, Part);\n  }\n}\n\nvoid InnerLoopVectorizer::scalarizeInstruction(Instruction *Instr, VPValue *Def,\n                                               VPUser &User,\n                                               const VPIteration &Instance,\n                                               bool IfPredicateInstr,\n                                               VPTransformState &State) {\n  assert(!Instr->getType()->isAggregateType() && \"Can't handle vectors\");\n\n  // llvm.experimental.noalias.scope.decl intrinsics must only be duplicated for\n  // the first lane and part.\n  if (isa<NoAliasScopeDeclInst>(Instr))\n    if (!Instance.isFirstIteration())\n      return;\n\n  setDebugLocFromInst(Builder, Instr);\n\n  // Does this instruction return a value ?\n  bool IsVoidRetTy = Instr->getType()->isVoidTy();\n\n  Instruction *Cloned = Instr->clone();\n  if (!IsVoidRetTy)\n    Cloned->setName(Instr->getName() + \".cloned\");\n\n  State.Builder.SetInsertPoint(Builder.GetInsertBlock(),\n                               Builder.GetInsertPoint());\n  // Replace the operands of the cloned instructions with their scalar\n  // equivalents in the new loop.\n  for (unsigned op = 0, e = User.getNumOperands(); op != e; ++op) {\n    auto *Operand = dyn_cast<Instruction>(Instr->getOperand(op));\n    auto InputInstance = Instance;\n    if (!Operand || !OrigLoop->contains(Operand) ||\n        (Cost->isUniformAfterVectorization(Operand, State.VF)))\n      InputInstance.Lane = VPLane::getFirstLane();\n    auto *NewOp = State.get(User.getOperand(op), InputInstance);\n    Cloned->setOperand(op, NewOp);\n  }\n  addNewMetadata(Cloned, Instr);\n\n  // Place the cloned scalar in the new loop.\n  Builder.Insert(Cloned);\n\n  State.set(Def, Cloned, Instance);\n\n  // If we just cloned a new assumption, add it the assumption cache.\n  if (auto *II = dyn_cast<IntrinsicInst>(Cloned))\n    if (II->getIntrinsicID() == Intrinsic::assume)\n      AC->registerAssumption(II);\n\n  // End if-block.\n  if (IfPredicateInstr)\n    PredicatedInstructions.push_back(Cloned);\n}\n\nPHINode *InnerLoopVectorizer::createInductionVariable(Loop *L, Value *Start,\n                                                      Value *End, Value *Step,\n                                                      Instruction *DL) {\n  BasicBlock *Header = L->getHeader();\n  BasicBlock *Latch = L->getLoopLatch();\n  // As we're just creating this loop, it's possible no latch exists\n  // yet. If so, use the header as this will be a single block loop.\n  if (!Latch)\n    Latch = Header;\n\n  IRBuilder<> Builder(&*Header->getFirstInsertionPt());\n  Instruction *OldInst = getDebugLocFromInstOrOperands(OldInduction);\n  setDebugLocFromInst(Builder, OldInst);\n  auto *Induction = Builder.CreatePHI(Start->getType(), 2, \"index\");\n\n  Builder.SetInsertPoint(Latch->getTerminator());\n  setDebugLocFromInst(Builder, OldInst);\n\n  // Create i+1 and fill the PHINode.\n  Value *Next = Builder.CreateAdd(Induction, Step, \"index.next\");\n  Induction->addIncoming(Start, L->getLoopPreheader());\n  Induction->addIncoming(Next, Latch);\n  // Create the compare.\n  Value *ICmp = Builder.CreateICmpEQ(Next, End);\n  Builder.CreateCondBr(ICmp, L->getUniqueExitBlock(), Header);\n\n  // Now we have two terminators. Remove the old one from the block.\n  Latch->getTerminator()->eraseFromParent();\n\n  return Induction;\n}\n\nValue *InnerLoopVectorizer::getOrCreateTripCount(Loop *L) {\n  if (TripCount)\n    return TripCount;\n\n  assert(L && \"Create Trip Count for null loop.\");\n  IRBuilder<> Builder(L->getLoopPreheader()->getTerminator());\n  // Find the loop boundaries.\n  ScalarEvolution *SE = PSE.getSE();\n  const SCEV *BackedgeTakenCount = PSE.getBackedgeTakenCount();\n  assert(!isa<SCEVCouldNotCompute>(BackedgeTakenCount) &&\n         \"Invalid loop count\");\n\n  Type *IdxTy = Legal->getWidestInductionType();\n  assert(IdxTy && \"No type for induction\");\n\n  // The exit count might have the type of i64 while the phi is i32. This can\n  // happen if we have an induction variable that is sign extended before the\n  // compare. The only way that we get a backedge taken count is that the\n  // induction variable was signed and as such will not overflow. In such a case\n  // truncation is legal.\n  if (SE->getTypeSizeInBits(BackedgeTakenCount->getType()) >\n      IdxTy->getPrimitiveSizeInBits())\n    BackedgeTakenCount = SE->getTruncateOrNoop(BackedgeTakenCount, IdxTy);\n  BackedgeTakenCount = SE->getNoopOrZeroExtend(BackedgeTakenCount, IdxTy);\n\n  // Get the total trip count from the count by adding 1.\n  const SCEV *ExitCount = SE->getAddExpr(\n      BackedgeTakenCount, SE->getOne(BackedgeTakenCount->getType()));\n\n  const DataLayout &DL = L->getHeader()->getModule()->getDataLayout();\n\n  // Expand the trip count and place the new instructions in the preheader.\n  // Notice that the pre-header does not change, only the loop body.\n  SCEVExpander Exp(*SE, DL, \"induction\");\n\n  // Count holds the overall loop count (N).\n  TripCount = Exp.expandCodeFor(ExitCount, ExitCount->getType(),\n                                L->getLoopPreheader()->getTerminator());\n\n  if (TripCount->getType()->isPointerTy())\n    TripCount =\n        CastInst::CreatePointerCast(TripCount, IdxTy, \"exitcount.ptrcnt.to.int\",\n                                    L->getLoopPreheader()->getTerminator());\n\n  return TripCount;\n}\n\nValue *InnerLoopVectorizer::getOrCreateVectorTripCount(Loop *L) {\n  if (VectorTripCount)\n    return VectorTripCount;\n\n  Value *TC = getOrCreateTripCount(L);\n  IRBuilder<> Builder(L->getLoopPreheader()->getTerminator());\n\n  Type *Ty = TC->getType();\n  // This is where we can make the step a runtime constant.\n  Value *Step = createStepForVF(Builder, ConstantInt::get(Ty, UF), VF);\n\n  // If the tail is to be folded by masking, round the number of iterations N\n  // up to a multiple of Step instead of rounding down. This is done by first\n  // adding Step-1 and then rounding down. Note that it's ok if this addition\n  // overflows: the vector induction variable will eventually wrap to zero given\n  // that it starts at zero and its Step is a power of two; the loop will then\n  // exit, with the last early-exit vector comparison also producing all-true.\n  if (Cost->foldTailByMasking()) {\n    assert(isPowerOf2_32(VF.getKnownMinValue() * UF) &&\n           \"VF*UF must be a power of 2 when folding tail by masking\");\n    assert(!VF.isScalable() &&\n           \"Tail folding not yet supported for scalable vectors\");\n    TC = Builder.CreateAdd(\n        TC, ConstantInt::get(Ty, VF.getKnownMinValue() * UF - 1), \"n.rnd.up\");\n  }\n\n  // Now we need to generate the expression for the part of the loop that the\n  // vectorized body will execute. This is equal to N - (N % Step) if scalar\n  // iterations are not required for correctness, or N - Step, otherwise. Step\n  // is equal to the vectorization factor (number of SIMD elements) times the\n  // unroll factor (number of SIMD instructions).\n  Value *R = Builder.CreateURem(TC, Step, \"n.mod.vf\");\n\n  // There are two cases where we need to ensure (at least) the last iteration\n  // runs in the scalar remainder loop. Thus, if the step evenly divides\n  // the trip count, we set the remainder to be equal to the step. If the step\n  // does not evenly divide the trip count, no adjustment is necessary since\n  // there will already be scalar iterations. Note that the minimum iterations\n  // check ensures that N >= Step. The cases are:\n  // 1) If there is a non-reversed interleaved group that may speculatively\n  //    access memory out-of-bounds.\n  // 2) If any instruction may follow a conditionally taken exit. That is, if\n  //    the loop contains multiple exiting blocks, or a single exiting block\n  //    which is not the latch.\n  if (VF.isVector() && Cost->requiresScalarEpilogue()) {\n    auto *IsZero = Builder.CreateICmpEQ(R, ConstantInt::get(R->getType(), 0));\n    R = Builder.CreateSelect(IsZero, Step, R);\n  }\n\n  VectorTripCount = Builder.CreateSub(TC, R, \"n.vec\");\n\n  return VectorTripCount;\n}\n\nValue *InnerLoopVectorizer::createBitOrPointerCast(Value *V, VectorType *DstVTy,\n                                                   const DataLayout &DL) {\n  // Verify that V is a vector type with same number of elements as DstVTy.\n  auto *DstFVTy = cast<FixedVectorType>(DstVTy);\n  unsigned VF = DstFVTy->getNumElements();\n  auto *SrcVecTy = cast<FixedVectorType>(V->getType());\n  assert((VF == SrcVecTy->getNumElements()) && \"Vector dimensions do not match\");\n  Type *SrcElemTy = SrcVecTy->getElementType();\n  Type *DstElemTy = DstFVTy->getElementType();\n  assert((DL.getTypeSizeInBits(SrcElemTy) == DL.getTypeSizeInBits(DstElemTy)) &&\n         \"Vector elements must have same size\");\n\n  // Do a direct cast if element types are castable.\n  if (CastInst::isBitOrNoopPointerCastable(SrcElemTy, DstElemTy, DL)) {\n    return Builder.CreateBitOrPointerCast(V, DstFVTy);\n  }\n  // V cannot be directly casted to desired vector type.\n  // May happen when V is a floating point vector but DstVTy is a vector of\n  // pointers or vice-versa. Handle this using a two-step bitcast using an\n  // intermediate Integer type for the bitcast i.e. Ptr <-> Int <-> Float.\n  assert((DstElemTy->isPointerTy() != SrcElemTy->isPointerTy()) &&\n         \"Only one type should be a pointer type\");\n  assert((DstElemTy->isFloatingPointTy() != SrcElemTy->isFloatingPointTy()) &&\n         \"Only one type should be a floating point type\");\n  Type *IntTy =\n      IntegerType::getIntNTy(V->getContext(), DL.getTypeSizeInBits(SrcElemTy));\n  auto *VecIntTy = FixedVectorType::get(IntTy, VF);\n  Value *CastVal = Builder.CreateBitOrPointerCast(V, VecIntTy);\n  return Builder.CreateBitOrPointerCast(CastVal, DstFVTy);\n}\n\nvoid InnerLoopVectorizer::emitMinimumIterationCountCheck(Loop *L,\n                                                         BasicBlock *Bypass) {\n  Value *Count = getOrCreateTripCount(L);\n  // Reuse existing vector loop preheader for TC checks.\n  // Note that new preheader block is generated for vector loop.\n  BasicBlock *const TCCheckBlock = LoopVectorPreHeader;\n  IRBuilder<> Builder(TCCheckBlock->getTerminator());\n\n  // Generate code to check if the loop's trip count is less than VF * UF, or\n  // equal to it in case a scalar epilogue is required; this implies that the\n  // vector trip count is zero. This check also covers the case where adding one\n  // to the backedge-taken count overflowed leading to an incorrect trip count\n  // of zero. In this case we will also jump to the scalar loop.\n  auto P = Cost->requiresScalarEpilogue() ? ICmpInst::ICMP_ULE\n                                          : ICmpInst::ICMP_ULT;\n\n  // If tail is to be folded, vector loop takes care of all iterations.\n  Value *CheckMinIters = Builder.getFalse();\n  if (!Cost->foldTailByMasking()) {\n    Value *Step =\n        createStepForVF(Builder, ConstantInt::get(Count->getType(), UF), VF);\n    CheckMinIters = Builder.CreateICmp(P, Count, Step, \"min.iters.check\");\n  }\n  // Create new preheader for vector loop.\n  LoopVectorPreHeader =\n      SplitBlock(TCCheckBlock, TCCheckBlock->getTerminator(), DT, LI, nullptr,\n                 \"vector.ph\");\n\n  assert(DT->properlyDominates(DT->getNode(TCCheckBlock),\n                               DT->getNode(Bypass)->getIDom()) &&\n         \"TC check is expected to dominate Bypass\");\n\n  // Update dominator for Bypass & LoopExit.\n  DT->changeImmediateDominator(Bypass, TCCheckBlock);\n  DT->changeImmediateDominator(LoopExitBlock, TCCheckBlock);\n\n  ReplaceInstWithInst(\n      TCCheckBlock->getTerminator(),\n      BranchInst::Create(Bypass, LoopVectorPreHeader, CheckMinIters));\n  LoopBypassBlocks.push_back(TCCheckBlock);\n}\n\nBasicBlock *InnerLoopVectorizer::emitSCEVChecks(Loop *L, BasicBlock *Bypass) {\n\n  BasicBlock *const SCEVCheckBlock =\n      RTChecks.emitSCEVChecks(L, Bypass, LoopVectorPreHeader, LoopExitBlock);\n  if (!SCEVCheckBlock)\n    return nullptr;\n\n  assert(!(SCEVCheckBlock->getParent()->hasOptSize() ||\n           (OptForSizeBasedOnProfile &&\n            Cost->Hints->getForce() != LoopVectorizeHints::FK_Enabled)) &&\n         \"Cannot SCEV check stride or overflow when optimizing for size\");\n\n\n  // Update dominator only if this is first RT check.\n  if (LoopBypassBlocks.empty()) {\n    DT->changeImmediateDominator(Bypass, SCEVCheckBlock);\n    DT->changeImmediateDominator(LoopExitBlock, SCEVCheckBlock);\n  }\n\n  LoopBypassBlocks.push_back(SCEVCheckBlock);\n  AddedSafetyChecks = true;\n  return SCEVCheckBlock;\n}\n\nBasicBlock *InnerLoopVectorizer::emitMemRuntimeChecks(Loop *L,\n                                                      BasicBlock *Bypass) {\n  // VPlan-native path does not do any analysis for runtime checks currently.\n  if (EnableVPlanNativePath)\n    return nullptr;\n\n  BasicBlock *const MemCheckBlock =\n      RTChecks.emitMemRuntimeChecks(L, Bypass, LoopVectorPreHeader);\n\n  // Check if we generated code that checks in runtime if arrays overlap. We put\n  // the checks into a separate block to make the more common case of few\n  // elements faster.\n  if (!MemCheckBlock)\n    return nullptr;\n\n  if (MemCheckBlock->getParent()->hasOptSize() || OptForSizeBasedOnProfile) {\n    assert(Cost->Hints->getForce() == LoopVectorizeHints::FK_Enabled &&\n           \"Cannot emit memory checks when optimizing for size, unless forced \"\n           \"to vectorize.\");\n    ORE->emit([&]() {\n      return OptimizationRemarkAnalysis(DEBUG_TYPE, \"VectorizationCodeSize\",\n                                        L->getStartLoc(), L->getHeader())\n             << \"Code-size may be reduced by not forcing \"\n                \"vectorization, or by source-code modifications \"\n                \"eliminating the need for runtime checks \"\n                \"(e.g., adding 'restrict').\";\n    });\n  }\n\n  LoopBypassBlocks.push_back(MemCheckBlock);\n\n  AddedSafetyChecks = true;\n\n  // We currently don't use LoopVersioning for the actual loop cloning but we\n  // still use it to add the noalias metadata.\n  LVer = std::make_unique<LoopVersioning>(\n      *Legal->getLAI(),\n      Legal->getLAI()->getRuntimePointerChecking()->getChecks(), OrigLoop, LI,\n      DT, PSE.getSE());\n  LVer->prepareNoAliasMetadata();\n  return MemCheckBlock;\n}\n\nValue *InnerLoopVectorizer::emitTransformedIndex(\n    IRBuilder<> &B, Value *Index, ScalarEvolution *SE, const DataLayout &DL,\n    const InductionDescriptor &ID) const {\n\n  SCEVExpander Exp(*SE, DL, \"induction\");\n  auto Step = ID.getStep();\n  auto StartValue = ID.getStartValue();\n  assert(Index->getType() == Step->getType() &&\n         \"Index type does not match StepValue type\");\n\n  // Note: the IR at this point is broken. We cannot use SE to create any new\n  // SCEV and then expand it, hoping that SCEV's simplification will give us\n  // a more optimal code. Unfortunately, attempt of doing so on invalid IR may\n  // lead to various SCEV crashes. So all we can do is to use builder and rely\n  // on InstCombine for future simplifications. Here we handle some trivial\n  // cases only.\n  auto CreateAdd = [&B](Value *X, Value *Y) {\n    assert(X->getType() == Y->getType() && \"Types don't match!\");\n    if (auto *CX = dyn_cast<ConstantInt>(X))\n      if (CX->isZero())\n        return Y;\n    if (auto *CY = dyn_cast<ConstantInt>(Y))\n      if (CY->isZero())\n        return X;\n    return B.CreateAdd(X, Y);\n  };\n\n  auto CreateMul = [&B](Value *X, Value *Y) {\n    assert(X->getType() == Y->getType() && \"Types don't match!\");\n    if (auto *CX = dyn_cast<ConstantInt>(X))\n      if (CX->isOne())\n        return Y;\n    if (auto *CY = dyn_cast<ConstantInt>(Y))\n      if (CY->isOne())\n        return X;\n    return B.CreateMul(X, Y);\n  };\n\n  // Get a suitable insert point for SCEV expansion. For blocks in the vector\n  // loop, choose the end of the vector loop header (=LoopVectorBody), because\n  // the DomTree is not kept up-to-date for additional blocks generated in the\n  // vector loop. By using the header as insertion point, we guarantee that the\n  // expanded instructions dominate all their uses.\n  auto GetInsertPoint = [this, &B]() {\n    BasicBlock *InsertBB = B.GetInsertPoint()->getParent();\n    if (InsertBB != LoopVectorBody &&\n        LI->getLoopFor(LoopVectorBody) == LI->getLoopFor(InsertBB))\n      return LoopVectorBody->getTerminator();\n    return &*B.GetInsertPoint();\n  };\n\n  switch (ID.getKind()) {\n  case InductionDescriptor::IK_IntInduction: {\n    assert(Index->getType() == StartValue->getType() &&\n           \"Index type does not match StartValue type\");\n    if (ID.getConstIntStepValue() && ID.getConstIntStepValue()->isMinusOne())\n      return B.CreateSub(StartValue, Index);\n    auto *Offset = CreateMul(\n        Index, Exp.expandCodeFor(Step, Index->getType(), GetInsertPoint()));\n    return CreateAdd(StartValue, Offset);\n  }\n  case InductionDescriptor::IK_PtrInduction: {\n    assert(isa<SCEVConstant>(Step) &&\n           \"Expected constant step for pointer induction\");\n    return B.CreateGEP(\n        StartValue->getType()->getPointerElementType(), StartValue,\n        CreateMul(Index,\n                  Exp.expandCodeFor(Step, Index->getType(), GetInsertPoint())));\n  }\n  case InductionDescriptor::IK_FpInduction: {\n    assert(Step->getType()->isFloatingPointTy() && \"Expected FP Step value\");\n    auto InductionBinOp = ID.getInductionBinOp();\n    assert(InductionBinOp &&\n           (InductionBinOp->getOpcode() == Instruction::FAdd ||\n            InductionBinOp->getOpcode() == Instruction::FSub) &&\n           \"Original bin op should be defined for FP induction\");\n\n    Value *StepValue = cast<SCEVUnknown>(Step)->getValue();\n    Value *MulExp = B.CreateFMul(StepValue, Index);\n    return B.CreateBinOp(InductionBinOp->getOpcode(), StartValue, MulExp,\n                         \"induction\");\n  }\n  case InductionDescriptor::IK_NoInduction:\n    return nullptr;\n  }\n  llvm_unreachable(\"invalid enum\");\n}\n\nLoop *InnerLoopVectorizer::createVectorLoopSkeleton(StringRef Prefix) {\n  LoopScalarBody = OrigLoop->getHeader();\n  LoopVectorPreHeader = OrigLoop->getLoopPreheader();\n  LoopExitBlock = OrigLoop->getUniqueExitBlock();\n  assert(LoopExitBlock && \"Must have an exit block\");\n  assert(LoopVectorPreHeader && \"Invalid loop structure\");\n\n  LoopMiddleBlock =\n      SplitBlock(LoopVectorPreHeader, LoopVectorPreHeader->getTerminator(), DT,\n                 LI, nullptr, Twine(Prefix) + \"middle.block\");\n  LoopScalarPreHeader =\n      SplitBlock(LoopMiddleBlock, LoopMiddleBlock->getTerminator(), DT, LI,\n                 nullptr, Twine(Prefix) + \"scalar.ph\");\n\n  // Set up branch from middle block to the exit and scalar preheader blocks.\n  // completeLoopSkeleton will update the condition to use an iteration check,\n  // if required to decide whether to execute the remainder.\n  BranchInst *BrInst =\n      BranchInst::Create(LoopExitBlock, LoopScalarPreHeader, Builder.getTrue());\n  auto *ScalarLatchTerm = OrigLoop->getLoopLatch()->getTerminator();\n  BrInst->setDebugLoc(ScalarLatchTerm->getDebugLoc());\n  ReplaceInstWithInst(LoopMiddleBlock->getTerminator(), BrInst);\n\n  // We intentionally don't let SplitBlock to update LoopInfo since\n  // LoopVectorBody should belong to another loop than LoopVectorPreHeader.\n  // LoopVectorBody is explicitly added to the correct place few lines later.\n  LoopVectorBody =\n      SplitBlock(LoopVectorPreHeader, LoopVectorPreHeader->getTerminator(), DT,\n                 nullptr, nullptr, Twine(Prefix) + \"vector.body\");\n\n  // Update dominator for loop exit.\n  DT->changeImmediateDominator(LoopExitBlock, LoopMiddleBlock);\n\n  // Create and register the new vector loop.\n  Loop *Lp = LI->AllocateLoop();\n  Loop *ParentLoop = OrigLoop->getParentLoop();\n\n  // Insert the new loop into the loop nest and register the new basic blocks\n  // before calling any utilities such as SCEV that require valid LoopInfo.\n  if (ParentLoop) {\n    ParentLoop->addChildLoop(Lp);\n  } else {\n    LI->addTopLevelLoop(Lp);\n  }\n  Lp->addBasicBlockToLoop(LoopVectorBody, *LI);\n  return Lp;\n}\n\nvoid InnerLoopVectorizer::createInductionResumeValues(\n    Loop *L, Value *VectorTripCount,\n    std::pair<BasicBlock *, Value *> AdditionalBypass) {\n  assert(VectorTripCount && L && \"Expected valid arguments\");\n  assert(((AdditionalBypass.first && AdditionalBypass.second) ||\n          (!AdditionalBypass.first && !AdditionalBypass.second)) &&\n         \"Inconsistent information about additional bypass.\");\n  // We are going to resume the execution of the scalar loop.\n  // Go over all of the induction variables that we found and fix the\n  // PHIs that are left in the scalar version of the loop.\n  // The starting values of PHI nodes depend on the counter of the last\n  // iteration in the vectorized loop.\n  // If we come from a bypass edge then we need to start from the original\n  // start value.\n  for (auto &InductionEntry : Legal->getInductionVars()) {\n    PHINode *OrigPhi = InductionEntry.first;\n    InductionDescriptor II = InductionEntry.second;\n\n    // Create phi nodes to merge from the  backedge-taken check block.\n    PHINode *BCResumeVal =\n        PHINode::Create(OrigPhi->getType(), 3, \"bc.resume.val\",\n                        LoopScalarPreHeader->getTerminator());\n    // Copy original phi DL over to the new one.\n    BCResumeVal->setDebugLoc(OrigPhi->getDebugLoc());\n    Value *&EndValue = IVEndValues[OrigPhi];\n    Value *EndValueFromAdditionalBypass = AdditionalBypass.second;\n    if (OrigPhi == OldInduction) {\n      // We know what the end value is.\n      EndValue = VectorTripCount;\n    } else {\n      IRBuilder<> B(L->getLoopPreheader()->getTerminator());\n\n      // Fast-math-flags propagate from the original induction instruction.\n      if (II.getInductionBinOp() && isa<FPMathOperator>(II.getInductionBinOp()))\n        B.setFastMathFlags(II.getInductionBinOp()->getFastMathFlags());\n\n      Type *StepType = II.getStep()->getType();\n      Instruction::CastOps CastOp =\n          CastInst::getCastOpcode(VectorTripCount, true, StepType, true);\n      Value *CRD = B.CreateCast(CastOp, VectorTripCount, StepType, \"cast.crd\");\n      const DataLayout &DL = LoopScalarBody->getModule()->getDataLayout();\n      EndValue = emitTransformedIndex(B, CRD, PSE.getSE(), DL, II);\n      EndValue->setName(\"ind.end\");\n\n      // Compute the end value for the additional bypass (if applicable).\n      if (AdditionalBypass.first) {\n        B.SetInsertPoint(&(*AdditionalBypass.first->getFirstInsertionPt()));\n        CastOp = CastInst::getCastOpcode(AdditionalBypass.second, true,\n                                         StepType, true);\n        CRD =\n            B.CreateCast(CastOp, AdditionalBypass.second, StepType, \"cast.crd\");\n        EndValueFromAdditionalBypass =\n            emitTransformedIndex(B, CRD, PSE.getSE(), DL, II);\n        EndValueFromAdditionalBypass->setName(\"ind.end\");\n      }\n    }\n    // The new PHI merges the original incoming value, in case of a bypass,\n    // or the value at the end of the vectorized loop.\n    BCResumeVal->addIncoming(EndValue, LoopMiddleBlock);\n\n    // Fix the scalar body counter (PHI node).\n    // The old induction's phi node in the scalar body needs the truncated\n    // value.\n    for (BasicBlock *BB : LoopBypassBlocks)\n      BCResumeVal->addIncoming(II.getStartValue(), BB);\n\n    if (AdditionalBypass.first)\n      BCResumeVal->setIncomingValueForBlock(AdditionalBypass.first,\n                                            EndValueFromAdditionalBypass);\n\n    OrigPhi->setIncomingValueForBlock(LoopScalarPreHeader, BCResumeVal);\n  }\n}\n\nBasicBlock *InnerLoopVectorizer::completeLoopSkeleton(Loop *L,\n                                                      MDNode *OrigLoopID) {\n  assert(L && \"Expected valid loop.\");\n\n  // The trip counts should be cached by now.\n  Value *Count = getOrCreateTripCount(L);\n  Value *VectorTripCount = getOrCreateVectorTripCount(L);\n\n  auto *ScalarLatchTerm = OrigLoop->getLoopLatch()->getTerminator();\n\n  // Add a check in the middle block to see if we have completed\n  // all of the iterations in the first vector loop.\n  // If (N - N%VF) == N, then we *don't* need to run the remainder.\n  // If tail is to be folded, we know we don't need to run the remainder.\n  if (!Cost->foldTailByMasking()) {\n    Instruction *CmpN = CmpInst::Create(Instruction::ICmp, CmpInst::ICMP_EQ,\n                                        Count, VectorTripCount, \"cmp.n\",\n                                        LoopMiddleBlock->getTerminator());\n\n    // Here we use the same DebugLoc as the scalar loop latch terminator instead\n    // of the corresponding compare because they may have ended up with\n    // different line numbers and we want to avoid awkward line stepping while\n    // debugging. Eg. if the compare has got a line number inside the loop.\n    CmpN->setDebugLoc(ScalarLatchTerm->getDebugLoc());\n    cast<BranchInst>(LoopMiddleBlock->getTerminator())->setCondition(CmpN);\n  }\n\n  // Get ready to start creating new instructions into the vectorized body.\n  assert(LoopVectorPreHeader == L->getLoopPreheader() &&\n         \"Inconsistent vector loop preheader\");\n  Builder.SetInsertPoint(&*LoopVectorBody->getFirstInsertionPt());\n\n  Optional<MDNode *> VectorizedLoopID =\n      makeFollowupLoopID(OrigLoopID, {LLVMLoopVectorizeFollowupAll,\n                                      LLVMLoopVectorizeFollowupVectorized});\n  if (VectorizedLoopID.hasValue()) {\n    L->setLoopID(VectorizedLoopID.getValue());\n\n    // Do not setAlreadyVectorized if loop attributes have been defined\n    // explicitly.\n    return LoopVectorPreHeader;\n  }\n\n  // Keep all loop hints from the original loop on the vector loop (we'll\n  // replace the vectorizer-specific hints below).\n  if (MDNode *LID = OrigLoop->getLoopID())\n    L->setLoopID(LID);\n\n  LoopVectorizeHints Hints(L, true, *ORE);\n  Hints.setAlreadyVectorized();\n\n#ifdef EXPENSIVE_CHECKS\n  assert(DT->verify(DominatorTree::VerificationLevel::Fast));\n  LI->verify(*DT);\n#endif\n\n  return LoopVectorPreHeader;\n}\n\nBasicBlock *InnerLoopVectorizer::createVectorizedLoopSkeleton() {\n  /*\n   In this function we generate a new loop. The new loop will contain\n   the vectorized instructions while the old loop will continue to run the\n   scalar remainder.\n\n       [ ] <-- loop iteration number check.\n    /   |\n   /    v\n  |    [ ] <-- vector loop bypass (may consist of multiple blocks).\n  |  /  |\n  | /   v\n  ||   [ ]     <-- vector pre header.\n  |/    |\n  |     v\n  |    [  ] \\\n  |    [  ]_|   <-- vector loop.\n  |     |\n  |     v\n  |   -[ ]   <--- middle-block.\n  |  /  |\n  | /   v\n  -|- >[ ]     <--- new preheader.\n   |    |\n   |    v\n   |   [ ] \\\n   |   [ ]_|   <-- old scalar loop to handle remainder.\n    \\   |\n     \\  v\n      >[ ]     <-- exit block.\n   ...\n   */\n\n  // Get the metadata of the original loop before it gets modified.\n  MDNode *OrigLoopID = OrigLoop->getLoopID();\n\n  // Create an empty vector loop, and prepare basic blocks for the runtime\n  // checks.\n  Loop *Lp = createVectorLoopSkeleton(\"\");\n\n  // Now, compare the new count to zero. If it is zero skip the vector loop and\n  // jump to the scalar loop. This check also covers the case where the\n  // backedge-taken count is uint##_max: adding one to it will overflow leading\n  // to an incorrect trip count of zero. In this (rare) case we will also jump\n  // to the scalar loop.\n  emitMinimumIterationCountCheck(Lp, LoopScalarPreHeader);\n\n  // Generate the code to check any assumptions that we've made for SCEV\n  // expressions.\n  emitSCEVChecks(Lp, LoopScalarPreHeader);\n\n  // Generate the code that checks in runtime if arrays overlap. We put the\n  // checks into a separate block to make the more common case of few elements\n  // faster.\n  emitMemRuntimeChecks(Lp, LoopScalarPreHeader);\n\n  // Some loops have a single integer induction variable, while other loops\n  // don't. One example is c++ iterators that often have multiple pointer\n  // induction variables. In the code below we also support a case where we\n  // don't have a single induction variable.\n  //\n  // We try to obtain an induction variable from the original loop as hard\n  // as possible. However if we don't find one that:\n  //   - is an integer\n  //   - counts from zero, stepping by one\n  //   - is the size of the widest induction variable type\n  // then we create a new one.\n  OldInduction = Legal->getPrimaryInduction();\n  Type *IdxTy = Legal->getWidestInductionType();\n  Value *StartIdx = ConstantInt::get(IdxTy, 0);\n  // The loop step is equal to the vectorization factor (num of SIMD elements)\n  // times the unroll factor (num of SIMD instructions).\n  Builder.SetInsertPoint(&*Lp->getHeader()->getFirstInsertionPt());\n  Value *Step = createStepForVF(Builder, ConstantInt::get(IdxTy, UF), VF);\n  Value *CountRoundDown = getOrCreateVectorTripCount(Lp);\n  Induction =\n      createInductionVariable(Lp, StartIdx, CountRoundDown, Step,\n                              getDebugLocFromInstOrOperands(OldInduction));\n\n  // Emit phis for the new starting index of the scalar loop.\n  createInductionResumeValues(Lp, CountRoundDown);\n\n  return completeLoopSkeleton(Lp, OrigLoopID);\n}\n\n// Fix up external users of the induction variable. At this point, we are\n// in LCSSA form, with all external PHIs that use the IV having one input value,\n// coming from the remainder loop. We need those PHIs to also have a correct\n// value for the IV when arriving directly from the middle block.\nvoid InnerLoopVectorizer::fixupIVUsers(PHINode *OrigPhi,\n                                       const InductionDescriptor &II,\n                                       Value *CountRoundDown, Value *EndValue,\n                                       BasicBlock *MiddleBlock) {\n  // There are two kinds of external IV usages - those that use the value\n  // computed in the last iteration (the PHI) and those that use the penultimate\n  // value (the value that feeds into the phi from the loop latch).\n  // We allow both, but they, obviously, have different values.\n\n  assert(OrigLoop->getUniqueExitBlock() && \"Expected a single exit block\");\n\n  DenseMap<Value *, Value *> MissingVals;\n\n  // An external user of the last iteration's value should see the value that\n  // the remainder loop uses to initialize its own IV.\n  Value *PostInc = OrigPhi->getIncomingValueForBlock(OrigLoop->getLoopLatch());\n  for (User *U : PostInc->users()) {\n    Instruction *UI = cast<Instruction>(U);\n    if (!OrigLoop->contains(UI)) {\n      assert(isa<PHINode>(UI) && \"Expected LCSSA form\");\n      MissingVals[UI] = EndValue;\n    }\n  }\n\n  // An external user of the penultimate value need to see EndValue - Step.\n  // The simplest way to get this is to recompute it from the constituent SCEVs,\n  // that is Start + (Step * (CRD - 1)).\n  for (User *U : OrigPhi->users()) {\n    auto *UI = cast<Instruction>(U);\n    if (!OrigLoop->contains(UI)) {\n      const DataLayout &DL =\n          OrigLoop->getHeader()->getModule()->getDataLayout();\n      assert(isa<PHINode>(UI) && \"Expected LCSSA form\");\n\n      IRBuilder<> B(MiddleBlock->getTerminator());\n\n      // Fast-math-flags propagate from the original induction instruction.\n      if (II.getInductionBinOp() && isa<FPMathOperator>(II.getInductionBinOp()))\n        B.setFastMathFlags(II.getInductionBinOp()->getFastMathFlags());\n\n      Value *CountMinusOne = B.CreateSub(\n          CountRoundDown, ConstantInt::get(CountRoundDown->getType(), 1));\n      Value *CMO =\n          !II.getStep()->getType()->isIntegerTy()\n              ? B.CreateCast(Instruction::SIToFP, CountMinusOne,\n                             II.getStep()->getType())\n              : B.CreateSExtOrTrunc(CountMinusOne, II.getStep()->getType());\n      CMO->setName(\"cast.cmo\");\n      Value *Escape = emitTransformedIndex(B, CMO, PSE.getSE(), DL, II);\n      Escape->setName(\"ind.escape\");\n      MissingVals[UI] = Escape;\n    }\n  }\n\n  for (auto &I : MissingVals) {\n    PHINode *PHI = cast<PHINode>(I.first);\n    // One corner case we have to handle is two IVs \"chasing\" each-other,\n    // that is %IV2 = phi [...], [ %IV1, %latch ]\n    // In this case, if IV1 has an external use, we need to avoid adding both\n    // \"last value of IV1\" and \"penultimate value of IV2\". So, verify that we\n    // don't already have an incoming value for the middle block.\n    if (PHI->getBasicBlockIndex(MiddleBlock) == -1)\n      PHI->addIncoming(I.second, MiddleBlock);\n  }\n}\n\nnamespace {\n\nstruct CSEDenseMapInfo {\n  static bool canHandle(const Instruction *I) {\n    return isa<InsertElementInst>(I) || isa<ExtractElementInst>(I) ||\n           isa<ShuffleVectorInst>(I) || isa<GetElementPtrInst>(I);\n  }\n\n  static inline Instruction *getEmptyKey() {\n    return DenseMapInfo<Instruction *>::getEmptyKey();\n  }\n\n  static inline Instruction *getTombstoneKey() {\n    return DenseMapInfo<Instruction *>::getTombstoneKey();\n  }\n\n  static unsigned getHashValue(const Instruction *I) {\n    assert(canHandle(I) && \"Unknown instruction!\");\n    return hash_combine(I->getOpcode(), hash_combine_range(I->value_op_begin(),\n                                                           I->value_op_end()));\n  }\n\n  static bool isEqual(const Instruction *LHS, const Instruction *RHS) {\n    if (LHS == getEmptyKey() || RHS == getEmptyKey() ||\n        LHS == getTombstoneKey() || RHS == getTombstoneKey())\n      return LHS == RHS;\n    return LHS->isIdenticalTo(RHS);\n  }\n};\n\n} // end anonymous namespace\n\n///Perform cse of induction variable instructions.\nstatic void cse(BasicBlock *BB) {\n  // Perform simple cse.\n  SmallDenseMap<Instruction *, Instruction *, 4, CSEDenseMapInfo> CSEMap;\n  for (BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E;) {\n    Instruction *In = &*I++;\n\n    if (!CSEDenseMapInfo::canHandle(In))\n      continue;\n\n    // Check if we can replace this instruction with any of the\n    // visited instructions.\n    if (Instruction *V = CSEMap.lookup(In)) {\n      In->replaceAllUsesWith(V);\n      In->eraseFromParent();\n      continue;\n    }\n\n    CSEMap[In] = In;\n  }\n}\n\nInstructionCost\nLoopVectorizationCostModel::getVectorCallCost(CallInst *CI, ElementCount VF,\n                                              bool &NeedToScalarize) {\n  Function *F = CI->getCalledFunction();\n  Type *ScalarRetTy = CI->getType();\n  SmallVector<Type *, 4> Tys, ScalarTys;\n  for (auto &ArgOp : CI->arg_operands())\n    ScalarTys.push_back(ArgOp->getType());\n\n  // Estimate cost of scalarized vector call. The source operands are assumed\n  // to be vectors, so we need to extract individual elements from there,\n  // execute VF scalar calls, and then gather the result into the vector return\n  // value.\n  InstructionCost ScalarCallCost =\n      TTI.getCallInstrCost(F, ScalarRetTy, ScalarTys, TTI::TCK_RecipThroughput);\n  if (VF.isScalar())\n    return ScalarCallCost;\n\n  // Compute corresponding vector type for return value and arguments.\n  Type *RetTy = ToVectorTy(ScalarRetTy, VF);\n  for (Type *ScalarTy : ScalarTys)\n    Tys.push_back(ToVectorTy(ScalarTy, VF));\n\n  // Compute costs of unpacking argument values for the scalar calls and\n  // packing the return values to a vector.\n  InstructionCost ScalarizationCost = getScalarizationOverhead(CI, VF);\n\n  InstructionCost Cost =\n      ScalarCallCost * VF.getKnownMinValue() + ScalarizationCost;\n\n  // If we can't emit a vector call for this function, then the currently found\n  // cost is the cost we need to return.\n  NeedToScalarize = true;\n  VFShape Shape = VFShape::get(*CI, VF, false /*HasGlobalPred*/);\n  Function *VecFunc = VFDatabase(*CI).getVectorizedFunction(Shape);\n\n  if (!TLI || CI->isNoBuiltin() || !VecFunc)\n    return Cost;\n\n  // If the corresponding vector cost is cheaper, return its cost.\n  InstructionCost VectorCallCost =\n      TTI.getCallInstrCost(nullptr, RetTy, Tys, TTI::TCK_RecipThroughput);\n  if (VectorCallCost < Cost) {\n    NeedToScalarize = false;\n    Cost = VectorCallCost;\n  }\n  return Cost;\n}\n\nstatic Type *MaybeVectorizeType(Type *Elt, ElementCount VF) {\n  if (VF.isScalar() || (!Elt->isIntOrPtrTy() && !Elt->isFloatingPointTy()))\n    return Elt;\n  return VectorType::get(Elt, VF);\n}\n\nInstructionCost\nLoopVectorizationCostModel::getVectorIntrinsicCost(CallInst *CI,\n                                                   ElementCount VF) {\n  Intrinsic::ID ID = getVectorIntrinsicIDForCall(CI, TLI);\n  assert(ID && \"Expected intrinsic call!\");\n  Type *RetTy = MaybeVectorizeType(CI->getType(), VF);\n  FastMathFlags FMF;\n  if (auto *FPMO = dyn_cast<FPMathOperator>(CI))\n    FMF = FPMO->getFastMathFlags();\n\n  SmallVector<const Value *> Arguments(CI->arg_begin(), CI->arg_end());\n  FunctionType *FTy = CI->getCalledFunction()->getFunctionType();\n  SmallVector<Type *> ParamTys;\n  std::transform(FTy->param_begin(), FTy->param_end(),\n                 std::back_inserter(ParamTys),\n                 [&](Type *Ty) { return MaybeVectorizeType(Ty, VF); });\n\n  IntrinsicCostAttributes CostAttrs(ID, RetTy, Arguments, ParamTys, FMF,\n                                    dyn_cast<IntrinsicInst>(CI));\n  return TTI.getIntrinsicInstrCost(CostAttrs,\n                                   TargetTransformInfo::TCK_RecipThroughput);\n}\n\nstatic Type *smallestIntegerVectorType(Type *T1, Type *T2) {\n  auto *I1 = cast<IntegerType>(cast<VectorType>(T1)->getElementType());\n  auto *I2 = cast<IntegerType>(cast<VectorType>(T2)->getElementType());\n  return I1->getBitWidth() < I2->getBitWidth() ? T1 : T2;\n}\n\nstatic Type *largestIntegerVectorType(Type *T1, Type *T2) {\n  auto *I1 = cast<IntegerType>(cast<VectorType>(T1)->getElementType());\n  auto *I2 = cast<IntegerType>(cast<VectorType>(T2)->getElementType());\n  return I1->getBitWidth() > I2->getBitWidth() ? T1 : T2;\n}\n\nvoid InnerLoopVectorizer::truncateToMinimalBitwidths(VPTransformState &State) {\n  // For every instruction `I` in MinBWs, truncate the operands, create a\n  // truncated version of `I` and reextend its result. InstCombine runs\n  // later and will remove any ext/trunc pairs.\n  SmallPtrSet<Value *, 4> Erased;\n  for (const auto &KV : Cost->getMinimalBitwidths()) {\n    // If the value wasn't vectorized, we must maintain the original scalar\n    // type. The absence of the value from State indicates that it\n    // wasn't vectorized.\n    VPValue *Def = State.Plan->getVPValue(KV.first);\n    if (!State.hasAnyVectorValue(Def))\n      continue;\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      Value *I = State.get(Def, Part);\n      if (Erased.count(I) || I->use_empty() || !isa<Instruction>(I))\n        continue;\n      Type *OriginalTy = I->getType();\n      Type *ScalarTruncatedTy =\n          IntegerType::get(OriginalTy->getContext(), KV.second);\n      auto *TruncatedTy = FixedVectorType::get(\n          ScalarTruncatedTy,\n          cast<FixedVectorType>(OriginalTy)->getNumElements());\n      if (TruncatedTy == OriginalTy)\n        continue;\n\n      IRBuilder<> B(cast<Instruction>(I));\n      auto ShrinkOperand = [&](Value *V) -> Value * {\n        if (auto *ZI = dyn_cast<ZExtInst>(V))\n          if (ZI->getSrcTy() == TruncatedTy)\n            return ZI->getOperand(0);\n        return B.CreateZExtOrTrunc(V, TruncatedTy);\n      };\n\n      // The actual instruction modification depends on the instruction type,\n      // unfortunately.\n      Value *NewI = nullptr;\n      if (auto *BO = dyn_cast<BinaryOperator>(I)) {\n        NewI = B.CreateBinOp(BO->getOpcode(), ShrinkOperand(BO->getOperand(0)),\n                             ShrinkOperand(BO->getOperand(1)));\n\n        // Any wrapping introduced by shrinking this operation shouldn't be\n        // considered undefined behavior. So, we can't unconditionally copy\n        // arithmetic wrapping flags to NewI.\n        cast<BinaryOperator>(NewI)->copyIRFlags(I, /*IncludeWrapFlags=*/false);\n      } else if (auto *CI = dyn_cast<ICmpInst>(I)) {\n        NewI =\n            B.CreateICmp(CI->getPredicate(), ShrinkOperand(CI->getOperand(0)),\n                         ShrinkOperand(CI->getOperand(1)));\n      } else if (auto *SI = dyn_cast<SelectInst>(I)) {\n        NewI = B.CreateSelect(SI->getCondition(),\n                              ShrinkOperand(SI->getTrueValue()),\n                              ShrinkOperand(SI->getFalseValue()));\n      } else if (auto *CI = dyn_cast<CastInst>(I)) {\n        switch (CI->getOpcode()) {\n        default:\n          llvm_unreachable(\"Unhandled cast!\");\n        case Instruction::Trunc:\n          NewI = ShrinkOperand(CI->getOperand(0));\n          break;\n        case Instruction::SExt:\n          NewI = B.CreateSExtOrTrunc(\n              CI->getOperand(0),\n              smallestIntegerVectorType(OriginalTy, TruncatedTy));\n          break;\n        case Instruction::ZExt:\n          NewI = B.CreateZExtOrTrunc(\n              CI->getOperand(0),\n              smallestIntegerVectorType(OriginalTy, TruncatedTy));\n          break;\n        }\n      } else if (auto *SI = dyn_cast<ShuffleVectorInst>(I)) {\n        auto Elements0 = cast<FixedVectorType>(SI->getOperand(0)->getType())\n                             ->getNumElements();\n        auto *O0 = B.CreateZExtOrTrunc(\n            SI->getOperand(0),\n            FixedVectorType::get(ScalarTruncatedTy, Elements0));\n        auto Elements1 = cast<FixedVectorType>(SI->getOperand(1)->getType())\n                             ->getNumElements();\n        auto *O1 = B.CreateZExtOrTrunc(\n            SI->getOperand(1),\n            FixedVectorType::get(ScalarTruncatedTy, Elements1));\n\n        NewI = B.CreateShuffleVector(O0, O1, SI->getShuffleMask());\n      } else if (isa<LoadInst>(I) || isa<PHINode>(I)) {\n        // Don't do anything with the operands, just extend the result.\n        continue;\n      } else if (auto *IE = dyn_cast<InsertElementInst>(I)) {\n        auto Elements = cast<FixedVectorType>(IE->getOperand(0)->getType())\n                            ->getNumElements();\n        auto *O0 = B.CreateZExtOrTrunc(\n            IE->getOperand(0),\n            FixedVectorType::get(ScalarTruncatedTy, Elements));\n        auto *O1 = B.CreateZExtOrTrunc(IE->getOperand(1), ScalarTruncatedTy);\n        NewI = B.CreateInsertElement(O0, O1, IE->getOperand(2));\n      } else if (auto *EE = dyn_cast<ExtractElementInst>(I)) {\n        auto Elements = cast<FixedVectorType>(EE->getOperand(0)->getType())\n                            ->getNumElements();\n        auto *O0 = B.CreateZExtOrTrunc(\n            EE->getOperand(0),\n            FixedVectorType::get(ScalarTruncatedTy, Elements));\n        NewI = B.CreateExtractElement(O0, EE->getOperand(2));\n      } else {\n        // If we don't know what to do, be conservative and don't do anything.\n        continue;\n      }\n\n      // Lastly, extend the result.\n      NewI->takeName(cast<Instruction>(I));\n      Value *Res = B.CreateZExtOrTrunc(NewI, OriginalTy);\n      I->replaceAllUsesWith(Res);\n      cast<Instruction>(I)->eraseFromParent();\n      Erased.insert(I);\n      State.reset(Def, Res, Part);\n    }\n  }\n\n  // We'll have created a bunch of ZExts that are now parentless. Clean up.\n  for (const auto &KV : Cost->getMinimalBitwidths()) {\n    // If the value wasn't vectorized, we must maintain the original scalar\n    // type. The absence of the value from State indicates that it\n    // wasn't vectorized.\n    VPValue *Def = State.Plan->getVPValue(KV.first);\n    if (!State.hasAnyVectorValue(Def))\n      continue;\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      Value *I = State.get(Def, Part);\n      ZExtInst *Inst = dyn_cast<ZExtInst>(I);\n      if (Inst && Inst->use_empty()) {\n        Value *NewI = Inst->getOperand(0);\n        Inst->eraseFromParent();\n        State.reset(Def, NewI, Part);\n      }\n    }\n  }\n}\n\nvoid InnerLoopVectorizer::fixVectorizedLoop(VPTransformState &State) {\n  // Insert truncates and extends for any truncated instructions as hints to\n  // InstCombine.\n  if (VF.isVector())\n    truncateToMinimalBitwidths(State);\n\n  // Fix widened non-induction PHIs by setting up the PHI operands.\n  if (OrigPHIsToFix.size()) {\n    assert(EnableVPlanNativePath &&\n           \"Unexpected non-induction PHIs for fixup in non VPlan-native path\");\n    fixNonInductionPHIs(State);\n  }\n\n  // At this point every instruction in the original loop is widened to a\n  // vector form. Now we need to fix the recurrences in the loop. These PHI\n  // nodes are currently empty because we did not want to introduce cycles.\n  // This is the second stage of vectorizing recurrences.\n  fixCrossIterationPHIs(State);\n\n  // Forget the original basic block.\n  PSE.getSE()->forgetLoop(OrigLoop);\n\n  // Fix-up external users of the induction variables.\n  for (auto &Entry : Legal->getInductionVars())\n    fixupIVUsers(Entry.first, Entry.second,\n                 getOrCreateVectorTripCount(LI->getLoopFor(LoopVectorBody)),\n                 IVEndValues[Entry.first], LoopMiddleBlock);\n\n  fixLCSSAPHIs(State);\n  for (Instruction *PI : PredicatedInstructions)\n    sinkScalarOperands(&*PI);\n\n  // Remove redundant induction instructions.\n  cse(LoopVectorBody);\n\n  // Set/update profile weights for the vector and remainder loops as original\n  // loop iterations are now distributed among them. Note that original loop\n  // represented by LoopScalarBody becomes remainder loop after vectorization.\n  //\n  // For cases like foldTailByMasking() and requiresScalarEpiloque() we may\n  // end up getting slightly roughened result but that should be OK since\n  // profile is not inherently precise anyway. Note also possible bypass of\n  // vector code caused by legality checks is ignored, assigning all the weight\n  // to the vector loop, optimistically.\n  //\n  // For scalable vectorization we can't know at compile time how many iterations\n  // of the loop are handled in one vector iteration, so instead assume a pessimistic\n  // vscale of '1'.\n  setProfileInfoAfterUnrolling(\n      LI->getLoopFor(LoopScalarBody), LI->getLoopFor(LoopVectorBody),\n      LI->getLoopFor(LoopScalarBody), VF.getKnownMinValue() * UF);\n}\n\nvoid InnerLoopVectorizer::fixCrossIterationPHIs(VPTransformState &State) {\n  // In order to support recurrences we need to be able to vectorize Phi nodes.\n  // Phi nodes have cycles, so we need to vectorize them in two stages. This is\n  // stage #2: We now need to fix the recurrences by adding incoming edges to\n  // the currently empty PHI nodes. At this point every instruction in the\n  // original loop is widened to a vector form so we can use them to construct\n  // the incoming edges.\n  for (PHINode &Phi : OrigLoop->getHeader()->phis()) {\n    // Handle first-order recurrences and reductions that need to be fixed.\n    if (Legal->isFirstOrderRecurrence(&Phi))\n      fixFirstOrderRecurrence(&Phi, State);\n    else if (Legal->isReductionVariable(&Phi))\n      fixReduction(&Phi, State);\n  }\n}\n\nvoid InnerLoopVectorizer::fixFirstOrderRecurrence(PHINode *Phi,\n                                                  VPTransformState &State) {\n  // This is the second phase of vectorizing first-order recurrences. An\n  // overview of the transformation is described below. Suppose we have the\n  // following loop.\n  //\n  //   for (int i = 0; i < n; ++i)\n  //     b[i] = a[i] - a[i - 1];\n  //\n  // There is a first-order recurrence on \"a\". For this loop, the shorthand\n  // scalar IR looks like:\n  //\n  //   scalar.ph:\n  //     s_init = a[-1]\n  //     br scalar.body\n  //\n  //   scalar.body:\n  //     i = phi [0, scalar.ph], [i+1, scalar.body]\n  //     s1 = phi [s_init, scalar.ph], [s2, scalar.body]\n  //     s2 = a[i]\n  //     b[i] = s2 - s1\n  //     br cond, scalar.body, ...\n  //\n  // In this example, s1 is a recurrence because it's value depends on the\n  // previous iteration. In the first phase of vectorization, we created a\n  // temporary value for s1. We now complete the vectorization and produce the\n  // shorthand vector IR shown below (for VF = 4, UF = 1).\n  //\n  //   vector.ph:\n  //     v_init = vector(..., ..., ..., a[-1])\n  //     br vector.body\n  //\n  //   vector.body\n  //     i = phi [0, vector.ph], [i+4, vector.body]\n  //     v1 = phi [v_init, vector.ph], [v2, vector.body]\n  //     v2 = a[i, i+1, i+2, i+3];\n  //     v3 = vector(v1(3), v2(0, 1, 2))\n  //     b[i, i+1, i+2, i+3] = v2 - v3\n  //     br cond, vector.body, middle.block\n  //\n  //   middle.block:\n  //     x = v2(3)\n  //     br scalar.ph\n  //\n  //   scalar.ph:\n  //     s_init = phi [x, middle.block], [a[-1], otherwise]\n  //     br scalar.body\n  //\n  // After execution completes the vector loop, we extract the next value of\n  // the recurrence (x) to use as the initial value in the scalar loop.\n\n  // Get the original loop preheader and single loop latch.\n  auto *Preheader = OrigLoop->getLoopPreheader();\n  auto *Latch = OrigLoop->getLoopLatch();\n\n  // Get the initial and previous values of the scalar recurrence.\n  auto *ScalarInit = Phi->getIncomingValueForBlock(Preheader);\n  auto *Previous = Phi->getIncomingValueForBlock(Latch);\n\n  // Create a vector from the initial value.\n  auto *VectorInit = ScalarInit;\n  if (VF.isVector()) {\n    Builder.SetInsertPoint(LoopVectorPreHeader->getTerminator());\n    assert(!VF.isScalable() && \"VF is assumed to be non scalable.\");\n    VectorInit = Builder.CreateInsertElement(\n        PoisonValue::get(VectorType::get(VectorInit->getType(), VF)), VectorInit,\n        Builder.getInt32(VF.getKnownMinValue() - 1), \"vector.recur.init\");\n  }\n\n  VPValue *PhiDef = State.Plan->getVPValue(Phi);\n  VPValue *PreviousDef = State.Plan->getVPValue(Previous);\n  // We constructed a temporary phi node in the first phase of vectorization.\n  // This phi node will eventually be deleted.\n  Builder.SetInsertPoint(cast<Instruction>(State.get(PhiDef, 0)));\n\n  // Create a phi node for the new recurrence. The current value will either be\n  // the initial value inserted into a vector or loop-varying vector value.\n  auto *VecPhi = Builder.CreatePHI(VectorInit->getType(), 2, \"vector.recur\");\n  VecPhi->addIncoming(VectorInit, LoopVectorPreHeader);\n\n  // Get the vectorized previous value of the last part UF - 1. It appears last\n  // among all unrolled iterations, due to the order of their construction.\n  Value *PreviousLastPart = State.get(PreviousDef, UF - 1);\n\n  // Find and set the insertion point after the previous value if it is an\n  // instruction.\n  BasicBlock::iterator InsertPt;\n  // Note that the previous value may have been constant-folded so it is not\n  // guaranteed to be an instruction in the vector loop.\n  // FIXME: Loop invariant values do not form recurrences. We should deal with\n  //        them earlier.\n  if (LI->getLoopFor(LoopVectorBody)->isLoopInvariant(PreviousLastPart))\n    InsertPt = LoopVectorBody->getFirstInsertionPt();\n  else {\n    Instruction *PreviousInst = cast<Instruction>(PreviousLastPart);\n    if (isa<PHINode>(PreviousLastPart))\n      // If the previous value is a phi node, we should insert after all the phi\n      // nodes in the block containing the PHI to avoid breaking basic block\n      // verification. Note that the basic block may be different to\n      // LoopVectorBody, in case we predicate the loop.\n      InsertPt = PreviousInst->getParent()->getFirstInsertionPt();\n    else\n      InsertPt = ++PreviousInst->getIterator();\n  }\n  Builder.SetInsertPoint(&*InsertPt);\n\n  // We will construct a vector for the recurrence by combining the values for\n  // the current and previous iterations. This is the required shuffle mask.\n  assert(!VF.isScalable());\n  SmallVector<int, 8> ShuffleMask(VF.getKnownMinValue());\n  ShuffleMask[0] = VF.getKnownMinValue() - 1;\n  for (unsigned I = 1; I < VF.getKnownMinValue(); ++I)\n    ShuffleMask[I] = I + VF.getKnownMinValue() - 1;\n\n  // The vector from which to take the initial value for the current iteration\n  // (actual or unrolled). Initially, this is the vector phi node.\n  Value *Incoming = VecPhi;\n\n  // Shuffle the current and previous vector and update the vector parts.\n  for (unsigned Part = 0; Part < UF; ++Part) {\n    Value *PreviousPart = State.get(PreviousDef, Part);\n    Value *PhiPart = State.get(PhiDef, Part);\n    auto *Shuffle =\n        VF.isVector()\n            ? Builder.CreateShuffleVector(Incoming, PreviousPart, ShuffleMask)\n            : Incoming;\n    PhiPart->replaceAllUsesWith(Shuffle);\n    cast<Instruction>(PhiPart)->eraseFromParent();\n    State.reset(PhiDef, Shuffle, Part);\n    Incoming = PreviousPart;\n  }\n\n  // Fix the latch value of the new recurrence in the vector loop.\n  VecPhi->addIncoming(Incoming, LI->getLoopFor(LoopVectorBody)->getLoopLatch());\n\n  // Extract the last vector element in the middle block. This will be the\n  // initial value for the recurrence when jumping to the scalar loop.\n  auto *ExtractForScalar = Incoming;\n  if (VF.isVector()) {\n    Builder.SetInsertPoint(LoopMiddleBlock->getTerminator());\n    ExtractForScalar = Builder.CreateExtractElement(\n        ExtractForScalar, Builder.getInt32(VF.getKnownMinValue() - 1),\n        \"vector.recur.extract\");\n  }\n  // Extract the second last element in the middle block if the\n  // Phi is used outside the loop. We need to extract the phi itself\n  // and not the last element (the phi update in the current iteration). This\n  // will be the value when jumping to the exit block from the LoopMiddleBlock,\n  // when the scalar loop is not run at all.\n  Value *ExtractForPhiUsedOutsideLoop = nullptr;\n  if (VF.isVector())\n    ExtractForPhiUsedOutsideLoop = Builder.CreateExtractElement(\n        Incoming, Builder.getInt32(VF.getKnownMinValue() - 2),\n        \"vector.recur.extract.for.phi\");\n  // When loop is unrolled without vectorizing, initialize\n  // ExtractForPhiUsedOutsideLoop with the value just prior to unrolled value of\n  // `Incoming`. This is analogous to the vectorized case above: extracting the\n  // second last element when VF > 1.\n  else if (UF > 1)\n    ExtractForPhiUsedOutsideLoop = State.get(PreviousDef, UF - 2);\n\n  // Fix the initial value of the original recurrence in the scalar loop.\n  Builder.SetInsertPoint(&*LoopScalarPreHeader->begin());\n  auto *Start = Builder.CreatePHI(Phi->getType(), 2, \"scalar.recur.init\");\n  for (auto *BB : predecessors(LoopScalarPreHeader)) {\n    auto *Incoming = BB == LoopMiddleBlock ? ExtractForScalar : ScalarInit;\n    Start->addIncoming(Incoming, BB);\n  }\n\n  Phi->setIncomingValueForBlock(LoopScalarPreHeader, Start);\n  Phi->setName(\"scalar.recur\");\n\n  // Finally, fix users of the recurrence outside the loop. The users will need\n  // either the last value of the scalar recurrence or the last value of the\n  // vector recurrence we extracted in the middle block. Since the loop is in\n  // LCSSA form, we just need to find all the phi nodes for the original scalar\n  // recurrence in the exit block, and then add an edge for the middle block.\n  // Note that LCSSA does not imply single entry when the original scalar loop\n  // had multiple exiting edges (as we always run the last iteration in the\n  // scalar epilogue); in that case, the exiting path through middle will be\n  // dynamically dead and the value picked for the phi doesn't matter.\n  for (PHINode &LCSSAPhi : LoopExitBlock->phis())\n    if (any_of(LCSSAPhi.incoming_values(),\n               [Phi](Value *V) { return V == Phi; }))\n      LCSSAPhi.addIncoming(ExtractForPhiUsedOutsideLoop, LoopMiddleBlock);\n}\n\nvoid InnerLoopVectorizer::fixReduction(PHINode *Phi, VPTransformState &State) {\n  // Get it's reduction variable descriptor.\n  assert(Legal->isReductionVariable(Phi) &&\n         \"Unable to find the reduction variable\");\n  RecurrenceDescriptor RdxDesc = Legal->getReductionVars()[Phi];\n\n  RecurKind RK = RdxDesc.getRecurrenceKind();\n  TrackingVH<Value> ReductionStartValue = RdxDesc.getRecurrenceStartValue();\n  Instruction *LoopExitInst = RdxDesc.getLoopExitInstr();\n  setDebugLocFromInst(Builder, ReductionStartValue);\n  bool IsInLoopReductionPhi = Cost->isInLoopReduction(Phi);\n\n  VPValue *LoopExitInstDef = State.Plan->getVPValue(LoopExitInst);\n  // This is the vector-clone of the value that leaves the loop.\n  Type *VecTy = State.get(LoopExitInstDef, 0)->getType();\n\n  // Wrap flags are in general invalid after vectorization, clear them.\n  clearReductionWrapFlags(RdxDesc, State);\n\n  // Fix the vector-loop phi.\n\n  // Reductions do not have to start at zero. They can start with\n  // any loop invariant values.\n  BasicBlock *Latch = OrigLoop->getLoopLatch();\n  Value *LoopVal = Phi->getIncomingValueForBlock(Latch);\n\n  for (unsigned Part = 0; Part < UF; ++Part) {\n    Value *VecRdxPhi = State.get(State.Plan->getVPValue(Phi), Part);\n    Value *Val = State.get(State.Plan->getVPValue(LoopVal), Part);\n    cast<PHINode>(VecRdxPhi)\n      ->addIncoming(Val, LI->getLoopFor(LoopVectorBody)->getLoopLatch());\n  }\n\n  // Before each round, move the insertion point right between\n  // the PHIs and the values we are going to write.\n  // This allows us to write both PHINodes and the extractelement\n  // instructions.\n  Builder.SetInsertPoint(&*LoopMiddleBlock->getFirstInsertionPt());\n\n  setDebugLocFromInst(Builder, LoopExitInst);\n\n  // If tail is folded by masking, the vector value to leave the loop should be\n  // a Select choosing between the vectorized LoopExitInst and vectorized Phi,\n  // instead of the former. For an inloop reduction the reduction will already\n  // be predicated, and does not need to be handled here.\n  if (Cost->foldTailByMasking() && !IsInLoopReductionPhi) {\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      Value *VecLoopExitInst = State.get(LoopExitInstDef, Part);\n      Value *Sel = nullptr;\n      for (User *U : VecLoopExitInst->users()) {\n        if (isa<SelectInst>(U)) {\n          assert(!Sel && \"Reduction exit feeding two selects\");\n          Sel = U;\n        } else\n          assert(isa<PHINode>(U) && \"Reduction exit must feed Phi's or select\");\n      }\n      assert(Sel && \"Reduction exit feeds no select\");\n      State.reset(LoopExitInstDef, Sel, Part);\n\n      // If the target can create a predicated operator for the reduction at no\n      // extra cost in the loop (for example a predicated vadd), it can be\n      // cheaper for the select to remain in the loop than be sunk out of it,\n      // and so use the select value for the phi instead of the old\n      // LoopExitValue.\n      RecurrenceDescriptor RdxDesc = Legal->getReductionVars()[Phi];\n      if (PreferPredicatedReductionSelect ||\n          TTI->preferPredicatedReductionSelect(\n              RdxDesc.getOpcode(), Phi->getType(),\n              TargetTransformInfo::ReductionFlags())) {\n        auto *VecRdxPhi =\n            cast<PHINode>(State.get(State.Plan->getVPValue(Phi), Part));\n        VecRdxPhi->setIncomingValueForBlock(\n            LI->getLoopFor(LoopVectorBody)->getLoopLatch(), Sel);\n      }\n    }\n  }\n\n  // If the vector reduction can be performed in a smaller type, we truncate\n  // then extend the loop exit value to enable InstCombine to evaluate the\n  // entire expression in the smaller type.\n  if (VF.isVector() && Phi->getType() != RdxDesc.getRecurrenceType()) {\n    assert(!IsInLoopReductionPhi && \"Unexpected truncated inloop reduction!\");\n    assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n    Type *RdxVecTy = VectorType::get(RdxDesc.getRecurrenceType(), VF);\n    Builder.SetInsertPoint(\n        LI->getLoopFor(LoopVectorBody)->getLoopLatch()->getTerminator());\n    VectorParts RdxParts(UF);\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      RdxParts[Part] = State.get(LoopExitInstDef, Part);\n      Value *Trunc = Builder.CreateTrunc(RdxParts[Part], RdxVecTy);\n      Value *Extnd = RdxDesc.isSigned() ? Builder.CreateSExt(Trunc, VecTy)\n                                        : Builder.CreateZExt(Trunc, VecTy);\n      for (Value::user_iterator UI = RdxParts[Part]->user_begin();\n           UI != RdxParts[Part]->user_end();)\n        if (*UI != Trunc) {\n          (*UI++)->replaceUsesOfWith(RdxParts[Part], Extnd);\n          RdxParts[Part] = Extnd;\n        } else {\n          ++UI;\n        }\n    }\n    Builder.SetInsertPoint(&*LoopMiddleBlock->getFirstInsertionPt());\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      RdxParts[Part] = Builder.CreateTrunc(RdxParts[Part], RdxVecTy);\n      State.reset(LoopExitInstDef, RdxParts[Part], Part);\n    }\n  }\n\n  // Reduce all of the unrolled parts into a single vector.\n  Value *ReducedPartRdx = State.get(LoopExitInstDef, 0);\n  unsigned Op = RecurrenceDescriptor::getOpcode(RK);\n\n  // The middle block terminator has already been assigned a DebugLoc here (the\n  // OrigLoop's single latch terminator). We want the whole middle block to\n  // appear to execute on this line because: (a) it is all compiler generated,\n  // (b) these instructions are always executed after evaluating the latch\n  // conditional branch, and (c) other passes may add new predecessors which\n  // terminate on this line. This is the easiest way to ensure we don't\n  // accidentally cause an extra step back into the loop while debugging.\n  setDebugLocFromInst(Builder, LoopMiddleBlock->getTerminator());\n  {\n    // Floating-point operations should have some FMF to enable the reduction.\n    IRBuilderBase::FastMathFlagGuard FMFG(Builder);\n    Builder.setFastMathFlags(RdxDesc.getFastMathFlags());\n    for (unsigned Part = 1; Part < UF; ++Part) {\n      Value *RdxPart = State.get(LoopExitInstDef, Part);\n      if (Op != Instruction::ICmp && Op != Instruction::FCmp) {\n        ReducedPartRdx = Builder.CreateBinOp(\n            (Instruction::BinaryOps)Op, RdxPart, ReducedPartRdx, \"bin.rdx\");\n      } else {\n        ReducedPartRdx = createMinMaxOp(Builder, RK, ReducedPartRdx, RdxPart);\n      }\n    }\n  }\n\n  // Create the reduction after the loop. Note that inloop reductions create the\n  // target reduction in the loop using a Reduction recipe.\n  if (VF.isVector() && !IsInLoopReductionPhi) {\n    ReducedPartRdx =\n        createTargetReduction(Builder, TTI, RdxDesc, ReducedPartRdx);\n    // If the reduction can be performed in a smaller type, we need to extend\n    // the reduction to the wider type before we branch to the original loop.\n    if (Phi->getType() != RdxDesc.getRecurrenceType())\n      ReducedPartRdx =\n        RdxDesc.isSigned()\n        ? Builder.CreateSExt(ReducedPartRdx, Phi->getType())\n        : Builder.CreateZExt(ReducedPartRdx, Phi->getType());\n  }\n\n  // Create a phi node that merges control-flow from the backedge-taken check\n  // block and the middle block.\n  PHINode *BCBlockPhi = PHINode::Create(Phi->getType(), 2, \"bc.merge.rdx\",\n                                        LoopScalarPreHeader->getTerminator());\n  for (unsigned I = 0, E = LoopBypassBlocks.size(); I != E; ++I)\n    BCBlockPhi->addIncoming(ReductionStartValue, LoopBypassBlocks[I]);\n  BCBlockPhi->addIncoming(ReducedPartRdx, LoopMiddleBlock);\n\n  // Now, we need to fix the users of the reduction variable\n  // inside and outside of the scalar remainder loop.\n\n  // We know that the loop is in LCSSA form. We need to update the PHI nodes\n  // in the exit blocks.  See comment on analogous loop in\n  // fixFirstOrderRecurrence for a more complete explaination of the logic.\n  for (PHINode &LCSSAPhi : LoopExitBlock->phis())\n    if (any_of(LCSSAPhi.incoming_values(),\n               [LoopExitInst](Value *V) { return V == LoopExitInst; }))\n      LCSSAPhi.addIncoming(ReducedPartRdx, LoopMiddleBlock);\n\n  // Fix the scalar loop reduction variable with the incoming reduction sum\n  // from the vector body and from the backedge value.\n  int IncomingEdgeBlockIdx =\n    Phi->getBasicBlockIndex(OrigLoop->getLoopLatch());\n  assert(IncomingEdgeBlockIdx >= 0 && \"Invalid block index\");\n  // Pick the other block.\n  int SelfEdgeBlockIdx = (IncomingEdgeBlockIdx ? 0 : 1);\n  Phi->setIncomingValue(SelfEdgeBlockIdx, BCBlockPhi);\n  Phi->setIncomingValue(IncomingEdgeBlockIdx, LoopExitInst);\n}\n\nvoid InnerLoopVectorizer::clearReductionWrapFlags(RecurrenceDescriptor &RdxDesc,\n                                                  VPTransformState &State) {\n  RecurKind RK = RdxDesc.getRecurrenceKind();\n  if (RK != RecurKind::Add && RK != RecurKind::Mul)\n    return;\n\n  Instruction *LoopExitInstr = RdxDesc.getLoopExitInstr();\n  assert(LoopExitInstr && \"null loop exit instruction\");\n  SmallVector<Instruction *, 8> Worklist;\n  SmallPtrSet<Instruction *, 8> Visited;\n  Worklist.push_back(LoopExitInstr);\n  Visited.insert(LoopExitInstr);\n\n  while (!Worklist.empty()) {\n    Instruction *Cur = Worklist.pop_back_val();\n    if (isa<OverflowingBinaryOperator>(Cur))\n      for (unsigned Part = 0; Part < UF; ++Part) {\n        Value *V = State.get(State.Plan->getVPValue(Cur), Part);\n        cast<Instruction>(V)->dropPoisonGeneratingFlags();\n      }\n\n    for (User *U : Cur->users()) {\n      Instruction *UI = cast<Instruction>(U);\n      if ((Cur != LoopExitInstr || OrigLoop->contains(UI->getParent())) &&\n          Visited.insert(UI).second)\n        Worklist.push_back(UI);\n    }\n  }\n}\n\nvoid InnerLoopVectorizer::fixLCSSAPHIs(VPTransformState &State) {\n  for (PHINode &LCSSAPhi : LoopExitBlock->phis()) {\n    if (LCSSAPhi.getBasicBlockIndex(LoopMiddleBlock) != -1)\n      // Some phis were already hand updated by the reduction and recurrence\n      // code above, leave them alone.\n      continue;\n\n    auto *IncomingValue = LCSSAPhi.getIncomingValue(0);\n    // Non-instruction incoming values will have only one value.\n\n    VPLane Lane = VPLane::getFirstLane();\n    if (isa<Instruction>(IncomingValue) &&\n        !Cost->isUniformAfterVectorization(cast<Instruction>(IncomingValue),\n                                           VF))\n      Lane = VPLane::getLastLaneForVF(VF);\n\n    // Can be a loop invariant incoming value or the last scalar value to be\n    // extracted from the vectorized loop.\n    Builder.SetInsertPoint(LoopMiddleBlock->getTerminator());\n    Value *lastIncomingValue =\n        OrigLoop->isLoopInvariant(IncomingValue)\n            ? IncomingValue\n            : State.get(State.Plan->getVPValue(IncomingValue),\n                        VPIteration(UF - 1, Lane));\n    LCSSAPhi.addIncoming(lastIncomingValue, LoopMiddleBlock);\n  }\n}\n\nvoid InnerLoopVectorizer::sinkScalarOperands(Instruction *PredInst) {\n  // The basic block and loop containing the predicated instruction.\n  auto *PredBB = PredInst->getParent();\n  auto *VectorLoop = LI->getLoopFor(PredBB);\n\n  // Initialize a worklist with the operands of the predicated instruction.\n  SetVector<Value *> Worklist(PredInst->op_begin(), PredInst->op_end());\n\n  // Holds instructions that we need to analyze again. An instruction may be\n  // reanalyzed if we don't yet know if we can sink it or not.\n  SmallVector<Instruction *, 8> InstsToReanalyze;\n\n  // Returns true if a given use occurs in the predicated block. Phi nodes use\n  // their operands in their corresponding predecessor blocks.\n  auto isBlockOfUsePredicated = [&](Use &U) -> bool {\n    auto *I = cast<Instruction>(U.getUser());\n    BasicBlock *BB = I->getParent();\n    if (auto *Phi = dyn_cast<PHINode>(I))\n      BB = Phi->getIncomingBlock(\n          PHINode::getIncomingValueNumForOperand(U.getOperandNo()));\n    return BB == PredBB;\n  };\n\n  // Iteratively sink the scalarized operands of the predicated instruction\n  // into the block we created for it. When an instruction is sunk, it's\n  // operands are then added to the worklist. The algorithm ends after one pass\n  // through the worklist doesn't sink a single instruction.\n  bool Changed;\n  do {\n    // Add the instructions that need to be reanalyzed to the worklist, and\n    // reset the changed indicator.\n    Worklist.insert(InstsToReanalyze.begin(), InstsToReanalyze.end());\n    InstsToReanalyze.clear();\n    Changed = false;\n\n    while (!Worklist.empty()) {\n      auto *I = dyn_cast<Instruction>(Worklist.pop_back_val());\n\n      // We can't sink an instruction if it is a phi node, is already in the\n      // predicated block, is not in the loop, or may have side effects.\n      if (!I || isa<PHINode>(I) || I->getParent() == PredBB ||\n          !VectorLoop->contains(I) || I->mayHaveSideEffects())\n        continue;\n\n      // It's legal to sink the instruction if all its uses occur in the\n      // predicated block. Otherwise, there's nothing to do yet, and we may\n      // need to reanalyze the instruction.\n      if (!llvm::all_of(I->uses(), isBlockOfUsePredicated)) {\n        InstsToReanalyze.push_back(I);\n        continue;\n      }\n\n      // Move the instruction to the beginning of the predicated block, and add\n      // it's operands to the worklist.\n      I->moveBefore(&*PredBB->getFirstInsertionPt());\n      Worklist.insert(I->op_begin(), I->op_end());\n\n      // The sinking may have enabled other instructions to be sunk, so we will\n      // need to iterate.\n      Changed = true;\n    }\n  } while (Changed);\n}\n\nvoid InnerLoopVectorizer::fixNonInductionPHIs(VPTransformState &State) {\n  for (PHINode *OrigPhi : OrigPHIsToFix) {\n    VPWidenPHIRecipe *VPPhi =\n        cast<VPWidenPHIRecipe>(State.Plan->getVPValue(OrigPhi));\n    PHINode *NewPhi = cast<PHINode>(State.get(VPPhi, 0));\n    // Make sure the builder has a valid insert point.\n    Builder.SetInsertPoint(NewPhi);\n    for (unsigned i = 0; i < VPPhi->getNumOperands(); ++i) {\n      VPValue *Inc = VPPhi->getIncomingValue(i);\n      VPBasicBlock *VPBB = VPPhi->getIncomingBlock(i);\n      NewPhi->addIncoming(State.get(Inc, 0), State.CFG.VPBB2IRBB[VPBB]);\n    }\n  }\n}\n\nvoid InnerLoopVectorizer::widenGEP(GetElementPtrInst *GEP, VPValue *VPDef,\n                                   VPUser &Operands, unsigned UF,\n                                   ElementCount VF, bool IsPtrLoopInvariant,\n                                   SmallBitVector &IsIndexLoopInvariant,\n                                   VPTransformState &State) {\n  // Construct a vector GEP by widening the operands of the scalar GEP as\n  // necessary. We mark the vector GEP 'inbounds' if appropriate. A GEP\n  // results in a vector of pointers when at least one operand of the GEP\n  // is vector-typed. Thus, to keep the representation compact, we only use\n  // vector-typed operands for loop-varying values.\n\n  if (VF.isVector() && IsPtrLoopInvariant && IsIndexLoopInvariant.all()) {\n    // If we are vectorizing, but the GEP has only loop-invariant operands,\n    // the GEP we build (by only using vector-typed operands for\n    // loop-varying values) would be a scalar pointer. Thus, to ensure we\n    // produce a vector of pointers, we need to either arbitrarily pick an\n    // operand to broadcast, or broadcast a clone of the original GEP.\n    // Here, we broadcast a clone of the original.\n    //\n    // TODO: If at some point we decide to scalarize instructions having\n    //       loop-invariant operands, this special case will no longer be\n    //       required. We would add the scalarization decision to\n    //       collectLoopScalars() and teach getVectorValue() to broadcast\n    //       the lane-zero scalar value.\n    auto *Clone = Builder.Insert(GEP->clone());\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      Value *EntryPart = Builder.CreateVectorSplat(VF, Clone);\n      State.set(VPDef, EntryPart, Part);\n      addMetadata(EntryPart, GEP);\n    }\n  } else {\n    // If the GEP has at least one loop-varying operand, we are sure to\n    // produce a vector of pointers. But if we are only unrolling, we want\n    // to produce a scalar GEP for each unroll part. Thus, the GEP we\n    // produce with the code below will be scalar (if VF == 1) or vector\n    // (otherwise). Note that for the unroll-only case, we still maintain\n    // values in the vector mapping with initVector, as we do for other\n    // instructions.\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      // The pointer operand of the new GEP. If it's loop-invariant, we\n      // won't broadcast it.\n      auto *Ptr = IsPtrLoopInvariant\n                      ? State.get(Operands.getOperand(0), VPIteration(0, 0))\n                      : State.get(Operands.getOperand(0), Part);\n\n      // Collect all the indices for the new GEP. If any index is\n      // loop-invariant, we won't broadcast it.\n      SmallVector<Value *, 4> Indices;\n      for (unsigned I = 1, E = Operands.getNumOperands(); I < E; I++) {\n        VPValue *Operand = Operands.getOperand(I);\n        if (IsIndexLoopInvariant[I - 1])\n          Indices.push_back(State.get(Operand, VPIteration(0, 0)));\n        else\n          Indices.push_back(State.get(Operand, Part));\n      }\n\n      // Create the new GEP. Note that this GEP may be a scalar if VF == 1,\n      // but it should be a vector, otherwise.\n      auto *NewGEP =\n          GEP->isInBounds()\n              ? Builder.CreateInBoundsGEP(GEP->getSourceElementType(), Ptr,\n                                          Indices)\n              : Builder.CreateGEP(GEP->getSourceElementType(), Ptr, Indices);\n      assert((VF.isScalar() || NewGEP->getType()->isVectorTy()) &&\n             \"NewGEP is not a pointer vector\");\n      State.set(VPDef, NewGEP, Part);\n      addMetadata(NewGEP, GEP);\n    }\n  }\n}\n\nvoid InnerLoopVectorizer::widenPHIInstruction(Instruction *PN,\n                                              RecurrenceDescriptor *RdxDesc,\n                                              VPValue *StartVPV, VPValue *Def,\n                                              VPTransformState &State) {\n  PHINode *P = cast<PHINode>(PN);\n  if (EnableVPlanNativePath) {\n    // Currently we enter here in the VPlan-native path for non-induction\n    // PHIs where all control flow is uniform. We simply widen these PHIs.\n    // Create a vector phi with no operands - the vector phi operands will be\n    // set at the end of vector code generation.\n    Type *VecTy = (State.VF.isScalar())\n                      ? PN->getType()\n                      : VectorType::get(PN->getType(), State.VF);\n    Value *VecPhi = Builder.CreatePHI(VecTy, PN->getNumOperands(), \"vec.phi\");\n    State.set(Def, VecPhi, 0);\n    OrigPHIsToFix.push_back(P);\n\n    return;\n  }\n\n  assert(PN->getParent() == OrigLoop->getHeader() &&\n         \"Non-header phis should have been handled elsewhere\");\n\n  Value *StartV = StartVPV ? StartVPV->getLiveInIRValue() : nullptr;\n  // In order to support recurrences we need to be able to vectorize Phi nodes.\n  // Phi nodes have cycles, so we need to vectorize them in two stages. This is\n  // stage #1: We create a new vector PHI node with no incoming edges. We'll use\n  // this value when we vectorize all of the instructions that use the PHI.\n  if (RdxDesc || Legal->isFirstOrderRecurrence(P)) {\n    Value *Iden = nullptr;\n    bool ScalarPHI =\n        (State.VF.isScalar()) || Cost->isInLoopReduction(cast<PHINode>(PN));\n    Type *VecTy =\n        ScalarPHI ? PN->getType() : VectorType::get(PN->getType(), State.VF);\n\n    if (RdxDesc) {\n      assert(Legal->isReductionVariable(P) && StartV &&\n             \"RdxDesc should only be set for reduction variables; in that case \"\n             \"a StartV is also required\");\n      RecurKind RK = RdxDesc->getRecurrenceKind();\n      if (RecurrenceDescriptor::isMinMaxRecurrenceKind(RK)) {\n        // MinMax reduction have the start value as their identify.\n        if (ScalarPHI) {\n          Iden = StartV;\n        } else {\n          IRBuilderBase::InsertPointGuard IPBuilder(Builder);\n          Builder.SetInsertPoint(LoopVectorPreHeader->getTerminator());\n          StartV = Iden =\n              Builder.CreateVectorSplat(State.VF, StartV, \"minmax.ident\");\n        }\n      } else {\n        Constant *IdenC = RecurrenceDescriptor::getRecurrenceIdentity(\n            RK, VecTy->getScalarType());\n        Iden = IdenC;\n\n        if (!ScalarPHI) {\n          Iden = ConstantVector::getSplat(State.VF, IdenC);\n          IRBuilderBase::InsertPointGuard IPBuilder(Builder);\n          Builder.SetInsertPoint(LoopVectorPreHeader->getTerminator());\n          Constant *Zero = Builder.getInt32(0);\n          StartV = Builder.CreateInsertElement(Iden, StartV, Zero);\n        }\n      }\n    }\n\n    for (unsigned Part = 0; Part < State.UF; ++Part) {\n      // This is phase one of vectorizing PHIs.\n      Value *EntryPart = PHINode::Create(\n          VecTy, 2, \"vec.phi\", &*LoopVectorBody->getFirstInsertionPt());\n      State.set(Def, EntryPart, Part);\n      if (StartV) {\n        // Make sure to add the reduction start value only to the\n        // first unroll part.\n        Value *StartVal = (Part == 0) ? StartV : Iden;\n        cast<PHINode>(EntryPart)->addIncoming(StartVal, LoopVectorPreHeader);\n      }\n    }\n    return;\n  }\n\n  assert(!Legal->isReductionVariable(P) &&\n         \"reductions should be handled above\");\n\n  setDebugLocFromInst(Builder, P);\n\n  // This PHINode must be an induction variable.\n  // Make sure that we know about it.\n  assert(Legal->getInductionVars().count(P) && \"Not an induction variable\");\n\n  InductionDescriptor II = Legal->getInductionVars().lookup(P);\n  const DataLayout &DL = OrigLoop->getHeader()->getModule()->getDataLayout();\n\n  // FIXME: The newly created binary instructions should contain nsw/nuw flags,\n  // which can be found from the original scalar operations.\n  switch (II.getKind()) {\n  case InductionDescriptor::IK_NoInduction:\n    llvm_unreachable(\"Unknown induction\");\n  case InductionDescriptor::IK_IntInduction:\n  case InductionDescriptor::IK_FpInduction:\n    llvm_unreachable(\"Integer/fp induction is handled elsewhere.\");\n  case InductionDescriptor::IK_PtrInduction: {\n    // Handle the pointer induction variable case.\n    assert(P->getType()->isPointerTy() && \"Unexpected type.\");\n\n    if (Cost->isScalarAfterVectorization(P, State.VF)) {\n      // This is the normalized GEP that starts counting at zero.\n      Value *PtrInd =\n          Builder.CreateSExtOrTrunc(Induction, II.getStep()->getType());\n      // Determine the number of scalars we need to generate for each unroll\n      // iteration. If the instruction is uniform, we only need to generate the\n      // first lane. Otherwise, we generate all VF values.\n      unsigned Lanes = Cost->isUniformAfterVectorization(P, State.VF)\n                           ? 1\n                           : State.VF.getKnownMinValue();\n      for (unsigned Part = 0; Part < UF; ++Part) {\n        for (unsigned Lane = 0; Lane < Lanes; ++Lane) {\n          Constant *Idx = ConstantInt::get(\n              PtrInd->getType(), Lane + Part * State.VF.getKnownMinValue());\n          Value *GlobalIdx = Builder.CreateAdd(PtrInd, Idx);\n          Value *SclrGep =\n              emitTransformedIndex(Builder, GlobalIdx, PSE.getSE(), DL, II);\n          SclrGep->setName(\"next.gep\");\n          State.set(Def, SclrGep, VPIteration(Part, Lane));\n        }\n      }\n      return;\n    }\n    assert(isa<SCEVConstant>(II.getStep()) &&\n           \"Induction step not a SCEV constant!\");\n    Type *PhiType = II.getStep()->getType();\n\n    // Build a pointer phi\n    Value *ScalarStartValue = II.getStartValue();\n    Type *ScStValueType = ScalarStartValue->getType();\n    PHINode *NewPointerPhi =\n        PHINode::Create(ScStValueType, 2, \"pointer.phi\", Induction);\n    NewPointerPhi->addIncoming(ScalarStartValue, LoopVectorPreHeader);\n\n    // A pointer induction, performed by using a gep\n    BasicBlock *LoopLatch = LI->getLoopFor(LoopVectorBody)->getLoopLatch();\n    Instruction *InductionLoc = LoopLatch->getTerminator();\n    const SCEV *ScalarStep = II.getStep();\n    SCEVExpander Exp(*PSE.getSE(), DL, \"induction\");\n    Value *ScalarStepValue =\n        Exp.expandCodeFor(ScalarStep, PhiType, InductionLoc);\n    Value *InductionGEP = GetElementPtrInst::Create(\n        ScStValueType->getPointerElementType(), NewPointerPhi,\n        Builder.CreateMul(\n            ScalarStepValue,\n            ConstantInt::get(PhiType, State.VF.getKnownMinValue() * State.UF)),\n        \"ptr.ind\", InductionLoc);\n    NewPointerPhi->addIncoming(InductionGEP, LoopLatch);\n\n    // Create UF many actual address geps that use the pointer\n    // phi as base and a vectorized version of the step value\n    // (<step*0, ..., step*N>) as offset.\n    for (unsigned Part = 0; Part < State.UF; ++Part) {\n      SmallVector<Constant *, 8> Indices;\n      // Create a vector of consecutive numbers from zero to VF.\n      for (unsigned i = 0; i < State.VF.getKnownMinValue(); ++i)\n        Indices.push_back(\n            ConstantInt::get(PhiType, i + Part * State.VF.getKnownMinValue()));\n      Constant *StartOffset = ConstantVector::get(Indices);\n\n      Value *GEP = Builder.CreateGEP(\n          ScStValueType->getPointerElementType(), NewPointerPhi,\n          Builder.CreateMul(StartOffset,\n                            Builder.CreateVectorSplat(\n                                State.VF.getKnownMinValue(), ScalarStepValue),\n                            \"vector.gep\"));\n      State.set(Def, GEP, Part);\n    }\n  }\n  }\n}\n\n/// A helper function for checking whether an integer division-related\n/// instruction may divide by zero (in which case it must be predicated if\n/// executed conditionally in the scalar code).\n/// TODO: It may be worthwhile to generalize and check isKnownNonZero().\n/// Non-zero divisors that are non compile-time constants will not be\n/// converted into multiplication, so we will still end up scalarizing\n/// the division, but can do so w/o predication.\nstatic bool mayDivideByZero(Instruction &I) {\n  assert((I.getOpcode() == Instruction::UDiv ||\n          I.getOpcode() == Instruction::SDiv ||\n          I.getOpcode() == Instruction::URem ||\n          I.getOpcode() == Instruction::SRem) &&\n         \"Unexpected instruction\");\n  Value *Divisor = I.getOperand(1);\n  auto *CInt = dyn_cast<ConstantInt>(Divisor);\n  return !CInt || CInt->isZero();\n}\n\nvoid InnerLoopVectorizer::widenInstruction(Instruction &I, VPValue *Def,\n                                           VPUser &User,\n                                           VPTransformState &State) {\n  switch (I.getOpcode()) {\n  case Instruction::Call:\n  case Instruction::Br:\n  case Instruction::PHI:\n  case Instruction::GetElementPtr:\n  case Instruction::Select:\n    llvm_unreachable(\"This instruction is handled by a different recipe.\");\n  case Instruction::UDiv:\n  case Instruction::SDiv:\n  case Instruction::SRem:\n  case Instruction::URem:\n  case Instruction::Add:\n  case Instruction::FAdd:\n  case Instruction::Sub:\n  case Instruction::FSub:\n  case Instruction::FNeg:\n  case Instruction::Mul:\n  case Instruction::FMul:\n  case Instruction::FDiv:\n  case Instruction::FRem:\n  case Instruction::Shl:\n  case Instruction::LShr:\n  case Instruction::AShr:\n  case Instruction::And:\n  case Instruction::Or:\n  case Instruction::Xor: {\n    // Just widen unops and binops.\n    setDebugLocFromInst(Builder, &I);\n\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      SmallVector<Value *, 2> Ops;\n      for (VPValue *VPOp : User.operands())\n        Ops.push_back(State.get(VPOp, Part));\n\n      Value *V = Builder.CreateNAryOp(I.getOpcode(), Ops);\n\n      if (auto *VecOp = dyn_cast<Instruction>(V))\n        VecOp->copyIRFlags(&I);\n\n      // Use this vector value for all users of the original instruction.\n      State.set(Def, V, Part);\n      addMetadata(V, &I);\n    }\n\n    break;\n  }\n  case Instruction::ICmp:\n  case Instruction::FCmp: {\n    // Widen compares. Generate vector compares.\n    bool FCmp = (I.getOpcode() == Instruction::FCmp);\n    auto *Cmp = cast<CmpInst>(&I);\n    setDebugLocFromInst(Builder, Cmp);\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      Value *A = State.get(User.getOperand(0), Part);\n      Value *B = State.get(User.getOperand(1), Part);\n      Value *C = nullptr;\n      if (FCmp) {\n        // Propagate fast math flags.\n        IRBuilder<>::FastMathFlagGuard FMFG(Builder);\n        Builder.setFastMathFlags(Cmp->getFastMathFlags());\n        C = Builder.CreateFCmp(Cmp->getPredicate(), A, B);\n      } else {\n        C = Builder.CreateICmp(Cmp->getPredicate(), A, B);\n      }\n      State.set(Def, C, Part);\n      addMetadata(C, &I);\n    }\n\n    break;\n  }\n\n  case Instruction::ZExt:\n  case Instruction::SExt:\n  case Instruction::FPToUI:\n  case Instruction::FPToSI:\n  case Instruction::FPExt:\n  case Instruction::PtrToInt:\n  case Instruction::IntToPtr:\n  case Instruction::SIToFP:\n  case Instruction::UIToFP:\n  case Instruction::Trunc:\n  case Instruction::FPTrunc:\n  case Instruction::BitCast: {\n    auto *CI = cast<CastInst>(&I);\n    setDebugLocFromInst(Builder, CI);\n\n    /// Vectorize casts.\n    Type *DestTy =\n        (VF.isScalar()) ? CI->getType() : VectorType::get(CI->getType(), VF);\n\n    for (unsigned Part = 0; Part < UF; ++Part) {\n      Value *A = State.get(User.getOperand(0), Part);\n      Value *Cast = Builder.CreateCast(CI->getOpcode(), A, DestTy);\n      State.set(Def, Cast, Part);\n      addMetadata(Cast, &I);\n    }\n    break;\n  }\n  default:\n    // This instruction is not vectorized by simple widening.\n    LLVM_DEBUG(dbgs() << \"LV: Found an unhandled instruction: \" << I);\n    llvm_unreachable(\"Unhandled instruction!\");\n  } // end of switch.\n}\n\nvoid InnerLoopVectorizer::widenCallInstruction(CallInst &I, VPValue *Def,\n                                               VPUser &ArgOperands,\n                                               VPTransformState &State) {\n  assert(!isa<DbgInfoIntrinsic>(I) &&\n         \"DbgInfoIntrinsic should have been dropped during VPlan construction\");\n  setDebugLocFromInst(Builder, &I);\n\n  Module *M = I.getParent()->getParent()->getParent();\n  auto *CI = cast<CallInst>(&I);\n\n  SmallVector<Type *, 4> Tys;\n  for (Value *ArgOperand : CI->arg_operands())\n    Tys.push_back(ToVectorTy(ArgOperand->getType(), VF.getKnownMinValue()));\n\n  Intrinsic::ID ID = getVectorIntrinsicIDForCall(CI, TLI);\n\n  // The flag shows whether we use Intrinsic or a usual Call for vectorized\n  // version of the instruction.\n  // Is it beneficial to perform intrinsic call compared to lib call?\n  bool NeedToScalarize = false;\n  InstructionCost CallCost = Cost->getVectorCallCost(CI, VF, NeedToScalarize);\n  InstructionCost IntrinsicCost = ID ? Cost->getVectorIntrinsicCost(CI, VF) : 0;\n  bool UseVectorIntrinsic = ID && IntrinsicCost <= CallCost;\n  assert((UseVectorIntrinsic || !NeedToScalarize) &&\n         \"Instruction should be scalarized elsewhere.\");\n  assert(IntrinsicCost.isValid() && CallCost.isValid() &&\n         \"Cannot have invalid costs while widening\");\n\n  for (unsigned Part = 0; Part < UF; ++Part) {\n    SmallVector<Value *, 4> Args;\n    for (auto &I : enumerate(ArgOperands.operands())) {\n      // Some intrinsics have a scalar argument - don't replace it with a\n      // vector.\n      Value *Arg;\n      if (!UseVectorIntrinsic || !hasVectorInstrinsicScalarOpd(ID, I.index()))\n        Arg = State.get(I.value(), Part);\n      else\n        Arg = State.get(I.value(), VPIteration(0, 0));\n      Args.push_back(Arg);\n    }\n\n    Function *VectorF;\n    if (UseVectorIntrinsic) {\n      // Use vector version of the intrinsic.\n      Type *TysForDecl[] = {CI->getType()};\n      if (VF.isVector())\n        TysForDecl[0] = VectorType::get(CI->getType()->getScalarType(), VF);\n      VectorF = Intrinsic::getDeclaration(M, ID, TysForDecl);\n      assert(VectorF && \"Can't retrieve vector intrinsic.\");\n    } else {\n      // Use vector version of the function call.\n      const VFShape Shape = VFShape::get(*CI, VF, false /*HasGlobalPred*/);\n#ifndef NDEBUG\n      assert(VFDatabase(*CI).getVectorizedFunction(Shape) != nullptr &&\n             \"Can't create vector function.\");\n#endif\n        VectorF = VFDatabase(*CI).getVectorizedFunction(Shape);\n    }\n      SmallVector<OperandBundleDef, 1> OpBundles;\n      CI->getOperandBundlesAsDefs(OpBundles);\n      CallInst *V = Builder.CreateCall(VectorF, Args, OpBundles);\n\n      if (isa<FPMathOperator>(V))\n        V->copyFastMathFlags(CI);\n\n      State.set(Def, V, Part);\n      addMetadata(V, &I);\n  }\n}\n\nvoid InnerLoopVectorizer::widenSelectInstruction(SelectInst &I, VPValue *VPDef,\n                                                 VPUser &Operands,\n                                                 bool InvariantCond,\n                                                 VPTransformState &State) {\n  setDebugLocFromInst(Builder, &I);\n\n  // The condition can be loop invariant  but still defined inside the\n  // loop. This means that we can't just use the original 'cond' value.\n  // We have to take the 'vectorized' value and pick the first lane.\n  // Instcombine will make this a no-op.\n  auto *InvarCond = InvariantCond\n                        ? State.get(Operands.getOperand(0), VPIteration(0, 0))\n                        : nullptr;\n\n  for (unsigned Part = 0; Part < UF; ++Part) {\n    Value *Cond =\n        InvarCond ? InvarCond : State.get(Operands.getOperand(0), Part);\n    Value *Op0 = State.get(Operands.getOperand(1), Part);\n    Value *Op1 = State.get(Operands.getOperand(2), Part);\n    Value *Sel = Builder.CreateSelect(Cond, Op0, Op1);\n    State.set(VPDef, Sel, Part);\n    addMetadata(Sel, &I);\n  }\n}\n\nvoid LoopVectorizationCostModel::collectLoopScalars(ElementCount VF) {\n  // We should not collect Scalars more than once per VF. Right now, this\n  // function is called from collectUniformsAndScalars(), which already does\n  // this check. Collecting Scalars for VF=1 does not make any sense.\n  assert(VF.isVector() && Scalars.find(VF) == Scalars.end() &&\n         \"This function should not be visited twice for the same VF\");\n\n  SmallSetVector<Instruction *, 8> Worklist;\n\n  // These sets are used to seed the analysis with pointers used by memory\n  // accesses that will remain scalar.\n  SmallSetVector<Instruction *, 8> ScalarPtrs;\n  SmallPtrSet<Instruction *, 8> PossibleNonScalarPtrs;\n  auto *Latch = TheLoop->getLoopLatch();\n\n  // A helper that returns true if the use of Ptr by MemAccess will be scalar.\n  // The pointer operands of loads and stores will be scalar as long as the\n  // memory access is not a gather or scatter operation. The value operand of a\n  // store will remain scalar if the store is scalarized.\n  auto isScalarUse = [&](Instruction *MemAccess, Value *Ptr) {\n    InstWidening WideningDecision = getWideningDecision(MemAccess, VF);\n    assert(WideningDecision != CM_Unknown &&\n           \"Widening decision should be ready at this moment\");\n    if (auto *Store = dyn_cast<StoreInst>(MemAccess))\n      if (Ptr == Store->getValueOperand())\n        return WideningDecision == CM_Scalarize;\n    assert(Ptr == getLoadStorePointerOperand(MemAccess) &&\n           \"Ptr is neither a value or pointer operand\");\n    return WideningDecision != CM_GatherScatter;\n  };\n\n  // A helper that returns true if the given value is a bitcast or\n  // getelementptr instruction contained in the loop.\n  auto isLoopVaryingBitCastOrGEP = [&](Value *V) {\n    return ((isa<BitCastInst>(V) && V->getType()->isPointerTy()) ||\n            isa<GetElementPtrInst>(V)) &&\n           !TheLoop->isLoopInvariant(V);\n  };\n\n  auto isScalarPtrInduction = [&](Instruction *MemAccess, Value *Ptr) {\n    if (!isa<PHINode>(Ptr) ||\n        !Legal->getInductionVars().count(cast<PHINode>(Ptr)))\n      return false;\n    auto &Induction = Legal->getInductionVars()[cast<PHINode>(Ptr)];\n    if (Induction.getKind() != InductionDescriptor::IK_PtrInduction)\n      return false;\n    return isScalarUse(MemAccess, Ptr);\n  };\n\n  // A helper that evaluates a memory access's use of a pointer. If the\n  // pointer is actually the pointer induction of a loop, it is being\n  // inserted into Worklist. If the use will be a scalar use, and the\n  // pointer is only used by memory accesses, we place the pointer in\n  // ScalarPtrs. Otherwise, the pointer is placed in PossibleNonScalarPtrs.\n  auto evaluatePtrUse = [&](Instruction *MemAccess, Value *Ptr) {\n    if (isScalarPtrInduction(MemAccess, Ptr)) {\n      Worklist.insert(cast<Instruction>(Ptr));\n      Instruction *Update = cast<Instruction>(\n          cast<PHINode>(Ptr)->getIncomingValueForBlock(Latch));\n      Worklist.insert(Update);\n      LLVM_DEBUG(dbgs() << \"LV: Found new scalar instruction: \" << *Ptr\n                        << \"\\n\");\n      LLVM_DEBUG(dbgs() << \"LV: Found new scalar instruction: \" << *Update\n                        << \"\\n\");\n      return;\n    }\n    // We only care about bitcast and getelementptr instructions contained in\n    // the loop.\n    if (!isLoopVaryingBitCastOrGEP(Ptr))\n      return;\n\n    // If the pointer has already been identified as scalar (e.g., if it was\n    // also identified as uniform), there's nothing to do.\n    auto *I = cast<Instruction>(Ptr);\n    if (Worklist.count(I))\n      return;\n\n    // If the use of the pointer will be a scalar use, and all users of the\n    // pointer are memory accesses, place the pointer in ScalarPtrs. Otherwise,\n    // place the pointer in PossibleNonScalarPtrs.\n    if (isScalarUse(MemAccess, Ptr) && llvm::all_of(I->users(), [&](User *U) {\n          return isa<LoadInst>(U) || isa<StoreInst>(U);\n        }))\n      ScalarPtrs.insert(I);\n    else\n      PossibleNonScalarPtrs.insert(I);\n  };\n\n  // We seed the scalars analysis with three classes of instructions: (1)\n  // instructions marked uniform-after-vectorization and (2) bitcast,\n  // getelementptr and (pointer) phi instructions used by memory accesses\n  // requiring a scalar use.\n  //\n  // (1) Add to the worklist all instructions that have been identified as\n  // uniform-after-vectorization.\n  Worklist.insert(Uniforms[VF].begin(), Uniforms[VF].end());\n\n  // (2) Add to the worklist all bitcast and getelementptr instructions used by\n  // memory accesses requiring a scalar use. The pointer operands of loads and\n  // stores will be scalar as long as the memory accesses is not a gather or\n  // scatter operation. The value operand of a store will remain scalar if the\n  // store is scalarized.\n  for (auto *BB : TheLoop->blocks())\n    for (auto &I : *BB) {\n      if (auto *Load = dyn_cast<LoadInst>(&I)) {\n        evaluatePtrUse(Load, Load->getPointerOperand());\n      } else if (auto *Store = dyn_cast<StoreInst>(&I)) {\n        evaluatePtrUse(Store, Store->getPointerOperand());\n        evaluatePtrUse(Store, Store->getValueOperand());\n      }\n    }\n  for (auto *I : ScalarPtrs)\n    if (!PossibleNonScalarPtrs.count(I)) {\n      LLVM_DEBUG(dbgs() << \"LV: Found scalar instruction: \" << *I << \"\\n\");\n      Worklist.insert(I);\n    }\n\n  // Insert the forced scalars.\n  // FIXME: Currently widenPHIInstruction() often creates a dead vector\n  // induction variable when the PHI user is scalarized.\n  auto ForcedScalar = ForcedScalars.find(VF);\n  if (ForcedScalar != ForcedScalars.end())\n    for (auto *I : ForcedScalar->second)\n      Worklist.insert(I);\n\n  // Expand the worklist by looking through any bitcasts and getelementptr\n  // instructions we've already identified as scalar. This is similar to the\n  // expansion step in collectLoopUniforms(); however, here we're only\n  // expanding to include additional bitcasts and getelementptr instructions.\n  unsigned Idx = 0;\n  while (Idx != Worklist.size()) {\n    Instruction *Dst = Worklist[Idx++];\n    if (!isLoopVaryingBitCastOrGEP(Dst->getOperand(0)))\n      continue;\n    auto *Src = cast<Instruction>(Dst->getOperand(0));\n    if (llvm::all_of(Src->users(), [&](User *U) -> bool {\n          auto *J = cast<Instruction>(U);\n          return !TheLoop->contains(J) || Worklist.count(J) ||\n                 ((isa<LoadInst>(J) || isa<StoreInst>(J)) &&\n                  isScalarUse(J, Src));\n        })) {\n      Worklist.insert(Src);\n      LLVM_DEBUG(dbgs() << \"LV: Found scalar instruction: \" << *Src << \"\\n\");\n    }\n  }\n\n  // An induction variable will remain scalar if all users of the induction\n  // variable and induction variable update remain scalar.\n  for (auto &Induction : Legal->getInductionVars()) {\n    auto *Ind = Induction.first;\n    auto *IndUpdate = cast<Instruction>(Ind->getIncomingValueForBlock(Latch));\n\n    // If tail-folding is applied, the primary induction variable will be used\n    // to feed a vector compare.\n    if (Ind == Legal->getPrimaryInduction() && foldTailByMasking())\n      continue;\n\n    // Determine if all users of the induction variable are scalar after\n    // vectorization.\n    auto ScalarInd = llvm::all_of(Ind->users(), [&](User *U) -> bool {\n      auto *I = cast<Instruction>(U);\n      return I == IndUpdate || !TheLoop->contains(I) || Worklist.count(I);\n    });\n    if (!ScalarInd)\n      continue;\n\n    // Determine if all users of the induction variable update instruction are\n    // scalar after vectorization.\n    auto ScalarIndUpdate =\n        llvm::all_of(IndUpdate->users(), [&](User *U) -> bool {\n          auto *I = cast<Instruction>(U);\n          return I == Ind || !TheLoop->contains(I) || Worklist.count(I);\n        });\n    if (!ScalarIndUpdate)\n      continue;\n\n    // The induction variable and its update instruction will remain scalar.\n    Worklist.insert(Ind);\n    Worklist.insert(IndUpdate);\n    LLVM_DEBUG(dbgs() << \"LV: Found scalar instruction: \" << *Ind << \"\\n\");\n    LLVM_DEBUG(dbgs() << \"LV: Found scalar instruction: \" << *IndUpdate\n                      << \"\\n\");\n  }\n\n  Scalars[VF].insert(Worklist.begin(), Worklist.end());\n}\n\nbool LoopVectorizationCostModel::isScalarWithPredication(Instruction *I,\n                                                         ElementCount VF) {\n  if (!blockNeedsPredication(I->getParent()))\n    return false;\n  switch(I->getOpcode()) {\n  default:\n    break;\n  case Instruction::Load:\n  case Instruction::Store: {\n    if (!Legal->isMaskRequired(I))\n      return false;\n    auto *Ptr = getLoadStorePointerOperand(I);\n    auto *Ty = getMemInstValueType(I);\n    // We have already decided how to vectorize this instruction, get that\n    // result.\n    if (VF.isVector()) {\n      InstWidening WideningDecision = getWideningDecision(I, VF);\n      assert(WideningDecision != CM_Unknown &&\n             \"Widening decision should be ready at this moment\");\n      return WideningDecision == CM_Scalarize;\n    }\n    const Align Alignment = getLoadStoreAlignment(I);\n    return isa<LoadInst>(I) ? !(isLegalMaskedLoad(Ty, Ptr, Alignment) ||\n                                isLegalMaskedGather(Ty, Alignment))\n                            : !(isLegalMaskedStore(Ty, Ptr, Alignment) ||\n                                isLegalMaskedScatter(Ty, Alignment));\n  }\n  case Instruction::UDiv:\n  case Instruction::SDiv:\n  case Instruction::SRem:\n  case Instruction::URem:\n    return mayDivideByZero(*I);\n  }\n  return false;\n}\n\nbool LoopVectorizationCostModel::interleavedAccessCanBeWidened(\n    Instruction *I, ElementCount VF) {\n  assert(isAccessInterleaved(I) && \"Expecting interleaved access.\");\n  assert(getWideningDecision(I, VF) == CM_Unknown &&\n         \"Decision should not be set yet.\");\n  auto *Group = getInterleavedAccessGroup(I);\n  assert(Group && \"Must have a group.\");\n\n  // If the instruction's allocated size doesn't equal it's type size, it\n  // requires padding and will be scalarized.\n  auto &DL = I->getModule()->getDataLayout();\n  auto *ScalarTy = getMemInstValueType(I);\n  if (hasIrregularType(ScalarTy, DL, VF))\n    return false;\n\n  // Check if masking is required.\n  // A Group may need masking for one of two reasons: it resides in a block that\n  // needs predication, or it was decided to use masking to deal with gaps.\n  bool PredicatedAccessRequiresMasking =\n      Legal->blockNeedsPredication(I->getParent()) && Legal->isMaskRequired(I);\n  bool AccessWithGapsRequiresMasking =\n      Group->requiresScalarEpilogue() && !isScalarEpilogueAllowed();\n  if (!PredicatedAccessRequiresMasking && !AccessWithGapsRequiresMasking)\n    return true;\n\n  // If masked interleaving is required, we expect that the user/target had\n  // enabled it, because otherwise it either wouldn't have been created or\n  // it should have been invalidated by the CostModel.\n  assert(useMaskedInterleavedAccesses(TTI) &&\n         \"Masked interleave-groups for predicated accesses are not enabled.\");\n\n  auto *Ty = getMemInstValueType(I);\n  const Align Alignment = getLoadStoreAlignment(I);\n  return isa<LoadInst>(I) ? TTI.isLegalMaskedLoad(Ty, Alignment)\n                          : TTI.isLegalMaskedStore(Ty, Alignment);\n}\n\nbool LoopVectorizationCostModel::memoryInstructionCanBeWidened(\n    Instruction *I, ElementCount VF) {\n  // Get and ensure we have a valid memory instruction.\n  LoadInst *LI = dyn_cast<LoadInst>(I);\n  StoreInst *SI = dyn_cast<StoreInst>(I);\n  assert((LI || SI) && \"Invalid memory instruction\");\n\n  auto *Ptr = getLoadStorePointerOperand(I);\n\n  // In order to be widened, the pointer should be consecutive, first of all.\n  if (!Legal->isConsecutivePtr(Ptr))\n    return false;\n\n  // If the instruction is a store located in a predicated block, it will be\n  // scalarized.\n  if (isScalarWithPredication(I))\n    return false;\n\n  // If the instruction's allocated size doesn't equal it's type size, it\n  // requires padding and will be scalarized.\n  auto &DL = I->getModule()->getDataLayout();\n  auto *ScalarTy = LI ? LI->getType() : SI->getValueOperand()->getType();\n  if (hasIrregularType(ScalarTy, DL, VF))\n    return false;\n\n  return true;\n}\n\nvoid LoopVectorizationCostModel::collectLoopUniforms(ElementCount VF) {\n  // We should not collect Uniforms more than once per VF. Right now,\n  // this function is called from collectUniformsAndScalars(), which\n  // already does this check. Collecting Uniforms for VF=1 does not make any\n  // sense.\n\n  assert(VF.isVector() && Uniforms.find(VF) == Uniforms.end() &&\n         \"This function should not be visited twice for the same VF\");\n\n  // Visit the list of Uniforms. If we'll not find any uniform value, we'll\n  // not analyze again.  Uniforms.count(VF) will return 1.\n  Uniforms[VF].clear();\n\n  // We now know that the loop is vectorizable!\n  // Collect instructions inside the loop that will remain uniform after\n  // vectorization.\n\n  // Global values, params and instructions outside of current loop are out of\n  // scope.\n  auto isOutOfScope = [&](Value *V) -> bool {\n    Instruction *I = dyn_cast<Instruction>(V);\n    return (!I || !TheLoop->contains(I));\n  };\n\n  SetVector<Instruction *> Worklist;\n  BasicBlock *Latch = TheLoop->getLoopLatch();\n\n  // Instructions that are scalar with predication must not be considered\n  // uniform after vectorization, because that would create an erroneous\n  // replicating region where only a single instance out of VF should be formed.\n  // TODO: optimize such seldom cases if found important, see PR40816.\n  auto addToWorklistIfAllowed = [&](Instruction *I) -> void {\n    if (isOutOfScope(I)) {\n      LLVM_DEBUG(dbgs() << \"LV: Found not uniform due to scope: \"\n                        << *I << \"\\n\");\n      return;\n    }\n    if (isScalarWithPredication(I, VF)) {\n      LLVM_DEBUG(dbgs() << \"LV: Found not uniform being ScalarWithPredication: \"\n                        << *I << \"\\n\");\n      return;\n    }\n    LLVM_DEBUG(dbgs() << \"LV: Found uniform instruction: \" << *I << \"\\n\");\n    Worklist.insert(I);\n  };\n\n  // Start with the conditional branch. If the branch condition is an\n  // instruction contained in the loop that is only used by the branch, it is\n  // uniform.\n  auto *Cmp = dyn_cast<Instruction>(Latch->getTerminator()->getOperand(0));\n  if (Cmp && TheLoop->contains(Cmp) && Cmp->hasOneUse())\n    addToWorklistIfAllowed(Cmp);\n\n  auto isUniformDecision = [&](Instruction *I, ElementCount VF) {\n    InstWidening WideningDecision = getWideningDecision(I, VF);\n    assert(WideningDecision != CM_Unknown &&\n           \"Widening decision should be ready at this moment\");\n\n    // A uniform memory op is itself uniform.  We exclude uniform stores\n    // here as they demand the last lane, not the first one.\n    if (isa<LoadInst>(I) && Legal->isUniformMemOp(*I)) {\n      assert(WideningDecision == CM_Scalarize);\n      return true;\n    }\n\n    return (WideningDecision == CM_Widen ||\n            WideningDecision == CM_Widen_Reverse ||\n            WideningDecision == CM_Interleave);\n  };\n\n\n  // Returns true if Ptr is the pointer operand of a memory access instruction\n  // I, and I is known to not require scalarization.\n  auto isVectorizedMemAccessUse = [&](Instruction *I, Value *Ptr) -> bool {\n    return getLoadStorePointerOperand(I) == Ptr && isUniformDecision(I, VF);\n  };\n\n  // Holds a list of values which are known to have at least one uniform use.\n  // Note that there may be other uses which aren't uniform.  A \"uniform use\"\n  // here is something which only demands lane 0 of the unrolled iterations;\n  // it does not imply that all lanes produce the same value (e.g. this is not\n  // the usual meaning of uniform)\n  SmallPtrSet<Value *, 8> HasUniformUse;\n\n  // Scan the loop for instructions which are either a) known to have only\n  // lane 0 demanded or b) are uses which demand only lane 0 of their operand.\n  for (auto *BB : TheLoop->blocks())\n    for (auto &I : *BB) {\n      // If there's no pointer operand, there's nothing to do.\n      auto *Ptr = getLoadStorePointerOperand(&I);\n      if (!Ptr)\n        continue;\n\n      // A uniform memory op is itself uniform.  We exclude uniform stores\n      // here as they demand the last lane, not the first one.\n      if (isa<LoadInst>(I) && Legal->isUniformMemOp(I))\n        addToWorklistIfAllowed(&I);\n\n      if (isUniformDecision(&I, VF)) {\n        assert(isVectorizedMemAccessUse(&I, Ptr) && \"consistency check\");\n        HasUniformUse.insert(Ptr);\n      }\n    }\n\n  // Add to the worklist any operands which have *only* uniform (e.g. lane 0\n  // demanding) users.  Since loops are assumed to be in LCSSA form, this\n  // disallows uses outside the loop as well.\n  for (auto *V : HasUniformUse) {\n    if (isOutOfScope(V))\n      continue;\n    auto *I = cast<Instruction>(V);\n    auto UsersAreMemAccesses =\n      llvm::all_of(I->users(), [&](User *U) -> bool {\n        return isVectorizedMemAccessUse(cast<Instruction>(U), V);\n      });\n    if (UsersAreMemAccesses)\n      addToWorklistIfAllowed(I);\n  }\n\n  // Expand Worklist in topological order: whenever a new instruction\n  // is added , its users should be already inside Worklist.  It ensures\n  // a uniform instruction will only be used by uniform instructions.\n  unsigned idx = 0;\n  while (idx != Worklist.size()) {\n    Instruction *I = Worklist[idx++];\n\n    for (auto OV : I->operand_values()) {\n      // isOutOfScope operands cannot be uniform instructions.\n      if (isOutOfScope(OV))\n        continue;\n      // First order recurrence Phi's should typically be considered\n      // non-uniform.\n      auto *OP = dyn_cast<PHINode>(OV);\n      if (OP && Legal->isFirstOrderRecurrence(OP))\n        continue;\n      // If all the users of the operand are uniform, then add the\n      // operand into the uniform worklist.\n      auto *OI = cast<Instruction>(OV);\n      if (llvm::all_of(OI->users(), [&](User *U) -> bool {\n            auto *J = cast<Instruction>(U);\n            return Worklist.count(J) || isVectorizedMemAccessUse(J, OI);\n          }))\n        addToWorklistIfAllowed(OI);\n    }\n  }\n\n  // For an instruction to be added into Worklist above, all its users inside\n  // the loop should also be in Worklist. However, this condition cannot be\n  // true for phi nodes that form a cyclic dependence. We must process phi\n  // nodes separately. An induction variable will remain uniform if all users\n  // of the induction variable and induction variable update remain uniform.\n  // The code below handles both pointer and non-pointer induction variables.\n  for (auto &Induction : Legal->getInductionVars()) {\n    auto *Ind = Induction.first;\n    auto *IndUpdate = cast<Instruction>(Ind->getIncomingValueForBlock(Latch));\n\n    // Determine if all users of the induction variable are uniform after\n    // vectorization.\n    auto UniformInd = llvm::all_of(Ind->users(), [&](User *U) -> bool {\n      auto *I = cast<Instruction>(U);\n      return I == IndUpdate || !TheLoop->contains(I) || Worklist.count(I) ||\n             isVectorizedMemAccessUse(I, Ind);\n    });\n    if (!UniformInd)\n      continue;\n\n    // Determine if all users of the induction variable update instruction are\n    // uniform after vectorization.\n    auto UniformIndUpdate =\n        llvm::all_of(IndUpdate->users(), [&](User *U) -> bool {\n          auto *I = cast<Instruction>(U);\n          return I == Ind || !TheLoop->contains(I) || Worklist.count(I) ||\n                 isVectorizedMemAccessUse(I, IndUpdate);\n        });\n    if (!UniformIndUpdate)\n      continue;\n\n    // The induction variable and its update instruction will remain uniform.\n    addToWorklistIfAllowed(Ind);\n    addToWorklistIfAllowed(IndUpdate);\n  }\n\n  Uniforms[VF].insert(Worklist.begin(), Worklist.end());\n}\n\nbool LoopVectorizationCostModel::runtimeChecksRequired() {\n  LLVM_DEBUG(dbgs() << \"LV: Performing code size checks.\\n\");\n\n  if (Legal->getRuntimePointerChecking()->Need) {\n    reportVectorizationFailure(\"Runtime ptr check is required with -Os/-Oz\",\n        \"runtime pointer checks needed. Enable vectorization of this \"\n        \"loop with '#pragma clang loop vectorize(enable)' when \"\n        \"compiling with -Os/-Oz\",\n        \"CantVersionLoopWithOptForSize\", ORE, TheLoop);\n    return true;\n  }\n\n  if (!PSE.getUnionPredicate().getPredicates().empty()) {\n    reportVectorizationFailure(\"Runtime SCEV check is required with -Os/-Oz\",\n        \"runtime SCEV checks needed. Enable vectorization of this \"\n        \"loop with '#pragma clang loop vectorize(enable)' when \"\n        \"compiling with -Os/-Oz\",\n        \"CantVersionLoopWithOptForSize\", ORE, TheLoop);\n    return true;\n  }\n\n  // FIXME: Avoid specializing for stride==1 instead of bailing out.\n  if (!Legal->getLAI()->getSymbolicStrides().empty()) {\n    reportVectorizationFailure(\"Runtime stride check for small trip count\",\n        \"runtime stride == 1 checks needed. Enable vectorization of \"\n        \"this loop without such check by compiling with -Os/-Oz\",\n        \"CantVersionLoopWithOptForSize\", ORE, TheLoop);\n    return true;\n  }\n\n  return false;\n}\n\nOptional<ElementCount>\nLoopVectorizationCostModel::computeMaxVF(ElementCount UserVF, unsigned UserIC) {\n  if (Legal->getRuntimePointerChecking()->Need && TTI.hasBranchDivergence()) {\n    // TODO: It may by useful to do since it's still likely to be dynamically\n    // uniform if the target can skip.\n    reportVectorizationFailure(\n        \"Not inserting runtime ptr check for divergent target\",\n        \"runtime pointer checks needed. Not enabled for divergent target\",\n        \"CantVersionLoopWithDivergentTarget\", ORE, TheLoop);\n    return None;\n  }\n\n  unsigned TC = PSE.getSE()->getSmallConstantTripCount(TheLoop);\n  LLVM_DEBUG(dbgs() << \"LV: Found trip count: \" << TC << '\\n');\n  if (TC == 1) {\n    reportVectorizationFailure(\"Single iteration (non) loop\",\n        \"loop trip count is one, irrelevant for vectorization\",\n        \"SingleIterationLoop\", ORE, TheLoop);\n    return None;\n  }\n\n  switch (ScalarEpilogueStatus) {\n  case CM_ScalarEpilogueAllowed:\n    return computeFeasibleMaxVF(TC, UserVF);\n  case CM_ScalarEpilogueNotAllowedUsePredicate:\n    LLVM_FALLTHROUGH;\n  case CM_ScalarEpilogueNotNeededUsePredicate:\n    LLVM_DEBUG(\n        dbgs() << \"LV: vector predicate hint/switch found.\\n\"\n               << \"LV: Not allowing scalar epilogue, creating predicated \"\n               << \"vector loop.\\n\");\n    break;\n  case CM_ScalarEpilogueNotAllowedLowTripLoop:\n    // fallthrough as a special case of OptForSize\n  case CM_ScalarEpilogueNotAllowedOptSize:\n    if (ScalarEpilogueStatus == CM_ScalarEpilogueNotAllowedOptSize)\n      LLVM_DEBUG(\n          dbgs() << \"LV: Not allowing scalar epilogue due to -Os/-Oz.\\n\");\n    else\n      LLVM_DEBUG(dbgs() << \"LV: Not allowing scalar epilogue due to low trip \"\n                        << \"count.\\n\");\n\n    // Bail if runtime checks are required, which are not good when optimising\n    // for size.\n    if (runtimeChecksRequired())\n      return None;\n\n    break;\n  }\n\n  // The only loops we can vectorize without a scalar epilogue, are loops with\n  // a bottom-test and a single exiting block. We'd have to handle the fact\n  // that not every instruction executes on the last iteration.  This will\n  // require a lane mask which varies through the vector loop body.  (TODO)\n  if (TheLoop->getExitingBlock() != TheLoop->getLoopLatch()) {\n    // If there was a tail-folding hint/switch, but we can't fold the tail by\n    // masking, fallback to a vectorization with a scalar epilogue.\n    if (ScalarEpilogueStatus == CM_ScalarEpilogueNotNeededUsePredicate) {\n      LLVM_DEBUG(dbgs() << \"LV: Cannot fold tail by masking: vectorize with a \"\n                           \"scalar epilogue instead.\\n\");\n      ScalarEpilogueStatus = CM_ScalarEpilogueAllowed;\n      return computeFeasibleMaxVF(TC, UserVF);\n    }\n    return None;\n  }\n\n  // Now try the tail folding\n\n  // Invalidate interleave groups that require an epilogue if we can't mask\n  // the interleave-group.\n  if (!useMaskedInterleavedAccesses(TTI)) {\n    assert(WideningDecisions.empty() && Uniforms.empty() && Scalars.empty() &&\n           \"No decisions should have been taken at this point\");\n    // Note: There is no need to invalidate any cost modeling decisions here, as\n    // non where taken so far.\n    InterleaveInfo.invalidateGroupsRequiringScalarEpilogue();\n  }\n\n  ElementCount MaxVF = computeFeasibleMaxVF(TC, UserVF);\n  assert(!MaxVF.isScalable() &&\n         \"Scalable vectors do not yet support tail folding\");\n  assert((UserVF.isNonZero() || isPowerOf2_32(MaxVF.getFixedValue())) &&\n         \"MaxVF must be a power of 2\");\n  unsigned MaxVFtimesIC =\n      UserIC ? MaxVF.getFixedValue() * UserIC : MaxVF.getFixedValue();\n  // Avoid tail folding if the trip count is known to be a multiple of any VF we\n  // chose.\n  ScalarEvolution *SE = PSE.getSE();\n  const SCEV *BackedgeTakenCount = PSE.getBackedgeTakenCount();\n  const SCEV *ExitCount = SE->getAddExpr(\n      BackedgeTakenCount, SE->getOne(BackedgeTakenCount->getType()));\n  const SCEV *Rem = SE->getURemExpr(\n      SE->applyLoopGuards(ExitCount, TheLoop),\n      SE->getConstant(BackedgeTakenCount->getType(), MaxVFtimesIC));\n  if (Rem->isZero()) {\n    // Accept MaxVF if we do not have a tail.\n    LLVM_DEBUG(dbgs() << \"LV: No tail will remain for any chosen VF.\\n\");\n    return MaxVF;\n  }\n\n  // If we don't know the precise trip count, or if the trip count that we\n  // found modulo the vectorization factor is not zero, try to fold the tail\n  // by masking.\n  // FIXME: look for a smaller MaxVF that does divide TC rather than masking.\n  if (Legal->prepareToFoldTailByMasking()) {\n    FoldTailByMasking = true;\n    return MaxVF;\n  }\n\n  // If there was a tail-folding hint/switch, but we can't fold the tail by\n  // masking, fallback to a vectorization with a scalar epilogue.\n  if (ScalarEpilogueStatus == CM_ScalarEpilogueNotNeededUsePredicate) {\n    LLVM_DEBUG(dbgs() << \"LV: Cannot fold tail by masking: vectorize with a \"\n                         \"scalar epilogue instead.\\n\");\n    ScalarEpilogueStatus = CM_ScalarEpilogueAllowed;\n    return MaxVF;\n  }\n\n  if (ScalarEpilogueStatus == CM_ScalarEpilogueNotAllowedUsePredicate) {\n    LLVM_DEBUG(dbgs() << \"LV: Can't fold tail by masking: don't vectorize\\n\");\n    return None;\n  }\n\n  if (TC == 0) {\n    reportVectorizationFailure(\n        \"Unable to calculate the loop count due to complex control flow\",\n        \"unable to calculate the loop count due to complex control flow\",\n        \"UnknownLoopCountComplexCFG\", ORE, TheLoop);\n    return None;\n  }\n\n  reportVectorizationFailure(\n      \"Cannot optimize for size and vectorize at the same time.\",\n      \"cannot optimize for size and vectorize at the same time. \"\n      \"Enable vectorization of this loop with '#pragma clang loop \"\n      \"vectorize(enable)' when compiling with -Os/-Oz\",\n      \"NoTailLoopWithOptForSize\", ORE, TheLoop);\n  return None;\n}\n\nElementCount\nLoopVectorizationCostModel::computeFeasibleMaxVF(unsigned ConstTripCount,\n                                                 ElementCount UserVF) {\n  bool IgnoreScalableUserVF = UserVF.isScalable() &&\n                              !TTI.supportsScalableVectors() &&\n                              !ForceTargetSupportsScalableVectors;\n  if (IgnoreScalableUserVF) {\n    LLVM_DEBUG(\n        dbgs() << \"LV: Ignoring VF=\" << UserVF\n               << \" because target does not support scalable vectors.\\n\");\n    ORE->emit([&]() {\n      return OptimizationRemarkAnalysis(DEBUG_TYPE, \"IgnoreScalableUserVF\",\n                                        TheLoop->getStartLoc(),\n                                        TheLoop->getHeader())\n             << \"Ignoring VF=\" << ore::NV(\"UserVF\", UserVF)\n             << \" because target does not support scalable vectors.\";\n    });\n  }\n\n  // Beyond this point two scenarios are handled. If UserVF isn't specified\n  // then a suitable VF is chosen. If UserVF is specified and there are\n  // dependencies, check if it's legal. However, if a UserVF is specified and\n  // there are no dependencies, then there's nothing to do.\n  if (UserVF.isNonZero() && !IgnoreScalableUserVF) {\n    if (!canVectorizeReductions(UserVF)) {\n      reportVectorizationFailure(\n          \"LV: Scalable vectorization not supported for the reduction \"\n          \"operations found in this loop. Using fixed-width \"\n          \"vectorization instead.\",\n          \"Scalable vectorization not supported for the reduction operations \"\n          \"found in this loop. Using fixed-width vectorization instead.\",\n          \"ScalableVFUnfeasible\", ORE, TheLoop);\n      return computeFeasibleMaxVF(\n          ConstTripCount, ElementCount::getFixed(UserVF.getKnownMinValue()));\n    }\n\n    if (Legal->isSafeForAnyVectorWidth())\n      return UserVF;\n  }\n\n  MinBWs = computeMinimumValueSizes(TheLoop->getBlocks(), *DB, &TTI);\n  unsigned SmallestType, WidestType;\n  std::tie(SmallestType, WidestType) = getSmallestAndWidestTypes();\n  unsigned WidestRegister = TTI.getRegisterBitWidth(true);\n\n  // Get the maximum safe dependence distance in bits computed by LAA.\n  // It is computed by MaxVF * sizeOf(type) * 8, where type is taken from\n  // the memory accesses that is most restrictive (involved in the smallest\n  // dependence distance).\n  unsigned MaxSafeVectorWidthInBits = Legal->getMaxSafeVectorWidthInBits();\n\n  // If the user vectorization factor is legally unsafe, clamp it to a safe\n  // value. Otherwise, return as is.\n  if (UserVF.isNonZero() && !IgnoreScalableUserVF) {\n    unsigned MaxSafeElements =\n        PowerOf2Floor(MaxSafeVectorWidthInBits / WidestType);\n    ElementCount MaxSafeVF = ElementCount::getFixed(MaxSafeElements);\n\n    if (UserVF.isScalable()) {\n      Optional<unsigned> MaxVScale = TTI.getMaxVScale();\n\n      // Scale VF by vscale before checking if it's safe.\n      MaxSafeVF = ElementCount::getScalable(\n          MaxVScale ? (MaxSafeElements / MaxVScale.getValue()) : 0);\n\n      if (MaxSafeVF.isZero()) {\n        // The dependence distance is too small to use scalable vectors,\n        // fallback on fixed.\n        LLVM_DEBUG(\n            dbgs()\n            << \"LV: Max legal vector width too small, scalable vectorization \"\n               \"unfeasible. Using fixed-width vectorization instead.\\n\");\n        ORE->emit([&]() {\n          return OptimizationRemarkAnalysis(DEBUG_TYPE, \"ScalableVFUnfeasible\",\n                                            TheLoop->getStartLoc(),\n                                            TheLoop->getHeader())\n                 << \"Max legal vector width too small, scalable vectorization \"\n                 << \"unfeasible. Using fixed-width vectorization instead.\";\n        });\n        return computeFeasibleMaxVF(\n            ConstTripCount, ElementCount::getFixed(UserVF.getKnownMinValue()));\n      }\n    }\n\n    LLVM_DEBUG(dbgs() << \"LV: The max safe VF is: \" << MaxSafeVF << \".\\n\");\n\n    if (ElementCount::isKnownLE(UserVF, MaxSafeVF))\n      return UserVF;\n\n    LLVM_DEBUG(dbgs() << \"LV: User VF=\" << UserVF\n                      << \" is unsafe, clamping to max safe VF=\" << MaxSafeVF\n                      << \".\\n\");\n    ORE->emit([&]() {\n      return OptimizationRemarkAnalysis(DEBUG_TYPE, \"VectorizationFactor\",\n                                        TheLoop->getStartLoc(),\n                                        TheLoop->getHeader())\n             << \"User-specified vectorization factor \"\n             << ore::NV(\"UserVectorizationFactor\", UserVF)\n             << \" is unsafe, clamping to maximum safe vectorization factor \"\n             << ore::NV(\"VectorizationFactor\", MaxSafeVF);\n    });\n    return MaxSafeVF;\n  }\n\n  WidestRegister = std::min(WidestRegister, MaxSafeVectorWidthInBits);\n\n  // Ensure MaxVF is a power of 2; the dependence distance bound may not be.\n  // Note that both WidestRegister and WidestType may not be a powers of 2.\n  auto MaxVectorSize =\n      ElementCount::getFixed(PowerOf2Floor(WidestRegister / WidestType));\n\n  LLVM_DEBUG(dbgs() << \"LV: The Smallest and Widest types: \" << SmallestType\n                    << \" / \" << WidestType << \" bits.\\n\");\n  LLVM_DEBUG(dbgs() << \"LV: The Widest register safe to use is: \"\n                    << WidestRegister << \" bits.\\n\");\n\n  assert(MaxVectorSize.getFixedValue() <= WidestRegister &&\n         \"Did not expect to pack so many elements\"\n         \" into one vector!\");\n  if (MaxVectorSize.getFixedValue() == 0) {\n    LLVM_DEBUG(dbgs() << \"LV: The target has no vector registers.\\n\");\n    return ElementCount::getFixed(1);\n  } else if (ConstTripCount && ConstTripCount < MaxVectorSize.getFixedValue() &&\n             isPowerOf2_32(ConstTripCount)) {\n    // We need to clamp the VF to be the ConstTripCount. There is no point in\n    // choosing a higher viable VF as done in the loop below.\n    LLVM_DEBUG(dbgs() << \"LV: Clamping the MaxVF to the constant trip count: \"\n                      << ConstTripCount << \"\\n\");\n    return ElementCount::getFixed(ConstTripCount);\n  }\n\n  ElementCount MaxVF = MaxVectorSize;\n  if (TTI.shouldMaximizeVectorBandwidth(!isScalarEpilogueAllowed()) ||\n      (MaximizeBandwidth && isScalarEpilogueAllowed())) {\n    // Collect all viable vectorization factors larger than the default MaxVF\n    // (i.e. MaxVectorSize).\n    SmallVector<ElementCount, 8> VFs;\n    auto MaxVectorSizeMaxBW =\n        ElementCount::getFixed(WidestRegister / SmallestType);\n    for (ElementCount VS = MaxVectorSize * 2;\n         ElementCount::isKnownLE(VS, MaxVectorSizeMaxBW); VS *= 2)\n      VFs.push_back(VS);\n\n    // For each VF calculate its register usage.\n    auto RUs = calculateRegisterUsage(VFs);\n\n    // Select the largest VF which doesn't require more registers than existing\n    // ones.\n    for (int i = RUs.size() - 1; i >= 0; --i) {\n      bool Selected = true;\n      for (auto &pair : RUs[i].MaxLocalUsers) {\n        unsigned TargetNumRegisters = TTI.getNumberOfRegisters(pair.first);\n        if (pair.second > TargetNumRegisters)\n          Selected = false;\n      }\n      if (Selected) {\n        MaxVF = VFs[i];\n        break;\n      }\n    }\n    if (ElementCount MinVF =\n            TTI.getMinimumVF(SmallestType, /*IsScalable=*/false)) {\n      if (ElementCount::isKnownLT(MaxVF, MinVF)) {\n        LLVM_DEBUG(dbgs() << \"LV: Overriding calculated MaxVF(\" << MaxVF\n                          << \") with target's minimum: \" << MinVF << '\\n');\n        MaxVF = MinVF;\n      }\n    }\n  }\n  return MaxVF;\n}\n\nVectorizationFactor\nLoopVectorizationCostModel::selectVectorizationFactor(ElementCount MaxVF) {\n  // FIXME: This can be fixed for scalable vectors later, because at this stage\n  // the LoopVectorizer will only consider vectorizing a loop with scalable\n  // vectors when the loop has a hint to enable vectorization for a given VF.\n  assert(!MaxVF.isScalable() && \"scalable vectors not yet supported\");\n\n  InstructionCost ExpectedCost = expectedCost(ElementCount::getFixed(1)).first;\n  LLVM_DEBUG(dbgs() << \"LV: Scalar loop costs: \" << ExpectedCost << \".\\n\");\n  assert(ExpectedCost.isValid() && \"Unexpected invalid cost for scalar loop\");\n\n  auto Width = ElementCount::getFixed(1);\n  const float ScalarCost = *ExpectedCost.getValue();\n  float Cost = ScalarCost;\n\n  bool ForceVectorization = Hints->getForce() == LoopVectorizeHints::FK_Enabled;\n  if (ForceVectorization && MaxVF.isVector()) {\n    // Ignore scalar width, because the user explicitly wants vectorization.\n    // Initialize cost to max so that VF = 2 is, at least, chosen during cost\n    // evaluation.\n    Cost = std::numeric_limits<float>::max();\n  }\n\n  for (auto i = ElementCount::getFixed(2); ElementCount::isKnownLE(i, MaxVF);\n       i *= 2) {\n    // Notice that the vector loop needs to be executed less times, so\n    // we need to divide the cost of the vector loops by the width of\n    // the vector elements.\n    VectorizationCostTy C = expectedCost(i);\n    assert(C.first.isValid() && \"Unexpected invalid cost for vector loop\");\n    float VectorCost = *C.first.getValue() / (float)i.getFixedValue();\n    LLVM_DEBUG(dbgs() << \"LV: Vector loop of width \" << i\n                      << \" costs: \" << (int)VectorCost << \".\\n\");\n    if (!C.second && !ForceVectorization) {\n      LLVM_DEBUG(\n          dbgs() << \"LV: Not considering vector loop of width \" << i\n                 << \" because it will not generate any vector instructions.\\n\");\n      continue;\n    }\n\n    // If profitable add it to ProfitableVF list.\n    if (VectorCost < ScalarCost) {\n      ProfitableVFs.push_back(VectorizationFactor(\n          {i, (unsigned)VectorCost}));\n    }\n\n    if (VectorCost < Cost) {\n      Cost = VectorCost;\n      Width = i;\n    }\n  }\n\n  if (!EnableCondStoresVectorization && NumPredStores) {\n    reportVectorizationFailure(\"There are conditional stores.\",\n        \"store that is conditionally executed prevents vectorization\",\n        \"ConditionalStore\", ORE, TheLoop);\n    Width = ElementCount::getFixed(1);\n    Cost = ScalarCost;\n  }\n\n  LLVM_DEBUG(if (ForceVectorization && !Width.isScalar() && Cost >= ScalarCost) dbgs()\n             << \"LV: Vectorization seems to be not beneficial, \"\n             << \"but was forced by a user.\\n\");\n  LLVM_DEBUG(dbgs() << \"LV: Selecting VF: \" << Width << \".\\n\");\n  VectorizationFactor Factor = {Width,\n                                (unsigned)(Width.getKnownMinValue() * Cost)};\n  return Factor;\n}\n\nbool LoopVectorizationCostModel::isCandidateForEpilogueVectorization(\n    const Loop &L, ElementCount VF) const {\n  // Cross iteration phis such as reductions need special handling and are\n  // currently unsupported.\n  if (any_of(L.getHeader()->phis(), [&](PHINode &Phi) {\n        return Legal->isFirstOrderRecurrence(&Phi) ||\n               Legal->isReductionVariable(&Phi);\n      }))\n    return false;\n\n  // Phis with uses outside of the loop require special handling and are\n  // currently unsupported.\n  for (auto &Entry : Legal->getInductionVars()) {\n    // Look for uses of the value of the induction at the last iteration.\n    Value *PostInc = Entry.first->getIncomingValueForBlock(L.getLoopLatch());\n    for (User *U : PostInc->users())\n      if (!L.contains(cast<Instruction>(U)))\n        return false;\n    // Look for uses of penultimate value of the induction.\n    for (User *U : Entry.first->users())\n      if (!L.contains(cast<Instruction>(U)))\n        return false;\n  }\n\n  // Induction variables that are widened require special handling that is\n  // currently not supported.\n  if (any_of(Legal->getInductionVars(), [&](auto &Entry) {\n        return !(this->isScalarAfterVectorization(Entry.first, VF) ||\n                 this->isProfitableToScalarize(Entry.first, VF));\n      }))\n    return false;\n\n  return true;\n}\n\nbool LoopVectorizationCostModel::isEpilogueVectorizationProfitable(\n    const ElementCount VF) const {\n  // FIXME: We need a much better cost-model to take different parameters such\n  // as register pressure, code size increase and cost of extra branches into\n  // account. For now we apply a very crude heuristic and only consider loops\n  // with vectorization factors larger than a certain value.\n  // We also consider epilogue vectorization unprofitable for targets that don't\n  // consider interleaving beneficial (eg. MVE).\n  if (TTI.getMaxInterleaveFactor(VF.getKnownMinValue()) <= 1)\n    return false;\n  if (VF.getFixedValue() >= EpilogueVectorizationMinVF)\n    return true;\n  return false;\n}\n\nVectorizationFactor\nLoopVectorizationCostModel::selectEpilogueVectorizationFactor(\n    const ElementCount MainLoopVF, const LoopVectorizationPlanner &LVP) {\n  VectorizationFactor Result = VectorizationFactor::Disabled();\n  if (!EnableEpilogueVectorization) {\n    LLVM_DEBUG(dbgs() << \"LEV: Epilogue vectorization is disabled.\\n\";);\n    return Result;\n  }\n\n  if (!isScalarEpilogueAllowed()) {\n    LLVM_DEBUG(\n        dbgs() << \"LEV: Unable to vectorize epilogue because no epilogue is \"\n                  \"allowed.\\n\";);\n    return Result;\n  }\n\n  // FIXME: This can be fixed for scalable vectors later, because at this stage\n  // the LoopVectorizer will only consider vectorizing a loop with scalable\n  // vectors when the loop has a hint to enable vectorization for a given VF.\n  if (MainLoopVF.isScalable()) {\n    LLVM_DEBUG(dbgs() << \"LEV: Epilogue vectorization for scalable vectors not \"\n                         \"yet supported.\\n\");\n    return Result;\n  }\n\n  // Not really a cost consideration, but check for unsupported cases here to\n  // simplify the logic.\n  if (!isCandidateForEpilogueVectorization(*TheLoop, MainLoopVF)) {\n    LLVM_DEBUG(\n        dbgs() << \"LEV: Unable to vectorize epilogue because the loop is \"\n                  \"not a supported candidate.\\n\";);\n    return Result;\n  }\n\n  if (EpilogueVectorizationForceVF > 1) {\n    LLVM_DEBUG(dbgs() << \"LEV: Epilogue vectorization factor is forced.\\n\";);\n    if (LVP.hasPlanWithVFs(\n            {MainLoopVF, ElementCount::getFixed(EpilogueVectorizationForceVF)}))\n      return {ElementCount::getFixed(EpilogueVectorizationForceVF), 0};\n    else {\n      LLVM_DEBUG(\n          dbgs()\n              << \"LEV: Epilogue vectorization forced factor is not viable.\\n\";);\n      return Result;\n    }\n  }\n\n  if (TheLoop->getHeader()->getParent()->hasOptSize() ||\n      TheLoop->getHeader()->getParent()->hasMinSize()) {\n    LLVM_DEBUG(\n        dbgs()\n            << \"LEV: Epilogue vectorization skipped due to opt for size.\\n\";);\n    return Result;\n  }\n\n  if (!isEpilogueVectorizationProfitable(MainLoopVF))\n    return Result;\n\n  for (auto &NextVF : ProfitableVFs)\n    if (ElementCount::isKnownLT(NextVF.Width, MainLoopVF) &&\n        (Result.Width.getFixedValue() == 1 || NextVF.Cost < Result.Cost) &&\n        LVP.hasPlanWithVFs({MainLoopVF, NextVF.Width}))\n      Result = NextVF;\n\n  if (Result != VectorizationFactor::Disabled())\n    LLVM_DEBUG(dbgs() << \"LEV: Vectorizing epilogue loop with VF = \"\n                      << Result.Width.getFixedValue() << \"\\n\";);\n  return Result;\n}\n\nstd::pair<unsigned, unsigned>\nLoopVectorizationCostModel::getSmallestAndWidestTypes() {\n  unsigned MinWidth = -1U;\n  unsigned MaxWidth = 8;\n  const DataLayout &DL = TheFunction->getParent()->getDataLayout();\n\n  // For each block.\n  for (BasicBlock *BB : TheLoop->blocks()) {\n    // For each instruction in the loop.\n    for (Instruction &I : BB->instructionsWithoutDebug()) {\n      Type *T = I.getType();\n\n      // Skip ignored values.\n      if (ValuesToIgnore.count(&I))\n        continue;\n\n      // Only examine Loads, Stores and PHINodes.\n      if (!isa<LoadInst>(I) && !isa<StoreInst>(I) && !isa<PHINode>(I))\n        continue;\n\n      // Examine PHI nodes that are reduction variables. Update the type to\n      // account for the recurrence type.\n      if (auto *PN = dyn_cast<PHINode>(&I)) {\n        if (!Legal->isReductionVariable(PN))\n          continue;\n        RecurrenceDescriptor RdxDesc = Legal->getReductionVars()[PN];\n        if (PreferInLoopReductions ||\n            TTI.preferInLoopReduction(RdxDesc.getOpcode(),\n                                      RdxDesc.getRecurrenceType(),\n                                      TargetTransformInfo::ReductionFlags()))\n          continue;\n        T = RdxDesc.getRecurrenceType();\n      }\n\n      // Examine the stored values.\n      if (auto *ST = dyn_cast<StoreInst>(&I))\n        T = ST->getValueOperand()->getType();\n\n      // Ignore loaded pointer types and stored pointer types that are not\n      // vectorizable.\n      //\n      // FIXME: The check here attempts to predict whether a load or store will\n      //        be vectorized. We only know this for certain after a VF has\n      //        been selected. Here, we assume that if an access can be\n      //        vectorized, it will be. We should also look at extending this\n      //        optimization to non-pointer types.\n      //\n      if (T->isPointerTy() && !isConsecutiveLoadOrStore(&I) &&\n          !isAccessInterleaved(&I) && !isLegalGatherOrScatter(&I))\n        continue;\n\n      MinWidth = std::min(MinWidth,\n                          (unsigned)DL.getTypeSizeInBits(T->getScalarType()));\n      MaxWidth = std::max(MaxWidth,\n                          (unsigned)DL.getTypeSizeInBits(T->getScalarType()));\n    }\n  }\n\n  return {MinWidth, MaxWidth};\n}\n\nunsigned LoopVectorizationCostModel::selectInterleaveCount(ElementCount VF,\n                                                           unsigned LoopCost) {\n  // -- The interleave heuristics --\n  // We interleave the loop in order to expose ILP and reduce the loop overhead.\n  // There are many micro-architectural considerations that we can't predict\n  // at this level. For example, frontend pressure (on decode or fetch) due to\n  // code size, or the number and capabilities of the execution ports.\n  //\n  // We use the following heuristics to select the interleave count:\n  // 1. If the code has reductions, then we interleave to break the cross\n  // iteration dependency.\n  // 2. If the loop is really small, then we interleave to reduce the loop\n  // overhead.\n  // 3. We don't interleave if we think that we will spill registers to memory\n  // due to the increased register pressure.\n\n  if (!isScalarEpilogueAllowed())\n    return 1;\n\n  // We used the distance for the interleave count.\n  if (Legal->getMaxSafeDepDistBytes() != -1U)\n    return 1;\n\n  auto BestKnownTC = getSmallBestKnownTC(*PSE.getSE(), TheLoop);\n  const bool HasReductions = !Legal->getReductionVars().empty();\n  // Do not interleave loops with a relatively small known or estimated trip\n  // count. But we will interleave when InterleaveSmallLoopScalarReduction is\n  // enabled, and the code has scalar reductions(HasReductions && VF = 1),\n  // because with the above conditions interleaving can expose ILP and break\n  // cross iteration dependences for reductions.\n  if (BestKnownTC && (*BestKnownTC < TinyTripCountInterleaveThreshold) &&\n      !(InterleaveSmallLoopScalarReduction && HasReductions && VF.isScalar()))\n    return 1;\n\n  RegisterUsage R = calculateRegisterUsage({VF})[0];\n  // We divide by these constants so assume that we have at least one\n  // instruction that uses at least one register.\n  for (auto& pair : R.MaxLocalUsers) {\n    pair.second = std::max(pair.second, 1U);\n  }\n\n  // We calculate the interleave count using the following formula.\n  // Subtract the number of loop invariants from the number of available\n  // registers. These registers are used by all of the interleaved instances.\n  // Next, divide the remaining registers by the number of registers that is\n  // required by the loop, in order to estimate how many parallel instances\n  // fit without causing spills. All of this is rounded down if necessary to be\n  // a power of two. We want power of two interleave count to simplify any\n  // addressing operations or alignment considerations.\n  // We also want power of two interleave counts to ensure that the induction\n  // variable of the vector loop wraps to zero, when tail is folded by masking;\n  // this currently happens when OptForSize, in which case IC is set to 1 above.\n  unsigned IC = UINT_MAX;\n\n  for (auto& pair : R.MaxLocalUsers) {\n    unsigned TargetNumRegisters = TTI.getNumberOfRegisters(pair.first);\n    LLVM_DEBUG(dbgs() << \"LV: The target has \" << TargetNumRegisters\n                      << \" registers of \"\n                      << TTI.getRegisterClassName(pair.first) << \" register class\\n\");\n    if (VF.isScalar()) {\n      if (ForceTargetNumScalarRegs.getNumOccurrences() > 0)\n        TargetNumRegisters = ForceTargetNumScalarRegs;\n    } else {\n      if (ForceTargetNumVectorRegs.getNumOccurrences() > 0)\n        TargetNumRegisters = ForceTargetNumVectorRegs;\n    }\n    unsigned MaxLocalUsers = pair.second;\n    unsigned LoopInvariantRegs = 0;\n    if (R.LoopInvariantRegs.find(pair.first) != R.LoopInvariantRegs.end())\n      LoopInvariantRegs = R.LoopInvariantRegs[pair.first];\n\n    unsigned TmpIC = PowerOf2Floor((TargetNumRegisters - LoopInvariantRegs) / MaxLocalUsers);\n    // Don't count the induction variable as interleaved.\n    if (EnableIndVarRegisterHeur) {\n      TmpIC =\n          PowerOf2Floor((TargetNumRegisters - LoopInvariantRegs - 1) /\n                        std::max(1U, (MaxLocalUsers - 1)));\n    }\n\n    IC = std::min(IC, TmpIC);\n  }\n\n  // Clamp the interleave ranges to reasonable counts.\n  unsigned MaxInterleaveCount =\n      TTI.getMaxInterleaveFactor(VF.getKnownMinValue());\n\n  // Check if the user has overridden the max.\n  if (VF.isScalar()) {\n    if (ForceTargetMaxScalarInterleaveFactor.getNumOccurrences() > 0)\n      MaxInterleaveCount = ForceTargetMaxScalarInterleaveFactor;\n  } else {\n    if (ForceTargetMaxVectorInterleaveFactor.getNumOccurrences() > 0)\n      MaxInterleaveCount = ForceTargetMaxVectorInterleaveFactor;\n  }\n\n  // If trip count is known or estimated compile time constant, limit the\n  // interleave count to be less than the trip count divided by VF, provided it\n  // is at least 1.\n  //\n  // For scalable vectors we can't know if interleaving is beneficial. It may\n  // not be beneficial for small loops if none of the lanes in the second vector\n  // iterations is enabled. However, for larger loops, there is likely to be a\n  // similar benefit as for fixed-width vectors. For now, we choose to leave\n  // the InterleaveCount as if vscale is '1', although if some information about\n  // the vector is known (e.g. min vector size), we can make a better decision.\n  if (BestKnownTC) {\n    MaxInterleaveCount =\n        std::min(*BestKnownTC / VF.getKnownMinValue(), MaxInterleaveCount);\n    // Make sure MaxInterleaveCount is greater than 0.\n    MaxInterleaveCount = std::max(1u, MaxInterleaveCount);\n  }\n\n  assert(MaxInterleaveCount > 0 &&\n         \"Maximum interleave count must be greater than 0\");\n\n  // Clamp the calculated IC to be between the 1 and the max interleave count\n  // that the target and trip count allows.\n  if (IC > MaxInterleaveCount)\n    IC = MaxInterleaveCount;\n  else\n    // Make sure IC is greater than 0.\n    IC = std::max(1u, IC);\n\n  assert(IC > 0 && \"Interleave count must be greater than 0.\");\n\n  // If we did not calculate the cost for VF (because the user selected the VF)\n  // then we calculate the cost of VF here.\n  if (LoopCost == 0) {\n    assert(expectedCost(VF).first.isValid() && \"Expected a valid cost\");\n    LoopCost = *expectedCost(VF).first.getValue();\n  }\n\n  assert(LoopCost && \"Non-zero loop cost expected\");\n\n  // Interleave if we vectorized this loop and there is a reduction that could\n  // benefit from interleaving.\n  if (VF.isVector() && HasReductions) {\n    LLVM_DEBUG(dbgs() << \"LV: Interleaving because of reductions.\\n\");\n    return IC;\n  }\n\n  // Note that if we've already vectorized the loop we will have done the\n  // runtime check and so interleaving won't require further checks.\n  bool InterleavingRequiresRuntimePointerCheck =\n      (VF.isScalar() && Legal->getRuntimePointerChecking()->Need);\n\n  // We want to interleave small loops in order to reduce the loop overhead and\n  // potentially expose ILP opportunities.\n  LLVM_DEBUG(dbgs() << \"LV: Loop cost is \" << LoopCost << '\\n'\n                    << \"LV: IC is \" << IC << '\\n'\n                    << \"LV: VF is \" << VF << '\\n');\n  const bool AggressivelyInterleaveReductions =\n      TTI.enableAggressiveInterleaving(HasReductions);\n  if (!InterleavingRequiresRuntimePointerCheck && LoopCost < SmallLoopCost) {\n    // We assume that the cost overhead is 1 and we use the cost model\n    // to estimate the cost of the loop and interleave until the cost of the\n    // loop overhead is about 5% of the cost of the loop.\n    unsigned SmallIC =\n        std::min(IC, (unsigned)PowerOf2Floor(SmallLoopCost / LoopCost));\n\n    // Interleave until store/load ports (estimated by max interleave count) are\n    // saturated.\n    unsigned NumStores = Legal->getNumStores();\n    unsigned NumLoads = Legal->getNumLoads();\n    unsigned StoresIC = IC / (NumStores ? NumStores : 1);\n    unsigned LoadsIC = IC / (NumLoads ? NumLoads : 1);\n\n    // If we have a scalar reduction (vector reductions are already dealt with\n    // by this point), we can increase the critical path length if the loop\n    // we're interleaving is inside another loop. Limit, by default to 2, so the\n    // critical path only gets increased by one reduction operation.\n    if (HasReductions && TheLoop->getLoopDepth() > 1) {\n      unsigned F = static_cast<unsigned>(MaxNestedScalarReductionIC);\n      SmallIC = std::min(SmallIC, F);\n      StoresIC = std::min(StoresIC, F);\n      LoadsIC = std::min(LoadsIC, F);\n    }\n\n    if (EnableLoadStoreRuntimeInterleave &&\n        std::max(StoresIC, LoadsIC) > SmallIC) {\n      LLVM_DEBUG(\n          dbgs() << \"LV: Interleaving to saturate store or load ports.\\n\");\n      return std::max(StoresIC, LoadsIC);\n    }\n\n    // If there are scalar reductions and TTI has enabled aggressive\n    // interleaving for reductions, we will interleave to expose ILP.\n    if (InterleaveSmallLoopScalarReduction && VF.isScalar() &&\n        AggressivelyInterleaveReductions) {\n      LLVM_DEBUG(dbgs() << \"LV: Interleaving to expose ILP.\\n\");\n      // Interleave no less than SmallIC but not as aggressive as the normal IC\n      // to satisfy the rare situation when resources are too limited.\n      return std::max(IC / 2, SmallIC);\n    } else {\n      LLVM_DEBUG(dbgs() << \"LV: Interleaving to reduce branch cost.\\n\");\n      return SmallIC;\n    }\n  }\n\n  // Interleave if this is a large loop (small loops are already dealt with by\n  // this point) that could benefit from interleaving.\n  if (AggressivelyInterleaveReductions) {\n    LLVM_DEBUG(dbgs() << \"LV: Interleaving to expose ILP.\\n\");\n    return IC;\n  }\n\n  LLVM_DEBUG(dbgs() << \"LV: Not Interleaving.\\n\");\n  return 1;\n}\n\nSmallVector<LoopVectorizationCostModel::RegisterUsage, 8>\nLoopVectorizationCostModel::calculateRegisterUsage(ArrayRef<ElementCount> VFs) {\n  // This function calculates the register usage by measuring the highest number\n  // of values that are alive at a single location. Obviously, this is a very\n  // rough estimation. We scan the loop in a topological order in order and\n  // assign a number to each instruction. We use RPO to ensure that defs are\n  // met before their users. We assume that each instruction that has in-loop\n  // users starts an interval. We record every time that an in-loop value is\n  // used, so we have a list of the first and last occurrences of each\n  // instruction. Next, we transpose this data structure into a multi map that\n  // holds the list of intervals that *end* at a specific location. This multi\n  // map allows us to perform a linear search. We scan the instructions linearly\n  // and record each time that a new interval starts, by placing it in a set.\n  // If we find this value in the multi-map then we remove it from the set.\n  // The max register usage is the maximum size of the set.\n  // We also search for instructions that are defined outside the loop, but are\n  // used inside the loop. We need this number separately from the max-interval\n  // usage number because when we unroll, loop-invariant values do not take\n  // more register.\n  LoopBlocksDFS DFS(TheLoop);\n  DFS.perform(LI);\n\n  RegisterUsage RU;\n\n  // Each 'key' in the map opens a new interval. The values\n  // of the map are the index of the 'last seen' usage of the\n  // instruction that is the key.\n  using IntervalMap = DenseMap<Instruction *, unsigned>;\n\n  // Maps instruction to its index.\n  SmallVector<Instruction *, 64> IdxToInstr;\n  // Marks the end of each interval.\n  IntervalMap EndPoint;\n  // Saves the list of instruction indices that are used in the loop.\n  SmallPtrSet<Instruction *, 8> Ends;\n  // Saves the list of values that are used in the loop but are\n  // defined outside the loop, such as arguments and constants.\n  SmallPtrSet<Value *, 8> LoopInvariants;\n\n  for (BasicBlock *BB : make_range(DFS.beginRPO(), DFS.endRPO())) {\n    for (Instruction &I : BB->instructionsWithoutDebug()) {\n      IdxToInstr.push_back(&I);\n\n      // Save the end location of each USE.\n      for (Value *U : I.operands()) {\n        auto *Instr = dyn_cast<Instruction>(U);\n\n        // Ignore non-instruction values such as arguments, constants, etc.\n        if (!Instr)\n          continue;\n\n        // If this instruction is outside the loop then record it and continue.\n        if (!TheLoop->contains(Instr)) {\n          LoopInvariants.insert(Instr);\n          continue;\n        }\n\n        // Overwrite previous end points.\n        EndPoint[Instr] = IdxToInstr.size();\n        Ends.insert(Instr);\n      }\n    }\n  }\n\n  // Saves the list of intervals that end with the index in 'key'.\n  using InstrList = SmallVector<Instruction *, 2>;\n  DenseMap<unsigned, InstrList> TransposeEnds;\n\n  // Transpose the EndPoints to a list of values that end at each index.\n  for (auto &Interval : EndPoint)\n    TransposeEnds[Interval.second].push_back(Interval.first);\n\n  SmallPtrSet<Instruction *, 8> OpenIntervals;\n  SmallVector<RegisterUsage, 8> RUs(VFs.size());\n  SmallVector<SmallMapVector<unsigned, unsigned, 4>, 8> MaxUsages(VFs.size());\n\n  LLVM_DEBUG(dbgs() << \"LV(REG): Calculating max register usage:\\n\");\n\n  // A lambda that gets the register usage for the given type and VF.\n  const auto &TTICapture = TTI;\n  auto GetRegUsage = [&TTICapture](Type *Ty, ElementCount VF) {\n    if (Ty->isTokenTy() || !VectorType::isValidElementType(Ty))\n      return 0U;\n    return TTICapture.getRegUsageForType(VectorType::get(Ty, VF));\n  };\n\n  for (unsigned int i = 0, s = IdxToInstr.size(); i < s; ++i) {\n    Instruction *I = IdxToInstr[i];\n\n    // Remove all of the instructions that end at this location.\n    InstrList &List = TransposeEnds[i];\n    for (Instruction *ToRemove : List)\n      OpenIntervals.erase(ToRemove);\n\n    // Ignore instructions that are never used within the loop.\n    if (!Ends.count(I))\n      continue;\n\n    // Skip ignored values.\n    if (ValuesToIgnore.count(I))\n      continue;\n\n    // For each VF find the maximum usage of registers.\n    for (unsigned j = 0, e = VFs.size(); j < e; ++j) {\n      // Count the number of live intervals.\n      SmallMapVector<unsigned, unsigned, 4> RegUsage;\n\n      if (VFs[j].isScalar()) {\n        for (auto Inst : OpenIntervals) {\n          unsigned ClassID = TTI.getRegisterClassForType(false, Inst->getType());\n          if (RegUsage.find(ClassID) == RegUsage.end())\n            RegUsage[ClassID] = 1;\n          else\n            RegUsage[ClassID] += 1;\n        }\n      } else {\n        collectUniformsAndScalars(VFs[j]);\n        for (auto Inst : OpenIntervals) {\n          // Skip ignored values for VF > 1.\n          if (VecValuesToIgnore.count(Inst))\n            continue;\n          if (isScalarAfterVectorization(Inst, VFs[j])) {\n            unsigned ClassID = TTI.getRegisterClassForType(false, Inst->getType());\n            if (RegUsage.find(ClassID) == RegUsage.end())\n              RegUsage[ClassID] = 1;\n            else\n              RegUsage[ClassID] += 1;\n          } else {\n            unsigned ClassID = TTI.getRegisterClassForType(true, Inst->getType());\n            if (RegUsage.find(ClassID) == RegUsage.end())\n              RegUsage[ClassID] = GetRegUsage(Inst->getType(), VFs[j]);\n            else\n              RegUsage[ClassID] += GetRegUsage(Inst->getType(), VFs[j]);\n          }\n        }\n      }\n\n      for (auto& pair : RegUsage) {\n        if (MaxUsages[j].find(pair.first) != MaxUsages[j].end())\n          MaxUsages[j][pair.first] = std::max(MaxUsages[j][pair.first], pair.second);\n        else\n          MaxUsages[j][pair.first] = pair.second;\n      }\n    }\n\n    LLVM_DEBUG(dbgs() << \"LV(REG): At #\" << i << \" Interval # \"\n                      << OpenIntervals.size() << '\\n');\n\n    // Add the current instruction to the list of open intervals.\n    OpenIntervals.insert(I);\n  }\n\n  for (unsigned i = 0, e = VFs.size(); i < e; ++i) {\n    SmallMapVector<unsigned, unsigned, 4> Invariant;\n\n    for (auto Inst : LoopInvariants) {\n      unsigned Usage =\n          VFs[i].isScalar() ? 1 : GetRegUsage(Inst->getType(), VFs[i]);\n      unsigned ClassID =\n          TTI.getRegisterClassForType(VFs[i].isVector(), Inst->getType());\n      if (Invariant.find(ClassID) == Invariant.end())\n        Invariant[ClassID] = Usage;\n      else\n        Invariant[ClassID] += Usage;\n    }\n\n    LLVM_DEBUG({\n      dbgs() << \"LV(REG): VF = \" << VFs[i] << '\\n';\n      dbgs() << \"LV(REG): Found max usage: \" << MaxUsages[i].size()\n             << \" item\\n\";\n      for (const auto &pair : MaxUsages[i]) {\n        dbgs() << \"LV(REG): RegisterClass: \"\n               << TTI.getRegisterClassName(pair.first) << \", \" << pair.second\n               << \" registers\\n\";\n      }\n      dbgs() << \"LV(REG): Found invariant usage: \" << Invariant.size()\n             << \" item\\n\";\n      for (const auto &pair : Invariant) {\n        dbgs() << \"LV(REG): RegisterClass: \"\n               << TTI.getRegisterClassName(pair.first) << \", \" << pair.second\n               << \" registers\\n\";\n      }\n    });\n\n    RU.LoopInvariantRegs = Invariant;\n    RU.MaxLocalUsers = MaxUsages[i];\n    RUs[i] = RU;\n  }\n\n  return RUs;\n}\n\nbool LoopVectorizationCostModel::useEmulatedMaskMemRefHack(Instruction *I){\n  // TODO: Cost model for emulated masked load/store is completely\n  // broken. This hack guides the cost model to use an artificially\n  // high enough value to practically disable vectorization with such\n  // operations, except where previously deployed legality hack allowed\n  // using very low cost values. This is to avoid regressions coming simply\n  // from moving \"masked load/store\" check from legality to cost model.\n  // Masked Load/Gather emulation was previously never allowed.\n  // Limited number of Masked Store/Scatter emulation was allowed.\n  assert(isPredicatedInst(I) && \"Expecting a scalar emulated instruction\");\n  return isa<LoadInst>(I) ||\n         (isa<StoreInst>(I) &&\n          NumPredStores > NumberOfStoresToPredicate);\n}\n\nvoid LoopVectorizationCostModel::collectInstsToScalarize(ElementCount VF) {\n  // If we aren't vectorizing the loop, or if we've already collected the\n  // instructions to scalarize, there's nothing to do. Collection may already\n  // have occurred if we have a user-selected VF and are now computing the\n  // expected cost for interleaving.\n  if (VF.isScalar() || VF.isZero() ||\n      InstsToScalarize.find(VF) != InstsToScalarize.end())\n    return;\n\n  // Initialize a mapping for VF in InstsToScalalarize. If we find that it's\n  // not profitable to scalarize any instructions, the presence of VF in the\n  // map will indicate that we've analyzed it already.\n  ScalarCostsTy &ScalarCostsVF = InstsToScalarize[VF];\n\n  // Find all the instructions that are scalar with predication in the loop and\n  // determine if it would be better to not if-convert the blocks they are in.\n  // If so, we also record the instructions to scalarize.\n  for (BasicBlock *BB : TheLoop->blocks()) {\n    if (!blockNeedsPredication(BB))\n      continue;\n    for (Instruction &I : *BB)\n      if (isScalarWithPredication(&I)) {\n        ScalarCostsTy ScalarCosts;\n        // Do not apply discount logic if hacked cost is needed\n        // for emulated masked memrefs.\n        if (!useEmulatedMaskMemRefHack(&I) &&\n            computePredInstDiscount(&I, ScalarCosts, VF) >= 0)\n          ScalarCostsVF.insert(ScalarCosts.begin(), ScalarCosts.end());\n        // Remember that BB will remain after vectorization.\n        PredicatedBBsAfterVectorization.insert(BB);\n      }\n  }\n}\n\nint LoopVectorizationCostModel::computePredInstDiscount(\n    Instruction *PredInst, ScalarCostsTy &ScalarCosts, ElementCount VF) {\n  assert(!isUniformAfterVectorization(PredInst, VF) &&\n         \"Instruction marked uniform-after-vectorization will be predicated\");\n\n  // Initialize the discount to zero, meaning that the scalar version and the\n  // vector version cost the same.\n  InstructionCost Discount = 0;\n\n  // Holds instructions to analyze. The instructions we visit are mapped in\n  // ScalarCosts. Those instructions are the ones that would be scalarized if\n  // we find that the scalar version costs less.\n  SmallVector<Instruction *, 8> Worklist;\n\n  // Returns true if the given instruction can be scalarized.\n  auto canBeScalarized = [&](Instruction *I) -> bool {\n    // We only attempt to scalarize instructions forming a single-use chain\n    // from the original predicated block that would otherwise be vectorized.\n    // Although not strictly necessary, we give up on instructions we know will\n    // already be scalar to avoid traversing chains that are unlikely to be\n    // beneficial.\n    if (!I->hasOneUse() || PredInst->getParent() != I->getParent() ||\n        isScalarAfterVectorization(I, VF))\n      return false;\n\n    // If the instruction is scalar with predication, it will be analyzed\n    // separately. We ignore it within the context of PredInst.\n    if (isScalarWithPredication(I))\n      return false;\n\n    // If any of the instruction's operands are uniform after vectorization,\n    // the instruction cannot be scalarized. This prevents, for example, a\n    // masked load from being scalarized.\n    //\n    // We assume we will only emit a value for lane zero of an instruction\n    // marked uniform after vectorization, rather than VF identical values.\n    // Thus, if we scalarize an instruction that uses a uniform, we would\n    // create uses of values corresponding to the lanes we aren't emitting code\n    // for. This behavior can be changed by allowing getScalarValue to clone\n    // the lane zero values for uniforms rather than asserting.\n    for (Use &U : I->operands())\n      if (auto *J = dyn_cast<Instruction>(U.get()))\n        if (isUniformAfterVectorization(J, VF))\n          return false;\n\n    // Otherwise, we can scalarize the instruction.\n    return true;\n  };\n\n  // Compute the expected cost discount from scalarizing the entire expression\n  // feeding the predicated instruction. We currently only consider expressions\n  // that are single-use instruction chains.\n  Worklist.push_back(PredInst);\n  while (!Worklist.empty()) {\n    Instruction *I = Worklist.pop_back_val();\n\n    // If we've already analyzed the instruction, there's nothing to do.\n    if (ScalarCosts.find(I) != ScalarCosts.end())\n      continue;\n\n    // Compute the cost of the vector instruction. Note that this cost already\n    // includes the scalarization overhead of the predicated instruction.\n    InstructionCost VectorCost = getInstructionCost(I, VF).first;\n\n    // Compute the cost of the scalarized instruction. This cost is the cost of\n    // the instruction as if it wasn't if-converted and instead remained in the\n    // predicated block. We will scale this cost by block probability after\n    // computing the scalarization overhead.\n    assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n    InstructionCost ScalarCost =\n        VF.getKnownMinValue() *\n        getInstructionCost(I, ElementCount::getFixed(1)).first;\n\n    // Compute the scalarization overhead of needed insertelement instructions\n    // and phi nodes.\n    if (isScalarWithPredication(I) && !I->getType()->isVoidTy()) {\n      ScalarCost += TTI.getScalarizationOverhead(\n          cast<VectorType>(ToVectorTy(I->getType(), VF)),\n          APInt::getAllOnesValue(VF.getKnownMinValue()), true, false);\n      assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n      ScalarCost +=\n          VF.getKnownMinValue() *\n          TTI.getCFInstrCost(Instruction::PHI, TTI::TCK_RecipThroughput);\n    }\n\n    // Compute the scalarization overhead of needed extractelement\n    // instructions. For each of the instruction's operands, if the operand can\n    // be scalarized, add it to the worklist; otherwise, account for the\n    // overhead.\n    for (Use &U : I->operands())\n      if (auto *J = dyn_cast<Instruction>(U.get())) {\n        assert(VectorType::isValidElementType(J->getType()) &&\n               \"Instruction has non-scalar type\");\n        if (canBeScalarized(J))\n          Worklist.push_back(J);\n        else if (needsExtract(J, VF)) {\n          assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n          ScalarCost += TTI.getScalarizationOverhead(\n              cast<VectorType>(ToVectorTy(J->getType(), VF)),\n              APInt::getAllOnesValue(VF.getKnownMinValue()), false, true);\n        }\n      }\n\n    // Scale the total scalar cost by block probability.\n    ScalarCost /= getReciprocalPredBlockProb();\n\n    // Compute the discount. A non-negative discount means the vector version\n    // of the instruction costs more, and scalarizing would be beneficial.\n    Discount += VectorCost - ScalarCost;\n    ScalarCosts[I] = ScalarCost;\n  }\n\n  return *Discount.getValue();\n}\n\nLoopVectorizationCostModel::VectorizationCostTy\nLoopVectorizationCostModel::expectedCost(ElementCount VF) {\n  VectorizationCostTy Cost;\n\n  // For each block.\n  for (BasicBlock *BB : TheLoop->blocks()) {\n    VectorizationCostTy BlockCost;\n\n    // For each instruction in the old loop.\n    for (Instruction &I : BB->instructionsWithoutDebug()) {\n      // Skip ignored values.\n      if (ValuesToIgnore.count(&I) ||\n          (VF.isVector() && VecValuesToIgnore.count(&I)))\n        continue;\n\n      VectorizationCostTy C = getInstructionCost(&I, VF);\n\n      // Check if we should override the cost.\n      if (ForceTargetInstructionCost.getNumOccurrences() > 0)\n        C.first = InstructionCost(ForceTargetInstructionCost);\n\n      BlockCost.first += C.first;\n      BlockCost.second |= C.second;\n      LLVM_DEBUG(dbgs() << \"LV: Found an estimated cost of \" << C.first\n                        << \" for VF \" << VF << \" For instruction: \" << I\n                        << '\\n');\n    }\n\n    // If we are vectorizing a predicated block, it will have been\n    // if-converted. This means that the block's instructions (aside from\n    // stores and instructions that may divide by zero) will now be\n    // unconditionally executed. For the scalar case, we may not always execute\n    // the predicated block, if it is an if-else block. Thus, scale the block's\n    // cost by the probability of executing it. blockNeedsPredication from\n    // Legal is used so as to not include all blocks in tail folded loops.\n    if (VF.isScalar() && Legal->blockNeedsPredication(BB))\n      BlockCost.first /= getReciprocalPredBlockProb();\n\n    Cost.first += BlockCost.first;\n    Cost.second |= BlockCost.second;\n  }\n\n  return Cost;\n}\n\n/// Gets Address Access SCEV after verifying that the access pattern\n/// is loop invariant except the induction variable dependence.\n///\n/// This SCEV can be sent to the Target in order to estimate the address\n/// calculation cost.\nstatic const SCEV *getAddressAccessSCEV(\n              Value *Ptr,\n              LoopVectorizationLegality *Legal,\n              PredicatedScalarEvolution &PSE,\n              const Loop *TheLoop) {\n\n  auto *Gep = dyn_cast<GetElementPtrInst>(Ptr);\n  if (!Gep)\n    return nullptr;\n\n  // We are looking for a gep with all loop invariant indices except for one\n  // which should be an induction variable.\n  auto SE = PSE.getSE();\n  unsigned NumOperands = Gep->getNumOperands();\n  for (unsigned i = 1; i < NumOperands; ++i) {\n    Value *Opd = Gep->getOperand(i);\n    if (!SE->isLoopInvariant(SE->getSCEV(Opd), TheLoop) &&\n        !Legal->isInductionVariable(Opd))\n      return nullptr;\n  }\n\n  // Now we know we have a GEP ptr, %inv, %ind, %inv. return the Ptr SCEV.\n  return PSE.getSCEV(Ptr);\n}\n\nstatic bool isStrideMul(Instruction *I, LoopVectorizationLegality *Legal) {\n  return Legal->hasStride(I->getOperand(0)) ||\n         Legal->hasStride(I->getOperand(1));\n}\n\nInstructionCost\nLoopVectorizationCostModel::getMemInstScalarizationCost(Instruction *I,\n                                                        ElementCount VF) {\n  assert(VF.isVector() &&\n         \"Scalarization cost of instruction implies vectorization.\");\n  assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n  Type *ValTy = getMemInstValueType(I);\n  auto SE = PSE.getSE();\n\n  unsigned AS = getLoadStoreAddressSpace(I);\n  Value *Ptr = getLoadStorePointerOperand(I);\n  Type *PtrTy = ToVectorTy(Ptr->getType(), VF);\n\n  // Figure out whether the access is strided and get the stride value\n  // if it's known in compile time\n  const SCEV *PtrSCEV = getAddressAccessSCEV(Ptr, Legal, PSE, TheLoop);\n\n  // Get the cost of the scalar memory instruction and address computation.\n  InstructionCost Cost =\n      VF.getKnownMinValue() * TTI.getAddressComputationCost(PtrTy, SE, PtrSCEV);\n\n  // Don't pass *I here, since it is scalar but will actually be part of a\n  // vectorized loop where the user of it is a vectorized instruction.\n  const Align Alignment = getLoadStoreAlignment(I);\n  Cost += VF.getKnownMinValue() *\n          TTI.getMemoryOpCost(I->getOpcode(), ValTy->getScalarType(), Alignment,\n                              AS, TTI::TCK_RecipThroughput);\n\n  // Get the overhead of the extractelement and insertelement instructions\n  // we might create due to scalarization.\n  Cost += getScalarizationOverhead(I, VF);\n\n  // If we have a predicated store, it may not be executed for each vector\n  // lane. Scale the cost by the probability of executing the predicated\n  // block.\n  if (isPredicatedInst(I)) {\n    Cost /= getReciprocalPredBlockProb();\n\n    if (useEmulatedMaskMemRefHack(I))\n      // Artificially setting to a high enough value to practically disable\n      // vectorization with such operations.\n      Cost = 3000000;\n  }\n\n  return Cost;\n}\n\nInstructionCost\nLoopVectorizationCostModel::getConsecutiveMemOpCost(Instruction *I,\n                                                    ElementCount VF) {\n  Type *ValTy = getMemInstValueType(I);\n  auto *VectorTy = cast<VectorType>(ToVectorTy(ValTy, VF));\n  Value *Ptr = getLoadStorePointerOperand(I);\n  unsigned AS = getLoadStoreAddressSpace(I);\n  int ConsecutiveStride = Legal->isConsecutivePtr(Ptr);\n  enum TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput;\n\n  assert((ConsecutiveStride == 1 || ConsecutiveStride == -1) &&\n         \"Stride should be 1 or -1 for consecutive memory access\");\n  const Align Alignment = getLoadStoreAlignment(I);\n  InstructionCost Cost = 0;\n  if (Legal->isMaskRequired(I))\n    Cost += TTI.getMaskedMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS,\n                                      CostKind);\n  else\n    Cost += TTI.getMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS,\n                                CostKind, I);\n\n  bool Reverse = ConsecutiveStride < 0;\n  if (Reverse)\n    Cost += TTI.getShuffleCost(TargetTransformInfo::SK_Reverse, VectorTy, 0);\n  return Cost;\n}\n\nInstructionCost\nLoopVectorizationCostModel::getUniformMemOpCost(Instruction *I,\n                                                ElementCount VF) {\n  assert(Legal->isUniformMemOp(*I));\n\n  Type *ValTy = getMemInstValueType(I);\n  auto *VectorTy = cast<VectorType>(ToVectorTy(ValTy, VF));\n  const Align Alignment = getLoadStoreAlignment(I);\n  unsigned AS = getLoadStoreAddressSpace(I);\n  enum TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput;\n  if (isa<LoadInst>(I)) {\n    return TTI.getAddressComputationCost(ValTy) +\n           TTI.getMemoryOpCost(Instruction::Load, ValTy, Alignment, AS,\n                               CostKind) +\n           TTI.getShuffleCost(TargetTransformInfo::SK_Broadcast, VectorTy);\n  }\n  StoreInst *SI = cast<StoreInst>(I);\n\n  bool isLoopInvariantStoreValue = Legal->isUniform(SI->getValueOperand());\n  return TTI.getAddressComputationCost(ValTy) +\n         TTI.getMemoryOpCost(Instruction::Store, ValTy, Alignment, AS,\n                             CostKind) +\n         (isLoopInvariantStoreValue\n              ? 0\n              : TTI.getVectorInstrCost(Instruction::ExtractElement, VectorTy,\n                                       VF.getKnownMinValue() - 1));\n}\n\nInstructionCost\nLoopVectorizationCostModel::getGatherScatterCost(Instruction *I,\n                                                 ElementCount VF) {\n  Type *ValTy = getMemInstValueType(I);\n  auto *VectorTy = cast<VectorType>(ToVectorTy(ValTy, VF));\n  const Align Alignment = getLoadStoreAlignment(I);\n  const Value *Ptr = getLoadStorePointerOperand(I);\n\n  return TTI.getAddressComputationCost(VectorTy) +\n         TTI.getGatherScatterOpCost(\n             I->getOpcode(), VectorTy, Ptr, Legal->isMaskRequired(I), Alignment,\n             TargetTransformInfo::TCK_RecipThroughput, I);\n}\n\nInstructionCost\nLoopVectorizationCostModel::getInterleaveGroupCost(Instruction *I,\n                                                   ElementCount VF) {\n  // TODO: Once we have support for interleaving with scalable vectors\n  // we can calculate the cost properly here.\n  if (VF.isScalable())\n    return InstructionCost::getInvalid();\n\n  Type *ValTy = getMemInstValueType(I);\n  auto *VectorTy = cast<VectorType>(ToVectorTy(ValTy, VF));\n  unsigned AS = getLoadStoreAddressSpace(I);\n\n  auto Group = getInterleavedAccessGroup(I);\n  assert(Group && \"Fail to get an interleaved access group.\");\n\n  unsigned InterleaveFactor = Group->getFactor();\n  auto *WideVecTy = VectorType::get(ValTy, VF * InterleaveFactor);\n\n  // Holds the indices of existing members in an interleaved load group.\n  // An interleaved store group doesn't need this as it doesn't allow gaps.\n  SmallVector<unsigned, 4> Indices;\n  if (isa<LoadInst>(I)) {\n    for (unsigned i = 0; i < InterleaveFactor; i++)\n      if (Group->getMember(i))\n        Indices.push_back(i);\n  }\n\n  // Calculate the cost of the whole interleaved group.\n  bool UseMaskForGaps =\n      Group->requiresScalarEpilogue() && !isScalarEpilogueAllowed();\n  InstructionCost Cost = TTI.getInterleavedMemoryOpCost(\n      I->getOpcode(), WideVecTy, Group->getFactor(), Indices, Group->getAlign(),\n      AS, TTI::TCK_RecipThroughput, Legal->isMaskRequired(I), UseMaskForGaps);\n\n  if (Group->isReverse()) {\n    // TODO: Add support for reversed masked interleaved access.\n    assert(!Legal->isMaskRequired(I) &&\n           \"Reverse masked interleaved access not supported.\");\n    Cost += Group->getNumMembers() *\n            TTI.getShuffleCost(TargetTransformInfo::SK_Reverse, VectorTy, 0);\n  }\n  return Cost;\n}\n\nInstructionCost LoopVectorizationCostModel::getReductionPatternCost(\n    Instruction *I, ElementCount VF, Type *Ty, TTI::TargetCostKind CostKind) {\n  // Early exit for no inloop reductions\n  if (InLoopReductionChains.empty() || VF.isScalar() || !isa<VectorType>(Ty))\n    return InstructionCost::getInvalid();\n  auto *VectorTy = cast<VectorType>(Ty);\n\n  // We are looking for a pattern of, and finding the minimal acceptable cost:\n  //  reduce(mul(ext(A), ext(B))) or\n  //  reduce(mul(A, B)) or\n  //  reduce(ext(A)) or\n  //  reduce(A).\n  // The basic idea is that we walk down the tree to do that, finding the root\n  // reduction instruction in InLoopReductionImmediateChains. From there we find\n  // the pattern of mul/ext and test the cost of the entire pattern vs the cost\n  // of the components. If the reduction cost is lower then we return it for the\n  // reduction instruction and 0 for the other instructions in the pattern. If\n  // it is not we return an invalid cost specifying the orignal cost method\n  // should be used.\n  Instruction *RetI = I;\n  if ((RetI->getOpcode() == Instruction::SExt ||\n       RetI->getOpcode() == Instruction::ZExt)) {\n    if (!RetI->hasOneUser())\n      return InstructionCost::getInvalid();\n    RetI = RetI->user_back();\n  }\n  if (RetI->getOpcode() == Instruction::Mul &&\n      RetI->user_back()->getOpcode() == Instruction::Add) {\n    if (!RetI->hasOneUser())\n      return InstructionCost::getInvalid();\n    RetI = RetI->user_back();\n  }\n\n  // Test if the found instruction is a reduction, and if not return an invalid\n  // cost specifying the parent to use the original cost modelling.\n  if (!InLoopReductionImmediateChains.count(RetI))\n    return InstructionCost::getInvalid();\n\n  // Find the reduction this chain is a part of and calculate the basic cost of\n  // the reduction on its own.\n  Instruction *LastChain = InLoopReductionImmediateChains[RetI];\n  Instruction *ReductionPhi = LastChain;\n  while (!isa<PHINode>(ReductionPhi))\n    ReductionPhi = InLoopReductionImmediateChains[ReductionPhi];\n\n  RecurrenceDescriptor RdxDesc =\n      Legal->getReductionVars()[cast<PHINode>(ReductionPhi)];\n  unsigned BaseCost = TTI.getArithmeticReductionCost(RdxDesc.getOpcode(),\n                                                     VectorTy, false, CostKind);\n\n  // Get the operand that was not the reduction chain and match it to one of the\n  // patterns, returning the better cost if it is found.\n  Instruction *RedOp = RetI->getOperand(1) == LastChain\n                           ? dyn_cast<Instruction>(RetI->getOperand(0))\n                           : dyn_cast<Instruction>(RetI->getOperand(1));\n\n  VectorTy = VectorType::get(I->getOperand(0)->getType(), VectorTy);\n\n  if (RedOp && (isa<SExtInst>(RedOp) || isa<ZExtInst>(RedOp)) &&\n      !TheLoop->isLoopInvariant(RedOp)) {\n    bool IsUnsigned = isa<ZExtInst>(RedOp);\n    auto *ExtType = VectorType::get(RedOp->getOperand(0)->getType(), VectorTy);\n    InstructionCost RedCost = TTI.getExtendedAddReductionCost(\n        /*IsMLA=*/false, IsUnsigned, RdxDesc.getRecurrenceType(), ExtType,\n        CostKind);\n\n    unsigned ExtCost =\n        TTI.getCastInstrCost(RedOp->getOpcode(), VectorTy, ExtType,\n                             TTI::CastContextHint::None, CostKind, RedOp);\n    if (RedCost.isValid() && RedCost < BaseCost + ExtCost)\n      return I == RetI ? *RedCost.getValue() : 0;\n  } else if (RedOp && RedOp->getOpcode() == Instruction::Mul) {\n    Instruction *Mul = RedOp;\n    Instruction *Op0 = dyn_cast<Instruction>(Mul->getOperand(0));\n    Instruction *Op1 = dyn_cast<Instruction>(Mul->getOperand(1));\n    if (Op0 && Op1 && (isa<SExtInst>(Op0) || isa<ZExtInst>(Op0)) &&\n        Op0->getOpcode() == Op1->getOpcode() &&\n        Op0->getOperand(0)->getType() == Op1->getOperand(0)->getType() &&\n        !TheLoop->isLoopInvariant(Op0) && !TheLoop->isLoopInvariant(Op1)) {\n      bool IsUnsigned = isa<ZExtInst>(Op0);\n      auto *ExtType = VectorType::get(Op0->getOperand(0)->getType(), VectorTy);\n      // reduce(mul(ext, ext))\n      unsigned ExtCost =\n          TTI.getCastInstrCost(Op0->getOpcode(), VectorTy, ExtType,\n                               TTI::CastContextHint::None, CostKind, Op0);\n      InstructionCost MulCost =\n          TTI.getArithmeticInstrCost(Mul->getOpcode(), VectorTy, CostKind);\n\n      InstructionCost RedCost = TTI.getExtendedAddReductionCost(\n          /*IsMLA=*/true, IsUnsigned, RdxDesc.getRecurrenceType(), ExtType,\n          CostKind);\n\n      if (RedCost.isValid() && RedCost < ExtCost * 2 + MulCost + BaseCost)\n        return I == RetI ? *RedCost.getValue() : 0;\n    } else {\n      InstructionCost MulCost =\n          TTI.getArithmeticInstrCost(Mul->getOpcode(), VectorTy, CostKind);\n\n      InstructionCost RedCost = TTI.getExtendedAddReductionCost(\n          /*IsMLA=*/true, true, RdxDesc.getRecurrenceType(), VectorTy,\n          CostKind);\n\n      if (RedCost.isValid() && RedCost < MulCost + BaseCost)\n        return I == RetI ? *RedCost.getValue() : 0;\n    }\n  }\n\n  return I == RetI ? BaseCost : InstructionCost::getInvalid();\n}\n\nInstructionCost\nLoopVectorizationCostModel::getMemoryInstructionCost(Instruction *I,\n                                                     ElementCount VF) {\n  // Calculate scalar cost only. Vectorization cost should be ready at this\n  // moment.\n  if (VF.isScalar()) {\n    Type *ValTy = getMemInstValueType(I);\n    const Align Alignment = getLoadStoreAlignment(I);\n    unsigned AS = getLoadStoreAddressSpace(I);\n\n    return TTI.getAddressComputationCost(ValTy) +\n           TTI.getMemoryOpCost(I->getOpcode(), ValTy, Alignment, AS,\n                               TTI::TCK_RecipThroughput, I);\n  }\n  return getWideningCost(I, VF);\n}\n\nLoopVectorizationCostModel::VectorizationCostTy\nLoopVectorizationCostModel::getInstructionCost(Instruction *I,\n                                               ElementCount VF) {\n  // If we know that this instruction will remain uniform, check the cost of\n  // the scalar version.\n  if (isUniformAfterVectorization(I, VF))\n    VF = ElementCount::getFixed(1);\n\n  if (VF.isVector() && isProfitableToScalarize(I, VF))\n    return VectorizationCostTy(InstsToScalarize[VF][I], false);\n\n  // Forced scalars do not have any scalarization overhead.\n  auto ForcedScalar = ForcedScalars.find(VF);\n  if (VF.isVector() && ForcedScalar != ForcedScalars.end()) {\n    auto InstSet = ForcedScalar->second;\n    if (InstSet.count(I))\n      return VectorizationCostTy(\n          (getInstructionCost(I, ElementCount::getFixed(1)).first *\n           VF.getKnownMinValue()),\n          false);\n  }\n\n  Type *VectorTy;\n  InstructionCost C = getInstructionCost(I, VF, VectorTy);\n\n  bool TypeNotScalarized =\n      VF.isVector() && VectorTy->isVectorTy() &&\n      TTI.getNumberOfParts(VectorTy) < VF.getKnownMinValue();\n  return VectorizationCostTy(C, TypeNotScalarized);\n}\n\nInstructionCost\nLoopVectorizationCostModel::getScalarizationOverhead(Instruction *I,\n                                                     ElementCount VF) {\n\n  if (VF.isScalable())\n    return InstructionCost::getInvalid();\n\n  if (VF.isScalar())\n    return 0;\n\n  InstructionCost Cost = 0;\n  Type *RetTy = ToVectorTy(I->getType(), VF);\n  if (!RetTy->isVoidTy() &&\n      (!isa<LoadInst>(I) || !TTI.supportsEfficientVectorElementLoadStore()))\n    Cost += TTI.getScalarizationOverhead(\n        cast<VectorType>(RetTy), APInt::getAllOnesValue(VF.getKnownMinValue()),\n        true, false);\n\n  // Some targets keep addresses scalar.\n  if (isa<LoadInst>(I) && !TTI.prefersVectorizedAddressing())\n    return Cost;\n\n  // Some targets support efficient element stores.\n  if (isa<StoreInst>(I) && TTI.supportsEfficientVectorElementLoadStore())\n    return Cost;\n\n  // Collect operands to consider.\n  CallInst *CI = dyn_cast<CallInst>(I);\n  Instruction::op_range Ops = CI ? CI->arg_operands() : I->operands();\n\n  // Skip operands that do not require extraction/scalarization and do not incur\n  // any overhead.\n  SmallVector<Type *> Tys;\n  for (auto *V : filterExtractingOperands(Ops, VF))\n    Tys.push_back(MaybeVectorizeType(V->getType(), VF));\n  return Cost + TTI.getOperandsScalarizationOverhead(\n                    filterExtractingOperands(Ops, VF), Tys);\n}\n\nvoid LoopVectorizationCostModel::setCostBasedWideningDecision(ElementCount VF) {\n  if (VF.isScalar())\n    return;\n  NumPredStores = 0;\n  for (BasicBlock *BB : TheLoop->blocks()) {\n    // For each instruction in the old loop.\n    for (Instruction &I : *BB) {\n      Value *Ptr =  getLoadStorePointerOperand(&I);\n      if (!Ptr)\n        continue;\n\n      // TODO: We should generate better code and update the cost model for\n      // predicated uniform stores. Today they are treated as any other\n      // predicated store (see added test cases in\n      // invariant-store-vectorization.ll).\n      if (isa<StoreInst>(&I) && isScalarWithPredication(&I))\n        NumPredStores++;\n\n      if (Legal->isUniformMemOp(I)) {\n        // TODO: Avoid replicating loads and stores instead of\n        // relying on instcombine to remove them.\n        // Load: Scalar load + broadcast\n        // Store: Scalar store + isLoopInvariantStoreValue ? 0 : extract\n        InstructionCost Cost = getUniformMemOpCost(&I, VF);\n        setWideningDecision(&I, VF, CM_Scalarize, Cost);\n        continue;\n      }\n\n      // We assume that widening is the best solution when possible.\n      if (memoryInstructionCanBeWidened(&I, VF)) {\n        InstructionCost Cost = getConsecutiveMemOpCost(&I, VF);\n        int ConsecutiveStride =\n               Legal->isConsecutivePtr(getLoadStorePointerOperand(&I));\n        assert((ConsecutiveStride == 1 || ConsecutiveStride == -1) &&\n               \"Expected consecutive stride.\");\n        InstWidening Decision =\n            ConsecutiveStride == 1 ? CM_Widen : CM_Widen_Reverse;\n        setWideningDecision(&I, VF, Decision, Cost);\n        continue;\n      }\n\n      // Choose between Interleaving, Gather/Scatter or Scalarization.\n      InstructionCost InterleaveCost = InstructionCost::getInvalid();\n      unsigned NumAccesses = 1;\n      if (isAccessInterleaved(&I)) {\n        auto Group = getInterleavedAccessGroup(&I);\n        assert(Group && \"Fail to get an interleaved access group.\");\n\n        // Make one decision for the whole group.\n        if (getWideningDecision(&I, VF) != CM_Unknown)\n          continue;\n\n        NumAccesses = Group->getNumMembers();\n        if (interleavedAccessCanBeWidened(&I, VF))\n          InterleaveCost = getInterleaveGroupCost(&I, VF);\n      }\n\n      InstructionCost GatherScatterCost =\n          isLegalGatherOrScatter(&I)\n              ? getGatherScatterCost(&I, VF) * NumAccesses\n              : InstructionCost::getInvalid();\n\n      InstructionCost ScalarizationCost =\n          !VF.isScalable() ? getMemInstScalarizationCost(&I, VF) * NumAccesses\n                           : InstructionCost::getInvalid();\n\n      // Choose better solution for the current VF,\n      // write down this decision and use it during vectorization.\n      InstructionCost Cost;\n      InstWidening Decision;\n      if (InterleaveCost <= GatherScatterCost &&\n          InterleaveCost < ScalarizationCost) {\n        Decision = CM_Interleave;\n        Cost = InterleaveCost;\n      } else if (GatherScatterCost < ScalarizationCost) {\n        Decision = CM_GatherScatter;\n        Cost = GatherScatterCost;\n      } else {\n        assert(!VF.isScalable() &&\n               \"We cannot yet scalarise for scalable vectors\");\n        Decision = CM_Scalarize;\n        Cost = ScalarizationCost;\n      }\n      // If the instructions belongs to an interleave group, the whole group\n      // receives the same decision. The whole group receives the cost, but\n      // the cost will actually be assigned to one instruction.\n      if (auto Group = getInterleavedAccessGroup(&I))\n        setWideningDecision(Group, VF, Decision, Cost);\n      else\n        setWideningDecision(&I, VF, Decision, Cost);\n    }\n  }\n\n  // Make sure that any load of address and any other address computation\n  // remains scalar unless there is gather/scatter support. This avoids\n  // inevitable extracts into address registers, and also has the benefit of\n  // activating LSR more, since that pass can't optimize vectorized\n  // addresses.\n  if (TTI.prefersVectorizedAddressing())\n    return;\n\n  // Start with all scalar pointer uses.\n  SmallPtrSet<Instruction *, 8> AddrDefs;\n  for (BasicBlock *BB : TheLoop->blocks())\n    for (Instruction &I : *BB) {\n      Instruction *PtrDef =\n        dyn_cast_or_null<Instruction>(getLoadStorePointerOperand(&I));\n      if (PtrDef && TheLoop->contains(PtrDef) &&\n          getWideningDecision(&I, VF) != CM_GatherScatter)\n        AddrDefs.insert(PtrDef);\n    }\n\n  // Add all instructions used to generate the addresses.\n  SmallVector<Instruction *, 4> Worklist;\n  append_range(Worklist, AddrDefs);\n  while (!Worklist.empty()) {\n    Instruction *I = Worklist.pop_back_val();\n    for (auto &Op : I->operands())\n      if (auto *InstOp = dyn_cast<Instruction>(Op))\n        if ((InstOp->getParent() == I->getParent()) && !isa<PHINode>(InstOp) &&\n            AddrDefs.insert(InstOp).second)\n          Worklist.push_back(InstOp);\n  }\n\n  for (auto *I : AddrDefs) {\n    if (isa<LoadInst>(I)) {\n      // Setting the desired widening decision should ideally be handled in\n      // by cost functions, but since this involves the task of finding out\n      // if the loaded register is involved in an address computation, it is\n      // instead changed here when we know this is the case.\n      InstWidening Decision = getWideningDecision(I, VF);\n      if (Decision == CM_Widen || Decision == CM_Widen_Reverse)\n        // Scalarize a widened load of address.\n        setWideningDecision(\n            I, VF, CM_Scalarize,\n            (VF.getKnownMinValue() *\n             getMemoryInstructionCost(I, ElementCount::getFixed(1))));\n      else if (auto Group = getInterleavedAccessGroup(I)) {\n        // Scalarize an interleave group of address loads.\n        for (unsigned I = 0; I < Group->getFactor(); ++I) {\n          if (Instruction *Member = Group->getMember(I))\n            setWideningDecision(\n                Member, VF, CM_Scalarize,\n                (VF.getKnownMinValue() *\n                 getMemoryInstructionCost(Member, ElementCount::getFixed(1))));\n        }\n      }\n    } else\n      // Make sure I gets scalarized and a cost estimate without\n      // scalarization overhead.\n      ForcedScalars[VF].insert(I);\n  }\n}\n\nInstructionCost\nLoopVectorizationCostModel::getInstructionCost(Instruction *I, ElementCount VF,\n                                               Type *&VectorTy) {\n  Type *RetTy = I->getType();\n  if (canTruncateToMinimalBitwidth(I, VF))\n    RetTy = IntegerType::get(RetTy->getContext(), MinBWs[I]);\n  VectorTy = isScalarAfterVectorization(I, VF) ? RetTy : ToVectorTy(RetTy, VF);\n  auto SE = PSE.getSE();\n  TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput;\n\n  // TODO: We need to estimate the cost of intrinsic calls.\n  switch (I->getOpcode()) {\n  case Instruction::GetElementPtr:\n    // We mark this instruction as zero-cost because the cost of GEPs in\n    // vectorized code depends on whether the corresponding memory instruction\n    // is scalarized or not. Therefore, we handle GEPs with the memory\n    // instruction cost.\n    return 0;\n  case Instruction::Br: {\n    // In cases of scalarized and predicated instructions, there will be VF\n    // predicated blocks in the vectorized loop. Each branch around these\n    // blocks requires also an extract of its vector compare i1 element.\n    bool ScalarPredicatedBB = false;\n    BranchInst *BI = cast<BranchInst>(I);\n    if (VF.isVector() && BI->isConditional() &&\n        (PredicatedBBsAfterVectorization.count(BI->getSuccessor(0)) ||\n         PredicatedBBsAfterVectorization.count(BI->getSuccessor(1))))\n      ScalarPredicatedBB = true;\n\n    if (ScalarPredicatedBB) {\n      // Return cost for branches around scalarized and predicated blocks.\n      assert(!VF.isScalable() && \"scalable vectors not yet supported.\");\n      auto *Vec_i1Ty =\n          VectorType::get(IntegerType::getInt1Ty(RetTy->getContext()), VF);\n      return (TTI.getScalarizationOverhead(\n                  Vec_i1Ty, APInt::getAllOnesValue(VF.getKnownMinValue()),\n                  false, true) +\n              (TTI.getCFInstrCost(Instruction::Br, CostKind) *\n               VF.getKnownMinValue()));\n    } else if (I->getParent() == TheLoop->getLoopLatch() || VF.isScalar())\n      // The back-edge branch will remain, as will all scalar branches.\n      return TTI.getCFInstrCost(Instruction::Br, CostKind);\n    else\n      // This branch will be eliminated by if-conversion.\n      return 0;\n    // Note: We currently assume zero cost for an unconditional branch inside\n    // a predicated block since it will become a fall-through, although we\n    // may decide in the future to call TTI for all branches.\n  }\n  case Instruction::PHI: {\n    auto *Phi = cast<PHINode>(I);\n\n    // First-order recurrences are replaced by vector shuffles inside the loop.\n    // NOTE: Don't use ToVectorTy as SK_ExtractSubvector expects a vector type.\n    if (VF.isVector() && Legal->isFirstOrderRecurrence(Phi))\n      return TTI.getShuffleCost(\n          TargetTransformInfo::SK_ExtractSubvector, cast<VectorType>(VectorTy),\n          VF.getKnownMinValue() - 1, FixedVectorType::get(RetTy, 1));\n\n    // Phi nodes in non-header blocks (not inductions, reductions, etc.) are\n    // converted into select instructions. We require N - 1 selects per phi\n    // node, where N is the number of incoming values.\n    if (VF.isVector() && Phi->getParent() != TheLoop->getHeader())\n      return (Phi->getNumIncomingValues() - 1) *\n             TTI.getCmpSelInstrCost(\n                 Instruction::Select, ToVectorTy(Phi->getType(), VF),\n                 ToVectorTy(Type::getInt1Ty(Phi->getContext()), VF),\n                 CmpInst::BAD_ICMP_PREDICATE, CostKind);\n\n    return TTI.getCFInstrCost(Instruction::PHI, CostKind);\n  }\n  case Instruction::UDiv:\n  case Instruction::SDiv:\n  case Instruction::URem:\n  case Instruction::SRem:\n    // If we have a predicated instruction, it may not be executed for each\n    // vector lane. Get the scalarization cost and scale this amount by the\n    // probability of executing the predicated block. If the instruction is not\n    // predicated, we fall through to the next case.\n    if (VF.isVector() && isScalarWithPredication(I)) {\n      InstructionCost Cost = 0;\n\n      // These instructions have a non-void type, so account for the phi nodes\n      // that we will create. This cost is likely to be zero. The phi node\n      // cost, if any, should be scaled by the block probability because it\n      // models a copy at the end of each predicated block.\n      Cost += VF.getKnownMinValue() *\n              TTI.getCFInstrCost(Instruction::PHI, CostKind);\n\n      // The cost of the non-predicated instruction.\n      Cost += VF.getKnownMinValue() *\n              TTI.getArithmeticInstrCost(I->getOpcode(), RetTy, CostKind);\n\n      // The cost of insertelement and extractelement instructions needed for\n      // scalarization.\n      Cost += getScalarizationOverhead(I, VF);\n\n      // Scale the cost by the probability of executing the predicated blocks.\n      // This assumes the predicated block for each vector lane is equally\n      // likely.\n      return Cost / getReciprocalPredBlockProb();\n    }\n    LLVM_FALLTHROUGH;\n  case Instruction::Add:\n  case Instruction::FAdd:\n  case Instruction::Sub:\n  case Instruction::FSub:\n  case Instruction::Mul:\n  case Instruction::FMul:\n  case Instruction::FDiv:\n  case Instruction::FRem:\n  case Instruction::Shl:\n  case Instruction::LShr:\n  case Instruction::AShr:\n  case Instruction::And:\n  case Instruction::Or:\n  case Instruction::Xor: {\n    // Since we will replace the stride by 1 the multiplication should go away.\n    if (I->getOpcode() == Instruction::Mul && isStrideMul(I, Legal))\n      return 0;\n\n    // Detect reduction patterns\n    InstructionCost RedCost;\n    if ((RedCost = getReductionPatternCost(I, VF, VectorTy, CostKind))\n            .isValid())\n      return RedCost;\n\n    // Certain instructions can be cheaper to vectorize if they have a constant\n    // second vector operand. One example of this are shifts on x86.\n    Value *Op2 = I->getOperand(1);\n    TargetTransformInfo::OperandValueProperties Op2VP;\n    TargetTransformInfo::OperandValueKind Op2VK =\n        TTI.getOperandInfo(Op2, Op2VP);\n    if (Op2VK == TargetTransformInfo::OK_AnyValue && Legal->isUniform(Op2))\n      Op2VK = TargetTransformInfo::OK_UniformValue;\n\n    SmallVector<const Value *, 4> Operands(I->operand_values());\n    unsigned N = isScalarAfterVectorization(I, VF) ? VF.getKnownMinValue() : 1;\n    return N * TTI.getArithmeticInstrCost(\n                   I->getOpcode(), VectorTy, CostKind,\n                   TargetTransformInfo::OK_AnyValue,\n                   Op2VK, TargetTransformInfo::OP_None, Op2VP, Operands, I);\n  }\n  case Instruction::FNeg: {\n    assert(!VF.isScalable() && \"VF is assumed to be non scalable.\");\n    unsigned N = isScalarAfterVectorization(I, VF) ? VF.getKnownMinValue() : 1;\n    return N * TTI.getArithmeticInstrCost(\n                   I->getOpcode(), VectorTy, CostKind,\n                   TargetTransformInfo::OK_AnyValue,\n                   TargetTransformInfo::OK_AnyValue,\n                   TargetTransformInfo::OP_None, TargetTransformInfo::OP_None,\n                   I->getOperand(0), I);\n  }\n  case Instruction::Select: {\n    SelectInst *SI = cast<SelectInst>(I);\n    const SCEV *CondSCEV = SE->getSCEV(SI->getCondition());\n    bool ScalarCond = (SE->isLoopInvariant(CondSCEV, TheLoop));\n    Type *CondTy = SI->getCondition()->getType();\n    if (!ScalarCond)\n      CondTy = VectorType::get(CondTy, VF);\n    return TTI.getCmpSelInstrCost(I->getOpcode(), VectorTy, CondTy,\n                                  CmpInst::BAD_ICMP_PREDICATE, CostKind, I);\n  }\n  case Instruction::ICmp:\n  case Instruction::FCmp: {\n    Type *ValTy = I->getOperand(0)->getType();\n    Instruction *Op0AsInstruction = dyn_cast<Instruction>(I->getOperand(0));\n    if (canTruncateToMinimalBitwidth(Op0AsInstruction, VF))\n      ValTy = IntegerType::get(ValTy->getContext(), MinBWs[Op0AsInstruction]);\n    VectorTy = ToVectorTy(ValTy, VF);\n    return TTI.getCmpSelInstrCost(I->getOpcode(), VectorTy, nullptr,\n                                  CmpInst::BAD_ICMP_PREDICATE, CostKind, I);\n  }\n  case Instruction::Store:\n  case Instruction::Load: {\n    ElementCount Width = VF;\n    if (Width.isVector()) {\n      InstWidening Decision = getWideningDecision(I, Width);\n      assert(Decision != CM_Unknown &&\n             \"CM decision should be taken at this point\");\n      if (Decision == CM_Scalarize)\n        Width = ElementCount::getFixed(1);\n    }\n    VectorTy = ToVectorTy(getMemInstValueType(I), Width);\n    return getMemoryInstructionCost(I, VF);\n  }\n  case Instruction::ZExt:\n  case Instruction::SExt:\n  case Instruction::FPToUI:\n  case Instruction::FPToSI:\n  case Instruction::FPExt:\n  case Instruction::PtrToInt:\n  case Instruction::IntToPtr:\n  case Instruction::SIToFP:\n  case Instruction::UIToFP:\n  case Instruction::Trunc:\n  case Instruction::FPTrunc:\n  case Instruction::BitCast: {\n    // Computes the CastContextHint from a Load/Store instruction.\n    auto ComputeCCH = [&](Instruction *I) -> TTI::CastContextHint {\n      assert((isa<LoadInst>(I) || isa<StoreInst>(I)) &&\n             \"Expected a load or a store!\");\n\n      if (VF.isScalar() || !TheLoop->contains(I))\n        return TTI::CastContextHint::Normal;\n\n      switch (getWideningDecision(I, VF)) {\n      case LoopVectorizationCostModel::CM_GatherScatter:\n        return TTI::CastContextHint::GatherScatter;\n      case LoopVectorizationCostModel::CM_Interleave:\n        return TTI::CastContextHint::Interleave;\n      case LoopVectorizationCostModel::CM_Scalarize:\n      case LoopVectorizationCostModel::CM_Widen:\n        return Legal->isMaskRequired(I) ? TTI::CastContextHint::Masked\n                                        : TTI::CastContextHint::Normal;\n      case LoopVectorizationCostModel::CM_Widen_Reverse:\n        return TTI::CastContextHint::Reversed;\n      case LoopVectorizationCostModel::CM_Unknown:\n        llvm_unreachable(\"Instr did not go through cost modelling?\");\n      }\n\n      llvm_unreachable(\"Unhandled case!\");\n    };\n\n    unsigned Opcode = I->getOpcode();\n    TTI::CastContextHint CCH = TTI::CastContextHint::None;\n    // For Trunc, the context is the only user, which must be a StoreInst.\n    if (Opcode == Instruction::Trunc || Opcode == Instruction::FPTrunc) {\n      if (I->hasOneUse())\n        if (StoreInst *Store = dyn_cast<StoreInst>(*I->user_begin()))\n          CCH = ComputeCCH(Store);\n    }\n    // For Z/Sext, the context is the operand, which must be a LoadInst.\n    else if (Opcode == Instruction::ZExt || Opcode == Instruction::SExt ||\n             Opcode == Instruction::FPExt) {\n      if (LoadInst *Load = dyn_cast<LoadInst>(I->getOperand(0)))\n        CCH = ComputeCCH(Load);\n    }\n\n    // We optimize the truncation of induction variables having constant\n    // integer steps. The cost of these truncations is the same as the scalar\n    // operation.\n    if (isOptimizableIVTruncate(I, VF)) {\n      auto *Trunc = cast<TruncInst>(I);\n      return TTI.getCastInstrCost(Instruction::Trunc, Trunc->getDestTy(),\n                                  Trunc->getSrcTy(), CCH, CostKind, Trunc);\n    }\n\n    // Detect reduction patterns\n    InstructionCost RedCost;\n    if ((RedCost = getReductionPatternCost(I, VF, VectorTy, CostKind))\n            .isValid())\n      return RedCost;\n\n    Type *SrcScalarTy = I->getOperand(0)->getType();\n    Type *SrcVecTy =\n        VectorTy->isVectorTy() ? ToVectorTy(SrcScalarTy, VF) : SrcScalarTy;\n    if (canTruncateToMinimalBitwidth(I, VF)) {\n      // This cast is going to be shrunk. This may remove the cast or it might\n      // turn it into slightly different cast. For example, if MinBW == 16,\n      // \"zext i8 %1 to i32\" becomes \"zext i8 %1 to i16\".\n      //\n      // Calculate the modified src and dest types.\n      Type *MinVecTy = VectorTy;\n      if (Opcode == Instruction::Trunc) {\n        SrcVecTy = smallestIntegerVectorType(SrcVecTy, MinVecTy);\n        VectorTy =\n            largestIntegerVectorType(ToVectorTy(I->getType(), VF), MinVecTy);\n      } else if (Opcode == Instruction::ZExt || Opcode == Instruction::SExt) {\n        SrcVecTy = largestIntegerVectorType(SrcVecTy, MinVecTy);\n        VectorTy =\n            smallestIntegerVectorType(ToVectorTy(I->getType(), VF), MinVecTy);\n      }\n    }\n\n    unsigned N;\n    if (isScalarAfterVectorization(I, VF)) {\n      assert(!VF.isScalable() && \"VF is assumed to be non scalable\");\n      N = VF.getKnownMinValue();\n    } else\n      N = 1;\n    return N *\n           TTI.getCastInstrCost(Opcode, VectorTy, SrcVecTy, CCH, CostKind, I);\n  }\n  case Instruction::Call: {\n    bool NeedToScalarize;\n    CallInst *CI = cast<CallInst>(I);\n    InstructionCost CallCost = getVectorCallCost(CI, VF, NeedToScalarize);\n    if (getVectorIntrinsicIDForCall(CI, TLI)) {\n      InstructionCost IntrinsicCost = getVectorIntrinsicCost(CI, VF);\n      return std::min(CallCost, IntrinsicCost);\n    }\n    return CallCost;\n  }\n  case Instruction::ExtractValue:\n    return TTI.getInstructionCost(I, TTI::TCK_RecipThroughput);\n  default:\n    // The cost of executing VF copies of the scalar instruction. This opcode\n    // is unknown. Assume that it is the same as 'mul'.\n    return VF.getKnownMinValue() * TTI.getArithmeticInstrCost(\n                                       Instruction::Mul, VectorTy, CostKind) +\n           getScalarizationOverhead(I, VF);\n  } // end of switch.\n}\n\nchar LoopVectorize::ID = 0;\n\nstatic const char lv_name[] = \"Loop Vectorization\";\n\nINITIALIZE_PASS_BEGIN(LoopVectorize, LV_NAME, lv_name, false, false)\nINITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(BasicAAWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(AAResultsWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(GlobalsAAWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)\nINITIALIZE_PASS_DEPENDENCY(BlockFrequencyInfoWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(ScalarEvolutionWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(LoopAccessLegacyAnalysis)\nINITIALIZE_PASS_DEPENDENCY(DemandedBitsWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(OptimizationRemarkEmitterWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(ProfileSummaryInfoWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(InjectTLIMappingsLegacy)\nINITIALIZE_PASS_END(LoopVectorize, LV_NAME, lv_name, false, false)\n\nnamespace llvm {\n\nPass *createLoopVectorizePass() { return new LoopVectorize(); }\n\nPass *createLoopVectorizePass(bool InterleaveOnlyWhenForced,\n                              bool VectorizeOnlyWhenForced) {\n  return new LoopVectorize(InterleaveOnlyWhenForced, VectorizeOnlyWhenForced);\n}\n\n} // end namespace llvm\n\nbool LoopVectorizationCostModel::isConsecutiveLoadOrStore(Instruction *Inst) {\n  // Check if the pointer operand of a load or store instruction is\n  // consecutive.\n  if (auto *Ptr = getLoadStorePointerOperand(Inst))\n    return Legal->isConsecutivePtr(Ptr);\n  return false;\n}\n\nvoid LoopVectorizationCostModel::collectValuesToIgnore() {\n  // Ignore ephemeral values.\n  CodeMetrics::collectEphemeralValues(TheLoop, AC, ValuesToIgnore);\n\n  // Ignore type-promoting instructions we identified during reduction\n  // detection.\n  for (auto &Reduction : Legal->getReductionVars()) {\n    RecurrenceDescriptor &RedDes = Reduction.second;\n    const SmallPtrSetImpl<Instruction *> &Casts = RedDes.getCastInsts();\n    VecValuesToIgnore.insert(Casts.begin(), Casts.end());\n  }\n  // Ignore type-casting instructions we identified during induction\n  // detection.\n  for (auto &Induction : Legal->getInductionVars()) {\n    InductionDescriptor &IndDes = Induction.second;\n    const SmallVectorImpl<Instruction *> &Casts = IndDes.getCastInsts();\n    VecValuesToIgnore.insert(Casts.begin(), Casts.end());\n  }\n}\n\nvoid LoopVectorizationCostModel::collectInLoopReductions() {\n  for (auto &Reduction : Legal->getReductionVars()) {\n    PHINode *Phi = Reduction.first;\n    RecurrenceDescriptor &RdxDesc = Reduction.second;\n\n    // We don't collect reductions that are type promoted (yet).\n    if (RdxDesc.getRecurrenceType() != Phi->getType())\n      continue;\n\n    // If the target would prefer this reduction to happen \"in-loop\", then we\n    // want to record it as such.\n    unsigned Opcode = RdxDesc.getOpcode();\n    if (!PreferInLoopReductions &&\n        !TTI.preferInLoopReduction(Opcode, Phi->getType(),\n                                   TargetTransformInfo::ReductionFlags()))\n      continue;\n\n    // Check that we can correctly put the reductions into the loop, by\n    // finding the chain of operations that leads from the phi to the loop\n    // exit value.\n    SmallVector<Instruction *, 4> ReductionOperations =\n        RdxDesc.getReductionOpChain(Phi, TheLoop);\n    bool InLoop = !ReductionOperations.empty();\n    if (InLoop) {\n      InLoopReductionChains[Phi] = ReductionOperations;\n      // Add the elements to InLoopReductionImmediateChains for cost modelling.\n      Instruction *LastChain = Phi;\n      for (auto *I : ReductionOperations) {\n        InLoopReductionImmediateChains[I] = LastChain;\n        LastChain = I;\n      }\n    }\n    LLVM_DEBUG(dbgs() << \"LV: Using \" << (InLoop ? \"inloop\" : \"out of loop\")\n                      << \" reduction for phi: \" << *Phi << \"\\n\");\n  }\n}\n\n// TODO: we could return a pair of values that specify the max VF and\n// min VF, to be used in `buildVPlans(MinVF, MaxVF)` instead of\n// `buildVPlans(VF, VF)`. We cannot do it because VPLAN at the moment\n// doesn't have a cost model that can choose which plan to execute if\n// more than one is generated.\nstatic unsigned determineVPlanVF(const unsigned WidestVectorRegBits,\n                                 LoopVectorizationCostModel &CM) {\n  unsigned WidestType;\n  std::tie(std::ignore, WidestType) = CM.getSmallestAndWidestTypes();\n  return WidestVectorRegBits / WidestType;\n}\n\nVectorizationFactor\nLoopVectorizationPlanner::planInVPlanNativePath(ElementCount UserVF) {\n  assert(!UserVF.isScalable() && \"scalable vectors not yet supported\");\n  ElementCount VF = UserVF;\n  // Outer loop handling: They may require CFG and instruction level\n  // transformations before even evaluating whether vectorization is profitable.\n  // Since we cannot modify the incoming IR, we need to build VPlan upfront in\n  // the vectorization pipeline.\n  if (!OrigLoop->isInnermost()) {\n    // If the user doesn't provide a vectorization factor, determine a\n    // reasonable one.\n    if (UserVF.isZero()) {\n      VF = ElementCount::getFixed(\n          determineVPlanVF(TTI->getRegisterBitWidth(true /* Vector*/), CM));\n      LLVM_DEBUG(dbgs() << \"LV: VPlan computed VF \" << VF << \".\\n\");\n\n      // Make sure we have a VF > 1 for stress testing.\n      if (VPlanBuildStressTest && (VF.isScalar() || VF.isZero())) {\n        LLVM_DEBUG(dbgs() << \"LV: VPlan stress testing: \"\n                          << \"overriding computed VF.\\n\");\n        VF = ElementCount::getFixed(4);\n      }\n    }\n    assert(EnableVPlanNativePath && \"VPlan-native path is not enabled.\");\n    assert(isPowerOf2_32(VF.getKnownMinValue()) &&\n           \"VF needs to be a power of two\");\n    LLVM_DEBUG(dbgs() << \"LV: Using \" << (!UserVF.isZero() ? \"user \" : \"\")\n                      << \"VF \" << VF << \" to build VPlans.\\n\");\n    buildVPlans(VF, VF);\n\n    // For VPlan build stress testing, we bail out after VPlan construction.\n    if (VPlanBuildStressTest)\n      return VectorizationFactor::Disabled();\n\n    return {VF, 0 /*Cost*/};\n  }\n\n  LLVM_DEBUG(\n      dbgs() << \"LV: Not vectorizing. Inner loops aren't supported in the \"\n                \"VPlan-native path.\\n\");\n  return VectorizationFactor::Disabled();\n}\n\nOptional<VectorizationFactor>\nLoopVectorizationPlanner::plan(ElementCount UserVF, unsigned UserIC) {\n  assert(OrigLoop->isInnermost() && \"Inner loop expected.\");\n  Optional<ElementCount> MaybeMaxVF = CM.computeMaxVF(UserVF, UserIC);\n  if (!MaybeMaxVF) // Cases that should not to be vectorized nor interleaved.\n    return None;\n\n  // Invalidate interleave groups if all blocks of loop will be predicated.\n  if (CM.blockNeedsPredication(OrigLoop->getHeader()) &&\n      !useMaskedInterleavedAccesses(*TTI)) {\n    LLVM_DEBUG(\n        dbgs()\n        << \"LV: Invalidate all interleaved groups due to fold-tail by masking \"\n           \"which requires masked-interleaved support.\\n\");\n    if (CM.InterleaveInfo.invalidateGroups())\n      // Invalidating interleave groups also requires invalidating all decisions\n      // based on them, which includes widening decisions and uniform and scalar\n      // values.\n      CM.invalidateCostModelingDecisions();\n  }\n\n  ElementCount MaxVF = MaybeMaxVF.getValue();\n  assert(MaxVF.isNonZero() && \"MaxVF is zero.\");\n\n  bool UserVFIsLegal = ElementCount::isKnownLE(UserVF, MaxVF);\n  if (!UserVF.isZero() &&\n      (UserVFIsLegal || (UserVF.isScalable() && MaxVF.isScalable()))) {\n    // FIXME: MaxVF is temporarily used inplace of UserVF for illegal scalable\n    // VFs here, this should be reverted to only use legal UserVFs once the\n    // loop below supports scalable VFs.\n    ElementCount VF = UserVFIsLegal ? UserVF : MaxVF;\n    LLVM_DEBUG(dbgs() << \"LV: Using \" << (UserVFIsLegal ? \"user\" : \"max\")\n                      << \" VF \" << VF << \".\\n\");\n    assert(isPowerOf2_32(VF.getKnownMinValue()) &&\n           \"VF needs to be a power of two\");\n    // Collect the instructions (and their associated costs) that will be more\n    // profitable to scalarize.\n    CM.selectUserVectorizationFactor(VF);\n    CM.collectInLoopReductions();\n    buildVPlansWithVPRecipes(VF, VF);\n    LLVM_DEBUG(printPlans(dbgs()));\n    return {{VF, 0}};\n  }\n\n  assert(!MaxVF.isScalable() &&\n         \"Scalable vectors not yet supported beyond this point\");\n\n  for (ElementCount VF = ElementCount::getFixed(1);\n       ElementCount::isKnownLE(VF, MaxVF); VF *= 2) {\n    // Collect Uniform and Scalar instructions after vectorization with VF.\n    CM.collectUniformsAndScalars(VF);\n\n    // Collect the instructions (and their associated costs) that will be more\n    // profitable to scalarize.\n    if (VF.isVector())\n      CM.collectInstsToScalarize(VF);\n  }\n\n  CM.collectInLoopReductions();\n\n  buildVPlansWithVPRecipes(ElementCount::getFixed(1), MaxVF);\n  LLVM_DEBUG(printPlans(dbgs()));\n  if (MaxVF.isScalar())\n    return VectorizationFactor::Disabled();\n\n  // Select the optimal vectorization factor.\n  return CM.selectVectorizationFactor(MaxVF);\n}\n\nvoid LoopVectorizationPlanner::setBestPlan(ElementCount VF, unsigned UF) {\n  LLVM_DEBUG(dbgs() << \"Setting best plan to VF=\" << VF << \", UF=\" << UF\n                    << '\\n');\n  BestVF = VF;\n  BestUF = UF;\n\n  erase_if(VPlans, [VF](const VPlanPtr &Plan) {\n    return !Plan->hasVF(VF);\n  });\n  assert(VPlans.size() == 1 && \"Best VF has not a single VPlan.\");\n}\n\nvoid LoopVectorizationPlanner::executePlan(InnerLoopVectorizer &ILV,\n                                           DominatorTree *DT) {\n  // Perform the actual loop transformation.\n\n  // 1. Create a new empty loop. Unlink the old loop and connect the new one.\n  assert(BestVF.hasValue() && \"Vectorization Factor is missing\");\n  assert(VPlans.size() == 1 && \"Not a single VPlan to execute.\");\n\n  VPTransformState State{\n      *BestVF, BestUF, LI, DT, ILV.Builder, &ILV, VPlans.front().get()};\n  State.CFG.PrevBB = ILV.createVectorizedLoopSkeleton();\n  State.TripCount = ILV.getOrCreateTripCount(nullptr);\n  State.CanonicalIV = ILV.Induction;\n\n  ILV.printDebugTracesAtStart();\n\n  //===------------------------------------------------===//\n  //\n  // Notice: any optimization or new instruction that go\n  // into the code below should also be implemented in\n  // the cost-model.\n  //\n  //===------------------------------------------------===//\n\n  // 2. Copy and widen instructions from the old loop into the new loop.\n  VPlans.front()->execute(&State);\n\n  // 3. Fix the vectorized code: take care of header phi's, live-outs,\n  //    predication, updating analyses.\n  ILV.fixVectorizedLoop(State);\n\n  ILV.printDebugTracesAtEnd();\n}\n\nvoid LoopVectorizationPlanner::collectTriviallyDeadInstructions(\n    SmallPtrSetImpl<Instruction *> &DeadInstructions) {\n\n  // We create new control-flow for the vectorized loop, so the original exit\n  // conditions will be dead after vectorization if it's only used by the\n  // terminator\n  SmallVector<BasicBlock*> ExitingBlocks;\n  OrigLoop->getExitingBlocks(ExitingBlocks);\n  for (auto *BB : ExitingBlocks) {\n    auto *Cmp = dyn_cast<Instruction>(BB->getTerminator()->getOperand(0));\n    if (!Cmp || !Cmp->hasOneUse())\n      continue;\n\n    // TODO: we should introduce a getUniqueExitingBlocks on Loop\n    if (!DeadInstructions.insert(Cmp).second)\n      continue;\n\n    // The operands of the icmp is often a dead trunc, used by IndUpdate.\n    // TODO: can recurse through operands in general\n    for (Value *Op : Cmp->operands()) {\n      if (isa<TruncInst>(Op) && Op->hasOneUse())\n          DeadInstructions.insert(cast<Instruction>(Op));\n    }\n  }\n\n  // We create new \"steps\" for induction variable updates to which the original\n  // induction variables map. An original update instruction will be dead if\n  // all its users except the induction variable are dead.\n  auto *Latch = OrigLoop->getLoopLatch();\n  for (auto &Induction : Legal->getInductionVars()) {\n    PHINode *Ind = Induction.first;\n    auto *IndUpdate = cast<Instruction>(Ind->getIncomingValueForBlock(Latch));\n\n    // If the tail is to be folded by masking, the primary induction variable,\n    // if exists, isn't dead: it will be used for masking. Don't kill it.\n    if (CM.foldTailByMasking() && IndUpdate == Legal->getPrimaryInduction())\n      continue;\n\n    if (llvm::all_of(IndUpdate->users(), [&](User *U) -> bool {\n          return U == Ind || DeadInstructions.count(cast<Instruction>(U));\n        }))\n      DeadInstructions.insert(IndUpdate);\n\n    // We record as \"Dead\" also the type-casting instructions we had identified\n    // during induction analysis. We don't need any handling for them in the\n    // vectorized loop because we have proven that, under a proper runtime\n    // test guarding the vectorized loop, the value of the phi, and the casted\n    // value of the phi, are the same. The last instruction in this casting chain\n    // will get its scalar/vector/widened def from the scalar/vector/widened def\n    // of the respective phi node. Any other casts in the induction def-use chain\n    // have no other uses outside the phi update chain, and will be ignored.\n    InductionDescriptor &IndDes = Induction.second;\n    const SmallVectorImpl<Instruction *> &Casts = IndDes.getCastInsts();\n    DeadInstructions.insert(Casts.begin(), Casts.end());\n  }\n}\n\nValue *InnerLoopUnroller::reverseVector(Value *Vec) { return Vec; }\n\nValue *InnerLoopUnroller::getBroadcastInstrs(Value *V) { return V; }\n\nValue *InnerLoopUnroller::getStepVector(Value *Val, int StartIdx, Value *Step,\n                                        Instruction::BinaryOps BinOp) {\n  // When unrolling and the VF is 1, we only need to add a simple scalar.\n  Type *Ty = Val->getType();\n  assert(!Ty->isVectorTy() && \"Val must be a scalar\");\n\n  if (Ty->isFloatingPointTy()) {\n    Constant *C = ConstantFP::get(Ty, (double)StartIdx);\n\n    // Floating-point operations inherit FMF via the builder's flags.\n    Value *MulOp = Builder.CreateFMul(C, Step);\n    return Builder.CreateBinOp(BinOp, Val, MulOp);\n  }\n  Constant *C = ConstantInt::get(Ty, StartIdx);\n  return Builder.CreateAdd(Val, Builder.CreateMul(C, Step), \"induction\");\n}\n\nstatic void AddRuntimeUnrollDisableMetaData(Loop *L) {\n  SmallVector<Metadata *, 4> MDs;\n  // Reserve first location for self reference to the LoopID metadata node.\n  MDs.push_back(nullptr);\n  bool IsUnrollMetadata = false;\n  MDNode *LoopID = L->getLoopID();\n  if (LoopID) {\n    // First find existing loop unrolling disable metadata.\n    for (unsigned i = 1, ie = LoopID->getNumOperands(); i < ie; ++i) {\n      auto *MD = dyn_cast<MDNode>(LoopID->getOperand(i));\n      if (MD) {\n        const auto *S = dyn_cast<MDString>(MD->getOperand(0));\n        IsUnrollMetadata =\n            S && S->getString().startswith(\"llvm.loop.unroll.disable\");\n      }\n      MDs.push_back(LoopID->getOperand(i));\n    }\n  }\n\n  if (!IsUnrollMetadata) {\n    // Add runtime unroll disable metadata.\n    LLVMContext &Context = L->getHeader()->getContext();\n    SmallVector<Metadata *, 1> DisableOperands;\n    DisableOperands.push_back(\n        MDString::get(Context, \"llvm.loop.unroll.runtime.disable\"));\n    MDNode *DisableNode = MDNode::get(Context, DisableOperands);\n    MDs.push_back(DisableNode);\n    MDNode *NewLoopID = MDNode::get(Context, MDs);\n    // Set operand 0 to refer to the loop id itself.\n    NewLoopID->replaceOperandWith(0, NewLoopID);\n    L->setLoopID(NewLoopID);\n  }\n}\n\n//===--------------------------------------------------------------------===//\n// EpilogueVectorizerMainLoop\n//===--------------------------------------------------------------------===//\n\n/// This function is partially responsible for generating the control flow\n/// depicted in https://llvm.org/docs/Vectorizers.html#epilogue-vectorization.\nBasicBlock *EpilogueVectorizerMainLoop::createEpilogueVectorizedLoopSkeleton() {\n  MDNode *OrigLoopID = OrigLoop->getLoopID();\n  Loop *Lp = createVectorLoopSkeleton(\"\");\n\n  // Generate the code to check the minimum iteration count of the vector\n  // epilogue (see below).\n  EPI.EpilogueIterationCountCheck =\n      emitMinimumIterationCountCheck(Lp, LoopScalarPreHeader, true);\n  EPI.EpilogueIterationCountCheck->setName(\"iter.check\");\n\n  // Generate the code to check any assumptions that we've made for SCEV\n  // expressions.\n  EPI.SCEVSafetyCheck = emitSCEVChecks(Lp, LoopScalarPreHeader);\n\n  // Generate the code that checks at runtime if arrays overlap. We put the\n  // checks into a separate block to make the more common case of few elements\n  // faster.\n  EPI.MemSafetyCheck = emitMemRuntimeChecks(Lp, LoopScalarPreHeader);\n\n  // Generate the iteration count check for the main loop, *after* the check\n  // for the epilogue loop, so that the path-length is shorter for the case\n  // that goes directly through the vector epilogue. The longer-path length for\n  // the main loop is compensated for, by the gain from vectorizing the larger\n  // trip count. Note: the branch will get updated later on when we vectorize\n  // the epilogue.\n  EPI.MainLoopIterationCountCheck =\n      emitMinimumIterationCountCheck(Lp, LoopScalarPreHeader, false);\n\n  // Generate the induction variable.\n  OldInduction = Legal->getPrimaryInduction();\n  Type *IdxTy = Legal->getWidestInductionType();\n  Value *StartIdx = ConstantInt::get(IdxTy, 0);\n  Constant *Step = ConstantInt::get(IdxTy, VF.getKnownMinValue() * UF);\n  Value *CountRoundDown = getOrCreateVectorTripCount(Lp);\n  EPI.VectorTripCount = CountRoundDown;\n  Induction =\n      createInductionVariable(Lp, StartIdx, CountRoundDown, Step,\n                              getDebugLocFromInstOrOperands(OldInduction));\n\n  // Skip induction resume value creation here because they will be created in\n  // the second pass. If we created them here, they wouldn't be used anyway,\n  // because the vplan in the second pass still contains the inductions from the\n  // original loop.\n\n  return completeLoopSkeleton(Lp, OrigLoopID);\n}\n\nvoid EpilogueVectorizerMainLoop::printDebugTracesAtStart() {\n  LLVM_DEBUG({\n    dbgs() << \"Create Skeleton for epilogue vectorized loop (first pass)\\n\"\n           << \"Main Loop VF:\" << EPI.MainLoopVF.getKnownMinValue()\n           << \", Main Loop UF:\" << EPI.MainLoopUF\n           << \", Epilogue Loop VF:\" << EPI.EpilogueVF.getKnownMinValue()\n           << \", Epilogue Loop UF:\" << EPI.EpilogueUF << \"\\n\";\n  });\n}\n\nvoid EpilogueVectorizerMainLoop::printDebugTracesAtEnd() {\n  DEBUG_WITH_TYPE(VerboseDebug, {\n    dbgs() << \"intermediate fn:\\n\" << *Induction->getFunction() << \"\\n\";\n  });\n}\n\nBasicBlock *EpilogueVectorizerMainLoop::emitMinimumIterationCountCheck(\n    Loop *L, BasicBlock *Bypass, bool ForEpilogue) {\n  assert(L && \"Expected valid Loop.\");\n  assert(Bypass && \"Expected valid bypass basic block.\");\n  unsigned VFactor =\n      ForEpilogue ? EPI.EpilogueVF.getKnownMinValue() : VF.getKnownMinValue();\n  unsigned UFactor = ForEpilogue ? EPI.EpilogueUF : UF;\n  Value *Count = getOrCreateTripCount(L);\n  // Reuse existing vector loop preheader for TC checks.\n  // Note that new preheader block is generated for vector loop.\n  BasicBlock *const TCCheckBlock = LoopVectorPreHeader;\n  IRBuilder<> Builder(TCCheckBlock->getTerminator());\n\n  // Generate code to check if the loop's trip count is less than VF * UF of the\n  // main vector loop.\n  auto P =\n      Cost->requiresScalarEpilogue() ? ICmpInst::ICMP_ULE : ICmpInst::ICMP_ULT;\n\n  Value *CheckMinIters = Builder.CreateICmp(\n      P, Count, ConstantInt::get(Count->getType(), VFactor * UFactor),\n      \"min.iters.check\");\n\n  if (!ForEpilogue)\n    TCCheckBlock->setName(\"vector.main.loop.iter.check\");\n\n  // Create new preheader for vector loop.\n  LoopVectorPreHeader = SplitBlock(TCCheckBlock, TCCheckBlock->getTerminator(),\n                                   DT, LI, nullptr, \"vector.ph\");\n\n  if (ForEpilogue) {\n    assert(DT->properlyDominates(DT->getNode(TCCheckBlock),\n                                 DT->getNode(Bypass)->getIDom()) &&\n           \"TC check is expected to dominate Bypass\");\n\n    // Update dominator for Bypass & LoopExit.\n    DT->changeImmediateDominator(Bypass, TCCheckBlock);\n    DT->changeImmediateDominator(LoopExitBlock, TCCheckBlock);\n\n    LoopBypassBlocks.push_back(TCCheckBlock);\n\n    // Save the trip count so we don't have to regenerate it in the\n    // vec.epilog.iter.check. This is safe to do because the trip count\n    // generated here dominates the vector epilog iter check.\n    EPI.TripCount = Count;\n  }\n\n  ReplaceInstWithInst(\n      TCCheckBlock->getTerminator(),\n      BranchInst::Create(Bypass, LoopVectorPreHeader, CheckMinIters));\n\n  return TCCheckBlock;\n}\n\n//===--------------------------------------------------------------------===//\n// EpilogueVectorizerEpilogueLoop\n//===--------------------------------------------------------------------===//\n\n/// This function is partially responsible for generating the control flow\n/// depicted in https://llvm.org/docs/Vectorizers.html#epilogue-vectorization.\nBasicBlock *\nEpilogueVectorizerEpilogueLoop::createEpilogueVectorizedLoopSkeleton() {\n  MDNode *OrigLoopID = OrigLoop->getLoopID();\n  Loop *Lp = createVectorLoopSkeleton(\"vec.epilog.\");\n\n  // Now, compare the remaining count and if there aren't enough iterations to\n  // execute the vectorized epilogue skip to the scalar part.\n  BasicBlock *VecEpilogueIterationCountCheck = LoopVectorPreHeader;\n  VecEpilogueIterationCountCheck->setName(\"vec.epilog.iter.check\");\n  LoopVectorPreHeader =\n      SplitBlock(LoopVectorPreHeader, LoopVectorPreHeader->getTerminator(), DT,\n                 LI, nullptr, \"vec.epilog.ph\");\n  emitMinimumVectorEpilogueIterCountCheck(Lp, LoopScalarPreHeader,\n                                          VecEpilogueIterationCountCheck);\n\n  // Adjust the control flow taking the state info from the main loop\n  // vectorization into account.\n  assert(EPI.MainLoopIterationCountCheck && EPI.EpilogueIterationCountCheck &&\n         \"expected this to be saved from the previous pass.\");\n  EPI.MainLoopIterationCountCheck->getTerminator()->replaceUsesOfWith(\n      VecEpilogueIterationCountCheck, LoopVectorPreHeader);\n\n  DT->changeImmediateDominator(LoopVectorPreHeader,\n                               EPI.MainLoopIterationCountCheck);\n\n  EPI.EpilogueIterationCountCheck->getTerminator()->replaceUsesOfWith(\n      VecEpilogueIterationCountCheck, LoopScalarPreHeader);\n\n  if (EPI.SCEVSafetyCheck)\n    EPI.SCEVSafetyCheck->getTerminator()->replaceUsesOfWith(\n        VecEpilogueIterationCountCheck, LoopScalarPreHeader);\n  if (EPI.MemSafetyCheck)\n    EPI.MemSafetyCheck->getTerminator()->replaceUsesOfWith(\n        VecEpilogueIterationCountCheck, LoopScalarPreHeader);\n\n  DT->changeImmediateDominator(\n      VecEpilogueIterationCountCheck,\n      VecEpilogueIterationCountCheck->getSinglePredecessor());\n\n  DT->changeImmediateDominator(LoopScalarPreHeader,\n                               EPI.EpilogueIterationCountCheck);\n  DT->changeImmediateDominator(LoopExitBlock, EPI.EpilogueIterationCountCheck);\n\n  // Keep track of bypass blocks, as they feed start values to the induction\n  // phis in the scalar loop preheader.\n  if (EPI.SCEVSafetyCheck)\n    LoopBypassBlocks.push_back(EPI.SCEVSafetyCheck);\n  if (EPI.MemSafetyCheck)\n    LoopBypassBlocks.push_back(EPI.MemSafetyCheck);\n  LoopBypassBlocks.push_back(EPI.EpilogueIterationCountCheck);\n\n  // Generate a resume induction for the vector epilogue and put it in the\n  // vector epilogue preheader\n  Type *IdxTy = Legal->getWidestInductionType();\n  PHINode *EPResumeVal = PHINode::Create(IdxTy, 2, \"vec.epilog.resume.val\",\n                                         LoopVectorPreHeader->getFirstNonPHI());\n  EPResumeVal->addIncoming(EPI.VectorTripCount, VecEpilogueIterationCountCheck);\n  EPResumeVal->addIncoming(ConstantInt::get(IdxTy, 0),\n                           EPI.MainLoopIterationCountCheck);\n\n  // Generate the induction variable.\n  OldInduction = Legal->getPrimaryInduction();\n  Value *CountRoundDown = getOrCreateVectorTripCount(Lp);\n  Constant *Step = ConstantInt::get(IdxTy, VF.getKnownMinValue() * UF);\n  Value *StartIdx = EPResumeVal;\n  Induction =\n      createInductionVariable(Lp, StartIdx, CountRoundDown, Step,\n                              getDebugLocFromInstOrOperands(OldInduction));\n\n  // Generate induction resume values. These variables save the new starting\n  // indexes for the scalar loop. They are used to test if there are any tail\n  // iterations left once the vector loop has completed.\n  // Note that when the vectorized epilogue is skipped due to iteration count\n  // check, then the resume value for the induction variable comes from\n  // the trip count of the main vector loop, hence passing the AdditionalBypass\n  // argument.\n  createInductionResumeValues(Lp, CountRoundDown,\n                              {VecEpilogueIterationCountCheck,\n                               EPI.VectorTripCount} /* AdditionalBypass */);\n\n  AddRuntimeUnrollDisableMetaData(Lp);\n  return completeLoopSkeleton(Lp, OrigLoopID);\n}\n\nBasicBlock *\nEpilogueVectorizerEpilogueLoop::emitMinimumVectorEpilogueIterCountCheck(\n    Loop *L, BasicBlock *Bypass, BasicBlock *Insert) {\n\n  assert(EPI.TripCount &&\n         \"Expected trip count to have been safed in the first pass.\");\n  assert(\n      (!isa<Instruction>(EPI.TripCount) ||\n       DT->dominates(cast<Instruction>(EPI.TripCount)->getParent(), Insert)) &&\n      \"saved trip count does not dominate insertion point.\");\n  Value *TC = EPI.TripCount;\n  IRBuilder<> Builder(Insert->getTerminator());\n  Value *Count = Builder.CreateSub(TC, EPI.VectorTripCount, \"n.vec.remaining\");\n\n  // Generate code to check if the loop's trip count is less than VF * UF of the\n  // vector epilogue loop.\n  auto P =\n      Cost->requiresScalarEpilogue() ? ICmpInst::ICMP_ULE : ICmpInst::ICMP_ULT;\n\n  Value *CheckMinIters = Builder.CreateICmp(\n      P, Count,\n      ConstantInt::get(Count->getType(),\n                       EPI.EpilogueVF.getKnownMinValue() * EPI.EpilogueUF),\n      \"min.epilog.iters.check\");\n\n  ReplaceInstWithInst(\n      Insert->getTerminator(),\n      BranchInst::Create(Bypass, LoopVectorPreHeader, CheckMinIters));\n\n  LoopBypassBlocks.push_back(Insert);\n  return Insert;\n}\n\nvoid EpilogueVectorizerEpilogueLoop::printDebugTracesAtStart() {\n  LLVM_DEBUG({\n    dbgs() << \"Create Skeleton for epilogue vectorized loop (second pass)\\n\"\n           << \"Main Loop VF:\" << EPI.MainLoopVF.getKnownMinValue()\n           << \", Main Loop UF:\" << EPI.MainLoopUF\n           << \", Epilogue Loop VF:\" << EPI.EpilogueVF.getKnownMinValue()\n           << \", Epilogue Loop UF:\" << EPI.EpilogueUF << \"\\n\";\n  });\n}\n\nvoid EpilogueVectorizerEpilogueLoop::printDebugTracesAtEnd() {\n  DEBUG_WITH_TYPE(VerboseDebug, {\n    dbgs() << \"final fn:\\n\" << *Induction->getFunction() << \"\\n\";\n  });\n}\n\nbool LoopVectorizationPlanner::getDecisionAndClampRange(\n    const std::function<bool(ElementCount)> &Predicate, VFRange &Range) {\n  assert(!Range.isEmpty() && \"Trying to test an empty VF range.\");\n  bool PredicateAtRangeStart = Predicate(Range.Start);\n\n  for (ElementCount TmpVF = Range.Start * 2;\n       ElementCount::isKnownLT(TmpVF, Range.End); TmpVF *= 2)\n    if (Predicate(TmpVF) != PredicateAtRangeStart) {\n      Range.End = TmpVF;\n      break;\n    }\n\n  return PredicateAtRangeStart;\n}\n\n/// Build VPlans for the full range of feasible VF's = {\\p MinVF, 2 * \\p MinVF,\n/// 4 * \\p MinVF, ..., \\p MaxVF} by repeatedly building a VPlan for a sub-range\n/// of VF's starting at a given VF and extending it as much as possible. Each\n/// vectorization decision can potentially shorten this sub-range during\n/// buildVPlan().\nvoid LoopVectorizationPlanner::buildVPlans(ElementCount MinVF,\n                                           ElementCount MaxVF) {\n  auto MaxVFPlusOne = MaxVF.getWithIncrement(1);\n  for (ElementCount VF = MinVF; ElementCount::isKnownLT(VF, MaxVFPlusOne);) {\n    VFRange SubRange = {VF, MaxVFPlusOne};\n    VPlans.push_back(buildVPlan(SubRange));\n    VF = SubRange.End;\n  }\n}\n\nVPValue *VPRecipeBuilder::createEdgeMask(BasicBlock *Src, BasicBlock *Dst,\n                                         VPlanPtr &Plan) {\n  assert(is_contained(predecessors(Dst), Src) && \"Invalid edge\");\n\n  // Look for cached value.\n  std::pair<BasicBlock *, BasicBlock *> Edge(Src, Dst);\n  EdgeMaskCacheTy::iterator ECEntryIt = EdgeMaskCache.find(Edge);\n  if (ECEntryIt != EdgeMaskCache.end())\n    return ECEntryIt->second;\n\n  VPValue *SrcMask = createBlockInMask(Src, Plan);\n\n  // The terminator has to be a branch inst!\n  BranchInst *BI = dyn_cast<BranchInst>(Src->getTerminator());\n  assert(BI && \"Unexpected terminator found\");\n\n  if (!BI->isConditional() || BI->getSuccessor(0) == BI->getSuccessor(1))\n    return EdgeMaskCache[Edge] = SrcMask;\n\n  // If source is an exiting block, we know the exit edge is dynamically dead\n  // in the vector loop, and thus we don't need to restrict the mask.  Avoid\n  // adding uses of an otherwise potentially dead instruction.\n  if (OrigLoop->isLoopExiting(Src))\n    return EdgeMaskCache[Edge] = SrcMask;\n\n  VPValue *EdgeMask = Plan->getOrAddVPValue(BI->getCondition());\n  assert(EdgeMask && \"No Edge Mask found for condition\");\n\n  if (BI->getSuccessor(0) != Dst)\n    EdgeMask = Builder.createNot(EdgeMask);\n\n  if (SrcMask) { // Otherwise block in-mask is all-one, no need to AND.\n    // The condition is 'SrcMask && EdgeMask', which is equivalent to\n    // 'select i1 SrcMask, i1 EdgeMask, i1 false'.\n    // The select version does not introduce new UB if SrcMask is false and\n    // EdgeMask is poison. Using 'and' here introduces undefined behavior.\n    VPValue *False = Plan->getOrAddVPValue(\n        ConstantInt::getFalse(BI->getCondition()->getType()));\n    EdgeMask = Builder.createSelect(SrcMask, EdgeMask, False);\n  }\n\n  return EdgeMaskCache[Edge] = EdgeMask;\n}\n\nVPValue *VPRecipeBuilder::createBlockInMask(BasicBlock *BB, VPlanPtr &Plan) {\n  assert(OrigLoop->contains(BB) && \"Block is not a part of a loop\");\n\n  // Look for cached value.\n  BlockMaskCacheTy::iterator BCEntryIt = BlockMaskCache.find(BB);\n  if (BCEntryIt != BlockMaskCache.end())\n    return BCEntryIt->second;\n\n  // All-one mask is modelled as no-mask following the convention for masked\n  // load/store/gather/scatter. Initialize BlockMask to no-mask.\n  VPValue *BlockMask = nullptr;\n\n  if (OrigLoop->getHeader() == BB) {\n    if (!CM.blockNeedsPredication(BB))\n      return BlockMaskCache[BB] = BlockMask; // Loop incoming mask is all-one.\n\n    // Create the block in mask as the first non-phi instruction in the block.\n    VPBuilder::InsertPointGuard Guard(Builder);\n    auto NewInsertionPoint = Builder.getInsertBlock()->getFirstNonPhi();\n    Builder.setInsertPoint(Builder.getInsertBlock(), NewInsertionPoint);\n\n    // Introduce the early-exit compare IV <= BTC to form header block mask.\n    // This is used instead of IV < TC because TC may wrap, unlike BTC.\n    // Start by constructing the desired canonical IV.\n    VPValue *IV = nullptr;\n    if (Legal->getPrimaryInduction())\n      IV = Plan->getOrAddVPValue(Legal->getPrimaryInduction());\n    else {\n      auto IVRecipe = new VPWidenCanonicalIVRecipe();\n      Builder.getInsertBlock()->insert(IVRecipe, NewInsertionPoint);\n      IV = IVRecipe->getVPValue();\n    }\n    VPValue *BTC = Plan->getOrCreateBackedgeTakenCount();\n    bool TailFolded = !CM.isScalarEpilogueAllowed();\n\n    if (TailFolded && CM.TTI.emitGetActiveLaneMask()) {\n      // While ActiveLaneMask is a binary op that consumes the loop tripcount\n      // as a second argument, we only pass the IV here and extract the\n      // tripcount from the transform state where codegen of the VP instructions\n      // happen.\n      BlockMask = Builder.createNaryOp(VPInstruction::ActiveLaneMask, {IV});\n    } else {\n      BlockMask = Builder.createNaryOp(VPInstruction::ICmpULE, {IV, BTC});\n    }\n    return BlockMaskCache[BB] = BlockMask;\n  }\n\n  // This is the block mask. We OR all incoming edges.\n  for (auto *Predecessor : predecessors(BB)) {\n    VPValue *EdgeMask = createEdgeMask(Predecessor, BB, Plan);\n    if (!EdgeMask) // Mask of predecessor is all-one so mask of block is too.\n      return BlockMaskCache[BB] = EdgeMask;\n\n    if (!BlockMask) { // BlockMask has its initialized nullptr value.\n      BlockMask = EdgeMask;\n      continue;\n    }\n\n    BlockMask = Builder.createOr(BlockMask, EdgeMask);\n  }\n\n  return BlockMaskCache[BB] = BlockMask;\n}\n\nVPRecipeBase *VPRecipeBuilder::tryToWidenMemory(Instruction *I, VFRange &Range,\n                                                VPlanPtr &Plan) {\n  assert((isa<LoadInst>(I) || isa<StoreInst>(I)) &&\n         \"Must be called with either a load or store\");\n\n  auto willWiden = [&](ElementCount VF) -> bool {\n    if (VF.isScalar())\n      return false;\n    LoopVectorizationCostModel::InstWidening Decision =\n        CM.getWideningDecision(I, VF);\n    assert(Decision != LoopVectorizationCostModel::CM_Unknown &&\n           \"CM decision should be taken at this point.\");\n    if (Decision == LoopVectorizationCostModel::CM_Interleave)\n      return true;\n    if (CM.isScalarAfterVectorization(I, VF) ||\n        CM.isProfitableToScalarize(I, VF))\n      return false;\n    return Decision != LoopVectorizationCostModel::CM_Scalarize;\n  };\n\n  if (!LoopVectorizationPlanner::getDecisionAndClampRange(willWiden, Range))\n    return nullptr;\n\n  VPValue *Mask = nullptr;\n  if (Legal->isMaskRequired(I))\n    Mask = createBlockInMask(I->getParent(), Plan);\n\n  VPValue *Addr = Plan->getOrAddVPValue(getLoadStorePointerOperand(I));\n  if (LoadInst *Load = dyn_cast<LoadInst>(I))\n    return new VPWidenMemoryInstructionRecipe(*Load, Addr, Mask);\n\n  StoreInst *Store = cast<StoreInst>(I);\n  VPValue *StoredValue = Plan->getOrAddVPValue(Store->getValueOperand());\n  return new VPWidenMemoryInstructionRecipe(*Store, Addr, StoredValue, Mask);\n}\n\nVPWidenIntOrFpInductionRecipe *\nVPRecipeBuilder::tryToOptimizeInductionPHI(PHINode *Phi, VPlan &Plan) const {\n  // Check if this is an integer or fp induction. If so, build the recipe that\n  // produces its scalar and vector values.\n  InductionDescriptor II = Legal->getInductionVars().lookup(Phi);\n  if (II.getKind() == InductionDescriptor::IK_IntInduction ||\n      II.getKind() == InductionDescriptor::IK_FpInduction) {\n    VPValue *Start = Plan.getOrAddVPValue(II.getStartValue());\n    const SmallVectorImpl<Instruction *> &Casts = II.getCastInsts();\n    return new VPWidenIntOrFpInductionRecipe(\n        Phi, Start, Casts.empty() ? nullptr : Casts.front());\n  }\n\n  return nullptr;\n}\n\nVPWidenIntOrFpInductionRecipe *\nVPRecipeBuilder::tryToOptimizeInductionTruncate(TruncInst *I, VFRange &Range,\n                                                VPlan &Plan) const {\n  // Optimize the special case where the source is a constant integer\n  // induction variable. Notice that we can only optimize the 'trunc' case\n  // because (a) FP conversions lose precision, (b) sext/zext may wrap, and\n  // (c) other casts depend on pointer size.\n\n  // Determine whether \\p K is a truncation based on an induction variable that\n  // can be optimized.\n  auto isOptimizableIVTruncate =\n      [&](Instruction *K) -> std::function<bool(ElementCount)> {\n    return [=](ElementCount VF) -> bool {\n      return CM.isOptimizableIVTruncate(K, VF);\n    };\n  };\n\n  if (LoopVectorizationPlanner::getDecisionAndClampRange(\n          isOptimizableIVTruncate(I), Range)) {\n\n    InductionDescriptor II =\n        Legal->getInductionVars().lookup(cast<PHINode>(I->getOperand(0)));\n    VPValue *Start = Plan.getOrAddVPValue(II.getStartValue());\n    return new VPWidenIntOrFpInductionRecipe(cast<PHINode>(I->getOperand(0)),\n                                             Start, nullptr, I);\n  }\n  return nullptr;\n}\n\nVPRecipeOrVPValueTy VPRecipeBuilder::tryToBlend(PHINode *Phi, VPlanPtr &Plan) {\n  // If all incoming values are equal, the incoming VPValue can be used directly\n  // instead of creating a new VPBlendRecipe.\n  Value *FirstIncoming = Phi->getIncomingValue(0);\n  if (all_of(Phi->incoming_values(), [FirstIncoming](const Value *Inc) {\n        return FirstIncoming == Inc;\n      })) {\n    return Plan->getOrAddVPValue(Phi->getIncomingValue(0));\n  }\n\n  // We know that all PHIs in non-header blocks are converted into selects, so\n  // we don't have to worry about the insertion order and we can just use the\n  // builder. At this point we generate the predication tree. There may be\n  // duplications since this is a simple recursive scan, but future\n  // optimizations will clean it up.\n  SmallVector<VPValue *, 2> Operands;\n  unsigned NumIncoming = Phi->getNumIncomingValues();\n\n  for (unsigned In = 0; In < NumIncoming; In++) {\n    VPValue *EdgeMask =\n      createEdgeMask(Phi->getIncomingBlock(In), Phi->getParent(), Plan);\n    assert((EdgeMask || NumIncoming == 1) &&\n           \"Multiple predecessors with one having a full mask\");\n    Operands.push_back(Plan->getOrAddVPValue(Phi->getIncomingValue(In)));\n    if (EdgeMask)\n      Operands.push_back(EdgeMask);\n  }\n  return toVPRecipeResult(new VPBlendRecipe(Phi, Operands));\n}\n\nVPWidenCallRecipe *VPRecipeBuilder::tryToWidenCall(CallInst *CI, VFRange &Range,\n                                                   VPlan &Plan) const {\n\n  bool IsPredicated = LoopVectorizationPlanner::getDecisionAndClampRange(\n      [this, CI](ElementCount VF) {\n        return CM.isScalarWithPredication(CI, VF);\n      },\n      Range);\n\n  if (IsPredicated)\n    return nullptr;\n\n  Intrinsic::ID ID = getVectorIntrinsicIDForCall(CI, TLI);\n  if (ID && (ID == Intrinsic::assume || ID == Intrinsic::lifetime_end ||\n             ID == Intrinsic::lifetime_start || ID == Intrinsic::sideeffect ||\n             ID == Intrinsic::pseudoprobe ||\n             ID == Intrinsic::experimental_noalias_scope_decl))\n    return nullptr;\n\n  auto willWiden = [&](ElementCount VF) -> bool {\n    Intrinsic::ID ID = getVectorIntrinsicIDForCall(CI, TLI);\n    // The following case may be scalarized depending on the VF.\n    // The flag shows whether we use Intrinsic or a usual Call for vectorized\n    // version of the instruction.\n    // Is it beneficial to perform intrinsic call compared to lib call?\n    bool NeedToScalarize = false;\n    InstructionCost CallCost = CM.getVectorCallCost(CI, VF, NeedToScalarize);\n    InstructionCost IntrinsicCost = ID ? CM.getVectorIntrinsicCost(CI, VF) : 0;\n    bool UseVectorIntrinsic = ID && IntrinsicCost <= CallCost;\n    assert(IntrinsicCost.isValid() && CallCost.isValid() &&\n           \"Cannot have invalid costs while widening\");\n    return UseVectorIntrinsic || !NeedToScalarize;\n  };\n\n  if (!LoopVectorizationPlanner::getDecisionAndClampRange(willWiden, Range))\n    return nullptr;\n\n  return new VPWidenCallRecipe(*CI, Plan.mapToVPValues(CI->arg_operands()));\n}\n\nbool VPRecipeBuilder::shouldWiden(Instruction *I, VFRange &Range) const {\n  assert(!isa<BranchInst>(I) && !isa<PHINode>(I) && !isa<LoadInst>(I) &&\n         !isa<StoreInst>(I) && \"Instruction should have been handled earlier\");\n  // Instruction should be widened, unless it is scalar after vectorization,\n  // scalarization is profitable or it is predicated.\n  auto WillScalarize = [this, I](ElementCount VF) -> bool {\n    return CM.isScalarAfterVectorization(I, VF) ||\n           CM.isProfitableToScalarize(I, VF) ||\n           CM.isScalarWithPredication(I, VF);\n  };\n  return !LoopVectorizationPlanner::getDecisionAndClampRange(WillScalarize,\n                                                             Range);\n}\n\nVPWidenRecipe *VPRecipeBuilder::tryToWiden(Instruction *I, VPlan &Plan) const {\n  auto IsVectorizableOpcode = [](unsigned Opcode) {\n    switch (Opcode) {\n    case Instruction::Add:\n    case Instruction::And:\n    case Instruction::AShr:\n    case Instruction::BitCast:\n    case Instruction::FAdd:\n    case Instruction::FCmp:\n    case Instruction::FDiv:\n    case Instruction::FMul:\n    case Instruction::FNeg:\n    case Instruction::FPExt:\n    case Instruction::FPToSI:\n    case Instruction::FPToUI:\n    case Instruction::FPTrunc:\n    case Instruction::FRem:\n    case Instruction::FSub:\n    case Instruction::ICmp:\n    case Instruction::IntToPtr:\n    case Instruction::LShr:\n    case Instruction::Mul:\n    case Instruction::Or:\n    case Instruction::PtrToInt:\n    case Instruction::SDiv:\n    case Instruction::Select:\n    case Instruction::SExt:\n    case Instruction::Shl:\n    case Instruction::SIToFP:\n    case Instruction::SRem:\n    case Instruction::Sub:\n    case Instruction::Trunc:\n    case Instruction::UDiv:\n    case Instruction::UIToFP:\n    case Instruction::URem:\n    case Instruction::Xor:\n    case Instruction::ZExt:\n      return true;\n    }\n    return false;\n  };\n\n  if (!IsVectorizableOpcode(I->getOpcode()))\n    return nullptr;\n\n  // Success: widen this instruction.\n  return new VPWidenRecipe(*I, Plan.mapToVPValues(I->operands()));\n}\n\nVPBasicBlock *VPRecipeBuilder::handleReplication(\n    Instruction *I, VFRange &Range, VPBasicBlock *VPBB,\n    DenseMap<Instruction *, VPReplicateRecipe *> &PredInst2Recipe,\n    VPlanPtr &Plan) {\n  bool IsUniform = LoopVectorizationPlanner::getDecisionAndClampRange(\n      [&](ElementCount VF) { return CM.isUniformAfterVectorization(I, VF); },\n      Range);\n\n  bool IsPredicated = LoopVectorizationPlanner::getDecisionAndClampRange(\n      [&](ElementCount VF) { return CM.isScalarWithPredication(I, VF); },\n      Range);\n\n  auto *Recipe = new VPReplicateRecipe(I, Plan->mapToVPValues(I->operands()),\n                                       IsUniform, IsPredicated);\n  setRecipe(I, Recipe);\n  Plan->addVPValue(I, Recipe);\n\n  // Find if I uses a predicated instruction. If so, it will use its scalar\n  // value. Avoid hoisting the insert-element which packs the scalar value into\n  // a vector value, as that happens iff all users use the vector value.\n  for (auto &Op : I->operands())\n    if (auto *PredInst = dyn_cast<Instruction>(Op))\n      if (PredInst2Recipe.find(PredInst) != PredInst2Recipe.end())\n        PredInst2Recipe[PredInst]->setAlsoPack(false);\n\n  // Finalize the recipe for Instr, first if it is not predicated.\n  if (!IsPredicated) {\n    LLVM_DEBUG(dbgs() << \"LV: Scalarizing:\" << *I << \"\\n\");\n    VPBB->appendRecipe(Recipe);\n    return VPBB;\n  }\n  LLVM_DEBUG(dbgs() << \"LV: Scalarizing and predicating:\" << *I << \"\\n\");\n  assert(VPBB->getSuccessors().empty() &&\n         \"VPBB has successors when handling predicated replication.\");\n  // Record predicated instructions for above packing optimizations.\n  PredInst2Recipe[I] = Recipe;\n  VPBlockBase *Region = createReplicateRegion(I, Recipe, Plan);\n  VPBlockUtils::insertBlockAfter(Region, VPBB);\n  auto *RegSucc = new VPBasicBlock();\n  VPBlockUtils::insertBlockAfter(RegSucc, Region);\n  return RegSucc;\n}\n\nVPRegionBlock *VPRecipeBuilder::createReplicateRegion(Instruction *Instr,\n                                                      VPRecipeBase *PredRecipe,\n                                                      VPlanPtr &Plan) {\n  // Instructions marked for predication are replicated and placed under an\n  // if-then construct to prevent side-effects.\n\n  // Generate recipes to compute the block mask for this region.\n  VPValue *BlockInMask = createBlockInMask(Instr->getParent(), Plan);\n\n  // Build the triangular if-then region.\n  std::string RegionName = (Twine(\"pred.\") + Instr->getOpcodeName()).str();\n  assert(Instr->getParent() && \"Predicated instruction not in any basic block\");\n  auto *BOMRecipe = new VPBranchOnMaskRecipe(BlockInMask);\n  auto *Entry = new VPBasicBlock(Twine(RegionName) + \".entry\", BOMRecipe);\n  auto *PHIRecipe = Instr->getType()->isVoidTy()\n                        ? nullptr\n                        : new VPPredInstPHIRecipe(Plan->getOrAddVPValue(Instr));\n  if (PHIRecipe) {\n    Plan->removeVPValueFor(Instr);\n    Plan->addVPValue(Instr, PHIRecipe);\n  }\n  auto *Exit = new VPBasicBlock(Twine(RegionName) + \".continue\", PHIRecipe);\n  auto *Pred = new VPBasicBlock(Twine(RegionName) + \".if\", PredRecipe);\n  VPRegionBlock *Region = new VPRegionBlock(Entry, Exit, RegionName, true);\n\n  // Note: first set Entry as region entry and then connect successors starting\n  // from it in order, to propagate the \"parent\" of each VPBasicBlock.\n  VPBlockUtils::insertTwoBlocksAfter(Pred, Exit, BlockInMask, Entry);\n  VPBlockUtils::connectBlocks(Pred, Exit);\n\n  return Region;\n}\n\nVPRecipeOrVPValueTy VPRecipeBuilder::tryToCreateWidenRecipe(Instruction *Instr,\n                                                            VFRange &Range,\n                                                            VPlanPtr &Plan) {\n  // First, check for specific widening recipes that deal with calls, memory\n  // operations, inductions and Phi nodes.\n  if (auto *CI = dyn_cast<CallInst>(Instr))\n    return toVPRecipeResult(tryToWidenCall(CI, Range, *Plan));\n\n  if (isa<LoadInst>(Instr) || isa<StoreInst>(Instr))\n    return toVPRecipeResult(tryToWidenMemory(Instr, Range, Plan));\n\n  VPRecipeBase *Recipe;\n  if (auto Phi = dyn_cast<PHINode>(Instr)) {\n    if (Phi->getParent() != OrigLoop->getHeader())\n      return tryToBlend(Phi, Plan);\n    if ((Recipe = tryToOptimizeInductionPHI(Phi, *Plan)))\n      return toVPRecipeResult(Recipe);\n\n    if (Legal->isReductionVariable(Phi)) {\n      RecurrenceDescriptor &RdxDesc = Legal->getReductionVars()[Phi];\n      VPValue *StartV =\n          Plan->getOrAddVPValue(RdxDesc.getRecurrenceStartValue());\n      return toVPRecipeResult(new VPWidenPHIRecipe(Phi, RdxDesc, *StartV));\n    }\n\n    return toVPRecipeResult(new VPWidenPHIRecipe(Phi));\n  }\n\n  if (isa<TruncInst>(Instr) && (Recipe = tryToOptimizeInductionTruncate(\n                                    cast<TruncInst>(Instr), Range, *Plan)))\n    return toVPRecipeResult(Recipe);\n\n  if (!shouldWiden(Instr, Range))\n    return nullptr;\n\n  if (auto GEP = dyn_cast<GetElementPtrInst>(Instr))\n    return toVPRecipeResult(new VPWidenGEPRecipe(\n        GEP, Plan->mapToVPValues(GEP->operands()), OrigLoop));\n\n  if (auto *SI = dyn_cast<SelectInst>(Instr)) {\n    bool InvariantCond =\n        PSE.getSE()->isLoopInvariant(PSE.getSCEV(SI->getOperand(0)), OrigLoop);\n    return toVPRecipeResult(new VPWidenSelectRecipe(\n        *SI, Plan->mapToVPValues(SI->operands()), InvariantCond));\n  }\n\n  return toVPRecipeResult(tryToWiden(Instr, *Plan));\n}\n\nvoid LoopVectorizationPlanner::buildVPlansWithVPRecipes(ElementCount MinVF,\n                                                        ElementCount MaxVF) {\n  assert(OrigLoop->isInnermost() && \"Inner loop expected.\");\n\n  // Collect instructions from the original loop that will become trivially dead\n  // in the vectorized loop. We don't need to vectorize these instructions. For\n  // example, original induction update instructions can become dead because we\n  // separately emit induction \"steps\" when generating code for the new loop.\n  // Similarly, we create a new latch condition when setting up the structure\n  // of the new loop, so the old one can become dead.\n  SmallPtrSet<Instruction *, 4> DeadInstructions;\n  collectTriviallyDeadInstructions(DeadInstructions);\n\n  // Add assume instructions we need to drop to DeadInstructions, to prevent\n  // them from being added to the VPlan.\n  // TODO: We only need to drop assumes in blocks that get flattend. If the\n  // control flow is preserved, we should keep them.\n  auto &ConditionalAssumes = Legal->getConditionalAssumes();\n  DeadInstructions.insert(ConditionalAssumes.begin(), ConditionalAssumes.end());\n\n  DenseMap<Instruction *, Instruction *> &SinkAfter = Legal->getSinkAfter();\n  // Dead instructions do not need sinking. Remove them from SinkAfter.\n  for (Instruction *I : DeadInstructions)\n    SinkAfter.erase(I);\n\n  auto MaxVFPlusOne = MaxVF.getWithIncrement(1);\n  for (ElementCount VF = MinVF; ElementCount::isKnownLT(VF, MaxVFPlusOne);) {\n    VFRange SubRange = {VF, MaxVFPlusOne};\n    VPlans.push_back(\n        buildVPlanWithVPRecipes(SubRange, DeadInstructions, SinkAfter));\n    VF = SubRange.End;\n  }\n}\n\nVPlanPtr LoopVectorizationPlanner::buildVPlanWithVPRecipes(\n    VFRange &Range, SmallPtrSetImpl<Instruction *> &DeadInstructions,\n    const DenseMap<Instruction *, Instruction *> &SinkAfter) {\n\n  // Hold a mapping from predicated instructions to their recipes, in order to\n  // fix their AlsoPack behavior if a user is determined to replicate and use a\n  // scalar instead of vector value.\n  DenseMap<Instruction *, VPReplicateRecipe *> PredInst2Recipe;\n\n  SmallPtrSet<const InterleaveGroup<Instruction> *, 1> InterleaveGroups;\n\n  VPRecipeBuilder RecipeBuilder(OrigLoop, TLI, Legal, CM, PSE, Builder);\n\n  // ---------------------------------------------------------------------------\n  // Pre-construction: record ingredients whose recipes we'll need to further\n  // process after constructing the initial VPlan.\n  // ---------------------------------------------------------------------------\n\n  // Mark instructions we'll need to sink later and their targets as\n  // ingredients whose recipe we'll need to record.\n  for (auto &Entry : SinkAfter) {\n    RecipeBuilder.recordRecipeOf(Entry.first);\n    RecipeBuilder.recordRecipeOf(Entry.second);\n  }\n  for (auto &Reduction : CM.getInLoopReductionChains()) {\n    PHINode *Phi = Reduction.first;\n    RecurKind Kind = Legal->getReductionVars()[Phi].getRecurrenceKind();\n    const SmallVector<Instruction *, 4> &ReductionOperations = Reduction.second;\n\n    RecipeBuilder.recordRecipeOf(Phi);\n    for (auto &R : ReductionOperations) {\n      RecipeBuilder.recordRecipeOf(R);\n      // For min/max reducitons, where we have a pair of icmp/select, we also\n      // need to record the ICmp recipe, so it can be removed later.\n      if (RecurrenceDescriptor::isMinMaxRecurrenceKind(Kind))\n        RecipeBuilder.recordRecipeOf(cast<Instruction>(R->getOperand(0)));\n    }\n  }\n\n  // For each interleave group which is relevant for this (possibly trimmed)\n  // Range, add it to the set of groups to be later applied to the VPlan and add\n  // placeholders for its members' Recipes which we'll be replacing with a\n  // single VPInterleaveRecipe.\n  for (InterleaveGroup<Instruction> *IG : IAI.getInterleaveGroups()) {\n    auto applyIG = [IG, this](ElementCount VF) -> bool {\n      return (VF.isVector() && // Query is illegal for VF == 1\n              CM.getWideningDecision(IG->getInsertPos(), VF) ==\n                  LoopVectorizationCostModel::CM_Interleave);\n    };\n    if (!getDecisionAndClampRange(applyIG, Range))\n      continue;\n    InterleaveGroups.insert(IG);\n    for (unsigned i = 0; i < IG->getFactor(); i++)\n      if (Instruction *Member = IG->getMember(i))\n        RecipeBuilder.recordRecipeOf(Member);\n  };\n\n  // ---------------------------------------------------------------------------\n  // Build initial VPlan: Scan the body of the loop in a topological order to\n  // visit each basic block after having visited its predecessor basic blocks.\n  // ---------------------------------------------------------------------------\n\n  // Create a dummy pre-entry VPBasicBlock to start building the VPlan.\n  auto Plan = std::make_unique<VPlan>();\n  VPBasicBlock *VPBB = new VPBasicBlock(\"Pre-Entry\");\n  Plan->setEntry(VPBB);\n\n  // Scan the body of the loop in a topological order to visit each basic block\n  // after having visited its predecessor basic blocks.\n  LoopBlocksDFS DFS(OrigLoop);\n  DFS.perform(LI);\n\n  for (BasicBlock *BB : make_range(DFS.beginRPO(), DFS.endRPO())) {\n    // Relevant instructions from basic block BB will be grouped into VPRecipe\n    // ingredients and fill a new VPBasicBlock.\n    unsigned VPBBsForBB = 0;\n    auto *FirstVPBBForBB = new VPBasicBlock(BB->getName());\n    VPBlockUtils::insertBlockAfter(FirstVPBBForBB, VPBB);\n    VPBB = FirstVPBBForBB;\n    Builder.setInsertPoint(VPBB);\n\n    // Introduce each ingredient into VPlan.\n    // TODO: Model and preserve debug instrinsics in VPlan.\n    for (Instruction &I : BB->instructionsWithoutDebug()) {\n      Instruction *Instr = &I;\n\n      // First filter out irrelevant instructions, to ensure no recipes are\n      // built for them.\n      if (isa<BranchInst>(Instr) || DeadInstructions.count(Instr))\n        continue;\n\n      if (auto RecipeOrValue =\n              RecipeBuilder.tryToCreateWidenRecipe(Instr, Range, Plan)) {\n        // If Instr can be simplified to an existing VPValue, use it.\n        if (RecipeOrValue.is<VPValue *>()) {\n          Plan->addVPValue(Instr, RecipeOrValue.get<VPValue *>());\n          continue;\n        }\n        // Otherwise, add the new recipe.\n        VPRecipeBase *Recipe = RecipeOrValue.get<VPRecipeBase *>();\n        for (auto *Def : Recipe->definedValues()) {\n          auto *UV = Def->getUnderlyingValue();\n          Plan->addVPValue(UV, Def);\n        }\n\n        RecipeBuilder.setRecipe(Instr, Recipe);\n        VPBB->appendRecipe(Recipe);\n        continue;\n      }\n\n      // Otherwise, if all widening options failed, Instruction is to be\n      // replicated. This may create a successor for VPBB.\n      VPBasicBlock *NextVPBB = RecipeBuilder.handleReplication(\n          Instr, Range, VPBB, PredInst2Recipe, Plan);\n      if (NextVPBB != VPBB) {\n        VPBB = NextVPBB;\n        VPBB->setName(BB->hasName() ? BB->getName() + \".\" + Twine(VPBBsForBB++)\n                                    : \"\");\n      }\n    }\n  }\n\n  // Discard empty dummy pre-entry VPBasicBlock. Note that other VPBasicBlocks\n  // may also be empty, such as the last one VPBB, reflecting original\n  // basic-blocks with no recipes.\n  VPBasicBlock *PreEntry = cast<VPBasicBlock>(Plan->getEntry());\n  assert(PreEntry->empty() && \"Expecting empty pre-entry block.\");\n  VPBlockBase *Entry = Plan->setEntry(PreEntry->getSingleSuccessor());\n  VPBlockUtils::disconnectBlocks(PreEntry, Entry);\n  delete PreEntry;\n\n  // ---------------------------------------------------------------------------\n  // Transform initial VPlan: Apply previously taken decisions, in order, to\n  // bring the VPlan to its final state.\n  // ---------------------------------------------------------------------------\n\n  // Apply Sink-After legal constraints.\n  for (auto &Entry : SinkAfter) {\n    VPRecipeBase *Sink = RecipeBuilder.getRecipe(Entry.first);\n    VPRecipeBase *Target = RecipeBuilder.getRecipe(Entry.second);\n    // If the target is in a replication region, make sure to move Sink to the\n    // block after it, not into the replication region itself.\n    if (auto *Region =\n            dyn_cast_or_null<VPRegionBlock>(Target->getParent()->getParent())) {\n      if (Region->isReplicator()) {\n        assert(Region->getNumSuccessors() == 1 && \"Expected SESE region!\");\n        VPBasicBlock *NextBlock =\n            cast<VPBasicBlock>(Region->getSuccessors().front());\n        Sink->moveBefore(*NextBlock, NextBlock->getFirstNonPhi());\n        continue;\n      }\n    }\n    Sink->moveAfter(Target);\n  }\n\n  // Interleave memory: for each Interleave Group we marked earlier as relevant\n  // for this VPlan, replace the Recipes widening its memory instructions with a\n  // single VPInterleaveRecipe at its insertion point.\n  for (auto IG : InterleaveGroups) {\n    auto *Recipe = cast<VPWidenMemoryInstructionRecipe>(\n        RecipeBuilder.getRecipe(IG->getInsertPos()));\n    SmallVector<VPValue *, 4> StoredValues;\n    for (unsigned i = 0; i < IG->getFactor(); ++i)\n      if (auto *SI = dyn_cast_or_null<StoreInst>(IG->getMember(i)))\n        StoredValues.push_back(Plan->getOrAddVPValue(SI->getOperand(0)));\n\n    auto *VPIG = new VPInterleaveRecipe(IG, Recipe->getAddr(), StoredValues,\n                                        Recipe->getMask());\n    VPIG->insertBefore(Recipe);\n    unsigned J = 0;\n    for (unsigned i = 0; i < IG->getFactor(); ++i)\n      if (Instruction *Member = IG->getMember(i)) {\n        if (!Member->getType()->isVoidTy()) {\n          VPValue *OriginalV = Plan->getVPValue(Member);\n          Plan->removeVPValueFor(Member);\n          Plan->addVPValue(Member, VPIG->getVPValue(J));\n          OriginalV->replaceAllUsesWith(VPIG->getVPValue(J));\n          J++;\n        }\n        RecipeBuilder.getRecipe(Member)->eraseFromParent();\n      }\n  }\n\n  // Adjust the recipes for any inloop reductions.\n  if (Range.Start.isVector())\n    adjustRecipesForInLoopReductions(Plan, RecipeBuilder);\n\n  // Finally, if tail is folded by masking, introduce selects between the phi\n  // and the live-out instruction of each reduction, at the end of the latch.\n  if (CM.foldTailByMasking() && !Legal->getReductionVars().empty()) {\n    Builder.setInsertPoint(VPBB);\n    auto *Cond = RecipeBuilder.createBlockInMask(OrigLoop->getHeader(), Plan);\n    for (auto &Reduction : Legal->getReductionVars()) {\n      if (CM.isInLoopReduction(Reduction.first))\n        continue;\n      VPValue *Phi = Plan->getOrAddVPValue(Reduction.first);\n      VPValue *Red = Plan->getOrAddVPValue(Reduction.second.getLoopExitInstr());\n      Builder.createNaryOp(Instruction::Select, {Cond, Red, Phi});\n    }\n  }\n\n  std::string PlanName;\n  raw_string_ostream RSO(PlanName);\n  ElementCount VF = Range.Start;\n  Plan->addVF(VF);\n  RSO << \"Initial VPlan for VF={\" << VF;\n  for (VF *= 2; ElementCount::isKnownLT(VF, Range.End); VF *= 2) {\n    Plan->addVF(VF);\n    RSO << \",\" << VF;\n  }\n  RSO << \"},UF>=1\";\n  RSO.flush();\n  Plan->setName(PlanName);\n\n  return Plan;\n}\n\nVPlanPtr LoopVectorizationPlanner::buildVPlan(VFRange &Range) {\n  // Outer loop handling: They may require CFG and instruction level\n  // transformations before even evaluating whether vectorization is profitable.\n  // Since we cannot modify the incoming IR, we need to build VPlan upfront in\n  // the vectorization pipeline.\n  assert(!OrigLoop->isInnermost());\n  assert(EnableVPlanNativePath && \"VPlan-native path is not enabled.\");\n\n  // Create new empty VPlan\n  auto Plan = std::make_unique<VPlan>();\n\n  // Build hierarchical CFG\n  VPlanHCFGBuilder HCFGBuilder(OrigLoop, LI, *Plan);\n  HCFGBuilder.buildHierarchicalCFG();\n\n  for (ElementCount VF = Range.Start; ElementCount::isKnownLT(VF, Range.End);\n       VF *= 2)\n    Plan->addVF(VF);\n\n  if (EnableVPlanPredication) {\n    VPlanPredicator VPP(*Plan);\n    VPP.predicate();\n\n    // Avoid running transformation to recipes until masked code generation in\n    // VPlan-native path is in place.\n    return Plan;\n  }\n\n  SmallPtrSet<Instruction *, 1> DeadInstructions;\n  VPlanTransforms::VPInstructionsToVPRecipes(OrigLoop, Plan,\n                                             Legal->getInductionVars(),\n                                             DeadInstructions, *PSE.getSE());\n  return Plan;\n}\n\n// Adjust the recipes for any inloop reductions. The chain of instructions\n// leading from the loop exit instr to the phi need to be converted to\n// reductions, with one operand being vector and the other being the scalar\n// reduction chain.\nvoid LoopVectorizationPlanner::adjustRecipesForInLoopReductions(\n    VPlanPtr &Plan, VPRecipeBuilder &RecipeBuilder) {\n  for (auto &Reduction : CM.getInLoopReductionChains()) {\n    PHINode *Phi = Reduction.first;\n    RecurrenceDescriptor &RdxDesc = Legal->getReductionVars()[Phi];\n    const SmallVector<Instruction *, 4> &ReductionOperations = Reduction.second;\n\n    // ReductionOperations are orders top-down from the phi's use to the\n    // LoopExitValue. We keep a track of the previous item (the Chain) to tell\n    // which of the two operands will remain scalar and which will be reduced.\n    // For minmax the chain will be the select instructions.\n    Instruction *Chain = Phi;\n    for (Instruction *R : ReductionOperations) {\n      VPRecipeBase *WidenRecipe = RecipeBuilder.getRecipe(R);\n      RecurKind Kind = RdxDesc.getRecurrenceKind();\n\n      VPValue *ChainOp = Plan->getVPValue(Chain);\n      unsigned FirstOpId;\n      if (RecurrenceDescriptor::isMinMaxRecurrenceKind(Kind)) {\n        assert(isa<VPWidenSelectRecipe>(WidenRecipe) &&\n               \"Expected to replace a VPWidenSelectSC\");\n        FirstOpId = 1;\n      } else {\n        assert(isa<VPWidenRecipe>(WidenRecipe) &&\n               \"Expected to replace a VPWidenSC\");\n        FirstOpId = 0;\n      }\n      unsigned VecOpId =\n          R->getOperand(FirstOpId) == Chain ? FirstOpId + 1 : FirstOpId;\n      VPValue *VecOp = Plan->getVPValue(R->getOperand(VecOpId));\n\n      auto *CondOp = CM.foldTailByMasking()\n                         ? RecipeBuilder.createBlockInMask(R->getParent(), Plan)\n                         : nullptr;\n      VPReductionRecipe *RedRecipe = new VPReductionRecipe(\n          &RdxDesc, R, ChainOp, VecOp, CondOp, TTI);\n      WidenRecipe->getVPValue()->replaceAllUsesWith(RedRecipe);\n      Plan->removeVPValueFor(R);\n      Plan->addVPValue(R, RedRecipe);\n      WidenRecipe->getParent()->insert(RedRecipe, WidenRecipe->getIterator());\n      WidenRecipe->getVPValue()->replaceAllUsesWith(RedRecipe);\n      WidenRecipe->eraseFromParent();\n\n      if (RecurrenceDescriptor::isMinMaxRecurrenceKind(Kind)) {\n        VPRecipeBase *CompareRecipe =\n            RecipeBuilder.getRecipe(cast<Instruction>(R->getOperand(0)));\n        assert(isa<VPWidenRecipe>(CompareRecipe) &&\n               \"Expected to replace a VPWidenSC\");\n        assert(cast<VPWidenRecipe>(CompareRecipe)->getNumUsers() == 0 &&\n               \"Expected no remaining users\");\n        CompareRecipe->eraseFromParent();\n      }\n      Chain = R;\n    }\n  }\n}\n\nvoid VPInterleaveRecipe::print(raw_ostream &O, const Twine &Indent,\n                               VPSlotTracker &SlotTracker) const {\n  O << Indent << \"\\\"INTERLEAVE-GROUP with factor \" << IG->getFactor() << \" at \";\n  IG->getInsertPos()->printAsOperand(O, false);\n  O << \", \";\n  getAddr()->printAsOperand(O, SlotTracker);\n  VPValue *Mask = getMask();\n  if (Mask) {\n    O << \", \";\n    Mask->printAsOperand(O, SlotTracker);\n  }\n  for (unsigned i = 0; i < IG->getFactor(); ++i)\n    if (Instruction *I = IG->getMember(i))\n      O << \"\\\\l\\\" +\\n\" << Indent << \"\\\"  \" << VPlanIngredient(I) << \" \" << i;\n}\n\nvoid VPWidenCallRecipe::execute(VPTransformState &State) {\n  State.ILV->widenCallInstruction(*cast<CallInst>(getUnderlyingInstr()), this,\n                                  *this, State);\n}\n\nvoid VPWidenSelectRecipe::execute(VPTransformState &State) {\n  State.ILV->widenSelectInstruction(*cast<SelectInst>(getUnderlyingInstr()),\n                                    this, *this, InvariantCond, State);\n}\n\nvoid VPWidenRecipe::execute(VPTransformState &State) {\n  State.ILV->widenInstruction(*getUnderlyingInstr(), this, *this, State);\n}\n\nvoid VPWidenGEPRecipe::execute(VPTransformState &State) {\n  State.ILV->widenGEP(cast<GetElementPtrInst>(getUnderlyingInstr()), this,\n                      *this, State.UF, State.VF, IsPtrLoopInvariant,\n                      IsIndexLoopInvariant, State);\n}\n\nvoid VPWidenIntOrFpInductionRecipe::execute(VPTransformState &State) {\n  assert(!State.Instance && \"Int or FP induction being replicated.\");\n  State.ILV->widenIntOrFpInduction(IV, getStartValue()->getLiveInIRValue(),\n                                   getTruncInst(), getVPValue(0),\n                                   getCastValue(), State);\n}\n\nvoid VPWidenPHIRecipe::execute(VPTransformState &State) {\n  State.ILV->widenPHIInstruction(cast<PHINode>(getUnderlyingValue()), RdxDesc,\n                                 getStartValue(), this, State);\n}\n\nvoid VPBlendRecipe::execute(VPTransformState &State) {\n  State.ILV->setDebugLocFromInst(State.Builder, Phi);\n  // We know that all PHIs in non-header blocks are converted into\n  // selects, so we don't have to worry about the insertion order and we\n  // can just use the builder.\n  // At this point we generate the predication tree. There may be\n  // duplications since this is a simple recursive scan, but future\n  // optimizations will clean it up.\n\n  unsigned NumIncoming = getNumIncomingValues();\n\n  // Generate a sequence of selects of the form:\n  // SELECT(Mask3, In3,\n  //        SELECT(Mask2, In2,\n  //               SELECT(Mask1, In1,\n  //                      In0)))\n  // Note that Mask0 is never used: lanes for which no path reaches this phi and\n  // are essentially undef are taken from In0.\n  InnerLoopVectorizer::VectorParts Entry(State.UF);\n  for (unsigned In = 0; In < NumIncoming; ++In) {\n    for (unsigned Part = 0; Part < State.UF; ++Part) {\n      // We might have single edge PHIs (blocks) - use an identity\n      // 'select' for the first PHI operand.\n      Value *In0 = State.get(getIncomingValue(In), Part);\n      if (In == 0)\n        Entry[Part] = In0; // Initialize with the first incoming value.\n      else {\n        // Select between the current value and the previous incoming edge\n        // based on the incoming mask.\n        Value *Cond = State.get(getMask(In), Part);\n        Entry[Part] =\n            State.Builder.CreateSelect(Cond, In0, Entry[Part], \"predphi\");\n      }\n    }\n  }\n  for (unsigned Part = 0; Part < State.UF; ++Part)\n    State.set(this, Entry[Part], Part);\n}\n\nvoid VPInterleaveRecipe::execute(VPTransformState &State) {\n  assert(!State.Instance && \"Interleave group being replicated.\");\n  State.ILV->vectorizeInterleaveGroup(IG, definedValues(), State, getAddr(),\n                                      getStoredValues(), getMask());\n}\n\nvoid VPReductionRecipe::execute(VPTransformState &State) {\n  assert(!State.Instance && \"Reduction being replicated.\");\n  for (unsigned Part = 0; Part < State.UF; ++Part) {\n    RecurKind Kind = RdxDesc->getRecurrenceKind();\n    Value *NewVecOp = State.get(getVecOp(), Part);\n    if (VPValue *Cond = getCondOp()) {\n      Value *NewCond = State.get(Cond, Part);\n      VectorType *VecTy = cast<VectorType>(NewVecOp->getType());\n      Constant *Iden = RecurrenceDescriptor::getRecurrenceIdentity(\n          Kind, VecTy->getElementType());\n      Constant *IdenVec =\n          ConstantVector::getSplat(VecTy->getElementCount(), Iden);\n      Value *Select = State.Builder.CreateSelect(NewCond, NewVecOp, IdenVec);\n      NewVecOp = Select;\n    }\n    Value *NewRed =\n        createTargetReduction(State.Builder, TTI, *RdxDesc, NewVecOp);\n    Value *PrevInChain = State.get(getChainOp(), Part);\n    Value *NextInChain;\n    if (RecurrenceDescriptor::isMinMaxRecurrenceKind(Kind)) {\n      NextInChain =\n          createMinMaxOp(State.Builder, RdxDesc->getRecurrenceKind(),\n                         NewRed, PrevInChain);\n    } else {\n      NextInChain = State.Builder.CreateBinOp(\n          (Instruction::BinaryOps)getUnderlyingInstr()->getOpcode(), NewRed,\n          PrevInChain);\n    }\n    State.set(this, NextInChain, Part);\n  }\n}\n\nvoid VPReplicateRecipe::execute(VPTransformState &State) {\n  if (State.Instance) { // Generate a single instance.\n    assert(!State.VF.isScalable() && \"Can't scalarize a scalable vector\");\n    State.ILV->scalarizeInstruction(getUnderlyingInstr(), this, *this,\n                                    *State.Instance, IsPredicated, State);\n    // Insert scalar instance packing it into a vector.\n    if (AlsoPack && State.VF.isVector()) {\n      // If we're constructing lane 0, initialize to start from poison.\n      if (State.Instance->Lane.isFirstLane()) {\n        assert(!State.VF.isScalable() && \"VF is assumed to be non scalable.\");\n        Value *Poison = PoisonValue::get(\n            VectorType::get(getUnderlyingValue()->getType(), State.VF));\n        State.set(this, Poison, State.Instance->Part);\n      }\n      State.ILV->packScalarIntoVectorValue(this, *State.Instance, State);\n    }\n    return;\n  }\n\n  // Generate scalar instances for all VF lanes of all UF parts, unless the\n  // instruction is uniform inwhich case generate only the first lane for each\n  // of the UF parts.\n  unsigned EndLane = IsUniform ? 1 : State.VF.getKnownMinValue();\n  assert((!State.VF.isScalable() || IsUniform) &&\n         \"Can't scalarize a scalable vector\");\n  for (unsigned Part = 0; Part < State.UF; ++Part)\n    for (unsigned Lane = 0; Lane < EndLane; ++Lane)\n      State.ILV->scalarizeInstruction(getUnderlyingInstr(), this, *this,\n                                      VPIteration(Part, Lane), IsPredicated,\n                                      State);\n}\n\nvoid VPBranchOnMaskRecipe::execute(VPTransformState &State) {\n  assert(State.Instance && \"Branch on Mask works only on single instance.\");\n\n  unsigned Part = State.Instance->Part;\n  unsigned Lane = State.Instance->Lane.getKnownLane();\n\n  Value *ConditionBit = nullptr;\n  VPValue *BlockInMask = getMask();\n  if (BlockInMask) {\n    ConditionBit = State.get(BlockInMask, Part);\n    if (ConditionBit->getType()->isVectorTy())\n      ConditionBit = State.Builder.CreateExtractElement(\n          ConditionBit, State.Builder.getInt32(Lane));\n  } else // Block in mask is all-one.\n    ConditionBit = State.Builder.getTrue();\n\n  // Replace the temporary unreachable terminator with a new conditional branch,\n  // whose two destinations will be set later when they are created.\n  auto *CurrentTerminator = State.CFG.PrevBB->getTerminator();\n  assert(isa<UnreachableInst>(CurrentTerminator) &&\n         \"Expected to replace unreachable terminator with conditional branch.\");\n  auto *CondBr = BranchInst::Create(State.CFG.PrevBB, nullptr, ConditionBit);\n  CondBr->setSuccessor(0, nullptr);\n  ReplaceInstWithInst(CurrentTerminator, CondBr);\n}\n\nvoid VPPredInstPHIRecipe::execute(VPTransformState &State) {\n  assert(State.Instance && \"Predicated instruction PHI works per instance.\");\n  Instruction *ScalarPredInst =\n      cast<Instruction>(State.get(getOperand(0), *State.Instance));\n  BasicBlock *PredicatedBB = ScalarPredInst->getParent();\n  BasicBlock *PredicatingBB = PredicatedBB->getSinglePredecessor();\n  assert(PredicatingBB && \"Predicated block has no single predecessor.\");\n  assert(isa<VPReplicateRecipe>(getOperand(0)) &&\n         \"operand must be VPReplicateRecipe\");\n\n  // By current pack/unpack logic we need to generate only a single phi node: if\n  // a vector value for the predicated instruction exists at this point it means\n  // the instruction has vector users only, and a phi for the vector value is\n  // needed. In this case the recipe of the predicated instruction is marked to\n  // also do that packing, thereby \"hoisting\" the insert-element sequence.\n  // Otherwise, a phi node for the scalar value is needed.\n  unsigned Part = State.Instance->Part;\n  if (State.hasVectorValue(getOperand(0), Part)) {\n    Value *VectorValue = State.get(getOperand(0), Part);\n    InsertElementInst *IEI = cast<InsertElementInst>(VectorValue);\n    PHINode *VPhi = State.Builder.CreatePHI(IEI->getType(), 2);\n    VPhi->addIncoming(IEI->getOperand(0), PredicatingBB); // Unmodified vector.\n    VPhi->addIncoming(IEI, PredicatedBB); // New vector with inserted element.\n    if (State.hasVectorValue(this, Part))\n      State.reset(this, VPhi, Part);\n    else\n      State.set(this, VPhi, Part);\n    // NOTE: Currently we need to update the value of the operand, so the next\n    // predicated iteration inserts its generated value in the correct vector.\n    State.reset(getOperand(0), VPhi, Part);\n  } else {\n    Type *PredInstType = getOperand(0)->getUnderlyingValue()->getType();\n    PHINode *Phi = State.Builder.CreatePHI(PredInstType, 2);\n    Phi->addIncoming(PoisonValue::get(ScalarPredInst->getType()),\n                     PredicatingBB);\n    Phi->addIncoming(ScalarPredInst, PredicatedBB);\n    if (State.hasScalarValue(this, *State.Instance))\n      State.reset(this, Phi, *State.Instance);\n    else\n      State.set(this, Phi, *State.Instance);\n    // NOTE: Currently we need to update the value of the operand, so the next\n    // predicated iteration inserts its generated value in the correct vector.\n    State.reset(getOperand(0), Phi, *State.Instance);\n  }\n}\n\nvoid VPWidenMemoryInstructionRecipe::execute(VPTransformState &State) {\n  VPValue *StoredValue = isStore() ? getStoredValue() : nullptr;\n  State.ILV->vectorizeMemoryInstruction(&Ingredient, State,\n                                        StoredValue ? nullptr : getVPValue(),\n                                        getAddr(), StoredValue, getMask());\n}\n\n// Determine how to lower the scalar epilogue, which depends on 1) optimising\n// for minimum code-size, 2) predicate compiler options, 3) loop hints forcing\n// predication, and 4) a TTI hook that analyses whether the loop is suitable\n// for predication.\nstatic ScalarEpilogueLowering getScalarEpilogueLowering(\n    Function *F, Loop *L, LoopVectorizeHints &Hints, ProfileSummaryInfo *PSI,\n    BlockFrequencyInfo *BFI, TargetTransformInfo *TTI, TargetLibraryInfo *TLI,\n    AssumptionCache *AC, LoopInfo *LI, ScalarEvolution *SE, DominatorTree *DT,\n    LoopVectorizationLegality &LVL) {\n  // 1) OptSize takes precedence over all other options, i.e. if this is set,\n  // don't look at hints or options, and don't request a scalar epilogue.\n  // (For PGSO, as shouldOptimizeForSize isn't currently accessible from\n  // LoopAccessInfo (due to code dependency and not being able to reliably get\n  // PSI/BFI from a loop analysis under NPM), we cannot suppress the collection\n  // of strides in LoopAccessInfo::analyzeLoop() and vectorize without\n  // versioning when the vectorization is forced, unlike hasOptSize. So revert\n  // back to the old way and vectorize with versioning when forced. See D81345.)\n  if (F->hasOptSize() || (llvm::shouldOptimizeForSize(L->getHeader(), PSI, BFI,\n                                                      PGSOQueryType::IRPass) &&\n                          Hints.getForce() != LoopVectorizeHints::FK_Enabled))\n    return CM_ScalarEpilogueNotAllowedOptSize;\n\n  // 2) If set, obey the directives\n  if (PreferPredicateOverEpilogue.getNumOccurrences()) {\n    switch (PreferPredicateOverEpilogue) {\n    case PreferPredicateTy::ScalarEpilogue:\n      return CM_ScalarEpilogueAllowed;\n    case PreferPredicateTy::PredicateElseScalarEpilogue:\n      return CM_ScalarEpilogueNotNeededUsePredicate;\n    case PreferPredicateTy::PredicateOrDontVectorize:\n      return CM_ScalarEpilogueNotAllowedUsePredicate;\n    };\n  }\n\n  // 3) If set, obey the hints\n  switch (Hints.getPredicate()) {\n  case LoopVectorizeHints::FK_Enabled:\n    return CM_ScalarEpilogueNotNeededUsePredicate;\n  case LoopVectorizeHints::FK_Disabled:\n    return CM_ScalarEpilogueAllowed;\n  };\n\n  // 4) if the TTI hook indicates this is profitable, request predication.\n  if (TTI->preferPredicateOverEpilogue(L, LI, *SE, *AC, TLI, DT,\n                                       LVL.getLAI()))\n    return CM_ScalarEpilogueNotNeededUsePredicate;\n\n  return CM_ScalarEpilogueAllowed;\n}\n\nValue *VPTransformState::get(VPValue *Def, unsigned Part) {\n  // If Values have been set for this Def return the one relevant for \\p Part.\n  if (hasVectorValue(Def, Part))\n    return Data.PerPartOutput[Def][Part];\n\n  if (!hasScalarValue(Def, {Part, 0})) {\n    Value *IRV = Def->getLiveInIRValue();\n    Value *B = ILV->getBroadcastInstrs(IRV);\n    set(Def, B, Part);\n    return B;\n  }\n\n  Value *ScalarValue = get(Def, {Part, 0});\n  // If we aren't vectorizing, we can just copy the scalar map values over\n  // to the vector map.\n  if (VF.isScalar()) {\n    set(Def, ScalarValue, Part);\n    return ScalarValue;\n  }\n\n  auto *RepR = dyn_cast<VPReplicateRecipe>(Def);\n  bool IsUniform = RepR && RepR->isUniform();\n\n  unsigned LastLane = IsUniform ? 0 : VF.getKnownMinValue() - 1;\n  auto *LastInst = cast<Instruction>(get(Def, {Part, LastLane}));\n\n  // Set the insert point after the last scalarized instruction. This\n  // ensures the insertelement sequence will directly follow the scalar\n  // definitions.\n  auto OldIP = Builder.saveIP();\n  auto NewIP = std::next(BasicBlock::iterator(LastInst));\n  Builder.SetInsertPoint(&*NewIP);\n\n  // However, if we are vectorizing, we need to construct the vector values.\n  // If the value is known to be uniform after vectorization, we can just\n  // broadcast the scalar value corresponding to lane zero for each unroll\n  // iteration. Otherwise, we construct the vector values using\n  // insertelement instructions. Since the resulting vectors are stored in\n  // State, we will only generate the insertelements once.\n  Value *VectorValue = nullptr;\n  if (IsUniform) {\n    VectorValue = ILV->getBroadcastInstrs(ScalarValue);\n    set(Def, VectorValue, Part);\n  } else {\n    // Initialize packing with insertelements to start from undef.\n    assert(!VF.isScalable() && \"VF is assumed to be non scalable.\");\n    Value *Undef = PoisonValue::get(VectorType::get(LastInst->getType(), VF));\n    set(Def, Undef, Part);\n    for (unsigned Lane = 0; Lane < VF.getKnownMinValue(); ++Lane)\n      ILV->packScalarIntoVectorValue(Def, {Part, Lane}, *this);\n    VectorValue = get(Def, Part);\n  }\n  Builder.restoreIP(OldIP);\n  return VectorValue;\n}\n\n// Process the loop in the VPlan-native vectorization path. This path builds\n// VPlan upfront in the vectorization pipeline, which allows to apply\n// VPlan-to-VPlan transformations from the very beginning without modifying the\n// input LLVM IR.\nstatic bool processLoopInVPlanNativePath(\n    Loop *L, PredicatedScalarEvolution &PSE, LoopInfo *LI, DominatorTree *DT,\n    LoopVectorizationLegality *LVL, TargetTransformInfo *TTI,\n    TargetLibraryInfo *TLI, DemandedBits *DB, AssumptionCache *AC,\n    OptimizationRemarkEmitter *ORE, BlockFrequencyInfo *BFI,\n    ProfileSummaryInfo *PSI, LoopVectorizeHints &Hints) {\n\n  if (isa<SCEVCouldNotCompute>(PSE.getBackedgeTakenCount())) {\n    LLVM_DEBUG(dbgs() << \"LV: cannot compute the outer-loop trip count\\n\");\n    return false;\n  }\n  assert(EnableVPlanNativePath && \"VPlan-native path is disabled.\");\n  Function *F = L->getHeader()->getParent();\n  InterleavedAccessInfo IAI(PSE, L, DT, LI, LVL->getLAI());\n\n  ScalarEpilogueLowering SEL = getScalarEpilogueLowering(\n      F, L, Hints, PSI, BFI, TTI, TLI, AC, LI, PSE.getSE(), DT, *LVL);\n\n  LoopVectorizationCostModel CM(SEL, L, PSE, LI, LVL, *TTI, TLI, DB, AC, ORE, F,\n                                &Hints, IAI);\n  // Use the planner for outer loop vectorization.\n  // TODO: CM is not used at this point inside the planner. Turn CM into an\n  // optional argument if we don't need it in the future.\n  LoopVectorizationPlanner LVP(L, LI, TLI, TTI, LVL, CM, IAI, PSE);\n\n  // Get user vectorization factor.\n  ElementCount UserVF = Hints.getWidth();\n\n  // Plan how to best vectorize, return the best VF and its cost.\n  const VectorizationFactor VF = LVP.planInVPlanNativePath(UserVF);\n\n  // If we are stress testing VPlan builds, do not attempt to generate vector\n  // code. Masked vector code generation support will follow soon.\n  // Also, do not attempt to vectorize if no vector code will be produced.\n  if (VPlanBuildStressTest || EnableVPlanPredication ||\n      VectorizationFactor::Disabled() == VF)\n    return false;\n\n  LVP.setBestPlan(VF.Width, 1);\n\n  {\n    GeneratedRTChecks Checks(*PSE.getSE(), DT, LI,\n                             F->getParent()->getDataLayout());\n    InnerLoopVectorizer LB(L, PSE, LI, DT, TLI, TTI, AC, ORE, VF.Width, 1, LVL,\n                           &CM, BFI, PSI, Checks);\n    LLVM_DEBUG(dbgs() << \"Vectorizing outer loop in \\\"\"\n                      << L->getHeader()->getParent()->getName() << \"\\\"\\n\");\n    LVP.executePlan(LB, DT);\n  }\n\n  // Mark the loop as already vectorized to avoid vectorizing again.\n  Hints.setAlreadyVectorized();\n  assert(!verifyFunction(*L->getHeader()->getParent(), &dbgs()));\n  return true;\n}\n\n// Emit a remark if there are stores to floats that required a floating point\n// extension. If the vectorized loop was generated with floating point there\n// will be a performance penalty from the conversion overhead and the change in\n// the vector width.\nstatic void checkMixedPrecision(Loop *L, OptimizationRemarkEmitter *ORE) {\n  SmallVector<Instruction *, 4> Worklist;\n  for (BasicBlock *BB : L->getBlocks()) {\n    for (Instruction &Inst : *BB) {\n      if (auto *S = dyn_cast<StoreInst>(&Inst)) {\n        if (S->getValueOperand()->getType()->isFloatTy())\n          Worklist.push_back(S);\n      }\n    }\n  }\n\n  // Traverse the floating point stores upwards searching, for floating point\n  // conversions.\n  SmallPtrSet<const Instruction *, 4> Visited;\n  SmallPtrSet<const Instruction *, 4> EmittedRemark;\n  while (!Worklist.empty()) {\n    auto *I = Worklist.pop_back_val();\n    if (!L->contains(I))\n      continue;\n    if (!Visited.insert(I).second)\n      continue;\n\n    // Emit a remark if the floating point store required a floating\n    // point conversion.\n    // TODO: More work could be done to identify the root cause such as a\n    // constant or a function return type and point the user to it.\n    if (isa<FPExtInst>(I) && EmittedRemark.insert(I).second)\n      ORE->emit([&]() {\n        return OptimizationRemarkAnalysis(LV_NAME, \"VectorMixedPrecision\",\n                                          I->getDebugLoc(), L->getHeader())\n               << \"floating point conversion changes vector width. \"\n               << \"Mixed floating point precision requires an up/down \"\n               << \"cast that will negatively impact performance.\";\n      });\n\n    for (Use &Op : I->operands())\n      if (auto *OpI = dyn_cast<Instruction>(Op))\n        Worklist.push_back(OpI);\n  }\n}\n\nLoopVectorizePass::LoopVectorizePass(LoopVectorizeOptions Opts)\n    : InterleaveOnlyWhenForced(Opts.InterleaveOnlyWhenForced ||\n                               !EnableLoopInterleaving),\n      VectorizeOnlyWhenForced(Opts.VectorizeOnlyWhenForced ||\n                              !EnableLoopVectorization) {}\n\nbool LoopVectorizePass::processLoop(Loop *L) {\n  assert((EnableVPlanNativePath || L->isInnermost()) &&\n         \"VPlan-native path is not enabled. Only process inner loops.\");\n\n#ifndef NDEBUG\n  const std::string DebugLocStr = getDebugLocString(L);\n#endif /* NDEBUG */\n\n  LLVM_DEBUG(dbgs() << \"\\nLV: Checking a loop in \\\"\"\n                    << L->getHeader()->getParent()->getName() << \"\\\" from \"\n                    << DebugLocStr << \"\\n\");\n\n  LoopVectorizeHints Hints(L, InterleaveOnlyWhenForced, *ORE);\n\n  LLVM_DEBUG(\n      dbgs() << \"LV: Loop hints:\"\n             << \" force=\"\n             << (Hints.getForce() == LoopVectorizeHints::FK_Disabled\n                     ? \"disabled\"\n                     : (Hints.getForce() == LoopVectorizeHints::FK_Enabled\n                            ? \"enabled\"\n                            : \"?\"))\n             << \" width=\" << Hints.getWidth()\n             << \" unroll=\" << Hints.getInterleave() << \"\\n\");\n\n  // Function containing loop\n  Function *F = L->getHeader()->getParent();\n\n  // Looking at the diagnostic output is the only way to determine if a loop\n  // was vectorized (other than looking at the IR or machine code), so it\n  // is important to generate an optimization remark for each loop. Most of\n  // these messages are generated as OptimizationRemarkAnalysis. Remarks\n  // generated as OptimizationRemark and OptimizationRemarkMissed are\n  // less verbose reporting vectorized loops and unvectorized loops that may\n  // benefit from vectorization, respectively.\n\n  if (!Hints.allowVectorization(F, L, VectorizeOnlyWhenForced)) {\n    LLVM_DEBUG(dbgs() << \"LV: Loop hints prevent vectorization.\\n\");\n    return false;\n  }\n\n  PredicatedScalarEvolution PSE(*SE, *L);\n\n  // Check if it is legal to vectorize the loop.\n  LoopVectorizationRequirements Requirements(*ORE);\n  LoopVectorizationLegality LVL(L, PSE, DT, TTI, TLI, AA, F, GetLAA, LI, ORE,\n                                &Requirements, &Hints, DB, AC, BFI, PSI);\n  if (!LVL.canVectorize(EnableVPlanNativePath)) {\n    LLVM_DEBUG(dbgs() << \"LV: Not vectorizing: Cannot prove legality.\\n\");\n    Hints.emitRemarkWithHints();\n    return false;\n  }\n\n  // Check the function attributes and profiles to find out if this function\n  // should be optimized for size.\n  ScalarEpilogueLowering SEL = getScalarEpilogueLowering(\n      F, L, Hints, PSI, BFI, TTI, TLI, AC, LI, PSE.getSE(), DT, LVL);\n\n  // Entrance to the VPlan-native vectorization path. Outer loops are processed\n  // here. They may require CFG and instruction level transformations before\n  // even evaluating whether vectorization is profitable. Since we cannot modify\n  // the incoming IR, we need to build VPlan upfront in the vectorization\n  // pipeline.\n  if (!L->isInnermost())\n    return processLoopInVPlanNativePath(L, PSE, LI, DT, &LVL, TTI, TLI, DB, AC,\n                                        ORE, BFI, PSI, Hints);\n\n  assert(L->isInnermost() && \"Inner loop expected.\");\n\n  // Check the loop for a trip count threshold: vectorize loops with a tiny trip\n  // count by optimizing for size, to minimize overheads.\n  auto ExpectedTC = getSmallBestKnownTC(*SE, L);\n  if (ExpectedTC && *ExpectedTC < TinyTripCountVectorThreshold) {\n    LLVM_DEBUG(dbgs() << \"LV: Found a loop with a very small trip count. \"\n                      << \"This loop is worth vectorizing only if no scalar \"\n                      << \"iteration overheads are incurred.\");\n    if (Hints.getForce() == LoopVectorizeHints::FK_Enabled)\n      LLVM_DEBUG(dbgs() << \" But vectorizing was explicitly forced.\\n\");\n    else {\n      LLVM_DEBUG(dbgs() << \"\\n\");\n      SEL = CM_ScalarEpilogueNotAllowedLowTripLoop;\n    }\n  }\n\n  // Check the function attributes to see if implicit floats are allowed.\n  // FIXME: This check doesn't seem possibly correct -- what if the loop is\n  // an integer loop and the vector instructions selected are purely integer\n  // vector instructions?\n  if (F->hasFnAttribute(Attribute::NoImplicitFloat)) {\n    reportVectorizationFailure(\n        \"Can't vectorize when the NoImplicitFloat attribute is used\",\n        \"loop not vectorized due to NoImplicitFloat attribute\",\n        \"NoImplicitFloat\", ORE, L);\n    Hints.emitRemarkWithHints();\n    return false;\n  }\n\n  // Check if the target supports potentially unsafe FP vectorization.\n  // FIXME: Add a check for the type of safety issue (denormal, signaling)\n  // for the target we're vectorizing for, to make sure none of the\n  // additional fp-math flags can help.\n  if (Hints.isPotentiallyUnsafe() &&\n      TTI->isFPVectorizationPotentiallyUnsafe()) {\n    reportVectorizationFailure(\n        \"Potentially unsafe FP op prevents vectorization\",\n        \"loop not vectorized due to unsafe FP support.\",\n        \"UnsafeFP\", ORE, L);\n    Hints.emitRemarkWithHints();\n    return false;\n  }\n\n  bool UseInterleaved = TTI->enableInterleavedAccessVectorization();\n  InterleavedAccessInfo IAI(PSE, L, DT, LI, LVL.getLAI());\n\n  // If an override option has been passed in for interleaved accesses, use it.\n  if (EnableInterleavedMemAccesses.getNumOccurrences() > 0)\n    UseInterleaved = EnableInterleavedMemAccesses;\n\n  // Analyze interleaved memory accesses.\n  if (UseInterleaved) {\n    IAI.analyzeInterleaving(useMaskedInterleavedAccesses(*TTI));\n  }\n\n  // Use the cost model.\n  LoopVectorizationCostModel CM(SEL, L, PSE, LI, &LVL, *TTI, TLI, DB, AC, ORE,\n                                F, &Hints, IAI);\n  CM.collectValuesToIgnore();\n\n  // Use the planner for vectorization.\n  LoopVectorizationPlanner LVP(L, LI, TLI, TTI, &LVL, CM, IAI, PSE);\n\n  // Get user vectorization factor and interleave count.\n  ElementCount UserVF = Hints.getWidth();\n  unsigned UserIC = Hints.getInterleave();\n\n  // Plan how to best vectorize, return the best VF and its cost.\n  Optional<VectorizationFactor> MaybeVF = LVP.plan(UserVF, UserIC);\n\n  VectorizationFactor VF = VectorizationFactor::Disabled();\n  unsigned IC = 1;\n\n  if (MaybeVF) {\n    VF = *MaybeVF;\n    // Select the interleave count.\n    IC = CM.selectInterleaveCount(VF.Width, VF.Cost);\n  }\n\n  // Identify the diagnostic messages that should be produced.\n  std::pair<StringRef, std::string> VecDiagMsg, IntDiagMsg;\n  bool VectorizeLoop = true, InterleaveLoop = true;\n  if (Requirements.doesNotMeet(F, L, Hints)) {\n    LLVM_DEBUG(dbgs() << \"LV: Not vectorizing: loop did not meet vectorization \"\n                         \"requirements.\\n\");\n    Hints.emitRemarkWithHints();\n    return false;\n  }\n\n  if (VF.Width.isScalar()) {\n    LLVM_DEBUG(dbgs() << \"LV: Vectorization is possible but not beneficial.\\n\");\n    VecDiagMsg = std::make_pair(\n        \"VectorizationNotBeneficial\",\n        \"the cost-model indicates that vectorization is not beneficial\");\n    VectorizeLoop = false;\n  }\n\n  if (!MaybeVF && UserIC > 1) {\n    // Tell the user interleaving was avoided up-front, despite being explicitly\n    // requested.\n    LLVM_DEBUG(dbgs() << \"LV: Ignoring UserIC, because vectorization and \"\n                         \"interleaving should be avoided up front\\n\");\n    IntDiagMsg = std::make_pair(\n        \"InterleavingAvoided\",\n        \"Ignoring UserIC, because interleaving was avoided up front\");\n    InterleaveLoop = false;\n  } else if (IC == 1 && UserIC <= 1) {\n    // Tell the user interleaving is not beneficial.\n    LLVM_DEBUG(dbgs() << \"LV: Interleaving is not beneficial.\\n\");\n    IntDiagMsg = std::make_pair(\n        \"InterleavingNotBeneficial\",\n        \"the cost-model indicates that interleaving is not beneficial\");\n    InterleaveLoop = false;\n    if (UserIC == 1) {\n      IntDiagMsg.first = \"InterleavingNotBeneficialAndDisabled\";\n      IntDiagMsg.second +=\n          \" and is explicitly disabled or interleave count is set to 1\";\n    }\n  } else if (IC > 1 && UserIC == 1) {\n    // Tell the user interleaving is beneficial, but it explicitly disabled.\n    LLVM_DEBUG(\n        dbgs() << \"LV: Interleaving is beneficial but is explicitly disabled.\");\n    IntDiagMsg = std::make_pair(\n        \"InterleavingBeneficialButDisabled\",\n        \"the cost-model indicates that interleaving is beneficial \"\n        \"but is explicitly disabled or interleave count is set to 1\");\n    InterleaveLoop = false;\n  }\n\n  // Override IC if user provided an interleave count.\n  IC = UserIC > 0 ? UserIC : IC;\n\n  // Emit diagnostic messages, if any.\n  const char *VAPassName = Hints.vectorizeAnalysisPassName();\n  if (!VectorizeLoop && !InterleaveLoop) {\n    // Do not vectorize or interleaving the loop.\n    ORE->emit([&]() {\n      return OptimizationRemarkMissed(VAPassName, VecDiagMsg.first,\n                                      L->getStartLoc(), L->getHeader())\n             << VecDiagMsg.second;\n    });\n    ORE->emit([&]() {\n      return OptimizationRemarkMissed(LV_NAME, IntDiagMsg.first,\n                                      L->getStartLoc(), L->getHeader())\n             << IntDiagMsg.second;\n    });\n    return false;\n  } else if (!VectorizeLoop && InterleaveLoop) {\n    LLVM_DEBUG(dbgs() << \"LV: Interleave Count is \" << IC << '\\n');\n    ORE->emit([&]() {\n      return OptimizationRemarkAnalysis(VAPassName, VecDiagMsg.first,\n                                        L->getStartLoc(), L->getHeader())\n             << VecDiagMsg.second;\n    });\n  } else if (VectorizeLoop && !InterleaveLoop) {\n    LLVM_DEBUG(dbgs() << \"LV: Found a vectorizable loop (\" << VF.Width\n                      << \") in \" << DebugLocStr << '\\n');\n    ORE->emit([&]() {\n      return OptimizationRemarkAnalysis(LV_NAME, IntDiagMsg.first,\n                                        L->getStartLoc(), L->getHeader())\n             << IntDiagMsg.second;\n    });\n  } else if (VectorizeLoop && InterleaveLoop) {\n    LLVM_DEBUG(dbgs() << \"LV: Found a vectorizable loop (\" << VF.Width\n                      << \") in \" << DebugLocStr << '\\n');\n    LLVM_DEBUG(dbgs() << \"LV: Interleave Count is \" << IC << '\\n');\n  }\n\n  bool DisableRuntimeUnroll = false;\n  MDNode *OrigLoopID = L->getLoopID();\n  {\n    // Optimistically generate runtime checks. Drop them if they turn out to not\n    // be profitable. Limit the scope of Checks, so the cleanup happens\n    // immediately after vector codegeneration is done.\n    GeneratedRTChecks Checks(*PSE.getSE(), DT, LI,\n                             F->getParent()->getDataLayout());\n    if (!VF.Width.isScalar() || IC > 1)\n      Checks.Create(L, *LVL.getLAI(), PSE.getUnionPredicate());\n    LVP.setBestPlan(VF.Width, IC);\n\n    using namespace ore;\n    if (!VectorizeLoop) {\n      assert(IC > 1 && \"interleave count should not be 1 or 0\");\n      // If we decided that it is not legal to vectorize the loop, then\n      // interleave it.\n      InnerLoopUnroller Unroller(L, PSE, LI, DT, TLI, TTI, AC, ORE, IC, &LVL,\n                                 &CM, BFI, PSI, Checks);\n      LVP.executePlan(Unroller, DT);\n\n      ORE->emit([&]() {\n        return OptimizationRemark(LV_NAME, \"Interleaved\", L->getStartLoc(),\n                                  L->getHeader())\n               << \"interleaved loop (interleaved count: \"\n               << NV(\"InterleaveCount\", IC) << \")\";\n      });\n    } else {\n      // If we decided that it is *legal* to vectorize the loop, then do it.\n\n      // Consider vectorizing the epilogue too if it's profitable.\n      VectorizationFactor EpilogueVF =\n          CM.selectEpilogueVectorizationFactor(VF.Width, LVP);\n      if (EpilogueVF.Width.isVector()) {\n\n        // The first pass vectorizes the main loop and creates a scalar epilogue\n        // to be vectorized by executing the plan (potentially with a different\n        // factor) again shortly afterwards.\n        EpilogueLoopVectorizationInfo EPI(VF.Width.getKnownMinValue(), IC,\n                                          EpilogueVF.Width.getKnownMinValue(),\n                                          1);\n        EpilogueVectorizerMainLoop MainILV(L, PSE, LI, DT, TLI, TTI, AC, ORE,\n                                           EPI, &LVL, &CM, BFI, PSI, Checks);\n\n        LVP.setBestPlan(EPI.MainLoopVF, EPI.MainLoopUF);\n        LVP.executePlan(MainILV, DT);\n        ++LoopsVectorized;\n\n        simplifyLoop(L, DT, LI, SE, AC, nullptr, false /* PreserveLCSSA */);\n        formLCSSARecursively(*L, *DT, LI, SE);\n\n        // Second pass vectorizes the epilogue and adjusts the control flow\n        // edges from the first pass.\n        LVP.setBestPlan(EPI.EpilogueVF, EPI.EpilogueUF);\n        EPI.MainLoopVF = EPI.EpilogueVF;\n        EPI.MainLoopUF = EPI.EpilogueUF;\n        EpilogueVectorizerEpilogueLoop EpilogILV(L, PSE, LI, DT, TLI, TTI, AC,\n                                                 ORE, EPI, &LVL, &CM, BFI, PSI,\n                                                 Checks);\n        LVP.executePlan(EpilogILV, DT);\n        ++LoopsEpilogueVectorized;\n\n        if (!MainILV.areSafetyChecksAdded())\n          DisableRuntimeUnroll = true;\n      } else {\n        InnerLoopVectorizer LB(L, PSE, LI, DT, TLI, TTI, AC, ORE, VF.Width, IC,\n                               &LVL, &CM, BFI, PSI, Checks);\n        LVP.executePlan(LB, DT);\n        ++LoopsVectorized;\n\n        // Add metadata to disable runtime unrolling a scalar loop when there\n        // are no runtime checks about strides and memory. A scalar loop that is\n        // rarely used is not worth unrolling.\n        if (!LB.areSafetyChecksAdded())\n          DisableRuntimeUnroll = true;\n      }\n      // Report the vectorization decision.\n      ORE->emit([&]() {\n        return OptimizationRemark(LV_NAME, \"Vectorized\", L->getStartLoc(),\n                                  L->getHeader())\n               << \"vectorized loop (vectorization width: \"\n               << NV(\"VectorizationFactor\", VF.Width)\n               << \", interleaved count: \" << NV(\"InterleaveCount\", IC) << \")\";\n      });\n    }\n\n    if (ORE->allowExtraAnalysis(LV_NAME))\n      checkMixedPrecision(L, ORE);\n  }\n\n  Optional<MDNode *> RemainderLoopID =\n      makeFollowupLoopID(OrigLoopID, {LLVMLoopVectorizeFollowupAll,\n                                      LLVMLoopVectorizeFollowupEpilogue});\n  if (RemainderLoopID.hasValue()) {\n    L->setLoopID(RemainderLoopID.getValue());\n  } else {\n    if (DisableRuntimeUnroll)\n      AddRuntimeUnrollDisableMetaData(L);\n\n    // Mark the loop as already vectorized to avoid vectorizing again.\n    Hints.setAlreadyVectorized();\n  }\n\n  assert(!verifyFunction(*L->getHeader()->getParent(), &dbgs()));\n  return true;\n}\n\nLoopVectorizeResult LoopVectorizePass::runImpl(\n    Function &F, ScalarEvolution &SE_, LoopInfo &LI_, TargetTransformInfo &TTI_,\n    DominatorTree &DT_, BlockFrequencyInfo &BFI_, TargetLibraryInfo *TLI_,\n    DemandedBits &DB_, AAResults &AA_, AssumptionCache &AC_,\n    std::function<const LoopAccessInfo &(Loop &)> &GetLAA_,\n    OptimizationRemarkEmitter &ORE_, ProfileSummaryInfo *PSI_) {\n  SE = &SE_;\n  LI = &LI_;\n  TTI = &TTI_;\n  DT = &DT_;\n  BFI = &BFI_;\n  TLI = TLI_;\n  AA = &AA_;\n  AC = &AC_;\n  GetLAA = &GetLAA_;\n  DB = &DB_;\n  ORE = &ORE_;\n  PSI = PSI_;\n\n  // Don't attempt if\n  // 1. the target claims to have no vector registers, and\n  // 2. interleaving won't help ILP.\n  //\n  // The second condition is necessary because, even if the target has no\n  // vector registers, loop vectorization may still enable scalar\n  // interleaving.\n  if (!TTI->getNumberOfRegisters(TTI->getRegisterClassForType(true)) &&\n      TTI->getMaxInterleaveFactor(1) < 2)\n    return LoopVectorizeResult(false, false);\n\n  bool Changed = false, CFGChanged = false;\n\n  // The vectorizer requires loops to be in simplified form.\n  // Since simplification may add new inner loops, it has to run before the\n  // legality and profitability checks. This means running the loop vectorizer\n  // will simplify all loops, regardless of whether anything end up being\n  // vectorized.\n  for (auto &L : *LI)\n    Changed |= CFGChanged |=\n        simplifyLoop(L, DT, LI, SE, AC, nullptr, false /* PreserveLCSSA */);\n\n  // Build up a worklist of inner-loops to vectorize. This is necessary as\n  // the act of vectorizing or partially unrolling a loop creates new loops\n  // and can invalidate iterators across the loops.\n  SmallVector<Loop *, 8> Worklist;\n\n  for (Loop *L : *LI)\n    collectSupportedLoops(*L, LI, ORE, Worklist);\n\n  LoopsAnalyzed += Worklist.size();\n\n  // Now walk the identified inner loops.\n  while (!Worklist.empty()) {\n    Loop *L = Worklist.pop_back_val();\n\n    // For the inner loops we actually process, form LCSSA to simplify the\n    // transform.\n    Changed |= formLCSSARecursively(*L, *DT, LI, SE);\n\n    Changed |= CFGChanged |= processLoop(L);\n  }\n\n  // Process each loop nest in the function.\n  return LoopVectorizeResult(Changed, CFGChanged);\n}\n\nPreservedAnalyses LoopVectorizePass::run(Function &F,\n                                         FunctionAnalysisManager &AM) {\n    auto &SE = AM.getResult<ScalarEvolutionAnalysis>(F);\n    auto &LI = AM.getResult<LoopAnalysis>(F);\n    auto &TTI = AM.getResult<TargetIRAnalysis>(F);\n    auto &DT = AM.getResult<DominatorTreeAnalysis>(F);\n    auto &BFI = AM.getResult<BlockFrequencyAnalysis>(F);\n    auto &TLI = AM.getResult<TargetLibraryAnalysis>(F);\n    auto &AA = AM.getResult<AAManager>(F);\n    auto &AC = AM.getResult<AssumptionAnalysis>(F);\n    auto &DB = AM.getResult<DemandedBitsAnalysis>(F);\n    auto &ORE = AM.getResult<OptimizationRemarkEmitterAnalysis>(F);\n    MemorySSA *MSSA = EnableMSSALoopDependency\n                          ? &AM.getResult<MemorySSAAnalysis>(F).getMSSA()\n                          : nullptr;\n\n    auto &LAM = AM.getResult<LoopAnalysisManagerFunctionProxy>(F).getManager();\n    std::function<const LoopAccessInfo &(Loop &)> GetLAA =\n        [&](Loop &L) -> const LoopAccessInfo & {\n      LoopStandardAnalysisResults AR = {AA,  AC,  DT,      LI,  SE,\n                                        TLI, TTI, nullptr, MSSA};\n      return LAM.getResult<LoopAccessAnalysis>(L, AR);\n    };\n    auto &MAMProxy = AM.getResult<ModuleAnalysisManagerFunctionProxy>(F);\n    ProfileSummaryInfo *PSI =\n        MAMProxy.getCachedResult<ProfileSummaryAnalysis>(*F.getParent());\n    LoopVectorizeResult Result =\n        runImpl(F, SE, LI, TTI, DT, BFI, &TLI, DB, AA, AC, GetLAA, ORE, PSI);\n    if (!Result.MadeAnyChange)\n      return PreservedAnalyses::all();\n    PreservedAnalyses PA;\n\n    // We currently do not preserve loopinfo/dominator analyses with outer loop\n    // vectorization. Until this is addressed, mark these analyses as preserved\n    // only for non-VPlan-native path.\n    // TODO: Preserve Loop and Dominator analyses for VPlan-native path.\n    if (!EnableVPlanNativePath) {\n      PA.preserve<LoopAnalysis>();\n      PA.preserve<DominatorTreeAnalysis>();\n    }\n    PA.preserve<BasicAA>();\n    PA.preserve<GlobalsAA>();\n    if (!Result.MadeCFGChange)\n      PA.preserveSet<CFGAnalyses>();\n    return PA;\n}\n"}, "71": {"id": 71, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.h", "content": "//===- VPlan.h - Represent A Vectorizer Plan --------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file\n/// This file contains the declarations of the Vectorization Plan base classes:\n/// 1. VPBasicBlock and VPRegionBlock that inherit from a common pure virtual\n///    VPBlockBase, together implementing a Hierarchical CFG;\n/// 2. Specializations of GraphTraits that allow VPBlockBase graphs to be\n///    treated as proper graphs for generic algorithms;\n/// 3. Pure virtual VPRecipeBase serving as the base class for recipes contained\n///    within VPBasicBlocks;\n/// 4. VPInstruction, a concrete Recipe and VPUser modeling a single planned\n///    instruction;\n/// 5. The VPlan class holding a candidate for vectorization;\n/// 6. The VPlanPrinter class providing a way to print a plan in dot format;\n/// These are documented in docs/VectorizationPlan.rst.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_TRANSFORMS_VECTORIZE_VPLAN_H\n#define LLVM_TRANSFORMS_VECTORIZE_VPLAN_H\n\n#include \"VPlanLoopInfo.h\"\n#include \"VPlanValue.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DepthFirstIterator.h\"\n#include \"llvm/ADT/GraphTraits.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/SmallBitVector.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/ilist.h\"\n#include \"llvm/ADT/ilist_node.h\"\n#include \"llvm/Analysis/VectorUtils.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <map>\n#include <string>\n\nnamespace llvm {\n\nclass BasicBlock;\nclass DominatorTree;\nclass InnerLoopVectorizer;\nclass LoopInfo;\nclass raw_ostream;\nclass RecurrenceDescriptor;\nclass Value;\nclass VPBasicBlock;\nclass VPRegionBlock;\nclass VPlan;\nclass VPlanSlp;\n\n/// Returns a calculation for the total number of elements for a given \\p VF.\n/// For fixed width vectors this value is a constant, whereas for scalable\n/// vectors it is an expression determined at runtime.\nValue *getRuntimeVF(IRBuilder<> &B, Type *Ty, ElementCount VF);\n\n/// A range of powers-of-2 vectorization factors with fixed start and\n/// adjustable end. The range includes start and excludes end, e.g.,:\n/// [1, 9) = {1, 2, 4, 8}\nstruct VFRange {\n  // A power of 2.\n  const ElementCount Start;\n\n  // Need not be a power of 2. If End <= Start range is empty.\n  ElementCount End;\n\n  bool isEmpty() const {\n    return End.getKnownMinValue() <= Start.getKnownMinValue();\n  }\n\n  VFRange(const ElementCount &Start, const ElementCount &End)\n      : Start(Start), End(End) {\n    assert(Start.isScalable() == End.isScalable() &&\n           \"Both Start and End should have the same scalable flag\");\n    assert(isPowerOf2_32(Start.getKnownMinValue()) &&\n           \"Expected Start to be a power of 2\");\n  }\n};\n\nusing VPlanPtr = std::unique_ptr<VPlan>;\n\n/// In what follows, the term \"input IR\" refers to code that is fed into the\n/// vectorizer whereas the term \"output IR\" refers to code that is generated by\n/// the vectorizer.\n\n/// VPLane provides a way to access lanes in both fixed width and scalable\n/// vectors, where for the latter the lane index sometimes needs calculating\n/// as a runtime expression.\nclass VPLane {\npublic:\n  /// Kind describes how to interpret Lane.\n  enum class Kind : uint8_t {\n    /// For First, Lane is the index into the first N elements of a\n    /// fixed-vector <N x <ElTy>> or a scalable vector <vscale x N x <ElTy>>.\n    First,\n    /// For ScalableLast, Lane is the offset from the start of the last\n    /// N-element subvector in a scalable vector <vscale x N x <ElTy>>. For\n    /// example, a Lane of 0 corresponds to lane `(vscale - 1) * N`, a Lane of\n    /// 1 corresponds to `((vscale - 1) * N) + 1`, etc.\n    ScalableLast\n  };\n\nprivate:\n  /// in [0..VF)\n  unsigned Lane;\n\n  /// Indicates how the Lane should be interpreted, as described above.\n  Kind LaneKind;\n\npublic:\n  VPLane(unsigned Lane, Kind LaneKind) : Lane(Lane), LaneKind(LaneKind) {}\n\n  static VPLane getFirstLane() { return VPLane(0, VPLane::Kind::First); }\n\n  static VPLane getLastLaneForVF(const ElementCount &VF) {\n    unsigned LaneOffset = VF.getKnownMinValue() - 1;\n    Kind LaneKind;\n    if (VF.isScalable())\n      // In this case 'LaneOffset' refers to the offset from the start of the\n      // last subvector with VF.getKnownMinValue() elements.\n      LaneKind = VPLane::Kind::ScalableLast;\n    else\n      LaneKind = VPLane::Kind::First;\n    return VPLane(LaneOffset, LaneKind);\n  }\n\n  /// Returns a compile-time known value for the lane index and asserts if the\n  /// lane can only be calculated at runtime.\n  unsigned getKnownLane() const {\n    assert(LaneKind == Kind::First);\n    return Lane;\n  }\n\n  /// Returns an expression describing the lane index that can be used at\n  /// runtime.\n  Value *getAsRuntimeExpr(IRBuilder<> &Builder, const ElementCount &VF) const;\n\n  /// Returns the Kind of lane offset.\n  Kind getKind() const { return LaneKind; }\n\n  /// Returns true if this is the first lane of the whole vector.\n  bool isFirstLane() const { return Lane == 0 && LaneKind == Kind::First; }\n\n  /// Maps the lane to a cache index based on \\p VF.\n  unsigned mapToCacheIndex(const ElementCount &VF) const {\n    switch (LaneKind) {\n    case VPLane::Kind::ScalableLast:\n      assert(VF.isScalable() && Lane < VF.getKnownMinValue());\n      return VF.getKnownMinValue() + Lane;\n    default:\n      assert(Lane < VF.getKnownMinValue());\n      return Lane;\n    }\n  }\n\n  /// Returns the maxmimum number of lanes that we are able to consider\n  /// caching for \\p VF.\n  static unsigned getNumCachedLanes(const ElementCount &VF) {\n    return VF.getKnownMinValue() * (VF.isScalable() ? 2 : 1);\n  }\n};\n\n/// VPIteration represents a single point in the iteration space of the output\n/// (vectorized and/or unrolled) IR loop.\nstruct VPIteration {\n  /// in [0..UF)\n  unsigned Part;\n\n  VPLane Lane;\n\n  VPIteration(unsigned Part, unsigned Lane,\n              VPLane::Kind Kind = VPLane::Kind::First)\n      : Part(Part), Lane(Lane, Kind) {}\n\n  VPIteration(unsigned Part, const VPLane &Lane) : Part(Part), Lane(Lane) {}\n\n  bool isFirstIteration() const { return Part == 0 && Lane.isFirstLane(); }\n};\n\n/// VPTransformState holds information passed down when \"executing\" a VPlan,\n/// needed for generating the output IR.\nstruct VPTransformState {\n  VPTransformState(ElementCount VF, unsigned UF, LoopInfo *LI,\n                   DominatorTree *DT, IRBuilder<> &Builder,\n                   InnerLoopVectorizer *ILV, VPlan *Plan)\n      : VF(VF), UF(UF), Instance(), LI(LI), DT(DT), Builder(Builder), ILV(ILV),\n        Plan(Plan) {}\n\n  /// The chosen Vectorization and Unroll Factors of the loop being vectorized.\n  ElementCount VF;\n  unsigned UF;\n\n  /// Hold the indices to generate specific scalar instructions. Null indicates\n  /// that all instances are to be generated, using either scalar or vector\n  /// instructions.\n  Optional<VPIteration> Instance;\n\n  struct DataState {\n    /// A type for vectorized values in the new loop. Each value from the\n    /// original loop, when vectorized, is represented by UF vector values in\n    /// the new unrolled loop, where UF is the unroll factor.\n    typedef SmallVector<Value *, 2> PerPartValuesTy;\n\n    DenseMap<VPValue *, PerPartValuesTy> PerPartOutput;\n\n    using ScalarsPerPartValuesTy = SmallVector<SmallVector<Value *, 4>, 2>;\n    DenseMap<VPValue *, ScalarsPerPartValuesTy> PerPartScalars;\n  } Data;\n\n  /// Get the generated Value for a given VPValue and a given Part. Note that\n  /// as some Defs are still created by ILV and managed in its ValueMap, this\n  /// method will delegate the call to ILV in such cases in order to provide\n  /// callers a consistent API.\n  /// \\see set.\n  Value *get(VPValue *Def, unsigned Part);\n\n  /// Get the generated Value for a given VPValue and given Part and Lane.\n  Value *get(VPValue *Def, const VPIteration &Instance);\n\n  bool hasVectorValue(VPValue *Def, unsigned Part) {\n    auto I = Data.PerPartOutput.find(Def);\n    return I != Data.PerPartOutput.end() && Part < I->second.size() &&\n           I->second[Part];\n  }\n\n  bool hasAnyVectorValue(VPValue *Def) const {\n    return Data.PerPartOutput.find(Def) != Data.PerPartOutput.end();\n  }\n\n  bool hasScalarValue(VPValue *Def, VPIteration Instance) {\n    auto I = Data.PerPartScalars.find(Def);\n    if (I == Data.PerPartScalars.end())\n      return false;\n    unsigned CacheIdx = Instance.Lane.mapToCacheIndex(VF);\n    return Instance.Part < I->second.size() &&\n           CacheIdx < I->second[Instance.Part].size() &&\n           I->second[Instance.Part][CacheIdx];\n  }\n\n  /// Set the generated Value for a given VPValue and a given Part.\n  void set(VPValue *Def, Value *V, unsigned Part) {\n    if (!Data.PerPartOutput.count(Def)) {\n      DataState::PerPartValuesTy Entry(UF);\n      Data.PerPartOutput[Def] = Entry;\n    }\n    Data.PerPartOutput[Def][Part] = V;\n  }\n  /// Reset an existing vector value for \\p Def and a given \\p Part.\n  void reset(VPValue *Def, Value *V, unsigned Part) {\n    auto Iter = Data.PerPartOutput.find(Def);\n    assert(Iter != Data.PerPartOutput.end() &&\n           \"need to overwrite existing value\");\n    Iter->second[Part] = V;\n  }\n\n  /// Set the generated scalar \\p V for \\p Def and the given \\p Instance.\n  void set(VPValue *Def, Value *V, const VPIteration &Instance) {\n    auto Iter = Data.PerPartScalars.insert({Def, {}});\n    auto &PerPartVec = Iter.first->second;\n    while (PerPartVec.size() <= Instance.Part)\n      PerPartVec.emplace_back();\n    auto &Scalars = PerPartVec[Instance.Part];\n    unsigned CacheIdx = Instance.Lane.mapToCacheIndex(VF);\n    while (Scalars.size() <= CacheIdx)\n      Scalars.push_back(nullptr);\n    assert(!Scalars[CacheIdx] && \"should overwrite existing value\");\n    Scalars[CacheIdx] = V;\n  }\n\n  /// Reset an existing scalar value for \\p Def and a given \\p Instance.\n  void reset(VPValue *Def, Value *V, const VPIteration &Instance) {\n    auto Iter = Data.PerPartScalars.find(Def);\n    assert(Iter != Data.PerPartScalars.end() &&\n           \"need to overwrite existing value\");\n    assert(Instance.Part < Iter->second.size() &&\n           \"need to overwrite existing value\");\n    unsigned CacheIdx = Instance.Lane.mapToCacheIndex(VF);\n    assert(CacheIdx < Iter->second[Instance.Part].size() &&\n           \"need to overwrite existing value\");\n    Iter->second[Instance.Part][CacheIdx] = V;\n  }\n\n  /// Hold state information used when constructing the CFG of the output IR,\n  /// traversing the VPBasicBlocks and generating corresponding IR BasicBlocks.\n  struct CFGState {\n    /// The previous VPBasicBlock visited. Initially set to null.\n    VPBasicBlock *PrevVPBB = nullptr;\n\n    /// The previous IR BasicBlock created or used. Initially set to the new\n    /// header BasicBlock.\n    BasicBlock *PrevBB = nullptr;\n\n    /// The last IR BasicBlock in the output IR. Set to the new latch\n    /// BasicBlock, used for placing the newly created BasicBlocks.\n    BasicBlock *LastBB = nullptr;\n\n    /// A mapping of each VPBasicBlock to the corresponding BasicBlock. In case\n    /// of replication, maps the BasicBlock of the last replica created.\n    SmallDenseMap<VPBasicBlock *, BasicBlock *> VPBB2IRBB;\n\n    /// Vector of VPBasicBlocks whose terminator instruction needs to be fixed\n    /// up at the end of vector code generation.\n    SmallVector<VPBasicBlock *, 8> VPBBsToFix;\n\n    CFGState() = default;\n  } CFG;\n\n  /// Hold a pointer to LoopInfo to register new basic blocks in the loop.\n  LoopInfo *LI;\n\n  /// Hold a pointer to Dominator Tree to register new basic blocks in the loop.\n  DominatorTree *DT;\n\n  /// Hold a reference to the IRBuilder used to generate output IR code.\n  IRBuilder<> &Builder;\n\n  VPValue2ValueTy VPValue2Value;\n\n  /// Hold the canonical scalar IV of the vector loop (start=0, step=VF*UF).\n  Value *CanonicalIV = nullptr;\n\n  /// Hold the trip count of the scalar loop.\n  Value *TripCount = nullptr;\n\n  /// Hold a pointer to InnerLoopVectorizer to reuse its IR generation methods.\n  InnerLoopVectorizer *ILV;\n\n  /// Pointer to the VPlan code is generated for.\n  VPlan *Plan;\n};\n\n/// VPBlockBase is the building block of the Hierarchical Control-Flow Graph.\n/// A VPBlockBase can be either a VPBasicBlock or a VPRegionBlock.\nclass VPBlockBase {\n  friend class VPBlockUtils;\n\n  const unsigned char SubclassID; ///< Subclass identifier (for isa/dyn_cast).\n\n  /// An optional name for the block.\n  std::string Name;\n\n  /// The immediate VPRegionBlock which this VPBlockBase belongs to, or null if\n  /// it is a topmost VPBlockBase.\n  VPRegionBlock *Parent = nullptr;\n\n  /// List of predecessor blocks.\n  SmallVector<VPBlockBase *, 1> Predecessors;\n\n  /// List of successor blocks.\n  SmallVector<VPBlockBase *, 1> Successors;\n\n  /// Successor selector managed by a VPUser. For blocks with zero or one\n  /// successors, there is no operand. Otherwise there is exactly one operand\n  /// which is the branch condition.\n  VPUser CondBitUser;\n\n  /// If the block is predicated, its predicate is stored as an operand of this\n  /// VPUser to maintain the def-use relations. Otherwise there is no operand\n  /// here.\n  VPUser PredicateUser;\n\n  /// VPlan containing the block. Can only be set on the entry block of the\n  /// plan.\n  VPlan *Plan = nullptr;\n\n  /// Add \\p Successor as the last successor to this block.\n  void appendSuccessor(VPBlockBase *Successor) {\n    assert(Successor && \"Cannot add nullptr successor!\");\n    Successors.push_back(Successor);\n  }\n\n  /// Add \\p Predecessor as the last predecessor to this block.\n  void appendPredecessor(VPBlockBase *Predecessor) {\n    assert(Predecessor && \"Cannot add nullptr predecessor!\");\n    Predecessors.push_back(Predecessor);\n  }\n\n  /// Remove \\p Predecessor from the predecessors of this block.\n  void removePredecessor(VPBlockBase *Predecessor) {\n    auto Pos = find(Predecessors, Predecessor);\n    assert(Pos && \"Predecessor does not exist\");\n    Predecessors.erase(Pos);\n  }\n\n  /// Remove \\p Successor from the successors of this block.\n  void removeSuccessor(VPBlockBase *Successor) {\n    auto Pos = find(Successors, Successor);\n    assert(Pos && \"Successor does not exist\");\n    Successors.erase(Pos);\n  }\n\nprotected:\n  VPBlockBase(const unsigned char SC, const std::string &N)\n      : SubclassID(SC), Name(N) {}\n\npublic:\n  /// An enumeration for keeping track of the concrete subclass of VPBlockBase\n  /// that are actually instantiated. Values of this enumeration are kept in the\n  /// SubclassID field of the VPBlockBase objects. They are used for concrete\n  /// type identification.\n  using VPBlockTy = enum { VPBasicBlockSC, VPRegionBlockSC };\n\n  using VPBlocksTy = SmallVectorImpl<VPBlockBase *>;\n\n  virtual ~VPBlockBase() = default;\n\n  const std::string &getName() const { return Name; }\n\n  void setName(const Twine &newName) { Name = newName.str(); }\n\n  /// \\return an ID for the concrete type of this object.\n  /// This is used to implement the classof checks. This should not be used\n  /// for any other purpose, as the values may change as LLVM evolves.\n  unsigned getVPBlockID() const { return SubclassID; }\n\n  VPRegionBlock *getParent() { return Parent; }\n  const VPRegionBlock *getParent() const { return Parent; }\n\n  /// \\return A pointer to the plan containing the current block.\n  VPlan *getPlan();\n  const VPlan *getPlan() const;\n\n  /// Sets the pointer of the plan containing the block. The block must be the\n  /// entry block into the VPlan.\n  void setPlan(VPlan *ParentPlan);\n\n  void setParent(VPRegionBlock *P) { Parent = P; }\n\n  /// \\return the VPBasicBlock that is the entry of this VPBlockBase,\n  /// recursively, if the latter is a VPRegionBlock. Otherwise, if this\n  /// VPBlockBase is a VPBasicBlock, it is returned.\n  const VPBasicBlock *getEntryBasicBlock() const;\n  VPBasicBlock *getEntryBasicBlock();\n\n  /// \\return the VPBasicBlock that is the exit of this VPBlockBase,\n  /// recursively, if the latter is a VPRegionBlock. Otherwise, if this\n  /// VPBlockBase is a VPBasicBlock, it is returned.\n  const VPBasicBlock *getExitBasicBlock() const;\n  VPBasicBlock *getExitBasicBlock();\n\n  const VPBlocksTy &getSuccessors() const { return Successors; }\n  VPBlocksTy &getSuccessors() { return Successors; }\n\n  const VPBlocksTy &getPredecessors() const { return Predecessors; }\n  VPBlocksTy &getPredecessors() { return Predecessors; }\n\n  /// \\return the successor of this VPBlockBase if it has a single successor.\n  /// Otherwise return a null pointer.\n  VPBlockBase *getSingleSuccessor() const {\n    return (Successors.size() == 1 ? *Successors.begin() : nullptr);\n  }\n\n  /// \\return the predecessor of this VPBlockBase if it has a single\n  /// predecessor. Otherwise return a null pointer.\n  VPBlockBase *getSinglePredecessor() const {\n    return (Predecessors.size() == 1 ? *Predecessors.begin() : nullptr);\n  }\n\n  size_t getNumSuccessors() const { return Successors.size(); }\n  size_t getNumPredecessors() const { return Predecessors.size(); }\n\n  /// An Enclosing Block of a block B is any block containing B, including B\n  /// itself. \\return the closest enclosing block starting from \"this\", which\n  /// has successors. \\return the root enclosing block if all enclosing blocks\n  /// have no successors.\n  VPBlockBase *getEnclosingBlockWithSuccessors();\n\n  /// \\return the closest enclosing block starting from \"this\", which has\n  /// predecessors. \\return the root enclosing block if all enclosing blocks\n  /// have no predecessors.\n  VPBlockBase *getEnclosingBlockWithPredecessors();\n\n  /// \\return the successors either attached directly to this VPBlockBase or, if\n  /// this VPBlockBase is the exit block of a VPRegionBlock and has no\n  /// successors of its own, search recursively for the first enclosing\n  /// VPRegionBlock that has successors and return them. If no such\n  /// VPRegionBlock exists, return the (empty) successors of the topmost\n  /// VPBlockBase reached.\n  const VPBlocksTy &getHierarchicalSuccessors() {\n    return getEnclosingBlockWithSuccessors()->getSuccessors();\n  }\n\n  /// \\return the hierarchical successor of this VPBlockBase if it has a single\n  /// hierarchical successor. Otherwise return a null pointer.\n  VPBlockBase *getSingleHierarchicalSuccessor() {\n    return getEnclosingBlockWithSuccessors()->getSingleSuccessor();\n  }\n\n  /// \\return the predecessors either attached directly to this VPBlockBase or,\n  /// if this VPBlockBase is the entry block of a VPRegionBlock and has no\n  /// predecessors of its own, search recursively for the first enclosing\n  /// VPRegionBlock that has predecessors and return them. If no such\n  /// VPRegionBlock exists, return the (empty) predecessors of the topmost\n  /// VPBlockBase reached.\n  const VPBlocksTy &getHierarchicalPredecessors() {\n    return getEnclosingBlockWithPredecessors()->getPredecessors();\n  }\n\n  /// \\return the hierarchical predecessor of this VPBlockBase if it has a\n  /// single hierarchical predecessor. Otherwise return a null pointer.\n  VPBlockBase *getSingleHierarchicalPredecessor() {\n    return getEnclosingBlockWithPredecessors()->getSinglePredecessor();\n  }\n\n  /// \\return the condition bit selecting the successor.\n  VPValue *getCondBit();\n  /// \\return the condition bit selecting the successor.\n  const VPValue *getCondBit() const;\n  /// Set the condition bit selecting the successor.\n  void setCondBit(VPValue *CV);\n\n  /// \\return the block's predicate.\n  VPValue *getPredicate();\n  /// \\return the block's predicate.\n  const VPValue *getPredicate() const;\n  /// Set the block's predicate.\n  void setPredicate(VPValue *Pred);\n\n  /// Set a given VPBlockBase \\p Successor as the single successor of this\n  /// VPBlockBase. This VPBlockBase is not added as predecessor of \\p Successor.\n  /// This VPBlockBase must have no successors.\n  void setOneSuccessor(VPBlockBase *Successor) {\n    assert(Successors.empty() && \"Setting one successor when others exist.\");\n    appendSuccessor(Successor);\n  }\n\n  /// Set two given VPBlockBases \\p IfTrue and \\p IfFalse to be the two\n  /// successors of this VPBlockBase. \\p Condition is set as the successor\n  /// selector. This VPBlockBase is not added as predecessor of \\p IfTrue or \\p\n  /// IfFalse. This VPBlockBase must have no successors.\n  void setTwoSuccessors(VPBlockBase *IfTrue, VPBlockBase *IfFalse,\n                        VPValue *Condition) {\n    assert(Successors.empty() && \"Setting two successors when others exist.\");\n    assert(Condition && \"Setting two successors without condition!\");\n    setCondBit(Condition);\n    appendSuccessor(IfTrue);\n    appendSuccessor(IfFalse);\n  }\n\n  /// Set each VPBasicBlock in \\p NewPreds as predecessor of this VPBlockBase.\n  /// This VPBlockBase must have no predecessors. This VPBlockBase is not added\n  /// as successor of any VPBasicBlock in \\p NewPreds.\n  void setPredecessors(ArrayRef<VPBlockBase *> NewPreds) {\n    assert(Predecessors.empty() && \"Block predecessors already set.\");\n    for (auto *Pred : NewPreds)\n      appendPredecessor(Pred);\n  }\n\n  /// Remove all the predecessor of this block.\n  void clearPredecessors() { Predecessors.clear(); }\n\n  /// Remove all the successors of this block and set to null its condition bit\n  void clearSuccessors() {\n    Successors.clear();\n    setCondBit(nullptr);\n  }\n\n  /// The method which generates the output IR that correspond to this\n  /// VPBlockBase, thereby \"executing\" the VPlan.\n  virtual void execute(struct VPTransformState *State) = 0;\n\n  /// Delete all blocks reachable from a given VPBlockBase, inclusive.\n  static void deleteCFG(VPBlockBase *Entry);\n\n  void printAsOperand(raw_ostream &OS, bool PrintType) const {\n    OS << getName();\n  }\n\n  void print(raw_ostream &OS) const {\n    // TODO: Only printing VPBB name for now since we only have dot printing\n    // support for VPInstructions/Recipes.\n    printAsOperand(OS, false);\n  }\n\n  /// Return true if it is legal to hoist instructions into this block.\n  bool isLegalToHoistInto() {\n    // There are currently no constraints that prevent an instruction to be\n    // hoisted into a VPBlockBase.\n    return true;\n  }\n\n  /// Replace all operands of VPUsers in the block with \\p NewValue and also\n  /// replaces all uses of VPValues defined in the block with NewValue.\n  virtual void dropAllReferences(VPValue *NewValue) = 0;\n};\n\n/// VPRecipeBase is a base class modeling a sequence of one or more output IR\n/// instructions. VPRecipeBase owns the the VPValues it defines through VPDef\n/// and is responsible for deleting its defined values. Single-value\n/// VPRecipeBases that also inherit from VPValue must make sure to inherit from\n/// VPRecipeBase before VPValue.\nclass VPRecipeBase : public ilist_node_with_parent<VPRecipeBase, VPBasicBlock>,\n                     public VPDef,\n                     public VPUser {\n  friend VPBasicBlock;\n  friend class VPBlockUtils;\n\n\n  /// Each VPRecipe belongs to a single VPBasicBlock.\n  VPBasicBlock *Parent = nullptr;\n\npublic:\n  VPRecipeBase(const unsigned char SC, ArrayRef<VPValue *> Operands)\n      : VPDef(SC), VPUser(Operands) {}\n\n  template <typename IterT>\n  VPRecipeBase(const unsigned char SC, iterator_range<IterT> Operands)\n      : VPDef(SC), VPUser(Operands) {}\n  virtual ~VPRecipeBase() = default;\n\n  /// \\return the VPBasicBlock which this VPRecipe belongs to.\n  VPBasicBlock *getParent() { return Parent; }\n  const VPBasicBlock *getParent() const { return Parent; }\n\n  /// The method which generates the output IR instructions that correspond to\n  /// this VPRecipe, thereby \"executing\" the VPlan.\n  virtual void execute(struct VPTransformState &State) = 0;\n\n  /// Insert an unlinked recipe into a basic block immediately before\n  /// the specified recipe.\n  void insertBefore(VPRecipeBase *InsertPos);\n\n  /// Insert an unlinked Recipe into a basic block immediately after\n  /// the specified Recipe.\n  void insertAfter(VPRecipeBase *InsertPos);\n\n  /// Unlink this recipe from its current VPBasicBlock and insert it into\n  /// the VPBasicBlock that MovePos lives in, right after MovePos.\n  void moveAfter(VPRecipeBase *MovePos);\n\n  /// Unlink this recipe and insert into BB before I.\n  ///\n  /// \\pre I is a valid iterator into BB.\n  void moveBefore(VPBasicBlock &BB, iplist<VPRecipeBase>::iterator I);\n\n  /// This method unlinks 'this' from the containing basic block, but does not\n  /// delete it.\n  void removeFromParent();\n\n  /// This method unlinks 'this' from the containing basic block and deletes it.\n  ///\n  /// \\returns an iterator pointing to the element after the erased one\n  iplist<VPRecipeBase>::iterator eraseFromParent();\n\n  /// Returns the underlying instruction, if the recipe is a VPValue or nullptr\n  /// otherwise.\n  Instruction *getUnderlyingInstr() {\n    return cast<Instruction>(getVPValue()->getUnderlyingValue());\n  }\n  const Instruction *getUnderlyingInstr() const {\n    return cast<Instruction>(getVPValue()->getUnderlyingValue());\n  }\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    // All VPDefs are also VPRecipeBases.\n    return true;\n  }\n};\n\ninline bool VPUser::classof(const VPDef *Def) {\n  return Def->getVPDefID() == VPRecipeBase::VPInstructionSC ||\n         Def->getVPDefID() == VPRecipeBase::VPWidenSC ||\n         Def->getVPDefID() == VPRecipeBase::VPWidenCallSC ||\n         Def->getVPDefID() == VPRecipeBase::VPWidenSelectSC ||\n         Def->getVPDefID() == VPRecipeBase::VPWidenGEPSC ||\n         Def->getVPDefID() == VPRecipeBase::VPBlendSC ||\n         Def->getVPDefID() == VPRecipeBase::VPInterleaveSC ||\n         Def->getVPDefID() == VPRecipeBase::VPReplicateSC ||\n         Def->getVPDefID() == VPRecipeBase::VPReductionSC ||\n         Def->getVPDefID() == VPRecipeBase::VPBranchOnMaskSC ||\n         Def->getVPDefID() == VPRecipeBase::VPWidenMemoryInstructionSC;\n}\n\n/// This is a concrete Recipe that models a single VPlan-level instruction.\n/// While as any Recipe it may generate a sequence of IR instructions when\n/// executed, these instructions would always form a single-def expression as\n/// the VPInstruction is also a single def-use vertex.\nclass VPInstruction : public VPRecipeBase, public VPValue {\n  friend class VPlanSlp;\n\npublic:\n  /// VPlan opcodes, extending LLVM IR with idiomatics instructions.\n  enum {\n    Not = Instruction::OtherOpsEnd + 1,\n    ICmpULE,\n    SLPLoad,\n    SLPStore,\n    ActiveLaneMask,\n  };\n\nprivate:\n  typedef unsigned char OpcodeTy;\n  OpcodeTy Opcode;\n\n  /// Utility method serving execute(): generates a single instance of the\n  /// modeled instruction.\n  void generateInstruction(VPTransformState &State, unsigned Part);\n\nprotected:\n  void setUnderlyingInstr(Instruction *I) { setUnderlyingValue(I); }\n\npublic:\n  VPInstruction(unsigned Opcode, ArrayRef<VPValue *> Operands)\n      : VPRecipeBase(VPRecipeBase::VPInstructionSC, Operands),\n        VPValue(VPValue::VPVInstructionSC, nullptr, this), Opcode(Opcode) {}\n\n  VPInstruction(unsigned Opcode, ArrayRef<VPInstruction *> Operands)\n      : VPRecipeBase(VPRecipeBase::VPInstructionSC, {}),\n        VPValue(VPValue::VPVInstructionSC, nullptr, this), Opcode(Opcode) {\n    for (auto *I : Operands)\n      addOperand(I->getVPValue());\n  }\n\n  VPInstruction(unsigned Opcode, std::initializer_list<VPValue *> Operands)\n      : VPInstruction(Opcode, ArrayRef<VPValue *>(Operands)) {}\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPValue *V) {\n    return V->getVPValueID() == VPValue::VPVInstructionSC;\n  }\n\n  VPInstruction *clone() const {\n    SmallVector<VPValue *, 2> Operands(operands());\n    return new VPInstruction(Opcode, Operands);\n  }\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *R) {\n    return R->getVPDefID() == VPRecipeBase::VPInstructionSC;\n  }\n\n  unsigned getOpcode() const { return Opcode; }\n\n  /// Generate the instruction.\n  /// TODO: We currently execute only per-part unless a specific instance is\n  /// provided.\n  void execute(VPTransformState &State) override;\n\n  /// Print the VPInstruction to \\p O.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n\n  /// Print the VPInstruction to dbgs() (for debugging).\n  void dump() const;\n\n  /// Return true if this instruction may modify memory.\n  bool mayWriteToMemory() const {\n    // TODO: we can use attributes of the called function to rule out memory\n    //       modifications.\n    return Opcode == Instruction::Store || Opcode == Instruction::Call ||\n           Opcode == Instruction::Invoke || Opcode == SLPStore;\n  }\n\n  bool hasResult() const {\n    // CallInst may or may not have a result, depending on the called function.\n    // Conservatively return calls have results for now.\n    switch (getOpcode()) {\n    case Instruction::Ret:\n    case Instruction::Br:\n    case Instruction::Store:\n    case Instruction::Switch:\n    case Instruction::IndirectBr:\n    case Instruction::Resume:\n    case Instruction::CatchRet:\n    case Instruction::Unreachable:\n    case Instruction::Fence:\n    case Instruction::AtomicRMW:\n      return false;\n    default:\n      return true;\n    }\n  }\n};\n\n/// VPWidenRecipe is a recipe for producing a copy of vector type its\n/// ingredient. This recipe covers most of the traditional vectorization cases\n/// where each ingredient transforms into a vectorized version of itself.\nclass VPWidenRecipe : public VPRecipeBase, public VPValue {\npublic:\n  template <typename IterT>\n  VPWidenRecipe(Instruction &I, iterator_range<IterT> Operands)\n      : VPRecipeBase(VPRecipeBase::VPWidenSC, Operands),\n        VPValue(VPValue::VPVWidenSC, &I, this) {}\n\n  ~VPWidenRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenSC;\n  }\n  static inline bool classof(const VPValue *V) {\n    return V->getVPValueID() == VPValue::VPVWidenSC;\n  }\n\n  /// Produce widened copies of all Ingredients.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// A recipe for widening Call instructions.\nclass VPWidenCallRecipe : public VPRecipeBase, public VPValue {\n\npublic:\n  template <typename IterT>\n  VPWidenCallRecipe(CallInst &I, iterator_range<IterT> CallArguments)\n      : VPRecipeBase(VPRecipeBase::VPWidenCallSC, CallArguments),\n        VPValue(VPValue::VPVWidenCallSC, &I, this) {}\n\n  ~VPWidenCallRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenCallSC;\n  }\n\n  /// Produce a widened version of the call instruction.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// A recipe for widening select instructions.\nclass VPWidenSelectRecipe : public VPRecipeBase, public VPValue {\n\n  /// Is the condition of the select loop invariant?\n  bool InvariantCond;\n\npublic:\n  template <typename IterT>\n  VPWidenSelectRecipe(SelectInst &I, iterator_range<IterT> Operands,\n                      bool InvariantCond)\n      : VPRecipeBase(VPRecipeBase::VPWidenSelectSC, Operands),\n        VPValue(VPValue::VPVWidenSelectSC, &I, this),\n        InvariantCond(InvariantCond) {}\n\n  ~VPWidenSelectRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenSelectSC;\n  }\n\n  /// Produce a widened version of the select instruction.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// A recipe for handling GEP instructions.\nclass VPWidenGEPRecipe : public VPRecipeBase, public VPValue {\n  bool IsPtrLoopInvariant;\n  SmallBitVector IsIndexLoopInvariant;\n\npublic:\n  template <typename IterT>\n  VPWidenGEPRecipe(GetElementPtrInst *GEP, iterator_range<IterT> Operands)\n      : VPRecipeBase(VPRecipeBase::VPWidenGEPSC, Operands),\n        VPValue(VPWidenGEPSC, GEP, this),\n        IsIndexLoopInvariant(GEP->getNumIndices(), false) {}\n\n  template <typename IterT>\n  VPWidenGEPRecipe(GetElementPtrInst *GEP, iterator_range<IterT> Operands,\n                   Loop *OrigLoop)\n      : VPRecipeBase(VPRecipeBase::VPWidenGEPSC, Operands),\n        VPValue(VPValue::VPVWidenGEPSC, GEP, this),\n        IsIndexLoopInvariant(GEP->getNumIndices(), false) {\n    IsPtrLoopInvariant = OrigLoop->isLoopInvariant(GEP->getPointerOperand());\n    for (auto Index : enumerate(GEP->indices()))\n      IsIndexLoopInvariant[Index.index()] =\n          OrigLoop->isLoopInvariant(Index.value().get());\n  }\n  ~VPWidenGEPRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenGEPSC;\n  }\n\n  /// Generate the gep nodes.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// A recipe for handling phi nodes of integer and floating-point inductions,\n/// producing their vector and scalar values.\nclass VPWidenIntOrFpInductionRecipe : public VPRecipeBase {\n  PHINode *IV;\n\npublic:\n  VPWidenIntOrFpInductionRecipe(PHINode *IV, VPValue *Start, Instruction *Cast,\n                                TruncInst *Trunc = nullptr)\n      : VPRecipeBase(VPWidenIntOrFpInductionSC, {Start}), IV(IV) {\n    if (Trunc)\n      new VPValue(Trunc, this);\n    else\n      new VPValue(IV, this);\n\n    if (Cast)\n      new VPValue(Cast, this);\n  }\n  ~VPWidenIntOrFpInductionRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenIntOrFpInductionSC;\n  }\n\n  /// Generate the vectorized and scalarized versions of the phi node as\n  /// needed by their users.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n\n  /// Returns the start value of the induction.\n  VPValue *getStartValue() { return getOperand(0); }\n\n  /// Returns the cast VPValue, if one is attached, or nullptr otherwise.\n  VPValue *getCastValue() {\n    if (getNumDefinedValues() != 2)\n      return nullptr;\n    return getVPValue(1);\n  }\n\n  /// Returns the first defined value as TruncInst, if it is one or nullptr\n  /// otherwise.\n  TruncInst *getTruncInst() {\n    return dyn_cast_or_null<TruncInst>(getVPValue(0)->getUnderlyingValue());\n  }\n  const TruncInst *getTruncInst() const {\n    return dyn_cast_or_null<TruncInst>(getVPValue(0)->getUnderlyingValue());\n  }\n};\n\n/// A recipe for handling all phi nodes except for integer and FP inductions.\n/// For reduction PHIs, RdxDesc must point to the corresponding recurrence\n/// descriptor and the start value is the first operand of the recipe.\n/// In the VPlan native path, all incoming VPValues & VPBasicBlock pairs are\n/// managed in the recipe directly.\nclass VPWidenPHIRecipe : public VPRecipeBase, public VPValue {\n  /// Descriptor for a reduction PHI.\n  RecurrenceDescriptor *RdxDesc = nullptr;\n\n  /// List of incoming blocks. Only used in the VPlan native path.\n  SmallVector<VPBasicBlock *, 2> IncomingBlocks;\n\npublic:\n  /// Create a new VPWidenPHIRecipe for the reduction \\p Phi described by \\p\n  /// RdxDesc.\n  VPWidenPHIRecipe(PHINode *Phi, RecurrenceDescriptor &RdxDesc, VPValue &Start)\n      : VPWidenPHIRecipe(Phi) {\n    this->RdxDesc = &RdxDesc;\n    addOperand(&Start);\n  }\n\n  /// Create a VPWidenPHIRecipe for \\p Phi\n  VPWidenPHIRecipe(PHINode *Phi)\n      : VPRecipeBase(VPWidenPHISC, {}),\n        VPValue(VPValue::VPVWidenPHISC, Phi, this) {}\n  ~VPWidenPHIRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenPHISC;\n  }\n  static inline bool classof(const VPValue *V) {\n    return V->getVPValueID() == VPValue::VPVWidenPHISC;\n  }\n\n  /// Generate the phi/select nodes.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n\n  /// Returns the start value of the phi, if it is a reduction.\n  VPValue *getStartValue() {\n    return getNumOperands() == 0 ? nullptr : getOperand(0);\n  }\n\n  /// Adds a pair (\\p IncomingV, \\p IncomingBlock) to the phi.\n  void addIncoming(VPValue *IncomingV, VPBasicBlock *IncomingBlock) {\n    addOperand(IncomingV);\n    IncomingBlocks.push_back(IncomingBlock);\n  }\n\n  /// Returns the \\p I th incoming VPValue.\n  VPValue *getIncomingValue(unsigned I) { return getOperand(I); }\n\n  /// Returns the \\p I th incoming VPBasicBlock.\n  VPBasicBlock *getIncomingBlock(unsigned I) { return IncomingBlocks[I]; }\n};\n\n/// A recipe for vectorizing a phi-node as a sequence of mask-based select\n/// instructions.\nclass VPBlendRecipe : public VPRecipeBase, public VPValue {\n  PHINode *Phi;\n\npublic:\n  /// The blend operation is a User of the incoming values and of their\n  /// respective masks, ordered [I0, M0, I1, M1, ...]. Note that a single value\n  /// might be incoming with a full mask for which there is no VPValue.\n  VPBlendRecipe(PHINode *Phi, ArrayRef<VPValue *> Operands)\n      : VPRecipeBase(VPBlendSC, Operands),\n        VPValue(VPValue::VPVBlendSC, Phi, this), Phi(Phi) {\n    assert(Operands.size() > 0 &&\n           ((Operands.size() == 1) || (Operands.size() % 2 == 0)) &&\n           \"Expected either a single incoming value or a positive even number \"\n           \"of operands\");\n  }\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPBlendSC;\n  }\n\n  /// Return the number of incoming values, taking into account that a single\n  /// incoming value has no mask.\n  unsigned getNumIncomingValues() const { return (getNumOperands() + 1) / 2; }\n\n  /// Return incoming value number \\p Idx.\n  VPValue *getIncomingValue(unsigned Idx) const { return getOperand(Idx * 2); }\n\n  /// Return mask number \\p Idx.\n  VPValue *getMask(unsigned Idx) const { return getOperand(Idx * 2 + 1); }\n\n  /// Generate the phi/select nodes.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// VPInterleaveRecipe is a recipe for transforming an interleave group of load\n/// or stores into one wide load/store and shuffles. The first operand of a\n/// VPInterleave recipe is the address, followed by the stored values, followed\n/// by an optional mask.\nclass VPInterleaveRecipe : public VPRecipeBase {\n  const InterleaveGroup<Instruction> *IG;\n\n  bool HasMask = false;\n\npublic:\n  VPInterleaveRecipe(const InterleaveGroup<Instruction> *IG, VPValue *Addr,\n                     ArrayRef<VPValue *> StoredValues, VPValue *Mask)\n      : VPRecipeBase(VPInterleaveSC, {Addr}), IG(IG) {\n    for (unsigned i = 0; i < IG->getFactor(); ++i)\n      if (Instruction *I = IG->getMember(i)) {\n        if (I->getType()->isVoidTy())\n          continue;\n        new VPValue(I, this);\n      }\n\n    for (auto *SV : StoredValues)\n      addOperand(SV);\n    if (Mask) {\n      HasMask = true;\n      addOperand(Mask);\n    }\n  }\n  ~VPInterleaveRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPInterleaveSC;\n  }\n\n  /// Return the address accessed by this recipe.\n  VPValue *getAddr() const {\n    return getOperand(0); // Address is the 1st, mandatory operand.\n  }\n\n  /// Return the mask used by this recipe. Note that a full mask is represented\n  /// by a nullptr.\n  VPValue *getMask() const {\n    // Mask is optional and therefore the last, currently 2nd operand.\n    return HasMask ? getOperand(getNumOperands() - 1) : nullptr;\n  }\n\n  /// Return the VPValues stored by this interleave group. If it is a load\n  /// interleave group, return an empty ArrayRef.\n  ArrayRef<VPValue *> getStoredValues() const {\n    // The first operand is the address, followed by the stored values, followed\n    // by an optional mask.\n    return ArrayRef<VPValue *>(op_begin(), getNumOperands())\n        .slice(1, getNumOperands() - (HasMask ? 2 : 1));\n  }\n\n  /// Generate the wide load or store, and shuffles.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n\n  const InterleaveGroup<Instruction> *getInterleaveGroup() { return IG; }\n};\n\n/// A recipe to represent inloop reduction operations, performing a reduction on\n/// a vector operand into a scalar value, and adding the result to a chain.\n/// The Operands are {ChainOp, VecOp, [Condition]}.\nclass VPReductionRecipe : public VPRecipeBase, public VPValue {\n  /// The recurrence decriptor for the reduction in question.\n  RecurrenceDescriptor *RdxDesc;\n  /// Pointer to the TTI, needed to create the target reduction\n  const TargetTransformInfo *TTI;\n\npublic:\n  VPReductionRecipe(RecurrenceDescriptor *R, Instruction *I, VPValue *ChainOp,\n                    VPValue *VecOp, VPValue *CondOp,\n                    const TargetTransformInfo *TTI)\n      : VPRecipeBase(VPRecipeBase::VPReductionSC, {ChainOp, VecOp}),\n        VPValue(VPValue::VPVReductionSC, I, this), RdxDesc(R), TTI(TTI) {\n    if (CondOp)\n      addOperand(CondOp);\n  }\n\n  ~VPReductionRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPValue *V) {\n    return V->getVPValueID() == VPValue::VPVReductionSC;\n  }\n\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPReductionSC;\n  }\n\n  /// Generate the reduction in the loop\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n\n  /// The VPValue of the scalar Chain being accumulated.\n  VPValue *getChainOp() const { return getOperand(0); }\n  /// The VPValue of the vector value to be reduced.\n  VPValue *getVecOp() const { return getOperand(1); }\n  /// The VPValue of the condition for the block.\n  VPValue *getCondOp() const {\n    return getNumOperands() > 2 ? getOperand(2) : nullptr;\n  }\n};\n\n/// VPReplicateRecipe replicates a given instruction producing multiple scalar\n/// copies of the original scalar type, one per lane, instead of producing a\n/// single copy of widened type for all lanes. If the instruction is known to be\n/// uniform only one copy, per lane zero, will be generated.\nclass VPReplicateRecipe : public VPRecipeBase, public VPValue {\n  /// Indicator if only a single replica per lane is needed.\n  bool IsUniform;\n\n  /// Indicator if the replicas are also predicated.\n  bool IsPredicated;\n\n  /// Indicator if the scalar values should also be packed into a vector.\n  bool AlsoPack;\n\npublic:\n  template <typename IterT>\n  VPReplicateRecipe(Instruction *I, iterator_range<IterT> Operands,\n                    bool IsUniform, bool IsPredicated = false)\n      : VPRecipeBase(VPReplicateSC, Operands), VPValue(VPVReplicateSC, I, this),\n        IsUniform(IsUniform), IsPredicated(IsPredicated) {\n    // Retain the previous behavior of predicateInstructions(), where an\n    // insert-element of a predicated instruction got hoisted into the\n    // predicated basic block iff it was its only user. This is achieved by\n    // having predicated instructions also pack their values into a vector by\n    // default unless they have a replicated user which uses their scalar value.\n    AlsoPack = IsPredicated && !I->use_empty();\n  }\n\n  ~VPReplicateRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPReplicateSC;\n  }\n\n  static inline bool classof(const VPValue *V) {\n    return V->getVPValueID() == VPValue::VPVReplicateSC;\n  }\n\n  /// Generate replicas of the desired Ingredient. Replicas will be generated\n  /// for all parts and lanes unless a specific part and lane are specified in\n  /// the \\p State.\n  void execute(VPTransformState &State) override;\n\n  void setAlsoPack(bool Pack) { AlsoPack = Pack; }\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n\n  bool isUniform() const { return IsUniform; }\n\n  bool isPacked() const { return AlsoPack; }\n};\n\n/// A recipe for generating conditional branches on the bits of a mask.\nclass VPBranchOnMaskRecipe : public VPRecipeBase {\npublic:\n  VPBranchOnMaskRecipe(VPValue *BlockInMask)\n      : VPRecipeBase(VPBranchOnMaskSC, {}) {\n    if (BlockInMask) // nullptr means all-one mask.\n      addOperand(BlockInMask);\n  }\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPBranchOnMaskSC;\n  }\n\n  /// Generate the extraction of the appropriate bit from the block mask and the\n  /// conditional branch.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override {\n    O << \" +\\n\" << Indent << \"\\\"BRANCH-ON-MASK \";\n    if (VPValue *Mask = getMask())\n      Mask->printAsOperand(O, SlotTracker);\n    else\n      O << \" All-One\";\n    O << \"\\\\l\\\"\";\n  }\n\n  /// Return the mask used by this recipe. Note that a full mask is represented\n  /// by a nullptr.\n  VPValue *getMask() const {\n    assert(getNumOperands() <= 1 && \"should have either 0 or 1 operands\");\n    // Mask is optional.\n    return getNumOperands() == 1 ? getOperand(0) : nullptr;\n  }\n};\n\n/// VPPredInstPHIRecipe is a recipe for generating the phi nodes needed when\n/// control converges back from a Branch-on-Mask. The phi nodes are needed in\n/// order to merge values that are set under such a branch and feed their uses.\n/// The phi nodes can be scalar or vector depending on the users of the value.\n/// This recipe works in concert with VPBranchOnMaskRecipe.\nclass VPPredInstPHIRecipe : public VPRecipeBase, public VPValue {\npublic:\n  /// Construct a VPPredInstPHIRecipe given \\p PredInst whose value needs a phi\n  /// nodes after merging back from a Branch-on-Mask.\n  VPPredInstPHIRecipe(VPValue *PredV)\n      : VPRecipeBase(VPPredInstPHISC, PredV),\n        VPValue(VPValue::VPVPredInstPHI, nullptr, this) {}\n  ~VPPredInstPHIRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPPredInstPHISC;\n  }\n\n  /// Generates phi nodes for live-outs as needed to retain SSA form.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// A Recipe for widening load/store operations.\n/// The recipe uses the following VPValues:\n/// - For load: Address, optional mask\n/// - For store: Address, stored value, optional mask\n/// TODO: We currently execute only per-part unless a specific instance is\n/// provided.\nclass VPWidenMemoryInstructionRecipe : public VPRecipeBase {\n  Instruction &Ingredient;\n\n  void setMask(VPValue *Mask) {\n    if (!Mask)\n      return;\n    addOperand(Mask);\n  }\n\n  bool isMasked() const {\n    return isStore() ? getNumOperands() == 3 : getNumOperands() == 2;\n  }\n\npublic:\n  VPWidenMemoryInstructionRecipe(LoadInst &Load, VPValue *Addr, VPValue *Mask)\n      : VPRecipeBase(VPWidenMemoryInstructionSC, {Addr}), Ingredient(Load) {\n    new VPValue(VPValue::VPVMemoryInstructionSC, &Load, this);\n    setMask(Mask);\n  }\n\n  VPWidenMemoryInstructionRecipe(StoreInst &Store, VPValue *Addr,\n                                 VPValue *StoredValue, VPValue *Mask)\n      : VPRecipeBase(VPWidenMemoryInstructionSC, {Addr, StoredValue}),\n        Ingredient(Store) {\n    setMask(Mask);\n  }\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenMemoryInstructionSC;\n  }\n\n  /// Return the address accessed by this recipe.\n  VPValue *getAddr() const {\n    return getOperand(0); // Address is the 1st, mandatory operand.\n  }\n\n  /// Return the mask used by this recipe. Note that a full mask is represented\n  /// by a nullptr.\n  VPValue *getMask() const {\n    // Mask is optional and therefore the last operand.\n    return isMasked() ? getOperand(getNumOperands() - 1) : nullptr;\n  }\n\n  /// Returns true if this recipe is a store.\n  bool isStore() const { return isa<StoreInst>(Ingredient); }\n\n  /// Return the address accessed by this recipe.\n  VPValue *getStoredValue() const {\n    assert(isStore() && \"Stored value only available for store instructions\");\n    return getOperand(1); // Stored value is the 2nd, mandatory operand.\n  }\n\n  /// Generate the wide load/store.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// A Recipe for widening the canonical induction variable of the vector loop.\nclass VPWidenCanonicalIVRecipe : public VPRecipeBase {\npublic:\n  VPWidenCanonicalIVRecipe() : VPRecipeBase(VPWidenCanonicalIVSC, {}) {\n    new VPValue(nullptr, this);\n  }\n\n  ~VPWidenCanonicalIVRecipe() override = default;\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPDef *D) {\n    return D->getVPDefID() == VPRecipeBase::VPWidenCanonicalIVSC;\n  }\n\n  /// Generate a canonical vector induction variable of the vector loop, with\n  /// start = {<Part*VF, Part*VF+1, ..., Part*VF+VF-1> for 0 <= Part < UF}, and\n  /// step = <VF*UF, VF*UF, ..., VF*UF>.\n  void execute(VPTransformState &State) override;\n\n  /// Print the recipe.\n  void print(raw_ostream &O, const Twine &Indent,\n             VPSlotTracker &SlotTracker) const override;\n};\n\n/// VPBasicBlock serves as the leaf of the Hierarchical Control-Flow Graph. It\n/// holds a sequence of zero or more VPRecipe's each representing a sequence of\n/// output IR instructions.\nclass VPBasicBlock : public VPBlockBase {\npublic:\n  using RecipeListTy = iplist<VPRecipeBase>;\n\nprivate:\n  /// The VPRecipes held in the order of output instructions to generate.\n  RecipeListTy Recipes;\n\npublic:\n  VPBasicBlock(const Twine &Name = \"\", VPRecipeBase *Recipe = nullptr)\n      : VPBlockBase(VPBasicBlockSC, Name.str()) {\n    if (Recipe)\n      appendRecipe(Recipe);\n  }\n\n  ~VPBasicBlock() override {\n    while (!Recipes.empty())\n      Recipes.pop_back();\n  }\n\n  /// Instruction iterators...\n  using iterator = RecipeListTy::iterator;\n  using const_iterator = RecipeListTy::const_iterator;\n  using reverse_iterator = RecipeListTy::reverse_iterator;\n  using const_reverse_iterator = RecipeListTy::const_reverse_iterator;\n\n  //===--------------------------------------------------------------------===//\n  /// Recipe iterator methods\n  ///\n  inline iterator begin() { return Recipes.begin(); }\n  inline const_iterator begin() const { return Recipes.begin(); }\n  inline iterator end() { return Recipes.end(); }\n  inline const_iterator end() const { return Recipes.end(); }\n\n  inline reverse_iterator rbegin() { return Recipes.rbegin(); }\n  inline const_reverse_iterator rbegin() const { return Recipes.rbegin(); }\n  inline reverse_iterator rend() { return Recipes.rend(); }\n  inline const_reverse_iterator rend() const { return Recipes.rend(); }\n\n  inline size_t size() const { return Recipes.size(); }\n  inline bool empty() const { return Recipes.empty(); }\n  inline const VPRecipeBase &front() const { return Recipes.front(); }\n  inline VPRecipeBase &front() { return Recipes.front(); }\n  inline const VPRecipeBase &back() const { return Recipes.back(); }\n  inline VPRecipeBase &back() { return Recipes.back(); }\n\n  /// Returns a reference to the list of recipes.\n  RecipeListTy &getRecipeList() { return Recipes; }\n\n  /// Returns a pointer to a member of the recipe list.\n  static RecipeListTy VPBasicBlock::*getSublistAccess(VPRecipeBase *) {\n    return &VPBasicBlock::Recipes;\n  }\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPBlockBase *V) {\n    return V->getVPBlockID() == VPBlockBase::VPBasicBlockSC;\n  }\n\n  void insert(VPRecipeBase *Recipe, iterator InsertPt) {\n    assert(Recipe && \"No recipe to append.\");\n    assert(!Recipe->Parent && \"Recipe already in VPlan\");\n    Recipe->Parent = this;\n    Recipes.insert(InsertPt, Recipe);\n  }\n\n  /// Augment the existing recipes of a VPBasicBlock with an additional\n  /// \\p Recipe as the last recipe.\n  void appendRecipe(VPRecipeBase *Recipe) { insert(Recipe, end()); }\n\n  /// The method which generates the output IR instructions that correspond to\n  /// this VPBasicBlock, thereby \"executing\" the VPlan.\n  void execute(struct VPTransformState *State) override;\n\n  /// Return the position of the first non-phi node recipe in the block.\n  iterator getFirstNonPhi();\n\n  void dropAllReferences(VPValue *NewValue) override;\n\nprivate:\n  /// Create an IR BasicBlock to hold the output instructions generated by this\n  /// VPBasicBlock, and return it. Update the CFGState accordingly.\n  BasicBlock *createEmptyBasicBlock(VPTransformState::CFGState &CFG);\n};\n\n/// VPRegionBlock represents a collection of VPBasicBlocks and VPRegionBlocks\n/// which form a Single-Entry-Single-Exit subgraph of the output IR CFG.\n/// A VPRegionBlock may indicate that its contents are to be replicated several\n/// times. This is designed to support predicated scalarization, in which a\n/// scalar if-then code structure needs to be generated VF * UF times. Having\n/// this replication indicator helps to keep a single model for multiple\n/// candidate VF's. The actual replication takes place only once the desired VF\n/// and UF have been determined.\nclass VPRegionBlock : public VPBlockBase {\n  /// Hold the Single Entry of the SESE region modelled by the VPRegionBlock.\n  VPBlockBase *Entry;\n\n  /// Hold the Single Exit of the SESE region modelled by the VPRegionBlock.\n  VPBlockBase *Exit;\n\n  /// An indicator whether this region is to generate multiple replicated\n  /// instances of output IR corresponding to its VPBlockBases.\n  bool IsReplicator;\n\npublic:\n  VPRegionBlock(VPBlockBase *Entry, VPBlockBase *Exit,\n                const std::string &Name = \"\", bool IsReplicator = false)\n      : VPBlockBase(VPRegionBlockSC, Name), Entry(Entry), Exit(Exit),\n        IsReplicator(IsReplicator) {\n    assert(Entry->getPredecessors().empty() && \"Entry block has predecessors.\");\n    assert(Exit->getSuccessors().empty() && \"Exit block has successors.\");\n    Entry->setParent(this);\n    Exit->setParent(this);\n  }\n  VPRegionBlock(const std::string &Name = \"\", bool IsReplicator = false)\n      : VPBlockBase(VPRegionBlockSC, Name), Entry(nullptr), Exit(nullptr),\n        IsReplicator(IsReplicator) {}\n\n  ~VPRegionBlock() override {\n    if (Entry) {\n      VPValue DummyValue;\n      Entry->dropAllReferences(&DummyValue);\n      deleteCFG(Entry);\n    }\n  }\n\n  /// Method to support type inquiry through isa, cast, and dyn_cast.\n  static inline bool classof(const VPBlockBase *V) {\n    return V->getVPBlockID() == VPBlockBase::VPRegionBlockSC;\n  }\n\n  const VPBlockBase *getEntry() const { return Entry; }\n  VPBlockBase *getEntry() { return Entry; }\n\n  /// Set \\p EntryBlock as the entry VPBlockBase of this VPRegionBlock. \\p\n  /// EntryBlock must have no predecessors.\n  void setEntry(VPBlockBase *EntryBlock) {\n    assert(EntryBlock->getPredecessors().empty() &&\n           \"Entry block cannot have predecessors.\");\n    Entry = EntryBlock;\n    EntryBlock->setParent(this);\n  }\n\n  // FIXME: DominatorTreeBase is doing 'A->getParent()->front()'. 'front' is a\n  // specific interface of llvm::Function, instead of using\n  // GraphTraints::getEntryNode. We should add a new template parameter to\n  // DominatorTreeBase representing the Graph type.\n  VPBlockBase &front() const { return *Entry; }\n\n  const VPBlockBase *getExit() const { return Exit; }\n  VPBlockBase *getExit() { return Exit; }\n\n  /// Set \\p ExitBlock as the exit VPBlockBase of this VPRegionBlock. \\p\n  /// ExitBlock must have no successors.\n  void setExit(VPBlockBase *ExitBlock) {\n    assert(ExitBlock->getSuccessors().empty() &&\n           \"Exit block cannot have successors.\");\n    Exit = ExitBlock;\n    ExitBlock->setParent(this);\n  }\n\n  /// An indicator whether this region is to generate multiple replicated\n  /// instances of output IR corresponding to its VPBlockBases.\n  bool isReplicator() const { return IsReplicator; }\n\n  /// The method which generates the output IR instructions that correspond to\n  /// this VPRegionBlock, thereby \"executing\" the VPlan.\n  void execute(struct VPTransformState *State) override;\n\n  void dropAllReferences(VPValue *NewValue) override;\n};\n\n//===----------------------------------------------------------------------===//\n// GraphTraits specializations for VPlan Hierarchical Control-Flow Graphs     //\n//===----------------------------------------------------------------------===//\n\n// The following set of template specializations implement GraphTraits to treat\n// any VPBlockBase as a node in a graph of VPBlockBases. It's important to note\n// that VPBlockBase traits don't recurse into VPRegioBlocks, i.e., if the\n// VPBlockBase is a VPRegionBlock, this specialization provides access to its\n// successors/predecessors but not to the blocks inside the region.\n\ntemplate <> struct GraphTraits<VPBlockBase *> {\n  using NodeRef = VPBlockBase *;\n  using ChildIteratorType = SmallVectorImpl<VPBlockBase *>::iterator;\n\n  static NodeRef getEntryNode(NodeRef N) { return N; }\n\n  static inline ChildIteratorType child_begin(NodeRef N) {\n    return N->getSuccessors().begin();\n  }\n\n  static inline ChildIteratorType child_end(NodeRef N) {\n    return N->getSuccessors().end();\n  }\n};\n\ntemplate <> struct GraphTraits<const VPBlockBase *> {\n  using NodeRef = const VPBlockBase *;\n  using ChildIteratorType = SmallVectorImpl<VPBlockBase *>::const_iterator;\n\n  static NodeRef getEntryNode(NodeRef N) { return N; }\n\n  static inline ChildIteratorType child_begin(NodeRef N) {\n    return N->getSuccessors().begin();\n  }\n\n  static inline ChildIteratorType child_end(NodeRef N) {\n    return N->getSuccessors().end();\n  }\n};\n\n// Inverse order specialization for VPBasicBlocks. Predecessors are used instead\n// of successors for the inverse traversal.\ntemplate <> struct GraphTraits<Inverse<VPBlockBase *>> {\n  using NodeRef = VPBlockBase *;\n  using ChildIteratorType = SmallVectorImpl<VPBlockBase *>::iterator;\n\n  static NodeRef getEntryNode(Inverse<NodeRef> B) { return B.Graph; }\n\n  static inline ChildIteratorType child_begin(NodeRef N) {\n    return N->getPredecessors().begin();\n  }\n\n  static inline ChildIteratorType child_end(NodeRef N) {\n    return N->getPredecessors().end();\n  }\n};\n\n// The following set of template specializations implement GraphTraits to\n// treat VPRegionBlock as a graph and recurse inside its nodes. It's important\n// to note that the blocks inside the VPRegionBlock are treated as VPBlockBases\n// (i.e., no dyn_cast is performed, VPBlockBases specialization is used), so\n// there won't be automatic recursion into other VPBlockBases that turn to be\n// VPRegionBlocks.\n\ntemplate <>\nstruct GraphTraits<VPRegionBlock *> : public GraphTraits<VPBlockBase *> {\n  using GraphRef = VPRegionBlock *;\n  using nodes_iterator = df_iterator<NodeRef>;\n\n  static NodeRef getEntryNode(GraphRef N) { return N->getEntry(); }\n\n  static nodes_iterator nodes_begin(GraphRef N) {\n    return nodes_iterator::begin(N->getEntry());\n  }\n\n  static nodes_iterator nodes_end(GraphRef N) {\n    // df_iterator::end() returns an empty iterator so the node used doesn't\n    // matter.\n    return nodes_iterator::end(N);\n  }\n};\n\ntemplate <>\nstruct GraphTraits<const VPRegionBlock *>\n    : public GraphTraits<const VPBlockBase *> {\n  using GraphRef = const VPRegionBlock *;\n  using nodes_iterator = df_iterator<NodeRef>;\n\n  static NodeRef getEntryNode(GraphRef N) { return N->getEntry(); }\n\n  static nodes_iterator nodes_begin(GraphRef N) {\n    return nodes_iterator::begin(N->getEntry());\n  }\n\n  static nodes_iterator nodes_end(GraphRef N) {\n    // df_iterator::end() returns an empty iterator so the node used doesn't\n    // matter.\n    return nodes_iterator::end(N);\n  }\n};\n\ntemplate <>\nstruct GraphTraits<Inverse<VPRegionBlock *>>\n    : public GraphTraits<Inverse<VPBlockBase *>> {\n  using GraphRef = VPRegionBlock *;\n  using nodes_iterator = df_iterator<NodeRef>;\n\n  static NodeRef getEntryNode(Inverse<GraphRef> N) {\n    return N.Graph->getExit();\n  }\n\n  static nodes_iterator nodes_begin(GraphRef N) {\n    return nodes_iterator::begin(N->getExit());\n  }\n\n  static nodes_iterator nodes_end(GraphRef N) {\n    // df_iterator::end() returns an empty iterator so the node used doesn't\n    // matter.\n    return nodes_iterator::end(N);\n  }\n};\n\n/// VPlan models a candidate for vectorization, encoding various decisions take\n/// to produce efficient output IR, including which branches, basic-blocks and\n/// output IR instructions to generate, and their cost. VPlan holds a\n/// Hierarchical-CFG of VPBasicBlocks and VPRegionBlocks rooted at an Entry\n/// VPBlock.\nclass VPlan {\n  friend class VPlanPrinter;\n  friend class VPSlotTracker;\n\n  /// Hold the single entry to the Hierarchical CFG of the VPlan.\n  VPBlockBase *Entry;\n\n  /// Holds the VFs applicable to this VPlan.\n  SmallSetVector<ElementCount, 2> VFs;\n\n  /// Holds the name of the VPlan, for printing.\n  std::string Name;\n\n  /// Holds all the external definitions created for this VPlan.\n  // TODO: Introduce a specific representation for external definitions in\n  // VPlan. External definitions must be immutable and hold a pointer to its\n  // underlying IR that will be used to implement its structural comparison\n  // (operators '==' and '<').\n  SmallPtrSet<VPValue *, 16> VPExternalDefs;\n\n  /// Represents the backedge taken count of the original loop, for folding\n  /// the tail.\n  VPValue *BackedgeTakenCount = nullptr;\n\n  /// Holds a mapping between Values and their corresponding VPValue inside\n  /// VPlan.\n  Value2VPValueTy Value2VPValue;\n\n  /// Contains all VPValues that been allocated by addVPValue directly and need\n  /// to be free when the plan's destructor is called.\n  SmallVector<VPValue *, 16> VPValuesToFree;\n\n  /// Holds the VPLoopInfo analysis for this VPlan.\n  VPLoopInfo VPLInfo;\n\npublic:\n  VPlan(VPBlockBase *Entry = nullptr) : Entry(Entry) {\n    if (Entry)\n      Entry->setPlan(this);\n  }\n\n  ~VPlan() {\n    if (Entry) {\n      VPValue DummyValue;\n      for (VPBlockBase *Block : depth_first(Entry))\n        Block->dropAllReferences(&DummyValue);\n\n      VPBlockBase::deleteCFG(Entry);\n    }\n    for (VPValue *VPV : VPValuesToFree)\n      delete VPV;\n    if (BackedgeTakenCount)\n      delete BackedgeTakenCount;\n    for (VPValue *Def : VPExternalDefs)\n      delete Def;\n  }\n\n  /// Generate the IR code for this VPlan.\n  void execute(struct VPTransformState *State);\n\n  VPBlockBase *getEntry() { return Entry; }\n  const VPBlockBase *getEntry() const { return Entry; }\n\n  VPBlockBase *setEntry(VPBlockBase *Block) {\n    Entry = Block;\n    Block->setPlan(this);\n    return Entry;\n  }\n\n  /// The backedge taken count of the original loop.\n  VPValue *getOrCreateBackedgeTakenCount() {\n    if (!BackedgeTakenCount)\n      BackedgeTakenCount = new VPValue();\n    return BackedgeTakenCount;\n  }\n\n  void addVF(ElementCount VF) { VFs.insert(VF); }\n\n  bool hasVF(ElementCount VF) { return VFs.count(VF); }\n\n  const std::string &getName() const { return Name; }\n\n  void setName(const Twine &newName) { Name = newName.str(); }\n\n  /// Add \\p VPVal to the pool of external definitions if it's not already\n  /// in the pool.\n  void addExternalDef(VPValue *VPVal) {\n    VPExternalDefs.insert(VPVal);\n  }\n\n  void addVPValue(Value *V) {\n    assert(V && \"Trying to add a null Value to VPlan\");\n    assert(!Value2VPValue.count(V) && \"Value already exists in VPlan\");\n    VPValue *VPV = new VPValue(V);\n    Value2VPValue[V] = VPV;\n    VPValuesToFree.push_back(VPV);\n  }\n\n  void addVPValue(Value *V, VPValue *VPV) {\n    assert(V && \"Trying to add a null Value to VPlan\");\n    assert(!Value2VPValue.count(V) && \"Value already exists in VPlan\");\n    Value2VPValue[V] = VPV;\n  }\n\n  VPValue *getVPValue(Value *V) {\n    assert(V && \"Trying to get the VPValue of a null Value\");\n    assert(Value2VPValue.count(V) && \"Value does not exist in VPlan\");\n    return Value2VPValue[V];\n  }\n\n  VPValue *getOrAddVPValue(Value *V) {\n    assert(V && \"Trying to get or add the VPValue of a null Value\");\n    if (!Value2VPValue.count(V))\n      addVPValue(V);\n    return getVPValue(V);\n  }\n\n  void removeVPValueFor(Value *V) { Value2VPValue.erase(V); }\n\n  /// Return the VPLoopInfo analysis for this VPlan.\n  VPLoopInfo &getVPLoopInfo() { return VPLInfo; }\n  const VPLoopInfo &getVPLoopInfo() const { return VPLInfo; }\n\n  /// Dump the plan to stderr (for debugging).\n  void dump() const;\n\n  /// Returns a range mapping the values the range \\p Operands to their\n  /// corresponding VPValues.\n  iterator_range<mapped_iterator<Use *, std::function<VPValue *(Value *)>>>\n  mapToVPValues(User::op_range Operands) {\n    std::function<VPValue *(Value *)> Fn = [this](Value *Op) {\n      return getOrAddVPValue(Op);\n    };\n    return map_range(Operands, Fn);\n  }\n\nprivate:\n  /// Add to the given dominator tree the header block and every new basic block\n  /// that was created between it and the latch block, inclusive.\n  static void updateDominatorTree(DominatorTree *DT, BasicBlock *LoopLatchBB,\n                                  BasicBlock *LoopPreHeaderBB,\n                                  BasicBlock *LoopExitBB);\n};\n\n/// VPlanPrinter prints a given VPlan to a given output stream. The printing is\n/// indented and follows the dot format.\nclass VPlanPrinter {\n  friend inline raw_ostream &operator<<(raw_ostream &OS, const VPlan &Plan);\n  friend inline raw_ostream &operator<<(raw_ostream &OS,\n                                        const struct VPlanIngredient &I);\n\nprivate:\n  raw_ostream &OS;\n  const VPlan &Plan;\n  unsigned Depth = 0;\n  unsigned TabWidth = 2;\n  std::string Indent;\n  unsigned BID = 0;\n  SmallDenseMap<const VPBlockBase *, unsigned> BlockID;\n\n  VPSlotTracker SlotTracker;\n\n  VPlanPrinter(raw_ostream &O, const VPlan &P)\n      : OS(O), Plan(P), SlotTracker(&P) {}\n\n  /// Handle indentation.\n  void bumpIndent(int b) { Indent = std::string((Depth += b) * TabWidth, ' '); }\n\n  /// Print a given \\p Block of the Plan.\n  void dumpBlock(const VPBlockBase *Block);\n\n  /// Print the information related to the CFG edges going out of a given\n  /// \\p Block, followed by printing the successor blocks themselves.\n  void dumpEdges(const VPBlockBase *Block);\n\n  /// Print a given \\p BasicBlock, including its VPRecipes, followed by printing\n  /// its successor blocks.\n  void dumpBasicBlock(const VPBasicBlock *BasicBlock);\n\n  /// Print a given \\p Region of the Plan.\n  void dumpRegion(const VPRegionBlock *Region);\n\n  unsigned getOrCreateBID(const VPBlockBase *Block) {\n    return BlockID.count(Block) ? BlockID[Block] : BlockID[Block] = BID++;\n  }\n\n  const Twine getOrCreateName(const VPBlockBase *Block);\n\n  const Twine getUID(const VPBlockBase *Block);\n\n  /// Print the information related to a CFG edge between two VPBlockBases.\n  void drawEdge(const VPBlockBase *From, const VPBlockBase *To, bool Hidden,\n                const Twine &Label);\n\n  void dump();\n\n  static void printAsIngredient(raw_ostream &O, const Value *V);\n};\n\nstruct VPlanIngredient {\n  const Value *V;\n\n  VPlanIngredient(const Value *V) : V(V) {}\n};\n\ninline raw_ostream &operator<<(raw_ostream &OS, const VPlanIngredient &I) {\n  VPlanPrinter::printAsIngredient(OS, I.V);\n  return OS;\n}\n\ninline raw_ostream &operator<<(raw_ostream &OS, const VPlan &Plan) {\n  VPlanPrinter Printer(OS, Plan);\n  Printer.dump();\n  return OS;\n}\n\n//===----------------------------------------------------------------------===//\n// VPlan Utilities\n//===----------------------------------------------------------------------===//\n\n/// Class that provides utilities for VPBlockBases in VPlan.\nclass VPBlockUtils {\npublic:\n  VPBlockUtils() = delete;\n\n  /// Insert disconnected VPBlockBase \\p NewBlock after \\p BlockPtr. Add \\p\n  /// NewBlock as successor of \\p BlockPtr and \\p BlockPtr as predecessor of \\p\n  /// NewBlock, and propagate \\p BlockPtr parent to \\p NewBlock. If \\p BlockPtr\n  /// has more than one successor, its conditional bit is propagated to \\p\n  /// NewBlock. \\p NewBlock must have neither successors nor predecessors.\n  static void insertBlockAfter(VPBlockBase *NewBlock, VPBlockBase *BlockPtr) {\n    assert(NewBlock->getSuccessors().empty() &&\n           \"Can't insert new block with successors.\");\n    // TODO: move successors from BlockPtr to NewBlock when this functionality\n    // is necessary. For now, setBlockSingleSuccessor will assert if BlockPtr\n    // already has successors.\n    BlockPtr->setOneSuccessor(NewBlock);\n    NewBlock->setPredecessors({BlockPtr});\n    NewBlock->setParent(BlockPtr->getParent());\n  }\n\n  /// Insert disconnected VPBlockBases \\p IfTrue and \\p IfFalse after \\p\n  /// BlockPtr. Add \\p IfTrue and \\p IfFalse as succesors of \\p BlockPtr and \\p\n  /// BlockPtr as predecessor of \\p IfTrue and \\p IfFalse. Propagate \\p BlockPtr\n  /// parent to \\p IfTrue and \\p IfFalse. \\p Condition is set as the successor\n  /// selector. \\p BlockPtr must have no successors and \\p IfTrue and \\p IfFalse\n  /// must have neither successors nor predecessors.\n  static void insertTwoBlocksAfter(VPBlockBase *IfTrue, VPBlockBase *IfFalse,\n                                   VPValue *Condition, VPBlockBase *BlockPtr) {\n    assert(IfTrue->getSuccessors().empty() &&\n           \"Can't insert IfTrue with successors.\");\n    assert(IfFalse->getSuccessors().empty() &&\n           \"Can't insert IfFalse with successors.\");\n    BlockPtr->setTwoSuccessors(IfTrue, IfFalse, Condition);\n    IfTrue->setPredecessors({BlockPtr});\n    IfFalse->setPredecessors({BlockPtr});\n    IfTrue->setParent(BlockPtr->getParent());\n    IfFalse->setParent(BlockPtr->getParent());\n  }\n\n  /// Connect VPBlockBases \\p From and \\p To bi-directionally. Append \\p To to\n  /// the successors of \\p From and \\p From to the predecessors of \\p To. Both\n  /// VPBlockBases must have the same parent, which can be null. Both\n  /// VPBlockBases can be already connected to other VPBlockBases.\n  static void connectBlocks(VPBlockBase *From, VPBlockBase *To) {\n    assert((From->getParent() == To->getParent()) &&\n           \"Can't connect two block with different parents\");\n    assert(From->getNumSuccessors() < 2 &&\n           \"Blocks can't have more than two successors.\");\n    From->appendSuccessor(To);\n    To->appendPredecessor(From);\n  }\n\n  /// Disconnect VPBlockBases \\p From and \\p To bi-directionally. Remove \\p To\n  /// from the successors of \\p From and \\p From from the predecessors of \\p To.\n  static void disconnectBlocks(VPBlockBase *From, VPBlockBase *To) {\n    assert(To && \"Successor to disconnect is null.\");\n    From->removeSuccessor(To);\n    To->removePredecessor(From);\n  }\n\n  /// Returns true if the edge \\p FromBlock -> \\p ToBlock is a back-edge.\n  static bool isBackEdge(const VPBlockBase *FromBlock,\n                         const VPBlockBase *ToBlock, const VPLoopInfo *VPLI) {\n    assert(FromBlock->getParent() == ToBlock->getParent() &&\n           FromBlock->getParent() && \"Must be in same region\");\n    const VPLoop *FromLoop = VPLI->getLoopFor(FromBlock);\n    const VPLoop *ToLoop = VPLI->getLoopFor(ToBlock);\n    if (!FromLoop || !ToLoop || FromLoop != ToLoop)\n      return false;\n\n    // A back-edge is a branch from the loop latch to its header.\n    return ToLoop->isLoopLatch(FromBlock) && ToBlock == ToLoop->getHeader();\n  }\n\n  /// Returns true if \\p Block is a loop latch\n  static bool blockIsLoopLatch(const VPBlockBase *Block,\n                               const VPLoopInfo *VPLInfo) {\n    if (const VPLoop *ParentVPL = VPLInfo->getLoopFor(Block))\n      return ParentVPL->isLoopLatch(Block);\n\n    return false;\n  }\n\n  /// Count and return the number of succesors of \\p PredBlock excluding any\n  /// backedges.\n  static unsigned countSuccessorsNoBE(VPBlockBase *PredBlock,\n                                      VPLoopInfo *VPLI) {\n    unsigned Count = 0;\n    for (VPBlockBase *SuccBlock : PredBlock->getSuccessors()) {\n      if (!VPBlockUtils::isBackEdge(PredBlock, SuccBlock, VPLI))\n        Count++;\n    }\n    return Count;\n  }\n};\n\nclass VPInterleavedAccessInfo {\n  DenseMap<VPInstruction *, InterleaveGroup<VPInstruction> *>\n      InterleaveGroupMap;\n\n  /// Type for mapping of instruction based interleave groups to VPInstruction\n  /// interleave groups\n  using Old2NewTy = DenseMap<InterleaveGroup<Instruction> *,\n                             InterleaveGroup<VPInstruction> *>;\n\n  /// Recursively \\p Region and populate VPlan based interleave groups based on\n  /// \\p IAI.\n  void visitRegion(VPRegionBlock *Region, Old2NewTy &Old2New,\n                   InterleavedAccessInfo &IAI);\n  /// Recursively traverse \\p Block and populate VPlan based interleave groups\n  /// based on \\p IAI.\n  void visitBlock(VPBlockBase *Block, Old2NewTy &Old2New,\n                  InterleavedAccessInfo &IAI);\n\npublic:\n  VPInterleavedAccessInfo(VPlan &Plan, InterleavedAccessInfo &IAI);\n\n  ~VPInterleavedAccessInfo() {\n    SmallPtrSet<InterleaveGroup<VPInstruction> *, 4> DelSet;\n    // Avoid releasing a pointer twice.\n    for (auto &I : InterleaveGroupMap)\n      DelSet.insert(I.second);\n    for (auto *Ptr : DelSet)\n      delete Ptr;\n  }\n\n  /// Get the interleave group that \\p Instr belongs to.\n  ///\n  /// \\returns nullptr if doesn't have such group.\n  InterleaveGroup<VPInstruction> *\n  getInterleaveGroup(VPInstruction *Instr) const {\n    return InterleaveGroupMap.lookup(Instr);\n  }\n};\n\n/// Class that maps (parts of) an existing VPlan to trees of combined\n/// VPInstructions.\nclass VPlanSlp {\n  enum class OpMode { Failed, Load, Opcode };\n\n  /// A DenseMapInfo implementation for using SmallVector<VPValue *, 4> as\n  /// DenseMap keys.\n  struct BundleDenseMapInfo {\n    static SmallVector<VPValue *, 4> getEmptyKey() {\n      return {reinterpret_cast<VPValue *>(-1)};\n    }\n\n    static SmallVector<VPValue *, 4> getTombstoneKey() {\n      return {reinterpret_cast<VPValue *>(-2)};\n    }\n\n    static unsigned getHashValue(const SmallVector<VPValue *, 4> &V) {\n      return static_cast<unsigned>(hash_combine_range(V.begin(), V.end()));\n    }\n\n    static bool isEqual(const SmallVector<VPValue *, 4> &LHS,\n                        const SmallVector<VPValue *, 4> &RHS) {\n      return LHS == RHS;\n    }\n  };\n\n  /// Mapping of values in the original VPlan to a combined VPInstruction.\n  DenseMap<SmallVector<VPValue *, 4>, VPInstruction *, BundleDenseMapInfo>\n      BundleToCombined;\n\n  VPInterleavedAccessInfo &IAI;\n\n  /// Basic block to operate on. For now, only instructions in a single BB are\n  /// considered.\n  const VPBasicBlock &BB;\n\n  /// Indicates whether we managed to combine all visited instructions or not.\n  bool CompletelySLP = true;\n\n  /// Width of the widest combined bundle in bits.\n  unsigned WidestBundleBits = 0;\n\n  using MultiNodeOpTy =\n      typename std::pair<VPInstruction *, SmallVector<VPValue *, 4>>;\n\n  // Input operand bundles for the current multi node. Each multi node operand\n  // bundle contains values not matching the multi node's opcode. They will\n  // be reordered in reorderMultiNodeOps, once we completed building a\n  // multi node.\n  SmallVector<MultiNodeOpTy, 4> MultiNodeOps;\n\n  /// Indicates whether we are building a multi node currently.\n  bool MultiNodeActive = false;\n\n  /// Check if we can vectorize Operands together.\n  bool areVectorizable(ArrayRef<VPValue *> Operands) const;\n\n  /// Add combined instruction \\p New for the bundle \\p Operands.\n  void addCombined(ArrayRef<VPValue *> Operands, VPInstruction *New);\n\n  /// Indicate we hit a bundle we failed to combine. Returns nullptr for now.\n  VPInstruction *markFailed();\n\n  /// Reorder operands in the multi node to maximize sequential memory access\n  /// and commutative operations.\n  SmallVector<MultiNodeOpTy, 4> reorderMultiNodeOps();\n\n  /// Choose the best candidate to use for the lane after \\p Last. The set of\n  /// candidates to choose from are values with an opcode matching \\p Last's\n  /// or loads consecutive to \\p Last.\n  std::pair<OpMode, VPValue *> getBest(OpMode Mode, VPValue *Last,\n                                       SmallPtrSetImpl<VPValue *> &Candidates,\n                                       VPInterleavedAccessInfo &IAI);\n\n  /// Print bundle \\p Values to dbgs().\n  void dumpBundle(ArrayRef<VPValue *> Values);\n\npublic:\n  VPlanSlp(VPInterleavedAccessInfo &IAI, VPBasicBlock &BB) : IAI(IAI), BB(BB) {}\n\n  ~VPlanSlp() = default;\n\n  /// Tries to build an SLP tree rooted at \\p Operands and returns a\n  /// VPInstruction combining \\p Operands, if they can be combined.\n  VPInstruction *buildGraph(ArrayRef<VPValue *> Operands);\n\n  /// Return the width of the widest combined bundle in bits.\n  unsigned getWidestBundleBits() const { return WidestBundleBits; }\n\n  /// Return true if all visited instruction can be combined.\n  bool isCompletelySLP() const { return CompletelySLP; }\n};\n} // end namespace llvm\n\n#endif // LLVM_TRANSFORMS_VECTORIZE_VPLAN_H\n"}}, "reports": [{"events": [{"location": {"col": 10, "file": 24, "line": 1317}, "message": "destructor '~ReductionFlags' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h", "reportHash": "5a9f39b1704f643b7807e7b5b06d4436", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 24, "line": 1317}, "message": "move constructor 'ReductionFlags' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h", "reportHash": "3393d327b59ea74f035906c8077580f0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 68, "line": 120}, "message": "destructor '~LoopVectorizeResult' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Transforms/Vectorize/LoopVectorize.h", "reportHash": "7d48ab0e399530d161109000b771bcf2", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 68, "line": 120}, "message": "move constructor 'LoopVectorizeResult' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Transforms/Vectorize/LoopVectorize.h", "reportHash": "40629a1c1704da991b916e7f90005873", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 69, "line": 177}, "message": "move constructor 'VectorizationFactor' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationPlanner.h", "reportHash": "96facb740bf805e96e1d43c3b1d38264", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 873}, "message": "destructor '~InnerLoopUnroller' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "90656132a559935013cee8c8e1cb55af", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 926}, "message": "destructor '~InnerLoopAndEpilogueVectorizer' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "d232263d5eb9ad8c627646027c238101", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 964}, "message": "destructor '~EpilogueVectorizerMainLoop' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "80ab592e7b2631bd1e58842dcd0a269b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 993}, "message": "destructor '~EpilogueVectorizerEpilogueLoop' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "796f7063ed0b3566340e499044b47467", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 70, "line": 1260}, "message": "default constructor 'RegisterUsage' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "5dc3ff9a5eedf0da7a049e57758dc59e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 47, "file": 70, "line": 1492}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ab234e6ca0d0b59526c97e68dc84796e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 70, "line": 2120}, "message": "destructor '~LoopVectorize' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "0837a188354d28a428f4730322a1a6d6", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 23, "file": 70, "line": 2301}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "2346568efc70c61b77332ee60031c526", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 23, "file": 70, "line": 2301}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "0f7462b0b98073434fbb4639f796a96e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 26, "file": 70, "line": 2357}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "448a801ee8bd4906455642ac8b064dc3", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 26, "file": 70, "line": 2357}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "9a540e8d8d541f2164afbcaaf144a360", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 70, "line": 2372}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "a0f054d52fdab5126633e3c0199e6a26", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 70, "line": 2372}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "e8e1b42db4f91e3f69a5f70380d7f33c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 24, "file": 70, "line": 2394}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "b36bad535e2fc30c752b15a160f9db5d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 24, "file": 70, "line": 2394}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "bd89c260d0f7eb895e272d9d3d23bc7e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 29, "file": 70, "line": 2850}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "428f2f817d1c25939771087284c819bc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 29, "file": 70, "line": 2850}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "b8270d9c1c180e1daf834dc80096e272", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 70, "line": 3247}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca49261cd2539ff59d396c7591dec002", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 3287}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "85e1f8e42ecfa70dec47250aee86ac0f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 3287}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "1b20adb5b466e9984c2b70fe65eea0dd", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 3298}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "79ddb0d150718ddb6dc535b67bcce4d9", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 3298}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "465940233d2a463e16010b2a5ee2bfcb", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 70, "line": 3314}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "0b6d711cb8b53d3a39ed0923b62f3c78", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 70, "line": 3314}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "b1c2e312d720b795547697f580a8c151", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 33, "file": 70, "line": 4482}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "77fad5c3cea167097b91d2ec4f5b03f3", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 33, "file": 70, "line": 4482}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "5f30eeb67c643f0fda4bba54795dcd94", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 22, "file": 70, "line": 5034}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "4f7bc431cf71bb3599f7ddc25225b187", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 22, "file": 70, "line": 5034}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "f0e51b7a12d8f5182e0a81005ead6f99", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 36, "file": 70, "line": 5048}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "0ff37b68847b586a07ea6943e501bcdf", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 36, "file": 70, "line": 5048}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "13fad157e1a8bc52a2df03fa30db7e7e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 31, "file": 70, "line": 5054}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "5f36123024123ecbbdf2a73f4d47fc7f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 31, "file": 70, "line": 5054}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "7daacae8dc59eb02e1562e68a25ad9d4", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 70, "line": 5069}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ffb739fcff92a23966fde14356fcaa10", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 70, "line": 5069}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "5f44f6ac40274c98a0aabc5ab6f49732", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 23, "file": 70, "line": 5322}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ebe64f69de1390dbd7a7998942d91680", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 23, "file": 70, "line": 5322}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "319884fa59c75208baf03df9a273d1a6", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 33, "file": 70, "line": 5334}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca5f637ec42a7eb517910d1fce52fe8d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 33, "file": 70, "line": 5334}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "5fdc3445079ecc7a55a04cbd7fec01d0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 28, "file": 70, "line": 5356}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "150523a9fa1ceb666e1e565658606491", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 28, "file": 70, "line": 5356}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ccbfe8ca772d98e1a33d4cd0807b8c5f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 35, "file": 70, "line": 5376}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "fd457bdfdc3aa108f75059f5f85d164a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 35, "file": 70, "line": 5376}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "e2d73e4fab3cd45bb5d67ceda60286d0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 70, "line": 5671}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca49261cd2539ff59d396c7591dec002", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 19, "file": 70, "line": 5733}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "b201ebcc7f84d0dededbb6925c02e771", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 70, "line": 5753}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca49261cd2539ff59d396c7591dec002", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 37, "file": 70, "line": 5906}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "aa6de3503b955cabdf0db6a570d85856", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 41, "file": 70, "line": 5928}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "98850d264690528deeed35ac7d94cf69", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 22, "file": 70, "line": 6373}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "784dabcb9d7401e71d5161a89507735e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 22, "file": 70, "line": 6373}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "86ce1b4911c4a7cec53b1600f6caaf9e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 26, "file": 70, "line": 6549}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "e155615c6bd4e30b2c5f96e996b1773d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 26, "file": 70, "line": 6549}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "032274f81f6d3592dd20e1033b889305", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 23, "file": 70, "line": 7439}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "6b2965875863658b644c486c6091a5bd", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 23, "file": 70, "line": 7439}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "d2552d0ddb924c8535fc9f920c8838ca", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 8328}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "9852688a115fc82a797057ce5cb41cfa", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 8328}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "8cf31c36c4ceec2912ca77af5a647bb6", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 8386}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "87a5d15380adebc6f245535c59fbf08d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 8386}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "f8c3a98d2d0c52607fb950ce0fcfbf85", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 38, "file": 70, "line": 8408}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ae6b14b0f10dd71bdc8e2f3e1d2e6c7e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 8438}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "bd29c704c7ef7cb1d95a2cd86cd803c1", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 8453}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "9852688a115fc82a797057ce5cb41cfa", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 70, "line": 8453}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "8cf31c36c4ceec2912ca77af5a647bb6", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 24, "file": 70, "line": 8479}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "5066e906560aa59b0321b2a44164febe", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 24, "file": 70, "line": 8479}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "fbbf5a22f91fd783fd45022bd3ac067f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 31, "file": 70, "line": 8489}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "64db96a3e2efcb31c42627cbbf7b67cb", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 31, "file": 70, "line": 8489}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "653c2b36e0cd2578dbeafb2db2adffe4", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 8542}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "84905bec82ddaff130f4474049a5c795", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 8546}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "193f7cf1ecf3fc1ea93b216a815cb3f8", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 70, "line": 9666}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca49261cd2539ff59d396c7591dec002", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 70, "line": 9671}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca49261cd2539ff59d396c7591dec002", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 70, "line": 9679}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca49261cd2539ff59d396c7591dec002", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 70, "line": 9687}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "ca49261cd2539ff59d396c7591dec002", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 17, "file": 70, "line": 9719}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "1a53eef9e4e86f38160e8b455a56e569", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 17, "file": 70, "line": 9775}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp", "reportHash": "1a53eef9e4e86f38160e8b455a56e569", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 71, "line": 100}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.h", "reportHash": "6e02f6b76bd9eba1661458faedf32209", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
