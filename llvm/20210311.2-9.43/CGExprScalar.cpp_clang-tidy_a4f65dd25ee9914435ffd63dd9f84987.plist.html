<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"12": {"id": 12, "path": "/home/vsts/work/1/llvm-project/clang/include/clang/AST/Expr.h", "content": "//===--- Expr.h - Classes for representing expressions ----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n//  This file defines the Expr interface and subclasses.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_AST_EXPR_H\n#define LLVM_CLANG_AST_EXPR_H\n\n#include \"clang/AST/APValue.h\"\n#include \"clang/AST/ASTVector.h\"\n#include \"clang/AST/ComputeDependence.h\"\n#include \"clang/AST/Decl.h\"\n#include \"clang/AST/DeclAccessPair.h\"\n#include \"clang/AST/DependenceFlags.h\"\n#include \"clang/AST/OperationKinds.h\"\n#include \"clang/AST/Stmt.h\"\n#include \"clang/AST/TemplateBase.h\"\n#include \"clang/AST/Type.h\"\n#include \"clang/Basic/CharInfo.h\"\n#include \"clang/Basic/LangOptions.h\"\n#include \"clang/Basic/SyncScope.h\"\n#include \"clang/Basic/TypeTraits.h\"\n#include \"llvm/ADT/APFloat.h\"\n#include \"llvm/ADT/APSInt.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/TrailingObjects.h\"\n\nnamespace clang {\n  class APValue;\n  class ASTContext;\n  class BlockDecl;\n  class CXXBaseSpecifier;\n  class CXXMemberCallExpr;\n  class CXXOperatorCallExpr;\n  class CastExpr;\n  class Decl;\n  class IdentifierInfo;\n  class MaterializeTemporaryExpr;\n  class NamedDecl;\n  class ObjCPropertyRefExpr;\n  class OpaqueValueExpr;\n  class ParmVarDecl;\n  class StringLiteral;\n  class TargetInfo;\n  class ValueDecl;\n\n/// A simple array of base specifiers.\ntypedef SmallVector<CXXBaseSpecifier*, 4> CXXCastPath;\n\n/// An adjustment to be made to the temporary created when emitting a\n/// reference binding, which accesses a particular subobject of that temporary.\nstruct SubobjectAdjustment {\n  enum {\n    DerivedToBaseAdjustment,\n    FieldAdjustment,\n    MemberPointerAdjustment\n  } Kind;\n\n  struct DTB {\n    const CastExpr *BasePath;\n    const CXXRecordDecl *DerivedClass;\n  };\n\n  struct P {\n    const MemberPointerType *MPT;\n    Expr *RHS;\n  };\n\n  union {\n    struct DTB DerivedToBase;\n    FieldDecl *Field;\n    struct P Ptr;\n  };\n\n  SubobjectAdjustment(const CastExpr *BasePath,\n                      const CXXRecordDecl *DerivedClass)\n    : Kind(DerivedToBaseAdjustment) {\n    DerivedToBase.BasePath = BasePath;\n    DerivedToBase.DerivedClass = DerivedClass;\n  }\n\n  SubobjectAdjustment(FieldDecl *Field)\n    : Kind(FieldAdjustment) {\n    this->Field = Field;\n  }\n\n  SubobjectAdjustment(const MemberPointerType *MPT, Expr *RHS)\n    : Kind(MemberPointerAdjustment) {\n    this->Ptr.MPT = MPT;\n    this->Ptr.RHS = RHS;\n  }\n};\n\n/// This represents one expression.  Note that Expr's are subclasses of Stmt.\n/// This allows an expression to be transparently used any place a Stmt is\n/// required.\nclass Expr : public ValueStmt {\n  QualType TR;\n\npublic:\n  Expr() = delete;\n  Expr(const Expr&) = delete;\n  Expr(Expr &&) = delete;\n  Expr &operator=(const Expr&) = delete;\n  Expr &operator=(Expr&&) = delete;\n\nprotected:\n  Expr(StmtClass SC, QualType T, ExprValueKind VK, ExprObjectKind OK)\n      : ValueStmt(SC) {\n    ExprBits.Dependent = 0;\n    ExprBits.ValueKind = VK;\n    ExprBits.ObjectKind = OK;\n    assert(ExprBits.ObjectKind == OK && \"truncated kind\");\n    setType(T);\n  }\n\n  /// Construct an empty expression.\n  explicit Expr(StmtClass SC, EmptyShell) : ValueStmt(SC) { }\n\n  /// Each concrete expr subclass is expected to compute its dependence and call\n  /// this in the constructor.\n  void setDependence(ExprDependence Deps) {\n    ExprBits.Dependent = static_cast<unsigned>(Deps);\n  }\n  friend class ASTImporter; // Sets dependence dircetly.\n  friend class ASTStmtReader; // Sets dependence dircetly.\n\npublic:\n  QualType getType() const { return TR; }\n  void setType(QualType t) {\n    // In C++, the type of an expression is always adjusted so that it\n    // will not have reference type (C++ [expr]p6). Use\n    // QualType::getNonReferenceType() to retrieve the non-reference\n    // type. Additionally, inspect Expr::isLvalue to determine whether\n    // an expression that is adjusted in this manner should be\n    // considered an lvalue.\n    assert((t.isNull() || !t->isReferenceType()) &&\n           \"Expressions can't have reference type\");\n\n    TR = t;\n  }\n\n  ExprDependence getDependence() const {\n    return static_cast<ExprDependence>(ExprBits.Dependent);\n  }\n\n  /// Determines whether the value of this expression depends on\n  ///   - a template parameter (C++ [temp.dep.constexpr])\n  ///   - or an error, whose resolution is unknown\n  ///\n  /// For example, the array bound of \"Chars\" in the following example is\n  /// value-dependent.\n  /// @code\n  /// template<int Size, char (&Chars)[Size]> struct meta_string;\n  /// @endcode\n  bool isValueDependent() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Value);\n  }\n\n  /// Determines whether the type of this expression depends on\n  ///   - a template paramter (C++ [temp.dep.expr], which means that its type\n  ///     could change from one template instantiation to the next)\n  ///   - or an error\n  ///\n  /// For example, the expressions \"x\" and \"x + y\" are type-dependent in\n  /// the following code, but \"y\" is not type-dependent:\n  /// @code\n  /// template<typename T>\n  /// void add(T x, int y) {\n  ///   x + y;\n  /// }\n  /// @endcode\n  bool isTypeDependent() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Type);\n  }\n\n  /// Whether this expression is instantiation-dependent, meaning that\n  /// it depends in some way on\n  ///    - a template parameter (even if neither its type nor (constant) value\n  ///      can change due to the template instantiation)\n  ///    - or an error\n  ///\n  /// In the following example, the expression \\c sizeof(sizeof(T() + T())) is\n  /// instantiation-dependent (since it involves a template parameter \\c T), but\n  /// is neither type- nor value-dependent, since the type of the inner\n  /// \\c sizeof is known (\\c std::size_t) and therefore the size of the outer\n  /// \\c sizeof is known.\n  ///\n  /// \\code\n  /// template<typename T>\n  /// void f(T x, T y) {\n  ///   sizeof(sizeof(T() + T());\n  /// }\n  /// \\endcode\n  ///\n  /// \\code\n  /// void func(int) {\n  ///   func(); // the expression is instantiation-dependent, because it depends\n  ///           // on an error.\n  /// }\n  /// \\endcode\n  bool isInstantiationDependent() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Instantiation);\n  }\n\n  /// Whether this expression contains an unexpanded parameter\n  /// pack (for C++11 variadic templates).\n  ///\n  /// Given the following function template:\n  ///\n  /// \\code\n  /// template<typename F, typename ...Types>\n  /// void forward(const F &f, Types &&...args) {\n  ///   f(static_cast<Types&&>(args)...);\n  /// }\n  /// \\endcode\n  ///\n  /// The expressions \\c args and \\c static_cast<Types&&>(args) both\n  /// contain parameter packs.\n  bool containsUnexpandedParameterPack() const {\n    return static_cast<bool>(getDependence() & ExprDependence::UnexpandedPack);\n  }\n\n  /// Whether this expression contains subexpressions which had errors, e.g. a\n  /// TypoExpr.\n  bool containsErrors() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Error);\n  }\n\n  /// getExprLoc - Return the preferred location for the arrow when diagnosing\n  /// a problem with a generic expression.\n  SourceLocation getExprLoc() const LLVM_READONLY;\n\n  /// Determine whether an lvalue-to-rvalue conversion should implicitly be\n  /// applied to this expression if it appears as a discarded-value expression\n  /// in C++11 onwards. This applies to certain forms of volatile glvalues.\n  bool isReadIfDiscardedInCPlusPlus11() const;\n\n  /// isUnusedResultAWarning - Return true if this immediate expression should\n  /// be warned about if the result is unused.  If so, fill in expr, location,\n  /// and ranges with expr to warn on and source locations/ranges appropriate\n  /// for a warning.\n  bool isUnusedResultAWarning(const Expr *&WarnExpr, SourceLocation &Loc,\n                              SourceRange &R1, SourceRange &R2,\n                              ASTContext &Ctx) const;\n\n  /// isLValue - True if this expression is an \"l-value\" according to\n  /// the rules of the current language.  C and C++ give somewhat\n  /// different rules for this concept, but in general, the result of\n  /// an l-value expression identifies a specific object whereas the\n  /// result of an r-value expression is a value detached from any\n  /// specific storage.\n  ///\n  /// C++11 divides the concept of \"r-value\" into pure r-values\n  /// (\"pr-values\") and so-called expiring values (\"x-values\"), which\n  /// identify specific objects that can be safely cannibalized for\n  /// their resources.  This is an unfortunate abuse of terminology on\n  /// the part of the C++ committee.  In Clang, when we say \"r-value\",\n  /// we generally mean a pr-value.\n  bool isLValue() const { return getValueKind() == VK_LValue; }\n  bool isRValue() const { return getValueKind() == VK_RValue; }\n  bool isXValue() const { return getValueKind() == VK_XValue; }\n  bool isGLValue() const { return getValueKind() != VK_RValue; }\n\n  enum LValueClassification {\n    LV_Valid,\n    LV_NotObjectType,\n    LV_IncompleteVoidType,\n    LV_DuplicateVectorComponents,\n    LV_InvalidExpression,\n    LV_InvalidMessageExpression,\n    LV_MemberFunction,\n    LV_SubObjCPropertySetting,\n    LV_ClassTemporary,\n    LV_ArrayTemporary\n  };\n  /// Reasons why an expression might not be an l-value.\n  LValueClassification ClassifyLValue(ASTContext &Ctx) const;\n\n  enum isModifiableLvalueResult {\n    MLV_Valid,\n    MLV_NotObjectType,\n    MLV_IncompleteVoidType,\n    MLV_DuplicateVectorComponents,\n    MLV_InvalidExpression,\n    MLV_LValueCast,           // Specialized form of MLV_InvalidExpression.\n    MLV_IncompleteType,\n    MLV_ConstQualified,\n    MLV_ConstQualifiedField,\n    MLV_ConstAddrSpace,\n    MLV_ArrayType,\n    MLV_NoSetterProperty,\n    MLV_MemberFunction,\n    MLV_SubObjCPropertySetting,\n    MLV_InvalidMessageExpression,\n    MLV_ClassTemporary,\n    MLV_ArrayTemporary\n  };\n  /// isModifiableLvalue - C99 6.3.2.1: an lvalue that does not have array type,\n  /// does not have an incomplete type, does not have a const-qualified type,\n  /// and if it is a structure or union, does not have any member (including,\n  /// recursively, any member or element of all contained aggregates or unions)\n  /// with a const-qualified type.\n  ///\n  /// \\param Loc [in,out] - A source location which *may* be filled\n  /// in with the location of the expression making this a\n  /// non-modifiable lvalue, if specified.\n  isModifiableLvalueResult\n  isModifiableLvalue(ASTContext &Ctx, SourceLocation *Loc = nullptr) const;\n\n  /// The return type of classify(). Represents the C++11 expression\n  ///        taxonomy.\n  class Classification {\n  public:\n    /// The various classification results. Most of these mean prvalue.\n    enum Kinds {\n      CL_LValue,\n      CL_XValue,\n      CL_Function, // Functions cannot be lvalues in C.\n      CL_Void, // Void cannot be an lvalue in C.\n      CL_AddressableVoid, // Void expression whose address can be taken in C.\n      CL_DuplicateVectorComponents, // A vector shuffle with dupes.\n      CL_MemberFunction, // An expression referring to a member function\n      CL_SubObjCPropertySetting,\n      CL_ClassTemporary, // A temporary of class type, or subobject thereof.\n      CL_ArrayTemporary, // A temporary of array type.\n      CL_ObjCMessageRValue, // ObjC message is an rvalue\n      CL_PRValue // A prvalue for any other reason, of any other type\n    };\n    /// The results of modification testing.\n    enum ModifiableType {\n      CM_Untested, // testModifiable was false.\n      CM_Modifiable,\n      CM_RValue, // Not modifiable because it's an rvalue\n      CM_Function, // Not modifiable because it's a function; C++ only\n      CM_LValueCast, // Same as CM_RValue, but indicates GCC cast-as-lvalue ext\n      CM_NoSetterProperty,// Implicit assignment to ObjC property without setter\n      CM_ConstQualified,\n      CM_ConstQualifiedField,\n      CM_ConstAddrSpace,\n      CM_ArrayType,\n      CM_IncompleteType\n    };\n\n  private:\n    friend class Expr;\n\n    unsigned short Kind;\n    unsigned short Modifiable;\n\n    explicit Classification(Kinds k, ModifiableType m)\n      : Kind(k), Modifiable(m)\n    {}\n\n  public:\n    Classification() {}\n\n    Kinds getKind() const { return static_cast<Kinds>(Kind); }\n    ModifiableType getModifiable() const {\n      assert(Modifiable != CM_Untested && \"Did not test for modifiability.\");\n      return static_cast<ModifiableType>(Modifiable);\n    }\n    bool isLValue() const { return Kind == CL_LValue; }\n    bool isXValue() const { return Kind == CL_XValue; }\n    bool isGLValue() const { return Kind <= CL_XValue; }\n    bool isPRValue() const { return Kind >= CL_Function; }\n    bool isRValue() const { return Kind >= CL_XValue; }\n    bool isModifiable() const { return getModifiable() == CM_Modifiable; }\n\n    /// Create a simple, modifiably lvalue\n    static Classification makeSimpleLValue() {\n      return Classification(CL_LValue, CM_Modifiable);\n    }\n\n  };\n  /// Classify - Classify this expression according to the C++11\n  ///        expression taxonomy.\n  ///\n  /// C++11 defines ([basic.lval]) a new taxonomy of expressions to replace the\n  /// old lvalue vs rvalue. This function determines the type of expression this\n  /// is. There are three expression types:\n  /// - lvalues are classical lvalues as in C++03.\n  /// - prvalues are equivalent to rvalues in C++03.\n  /// - xvalues are expressions yielding unnamed rvalue references, e.g. a\n  ///   function returning an rvalue reference.\n  /// lvalues and xvalues are collectively referred to as glvalues, while\n  /// prvalues and xvalues together form rvalues.\n  Classification Classify(ASTContext &Ctx) const {\n    return ClassifyImpl(Ctx, nullptr);\n  }\n\n  /// ClassifyModifiable - Classify this expression according to the\n  ///        C++11 expression taxonomy, and see if it is valid on the left side\n  ///        of an assignment.\n  ///\n  /// This function extends classify in that it also tests whether the\n  /// expression is modifiable (C99 6.3.2.1p1).\n  /// \\param Loc A source location that might be filled with a relevant location\n  ///            if the expression is not modifiable.\n  Classification ClassifyModifiable(ASTContext &Ctx, SourceLocation &Loc) const{\n    return ClassifyImpl(Ctx, &Loc);\n  }\n\n  /// Returns the set of floating point options that apply to this expression.\n  /// Only meaningful for operations on floating point values.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const;\n\n  /// getValueKindForType - Given a formal return or parameter type,\n  /// give its value kind.\n  static ExprValueKind getValueKindForType(QualType T) {\n    if (const ReferenceType *RT = T->getAs<ReferenceType>())\n      return (isa<LValueReferenceType>(RT)\n                ? VK_LValue\n                : (RT->getPointeeType()->isFunctionType()\n                     ? VK_LValue : VK_XValue));\n    return VK_RValue;\n  }\n\n  /// getValueKind - The value kind that this expression produces.\n  ExprValueKind getValueKind() const {\n    return static_cast<ExprValueKind>(ExprBits.ValueKind);\n  }\n\n  /// getObjectKind - The object kind that this expression produces.\n  /// Object kinds are meaningful only for expressions that yield an\n  /// l-value or x-value.\n  ExprObjectKind getObjectKind() const {\n    return static_cast<ExprObjectKind>(ExprBits.ObjectKind);\n  }\n\n  bool isOrdinaryOrBitFieldObject() const {\n    ExprObjectKind OK = getObjectKind();\n    return (OK == OK_Ordinary || OK == OK_BitField);\n  }\n\n  /// setValueKind - Set the value kind produced by this expression.\n  void setValueKind(ExprValueKind Cat) { ExprBits.ValueKind = Cat; }\n\n  /// setObjectKind - Set the object kind produced by this expression.\n  void setObjectKind(ExprObjectKind Cat) { ExprBits.ObjectKind = Cat; }\n\nprivate:\n  Classification ClassifyImpl(ASTContext &Ctx, SourceLocation *Loc) const;\n\npublic:\n\n  /// Returns true if this expression is a gl-value that\n  /// potentially refers to a bit-field.\n  ///\n  /// In C++, whether a gl-value refers to a bitfield is essentially\n  /// an aspect of the value-kind type system.\n  bool refersToBitField() const { return getObjectKind() == OK_BitField; }\n\n  /// If this expression refers to a bit-field, retrieve the\n  /// declaration of that bit-field.\n  ///\n  /// Note that this returns a non-null pointer in subtly different\n  /// places than refersToBitField returns true.  In particular, this can\n  /// return a non-null pointer even for r-values loaded from\n  /// bit-fields, but it will return null for a conditional bit-field.\n  FieldDecl *getSourceBitField();\n\n  const FieldDecl *getSourceBitField() const {\n    return const_cast<Expr*>(this)->getSourceBitField();\n  }\n\n  Decl *getReferencedDeclOfCallee();\n  const Decl *getReferencedDeclOfCallee() const {\n    return const_cast<Expr*>(this)->getReferencedDeclOfCallee();\n  }\n\n  /// If this expression is an l-value for an Objective C\n  /// property, find the underlying property reference expression.\n  const ObjCPropertyRefExpr *getObjCProperty() const;\n\n  /// Check if this expression is the ObjC 'self' implicit parameter.\n  bool isObjCSelfExpr() const;\n\n  /// Returns whether this expression refers to a vector element.\n  bool refersToVectorElement() const;\n\n  /// Returns whether this expression refers to a matrix element.\n  bool refersToMatrixElement() const {\n    return getObjectKind() == OK_MatrixComponent;\n  }\n\n  /// Returns whether this expression refers to a global register\n  /// variable.\n  bool refersToGlobalRegisterVar() const;\n\n  /// Returns whether this expression has a placeholder type.\n  bool hasPlaceholderType() const {\n    return getType()->isPlaceholderType();\n  }\n\n  /// Returns whether this expression has a specific placeholder type.\n  bool hasPlaceholderType(BuiltinType::Kind K) const {\n    assert(BuiltinType::isPlaceholderTypeKind(K));\n    if (const BuiltinType *BT = dyn_cast<BuiltinType>(getType()))\n      return BT->getKind() == K;\n    return false;\n  }\n\n  /// isKnownToHaveBooleanValue - Return true if this is an integer expression\n  /// that is known to return 0 or 1.  This happens for _Bool/bool expressions\n  /// but also int expressions which are produced by things like comparisons in\n  /// C.\n  ///\n  /// \\param Semantic If true, only return true for expressions that are known\n  /// to be semantically boolean, which might not be true even for expressions\n  /// that are known to evaluate to 0/1. For instance, reading an unsigned\n  /// bit-field with width '1' will evaluate to 0/1, but doesn't necessarily\n  /// semantically correspond to a bool.\n  bool isKnownToHaveBooleanValue(bool Semantic = true) const;\n\n  /// isIntegerConstantExpr - Return the value if this expression is a valid\n  /// integer constant expression.  If not a valid i-c-e, return None and fill\n  /// in Loc (if specified) with the location of the invalid expression.\n  ///\n  /// Note: This does not perform the implicit conversions required by C++11\n  /// [expr.const]p5.\n  Optional<llvm::APSInt> getIntegerConstantExpr(const ASTContext &Ctx,\n                                                SourceLocation *Loc = nullptr,\n                                                bool isEvaluated = true) const;\n  bool isIntegerConstantExpr(const ASTContext &Ctx,\n                             SourceLocation *Loc = nullptr) const;\n\n  /// isCXX98IntegralConstantExpr - Return true if this expression is an\n  /// integral constant expression in C++98. Can only be used in C++.\n  bool isCXX98IntegralConstantExpr(const ASTContext &Ctx) const;\n\n  /// isCXX11ConstantExpr - Return true if this expression is a constant\n  /// expression in C++11. Can only be used in C++.\n  ///\n  /// Note: This does not perform the implicit conversions required by C++11\n  /// [expr.const]p5.\n  bool isCXX11ConstantExpr(const ASTContext &Ctx, APValue *Result = nullptr,\n                           SourceLocation *Loc = nullptr) const;\n\n  /// isPotentialConstantExpr - Return true if this function's definition\n  /// might be usable in a constant expression in C++11, if it were marked\n  /// constexpr. Return false if the function can never produce a constant\n  /// expression, along with diagnostics describing why not.\n  static bool isPotentialConstantExpr(const FunctionDecl *FD,\n                                      SmallVectorImpl<\n                                        PartialDiagnosticAt> &Diags);\n\n  /// isPotentialConstantExprUnevaluted - Return true if this expression might\n  /// be usable in a constant expression in C++11 in an unevaluated context, if\n  /// it were in function FD marked constexpr. Return false if the function can\n  /// never produce a constant expression, along with diagnostics describing\n  /// why not.\n  static bool isPotentialConstantExprUnevaluated(Expr *E,\n                                                 const FunctionDecl *FD,\n                                                 SmallVectorImpl<\n                                                   PartialDiagnosticAt> &Diags);\n\n  /// isConstantInitializer - Returns true if this expression can be emitted to\n  /// IR as a constant, and thus can be used as a constant initializer in C.\n  /// If this expression is not constant and Culprit is non-null,\n  /// it is used to store the address of first non constant expr.\n  bool isConstantInitializer(ASTContext &Ctx, bool ForRef,\n                             const Expr **Culprit = nullptr) const;\n\n  /// EvalStatus is a struct with detailed info about an evaluation in progress.\n  struct EvalStatus {\n    /// Whether the evaluated expression has side effects.\n    /// For example, (f() && 0) can be folded, but it still has side effects.\n    bool HasSideEffects;\n\n    /// Whether the evaluation hit undefined behavior.\n    /// For example, 1.0 / 0.0 can be folded to Inf, but has undefined behavior.\n    /// Likewise, INT_MAX + 1 can be folded to INT_MIN, but has UB.\n    bool HasUndefinedBehavior;\n\n    /// Diag - If this is non-null, it will be filled in with a stack of notes\n    /// indicating why evaluation failed (or why it failed to produce a constant\n    /// expression).\n    /// If the expression is unfoldable, the notes will indicate why it's not\n    /// foldable. If the expression is foldable, but not a constant expression,\n    /// the notes will describes why it isn't a constant expression. If the\n    /// expression *is* a constant expression, no notes will be produced.\n    SmallVectorImpl<PartialDiagnosticAt> *Diag;\n\n    EvalStatus()\n        : HasSideEffects(false), HasUndefinedBehavior(false), Diag(nullptr) {}\n\n    // hasSideEffects - Return true if the evaluated expression has\n    // side effects.\n    bool hasSideEffects() const {\n      return HasSideEffects;\n    }\n  };\n\n  /// EvalResult is a struct with detailed info about an evaluated expression.\n  struct EvalResult : EvalStatus {\n    /// Val - This is the value the expression can be folded to.\n    APValue Val;\n\n    // isGlobalLValue - Return true if the evaluated lvalue expression\n    // is global.\n    bool isGlobalLValue() const;\n  };\n\n  /// EvaluateAsRValue - Return true if this is a constant which we can fold to\n  /// an rvalue using any crazy technique (that has nothing to do with language\n  /// standards) that we want to, even if the expression has side-effects. If\n  /// this function returns true, it returns the folded constant in Result. If\n  /// the expression is a glvalue, an lvalue-to-rvalue conversion will be\n  /// applied.\n  bool EvaluateAsRValue(EvalResult &Result, const ASTContext &Ctx,\n                        bool InConstantContext = false) const;\n\n  /// EvaluateAsBooleanCondition - Return true if this is a constant\n  /// which we can fold and convert to a boolean condition using\n  /// any crazy technique that we want to, even if the expression has\n  /// side-effects.\n  bool EvaluateAsBooleanCondition(bool &Result, const ASTContext &Ctx,\n                                  bool InConstantContext = false) const;\n\n  enum SideEffectsKind {\n    SE_NoSideEffects,          ///< Strictly evaluate the expression.\n    SE_AllowUndefinedBehavior, ///< Allow UB that we can give a value, but not\n                               ///< arbitrary unmodeled side effects.\n    SE_AllowSideEffects        ///< Allow any unmodeled side effect.\n  };\n\n  /// EvaluateAsInt - Return true if this is a constant which we can fold and\n  /// convert to an integer, using any crazy technique that we want to.\n  bool EvaluateAsInt(EvalResult &Result, const ASTContext &Ctx,\n                     SideEffectsKind AllowSideEffects = SE_NoSideEffects,\n                     bool InConstantContext = false) const;\n\n  /// EvaluateAsFloat - Return true if this is a constant which we can fold and\n  /// convert to a floating point value, using any crazy technique that we\n  /// want to.\n  bool EvaluateAsFloat(llvm::APFloat &Result, const ASTContext &Ctx,\n                       SideEffectsKind AllowSideEffects = SE_NoSideEffects,\n                       bool InConstantContext = false) const;\n\n  /// EvaluateAsFloat - Return true if this is a constant which we can fold and\n  /// convert to a fixed point value.\n  bool EvaluateAsFixedPoint(EvalResult &Result, const ASTContext &Ctx,\n                            SideEffectsKind AllowSideEffects = SE_NoSideEffects,\n                            bool InConstantContext = false) const;\n\n  /// isEvaluatable - Call EvaluateAsRValue to see if this expression can be\n  /// constant folded without side-effects, but discard the result.\n  bool isEvaluatable(const ASTContext &Ctx,\n                     SideEffectsKind AllowSideEffects = SE_NoSideEffects) const;\n\n  /// HasSideEffects - This routine returns true for all those expressions\n  /// which have any effect other than producing a value. Example is a function\n  /// call, volatile variable read, or throwing an exception. If\n  /// IncludePossibleEffects is false, this call treats certain expressions with\n  /// potential side effects (such as function call-like expressions,\n  /// instantiation-dependent expressions, or invocations from a macro) as not\n  /// having side effects.\n  bool HasSideEffects(const ASTContext &Ctx,\n                      bool IncludePossibleEffects = true) const;\n\n  /// Determine whether this expression involves a call to any function\n  /// that is not trivial.\n  bool hasNonTrivialCall(const ASTContext &Ctx) const;\n\n  /// EvaluateKnownConstInt - Call EvaluateAsRValue and return the folded\n  /// integer. This must be called on an expression that constant folds to an\n  /// integer.\n  llvm::APSInt EvaluateKnownConstInt(\n      const ASTContext &Ctx,\n      SmallVectorImpl<PartialDiagnosticAt> *Diag = nullptr) const;\n\n  llvm::APSInt EvaluateKnownConstIntCheckOverflow(\n      const ASTContext &Ctx,\n      SmallVectorImpl<PartialDiagnosticAt> *Diag = nullptr) const;\n\n  void EvaluateForOverflow(const ASTContext &Ctx) const;\n\n  /// EvaluateAsLValue - Evaluate an expression to see if we can fold it to an\n  /// lvalue with link time known address, with no side-effects.\n  bool EvaluateAsLValue(EvalResult &Result, const ASTContext &Ctx,\n                        bool InConstantContext = false) const;\n\n  /// EvaluateAsInitializer - Evaluate an expression as if it were the\n  /// initializer of the given declaration. Returns true if the initializer\n  /// can be folded to a constant, and produces any relevant notes. In C++11,\n  /// notes will be produced if the expression is not a constant expression.\n  bool EvaluateAsInitializer(APValue &Result, const ASTContext &Ctx,\n                             const VarDecl *VD,\n                             SmallVectorImpl<PartialDiagnosticAt> &Notes,\n                             bool IsConstantInitializer) const;\n\n  /// EvaluateWithSubstitution - Evaluate an expression as if from the context\n  /// of a call to the given function with the given arguments, inside an\n  /// unevaluated context. Returns true if the expression could be folded to a\n  /// constant.\n  bool EvaluateWithSubstitution(APValue &Value, ASTContext &Ctx,\n                                const FunctionDecl *Callee,\n                                ArrayRef<const Expr*> Args,\n                                const Expr *This = nullptr) const;\n\n  enum class ConstantExprKind {\n    /// An integer constant expression (an array bound, enumerator, case value,\n    /// bit-field width, or similar) or similar.\n    Normal,\n    /// A non-class template argument. Such a value is only used for mangling,\n    /// not for code generation, so can refer to dllimported functions.\n    NonClassTemplateArgument,\n    /// A class template argument. Such a value is used for code generation.\n    ClassTemplateArgument,\n    /// An immediate invocation. The destruction of the end result of this\n    /// evaluation is not part of the evaluation, but all other temporaries\n    /// are destroyed.\n    ImmediateInvocation,\n  };\n\n  /// Evaluate an expression that is required to be a constant expression. Does\n  /// not check the syntactic constraints for C and C++98 constant expressions.\n  bool EvaluateAsConstantExpr(\n      EvalResult &Result, const ASTContext &Ctx,\n      ConstantExprKind Kind = ConstantExprKind::Normal) const;\n\n  /// If the current Expr is a pointer, this will try to statically\n  /// determine the number of bytes available where the pointer is pointing.\n  /// Returns true if all of the above holds and we were able to figure out the\n  /// size, false otherwise.\n  ///\n  /// \\param Type - How to evaluate the size of the Expr, as defined by the\n  /// \"type\" parameter of __builtin_object_size\n  bool tryEvaluateObjectSize(uint64_t &Result, ASTContext &Ctx,\n                             unsigned Type) const;\n\n  /// Enumeration used to describe the kind of Null pointer constant\n  /// returned from \\c isNullPointerConstant().\n  enum NullPointerConstantKind {\n    /// Expression is not a Null pointer constant.\n    NPCK_NotNull = 0,\n\n    /// Expression is a Null pointer constant built from a zero integer\n    /// expression that is not a simple, possibly parenthesized, zero literal.\n    /// C++ Core Issue 903 will classify these expressions as \"not pointers\"\n    /// once it is adopted.\n    /// http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_active.html#903\n    NPCK_ZeroExpression,\n\n    /// Expression is a Null pointer constant built from a literal zero.\n    NPCK_ZeroLiteral,\n\n    /// Expression is a C++11 nullptr.\n    NPCK_CXX11_nullptr,\n\n    /// Expression is a GNU-style __null constant.\n    NPCK_GNUNull\n  };\n\n  /// Enumeration used to describe how \\c isNullPointerConstant()\n  /// should cope with value-dependent expressions.\n  enum NullPointerConstantValueDependence {\n    /// Specifies that the expression should never be value-dependent.\n    NPC_NeverValueDependent = 0,\n\n    /// Specifies that a value-dependent expression of integral or\n    /// dependent type should be considered a null pointer constant.\n    NPC_ValueDependentIsNull,\n\n    /// Specifies that a value-dependent expression should be considered\n    /// to never be a null pointer constant.\n    NPC_ValueDependentIsNotNull\n  };\n\n  /// isNullPointerConstant - C99 6.3.2.3p3 - Test if this reduces down to\n  /// a Null pointer constant. The return value can further distinguish the\n  /// kind of NULL pointer constant that was detected.\n  NullPointerConstantKind isNullPointerConstant(\n      ASTContext &Ctx,\n      NullPointerConstantValueDependence NPC) const;\n\n  /// isOBJCGCCandidate - Return true if this expression may be used in a read/\n  /// write barrier.\n  bool isOBJCGCCandidate(ASTContext &Ctx) const;\n\n  /// Returns true if this expression is a bound member function.\n  bool isBoundMemberFunction(ASTContext &Ctx) const;\n\n  /// Given an expression of bound-member type, find the type\n  /// of the member.  Returns null if this is an *overloaded* bound\n  /// member expression.\n  static QualType findBoundMemberType(const Expr *expr);\n\n  /// Skip past any invisble AST nodes which might surround this\n  /// statement, such as ExprWithCleanups or ImplicitCastExpr nodes,\n  /// but also injected CXXMemberExpr and CXXConstructExpr which represent\n  /// implicit conversions.\n  Expr *IgnoreUnlessSpelledInSource();\n  const Expr *IgnoreUnlessSpelledInSource() const {\n    return const_cast<Expr *>(this)->IgnoreUnlessSpelledInSource();\n  }\n\n  /// Skip past any implicit casts which might surround this expression until\n  /// reaching a fixed point. Skips:\n  /// * ImplicitCastExpr\n  /// * FullExpr\n  Expr *IgnoreImpCasts() LLVM_READONLY;\n  const Expr *IgnoreImpCasts() const {\n    return const_cast<Expr *>(this)->IgnoreImpCasts();\n  }\n\n  /// Skip past any casts which might surround this expression until reaching\n  /// a fixed point. Skips:\n  /// * CastExpr\n  /// * FullExpr\n  /// * MaterializeTemporaryExpr\n  /// * SubstNonTypeTemplateParmExpr\n  Expr *IgnoreCasts() LLVM_READONLY;\n  const Expr *IgnoreCasts() const {\n    return const_cast<Expr *>(this)->IgnoreCasts();\n  }\n\n  /// Skip past any implicit AST nodes which might surround this expression\n  /// until reaching a fixed point. Skips:\n  /// * What IgnoreImpCasts() skips\n  /// * MaterializeTemporaryExpr\n  /// * CXXBindTemporaryExpr\n  Expr *IgnoreImplicit() LLVM_READONLY;\n  const Expr *IgnoreImplicit() const {\n    return const_cast<Expr *>(this)->IgnoreImplicit();\n  }\n\n  /// Skip past any implicit AST nodes which might surround this expression\n  /// until reaching a fixed point. Same as IgnoreImplicit, except that it\n  /// also skips over implicit calls to constructors and conversion functions.\n  ///\n  /// FIXME: Should IgnoreImplicit do this?\n  Expr *IgnoreImplicitAsWritten() LLVM_READONLY;\n  const Expr *IgnoreImplicitAsWritten() const {\n    return const_cast<Expr *>(this)->IgnoreImplicitAsWritten();\n  }\n\n  /// Skip past any parentheses which might surround this expression until\n  /// reaching a fixed point. Skips:\n  /// * ParenExpr\n  /// * UnaryOperator if `UO_Extension`\n  /// * GenericSelectionExpr if `!isResultDependent()`\n  /// * ChooseExpr if `!isConditionDependent()`\n  /// * ConstantExpr\n  Expr *IgnoreParens() LLVM_READONLY;\n  const Expr *IgnoreParens() const {\n    return const_cast<Expr *>(this)->IgnoreParens();\n  }\n\n  /// Skip past any parentheses and implicit casts which might surround this\n  /// expression until reaching a fixed point.\n  /// FIXME: IgnoreParenImpCasts really ought to be equivalent to\n  /// IgnoreParens() + IgnoreImpCasts() until reaching a fixed point. However\n  /// this is currently not the case. Instead IgnoreParenImpCasts() skips:\n  /// * What IgnoreParens() skips\n  /// * What IgnoreImpCasts() skips\n  /// * MaterializeTemporaryExpr\n  /// * SubstNonTypeTemplateParmExpr\n  Expr *IgnoreParenImpCasts() LLVM_READONLY;\n  const Expr *IgnoreParenImpCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenImpCasts();\n  }\n\n  /// Skip past any parentheses and casts which might surround this expression\n  /// until reaching a fixed point. Skips:\n  /// * What IgnoreParens() skips\n  /// * What IgnoreCasts() skips\n  Expr *IgnoreParenCasts() LLVM_READONLY;\n  const Expr *IgnoreParenCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenCasts();\n  }\n\n  /// Skip conversion operators. If this Expr is a call to a conversion\n  /// operator, return the argument.\n  Expr *IgnoreConversionOperatorSingleStep() LLVM_READONLY;\n  const Expr *IgnoreConversionOperatorSingleStep() const {\n    return const_cast<Expr *>(this)->IgnoreConversionOperatorSingleStep();\n  }\n\n  /// Skip past any parentheses and lvalue casts which might surround this\n  /// expression until reaching a fixed point. Skips:\n  /// * What IgnoreParens() skips\n  /// * What IgnoreCasts() skips, except that only lvalue-to-rvalue\n  ///   casts are skipped\n  /// FIXME: This is intended purely as a temporary workaround for code\n  /// that hasn't yet been rewritten to do the right thing about those\n  /// casts, and may disappear along with the last internal use.\n  Expr *IgnoreParenLValueCasts() LLVM_READONLY;\n  const Expr *IgnoreParenLValueCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenLValueCasts();\n  }\n\n  /// Skip past any parenthese and casts which do not change the value\n  /// (including ptr->int casts of the same size) until reaching a fixed point.\n  /// Skips:\n  /// * What IgnoreParens() skips\n  /// * CastExpr which do not change the value\n  /// * SubstNonTypeTemplateParmExpr\n  Expr *IgnoreParenNoopCasts(const ASTContext &Ctx) LLVM_READONLY;\n  const Expr *IgnoreParenNoopCasts(const ASTContext &Ctx) const {\n    return const_cast<Expr *>(this)->IgnoreParenNoopCasts(Ctx);\n  }\n\n  /// Skip past any parentheses and derived-to-base casts until reaching a\n  /// fixed point. Skips:\n  /// * What IgnoreParens() skips\n  /// * CastExpr which represent a derived-to-base cast (CK_DerivedToBase,\n  ///   CK_UncheckedDerivedToBase and CK_NoOp)\n  Expr *IgnoreParenBaseCasts() LLVM_READONLY;\n  const Expr *IgnoreParenBaseCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenBaseCasts();\n  }\n\n  /// Determine whether this expression is a default function argument.\n  ///\n  /// Default arguments are implicitly generated in the abstract syntax tree\n  /// by semantic analysis for function calls, object constructions, etc. in\n  /// C++. Default arguments are represented by \\c CXXDefaultArgExpr nodes;\n  /// this routine also looks through any implicit casts to determine whether\n  /// the expression is a default argument.\n  bool isDefaultArgument() const;\n\n  /// Determine whether the result of this expression is a\n  /// temporary object of the given class type.\n  bool isTemporaryObject(ASTContext &Ctx, const CXXRecordDecl *TempTy) const;\n\n  /// Whether this expression is an implicit reference to 'this' in C++.\n  bool isImplicitCXXThis() const;\n\n  static bool hasAnyTypeDependentArguments(ArrayRef<Expr *> Exprs);\n\n  /// For an expression of class type or pointer to class type,\n  /// return the most derived class decl the expression is known to refer to.\n  ///\n  /// If this expression is a cast, this method looks through it to find the\n  /// most derived decl that can be inferred from the expression.\n  /// This is valid because derived-to-base conversions have undefined\n  /// behavior if the object isn't dynamically of the derived type.\n  const CXXRecordDecl *getBestDynamicClassType() const;\n\n  /// Get the inner expression that determines the best dynamic class.\n  /// If this is a prvalue, we guarantee that it is of the most-derived type\n  /// for the object itself.\n  const Expr *getBestDynamicClassTypeExpr() const;\n\n  /// Walk outwards from an expression we want to bind a reference to and\n  /// find the expression whose lifetime needs to be extended. Record\n  /// the LHSs of comma expressions and adjustments needed along the path.\n  const Expr *skipRValueSubobjectAdjustments(\n      SmallVectorImpl<const Expr *> &CommaLHS,\n      SmallVectorImpl<SubobjectAdjustment> &Adjustments) const;\n  const Expr *skipRValueSubobjectAdjustments() const {\n    SmallVector<const Expr *, 8> CommaLHSs;\n    SmallVector<SubobjectAdjustment, 8> Adjustments;\n    return skipRValueSubobjectAdjustments(CommaLHSs, Adjustments);\n  }\n\n  /// Checks that the two Expr's will refer to the same value as a comparison\n  /// operand.  The caller must ensure that the values referenced by the Expr's\n  /// are not modified between E1 and E2 or the result my be invalid.\n  static bool isSameComparisonOperand(const Expr* E1, const Expr* E2);\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstExprConstant &&\n           T->getStmtClass() <= lastExprConstant;\n  }\n};\n// PointerLikeTypeTraits is specialized so it can be used with a forward-decl of\n// Expr. Verify that we got it right.\nstatic_assert(llvm::PointerLikeTypeTraits<Expr *>::NumLowBitsAvailable <=\n                  llvm::detail::ConstantLog2<alignof(Expr)>::value,\n              \"PointerLikeTypeTraits<Expr*> assumes too much alignment.\");\n\nusing ConstantExprKind = Expr::ConstantExprKind;\n\n//===----------------------------------------------------------------------===//\n// Wrapper Expressions.\n//===----------------------------------------------------------------------===//\n\n/// FullExpr - Represents a \"full-expression\" node.\nclass FullExpr : public Expr {\nprotected:\n Stmt *SubExpr;\n\n FullExpr(StmtClass SC, Expr *subexpr)\n     : Expr(SC, subexpr->getType(), subexpr->getValueKind(),\n            subexpr->getObjectKind()),\n       SubExpr(subexpr) {\n   setDependence(computeDependence(this));\n }\n  FullExpr(StmtClass SC, EmptyShell Empty)\n    : Expr(SC, Empty) {}\npublic:\n  const Expr *getSubExpr() const { return cast<Expr>(SubExpr); }\n  Expr *getSubExpr() { return cast<Expr>(SubExpr); }\n\n  /// As with any mutator of the AST, be very careful when modifying an\n  /// existing AST to preserve its invariants.\n  void setSubExpr(Expr *E) { SubExpr = E; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstFullExprConstant &&\n           T->getStmtClass() <= lastFullExprConstant;\n  }\n};\n\n/// ConstantExpr - An expression that occurs in a constant context and\n/// optionally the result of evaluating the expression.\nclass ConstantExpr final\n    : public FullExpr,\n      private llvm::TrailingObjects<ConstantExpr, APValue, uint64_t> {\n  static_assert(std::is_same<uint64_t, llvm::APInt::WordType>::value,\n                \"ConstantExpr assumes that llvm::APInt::WordType is uint64_t \"\n                \"for tail-allocated storage\");\n  friend TrailingObjects;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n\npublic:\n  /// Describes the kind of result that can be tail-allocated.\n  enum ResultStorageKind { RSK_None, RSK_Int64, RSK_APValue };\n\nprivate:\n  size_t numTrailingObjects(OverloadToken<APValue>) const {\n    return ConstantExprBits.ResultKind == ConstantExpr::RSK_APValue;\n  }\n  size_t numTrailingObjects(OverloadToken<uint64_t>) const {\n    return ConstantExprBits.ResultKind == ConstantExpr::RSK_Int64;\n  }\n\n  uint64_t &Int64Result() {\n    assert(ConstantExprBits.ResultKind == ConstantExpr::RSK_Int64 &&\n           \"invalid accessor\");\n    return *getTrailingObjects<uint64_t>();\n  }\n  const uint64_t &Int64Result() const {\n    return const_cast<ConstantExpr *>(this)->Int64Result();\n  }\n  APValue &APValueResult() {\n    assert(ConstantExprBits.ResultKind == ConstantExpr::RSK_APValue &&\n           \"invalid accessor\");\n    return *getTrailingObjects<APValue>();\n  }\n  APValue &APValueResult() const {\n    return const_cast<ConstantExpr *>(this)->APValueResult();\n  }\n\n  ConstantExpr(Expr *SubExpr, ResultStorageKind StorageKind,\n               bool IsImmediateInvocation);\n  ConstantExpr(EmptyShell Empty, ResultStorageKind StorageKind);\n\npublic:\n  static ConstantExpr *Create(const ASTContext &Context, Expr *E,\n                              const APValue &Result);\n  static ConstantExpr *Create(const ASTContext &Context, Expr *E,\n                              ResultStorageKind Storage = RSK_None,\n                              bool IsImmediateInvocation = false);\n  static ConstantExpr *CreateEmpty(const ASTContext &Context,\n                                   ResultStorageKind StorageKind);\n\n  static ResultStorageKind getStorageKind(const APValue &Value);\n  static ResultStorageKind getStorageKind(const Type *T,\n                                          const ASTContext &Context);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return SubExpr->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return SubExpr->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConstantExprClass;\n  }\n\n  void SetResult(APValue Value, const ASTContext &Context) {\n    MoveIntoResult(Value, Context);\n  }\n  void MoveIntoResult(APValue &Value, const ASTContext &Context);\n\n  APValue::ValueKind getResultAPValueKind() const {\n    return static_cast<APValue::ValueKind>(ConstantExprBits.APValueKind);\n  }\n  ResultStorageKind getResultStorageKind() const {\n    return static_cast<ResultStorageKind>(ConstantExprBits.ResultKind);\n  }\n  bool isImmediateInvocation() const {\n    return ConstantExprBits.IsImmediateInvocation;\n  }\n  bool hasAPValueResult() const {\n    return ConstantExprBits.APValueKind != APValue::None;\n  }\n  APValue getAPValueResult() const;\n  APValue &getResultAsAPValue() const { return APValueResult(); }\n  llvm::APSInt getResultAsAPSInt() const;\n  // Iterators\n  child_range children() { return child_range(&SubExpr, &SubExpr+1); }\n  const_child_range children() const {\n    return const_child_range(&SubExpr, &SubExpr + 1);\n  }\n};\n\n//===----------------------------------------------------------------------===//\n// Primary Expressions.\n//===----------------------------------------------------------------------===//\n\n/// OpaqueValueExpr - An expression referring to an opaque object of a\n/// fixed type and value class.  These don't correspond to concrete\n/// syntax; instead they're used to express operations (usually copy\n/// operations) on values whose source is generally obvious from\n/// context.\nclass OpaqueValueExpr : public Expr {\n  friend class ASTStmtReader;\n  Expr *SourceExpr;\n\npublic:\n  OpaqueValueExpr(SourceLocation Loc, QualType T, ExprValueKind VK,\n                  ExprObjectKind OK = OK_Ordinary, Expr *SourceExpr = nullptr)\n      : Expr(OpaqueValueExprClass, T, VK, OK), SourceExpr(SourceExpr) {\n    setIsUnique(false);\n    OpaqueValueExprBits.Loc = Loc;\n    setDependence(computeDependence(this));\n  }\n\n  /// Given an expression which invokes a copy constructor --- i.e.  a\n  /// CXXConstructExpr, possibly wrapped in an ExprWithCleanups ---\n  /// find the OpaqueValueExpr that's the source of the construction.\n  static const OpaqueValueExpr *findInCopyConstruct(const Expr *expr);\n\n  explicit OpaqueValueExpr(EmptyShell Empty)\n    : Expr(OpaqueValueExprClass, Empty) {}\n\n  /// Retrieve the location of this expression.\n  SourceLocation getLocation() const { return OpaqueValueExprBits.Loc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return SourceExpr ? SourceExpr->getBeginLoc() : getLocation();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return SourceExpr ? SourceExpr->getEndLoc() : getLocation();\n  }\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return SourceExpr ? SourceExpr->getExprLoc() : getLocation();\n  }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n\n  /// The source expression of an opaque value expression is the\n  /// expression which originally generated the value.  This is\n  /// provided as a convenience for analyses that don't wish to\n  /// precisely model the execution behavior of the program.\n  ///\n  /// The source expression is typically set when building the\n  /// expression which binds the opaque value expression in the first\n  /// place.\n  Expr *getSourceExpr() const { return SourceExpr; }\n\n  void setIsUnique(bool V) {\n    assert((!V || SourceExpr) &&\n           \"unique OVEs are expected to have source expressions\");\n    OpaqueValueExprBits.IsUnique = V;\n  }\n\n  bool isUnique() const { return OpaqueValueExprBits.IsUnique; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == OpaqueValueExprClass;\n  }\n};\n\n/// A reference to a declared variable, function, enum, etc.\n/// [C99 6.5.1p2]\n///\n/// This encodes all the information about how a declaration is referenced\n/// within an expression.\n///\n/// There are several optional constructs attached to DeclRefExprs only when\n/// they apply in order to conserve memory. These are laid out past the end of\n/// the object, and flags in the DeclRefExprBitfield track whether they exist:\n///\n///   DeclRefExprBits.HasQualifier:\n///       Specifies when this declaration reference expression has a C++\n///       nested-name-specifier.\n///   DeclRefExprBits.HasFoundDecl:\n///       Specifies when this declaration reference expression has a record of\n///       a NamedDecl (different from the referenced ValueDecl) which was found\n///       during name lookup and/or overload resolution.\n///   DeclRefExprBits.HasTemplateKWAndArgsInfo:\n///       Specifies when this declaration reference expression has an explicit\n///       C++ template keyword and/or template argument list.\n///   DeclRefExprBits.RefersToEnclosingVariableOrCapture\n///       Specifies when this declaration reference expression (validly)\n///       refers to an enclosed local or a captured variable.\nclass DeclRefExpr final\n    : public Expr,\n      private llvm::TrailingObjects<DeclRefExpr, NestedNameSpecifierLoc,\n                                    NamedDecl *, ASTTemplateKWAndArgsInfo,\n                                    TemplateArgumentLoc> {\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n  friend TrailingObjects;\n\n  /// The declaration that we are referencing.\n  ValueDecl *D;\n\n  /// Provides source/type location info for the declaration name\n  /// embedded in D.\n  DeclarationNameLoc DNLoc;\n\n  size_t numTrailingObjects(OverloadToken<NestedNameSpecifierLoc>) const {\n    return hasQualifier();\n  }\n\n  size_t numTrailingObjects(OverloadToken<NamedDecl *>) const {\n    return hasFoundDecl();\n  }\n\n  size_t numTrailingObjects(OverloadToken<ASTTemplateKWAndArgsInfo>) const {\n    return hasTemplateKWAndArgsInfo();\n  }\n\n  /// Test whether there is a distinct FoundDecl attached to the end of\n  /// this DRE.\n  bool hasFoundDecl() const { return DeclRefExprBits.HasFoundDecl; }\n\n  DeclRefExpr(const ASTContext &Ctx, NestedNameSpecifierLoc QualifierLoc,\n              SourceLocation TemplateKWLoc, ValueDecl *D,\n              bool RefersToEnlosingVariableOrCapture,\n              const DeclarationNameInfo &NameInfo, NamedDecl *FoundD,\n              const TemplateArgumentListInfo *TemplateArgs, QualType T,\n              ExprValueKind VK, NonOdrUseReason NOUR);\n\n  /// Construct an empty declaration reference expression.\n  explicit DeclRefExpr(EmptyShell Empty) : Expr(DeclRefExprClass, Empty) {}\n\npublic:\n  DeclRefExpr(const ASTContext &Ctx, ValueDecl *D,\n              bool RefersToEnclosingVariableOrCapture, QualType T,\n              ExprValueKind VK, SourceLocation L,\n              const DeclarationNameLoc &LocInfo = DeclarationNameLoc(),\n              NonOdrUseReason NOUR = NOUR_None);\n\n  static DeclRefExpr *\n  Create(const ASTContext &Context, NestedNameSpecifierLoc QualifierLoc,\n         SourceLocation TemplateKWLoc, ValueDecl *D,\n         bool RefersToEnclosingVariableOrCapture, SourceLocation NameLoc,\n         QualType T, ExprValueKind VK, NamedDecl *FoundD = nullptr,\n         const TemplateArgumentListInfo *TemplateArgs = nullptr,\n         NonOdrUseReason NOUR = NOUR_None);\n\n  static DeclRefExpr *\n  Create(const ASTContext &Context, NestedNameSpecifierLoc QualifierLoc,\n         SourceLocation TemplateKWLoc, ValueDecl *D,\n         bool RefersToEnclosingVariableOrCapture,\n         const DeclarationNameInfo &NameInfo, QualType T, ExprValueKind VK,\n         NamedDecl *FoundD = nullptr,\n         const TemplateArgumentListInfo *TemplateArgs = nullptr,\n         NonOdrUseReason NOUR = NOUR_None);\n\n  /// Construct an empty declaration reference expression.\n  static DeclRefExpr *CreateEmpty(const ASTContext &Context, bool HasQualifier,\n                                  bool HasFoundDecl,\n                                  bool HasTemplateKWAndArgsInfo,\n                                  unsigned NumTemplateArgs);\n\n  ValueDecl *getDecl() { return D; }\n  const ValueDecl *getDecl() const { return D; }\n  void setDecl(ValueDecl *NewD);\n\n  DeclarationNameInfo getNameInfo() const {\n    return DeclarationNameInfo(getDecl()->getDeclName(), getLocation(), DNLoc);\n  }\n\n  SourceLocation getLocation() const { return DeclRefExprBits.Loc; }\n  void setLocation(SourceLocation L) { DeclRefExprBits.Loc = L; }\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  /// Determine whether this declaration reference was preceded by a\n  /// C++ nested-name-specifier, e.g., \\c N::foo.\n  bool hasQualifier() const { return DeclRefExprBits.HasQualifier; }\n\n  /// If the name was qualified, retrieves the nested-name-specifier\n  /// that precedes the name, with source-location information.\n  NestedNameSpecifierLoc getQualifierLoc() const {\n    if (!hasQualifier())\n      return NestedNameSpecifierLoc();\n    return *getTrailingObjects<NestedNameSpecifierLoc>();\n  }\n\n  /// If the name was qualified, retrieves the nested-name-specifier\n  /// that precedes the name. Otherwise, returns NULL.\n  NestedNameSpecifier *getQualifier() const {\n    return getQualifierLoc().getNestedNameSpecifier();\n  }\n\n  /// Get the NamedDecl through which this reference occurred.\n  ///\n  /// This Decl may be different from the ValueDecl actually referred to in the\n  /// presence of using declarations, etc. It always returns non-NULL, and may\n  /// simple return the ValueDecl when appropriate.\n\n  NamedDecl *getFoundDecl() {\n    return hasFoundDecl() ? *getTrailingObjects<NamedDecl *>() : D;\n  }\n\n  /// Get the NamedDecl through which this reference occurred.\n  /// See non-const variant.\n  const NamedDecl *getFoundDecl() const {\n    return hasFoundDecl() ? *getTrailingObjects<NamedDecl *>() : D;\n  }\n\n  bool hasTemplateKWAndArgsInfo() const {\n    return DeclRefExprBits.HasTemplateKWAndArgsInfo;\n  }\n\n  /// Retrieve the location of the template keyword preceding\n  /// this name, if any.\n  SourceLocation getTemplateKeywordLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->TemplateKWLoc;\n  }\n\n  /// Retrieve the location of the left angle bracket starting the\n  /// explicit template argument list following the name, if any.\n  SourceLocation getLAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->LAngleLoc;\n  }\n\n  /// Retrieve the location of the right angle bracket ending the\n  /// explicit template argument list following the name, if any.\n  SourceLocation getRAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->RAngleLoc;\n  }\n\n  /// Determines whether the name in this declaration reference\n  /// was preceded by the template keyword.\n  bool hasTemplateKeyword() const { return getTemplateKeywordLoc().isValid(); }\n\n  /// Determines whether this declaration reference was followed by an\n  /// explicit template argument list.\n  bool hasExplicitTemplateArgs() const { return getLAngleLoc().isValid(); }\n\n  /// Copies the template arguments (if present) into the given\n  /// structure.\n  void copyTemplateArgumentsInto(TemplateArgumentListInfo &List) const {\n    if (hasExplicitTemplateArgs())\n      getTrailingObjects<ASTTemplateKWAndArgsInfo>()->copyInto(\n          getTrailingObjects<TemplateArgumentLoc>(), List);\n  }\n\n  /// Retrieve the template arguments provided as part of this\n  /// template-id.\n  const TemplateArgumentLoc *getTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return nullptr;\n    return getTrailingObjects<TemplateArgumentLoc>();\n  }\n\n  /// Retrieve the number of template arguments provided as part of this\n  /// template-id.\n  unsigned getNumTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return 0;\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->NumTemplateArgs;\n  }\n\n  ArrayRef<TemplateArgumentLoc> template_arguments() const {\n    return {getTemplateArgs(), getNumTemplateArgs()};\n  }\n\n  /// Returns true if this expression refers to a function that\n  /// was resolved from an overloaded set having size greater than 1.\n  bool hadMultipleCandidates() const {\n    return DeclRefExprBits.HadMultipleCandidates;\n  }\n  /// Sets the flag telling whether this expression refers to\n  /// a function that was resolved from an overloaded set having size\n  /// greater than 1.\n  void setHadMultipleCandidates(bool V = true) {\n    DeclRefExprBits.HadMultipleCandidates = V;\n  }\n\n  /// Is this expression a non-odr-use reference, and if so, why?\n  NonOdrUseReason isNonOdrUse() const {\n    return static_cast<NonOdrUseReason>(DeclRefExprBits.NonOdrUseReason);\n  }\n\n  /// Does this DeclRefExpr refer to an enclosing local or a captured\n  /// variable?\n  bool refersToEnclosingVariableOrCapture() const {\n    return DeclRefExprBits.RefersToEnclosingVariableOrCapture;\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == DeclRefExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// Used by IntegerLiteral/FloatingLiteral to store the numeric without\n/// leaking memory.\n///\n/// For large floats/integers, APFloat/APInt will allocate memory from the heap\n/// to represent these numbers.  Unfortunately, when we use a BumpPtrAllocator\n/// to allocate IntegerLiteral/FloatingLiteral nodes the memory associated with\n/// the APFloat/APInt values will never get freed. APNumericStorage uses\n/// ASTContext's allocator for memory allocation.\nclass APNumericStorage {\n  union {\n    uint64_t VAL;    ///< Used to store the <= 64 bits integer value.\n    uint64_t *pVal;  ///< Used to store the >64 bits integer value.\n  };\n  unsigned BitWidth;\n\n  bool hasAllocation() const { return llvm::APInt::getNumWords(BitWidth) > 1; }\n\n  APNumericStorage(const APNumericStorage &) = delete;\n  void operator=(const APNumericStorage &) = delete;\n\nprotected:\n  APNumericStorage() : VAL(0), BitWidth(0) { }\n\n  llvm::APInt getIntValue() const {\n    unsigned NumWords = llvm::APInt::getNumWords(BitWidth);\n    if (NumWords > 1)\n      return llvm::APInt(BitWidth, NumWords, pVal);\n    else\n      return llvm::APInt(BitWidth, VAL);\n  }\n  void setIntValue(const ASTContext &C, const llvm::APInt &Val);\n};\n\nclass APIntStorage : private APNumericStorage {\npublic:\n  llvm::APInt getValue() const { return getIntValue(); }\n  void setValue(const ASTContext &C, const llvm::APInt &Val) {\n    setIntValue(C, Val);\n  }\n};\n\nclass APFloatStorage : private APNumericStorage {\npublic:\n  llvm::APFloat getValue(const llvm::fltSemantics &Semantics) const {\n    return llvm::APFloat(Semantics, getIntValue());\n  }\n  void setValue(const ASTContext &C, const llvm::APFloat &Val) {\n    setIntValue(C, Val.bitcastToAPInt());\n  }\n};\n\nclass IntegerLiteral : public Expr, public APIntStorage {\n  SourceLocation Loc;\n\n  /// Construct an empty integer literal.\n  explicit IntegerLiteral(EmptyShell Empty)\n    : Expr(IntegerLiteralClass, Empty) { }\n\npublic:\n  // type should be IntTy, LongTy, LongLongTy, UnsignedIntTy, UnsignedLongTy,\n  // or UnsignedLongLongTy\n  IntegerLiteral(const ASTContext &C, const llvm::APInt &V, QualType type,\n                 SourceLocation l);\n\n  /// Returns a new integer literal with value 'V' and type 'type'.\n  /// \\param type - either IntTy, LongTy, LongLongTy, UnsignedIntTy,\n  /// UnsignedLongTy, or UnsignedLongLongTy which should match the size of V\n  /// \\param V - the value that the returned integer literal contains.\n  static IntegerLiteral *Create(const ASTContext &C, const llvm::APInt &V,\n                                QualType type, SourceLocation l);\n  /// Returns a new empty integer literal.\n  static IntegerLiteral *Create(const ASTContext &C, EmptyShell Empty);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  /// Retrieve the location of the literal.\n  SourceLocation getLocation() const { return Loc; }\n\n  void setLocation(SourceLocation Location) { Loc = Location; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == IntegerLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass FixedPointLiteral : public Expr, public APIntStorage {\n  SourceLocation Loc;\n  unsigned Scale;\n\n  /// \\brief Construct an empty fixed-point literal.\n  explicit FixedPointLiteral(EmptyShell Empty)\n      : Expr(FixedPointLiteralClass, Empty) {}\n\n public:\n  FixedPointLiteral(const ASTContext &C, const llvm::APInt &V, QualType type,\n                    SourceLocation l, unsigned Scale);\n\n  // Store the int as is without any bit shifting.\n  static FixedPointLiteral *CreateFromRawInt(const ASTContext &C,\n                                             const llvm::APInt &V,\n                                             QualType type, SourceLocation l,\n                                             unsigned Scale);\n\n  /// Returns an empty fixed-point literal.\n  static FixedPointLiteral *Create(const ASTContext &C, EmptyShell Empty);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  /// \\brief Retrieve the location of the literal.\n  SourceLocation getLocation() const { return Loc; }\n\n  void setLocation(SourceLocation Location) { Loc = Location; }\n\n  unsigned getScale() const { return Scale; }\n  void setScale(unsigned S) { Scale = S; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == FixedPointLiteralClass;\n  }\n\n  std::string getValueAsString(unsigned Radix) const;\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass CharacterLiteral : public Expr {\npublic:\n  enum CharacterKind {\n    Ascii,\n    Wide,\n    UTF8,\n    UTF16,\n    UTF32\n  };\n\nprivate:\n  unsigned Value;\n  SourceLocation Loc;\npublic:\n  // type should be IntTy\n  CharacterLiteral(unsigned value, CharacterKind kind, QualType type,\n                   SourceLocation l)\n      : Expr(CharacterLiteralClass, type, VK_RValue, OK_Ordinary), Value(value),\n        Loc(l) {\n    CharacterLiteralBits.Kind = kind;\n    setDependence(ExprDependence::None);\n  }\n\n  /// Construct an empty character literal.\n  CharacterLiteral(EmptyShell Empty) : Expr(CharacterLiteralClass, Empty) { }\n\n  SourceLocation getLocation() const { return Loc; }\n  CharacterKind getKind() const {\n    return static_cast<CharacterKind>(CharacterLiteralBits.Kind);\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  unsigned getValue() const { return Value; }\n\n  void setLocation(SourceLocation Location) { Loc = Location; }\n  void setKind(CharacterKind kind) { CharacterLiteralBits.Kind = kind; }\n  void setValue(unsigned Val) { Value = Val; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == CharacterLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass FloatingLiteral : public Expr, private APFloatStorage {\n  SourceLocation Loc;\n\n  FloatingLiteral(const ASTContext &C, const llvm::APFloat &V, bool isexact,\n                  QualType Type, SourceLocation L);\n\n  /// Construct an empty floating-point literal.\n  explicit FloatingLiteral(const ASTContext &C, EmptyShell Empty);\n\npublic:\n  static FloatingLiteral *Create(const ASTContext &C, const llvm::APFloat &V,\n                                 bool isexact, QualType Type, SourceLocation L);\n  static FloatingLiteral *Create(const ASTContext &C, EmptyShell Empty);\n\n  llvm::APFloat getValue() const {\n    return APFloatStorage::getValue(getSemantics());\n  }\n  void setValue(const ASTContext &C, const llvm::APFloat &Val) {\n    assert(&getSemantics() == &Val.getSemantics() && \"Inconsistent semantics\");\n    APFloatStorage::setValue(C, Val);\n  }\n\n  /// Get a raw enumeration value representing the floating-point semantics of\n  /// this literal (32-bit IEEE, x87, ...), suitable for serialisation.\n  llvm::APFloatBase::Semantics getRawSemantics() const {\n    return static_cast<llvm::APFloatBase::Semantics>(\n        FloatingLiteralBits.Semantics);\n  }\n\n  /// Set the raw enumeration value representing the floating-point semantics of\n  /// this literal (32-bit IEEE, x87, ...), suitable for serialisation.\n  void setRawSemantics(llvm::APFloatBase::Semantics Sem) {\n    FloatingLiteralBits.Semantics = Sem;\n  }\n\n  /// Return the APFloat semantics this literal uses.\n  const llvm::fltSemantics &getSemantics() const {\n    return llvm::APFloatBase::EnumToSemantics(\n        static_cast<llvm::APFloatBase::Semantics>(\n            FloatingLiteralBits.Semantics));\n  }\n\n  /// Set the APFloat semantics this literal uses.\n  void setSemantics(const llvm::fltSemantics &Sem) {\n    FloatingLiteralBits.Semantics = llvm::APFloatBase::SemanticsToEnum(Sem);\n  }\n\n  bool isExact() const { return FloatingLiteralBits.IsExact; }\n  void setExact(bool E) { FloatingLiteralBits.IsExact = E; }\n\n  /// getValueAsApproximateDouble - This returns the value as an inaccurate\n  /// double.  Note that this may cause loss of precision, but is useful for\n  /// debugging dumps, etc.\n  double getValueAsApproximateDouble() const;\n\n  SourceLocation getLocation() const { return Loc; }\n  void setLocation(SourceLocation L) { Loc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == FloatingLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// ImaginaryLiteral - We support imaginary integer and floating point literals,\n/// like \"1.0i\".  We represent these as a wrapper around FloatingLiteral and\n/// IntegerLiteral classes.  Instances of this class always have a Complex type\n/// whose element type matches the subexpression.\n///\nclass ImaginaryLiteral : public Expr {\n  Stmt *Val;\npublic:\n  ImaginaryLiteral(Expr *val, QualType Ty)\n      : Expr(ImaginaryLiteralClass, Ty, VK_RValue, OK_Ordinary), Val(val) {\n    setDependence(ExprDependence::None);\n  }\n\n  /// Build an empty imaginary literal.\n  explicit ImaginaryLiteral(EmptyShell Empty)\n    : Expr(ImaginaryLiteralClass, Empty) { }\n\n  const Expr *getSubExpr() const { return cast<Expr>(Val); }\n  Expr *getSubExpr() { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return Val->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Val->getEndLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ImaginaryLiteralClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n};\n\n/// StringLiteral - This represents a string literal expression, e.g. \"foo\"\n/// or L\"bar\" (wide strings). The actual string data can be obtained with\n/// getBytes() and is NOT null-terminated. The length of the string data is\n/// determined by calling getByteLength().\n///\n/// The C type for a string is always a ConstantArrayType. In C++, the char\n/// type is const qualified, in C it is not.\n///\n/// Note that strings in C can be formed by concatenation of multiple string\n/// literal pptokens in translation phase #6. This keeps track of the locations\n/// of each of these pieces.\n///\n/// Strings in C can also be truncated and extended by assigning into arrays,\n/// e.g. with constructs like:\n///   char X[2] = \"foobar\";\n/// In this case, getByteLength() will return 6, but the string literal will\n/// have type \"char[2]\".\nclass StringLiteral final\n    : public Expr,\n      private llvm::TrailingObjects<StringLiteral, unsigned, SourceLocation,\n                                    char> {\n  friend class ASTStmtReader;\n  friend TrailingObjects;\n\n  /// StringLiteral is followed by several trailing objects. They are in order:\n  ///\n  /// * A single unsigned storing the length in characters of this string. The\n  ///   length in bytes is this length times the width of a single character.\n  ///   Always present and stored as a trailing objects because storing it in\n  ///   StringLiteral would increase the size of StringLiteral by sizeof(void *)\n  ///   due to alignment requirements. If you add some data to StringLiteral,\n  ///   consider moving it inside StringLiteral.\n  ///\n  /// * An array of getNumConcatenated() SourceLocation, one for each of the\n  ///   token this string is made of.\n  ///\n  /// * An array of getByteLength() char used to store the string data.\n\npublic:\n  enum StringKind { Ascii, Wide, UTF8, UTF16, UTF32 };\n\nprivate:\n  unsigned numTrailingObjects(OverloadToken<unsigned>) const { return 1; }\n  unsigned numTrailingObjects(OverloadToken<SourceLocation>) const {\n    return getNumConcatenated();\n  }\n\n  unsigned numTrailingObjects(OverloadToken<char>) const {\n    return getByteLength();\n  }\n\n  char *getStrDataAsChar() { return getTrailingObjects<char>(); }\n  const char *getStrDataAsChar() const { return getTrailingObjects<char>(); }\n\n  const uint16_t *getStrDataAsUInt16() const {\n    return reinterpret_cast<const uint16_t *>(getTrailingObjects<char>());\n  }\n\n  const uint32_t *getStrDataAsUInt32() const {\n    return reinterpret_cast<const uint32_t *>(getTrailingObjects<char>());\n  }\n\n  /// Build a string literal.\n  StringLiteral(const ASTContext &Ctx, StringRef Str, StringKind Kind,\n                bool Pascal, QualType Ty, const SourceLocation *Loc,\n                unsigned NumConcatenated);\n\n  /// Build an empty string literal.\n  StringLiteral(EmptyShell Empty, unsigned NumConcatenated, unsigned Length,\n                unsigned CharByteWidth);\n\n  /// Map a target and string kind to the appropriate character width.\n  static unsigned mapCharByteWidth(TargetInfo const &Target, StringKind SK);\n\n  /// Set one of the string literal token.\n  void setStrTokenLoc(unsigned TokNum, SourceLocation L) {\n    assert(TokNum < getNumConcatenated() && \"Invalid tok number\");\n    getTrailingObjects<SourceLocation>()[TokNum] = L;\n  }\n\npublic:\n  /// This is the \"fully general\" constructor that allows representation of\n  /// strings formed from multiple concatenated tokens.\n  static StringLiteral *Create(const ASTContext &Ctx, StringRef Str,\n                               StringKind Kind, bool Pascal, QualType Ty,\n                               const SourceLocation *Loc,\n                               unsigned NumConcatenated);\n\n  /// Simple constructor for string literals made from one token.\n  static StringLiteral *Create(const ASTContext &Ctx, StringRef Str,\n                               StringKind Kind, bool Pascal, QualType Ty,\n                               SourceLocation Loc) {\n    return Create(Ctx, Str, Kind, Pascal, Ty, &Loc, 1);\n  }\n\n  /// Construct an empty string literal.\n  static StringLiteral *CreateEmpty(const ASTContext &Ctx,\n                                    unsigned NumConcatenated, unsigned Length,\n                                    unsigned CharByteWidth);\n\n  StringRef getString() const {\n    assert(getCharByteWidth() == 1 &&\n           \"This function is used in places that assume strings use char\");\n    return StringRef(getStrDataAsChar(), getByteLength());\n  }\n\n  /// Allow access to clients that need the byte representation, such as\n  /// ASTWriterStmt::VisitStringLiteral().\n  StringRef getBytes() const {\n    // FIXME: StringRef may not be the right type to use as a result for this.\n    return StringRef(getStrDataAsChar(), getByteLength());\n  }\n\n  void outputString(raw_ostream &OS) const;\n\n  uint32_t getCodeUnit(size_t i) const {\n    assert(i < getLength() && \"out of bounds access\");\n    switch (getCharByteWidth()) {\n    case 1:\n      return static_cast<unsigned char>(getStrDataAsChar()[i]);\n    case 2:\n      return getStrDataAsUInt16()[i];\n    case 4:\n      return getStrDataAsUInt32()[i];\n    }\n    llvm_unreachable(\"Unsupported character width!\");\n  }\n\n  unsigned getByteLength() const { return getCharByteWidth() * getLength(); }\n  unsigned getLength() const { return *getTrailingObjects<unsigned>(); }\n  unsigned getCharByteWidth() const { return StringLiteralBits.CharByteWidth; }\n\n  StringKind getKind() const {\n    return static_cast<StringKind>(StringLiteralBits.Kind);\n  }\n\n  bool isAscii() const { return getKind() == Ascii; }\n  bool isWide() const { return getKind() == Wide; }\n  bool isUTF8() const { return getKind() == UTF8; }\n  bool isUTF16() const { return getKind() == UTF16; }\n  bool isUTF32() const { return getKind() == UTF32; }\n  bool isPascal() const { return StringLiteralBits.IsPascal; }\n\n  bool containsNonAscii() const {\n    for (auto c : getString())\n      if (!isASCII(c))\n        return true;\n    return false;\n  }\n\n  bool containsNonAsciiOrNull() const {\n    for (auto c : getString())\n      if (!isASCII(c) || !c)\n        return true;\n    return false;\n  }\n\n  /// getNumConcatenated - Get the number of string literal tokens that were\n  /// concatenated in translation phase #6 to form this string literal.\n  unsigned getNumConcatenated() const {\n    return StringLiteralBits.NumConcatenated;\n  }\n\n  /// Get one of the string literal token.\n  SourceLocation getStrTokenLoc(unsigned TokNum) const {\n    assert(TokNum < getNumConcatenated() && \"Invalid tok number\");\n    return getTrailingObjects<SourceLocation>()[TokNum];\n  }\n\n  /// getLocationOfByte - Return a source location that points to the specified\n  /// byte of this string literal.\n  ///\n  /// Strings are amazingly complex.  They can be formed from multiple tokens\n  /// and can have escape sequences in them in addition to the usual trigraph\n  /// and escaped newline business.  This routine handles this complexity.\n  ///\n  SourceLocation\n  getLocationOfByte(unsigned ByteNo, const SourceManager &SM,\n                    const LangOptions &Features, const TargetInfo &Target,\n                    unsigned *StartToken = nullptr,\n                    unsigned *StartTokenByteOffset = nullptr) const;\n\n  typedef const SourceLocation *tokloc_iterator;\n\n  tokloc_iterator tokloc_begin() const {\n    return getTrailingObjects<SourceLocation>();\n  }\n\n  tokloc_iterator tokloc_end() const {\n    return getTrailingObjects<SourceLocation>() + getNumConcatenated();\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return *tokloc_begin(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return *(tokloc_end() - 1); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == StringLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// [C99 6.4.2.2] - A predefined identifier such as __func__.\nclass PredefinedExpr final\n    : public Expr,\n      private llvm::TrailingObjects<PredefinedExpr, Stmt *> {\n  friend class ASTStmtReader;\n  friend TrailingObjects;\n\n  // PredefinedExpr is optionally followed by a single trailing\n  // \"Stmt *\" for the predefined identifier. It is present if and only if\n  // hasFunctionName() is true and is always a \"StringLiteral *\".\n\npublic:\n  enum IdentKind {\n    Func,\n    Function,\n    LFunction, // Same as Function, but as wide string.\n    FuncDName,\n    FuncSig,\n    LFuncSig, // Same as FuncSig, but as as wide string\n    PrettyFunction,\n    /// The same as PrettyFunction, except that the\n    /// 'virtual' keyword is omitted for virtual member functions.\n    PrettyFunctionNoVirtual\n  };\n\nprivate:\n  PredefinedExpr(SourceLocation L, QualType FNTy, IdentKind IK,\n                 StringLiteral *SL);\n\n  explicit PredefinedExpr(EmptyShell Empty, bool HasFunctionName);\n\n  /// True if this PredefinedExpr has storage for a function name.\n  bool hasFunctionName() const { return PredefinedExprBits.HasFunctionName; }\n\n  void setFunctionName(StringLiteral *SL) {\n    assert(hasFunctionName() &&\n           \"This PredefinedExpr has no storage for a function name!\");\n    *getTrailingObjects<Stmt *>() = SL;\n  }\n\npublic:\n  /// Create a PredefinedExpr.\n  static PredefinedExpr *Create(const ASTContext &Ctx, SourceLocation L,\n                                QualType FNTy, IdentKind IK, StringLiteral *SL);\n\n  /// Create an empty PredefinedExpr.\n  static PredefinedExpr *CreateEmpty(const ASTContext &Ctx,\n                                     bool HasFunctionName);\n\n  IdentKind getIdentKind() const {\n    return static_cast<IdentKind>(PredefinedExprBits.Kind);\n  }\n\n  SourceLocation getLocation() const { return PredefinedExprBits.Loc; }\n  void setLocation(SourceLocation L) { PredefinedExprBits.Loc = L; }\n\n  StringLiteral *getFunctionName() {\n    return hasFunctionName()\n               ? static_cast<StringLiteral *>(*getTrailingObjects<Stmt *>())\n               : nullptr;\n  }\n\n  const StringLiteral *getFunctionName() const {\n    return hasFunctionName()\n               ? static_cast<StringLiteral *>(*getTrailingObjects<Stmt *>())\n               : nullptr;\n  }\n\n  static StringRef getIdentKindName(IdentKind IK);\n  StringRef getIdentKindName() const {\n    return getIdentKindName(getIdentKind());\n  }\n\n  static std::string ComputeName(IdentKind IK, const Decl *CurrentDecl);\n\n  SourceLocation getBeginLoc() const { return getLocation(); }\n  SourceLocation getEndLoc() const { return getLocation(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == PredefinedExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(getTrailingObjects<Stmt *>(),\n                       getTrailingObjects<Stmt *>() + hasFunctionName());\n  }\n\n  const_child_range children() const {\n    return const_child_range(getTrailingObjects<Stmt *>(),\n                             getTrailingObjects<Stmt *>() + hasFunctionName());\n  }\n};\n\n/// ParenExpr - This represents a parethesized expression, e.g. \"(1)\".  This\n/// AST node is only formed if full location information is requested.\nclass ParenExpr : public Expr {\n  SourceLocation L, R;\n  Stmt *Val;\npublic:\n  ParenExpr(SourceLocation l, SourceLocation r, Expr *val)\n      : Expr(ParenExprClass, val->getType(), val->getValueKind(),\n             val->getObjectKind()),\n        L(l), R(r), Val(val) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Construct an empty parenthesized expression.\n  explicit ParenExpr(EmptyShell Empty)\n    : Expr(ParenExprClass, Empty) { }\n\n  const Expr *getSubExpr() const { return cast<Expr>(Val); }\n  Expr *getSubExpr() { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return L; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return R; }\n\n  /// Get the location of the left parentheses '('.\n  SourceLocation getLParen() const { return L; }\n  void setLParen(SourceLocation Loc) { L = Loc; }\n\n  /// Get the location of the right parentheses ')'.\n  SourceLocation getRParen() const { return R; }\n  void setRParen(SourceLocation Loc) { R = Loc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ParenExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n};\n\n/// UnaryOperator - This represents the unary-expression's (except sizeof and\n/// alignof), the postinc/postdec operators from postfix-expression, and various\n/// extensions.\n///\n/// Notes on various nodes:\n///\n/// Real/Imag - These return the real/imag part of a complex operand.  If\n///   applied to a non-complex value, the former returns its operand and the\n///   later returns zero in the type of the operand.\n///\nclass UnaryOperator final\n    : public Expr,\n      private llvm::TrailingObjects<UnaryOperator, FPOptionsOverride> {\n  Stmt *Val;\n\n  size_t numTrailingObjects(OverloadToken<FPOptionsOverride>) const {\n    return UnaryOperatorBits.HasFPFeatures ? 1 : 0;\n  }\n\n  FPOptionsOverride &getTrailingFPFeatures() {\n    assert(UnaryOperatorBits.HasFPFeatures);\n    return *getTrailingObjects<FPOptionsOverride>();\n  }\n\n  const FPOptionsOverride &getTrailingFPFeatures() const {\n    assert(UnaryOperatorBits.HasFPFeatures);\n    return *getTrailingObjects<FPOptionsOverride>();\n  }\n\npublic:\n  typedef UnaryOperatorKind Opcode;\n\nprotected:\n  UnaryOperator(const ASTContext &Ctx, Expr *input, Opcode opc, QualType type,\n                ExprValueKind VK, ExprObjectKind OK, SourceLocation l,\n                bool CanOverflow, FPOptionsOverride FPFeatures);\n\n  /// Build an empty unary operator.\n  explicit UnaryOperator(bool HasFPFeatures, EmptyShell Empty)\n      : Expr(UnaryOperatorClass, Empty) {\n    UnaryOperatorBits.Opc = UO_AddrOf;\n    UnaryOperatorBits.HasFPFeatures = HasFPFeatures;\n  }\n\npublic:\n  static UnaryOperator *CreateEmpty(const ASTContext &C, bool hasFPFeatures);\n\n  static UnaryOperator *Create(const ASTContext &C, Expr *input, Opcode opc,\n                               QualType type, ExprValueKind VK,\n                               ExprObjectKind OK, SourceLocation l,\n                               bool CanOverflow, FPOptionsOverride FPFeatures);\n\n  Opcode getOpcode() const {\n    return static_cast<Opcode>(UnaryOperatorBits.Opc);\n  }\n  void setOpcode(Opcode Opc) { UnaryOperatorBits.Opc = Opc; }\n\n  Expr *getSubExpr() const { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  /// getOperatorLoc - Return the location of the operator.\n  SourceLocation getOperatorLoc() const { return UnaryOperatorBits.Loc; }\n  void setOperatorLoc(SourceLocation L) { UnaryOperatorBits.Loc = L; }\n\n  /// Returns true if the unary operator can cause an overflow. For instance,\n  ///   signed int i = INT_MAX; i++;\n  ///   signed char c = CHAR_MAX; c++;\n  /// Due to integer promotions, c++ is promoted to an int before the postfix\n  /// increment, and the result is an int that cannot overflow. However, i++\n  /// can overflow.\n  bool canOverflow() const { return UnaryOperatorBits.CanOverflow; }\n  void setCanOverflow(bool C) { UnaryOperatorBits.CanOverflow = C; }\n\n  // Get the FP contractability status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFPContractableWithinStatement(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).allowFPContractWithinStatement();\n  }\n\n  // Get the FENV_ACCESS status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFEnvAccessOn(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).getAllowFEnvAccess();\n  }\n\n  /// isPostfix - Return true if this is a postfix operation, like x++.\n  static bool isPostfix(Opcode Op) {\n    return Op == UO_PostInc || Op == UO_PostDec;\n  }\n\n  /// isPrefix - Return true if this is a prefix operation, like --x.\n  static bool isPrefix(Opcode Op) {\n    return Op == UO_PreInc || Op == UO_PreDec;\n  }\n\n  bool isPrefix() const { return isPrefix(getOpcode()); }\n  bool isPostfix() const { return isPostfix(getOpcode()); }\n\n  static bool isIncrementOp(Opcode Op) {\n    return Op == UO_PreInc || Op == UO_PostInc;\n  }\n  bool isIncrementOp() const {\n    return isIncrementOp(getOpcode());\n  }\n\n  static bool isDecrementOp(Opcode Op) {\n    return Op == UO_PreDec || Op == UO_PostDec;\n  }\n  bool isDecrementOp() const {\n    return isDecrementOp(getOpcode());\n  }\n\n  static bool isIncrementDecrementOp(Opcode Op) { return Op <= UO_PreDec; }\n  bool isIncrementDecrementOp() const {\n    return isIncrementDecrementOp(getOpcode());\n  }\n\n  static bool isArithmeticOp(Opcode Op) {\n    return Op >= UO_Plus && Op <= UO_LNot;\n  }\n  bool isArithmeticOp() const { return isArithmeticOp(getOpcode()); }\n\n  /// getOpcodeStr - Turn an Opcode enum value into the punctuation char it\n  /// corresponds to, e.g. \"sizeof\" or \"[pre]++\"\n  static StringRef getOpcodeStr(Opcode Op);\n\n  /// Retrieve the unary opcode that corresponds to the given\n  /// overloaded operator.\n  static Opcode getOverloadedOpcode(OverloadedOperatorKind OO, bool Postfix);\n\n  /// Retrieve the overloaded operator kind that corresponds to\n  /// the given unary opcode.\n  static OverloadedOperatorKind getOverloadedOperator(Opcode Opc);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return isPostfix() ? Val->getBeginLoc() : getOperatorLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return isPostfix() ? getOperatorLoc() : Val->getEndLoc();\n  }\n  SourceLocation getExprLoc() const { return getOperatorLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == UnaryOperatorClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n\n  /// Is FPFeatures in Trailing Storage?\n  bool hasStoredFPFeatures() const { return UnaryOperatorBits.HasFPFeatures; }\n\n  /// Get FPFeatures from trailing storage.\n  FPOptionsOverride getStoredFPFeatures() const {\n    return getTrailingFPFeatures();\n  }\n\nprotected:\n  /// Set FPFeatures in trailing storage, used only by Serialization\n  void setStoredFPFeatures(FPOptionsOverride F) { getTrailingFPFeatures() = F; }\n\npublic:\n  // Get the FP features status of this operator. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (UnaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n  FPOptionsOverride getFPOptionsOverride() const {\n    if (UnaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  friend TrailingObjects;\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n/// Helper class for OffsetOfExpr.\n\n// __builtin_offsetof(type, identifier(.identifier|[expr])*)\nclass OffsetOfNode {\npublic:\n  /// The kind of offsetof node we have.\n  enum Kind {\n    /// An index into an array.\n    Array = 0x00,\n    /// A field.\n    Field = 0x01,\n    /// A field in a dependent type, known only by its name.\n    Identifier = 0x02,\n    /// An implicit indirection through a C++ base class, when the\n    /// field found is in a base class.\n    Base = 0x03\n  };\n\nprivate:\n  enum { MaskBits = 2, Mask = 0x03 };\n\n  /// The source range that covers this part of the designator.\n  SourceRange Range;\n\n  /// The data describing the designator, which comes in three\n  /// different forms, depending on the lower two bits.\n  ///   - An unsigned index into the array of Expr*'s stored after this node\n  ///     in memory, for [constant-expression] designators.\n  ///   - A FieldDecl*, for references to a known field.\n  ///   - An IdentifierInfo*, for references to a field with a given name\n  ///     when the class type is dependent.\n  ///   - A CXXBaseSpecifier*, for references that look at a field in a\n  ///     base class.\n  uintptr_t Data;\n\npublic:\n  /// Create an offsetof node that refers to an array element.\n  OffsetOfNode(SourceLocation LBracketLoc, unsigned Index,\n               SourceLocation RBracketLoc)\n      : Range(LBracketLoc, RBracketLoc), Data((Index << 2) | Array) {}\n\n  /// Create an offsetof node that refers to a field.\n  OffsetOfNode(SourceLocation DotLoc, FieldDecl *Field, SourceLocation NameLoc)\n      : Range(DotLoc.isValid() ? DotLoc : NameLoc, NameLoc),\n        Data(reinterpret_cast<uintptr_t>(Field) | OffsetOfNode::Field) {}\n\n  /// Create an offsetof node that refers to an identifier.\n  OffsetOfNode(SourceLocation DotLoc, IdentifierInfo *Name,\n               SourceLocation NameLoc)\n      : Range(DotLoc.isValid() ? DotLoc : NameLoc, NameLoc),\n        Data(reinterpret_cast<uintptr_t>(Name) | Identifier) {}\n\n  /// Create an offsetof node that refers into a C++ base class.\n  explicit OffsetOfNode(const CXXBaseSpecifier *Base)\n      : Range(), Data(reinterpret_cast<uintptr_t>(Base) | OffsetOfNode::Base) {}\n\n  /// Determine what kind of offsetof node this is.\n  Kind getKind() const { return static_cast<Kind>(Data & Mask); }\n\n  /// For an array element node, returns the index into the array\n  /// of expressions.\n  unsigned getArrayExprIndex() const {\n    assert(getKind() == Array);\n    return Data >> 2;\n  }\n\n  /// For a field offsetof node, returns the field.\n  FieldDecl *getField() const {\n    assert(getKind() == Field);\n    return reinterpret_cast<FieldDecl *>(Data & ~(uintptr_t)Mask);\n  }\n\n  /// For a field or identifier offsetof node, returns the name of\n  /// the field.\n  IdentifierInfo *getFieldName() const;\n\n  /// For a base class node, returns the base specifier.\n  CXXBaseSpecifier *getBase() const {\n    assert(getKind() == Base);\n    return reinterpret_cast<CXXBaseSpecifier *>(Data & ~(uintptr_t)Mask);\n  }\n\n  /// Retrieve the source range that covers this offsetof node.\n  ///\n  /// For an array element node, the source range contains the locations of\n  /// the square brackets. For a field or identifier node, the source range\n  /// contains the location of the period (if there is one) and the\n  /// identifier.\n  SourceRange getSourceRange() const LLVM_READONLY { return Range; }\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Range.getBegin(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Range.getEnd(); }\n};\n\n/// OffsetOfExpr - [C99 7.17] - This represents an expression of the form\n/// offsetof(record-type, member-designator). For example, given:\n/// @code\n/// struct S {\n///   float f;\n///   double d;\n/// };\n/// struct T {\n///   int i;\n///   struct S s[10];\n/// };\n/// @endcode\n/// we can represent and evaluate the expression @c offsetof(struct T, s[2].d).\n\nclass OffsetOfExpr final\n    : public Expr,\n      private llvm::TrailingObjects<OffsetOfExpr, OffsetOfNode, Expr *> {\n  SourceLocation OperatorLoc, RParenLoc;\n  // Base type;\n  TypeSourceInfo *TSInfo;\n  // Number of sub-components (i.e. instances of OffsetOfNode).\n  unsigned NumComps;\n  // Number of sub-expressions (i.e. array subscript expressions).\n  unsigned NumExprs;\n\n  size_t numTrailingObjects(OverloadToken<OffsetOfNode>) const {\n    return NumComps;\n  }\n\n  OffsetOfExpr(const ASTContext &C, QualType type,\n               SourceLocation OperatorLoc, TypeSourceInfo *tsi,\n               ArrayRef<OffsetOfNode> comps, ArrayRef<Expr*> exprs,\n               SourceLocation RParenLoc);\n\n  explicit OffsetOfExpr(unsigned numComps, unsigned numExprs)\n    : Expr(OffsetOfExprClass, EmptyShell()),\n      TSInfo(nullptr), NumComps(numComps), NumExprs(numExprs) {}\n\npublic:\n\n  static OffsetOfExpr *Create(const ASTContext &C, QualType type,\n                              SourceLocation OperatorLoc, TypeSourceInfo *tsi,\n                              ArrayRef<OffsetOfNode> comps,\n                              ArrayRef<Expr*> exprs, SourceLocation RParenLoc);\n\n  static OffsetOfExpr *CreateEmpty(const ASTContext &C,\n                                   unsigned NumComps, unsigned NumExprs);\n\n  /// getOperatorLoc - Return the location of the operator.\n  SourceLocation getOperatorLoc() const { return OperatorLoc; }\n  void setOperatorLoc(SourceLocation L) { OperatorLoc = L; }\n\n  /// Return the location of the right parentheses.\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation R) { RParenLoc = R; }\n\n  TypeSourceInfo *getTypeSourceInfo() const {\n    return TSInfo;\n  }\n  void setTypeSourceInfo(TypeSourceInfo *tsi) {\n    TSInfo = tsi;\n  }\n\n  const OffsetOfNode &getComponent(unsigned Idx) const {\n    assert(Idx < NumComps && \"Subscript out of range\");\n    return getTrailingObjects<OffsetOfNode>()[Idx];\n  }\n\n  void setComponent(unsigned Idx, OffsetOfNode ON) {\n    assert(Idx < NumComps && \"Subscript out of range\");\n    getTrailingObjects<OffsetOfNode>()[Idx] = ON;\n  }\n\n  unsigned getNumComponents() const {\n    return NumComps;\n  }\n\n  Expr* getIndexExpr(unsigned Idx) {\n    assert(Idx < NumExprs && \"Subscript out of range\");\n    return getTrailingObjects<Expr *>()[Idx];\n  }\n\n  const Expr *getIndexExpr(unsigned Idx) const {\n    assert(Idx < NumExprs && \"Subscript out of range\");\n    return getTrailingObjects<Expr *>()[Idx];\n  }\n\n  void setIndexExpr(unsigned Idx, Expr* E) {\n    assert(Idx < NumComps && \"Subscript out of range\");\n    getTrailingObjects<Expr *>()[Idx] = E;\n  }\n\n  unsigned getNumExpressions() const {\n    return NumExprs;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return OperatorLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == OffsetOfExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    Stmt **begin = reinterpret_cast<Stmt **>(getTrailingObjects<Expr *>());\n    return child_range(begin, begin + NumExprs);\n  }\n  const_child_range children() const {\n    Stmt *const *begin =\n        reinterpret_cast<Stmt *const *>(getTrailingObjects<Expr *>());\n    return const_child_range(begin, begin + NumExprs);\n  }\n  friend TrailingObjects;\n};\n\n/// UnaryExprOrTypeTraitExpr - expression with either a type or (unevaluated)\n/// expression operand.  Used for sizeof/alignof (C99 6.5.3.4) and\n/// vec_step (OpenCL 1.1 6.11.12).\nclass UnaryExprOrTypeTraitExpr : public Expr {\n  union {\n    TypeSourceInfo *Ty;\n    Stmt *Ex;\n  } Argument;\n  SourceLocation OpLoc, RParenLoc;\n\npublic:\n  UnaryExprOrTypeTraitExpr(UnaryExprOrTypeTrait ExprKind, TypeSourceInfo *TInfo,\n                           QualType resultType, SourceLocation op,\n                           SourceLocation rp)\n      : Expr(UnaryExprOrTypeTraitExprClass, resultType, VK_RValue, OK_Ordinary),\n        OpLoc(op), RParenLoc(rp) {\n    assert(ExprKind <= UETT_Last && \"invalid enum value!\");\n    UnaryExprOrTypeTraitExprBits.Kind = ExprKind;\n    assert(static_cast<unsigned>(ExprKind) ==\n               UnaryExprOrTypeTraitExprBits.Kind &&\n           \"UnaryExprOrTypeTraitExprBits.Kind overflow!\");\n    UnaryExprOrTypeTraitExprBits.IsType = true;\n    Argument.Ty = TInfo;\n    setDependence(computeDependence(this));\n  }\n\n  UnaryExprOrTypeTraitExpr(UnaryExprOrTypeTrait ExprKind, Expr *E,\n                           QualType resultType, SourceLocation op,\n                           SourceLocation rp);\n\n  /// Construct an empty sizeof/alignof expression.\n  explicit UnaryExprOrTypeTraitExpr(EmptyShell Empty)\n    : Expr(UnaryExprOrTypeTraitExprClass, Empty) { }\n\n  UnaryExprOrTypeTrait getKind() const {\n    return static_cast<UnaryExprOrTypeTrait>(UnaryExprOrTypeTraitExprBits.Kind);\n  }\n  void setKind(UnaryExprOrTypeTrait K) {\n    assert(K <= UETT_Last && \"invalid enum value!\");\n    UnaryExprOrTypeTraitExprBits.Kind = K;\n    assert(static_cast<unsigned>(K) == UnaryExprOrTypeTraitExprBits.Kind &&\n           \"UnaryExprOrTypeTraitExprBits.Kind overflow!\");\n  }\n\n  bool isArgumentType() const { return UnaryExprOrTypeTraitExprBits.IsType; }\n  QualType getArgumentType() const {\n    return getArgumentTypeInfo()->getType();\n  }\n  TypeSourceInfo *getArgumentTypeInfo() const {\n    assert(isArgumentType() && \"calling getArgumentType() when arg is expr\");\n    return Argument.Ty;\n  }\n  Expr *getArgumentExpr() {\n    assert(!isArgumentType() && \"calling getArgumentExpr() when arg is type\");\n    return static_cast<Expr*>(Argument.Ex);\n  }\n  const Expr *getArgumentExpr() const {\n    return const_cast<UnaryExprOrTypeTraitExpr*>(this)->getArgumentExpr();\n  }\n\n  void setArgument(Expr *E) {\n    Argument.Ex = E;\n    UnaryExprOrTypeTraitExprBits.IsType = false;\n  }\n  void setArgument(TypeSourceInfo *TInfo) {\n    Argument.Ty = TInfo;\n    UnaryExprOrTypeTraitExprBits.IsType = true;\n  }\n\n  /// Gets the argument type, or the type of the argument expression, whichever\n  /// is appropriate.\n  QualType getTypeOfArgument() const {\n    return isArgumentType() ? getArgumentType() : getArgumentExpr()->getType();\n  }\n\n  SourceLocation getOperatorLoc() const { return OpLoc; }\n  void setOperatorLoc(SourceLocation L) { OpLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return OpLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == UnaryExprOrTypeTraitExprClass;\n  }\n\n  // Iterators\n  child_range children();\n  const_child_range children() const;\n};\n\n//===----------------------------------------------------------------------===//\n// Postfix Operators.\n//===----------------------------------------------------------------------===//\n\n/// ArraySubscriptExpr - [C99 6.5.2.1] Array Subscripting.\nclass ArraySubscriptExpr : public Expr {\n  enum { LHS, RHS, END_EXPR };\n  Stmt *SubExprs[END_EXPR];\n\n  bool lhsIsBase() const { return getRHS()->getType()->isIntegerType(); }\n\npublic:\n  ArraySubscriptExpr(Expr *lhs, Expr *rhs, QualType t, ExprValueKind VK,\n                     ExprObjectKind OK, SourceLocation rbracketloc)\n      : Expr(ArraySubscriptExprClass, t, VK, OK) {\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = rbracketloc;\n    setDependence(computeDependence(this));\n  }\n\n  /// Create an empty array subscript expression.\n  explicit ArraySubscriptExpr(EmptyShell Shell)\n    : Expr(ArraySubscriptExprClass, Shell) { }\n\n  /// An array access can be written A[4] or 4[A] (both are equivalent).\n  /// - getBase() and getIdx() always present the normalized view: A[4].\n  ///    In this case getBase() returns \"A\" and getIdx() returns \"4\".\n  /// - getLHS() and getRHS() present the syntactic view. e.g. for\n  ///    4[A] getLHS() returns \"4\".\n  /// Note: Because vector element access is also written A[4] we must\n  /// predicate the format conversion in getBase and getIdx only on the\n  /// the type of the RHS, as it is possible for the LHS to be a vector of\n  /// integer type\n  Expr *getLHS() { return cast<Expr>(SubExprs[LHS]); }\n  const Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  void setLHS(Expr *E) { SubExprs[LHS] = E; }\n\n  Expr *getRHS() { return cast<Expr>(SubExprs[RHS]); }\n  const Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n  void setRHS(Expr *E) { SubExprs[RHS] = E; }\n\n  Expr *getBase() { return lhsIsBase() ? getLHS() : getRHS(); }\n  const Expr *getBase() const { return lhsIsBase() ? getLHS() : getRHS(); }\n\n  Expr *getIdx() { return lhsIsBase() ? getRHS() : getLHS(); }\n  const Expr *getIdx() const { return lhsIsBase() ? getRHS() : getLHS(); }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getLHS()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const { return getRBracketLoc(); }\n\n  SourceLocation getRBracketLoc() const {\n    return ArrayOrMatrixSubscriptExprBits.RBracketLoc;\n  }\n  void setRBracketLoc(SourceLocation L) {\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = L;\n  }\n\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return getBase()->getExprLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ArraySubscriptExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// MatrixSubscriptExpr - Matrix subscript expression for the MatrixType\n/// extension.\n/// MatrixSubscriptExpr can be either incomplete (only Base and RowIdx are set\n/// so far, the type is IncompleteMatrixIdx) or complete (Base, RowIdx and\n/// ColumnIdx refer to valid expressions). Incomplete matrix expressions only\n/// exist during the initial construction of the AST.\nclass MatrixSubscriptExpr : public Expr {\n  enum { BASE, ROW_IDX, COLUMN_IDX, END_EXPR };\n  Stmt *SubExprs[END_EXPR];\n\npublic:\n  MatrixSubscriptExpr(Expr *Base, Expr *RowIdx, Expr *ColumnIdx, QualType T,\n                      SourceLocation RBracketLoc)\n      : Expr(MatrixSubscriptExprClass, T, Base->getValueKind(),\n             OK_MatrixComponent) {\n    SubExprs[BASE] = Base;\n    SubExprs[ROW_IDX] = RowIdx;\n    SubExprs[COLUMN_IDX] = ColumnIdx;\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = RBracketLoc;\n    setDependence(computeDependence(this));\n  }\n\n  /// Create an empty matrix subscript expression.\n  explicit MatrixSubscriptExpr(EmptyShell Shell)\n      : Expr(MatrixSubscriptExprClass, Shell) {}\n\n  bool isIncomplete() const {\n    bool IsIncomplete = hasPlaceholderType(BuiltinType::IncompleteMatrixIdx);\n    assert((SubExprs[COLUMN_IDX] || IsIncomplete) &&\n           \"expressions without column index must be marked as incomplete\");\n    return IsIncomplete;\n  }\n  Expr *getBase() { return cast<Expr>(SubExprs[BASE]); }\n  const Expr *getBase() const { return cast<Expr>(SubExprs[BASE]); }\n  void setBase(Expr *E) { SubExprs[BASE] = E; }\n\n  Expr *getRowIdx() { return cast<Expr>(SubExprs[ROW_IDX]); }\n  const Expr *getRowIdx() const { return cast<Expr>(SubExprs[ROW_IDX]); }\n  void setRowIdx(Expr *E) { SubExprs[ROW_IDX] = E; }\n\n  Expr *getColumnIdx() { return cast_or_null<Expr>(SubExprs[COLUMN_IDX]); }\n  const Expr *getColumnIdx() const {\n    assert(!isIncomplete() &&\n           \"cannot get the column index of an incomplete expression\");\n    return cast<Expr>(SubExprs[COLUMN_IDX]);\n  }\n  void setColumnIdx(Expr *E) { SubExprs[COLUMN_IDX] = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getBase()->getBeginLoc();\n  }\n\n  SourceLocation getEndLoc() const { return getRBracketLoc(); }\n\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return getBase()->getExprLoc();\n  }\n\n  SourceLocation getRBracketLoc() const {\n    return ArrayOrMatrixSubscriptExprBits.RBracketLoc;\n  }\n  void setRBracketLoc(SourceLocation L) {\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = L;\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == MatrixSubscriptExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// CallExpr - Represents a function call (C99 6.5.2.2, C++ [expr.call]).\n/// CallExpr itself represents a normal function call, e.g., \"f(x, 2)\",\n/// while its subclasses may represent alternative syntax that (semantically)\n/// results in a function call. For example, CXXOperatorCallExpr is\n/// a subclass for overloaded operator calls that use operator syntax, e.g.,\n/// \"str1 + str2\" to resolve to a function call.\nclass CallExpr : public Expr {\n  enum { FN = 0, PREARGS_START = 1 };\n\n  /// The number of arguments in the call expression.\n  unsigned NumArgs;\n\n  /// The location of the right parenthese. This has a different meaning for\n  /// the derived classes of CallExpr.\n  SourceLocation RParenLoc;\n\n  // CallExpr store some data in trailing objects. However since CallExpr\n  // is used a base of other expression classes we cannot use\n  // llvm::TrailingObjects. Instead we manually perform the pointer arithmetic\n  // and casts.\n  //\n  // The trailing objects are in order:\n  //\n  // * A single \"Stmt *\" for the callee expression.\n  //\n  // * An array of getNumPreArgs() \"Stmt *\" for the pre-argument expressions.\n  //\n  // * An array of getNumArgs() \"Stmt *\" for the argument expressions.\n  //\n  // * An optional of type FPOptionsOverride.\n  //\n  // Note that we store the offset in bytes from the this pointer to the start\n  // of the trailing objects. It would be perfectly possible to compute it\n  // based on the dynamic kind of the CallExpr. However 1.) we have plenty of\n  // space in the bit-fields of Stmt. 2.) It was benchmarked to be faster to\n  // compute this once and then load the offset from the bit-fields of Stmt,\n  // instead of re-computing the offset each time the trailing objects are\n  // accessed.\n\n  /// Return a pointer to the start of the trailing array of \"Stmt *\".\n  Stmt **getTrailingStmts() {\n    return reinterpret_cast<Stmt **>(reinterpret_cast<char *>(this) +\n                                     CallExprBits.OffsetToTrailingObjects);\n  }\n  Stmt *const *getTrailingStmts() const {\n    return const_cast<CallExpr *>(this)->getTrailingStmts();\n  }\n\n  /// Map a statement class to the appropriate offset in bytes from the\n  /// this pointer to the trailing objects.\n  static unsigned offsetToTrailingObjects(StmtClass SC);\n\n  unsigned getSizeOfTrailingStmts() const {\n    return (1 + getNumPreArgs() + getNumArgs()) * sizeof(Stmt *);\n  }\n\n  size_t getOffsetOfTrailingFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return CallExprBits.OffsetToTrailingObjects + getSizeOfTrailingStmts();\n  }\n\npublic:\n  enum class ADLCallKind : bool { NotADL, UsesADL };\n  static constexpr ADLCallKind NotADL = ADLCallKind::NotADL;\n  static constexpr ADLCallKind UsesADL = ADLCallKind::UsesADL;\n\nprotected:\n  /// Build a call expression, assuming that appropriate storage has been\n  /// allocated for the trailing objects.\n  CallExpr(StmtClass SC, Expr *Fn, ArrayRef<Expr *> PreArgs,\n           ArrayRef<Expr *> Args, QualType Ty, ExprValueKind VK,\n           SourceLocation RParenLoc, FPOptionsOverride FPFeatures,\n           unsigned MinNumArgs, ADLCallKind UsesADL);\n\n  /// Build an empty call expression, for deserialization.\n  CallExpr(StmtClass SC, unsigned NumPreArgs, unsigned NumArgs,\n           bool hasFPFeatures, EmptyShell Empty);\n\n  /// Return the size in bytes needed for the trailing objects.\n  /// Used by the derived classes to allocate the right amount of storage.\n  static unsigned sizeOfTrailingObjects(unsigned NumPreArgs, unsigned NumArgs,\n                                        bool HasFPFeatures) {\n    return (1 + NumPreArgs + NumArgs) * sizeof(Stmt *) +\n           HasFPFeatures * sizeof(FPOptionsOverride);\n  }\n\n  Stmt *getPreArg(unsigned I) {\n    assert(I < getNumPreArgs() && \"Prearg access out of range!\");\n    return getTrailingStmts()[PREARGS_START + I];\n  }\n  const Stmt *getPreArg(unsigned I) const {\n    assert(I < getNumPreArgs() && \"Prearg access out of range!\");\n    return getTrailingStmts()[PREARGS_START + I];\n  }\n  void setPreArg(unsigned I, Stmt *PreArg) {\n    assert(I < getNumPreArgs() && \"Prearg access out of range!\");\n    getTrailingStmts()[PREARGS_START + I] = PreArg;\n  }\n\n  unsigned getNumPreArgs() const { return CallExprBits.NumPreArgs; }\n\n  /// Return a pointer to the trailing FPOptions\n  FPOptionsOverride *getTrailingFPFeatures() {\n    assert(hasStoredFPFeatures());\n    return reinterpret_cast<FPOptionsOverride *>(\n        reinterpret_cast<char *>(this) + CallExprBits.OffsetToTrailingObjects +\n        getSizeOfTrailingStmts());\n  }\n  const FPOptionsOverride *getTrailingFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return reinterpret_cast<const FPOptionsOverride *>(\n        reinterpret_cast<const char *>(this) +\n        CallExprBits.OffsetToTrailingObjects + getSizeOfTrailingStmts());\n  }\n\npublic:\n  /// Create a call expression.\n  /// \\param Fn     The callee expression,\n  /// \\param Args   The argument array,\n  /// \\param Ty     The type of the call expression (which is *not* the return\n  ///               type in general),\n  /// \\param VK     The value kind of the call expression (lvalue, rvalue, ...),\n  /// \\param RParenLoc  The location of the right parenthesis in the call\n  ///                   expression.\n  /// \\param FPFeatures Floating-point features associated with the call,\n  /// \\param MinNumArgs Specifies the minimum number of arguments. The actual\n  ///                   number of arguments will be the greater of Args.size()\n  ///                   and MinNumArgs. This is used in a few places to allocate\n  ///                   enough storage for the default arguments.\n  /// \\param UsesADL    Specifies whether the callee was found through\n  ///                   argument-dependent lookup.\n  ///\n  /// Note that you can use CreateTemporary if you need a temporary call\n  /// expression on the stack.\n  static CallExpr *Create(const ASTContext &Ctx, Expr *Fn,\n                          ArrayRef<Expr *> Args, QualType Ty, ExprValueKind VK,\n                          SourceLocation RParenLoc,\n                          FPOptionsOverride FPFeatures, unsigned MinNumArgs = 0,\n                          ADLCallKind UsesADL = NotADL);\n\n  /// Create a temporary call expression with no arguments in the memory\n  /// pointed to by Mem. Mem must points to at least sizeof(CallExpr)\n  /// + sizeof(Stmt *) bytes of storage, aligned to alignof(CallExpr):\n  ///\n  /// \\code{.cpp}\n  ///   alignas(CallExpr) char Buffer[sizeof(CallExpr) + sizeof(Stmt *)];\n  ///   CallExpr *TheCall = CallExpr::CreateTemporary(Buffer, etc);\n  /// \\endcode\n  static CallExpr *CreateTemporary(void *Mem, Expr *Fn, QualType Ty,\n                                   ExprValueKind VK, SourceLocation RParenLoc,\n                                   ADLCallKind UsesADL = NotADL);\n\n  /// Create an empty call expression, for deserialization.\n  static CallExpr *CreateEmpty(const ASTContext &Ctx, unsigned NumArgs,\n                               bool HasFPFeatures, EmptyShell Empty);\n\n  Expr *getCallee() { return cast<Expr>(getTrailingStmts()[FN]); }\n  const Expr *getCallee() const { return cast<Expr>(getTrailingStmts()[FN]); }\n  void setCallee(Expr *F) { getTrailingStmts()[FN] = F; }\n\n  ADLCallKind getADLCallKind() const {\n    return static_cast<ADLCallKind>(CallExprBits.UsesADL);\n  }\n  void setADLCallKind(ADLCallKind V = UsesADL) {\n    CallExprBits.UsesADL = static_cast<bool>(V);\n  }\n  bool usesADL() const { return getADLCallKind() == UsesADL; }\n\n  bool hasStoredFPFeatures() const { return CallExprBits.HasFPFeatures; }\n\n  Decl *getCalleeDecl() { return getCallee()->getReferencedDeclOfCallee(); }\n  const Decl *getCalleeDecl() const {\n    return getCallee()->getReferencedDeclOfCallee();\n  }\n\n  /// If the callee is a FunctionDecl, return it. Otherwise return null.\n  FunctionDecl *getDirectCallee() {\n    return dyn_cast_or_null<FunctionDecl>(getCalleeDecl());\n  }\n  const FunctionDecl *getDirectCallee() const {\n    return dyn_cast_or_null<FunctionDecl>(getCalleeDecl());\n  }\n\n  /// getNumArgs - Return the number of actual arguments to this call.\n  unsigned getNumArgs() const { return NumArgs; }\n\n  /// Retrieve the call arguments.\n  Expr **getArgs() {\n    return reinterpret_cast<Expr **>(getTrailingStmts() + PREARGS_START +\n                                     getNumPreArgs());\n  }\n  const Expr *const *getArgs() const {\n    return reinterpret_cast<const Expr *const *>(\n        getTrailingStmts() + PREARGS_START + getNumPreArgs());\n  }\n\n  /// getArg - Return the specified argument.\n  Expr *getArg(unsigned Arg) {\n    assert(Arg < getNumArgs() && \"Arg access out of range!\");\n    return getArgs()[Arg];\n  }\n  const Expr *getArg(unsigned Arg) const {\n    assert(Arg < getNumArgs() && \"Arg access out of range!\");\n    return getArgs()[Arg];\n  }\n\n  /// setArg - Set the specified argument.\n  void setArg(unsigned Arg, Expr *ArgExpr) {\n    assert(Arg < getNumArgs() && \"Arg access out of range!\");\n    getArgs()[Arg] = ArgExpr;\n  }\n\n  /// Reduce the number of arguments in this call expression. This is used for\n  /// example during error recovery to drop extra arguments. There is no way\n  /// to perform the opposite because: 1.) We don't track how much storage\n  /// we have for the argument array 2.) This would potentially require growing\n  /// the argument array, something we cannot support since the arguments are\n  /// stored in a trailing array.\n  void shrinkNumArgs(unsigned NewNumArgs) {\n    assert((NewNumArgs <= getNumArgs()) &&\n           \"shrinkNumArgs cannot increase the number of arguments!\");\n    NumArgs = NewNumArgs;\n  }\n\n  /// Bluntly set a new number of arguments without doing any checks whatsoever.\n  /// Only used during construction of a CallExpr in a few places in Sema.\n  /// FIXME: Find a way to remove it.\n  void setNumArgsUnsafe(unsigned NewNumArgs) { NumArgs = NewNumArgs; }\n\n  typedef ExprIterator arg_iterator;\n  typedef ConstExprIterator const_arg_iterator;\n  typedef llvm::iterator_range<arg_iterator> arg_range;\n  typedef llvm::iterator_range<const_arg_iterator> const_arg_range;\n\n  arg_range arguments() { return arg_range(arg_begin(), arg_end()); }\n  const_arg_range arguments() const {\n    return const_arg_range(arg_begin(), arg_end());\n  }\n\n  arg_iterator arg_begin() {\n    return getTrailingStmts() + PREARGS_START + getNumPreArgs();\n  }\n  arg_iterator arg_end() { return arg_begin() + getNumArgs(); }\n\n  const_arg_iterator arg_begin() const {\n    return getTrailingStmts() + PREARGS_START + getNumPreArgs();\n  }\n  const_arg_iterator arg_end() const { return arg_begin() + getNumArgs(); }\n\n  /// This method provides fast access to all the subexpressions of\n  /// a CallExpr without going through the slower virtual child_iterator\n  /// interface.  This provides efficient reverse iteration of the\n  /// subexpressions.  This is currently used for CFG construction.\n  ArrayRef<Stmt *> getRawSubExprs() {\n    return llvm::makeArrayRef(getTrailingStmts(),\n                              PREARGS_START + getNumPreArgs() + getNumArgs());\n  }\n\n  /// getNumCommas - Return the number of commas that must have been present in\n  /// this function call.\n  unsigned getNumCommas() const { return getNumArgs() ? getNumArgs() - 1 : 0; }\n\n  /// Get FPOptionsOverride from trailing storage.\n  FPOptionsOverride getStoredFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return *getTrailingFPFeatures();\n  }\n  /// Set FPOptionsOverride in trailing storage. Used only by Serialization.\n  void setStoredFPFeatures(FPOptionsOverride F) {\n    assert(hasStoredFPFeatures());\n    *getTrailingFPFeatures() = F;\n  }\n\n  // Get the FP features status of this operator. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n\n  FPOptionsOverride getFPFeatures() const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  /// getBuiltinCallee - If this is a call to a builtin, return the builtin ID\n  /// of the callee. If not, return 0.\n  unsigned getBuiltinCallee() const;\n\n  /// Returns \\c true if this is a call to a builtin which does not\n  /// evaluate side-effects within its arguments.\n  bool isUnevaluatedBuiltinCall(const ASTContext &Ctx) const;\n\n  /// getCallReturnType - Get the return type of the call expr. This is not\n  /// always the type of the expr itself, if the return type is a reference\n  /// type.\n  QualType getCallReturnType(const ASTContext &Ctx) const;\n\n  /// Returns the WarnUnusedResultAttr that is either declared on the called\n  /// function, or its return type declaration.\n  const Attr *getUnusedResultAttr(const ASTContext &Ctx) const;\n\n  /// Returns true if this call expression should warn on unused results.\n  bool hasUnusedResultAttr(const ASTContext &Ctx) const {\n    return getUnusedResultAttr(Ctx) != nullptr;\n  }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  /// Return true if this is a call to __assume() or __builtin_assume() with\n  /// a non-value-dependent constant parameter evaluating as false.\n  bool isBuiltinAssumeFalse(const ASTContext &Ctx) const;\n\n  /// Used by Sema to implement MSVC-compatible delayed name lookup.\n  /// (Usually Exprs themselves should set dependence).\n  void markDependentForPostponedNameLookup() {\n    setDependence(getDependence() | ExprDependence::TypeValueInstantiation);\n  }\n\n  bool isCallToStdMove() const {\n    const FunctionDecl *FD = getDirectCallee();\n    return getNumArgs() == 1 && FD && FD->isInStdNamespace() &&\n           FD->getIdentifier() && FD->getIdentifier()->isStr(\"move\");\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstCallExprConstant &&\n           T->getStmtClass() <= lastCallExprConstant;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(getTrailingStmts(), getTrailingStmts() + PREARGS_START +\n                                               getNumPreArgs() + getNumArgs());\n  }\n\n  const_child_range children() const {\n    return const_child_range(getTrailingStmts(),\n                             getTrailingStmts() + PREARGS_START +\n                                 getNumPreArgs() + getNumArgs());\n  }\n};\n\n/// Extra data stored in some MemberExpr objects.\nstruct MemberExprNameQualifier {\n  /// The nested-name-specifier that qualifies the name, including\n  /// source-location information.\n  NestedNameSpecifierLoc QualifierLoc;\n\n  /// The DeclAccessPair through which the MemberDecl was found due to\n  /// name qualifiers.\n  DeclAccessPair FoundDecl;\n};\n\n/// MemberExpr - [C99 6.5.2.3] Structure and Union Members.  X->F and X.F.\n///\nclass MemberExpr final\n    : public Expr,\n      private llvm::TrailingObjects<MemberExpr, MemberExprNameQualifier,\n                                    ASTTemplateKWAndArgsInfo,\n                                    TemplateArgumentLoc> {\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n  friend TrailingObjects;\n\n  /// Base - the expression for the base pointer or structure references.  In\n  /// X.F, this is \"X\".\n  Stmt *Base;\n\n  /// MemberDecl - This is the decl being referenced by the field/member name.\n  /// In X.F, this is the decl referenced by F.\n  ValueDecl *MemberDecl;\n\n  /// MemberDNLoc - Provides source/type location info for the\n  /// declaration name embedded in MemberDecl.\n  DeclarationNameLoc MemberDNLoc;\n\n  /// MemberLoc - This is the location of the member name.\n  SourceLocation MemberLoc;\n\n  size_t numTrailingObjects(OverloadToken<MemberExprNameQualifier>) const {\n    return hasQualifierOrFoundDecl();\n  }\n\n  size_t numTrailingObjects(OverloadToken<ASTTemplateKWAndArgsInfo>) const {\n    return hasTemplateKWAndArgsInfo();\n  }\n\n  bool hasQualifierOrFoundDecl() const {\n    return MemberExprBits.HasQualifierOrFoundDecl;\n  }\n\n  bool hasTemplateKWAndArgsInfo() const {\n    return MemberExprBits.HasTemplateKWAndArgsInfo;\n  }\n\n  MemberExpr(Expr *Base, bool IsArrow, SourceLocation OperatorLoc,\n             ValueDecl *MemberDecl, const DeclarationNameInfo &NameInfo,\n             QualType T, ExprValueKind VK, ExprObjectKind OK,\n             NonOdrUseReason NOUR);\n  MemberExpr(EmptyShell Empty)\n      : Expr(MemberExprClass, Empty), Base(), MemberDecl() {}\n\npublic:\n  static MemberExpr *Create(const ASTContext &C, Expr *Base, bool IsArrow,\n                            SourceLocation OperatorLoc,\n                            NestedNameSpecifierLoc QualifierLoc,\n                            SourceLocation TemplateKWLoc, ValueDecl *MemberDecl,\n                            DeclAccessPair FoundDecl,\n                            DeclarationNameInfo MemberNameInfo,\n                            const TemplateArgumentListInfo *TemplateArgs,\n                            QualType T, ExprValueKind VK, ExprObjectKind OK,\n                            NonOdrUseReason NOUR);\n\n  /// Create an implicit MemberExpr, with no location, qualifier, template\n  /// arguments, and so on. Suitable only for non-static member access.\n  static MemberExpr *CreateImplicit(const ASTContext &C, Expr *Base,\n                                    bool IsArrow, ValueDecl *MemberDecl,\n                                    QualType T, ExprValueKind VK,\n                                    ExprObjectKind OK) {\n    return Create(C, Base, IsArrow, SourceLocation(), NestedNameSpecifierLoc(),\n                  SourceLocation(), MemberDecl,\n                  DeclAccessPair::make(MemberDecl, MemberDecl->getAccess()),\n                  DeclarationNameInfo(), nullptr, T, VK, OK, NOUR_None);\n  }\n\n  static MemberExpr *CreateEmpty(const ASTContext &Context, bool HasQualifier,\n                                 bool HasFoundDecl,\n                                 bool HasTemplateKWAndArgsInfo,\n                                 unsigned NumTemplateArgs);\n\n  void setBase(Expr *E) { Base = E; }\n  Expr *getBase() const { return cast<Expr>(Base); }\n\n  /// Retrieve the member declaration to which this expression refers.\n  ///\n  /// The returned declaration will be a FieldDecl or (in C++) a VarDecl (for\n  /// static data members), a CXXMethodDecl, or an EnumConstantDecl.\n  ValueDecl *getMemberDecl() const { return MemberDecl; }\n  void setMemberDecl(ValueDecl *D);\n\n  /// Retrieves the declaration found by lookup.\n  DeclAccessPair getFoundDecl() const {\n    if (!hasQualifierOrFoundDecl())\n      return DeclAccessPair::make(getMemberDecl(),\n                                  getMemberDecl()->getAccess());\n    return getTrailingObjects<MemberExprNameQualifier>()->FoundDecl;\n  }\n\n  /// Determines whether this member expression actually had\n  /// a C++ nested-name-specifier prior to the name of the member, e.g.,\n  /// x->Base::foo.\n  bool hasQualifier() const { return getQualifier() != nullptr; }\n\n  /// If the member name was qualified, retrieves the\n  /// nested-name-specifier that precedes the member name, with source-location\n  /// information.\n  NestedNameSpecifierLoc getQualifierLoc() const {\n    if (!hasQualifierOrFoundDecl())\n      return NestedNameSpecifierLoc();\n    return getTrailingObjects<MemberExprNameQualifier>()->QualifierLoc;\n  }\n\n  /// If the member name was qualified, retrieves the\n  /// nested-name-specifier that precedes the member name. Otherwise, returns\n  /// NULL.\n  NestedNameSpecifier *getQualifier() const {\n    return getQualifierLoc().getNestedNameSpecifier();\n  }\n\n  /// Retrieve the location of the template keyword preceding\n  /// the member name, if any.\n  SourceLocation getTemplateKeywordLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->TemplateKWLoc;\n  }\n\n  /// Retrieve the location of the left angle bracket starting the\n  /// explicit template argument list following the member name, if any.\n  SourceLocation getLAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->LAngleLoc;\n  }\n\n  /// Retrieve the location of the right angle bracket ending the\n  /// explicit template argument list following the member name, if any.\n  SourceLocation getRAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->RAngleLoc;\n  }\n\n  /// Determines whether the member name was preceded by the template keyword.\n  bool hasTemplateKeyword() const { return getTemplateKeywordLoc().isValid(); }\n\n  /// Determines whether the member name was followed by an\n  /// explicit template argument list.\n  bool hasExplicitTemplateArgs() const { return getLAngleLoc().isValid(); }\n\n  /// Copies the template arguments (if present) into the given\n  /// structure.\n  void copyTemplateArgumentsInto(TemplateArgumentListInfo &List) const {\n    if (hasExplicitTemplateArgs())\n      getTrailingObjects<ASTTemplateKWAndArgsInfo>()->copyInto(\n          getTrailingObjects<TemplateArgumentLoc>(), List);\n  }\n\n  /// Retrieve the template arguments provided as part of this\n  /// template-id.\n  const TemplateArgumentLoc *getTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return nullptr;\n\n    return getTrailingObjects<TemplateArgumentLoc>();\n  }\n\n  /// Retrieve the number of template arguments provided as part of this\n  /// template-id.\n  unsigned getNumTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return 0;\n\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->NumTemplateArgs;\n  }\n\n  ArrayRef<TemplateArgumentLoc> template_arguments() const {\n    return {getTemplateArgs(), getNumTemplateArgs()};\n  }\n\n  /// Retrieve the member declaration name info.\n  DeclarationNameInfo getMemberNameInfo() const {\n    return DeclarationNameInfo(MemberDecl->getDeclName(),\n                               MemberLoc, MemberDNLoc);\n  }\n\n  SourceLocation getOperatorLoc() const { return MemberExprBits.OperatorLoc; }\n\n  bool isArrow() const { return MemberExprBits.IsArrow; }\n  void setArrow(bool A) { MemberExprBits.IsArrow = A; }\n\n  /// getMemberLoc - Return the location of the \"member\", in X->F, it is the\n  /// location of 'F'.\n  SourceLocation getMemberLoc() const { return MemberLoc; }\n  void setMemberLoc(SourceLocation L) { MemberLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  SourceLocation getExprLoc() const LLVM_READONLY { return MemberLoc; }\n\n  /// Determine whether the base of this explicit is implicit.\n  bool isImplicitAccess() const {\n    return getBase() && getBase()->isImplicitCXXThis();\n  }\n\n  /// Returns true if this member expression refers to a method that\n  /// was resolved from an overloaded set having size greater than 1.\n  bool hadMultipleCandidates() const {\n    return MemberExprBits.HadMultipleCandidates;\n  }\n  /// Sets the flag telling whether this expression refers to\n  /// a method that was resolved from an overloaded set having size\n  /// greater than 1.\n  void setHadMultipleCandidates(bool V = true) {\n    MemberExprBits.HadMultipleCandidates = V;\n  }\n\n  /// Returns true if virtual dispatch is performed.\n  /// If the member access is fully qualified, (i.e. X::f()), virtual\n  /// dispatching is not performed. In -fapple-kext mode qualified\n  /// calls to virtual method will still go through the vtable.\n  bool performsVirtualDispatch(const LangOptions &LO) const {\n    return LO.AppleKext || !hasQualifier();\n  }\n\n  /// Is this expression a non-odr-use reference, and if so, why?\n  /// This is only meaningful if the named member is a static member.\n  NonOdrUseReason isNonOdrUse() const {\n    return static_cast<NonOdrUseReason>(MemberExprBits.NonOdrUseReason);\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == MemberExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Base, &Base+1); }\n  const_child_range children() const {\n    return const_child_range(&Base, &Base + 1);\n  }\n};\n\n/// CompoundLiteralExpr - [C99 6.5.2.5]\n///\nclass CompoundLiteralExpr : public Expr {\n  /// LParenLoc - If non-null, this is the location of the left paren in a\n  /// compound literal like \"(int){4}\".  This can be null if this is a\n  /// synthesized compound expression.\n  SourceLocation LParenLoc;\n\n  /// The type as written.  This can be an incomplete array type, in\n  /// which case the actual expression type will be different.\n  /// The int part of the pair stores whether this expr is file scope.\n  llvm::PointerIntPair<TypeSourceInfo *, 1, bool> TInfoAndScope;\n  Stmt *Init;\npublic:\n  CompoundLiteralExpr(SourceLocation lparenloc, TypeSourceInfo *tinfo,\n                      QualType T, ExprValueKind VK, Expr *init, bool fileScope)\n      : Expr(CompoundLiteralExprClass, T, VK, OK_Ordinary),\n        LParenLoc(lparenloc), TInfoAndScope(tinfo, fileScope), Init(init) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Construct an empty compound literal.\n  explicit CompoundLiteralExpr(EmptyShell Empty)\n    : Expr(CompoundLiteralExprClass, Empty) { }\n\n  const Expr *getInitializer() const { return cast<Expr>(Init); }\n  Expr *getInitializer() { return cast<Expr>(Init); }\n  void setInitializer(Expr *E) { Init = E; }\n\n  bool isFileScope() const { return TInfoAndScope.getInt(); }\n  void setFileScope(bool FS) { TInfoAndScope.setInt(FS); }\n\n  SourceLocation getLParenLoc() const { return LParenLoc; }\n  void setLParenLoc(SourceLocation L) { LParenLoc = L; }\n\n  TypeSourceInfo *getTypeSourceInfo() const {\n    return TInfoAndScope.getPointer();\n  }\n  void setTypeSourceInfo(TypeSourceInfo *tinfo) {\n    TInfoAndScope.setPointer(tinfo);\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    // FIXME: Init should never be null.\n    if (!Init)\n      return SourceLocation();\n    if (LParenLoc.isInvalid())\n      return Init->getBeginLoc();\n    return LParenLoc;\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    // FIXME: Init should never be null.\n    if (!Init)\n      return SourceLocation();\n    return Init->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == CompoundLiteralExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Init, &Init+1); }\n  const_child_range children() const {\n    return const_child_range(&Init, &Init + 1);\n  }\n};\n\n/// CastExpr - Base class for type casts, including both implicit\n/// casts (ImplicitCastExpr) and explicit casts that have some\n/// representation in the source code (ExplicitCastExpr's derived\n/// classes).\nclass CastExpr : public Expr {\n  Stmt *Op;\n\n  bool CastConsistency() const;\n\n  const CXXBaseSpecifier * const *path_buffer() const {\n    return const_cast<CastExpr*>(this)->path_buffer();\n  }\n  CXXBaseSpecifier **path_buffer();\n\n  friend class ASTStmtReader;\n\nprotected:\n  CastExpr(StmtClass SC, QualType ty, ExprValueKind VK, const CastKind kind,\n           Expr *op, unsigned BasePathSize, bool HasFPFeatures)\n      : Expr(SC, ty, VK, OK_Ordinary), Op(op) {\n    CastExprBits.Kind = kind;\n    CastExprBits.PartOfExplicitCast = false;\n    CastExprBits.BasePathSize = BasePathSize;\n    assert((CastExprBits.BasePathSize == BasePathSize) &&\n           \"BasePathSize overflow!\");\n    setDependence(computeDependence(this));\n    assert(CastConsistency());\n    CastExprBits.HasFPFeatures = HasFPFeatures;\n  }\n\n  /// Construct an empty cast.\n  CastExpr(StmtClass SC, EmptyShell Empty, unsigned BasePathSize,\n           bool HasFPFeatures)\n      : Expr(SC, Empty) {\n    CastExprBits.PartOfExplicitCast = false;\n    CastExprBits.BasePathSize = BasePathSize;\n    CastExprBits.HasFPFeatures = HasFPFeatures;\n    assert((CastExprBits.BasePathSize == BasePathSize) &&\n           \"BasePathSize overflow!\");\n  }\n\n  /// Return a pointer to the trailing FPOptions.\n  /// \\pre hasStoredFPFeatures() == true\n  FPOptionsOverride *getTrailingFPFeatures();\n  const FPOptionsOverride *getTrailingFPFeatures() const {\n    return const_cast<CastExpr *>(this)->getTrailingFPFeatures();\n  }\n\npublic:\n  CastKind getCastKind() const { return (CastKind) CastExprBits.Kind; }\n  void setCastKind(CastKind K) { CastExprBits.Kind = K; }\n\n  static const char *getCastKindName(CastKind CK);\n  const char *getCastKindName() const { return getCastKindName(getCastKind()); }\n\n  Expr *getSubExpr() { return cast<Expr>(Op); }\n  const Expr *getSubExpr() const { return cast<Expr>(Op); }\n  void setSubExpr(Expr *E) { Op = E; }\n\n  /// Retrieve the cast subexpression as it was written in the source\n  /// code, looking through any implicit casts or other intermediate nodes\n  /// introduced by semantic analysis.\n  Expr *getSubExprAsWritten();\n  const Expr *getSubExprAsWritten() const {\n    return const_cast<CastExpr *>(this)->getSubExprAsWritten();\n  }\n\n  /// If this cast applies a user-defined conversion, retrieve the conversion\n  /// function that it invokes.\n  NamedDecl *getConversionFunction() const;\n\n  typedef CXXBaseSpecifier **path_iterator;\n  typedef const CXXBaseSpecifier *const *path_const_iterator;\n  bool path_empty() const { return path_size() == 0; }\n  unsigned path_size() const { return CastExprBits.BasePathSize; }\n  path_iterator path_begin() { return path_buffer(); }\n  path_iterator path_end() { return path_buffer() + path_size(); }\n  path_const_iterator path_begin() const { return path_buffer(); }\n  path_const_iterator path_end() const { return path_buffer() + path_size(); }\n\n  llvm::iterator_range<path_iterator> path() {\n    return llvm::make_range(path_begin(), path_end());\n  }\n  llvm::iterator_range<path_const_iterator> path() const {\n    return llvm::make_range(path_begin(), path_end());\n  }\n\n  const FieldDecl *getTargetUnionField() const {\n    assert(getCastKind() == CK_ToUnion);\n    return getTargetFieldForToUnionCast(getType(), getSubExpr()->getType());\n  }\n\n  bool hasStoredFPFeatures() const { return CastExprBits.HasFPFeatures; }\n\n  /// Get FPOptionsOverride from trailing storage.\n  FPOptionsOverride getStoredFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return *getTrailingFPFeatures();\n  }\n\n  // Get the FP features status of this operation. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n\n  FPOptionsOverride getFPFeatures() const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  static const FieldDecl *getTargetFieldForToUnionCast(QualType unionType,\n                                                       QualType opType);\n  static const FieldDecl *getTargetFieldForToUnionCast(const RecordDecl *RD,\n                                                       QualType opType);\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstCastExprConstant &&\n           T->getStmtClass() <= lastCastExprConstant;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Op, &Op+1); }\n  const_child_range children() const { return const_child_range(&Op, &Op + 1); }\n};\n\n/// ImplicitCastExpr - Allows us to explicitly represent implicit type\n/// conversions, which have no direct representation in the original\n/// source code. For example: converting T[]->T*, void f()->void\n/// (*f)(), float->double, short->int, etc.\n///\n/// In C, implicit casts always produce rvalues. However, in C++, an\n/// implicit cast whose result is being bound to a reference will be\n/// an lvalue or xvalue. For example:\n///\n/// @code\n/// class Base { };\n/// class Derived : public Base { };\n/// Derived &&ref();\n/// void f(Derived d) {\n///   Base& b = d; // initializer is an ImplicitCastExpr\n///                // to an lvalue of type Base\n///   Base&& r = ref(); // initializer is an ImplicitCastExpr\n///                     // to an xvalue of type Base\n/// }\n/// @endcode\nclass ImplicitCastExpr final\n    : public CastExpr,\n      private llvm::TrailingObjects<ImplicitCastExpr, CXXBaseSpecifier *,\n                                    FPOptionsOverride> {\n\n  ImplicitCastExpr(QualType ty, CastKind kind, Expr *op,\n                   unsigned BasePathLength, FPOptionsOverride FPO,\n                   ExprValueKind VK)\n      : CastExpr(ImplicitCastExprClass, ty, VK, kind, op, BasePathLength,\n                 FPO.requiresTrailingStorage()) {\n    if (hasStoredFPFeatures())\n      *getTrailingFPFeatures() = FPO;\n  }\n\n  /// Construct an empty implicit cast.\n  explicit ImplicitCastExpr(EmptyShell Shell, unsigned PathSize,\n                            bool HasFPFeatures)\n      : CastExpr(ImplicitCastExprClass, Shell, PathSize, HasFPFeatures) {}\n\n  unsigned numTrailingObjects(OverloadToken<CXXBaseSpecifier *>) const {\n    return path_size();\n  }\n\npublic:\n  enum OnStack_t { OnStack };\n  ImplicitCastExpr(OnStack_t _, QualType ty, CastKind kind, Expr *op,\n                   ExprValueKind VK, FPOptionsOverride FPO)\n      : CastExpr(ImplicitCastExprClass, ty, VK, kind, op, 0,\n                 FPO.requiresTrailingStorage()) {\n    if (hasStoredFPFeatures())\n      *getTrailingFPFeatures() = FPO;\n  }\n\n  bool isPartOfExplicitCast() const { return CastExprBits.PartOfExplicitCast; }\n  void setIsPartOfExplicitCast(bool PartOfExplicitCast) {\n    CastExprBits.PartOfExplicitCast = PartOfExplicitCast;\n  }\n\n  static ImplicitCastExpr *Create(const ASTContext &Context, QualType T,\n                                  CastKind Kind, Expr *Operand,\n                                  const CXXCastPath *BasePath,\n                                  ExprValueKind Cat, FPOptionsOverride FPO);\n\n  static ImplicitCastExpr *CreateEmpty(const ASTContext &Context,\n                                       unsigned PathSize, bool HasFPFeatures);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getSubExpr()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getSubExpr()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ImplicitCastExprClass;\n  }\n\n  friend TrailingObjects;\n  friend class CastExpr;\n};\n\n/// ExplicitCastExpr - An explicit cast written in the source\n/// code.\n///\n/// This class is effectively an abstract class, because it provides\n/// the basic representation of an explicitly-written cast without\n/// specifying which kind of cast (C cast, functional cast, static\n/// cast, etc.) was written; specific derived classes represent the\n/// particular style of cast and its location information.\n///\n/// Unlike implicit casts, explicit cast nodes have two different\n/// types: the type that was written into the source code, and the\n/// actual type of the expression as determined by semantic\n/// analysis. These types may differ slightly. For example, in C++ one\n/// can cast to a reference type, which indicates that the resulting\n/// expression will be an lvalue or xvalue. The reference type, however,\n/// will not be used as the type of the expression.\nclass ExplicitCastExpr : public CastExpr {\n  /// TInfo - Source type info for the (written) type\n  /// this expression is casting to.\n  TypeSourceInfo *TInfo;\n\nprotected:\n  ExplicitCastExpr(StmtClass SC, QualType exprTy, ExprValueKind VK,\n                   CastKind kind, Expr *op, unsigned PathSize,\n                   bool HasFPFeatures, TypeSourceInfo *writtenTy)\n      : CastExpr(SC, exprTy, VK, kind, op, PathSize, HasFPFeatures),\n        TInfo(writtenTy) {}\n\n  /// Construct an empty explicit cast.\n  ExplicitCastExpr(StmtClass SC, EmptyShell Shell, unsigned PathSize,\n                   bool HasFPFeatures)\n      : CastExpr(SC, Shell, PathSize, HasFPFeatures) {}\n\npublic:\n  /// getTypeInfoAsWritten - Returns the type source info for the type\n  /// that this expression is casting to.\n  TypeSourceInfo *getTypeInfoAsWritten() const { return TInfo; }\n  void setTypeInfoAsWritten(TypeSourceInfo *writtenTy) { TInfo = writtenTy; }\n\n  /// getTypeAsWritten - Returns the type that this expression is\n  /// casting to, as written in the source code.\n  QualType getTypeAsWritten() const { return TInfo->getType(); }\n\n  static bool classof(const Stmt *T) {\n     return T->getStmtClass() >= firstExplicitCastExprConstant &&\n            T->getStmtClass() <= lastExplicitCastExprConstant;\n  }\n};\n\n/// CStyleCastExpr - An explicit cast in C (C99 6.5.4) or a C-style\n/// cast in C++ (C++ [expr.cast]), which uses the syntax\n/// (Type)expr. For example: @c (int)f.\nclass CStyleCastExpr final\n    : public ExplicitCastExpr,\n      private llvm::TrailingObjects<CStyleCastExpr, CXXBaseSpecifier *,\n                                    FPOptionsOverride> {\n  SourceLocation LPLoc; // the location of the left paren\n  SourceLocation RPLoc; // the location of the right paren\n\n  CStyleCastExpr(QualType exprTy, ExprValueKind vk, CastKind kind, Expr *op,\n                 unsigned PathSize, FPOptionsOverride FPO,\n                 TypeSourceInfo *writtenTy, SourceLocation l, SourceLocation r)\n      : ExplicitCastExpr(CStyleCastExprClass, exprTy, vk, kind, op, PathSize,\n                         FPO.requiresTrailingStorage(), writtenTy),\n        LPLoc(l), RPLoc(r) {\n    if (hasStoredFPFeatures())\n      *getTrailingFPFeatures() = FPO;\n  }\n\n  /// Construct an empty C-style explicit cast.\n  explicit CStyleCastExpr(EmptyShell Shell, unsigned PathSize,\n                          bool HasFPFeatures)\n      : ExplicitCastExpr(CStyleCastExprClass, Shell, PathSize, HasFPFeatures) {}\n\n  unsigned numTrailingObjects(OverloadToken<CXXBaseSpecifier *>) const {\n    return path_size();\n  }\n\npublic:\n  static CStyleCastExpr *\n  Create(const ASTContext &Context, QualType T, ExprValueKind VK, CastKind K,\n         Expr *Op, const CXXCastPath *BasePath, FPOptionsOverride FPO,\n         TypeSourceInfo *WrittenTy, SourceLocation L, SourceLocation R);\n\n  static CStyleCastExpr *CreateEmpty(const ASTContext &Context,\n                                     unsigned PathSize, bool HasFPFeatures);\n\n  SourceLocation getLParenLoc() const { return LPLoc; }\n  void setLParenLoc(SourceLocation L) { LPLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RPLoc; }\n  void setRParenLoc(SourceLocation L) { RPLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return LPLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getSubExpr()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == CStyleCastExprClass;\n  }\n\n  friend TrailingObjects;\n  friend class CastExpr;\n};\n\n/// A builtin binary operation expression such as \"x + y\" or \"x <= y\".\n///\n/// This expression node kind describes a builtin binary operation,\n/// such as \"x + y\" for integer values \"x\" and \"y\". The operands will\n/// already have been converted to appropriate types (e.g., by\n/// performing promotions or conversions).\n///\n/// In C++, where operators may be overloaded, a different kind of\n/// expression node (CXXOperatorCallExpr) is used to express the\n/// invocation of an overloaded operator with operator syntax. Within\n/// a C++ template, whether BinaryOperator or CXXOperatorCallExpr is\n/// used to store an expression \"x + y\" depends on the subexpressions\n/// for x and y. If neither x or y is type-dependent, and the \"+\"\n/// operator resolves to a built-in operation, BinaryOperator will be\n/// used to express the computation (x and y may still be\n/// value-dependent). If either x or y is type-dependent, or if the\n/// \"+\" resolves to an overloaded operator, CXXOperatorCallExpr will\n/// be used to express the computation.\nclass BinaryOperator : public Expr {\n  enum { LHS, RHS, END_EXPR };\n  Stmt *SubExprs[END_EXPR];\n\npublic:\n  typedef BinaryOperatorKind Opcode;\n\nprotected:\n  size_t offsetOfTrailingStorage() const;\n\n  /// Return a pointer to the trailing FPOptions\n  FPOptionsOverride *getTrailingFPFeatures() {\n    assert(BinaryOperatorBits.HasFPFeatures);\n    return reinterpret_cast<FPOptionsOverride *>(\n        reinterpret_cast<char *>(this) + offsetOfTrailingStorage());\n  }\n  const FPOptionsOverride *getTrailingFPFeatures() const {\n    assert(BinaryOperatorBits.HasFPFeatures);\n    return reinterpret_cast<const FPOptionsOverride *>(\n        reinterpret_cast<const char *>(this) + offsetOfTrailingStorage());\n  }\n\n  /// Build a binary operator, assuming that appropriate storage has been\n  /// allocated for the trailing objects when needed.\n  BinaryOperator(const ASTContext &Ctx, Expr *lhs, Expr *rhs, Opcode opc,\n                 QualType ResTy, ExprValueKind VK, ExprObjectKind OK,\n                 SourceLocation opLoc, FPOptionsOverride FPFeatures);\n\n  /// Construct an empty binary operator.\n  explicit BinaryOperator(EmptyShell Empty) : Expr(BinaryOperatorClass, Empty) {\n    BinaryOperatorBits.Opc = BO_Comma;\n  }\n\npublic:\n  static BinaryOperator *CreateEmpty(const ASTContext &C, bool hasFPFeatures);\n\n  static BinaryOperator *Create(const ASTContext &C, Expr *lhs, Expr *rhs,\n                                Opcode opc, QualType ResTy, ExprValueKind VK,\n                                ExprObjectKind OK, SourceLocation opLoc,\n                                FPOptionsOverride FPFeatures);\n  SourceLocation getExprLoc() const { return getOperatorLoc(); }\n  SourceLocation getOperatorLoc() const { return BinaryOperatorBits.OpLoc; }\n  void setOperatorLoc(SourceLocation L) { BinaryOperatorBits.OpLoc = L; }\n\n  Opcode getOpcode() const {\n    return static_cast<Opcode>(BinaryOperatorBits.Opc);\n  }\n  void setOpcode(Opcode Opc) { BinaryOperatorBits.Opc = Opc; }\n\n  Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  void setLHS(Expr *E) { SubExprs[LHS] = E; }\n  Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n  void setRHS(Expr *E) { SubExprs[RHS] = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getLHS()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getRHS()->getEndLoc();\n  }\n\n  /// getOpcodeStr - Turn an Opcode enum value into the punctuation char it\n  /// corresponds to, e.g. \"<<=\".\n  static StringRef getOpcodeStr(Opcode Op);\n\n  StringRef getOpcodeStr() const { return getOpcodeStr(getOpcode()); }\n\n  /// Retrieve the binary opcode that corresponds to the given\n  /// overloaded operator.\n  static Opcode getOverloadedOpcode(OverloadedOperatorKind OO);\n\n  /// Retrieve the overloaded operator kind that corresponds to\n  /// the given binary opcode.\n  static OverloadedOperatorKind getOverloadedOperator(Opcode Opc);\n\n  /// predicates to categorize the respective opcodes.\n  static bool isPtrMemOp(Opcode Opc) {\n    return Opc == BO_PtrMemD || Opc == BO_PtrMemI;\n  }\n  bool isPtrMemOp() const { return isPtrMemOp(getOpcode()); }\n\n  static bool isMultiplicativeOp(Opcode Opc) {\n    return Opc >= BO_Mul && Opc <= BO_Rem;\n  }\n  bool isMultiplicativeOp() const { return isMultiplicativeOp(getOpcode()); }\n  static bool isAdditiveOp(Opcode Opc) { return Opc == BO_Add || Opc==BO_Sub; }\n  bool isAdditiveOp() const { return isAdditiveOp(getOpcode()); }\n  static bool isShiftOp(Opcode Opc) { return Opc == BO_Shl || Opc == BO_Shr; }\n  bool isShiftOp() const { return isShiftOp(getOpcode()); }\n\n  static bool isBitwiseOp(Opcode Opc) { return Opc >= BO_And && Opc <= BO_Or; }\n  bool isBitwiseOp() const { return isBitwiseOp(getOpcode()); }\n\n  static bool isRelationalOp(Opcode Opc) { return Opc >= BO_LT && Opc<=BO_GE; }\n  bool isRelationalOp() const { return isRelationalOp(getOpcode()); }\n\n  static bool isEqualityOp(Opcode Opc) { return Opc == BO_EQ || Opc == BO_NE; }\n  bool isEqualityOp() const { return isEqualityOp(getOpcode()); }\n\n  static bool isComparisonOp(Opcode Opc) { return Opc >= BO_Cmp && Opc<=BO_NE; }\n  bool isComparisonOp() const { return isComparisonOp(getOpcode()); }\n\n  static bool isCommaOp(Opcode Opc) { return Opc == BO_Comma; }\n  bool isCommaOp() const { return isCommaOp(getOpcode()); }\n\n  static Opcode negateComparisonOp(Opcode Opc) {\n    switch (Opc) {\n    default:\n      llvm_unreachable(\"Not a comparison operator.\");\n    case BO_LT: return BO_GE;\n    case BO_GT: return BO_LE;\n    case BO_LE: return BO_GT;\n    case BO_GE: return BO_LT;\n    case BO_EQ: return BO_NE;\n    case BO_NE: return BO_EQ;\n    }\n  }\n\n  static Opcode reverseComparisonOp(Opcode Opc) {\n    switch (Opc) {\n    default:\n      llvm_unreachable(\"Not a comparison operator.\");\n    case BO_LT: return BO_GT;\n    case BO_GT: return BO_LT;\n    case BO_LE: return BO_GE;\n    case BO_GE: return BO_LE;\n    case BO_EQ:\n    case BO_NE:\n      return Opc;\n    }\n  }\n\n  static bool isLogicalOp(Opcode Opc) { return Opc == BO_LAnd || Opc==BO_LOr; }\n  bool isLogicalOp() const { return isLogicalOp(getOpcode()); }\n\n  static bool isAssignmentOp(Opcode Opc) {\n    return Opc >= BO_Assign && Opc <= BO_OrAssign;\n  }\n  bool isAssignmentOp() const { return isAssignmentOp(getOpcode()); }\n\n  static bool isCompoundAssignmentOp(Opcode Opc) {\n    return Opc > BO_Assign && Opc <= BO_OrAssign;\n  }\n  bool isCompoundAssignmentOp() const {\n    return isCompoundAssignmentOp(getOpcode());\n  }\n  static Opcode getOpForCompoundAssignment(Opcode Opc) {\n    assert(isCompoundAssignmentOp(Opc));\n    if (Opc >= BO_AndAssign)\n      return Opcode(unsigned(Opc) - BO_AndAssign + BO_And);\n    else\n      return Opcode(unsigned(Opc) - BO_MulAssign + BO_Mul);\n  }\n\n  static bool isShiftAssignOp(Opcode Opc) {\n    return Opc == BO_ShlAssign || Opc == BO_ShrAssign;\n  }\n  bool isShiftAssignOp() const {\n    return isShiftAssignOp(getOpcode());\n  }\n\n  // Return true if a binary operator using the specified opcode and operands\n  // would match the 'p = (i8*)nullptr + n' idiom for casting a pointer-sized\n  // integer to a pointer.\n  static bool isNullPointerArithmeticExtension(ASTContext &Ctx, Opcode Opc,\n                                               Expr *LHS, Expr *RHS);\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() >= firstBinaryOperatorConstant &&\n           S->getStmtClass() <= lastBinaryOperatorConstant;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n\n  /// Set and fetch the bit that shows whether FPFeatures needs to be\n  /// allocated in Trailing Storage\n  void setHasStoredFPFeatures(bool B) { BinaryOperatorBits.HasFPFeatures = B; }\n  bool hasStoredFPFeatures() const { return BinaryOperatorBits.HasFPFeatures; }\n\n  /// Get FPFeatures from trailing storage\n  FPOptionsOverride getStoredFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return *getTrailingFPFeatures();\n  }\n  /// Set FPFeatures in trailing storage, used only by Serialization\n  void setStoredFPFeatures(FPOptionsOverride F) {\n    assert(BinaryOperatorBits.HasFPFeatures);\n    *getTrailingFPFeatures() = F;\n  }\n\n  // Get the FP features status of this operator. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (BinaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n\n  // This is used in ASTImporter\n  FPOptionsOverride getFPFeatures(const LangOptions &LO) const {\n    if (BinaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  // Get the FP contractability status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFPContractableWithinStatement(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).allowFPContractWithinStatement();\n  }\n\n  // Get the FENV_ACCESS status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFEnvAccessOn(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).getAllowFEnvAccess();\n  }\n\nprotected:\n  BinaryOperator(const ASTContext &Ctx, Expr *lhs, Expr *rhs, Opcode opc,\n                 QualType ResTy, ExprValueKind VK, ExprObjectKind OK,\n                 SourceLocation opLoc, FPOptionsOverride FPFeatures,\n                 bool dead2);\n\n  /// Construct an empty BinaryOperator, SC is CompoundAssignOperator.\n  BinaryOperator(StmtClass SC, EmptyShell Empty) : Expr(SC, Empty) {\n    BinaryOperatorBits.Opc = BO_MulAssign;\n  }\n\n  /// Return the size in bytes needed for the trailing objects.\n  /// Used to allocate the right amount of storage.\n  static unsigned sizeOfTrailingObjects(bool HasFPFeatures) {\n    return HasFPFeatures * sizeof(FPOptionsOverride);\n  }\n};\n\n/// CompoundAssignOperator - For compound assignments (e.g. +=), we keep\n/// track of the type the operation is performed in.  Due to the semantics of\n/// these operators, the operands are promoted, the arithmetic performed, an\n/// implicit conversion back to the result type done, then the assignment takes\n/// place.  This captures the intermediate type which the computation is done\n/// in.\nclass CompoundAssignOperator : public BinaryOperator {\n  QualType ComputationLHSType;\n  QualType ComputationResultType;\n\n  /// Construct an empty CompoundAssignOperator.\n  explicit CompoundAssignOperator(const ASTContext &C, EmptyShell Empty,\n                                  bool hasFPFeatures)\n      : BinaryOperator(CompoundAssignOperatorClass, Empty) {}\n\nprotected:\n  CompoundAssignOperator(const ASTContext &C, Expr *lhs, Expr *rhs, Opcode opc,\n                         QualType ResType, ExprValueKind VK, ExprObjectKind OK,\n                         SourceLocation OpLoc, FPOptionsOverride FPFeatures,\n                         QualType CompLHSType, QualType CompResultType)\n      : BinaryOperator(C, lhs, rhs, opc, ResType, VK, OK, OpLoc, FPFeatures,\n                       true),\n        ComputationLHSType(CompLHSType), ComputationResultType(CompResultType) {\n    assert(isCompoundAssignmentOp() &&\n           \"Only should be used for compound assignments\");\n  }\n\npublic:\n  static CompoundAssignOperator *CreateEmpty(const ASTContext &C,\n                                             bool hasFPFeatures);\n\n  static CompoundAssignOperator *\n  Create(const ASTContext &C, Expr *lhs, Expr *rhs, Opcode opc, QualType ResTy,\n         ExprValueKind VK, ExprObjectKind OK, SourceLocation opLoc,\n         FPOptionsOverride FPFeatures, QualType CompLHSType = QualType(),\n         QualType CompResultType = QualType());\n\n  // The two computation types are the type the LHS is converted\n  // to for the computation and the type of the result; the two are\n  // distinct in a few cases (specifically, int+=ptr and ptr-=ptr).\n  QualType getComputationLHSType() const { return ComputationLHSType; }\n  void setComputationLHSType(QualType T) { ComputationLHSType = T; }\n\n  QualType getComputationResultType() const { return ComputationResultType; }\n  void setComputationResultType(QualType T) { ComputationResultType = T; }\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() == CompoundAssignOperatorClass;\n  }\n};\n\ninline size_t BinaryOperator::offsetOfTrailingStorage() const {\n  assert(BinaryOperatorBits.HasFPFeatures);\n  return isa<CompoundAssignOperator>(this) ? sizeof(CompoundAssignOperator)\n                                           : sizeof(BinaryOperator);\n}\n\n/// AbstractConditionalOperator - An abstract base class for\n/// ConditionalOperator and BinaryConditionalOperator.\nclass AbstractConditionalOperator : public Expr {\n  SourceLocation QuestionLoc, ColonLoc;\n  friend class ASTStmtReader;\n\nprotected:\n  AbstractConditionalOperator(StmtClass SC, QualType T, ExprValueKind VK,\n                              ExprObjectKind OK, SourceLocation qloc,\n                              SourceLocation cloc)\n      : Expr(SC, T, VK, OK), QuestionLoc(qloc), ColonLoc(cloc) {}\n\n  AbstractConditionalOperator(StmtClass SC, EmptyShell Empty)\n    : Expr(SC, Empty) { }\n\npublic:\n  // getCond - Return the expression representing the condition for\n  //   the ?: operator.\n  Expr *getCond() const;\n\n  // getTrueExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to true.\n  Expr *getTrueExpr() const;\n\n  // getFalseExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to false.  This is\n  //   the same as getRHS.\n  Expr *getFalseExpr() const;\n\n  SourceLocation getQuestionLoc() const { return QuestionLoc; }\n  SourceLocation getColonLoc() const { return ColonLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConditionalOperatorClass ||\n           T->getStmtClass() == BinaryConditionalOperatorClass;\n  }\n};\n\n/// ConditionalOperator - The ?: ternary operator.  The GNU \"missing\n/// middle\" extension is a BinaryConditionalOperator.\nclass ConditionalOperator : public AbstractConditionalOperator {\n  enum { COND, LHS, RHS, END_EXPR };\n  Stmt* SubExprs[END_EXPR]; // Left/Middle/Right hand sides.\n\n  friend class ASTStmtReader;\npublic:\n  ConditionalOperator(Expr *cond, SourceLocation QLoc, Expr *lhs,\n                      SourceLocation CLoc, Expr *rhs, QualType t,\n                      ExprValueKind VK, ExprObjectKind OK)\n      : AbstractConditionalOperator(ConditionalOperatorClass, t, VK, OK, QLoc,\n                                    CLoc) {\n    SubExprs[COND] = cond;\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty conditional operator.\n  explicit ConditionalOperator(EmptyShell Empty)\n    : AbstractConditionalOperator(ConditionalOperatorClass, Empty) { }\n\n  // getCond - Return the expression representing the condition for\n  //   the ?: operator.\n  Expr *getCond() const { return cast<Expr>(SubExprs[COND]); }\n\n  // getTrueExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to true.\n  Expr *getTrueExpr() const { return cast<Expr>(SubExprs[LHS]); }\n\n  // getFalseExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to false.  This is\n  //   the same as getRHS.\n  Expr *getFalseExpr() const { return cast<Expr>(SubExprs[RHS]); }\n\n  Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCond()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getRHS()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConditionalOperatorClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// BinaryConditionalOperator - The GNU extension to the conditional\n/// operator which allows the middle operand to be omitted.\n///\n/// This is a different expression kind on the assumption that almost\n/// every client ends up needing to know that these are different.\nclass BinaryConditionalOperator : public AbstractConditionalOperator {\n  enum { COMMON, COND, LHS, RHS, NUM_SUBEXPRS };\n\n  /// - the common condition/left-hand-side expression, which will be\n  ///   evaluated as the opaque value\n  /// - the condition, expressed in terms of the opaque value\n  /// - the left-hand-side, expressed in terms of the opaque value\n  /// - the right-hand-side\n  Stmt *SubExprs[NUM_SUBEXPRS];\n  OpaqueValueExpr *OpaqueValue;\n\n  friend class ASTStmtReader;\npublic:\n  BinaryConditionalOperator(Expr *common, OpaqueValueExpr *opaqueValue,\n                            Expr *cond, Expr *lhs, Expr *rhs,\n                            SourceLocation qloc, SourceLocation cloc,\n                            QualType t, ExprValueKind VK, ExprObjectKind OK)\n      : AbstractConditionalOperator(BinaryConditionalOperatorClass, t, VK, OK,\n                                    qloc, cloc),\n        OpaqueValue(opaqueValue) {\n    SubExprs[COMMON] = common;\n    SubExprs[COND] = cond;\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n    assert(OpaqueValue->getSourceExpr() == common && \"Wrong opaque value\");\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty conditional operator.\n  explicit BinaryConditionalOperator(EmptyShell Empty)\n    : AbstractConditionalOperator(BinaryConditionalOperatorClass, Empty) { }\n\n  /// getCommon - Return the common expression, written to the\n  ///   left of the condition.  The opaque value will be bound to the\n  ///   result of this expression.\n  Expr *getCommon() const { return cast<Expr>(SubExprs[COMMON]); }\n\n  /// getOpaqueValue - Return the opaque value placeholder.\n  OpaqueValueExpr *getOpaqueValue() const { return OpaqueValue; }\n\n  /// getCond - Return the condition expression; this is defined\n  ///   in terms of the opaque value.\n  Expr *getCond() const { return cast<Expr>(SubExprs[COND]); }\n\n  /// getTrueExpr - Return the subexpression which will be\n  ///   evaluated if the condition evaluates to true;  this is defined\n  ///   in terms of the opaque value.\n  Expr *getTrueExpr() const {\n    return cast<Expr>(SubExprs[LHS]);\n  }\n\n  /// getFalseExpr - Return the subexpression which will be\n  ///   evaluated if the condnition evaluates to false; this is\n  ///   defined in terms of the opaque value.\n  Expr *getFalseExpr() const {\n    return cast<Expr>(SubExprs[RHS]);\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCommon()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getFalseExpr()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == BinaryConditionalOperatorClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(SubExprs, SubExprs + NUM_SUBEXPRS);\n  }\n  const_child_range children() const {\n    return const_child_range(SubExprs, SubExprs + NUM_SUBEXPRS);\n  }\n};\n\ninline Expr *AbstractConditionalOperator::getCond() const {\n  if (const ConditionalOperator *co = dyn_cast<ConditionalOperator>(this))\n    return co->getCond();\n  return cast<BinaryConditionalOperator>(this)->getCond();\n}\n\ninline Expr *AbstractConditionalOperator::getTrueExpr() const {\n  if (const ConditionalOperator *co = dyn_cast<ConditionalOperator>(this))\n    return co->getTrueExpr();\n  return cast<BinaryConditionalOperator>(this)->getTrueExpr();\n}\n\ninline Expr *AbstractConditionalOperator::getFalseExpr() const {\n  if (const ConditionalOperator *co = dyn_cast<ConditionalOperator>(this))\n    return co->getFalseExpr();\n  return cast<BinaryConditionalOperator>(this)->getFalseExpr();\n}\n\n/// AddrLabelExpr - The GNU address of label extension, representing &&label.\nclass AddrLabelExpr : public Expr {\n  SourceLocation AmpAmpLoc, LabelLoc;\n  LabelDecl *Label;\npublic:\n  AddrLabelExpr(SourceLocation AALoc, SourceLocation LLoc, LabelDecl *L,\n                QualType t)\n      : Expr(AddrLabelExprClass, t, VK_RValue, OK_Ordinary), AmpAmpLoc(AALoc),\n        LabelLoc(LLoc), Label(L) {\n    setDependence(ExprDependence::None);\n  }\n\n  /// Build an empty address of a label expression.\n  explicit AddrLabelExpr(EmptyShell Empty)\n    : Expr(AddrLabelExprClass, Empty) { }\n\n  SourceLocation getAmpAmpLoc() const { return AmpAmpLoc; }\n  void setAmpAmpLoc(SourceLocation L) { AmpAmpLoc = L; }\n  SourceLocation getLabelLoc() const { return LabelLoc; }\n  void setLabelLoc(SourceLocation L) { LabelLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return AmpAmpLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return LabelLoc; }\n\n  LabelDecl *getLabel() const { return Label; }\n  void setLabel(LabelDecl *L) { Label = L; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == AddrLabelExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// StmtExpr - This is the GNU Statement Expression extension: ({int X=4; X;}).\n/// The StmtExpr contains a single CompoundStmt node, which it evaluates and\n/// takes the value of the last subexpression.\n///\n/// A StmtExpr is always an r-value; values \"returned\" out of a\n/// StmtExpr will be copied.\nclass StmtExpr : public Expr {\n  Stmt *SubStmt;\n  SourceLocation LParenLoc, RParenLoc;\npublic:\n  StmtExpr(CompoundStmt *SubStmt, QualType T, SourceLocation LParenLoc,\n           SourceLocation RParenLoc, unsigned TemplateDepth)\n      : Expr(StmtExprClass, T, VK_RValue, OK_Ordinary), SubStmt(SubStmt),\n        LParenLoc(LParenLoc), RParenLoc(RParenLoc) {\n    setDependence(computeDependence(this, TemplateDepth));\n    // FIXME: A templated statement expression should have an associated\n    // DeclContext so that nested declarations always have a dependent context.\n    StmtExprBits.TemplateDepth = TemplateDepth;\n  }\n\n  /// Build an empty statement expression.\n  explicit StmtExpr(EmptyShell Empty) : Expr(StmtExprClass, Empty) { }\n\n  CompoundStmt *getSubStmt() { return cast<CompoundStmt>(SubStmt); }\n  const CompoundStmt *getSubStmt() const { return cast<CompoundStmt>(SubStmt); }\n  void setSubStmt(CompoundStmt *S) { SubStmt = S; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return LParenLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  SourceLocation getLParenLoc() const { return LParenLoc; }\n  void setLParenLoc(SourceLocation L) { LParenLoc = L; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  unsigned getTemplateDepth() const { return StmtExprBits.TemplateDepth; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == StmtExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&SubStmt, &SubStmt+1); }\n  const_child_range children() const {\n    return const_child_range(&SubStmt, &SubStmt + 1);\n  }\n};\n\n/// ShuffleVectorExpr - clang-specific builtin-in function\n/// __builtin_shufflevector.\n/// This AST node represents a operator that does a constant\n/// shuffle, similar to LLVM's shufflevector instruction. It takes\n/// two vectors and a variable number of constant indices,\n/// and returns the appropriately shuffled vector.\nclass ShuffleVectorExpr : public Expr {\n  SourceLocation BuiltinLoc, RParenLoc;\n\n  // SubExprs - the list of values passed to the __builtin_shufflevector\n  // function. The first two are vectors, and the rest are constant\n  // indices.  The number of values in this list is always\n  // 2+the number of indices in the vector type.\n  Stmt **SubExprs;\n  unsigned NumExprs;\n\npublic:\n  ShuffleVectorExpr(const ASTContext &C, ArrayRef<Expr*> args, QualType Type,\n                    SourceLocation BLoc, SourceLocation RP);\n\n  /// Build an empty vector-shuffle expression.\n  explicit ShuffleVectorExpr(EmptyShell Empty)\n    : Expr(ShuffleVectorExprClass, Empty), SubExprs(nullptr) { }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  void setBuiltinLoc(SourceLocation L) { BuiltinLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ShuffleVectorExprClass;\n  }\n\n  /// getNumSubExprs - Return the size of the SubExprs array.  This includes the\n  /// constant expression, the actual arguments passed in, and the function\n  /// pointers.\n  unsigned getNumSubExprs() const { return NumExprs; }\n\n  /// Retrieve the array of expressions.\n  Expr **getSubExprs() { return reinterpret_cast<Expr **>(SubExprs); }\n\n  /// getExpr - Return the Expr at the specified index.\n  Expr *getExpr(unsigned Index) {\n    assert((Index < NumExprs) && \"Arg access out of range!\");\n    return cast<Expr>(SubExprs[Index]);\n  }\n  const Expr *getExpr(unsigned Index) const {\n    assert((Index < NumExprs) && \"Arg access out of range!\");\n    return cast<Expr>(SubExprs[Index]);\n  }\n\n  void setExprs(const ASTContext &C, ArrayRef<Expr *> Exprs);\n\n  llvm::APSInt getShuffleMaskIdx(const ASTContext &Ctx, unsigned N) const {\n    assert((N < NumExprs - 2) && \"Shuffle idx out of range!\");\n    return getExpr(N+2)->EvaluateKnownConstInt(Ctx);\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+NumExprs);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + NumExprs);\n  }\n};\n\n/// ConvertVectorExpr - Clang builtin function __builtin_convertvector\n/// This AST node provides support for converting a vector type to another\n/// vector type of the same arity.\nclass ConvertVectorExpr : public Expr {\nprivate:\n  Stmt *SrcExpr;\n  TypeSourceInfo *TInfo;\n  SourceLocation BuiltinLoc, RParenLoc;\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  explicit ConvertVectorExpr(EmptyShell Empty) : Expr(ConvertVectorExprClass, Empty) {}\n\npublic:\n  ConvertVectorExpr(Expr *SrcExpr, TypeSourceInfo *TI, QualType DstType,\n                    ExprValueKind VK, ExprObjectKind OK,\n                    SourceLocation BuiltinLoc, SourceLocation RParenLoc)\n      : Expr(ConvertVectorExprClass, DstType, VK, OK), SrcExpr(SrcExpr),\n        TInfo(TI), BuiltinLoc(BuiltinLoc), RParenLoc(RParenLoc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// getSrcExpr - Return the Expr to be converted.\n  Expr *getSrcExpr() const { return cast<Expr>(SrcExpr); }\n\n  /// getTypeSourceInfo - Return the destination type.\n  TypeSourceInfo *getTypeSourceInfo() const {\n    return TInfo;\n  }\n  void setTypeSourceInfo(TypeSourceInfo *ti) {\n    TInfo = ti;\n  }\n\n  /// getBuiltinLoc - Return the location of the __builtin_convertvector token.\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n\n  /// getRParenLoc - Return the location of final right parenthesis.\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConvertVectorExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&SrcExpr, &SrcExpr+1); }\n  const_child_range children() const {\n    return const_child_range(&SrcExpr, &SrcExpr + 1);\n  }\n};\n\n/// ChooseExpr - GNU builtin-in function __builtin_choose_expr.\n/// This AST node is similar to the conditional operator (?:) in C, with\n/// the following exceptions:\n/// - the test expression must be a integer constant expression.\n/// - the expression returned acts like the chosen subexpression in every\n///   visible way: the type is the same as that of the chosen subexpression,\n///   and all predicates (whether it's an l-value, whether it's an integer\n///   constant expression, etc.) return the same result as for the chosen\n///   sub-expression.\nclass ChooseExpr : public Expr {\n  enum { COND, LHS, RHS, END_EXPR };\n  Stmt* SubExprs[END_EXPR]; // Left/Middle/Right hand sides.\n  SourceLocation BuiltinLoc, RParenLoc;\n  bool CondIsTrue;\npublic:\n  ChooseExpr(SourceLocation BLoc, Expr *cond, Expr *lhs, Expr *rhs, QualType t,\n             ExprValueKind VK, ExprObjectKind OK, SourceLocation RP,\n             bool condIsTrue)\n      : Expr(ChooseExprClass, t, VK, OK), BuiltinLoc(BLoc), RParenLoc(RP),\n        CondIsTrue(condIsTrue) {\n    SubExprs[COND] = cond;\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty __builtin_choose_expr.\n  explicit ChooseExpr(EmptyShell Empty) : Expr(ChooseExprClass, Empty) { }\n\n  /// isConditionTrue - Return whether the condition is true (i.e. not\n  /// equal to zero).\n  bool isConditionTrue() const {\n    assert(!isConditionDependent() &&\n           \"Dependent condition isn't true or false\");\n    return CondIsTrue;\n  }\n  void setIsConditionTrue(bool isTrue) { CondIsTrue = isTrue; }\n\n  bool isConditionDependent() const {\n    return getCond()->isTypeDependent() || getCond()->isValueDependent();\n  }\n\n  /// getChosenSubExpr - Return the subexpression chosen according to the\n  /// condition.\n  Expr *getChosenSubExpr() const {\n    return isConditionTrue() ? getLHS() : getRHS();\n  }\n\n  Expr *getCond() const { return cast<Expr>(SubExprs[COND]); }\n  void setCond(Expr *E) { SubExprs[COND] = E; }\n  Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  void setLHS(Expr *E) { SubExprs[LHS] = E; }\n  Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n  void setRHS(Expr *E) { SubExprs[RHS] = E; }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  void setBuiltinLoc(SourceLocation L) { BuiltinLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ChooseExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// GNUNullExpr - Implements the GNU __null extension, which is a name\n/// for a null pointer constant that has integral type (e.g., int or\n/// long) and is the same size and alignment as a pointer. The __null\n/// extension is typically only used by system headers, which define\n/// NULL as __null in C++ rather than using 0 (which is an integer\n/// that may not match the size of a pointer).\nclass GNUNullExpr : public Expr {\n  /// TokenLoc - The location of the __null keyword.\n  SourceLocation TokenLoc;\n\npublic:\n  GNUNullExpr(QualType Ty, SourceLocation Loc)\n      : Expr(GNUNullExprClass, Ty, VK_RValue, OK_Ordinary), TokenLoc(Loc) {\n    setDependence(ExprDependence::None);\n  }\n\n  /// Build an empty GNU __null expression.\n  explicit GNUNullExpr(EmptyShell Empty) : Expr(GNUNullExprClass, Empty) { }\n\n  /// getTokenLocation - The location of the __null token.\n  SourceLocation getTokenLocation() const { return TokenLoc; }\n  void setTokenLocation(SourceLocation L) { TokenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return TokenLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return TokenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == GNUNullExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// Represents a call to the builtin function \\c __builtin_va_arg.\nclass VAArgExpr : public Expr {\n  Stmt *Val;\n  llvm::PointerIntPair<TypeSourceInfo *, 1, bool> TInfo;\n  SourceLocation BuiltinLoc, RParenLoc;\npublic:\n  VAArgExpr(SourceLocation BLoc, Expr *e, TypeSourceInfo *TInfo,\n            SourceLocation RPLoc, QualType t, bool IsMS)\n      : Expr(VAArgExprClass, t, VK_RValue, OK_Ordinary), Val(e),\n        TInfo(TInfo, IsMS), BuiltinLoc(BLoc), RParenLoc(RPLoc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Create an empty __builtin_va_arg expression.\n  explicit VAArgExpr(EmptyShell Empty)\n      : Expr(VAArgExprClass, Empty), Val(nullptr), TInfo(nullptr, false) {}\n\n  const Expr *getSubExpr() const { return cast<Expr>(Val); }\n  Expr *getSubExpr() { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  /// Returns whether this is really a Win64 ABI va_arg expression.\n  bool isMicrosoftABI() const { return TInfo.getInt(); }\n  void setIsMicrosoftABI(bool IsMS) { TInfo.setInt(IsMS); }\n\n  TypeSourceInfo *getWrittenTypeInfo() const { return TInfo.getPointer(); }\n  void setWrittenTypeInfo(TypeSourceInfo *TI) { TInfo.setPointer(TI); }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  void setBuiltinLoc(SourceLocation L) { BuiltinLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == VAArgExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n};\n\n/// Represents a function call to one of __builtin_LINE(), __builtin_COLUMN(),\n/// __builtin_FUNCTION(), or __builtin_FILE().\nclass SourceLocExpr final : public Expr {\n  SourceLocation BuiltinLoc, RParenLoc;\n  DeclContext *ParentContext;\n\npublic:\n  enum IdentKind { Function, File, Line, Column };\n\n  SourceLocExpr(const ASTContext &Ctx, IdentKind Type, SourceLocation BLoc,\n                SourceLocation RParenLoc, DeclContext *Context);\n\n  /// Build an empty call expression.\n  explicit SourceLocExpr(EmptyShell Empty) : Expr(SourceLocExprClass, Empty) {}\n\n  /// Return the result of evaluating this SourceLocExpr in the specified\n  /// (and possibly null) default argument or initialization context.\n  APValue EvaluateInContext(const ASTContext &Ctx,\n                            const Expr *DefaultExpr) const;\n\n  /// Return a string representing the name of the specific builtin function.\n  StringRef getBuiltinStr() const;\n\n  IdentKind getIdentKind() const {\n    return static_cast<IdentKind>(SourceLocExprBits.Kind);\n  }\n\n  bool isStringType() const {\n    switch (getIdentKind()) {\n    case File:\n    case Function:\n      return true;\n    case Line:\n    case Column:\n      return false;\n    }\n    llvm_unreachable(\"unknown source location expression kind\");\n  }\n  bool isIntType() const LLVM_READONLY { return !isStringType(); }\n\n  /// If the SourceLocExpr has been resolved return the subexpression\n  /// representing the resolved value. Otherwise return null.\n  const DeclContext *getParentContext() const { return ParentContext; }\n  DeclContext *getParentContext() { return ParentContext; }\n\n  SourceLocation getLocation() const { return BuiltinLoc; }\n  SourceLocation getBeginLoc() const { return BuiltinLoc; }\n  SourceLocation getEndLoc() const { return RParenLoc; }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n\n  const_child_range children() const {\n    return const_child_range(child_iterator(), child_iterator());\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == SourceLocExprClass;\n  }\n\nprivate:\n  friend class ASTStmtReader;\n};\n\n/// Describes an C or C++ initializer list.\n///\n/// InitListExpr describes an initializer list, which can be used to\n/// initialize objects of different types, including\n/// struct/class/union types, arrays, and vectors. For example:\n///\n/// @code\n/// struct foo x = { 1, { 2, 3 } };\n/// @endcode\n///\n/// Prior to semantic analysis, an initializer list will represent the\n/// initializer list as written by the user, but will have the\n/// placeholder type \"void\". This initializer list is called the\n/// syntactic form of the initializer, and may contain C99 designated\n/// initializers (represented as DesignatedInitExprs), initializations\n/// of subobject members without explicit braces, and so on. Clients\n/// interested in the original syntax of the initializer list should\n/// use the syntactic form of the initializer list.\n///\n/// After semantic analysis, the initializer list will represent the\n/// semantic form of the initializer, where the initializations of all\n/// subobjects are made explicit with nested InitListExpr nodes and\n/// C99 designators have been eliminated by placing the designated\n/// initializations into the subobject they initialize. Additionally,\n/// any \"holes\" in the initialization, where no initializer has been\n/// specified for a particular subobject, will be replaced with\n/// implicitly-generated ImplicitValueInitExpr expressions that\n/// value-initialize the subobjects. Note, however, that the\n/// initializer lists may still have fewer initializers than there are\n/// elements to initialize within the object.\n///\n/// After semantic analysis has completed, given an initializer list,\n/// method isSemanticForm() returns true if and only if this is the\n/// semantic form of the initializer list (note: the same AST node\n/// may at the same time be the syntactic form).\n/// Given the semantic form of the initializer list, one can retrieve\n/// the syntactic form of that initializer list (when different)\n/// using method getSyntacticForm(); the method returns null if applied\n/// to a initializer list which is already in syntactic form.\n/// Similarly, given the syntactic form (i.e., an initializer list such\n/// that isSemanticForm() returns false), one can retrieve the semantic\n/// form using method getSemanticForm().\n/// Since many initializer lists have the same syntactic and semantic forms,\n/// getSyntacticForm() may return NULL, indicating that the current\n/// semantic initializer list also serves as its syntactic form.\nclass InitListExpr : public Expr {\n  // FIXME: Eliminate this vector in favor of ASTContext allocation\n  typedef ASTVector<Stmt *> InitExprsTy;\n  InitExprsTy InitExprs;\n  SourceLocation LBraceLoc, RBraceLoc;\n\n  /// The alternative form of the initializer list (if it exists).\n  /// The int part of the pair stores whether this initializer list is\n  /// in semantic form. If not null, the pointer points to:\n  ///   - the syntactic form, if this is in semantic form;\n  ///   - the semantic form, if this is in syntactic form.\n  llvm::PointerIntPair<InitListExpr *, 1, bool> AltForm;\n\n  /// Either:\n  ///  If this initializer list initializes an array with more elements than\n  ///  there are initializers in the list, specifies an expression to be used\n  ///  for value initialization of the rest of the elements.\n  /// Or\n  ///  If this initializer list initializes a union, specifies which\n  ///  field within the union will be initialized.\n  llvm::PointerUnion<Expr *, FieldDecl *> ArrayFillerOrUnionFieldInit;\n\npublic:\n  InitListExpr(const ASTContext &C, SourceLocation lbraceloc,\n               ArrayRef<Expr*> initExprs, SourceLocation rbraceloc);\n\n  /// Build an empty initializer list.\n  explicit InitListExpr(EmptyShell Empty)\n    : Expr(InitListExprClass, Empty), AltForm(nullptr, true) { }\n\n  unsigned getNumInits() const { return InitExprs.size(); }\n\n  /// Retrieve the set of initializers.\n  Expr **getInits() { return reinterpret_cast<Expr **>(InitExprs.data()); }\n\n  /// Retrieve the set of initializers.\n  Expr * const *getInits() const {\n    return reinterpret_cast<Expr * const *>(InitExprs.data());\n  }\n\n  ArrayRef<Expr *> inits() {\n    return llvm::makeArrayRef(getInits(), getNumInits());\n  }\n\n  ArrayRef<Expr *> inits() const {\n    return llvm::makeArrayRef(getInits(), getNumInits());\n  }\n\n  const Expr *getInit(unsigned Init) const {\n    assert(Init < getNumInits() && \"Initializer access out of range!\");\n    return cast_or_null<Expr>(InitExprs[Init]);\n  }\n\n  Expr *getInit(unsigned Init) {\n    assert(Init < getNumInits() && \"Initializer access out of range!\");\n    return cast_or_null<Expr>(InitExprs[Init]);\n  }\n\n  void setInit(unsigned Init, Expr *expr) {\n    assert(Init < getNumInits() && \"Initializer access out of range!\");\n    InitExprs[Init] = expr;\n\n    if (expr)\n      setDependence(getDependence() | expr->getDependence());\n  }\n\n  /// Mark the semantic form of the InitListExpr as error when the semantic\n  /// analysis fails.\n  void markError() {\n    assert(isSemanticForm());\n    setDependence(getDependence() | ExprDependence::ErrorDependent);\n  }\n\n  /// Reserve space for some number of initializers.\n  void reserveInits(const ASTContext &C, unsigned NumInits);\n\n  /// Specify the number of initializers\n  ///\n  /// If there are more than @p NumInits initializers, the remaining\n  /// initializers will be destroyed. If there are fewer than @p\n  /// NumInits initializers, NULL expressions will be added for the\n  /// unknown initializers.\n  void resizeInits(const ASTContext &Context, unsigned NumInits);\n\n  /// Updates the initializer at index @p Init with the new\n  /// expression @p expr, and returns the old expression at that\n  /// location.\n  ///\n  /// When @p Init is out of range for this initializer list, the\n  /// initializer list will be extended with NULL expressions to\n  /// accommodate the new entry.\n  Expr *updateInit(const ASTContext &C, unsigned Init, Expr *expr);\n\n  /// If this initializer list initializes an array with more elements\n  /// than there are initializers in the list, specifies an expression to be\n  /// used for value initialization of the rest of the elements.\n  Expr *getArrayFiller() {\n    return ArrayFillerOrUnionFieldInit.dyn_cast<Expr *>();\n  }\n  const Expr *getArrayFiller() const {\n    return const_cast<InitListExpr *>(this)->getArrayFiller();\n  }\n  void setArrayFiller(Expr *filler);\n\n  /// Return true if this is an array initializer and its array \"filler\"\n  /// has been set.\n  bool hasArrayFiller() const { return getArrayFiller(); }\n\n  /// If this initializes a union, specifies which field in the\n  /// union to initialize.\n  ///\n  /// Typically, this field is the first named field within the\n  /// union. However, a designated initializer can specify the\n  /// initialization of a different field within the union.\n  FieldDecl *getInitializedFieldInUnion() {\n    return ArrayFillerOrUnionFieldInit.dyn_cast<FieldDecl *>();\n  }\n  const FieldDecl *getInitializedFieldInUnion() const {\n    return const_cast<InitListExpr *>(this)->getInitializedFieldInUnion();\n  }\n  void setInitializedFieldInUnion(FieldDecl *FD) {\n    assert((FD == nullptr\n            || getInitializedFieldInUnion() == nullptr\n            || getInitializedFieldInUnion() == FD)\n           && \"Only one field of a union may be initialized at a time!\");\n    ArrayFillerOrUnionFieldInit = FD;\n  }\n\n  // Explicit InitListExpr's originate from source code (and have valid source\n  // locations). Implicit InitListExpr's are created by the semantic analyzer.\n  // FIXME: This is wrong; InitListExprs created by semantic analysis have\n  // valid source locations too!\n  bool isExplicit() const {\n    return LBraceLoc.isValid() && RBraceLoc.isValid();\n  }\n\n  // Is this an initializer for an array of characters, initialized by a string\n  // literal or an @encode?\n  bool isStringLiteralInit() const;\n\n  /// Is this a transparent initializer list (that is, an InitListExpr that is\n  /// purely syntactic, and whose semantics are that of the sole contained\n  /// initializer)?\n  bool isTransparent() const;\n\n  /// Is this the zero initializer {0} in a language which considers it\n  /// idiomatic?\n  bool isIdiomaticZeroInitializer(const LangOptions &LangOpts) const;\n\n  SourceLocation getLBraceLoc() const { return LBraceLoc; }\n  void setLBraceLoc(SourceLocation Loc) { LBraceLoc = Loc; }\n  SourceLocation getRBraceLoc() const { return RBraceLoc; }\n  void setRBraceLoc(SourceLocation Loc) { RBraceLoc = Loc; }\n\n  bool isSemanticForm() const { return AltForm.getInt(); }\n  InitListExpr *getSemanticForm() const {\n    return isSemanticForm() ? nullptr : AltForm.getPointer();\n  }\n  bool isSyntacticForm() const {\n    return !AltForm.getInt() || !AltForm.getPointer();\n  }\n  InitListExpr *getSyntacticForm() const {\n    return isSemanticForm() ? AltForm.getPointer() : nullptr;\n  }\n\n  void setSyntacticForm(InitListExpr *Init) {\n    AltForm.setPointer(Init);\n    AltForm.setInt(true);\n    Init->AltForm.setPointer(this);\n    Init->AltForm.setInt(false);\n  }\n\n  bool hadArrayRangeDesignator() const {\n    return InitListExprBits.HadArrayRangeDesignator != 0;\n  }\n  void sawArrayRangeDesignator(bool ARD = true) {\n    InitListExprBits.HadArrayRangeDesignator = ARD;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == InitListExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    const_child_range CCR = const_cast<const InitListExpr *>(this)->children();\n    return child_range(cast_away_const(CCR.begin()),\n                       cast_away_const(CCR.end()));\n  }\n\n  const_child_range children() const {\n    // FIXME: This does not include the array filler expression.\n    if (InitExprs.empty())\n      return const_child_range(const_child_iterator(), const_child_iterator());\n    return const_child_range(&InitExprs[0], &InitExprs[0] + InitExprs.size());\n  }\n\n  typedef InitExprsTy::iterator iterator;\n  typedef InitExprsTy::const_iterator const_iterator;\n  typedef InitExprsTy::reverse_iterator reverse_iterator;\n  typedef InitExprsTy::const_reverse_iterator const_reverse_iterator;\n\n  iterator begin() { return InitExprs.begin(); }\n  const_iterator begin() const { return InitExprs.begin(); }\n  iterator end() { return InitExprs.end(); }\n  const_iterator end() const { return InitExprs.end(); }\n  reverse_iterator rbegin() { return InitExprs.rbegin(); }\n  const_reverse_iterator rbegin() const { return InitExprs.rbegin(); }\n  reverse_iterator rend() { return InitExprs.rend(); }\n  const_reverse_iterator rend() const { return InitExprs.rend(); }\n\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n/// Represents a C99 designated initializer expression.\n///\n/// A designated initializer expression (C99 6.7.8) contains one or\n/// more designators (which can be field designators, array\n/// designators, or GNU array-range designators) followed by an\n/// expression that initializes the field or element(s) that the\n/// designators refer to. For example, given:\n///\n/// @code\n/// struct point {\n///   double x;\n///   double y;\n/// };\n/// struct point ptarray[10] = { [2].y = 1.0, [2].x = 2.0, [0].x = 1.0 };\n/// @endcode\n///\n/// The InitListExpr contains three DesignatedInitExprs, the first of\n/// which covers @c [2].y=1.0. This DesignatedInitExpr will have two\n/// designators, one array designator for @c [2] followed by one field\n/// designator for @c .y. The initialization expression will be 1.0.\nclass DesignatedInitExpr final\n    : public Expr,\n      private llvm::TrailingObjects<DesignatedInitExpr, Stmt *> {\npublic:\n  /// Forward declaration of the Designator class.\n  class Designator;\n\nprivate:\n  /// The location of the '=' or ':' prior to the actual initializer\n  /// expression.\n  SourceLocation EqualOrColonLoc;\n\n  /// Whether this designated initializer used the GNU deprecated\n  /// syntax rather than the C99 '=' syntax.\n  unsigned GNUSyntax : 1;\n\n  /// The number of designators in this initializer expression.\n  unsigned NumDesignators : 15;\n\n  /// The number of subexpressions of this initializer expression,\n  /// which contains both the initializer and any additional\n  /// expressions used by array and array-range designators.\n  unsigned NumSubExprs : 16;\n\n  /// The designators in this designated initialization\n  /// expression.\n  Designator *Designators;\n\n  DesignatedInitExpr(const ASTContext &C, QualType Ty,\n                     llvm::ArrayRef<Designator> Designators,\n                     SourceLocation EqualOrColonLoc, bool GNUSyntax,\n                     ArrayRef<Expr *> IndexExprs, Expr *Init);\n\n  explicit DesignatedInitExpr(unsigned NumSubExprs)\n    : Expr(DesignatedInitExprClass, EmptyShell()),\n      NumDesignators(0), NumSubExprs(NumSubExprs), Designators(nullptr) { }\n\npublic:\n  /// A field designator, e.g., \".x\".\n  struct FieldDesignator {\n    /// Refers to the field that is being initialized. The low bit\n    /// of this field determines whether this is actually a pointer\n    /// to an IdentifierInfo (if 1) or a FieldDecl (if 0). When\n    /// initially constructed, a field designator will store an\n    /// IdentifierInfo*. After semantic analysis has resolved that\n    /// name, the field designator will instead store a FieldDecl*.\n    uintptr_t NameOrField;\n\n    /// The location of the '.' in the designated initializer.\n    SourceLocation DotLoc;\n\n    /// The location of the field name in the designated initializer.\n    SourceLocation FieldLoc;\n  };\n\n  /// An array or GNU array-range designator, e.g., \"[9]\" or \"[10..15]\".\n  struct ArrayOrRangeDesignator {\n    /// Location of the first index expression within the designated\n    /// initializer expression's list of subexpressions.\n    unsigned Index;\n    /// The location of the '[' starting the array range designator.\n    SourceLocation LBracketLoc;\n    /// The location of the ellipsis separating the start and end\n    /// indices. Only valid for GNU array-range designators.\n    SourceLocation EllipsisLoc;\n    /// The location of the ']' terminating the array range designator.\n    SourceLocation RBracketLoc;\n  };\n\n  /// Represents a single C99 designator.\n  ///\n  /// @todo This class is infuriatingly similar to clang::Designator,\n  /// but minor differences (storing indices vs. storing pointers)\n  /// keep us from reusing it. Try harder, later, to rectify these\n  /// differences.\n  class Designator {\n    /// The kind of designator this describes.\n    enum {\n      FieldDesignator,\n      ArrayDesignator,\n      ArrayRangeDesignator\n    } Kind;\n\n    union {\n      /// A field designator, e.g., \".x\".\n      struct FieldDesignator Field;\n      /// An array or GNU array-range designator, e.g., \"[9]\" or \"[10..15]\".\n      struct ArrayOrRangeDesignator ArrayOrRange;\n    };\n    friend class DesignatedInitExpr;\n\n  public:\n    Designator() {}\n\n    /// Initializes a field designator.\n    Designator(const IdentifierInfo *FieldName, SourceLocation DotLoc,\n               SourceLocation FieldLoc)\n      : Kind(FieldDesignator) {\n      new (&Field) DesignatedInitExpr::FieldDesignator;\n      Field.NameOrField = reinterpret_cast<uintptr_t>(FieldName) | 0x01;\n      Field.DotLoc = DotLoc;\n      Field.FieldLoc = FieldLoc;\n    }\n\n    /// Initializes an array designator.\n    Designator(unsigned Index, SourceLocation LBracketLoc,\n               SourceLocation RBracketLoc)\n      : Kind(ArrayDesignator) {\n      new (&ArrayOrRange) DesignatedInitExpr::ArrayOrRangeDesignator;\n      ArrayOrRange.Index = Index;\n      ArrayOrRange.LBracketLoc = LBracketLoc;\n      ArrayOrRange.EllipsisLoc = SourceLocation();\n      ArrayOrRange.RBracketLoc = RBracketLoc;\n    }\n\n    /// Initializes a GNU array-range designator.\n    Designator(unsigned Index, SourceLocation LBracketLoc,\n               SourceLocation EllipsisLoc, SourceLocation RBracketLoc)\n      : Kind(ArrayRangeDesignator) {\n      new (&ArrayOrRange) DesignatedInitExpr::ArrayOrRangeDesignator;\n      ArrayOrRange.Index = Index;\n      ArrayOrRange.LBracketLoc = LBracketLoc;\n      ArrayOrRange.EllipsisLoc = EllipsisLoc;\n      ArrayOrRange.RBracketLoc = RBracketLoc;\n    }\n\n    bool isFieldDesignator() const { return Kind == FieldDesignator; }\n    bool isArrayDesignator() const { return Kind == ArrayDesignator; }\n    bool isArrayRangeDesignator() const { return Kind == ArrayRangeDesignator; }\n\n    IdentifierInfo *getFieldName() const;\n\n    FieldDecl *getField() const {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      if (Field.NameOrField & 0x01)\n        return nullptr;\n      else\n        return reinterpret_cast<FieldDecl *>(Field.NameOrField);\n    }\n\n    void setField(FieldDecl *FD) {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      Field.NameOrField = reinterpret_cast<uintptr_t>(FD);\n    }\n\n    SourceLocation getDotLoc() const {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      return Field.DotLoc;\n    }\n\n    SourceLocation getFieldLoc() const {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      return Field.FieldLoc;\n    }\n\n    SourceLocation getLBracketLoc() const {\n      assert((Kind == ArrayDesignator || Kind == ArrayRangeDesignator) &&\n             \"Only valid on an array or array-range designator\");\n      return ArrayOrRange.LBracketLoc;\n    }\n\n    SourceLocation getRBracketLoc() const {\n      assert((Kind == ArrayDesignator || Kind == ArrayRangeDesignator) &&\n             \"Only valid on an array or array-range designator\");\n      return ArrayOrRange.RBracketLoc;\n    }\n\n    SourceLocation getEllipsisLoc() const {\n      assert(Kind == ArrayRangeDesignator &&\n             \"Only valid on an array-range designator\");\n      return ArrayOrRange.EllipsisLoc;\n    }\n\n    unsigned getFirstExprIndex() const {\n      assert((Kind == ArrayDesignator || Kind == ArrayRangeDesignator) &&\n             \"Only valid on an array or array-range designator\");\n      return ArrayOrRange.Index;\n    }\n\n    SourceLocation getBeginLoc() const LLVM_READONLY {\n      if (Kind == FieldDesignator)\n        return getDotLoc().isInvalid()? getFieldLoc() : getDotLoc();\n      else\n        return getLBracketLoc();\n    }\n    SourceLocation getEndLoc() const LLVM_READONLY {\n      return Kind == FieldDesignator ? getFieldLoc() : getRBracketLoc();\n    }\n    SourceRange getSourceRange() const LLVM_READONLY {\n      return SourceRange(getBeginLoc(), getEndLoc());\n    }\n  };\n\n  static DesignatedInitExpr *Create(const ASTContext &C,\n                                    llvm::ArrayRef<Designator> Designators,\n                                    ArrayRef<Expr*> IndexExprs,\n                                    SourceLocation EqualOrColonLoc,\n                                    bool GNUSyntax, Expr *Init);\n\n  static DesignatedInitExpr *CreateEmpty(const ASTContext &C,\n                                         unsigned NumIndexExprs);\n\n  /// Returns the number of designators in this initializer.\n  unsigned size() const { return NumDesignators; }\n\n  // Iterator access to the designators.\n  llvm::MutableArrayRef<Designator> designators() {\n    return {Designators, NumDesignators};\n  }\n\n  llvm::ArrayRef<Designator> designators() const {\n    return {Designators, NumDesignators};\n  }\n\n  Designator *getDesignator(unsigned Idx) { return &designators()[Idx]; }\n  const Designator *getDesignator(unsigned Idx) const {\n    return &designators()[Idx];\n  }\n\n  void setDesignators(const ASTContext &C, const Designator *Desigs,\n                      unsigned NumDesigs);\n\n  Expr *getArrayIndex(const Designator &D) const;\n  Expr *getArrayRangeStart(const Designator &D) const;\n  Expr *getArrayRangeEnd(const Designator &D) const;\n\n  /// Retrieve the location of the '=' that precedes the\n  /// initializer value itself, if present.\n  SourceLocation getEqualOrColonLoc() const { return EqualOrColonLoc; }\n  void setEqualOrColonLoc(SourceLocation L) { EqualOrColonLoc = L; }\n\n  /// Whether this designated initializer should result in direct-initialization\n  /// of the designated subobject (eg, '{.foo{1, 2, 3}}').\n  bool isDirectInit() const { return EqualOrColonLoc.isInvalid(); }\n\n  /// Determines whether this designated initializer used the\n  /// deprecated GNU syntax for designated initializers.\n  bool usesGNUSyntax() const { return GNUSyntax; }\n  void setGNUSyntax(bool GNU) { GNUSyntax = GNU; }\n\n  /// Retrieve the initializer value.\n  Expr *getInit() const {\n    return cast<Expr>(*const_cast<DesignatedInitExpr*>(this)->child_begin());\n  }\n\n  void setInit(Expr *init) {\n    *child_begin() = init;\n  }\n\n  /// Retrieve the total number of subexpressions in this\n  /// designated initializer expression, including the actual\n  /// initialized value and any expressions that occur within array\n  /// and array-range designators.\n  unsigned getNumSubExprs() const { return NumSubExprs; }\n\n  Expr *getSubExpr(unsigned Idx) const {\n    assert(Idx < NumSubExprs && \"Subscript out of range\");\n    return cast<Expr>(getTrailingObjects<Stmt *>()[Idx]);\n  }\n\n  void setSubExpr(unsigned Idx, Expr *E) {\n    assert(Idx < NumSubExprs && \"Subscript out of range\");\n    getTrailingObjects<Stmt *>()[Idx] = E;\n  }\n\n  /// Replaces the designator at index @p Idx with the series\n  /// of designators in [First, Last).\n  void ExpandDesignator(const ASTContext &C, unsigned Idx,\n                        const Designator *First, const Designator *Last);\n\n  SourceRange getDesignatorsSourceRange() const;\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == DesignatedInitExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    Stmt **begin = getTrailingObjects<Stmt *>();\n    return child_range(begin, begin + NumSubExprs);\n  }\n  const_child_range children() const {\n    Stmt * const *begin = getTrailingObjects<Stmt *>();\n    return const_child_range(begin, begin + NumSubExprs);\n  }\n\n  friend TrailingObjects;\n};\n\n/// Represents a place-holder for an object not to be initialized by\n/// anything.\n///\n/// This only makes sense when it appears as part of an updater of a\n/// DesignatedInitUpdateExpr (see below). The base expression of a DIUE\n/// initializes a big object, and the NoInitExpr's mark the spots within the\n/// big object not to be overwritten by the updater.\n///\n/// \\see DesignatedInitUpdateExpr\nclass NoInitExpr : public Expr {\npublic:\n  explicit NoInitExpr(QualType ty)\n      : Expr(NoInitExprClass, ty, VK_RValue, OK_Ordinary) {\n    setDependence(computeDependence(this));\n  }\n\n  explicit NoInitExpr(EmptyShell Empty)\n    : Expr(NoInitExprClass, Empty) { }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == NoInitExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return SourceLocation(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return SourceLocation(); }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n// In cases like:\n//   struct Q { int a, b, c; };\n//   Q *getQ();\n//   void foo() {\n//     struct A { Q q; } a = { *getQ(), .q.b = 3 };\n//   }\n//\n// We will have an InitListExpr for a, with type A, and then a\n// DesignatedInitUpdateExpr for \"a.q\" with type Q. The \"base\" for this DIUE\n// is the call expression *getQ(); the \"updater\" for the DIUE is \".q.b = 3\"\n//\nclass DesignatedInitUpdateExpr : public Expr {\n  // BaseAndUpdaterExprs[0] is the base expression;\n  // BaseAndUpdaterExprs[1] is an InitListExpr overwriting part of the base.\n  Stmt *BaseAndUpdaterExprs[2];\n\npublic:\n  DesignatedInitUpdateExpr(const ASTContext &C, SourceLocation lBraceLoc,\n                           Expr *baseExprs, SourceLocation rBraceLoc);\n\n  explicit DesignatedInitUpdateExpr(EmptyShell Empty)\n    : Expr(DesignatedInitUpdateExprClass, Empty) { }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == DesignatedInitUpdateExprClass;\n  }\n\n  Expr *getBase() const { return cast<Expr>(BaseAndUpdaterExprs[0]); }\n  void setBase(Expr *Base) { BaseAndUpdaterExprs[0] = Base; }\n\n  InitListExpr *getUpdater() const {\n    return cast<InitListExpr>(BaseAndUpdaterExprs[1]);\n  }\n  void setUpdater(Expr *Updater) { BaseAndUpdaterExprs[1] = Updater; }\n\n  // Iterators\n  // children = the base and the updater\n  child_range children() {\n    return child_range(&BaseAndUpdaterExprs[0], &BaseAndUpdaterExprs[0] + 2);\n  }\n  const_child_range children() const {\n    return const_child_range(&BaseAndUpdaterExprs[0],\n                             &BaseAndUpdaterExprs[0] + 2);\n  }\n};\n\n/// Represents a loop initializing the elements of an array.\n///\n/// The need to initialize the elements of an array occurs in a number of\n/// contexts:\n///\n///  * in the implicit copy/move constructor for a class with an array member\n///  * when a lambda-expression captures an array by value\n///  * when a decomposition declaration decomposes an array\n///\n/// There are two subexpressions: a common expression (the source array)\n/// that is evaluated once up-front, and a per-element initializer that\n/// runs once for each array element.\n///\n/// Within the per-element initializer, the common expression may be referenced\n/// via an OpaqueValueExpr, and the current index may be obtained via an\n/// ArrayInitIndexExpr.\nclass ArrayInitLoopExpr : public Expr {\n  Stmt *SubExprs[2];\n\n  explicit ArrayInitLoopExpr(EmptyShell Empty)\n      : Expr(ArrayInitLoopExprClass, Empty), SubExprs{} {}\n\npublic:\n  explicit ArrayInitLoopExpr(QualType T, Expr *CommonInit, Expr *ElementInit)\n      : Expr(ArrayInitLoopExprClass, T, VK_RValue, OK_Ordinary),\n        SubExprs{CommonInit, ElementInit} {\n    setDependence(computeDependence(this));\n  }\n\n  /// Get the common subexpression shared by all initializations (the source\n  /// array).\n  OpaqueValueExpr *getCommonExpr() const {\n    return cast<OpaqueValueExpr>(SubExprs[0]);\n  }\n\n  /// Get the initializer to use for each array element.\n  Expr *getSubExpr() const { return cast<Expr>(SubExprs[1]); }\n\n  llvm::APInt getArraySize() const {\n    return cast<ConstantArrayType>(getType()->castAsArrayTypeUnsafe())\n        ->getSize();\n  }\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() == ArrayInitLoopExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCommonExpr()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getCommonExpr()->getEndLoc();\n  }\n\n  child_range children() {\n    return child_range(SubExprs, SubExprs + 2);\n  }\n  const_child_range children() const {\n    return const_child_range(SubExprs, SubExprs + 2);\n  }\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n/// Represents the index of the current element of an array being\n/// initialized by an ArrayInitLoopExpr. This can only appear within the\n/// subexpression of an ArrayInitLoopExpr.\nclass ArrayInitIndexExpr : public Expr {\n  explicit ArrayInitIndexExpr(EmptyShell Empty)\n      : Expr(ArrayInitIndexExprClass, Empty) {}\n\npublic:\n  explicit ArrayInitIndexExpr(QualType T)\n      : Expr(ArrayInitIndexExprClass, T, VK_RValue, OK_Ordinary) {\n    setDependence(ExprDependence::None);\n  }\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() == ArrayInitIndexExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return SourceLocation(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return SourceLocation(); }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n};\n\n/// Represents an implicitly-generated value initialization of\n/// an object of a given type.\n///\n/// Implicit value initializations occur within semantic initializer\n/// list expressions (InitListExpr) as placeholders for subobject\n/// initializations not explicitly specified by the user.\n///\n/// \\see InitListExpr\nclass ImplicitValueInitExpr : public Expr {\npublic:\n  explicit ImplicitValueInitExpr(QualType ty)\n      : Expr(ImplicitValueInitExprClass, ty, VK_RValue, OK_Ordinary) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Construct an empty implicit value initialization.\n  explicit ImplicitValueInitExpr(EmptyShell Empty)\n    : Expr(ImplicitValueInitExprClass, Empty) { }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ImplicitValueInitExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return SourceLocation(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return SourceLocation(); }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass ParenListExpr final\n    : public Expr,\n      private llvm::TrailingObjects<ParenListExpr, Stmt *> {\n  friend class ASTStmtReader;\n  friend TrailingObjects;\n\n  /// The location of the left and right parentheses.\n  SourceLocation LParenLoc, RParenLoc;\n\n  /// Build a paren list.\n  ParenListExpr(SourceLocation LParenLoc, ArrayRef<Expr *> Exprs,\n                SourceLocation RParenLoc);\n\n  /// Build an empty paren list.\n  ParenListExpr(EmptyShell Empty, unsigned NumExprs);\n\npublic:\n  /// Create a paren list.\n  static ParenListExpr *Create(const ASTContext &Ctx, SourceLocation LParenLoc,\n                               ArrayRef<Expr *> Exprs,\n                               SourceLocation RParenLoc);\n\n  /// Create an empty paren list.\n  static ParenListExpr *CreateEmpty(const ASTContext &Ctx, unsigned NumExprs);\n\n  /// Return the number of expressions in this paren list.\n  unsigned getNumExprs() const { return ParenListExprBits.NumExprs; }\n\n  Expr *getExpr(unsigned Init) {\n    assert(Init < getNumExprs() && \"Initializer access out of range!\");\n    return getExprs()[Init];\n  }\n\n  const Expr *getExpr(unsigned Init) const {\n    return const_cast<ParenListExpr *>(this)->getExpr(Init);\n  }\n\n  Expr **getExprs() {\n    return reinterpret_cast<Expr **>(getTrailingObjects<Stmt *>());\n  }\n\n  ArrayRef<Expr *> exprs() {\n    return llvm::makeArrayRef(getExprs(), getNumExprs());\n  }\n\n  SourceLocation getLParenLoc() const { return LParenLoc; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  SourceLocation getBeginLoc() const { return getLParenLoc(); }\n  SourceLocation getEndLoc() const { return getRParenLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ParenListExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(getTrailingObjects<Stmt *>(),\n                       getTrailingObjects<Stmt *>() + getNumExprs());\n  }\n  const_child_range children() const {\n    return const_child_range(getTrailingObjects<Stmt *>(),\n                             getTrailingObjects<Stmt *>() + getNumExprs());\n  }\n};\n\n/// Represents a C11 generic selection.\n///\n/// A generic selection (C11 6.5.1.1) contains an unevaluated controlling\n/// expression, followed by one or more generic associations.  Each generic\n/// association specifies a type name and an expression, or \"default\" and an\n/// expression (in which case it is known as a default generic association).\n/// The type and value of the generic selection are identical to those of its\n/// result expression, which is defined as the expression in the generic\n/// association with a type name that is compatible with the type of the\n/// controlling expression, or the expression in the default generic association\n/// if no types are compatible.  For example:\n///\n/// @code\n/// _Generic(X, double: 1, float: 2, default: 3)\n/// @endcode\n///\n/// The above expression evaluates to 1 if 1.0 is substituted for X, 2 if 1.0f\n/// or 3 if \"hello\".\n///\n/// As an extension, generic selections are allowed in C++, where the following\n/// additional semantics apply:\n///\n/// Any generic selection whose controlling expression is type-dependent or\n/// which names a dependent type in its association list is result-dependent,\n/// which means that the choice of result expression is dependent.\n/// Result-dependent generic associations are both type- and value-dependent.\nclass GenericSelectionExpr final\n    : public Expr,\n      private llvm::TrailingObjects<GenericSelectionExpr, Stmt *,\n                                    TypeSourceInfo *> {\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n  friend TrailingObjects;\n\n  /// The number of association expressions and the index of the result\n  /// expression in the case where the generic selection expression is not\n  /// result-dependent. The result index is equal to ResultDependentIndex\n  /// if and only if the generic selection expression is result-dependent.\n  unsigned NumAssocs, ResultIndex;\n  enum : unsigned {\n    ResultDependentIndex = std::numeric_limits<unsigned>::max(),\n    ControllingIndex = 0,\n    AssocExprStartIndex = 1\n  };\n\n  /// The location of the \"default\" and of the right parenthesis.\n  SourceLocation DefaultLoc, RParenLoc;\n\n  // GenericSelectionExpr is followed by several trailing objects.\n  // They are (in order):\n  //\n  // * A single Stmt * for the controlling expression.\n  // * An array of getNumAssocs() Stmt * for the association expressions.\n  // * An array of getNumAssocs() TypeSourceInfo *, one for each of the\n  //   association expressions.\n  unsigned numTrailingObjects(OverloadToken<Stmt *>) const {\n    // Add one to account for the controlling expression; the remainder\n    // are the associated expressions.\n    return 1 + getNumAssocs();\n  }\n\n  unsigned numTrailingObjects(OverloadToken<TypeSourceInfo *>) const {\n    return getNumAssocs();\n  }\n\n  template <bool Const> class AssociationIteratorTy;\n  /// Bundle together an association expression and its TypeSourceInfo.\n  /// The Const template parameter is for the const and non-const versions\n  /// of AssociationTy.\n  template <bool Const> class AssociationTy {\n    friend class GenericSelectionExpr;\n    template <bool OtherConst> friend class AssociationIteratorTy;\n    using ExprPtrTy = std::conditional_t<Const, const Expr *, Expr *>;\n    using TSIPtrTy =\n        std::conditional_t<Const, const TypeSourceInfo *, TypeSourceInfo *>;\n    ExprPtrTy E;\n    TSIPtrTy TSI;\n    bool Selected;\n    AssociationTy(ExprPtrTy E, TSIPtrTy TSI, bool Selected)\n        : E(E), TSI(TSI), Selected(Selected) {}\n\n  public:\n    ExprPtrTy getAssociationExpr() const { return E; }\n    TSIPtrTy getTypeSourceInfo() const { return TSI; }\n    QualType getType() const { return TSI ? TSI->getType() : QualType(); }\n    bool isSelected() const { return Selected; }\n    AssociationTy *operator->() { return this; }\n    const AssociationTy *operator->() const { return this; }\n  }; // class AssociationTy\n\n  /// Iterator over const and non-const Association objects. The Association\n  /// objects are created on the fly when the iterator is dereferenced.\n  /// This abstract over how exactly the association expressions and the\n  /// corresponding TypeSourceInfo * are stored.\n  template <bool Const>\n  class AssociationIteratorTy\n      : public llvm::iterator_facade_base<\n            AssociationIteratorTy<Const>, std::input_iterator_tag,\n            AssociationTy<Const>, std::ptrdiff_t, AssociationTy<Const>,\n            AssociationTy<Const>> {\n    friend class GenericSelectionExpr;\n    // FIXME: This iterator could conceptually be a random access iterator, and\n    // it would be nice if we could strengthen the iterator category someday.\n    // However this iterator does not satisfy two requirements of forward\n    // iterators:\n    // a) reference = T& or reference = const T&\n    // b) If It1 and It2 are both dereferenceable, then It1 == It2 if and only\n    //    if *It1 and *It2 are bound to the same objects.\n    // An alternative design approach was discussed during review;\n    // store an Association object inside the iterator, and return a reference\n    // to it when dereferenced. This idea was discarded beacuse of nasty\n    // lifetime issues:\n    //    AssociationIterator It = ...;\n    //    const Association &Assoc = *It++; // Oops, Assoc is dangling.\n    using BaseTy = typename AssociationIteratorTy::iterator_facade_base;\n    using StmtPtrPtrTy =\n        std::conditional_t<Const, const Stmt *const *, Stmt **>;\n    using TSIPtrPtrTy = std::conditional_t<Const, const TypeSourceInfo *const *,\n                                           TypeSourceInfo **>;\n    StmtPtrPtrTy E; // = nullptr; FIXME: Once support for gcc 4.8 is dropped.\n    TSIPtrPtrTy TSI; // Kept in sync with E.\n    unsigned Offset = 0, SelectedOffset = 0;\n    AssociationIteratorTy(StmtPtrPtrTy E, TSIPtrPtrTy TSI, unsigned Offset,\n                          unsigned SelectedOffset)\n        : E(E), TSI(TSI), Offset(Offset), SelectedOffset(SelectedOffset) {}\n\n  public:\n    AssociationIteratorTy() : E(nullptr), TSI(nullptr) {}\n    typename BaseTy::reference operator*() const {\n      return AssociationTy<Const>(cast<Expr>(*E), *TSI,\n                                  Offset == SelectedOffset);\n    }\n    typename BaseTy::pointer operator->() const { return **this; }\n    using BaseTy::operator++;\n    AssociationIteratorTy &operator++() {\n      ++E;\n      ++TSI;\n      ++Offset;\n      return *this;\n    }\n    bool operator==(AssociationIteratorTy Other) const { return E == Other.E; }\n  }; // class AssociationIterator\n\n  /// Build a non-result-dependent generic selection expression.\n  GenericSelectionExpr(const ASTContext &Context, SourceLocation GenericLoc,\n                       Expr *ControllingExpr,\n                       ArrayRef<TypeSourceInfo *> AssocTypes,\n                       ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n                       SourceLocation RParenLoc,\n                       bool ContainsUnexpandedParameterPack,\n                       unsigned ResultIndex);\n\n  /// Build a result-dependent generic selection expression.\n  GenericSelectionExpr(const ASTContext &Context, SourceLocation GenericLoc,\n                       Expr *ControllingExpr,\n                       ArrayRef<TypeSourceInfo *> AssocTypes,\n                       ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n                       SourceLocation RParenLoc,\n                       bool ContainsUnexpandedParameterPack);\n\n  /// Build an empty generic selection expression for deserialization.\n  explicit GenericSelectionExpr(EmptyShell Empty, unsigned NumAssocs);\n\npublic:\n  /// Create a non-result-dependent generic selection expression.\n  static GenericSelectionExpr *\n  Create(const ASTContext &Context, SourceLocation GenericLoc,\n         Expr *ControllingExpr, ArrayRef<TypeSourceInfo *> AssocTypes,\n         ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n         SourceLocation RParenLoc, bool ContainsUnexpandedParameterPack,\n         unsigned ResultIndex);\n\n  /// Create a result-dependent generic selection expression.\n  static GenericSelectionExpr *\n  Create(const ASTContext &Context, SourceLocation GenericLoc,\n         Expr *ControllingExpr, ArrayRef<TypeSourceInfo *> AssocTypes,\n         ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n         SourceLocation RParenLoc, bool ContainsUnexpandedParameterPack);\n\n  /// Create an empty generic selection expression for deserialization.\n  static GenericSelectionExpr *CreateEmpty(const ASTContext &Context,\n                                           unsigned NumAssocs);\n\n  using Association = AssociationTy<false>;\n  using ConstAssociation = AssociationTy<true>;\n  using AssociationIterator = AssociationIteratorTy<false>;\n  using ConstAssociationIterator = AssociationIteratorTy<true>;\n  using association_range = llvm::iterator_range<AssociationIterator>;\n  using const_association_range =\n      llvm::iterator_range<ConstAssociationIterator>;\n\n  /// The number of association expressions.\n  unsigned getNumAssocs() const { return NumAssocs; }\n\n  /// The zero-based index of the result expression's generic association in\n  /// the generic selection's association list.  Defined only if the\n  /// generic selection is not result-dependent.\n  unsigned getResultIndex() const {\n    assert(!isResultDependent() &&\n           \"Generic selection is result-dependent but getResultIndex called!\");\n    return ResultIndex;\n  }\n\n  /// Whether this generic selection is result-dependent.\n  bool isResultDependent() const { return ResultIndex == ResultDependentIndex; }\n\n  /// Return the controlling expression of this generic selection expression.\n  Expr *getControllingExpr() {\n    return cast<Expr>(getTrailingObjects<Stmt *>()[ControllingIndex]);\n  }\n  const Expr *getControllingExpr() const {\n    return cast<Expr>(getTrailingObjects<Stmt *>()[ControllingIndex]);\n  }\n\n  /// Return the result expression of this controlling expression. Defined if\n  /// and only if the generic selection expression is not result-dependent.\n  Expr *getResultExpr() {\n    return cast<Expr>(\n        getTrailingObjects<Stmt *>()[AssocExprStartIndex + getResultIndex()]);\n  }\n  const Expr *getResultExpr() const {\n    return cast<Expr>(\n        getTrailingObjects<Stmt *>()[AssocExprStartIndex + getResultIndex()]);\n  }\n\n  ArrayRef<Expr *> getAssocExprs() const {\n    return {reinterpret_cast<Expr *const *>(getTrailingObjects<Stmt *>() +\n                                            AssocExprStartIndex),\n            NumAssocs};\n  }\n  ArrayRef<TypeSourceInfo *> getAssocTypeSourceInfos() const {\n    return {getTrailingObjects<TypeSourceInfo *>(), NumAssocs};\n  }\n\n  /// Return the Ith association expression with its TypeSourceInfo,\n  /// bundled together in GenericSelectionExpr::(Const)Association.\n  Association getAssociation(unsigned I) {\n    assert(I < getNumAssocs() &&\n           \"Out-of-range index in GenericSelectionExpr::getAssociation!\");\n    return Association(\n        cast<Expr>(getTrailingObjects<Stmt *>()[AssocExprStartIndex + I]),\n        getTrailingObjects<TypeSourceInfo *>()[I],\n        !isResultDependent() && (getResultIndex() == I));\n  }\n  ConstAssociation getAssociation(unsigned I) const {\n    assert(I < getNumAssocs() &&\n           \"Out-of-range index in GenericSelectionExpr::getAssociation!\");\n    return ConstAssociation(\n        cast<Expr>(getTrailingObjects<Stmt *>()[AssocExprStartIndex + I]),\n        getTrailingObjects<TypeSourceInfo *>()[I],\n        !isResultDependent() && (getResultIndex() == I));\n  }\n\n  association_range associations() {\n    AssociationIterator Begin(getTrailingObjects<Stmt *>() +\n                                  AssocExprStartIndex,\n                              getTrailingObjects<TypeSourceInfo *>(),\n                              /*Offset=*/0, ResultIndex);\n    AssociationIterator End(Begin.E + NumAssocs, Begin.TSI + NumAssocs,\n                            /*Offset=*/NumAssocs, ResultIndex);\n    return llvm::make_range(Begin, End);\n  }\n\n  const_association_range associations() const {\n    ConstAssociationIterator Begin(getTrailingObjects<Stmt *>() +\n                                       AssocExprStartIndex,\n                                   getTrailingObjects<TypeSourceInfo *>(),\n                                   /*Offset=*/0, ResultIndex);\n    ConstAssociationIterator End(Begin.E + NumAssocs, Begin.TSI + NumAssocs,\n                                 /*Offset=*/NumAssocs, ResultIndex);\n    return llvm::make_range(Begin, End);\n  }\n\n  SourceLocation getGenericLoc() const {\n    return GenericSelectionExprBits.GenericLoc;\n  }\n  SourceLocation getDefaultLoc() const { return DefaultLoc; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  SourceLocation getBeginLoc() const { return getGenericLoc(); }\n  SourceLocation getEndLoc() const { return getRParenLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == GenericSelectionExprClass;\n  }\n\n  child_range children() {\n    return child_range(getTrailingObjects<Stmt *>(),\n                       getTrailingObjects<Stmt *>() +\n                           numTrailingObjects(OverloadToken<Stmt *>()));\n  }\n  const_child_range children() const {\n    return const_child_range(getTrailingObjects<Stmt *>(),\n                             getTrailingObjects<Stmt *>() +\n                                 numTrailingObjects(OverloadToken<Stmt *>()));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n// Clang Extensions\n//===----------------------------------------------------------------------===//\n\n/// ExtVectorElementExpr - This represents access to specific elements of a\n/// vector, and may occur on the left hand side or right hand side.  For example\n/// the following is legal:  \"V.xy = V.zw\" if V is a 4 element extended vector.\n///\n/// Note that the base may have either vector or pointer to vector type, just\n/// like a struct field reference.\n///\nclass ExtVectorElementExpr : public Expr {\n  Stmt *Base;\n  IdentifierInfo *Accessor;\n  SourceLocation AccessorLoc;\npublic:\n  ExtVectorElementExpr(QualType ty, ExprValueKind VK, Expr *base,\n                       IdentifierInfo &accessor, SourceLocation loc)\n      : Expr(ExtVectorElementExprClass, ty, VK,\n             (VK == VK_RValue ? OK_Ordinary : OK_VectorComponent)),\n        Base(base), Accessor(&accessor), AccessorLoc(loc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty vector element expression.\n  explicit ExtVectorElementExpr(EmptyShell Empty)\n    : Expr(ExtVectorElementExprClass, Empty) { }\n\n  const Expr *getBase() const { return cast<Expr>(Base); }\n  Expr *getBase() { return cast<Expr>(Base); }\n  void setBase(Expr *E) { Base = E; }\n\n  IdentifierInfo &getAccessor() const { return *Accessor; }\n  void setAccessor(IdentifierInfo *II) { Accessor = II; }\n\n  SourceLocation getAccessorLoc() const { return AccessorLoc; }\n  void setAccessorLoc(SourceLocation L) { AccessorLoc = L; }\n\n  /// getNumElements - Get the number of components being selected.\n  unsigned getNumElements() const;\n\n  /// containsDuplicateElements - Return true if any element access is\n  /// repeated.\n  bool containsDuplicateElements() const;\n\n  /// getEncodedElementAccess - Encode the elements accessed into an llvm\n  /// aggregate Constant of ConstantInt(s).\n  void getEncodedElementAccess(SmallVectorImpl<uint32_t> &Elts) const;\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getBase()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY { return AccessorLoc; }\n\n  /// isArrow - Return true if the base expression is a pointer to vector,\n  /// return false if the base expression is a vector.\n  bool isArrow() const;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ExtVectorElementExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Base, &Base+1); }\n  const_child_range children() const {\n    return const_child_range(&Base, &Base + 1);\n  }\n};\n\n/// BlockExpr - Adaptor class for mixing a BlockDecl with expressions.\n/// ^{ statement-body }   or   ^(int arg1, float arg2){ statement-body }\nclass BlockExpr : public Expr {\nprotected:\n  BlockDecl *TheBlock;\npublic:\n  BlockExpr(BlockDecl *BD, QualType ty)\n      : Expr(BlockExprClass, ty, VK_RValue, OK_Ordinary), TheBlock(BD) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty block expression.\n  explicit BlockExpr(EmptyShell Empty) : Expr(BlockExprClass, Empty) { }\n\n  const BlockDecl *getBlockDecl() const { return TheBlock; }\n  BlockDecl *getBlockDecl() { return TheBlock; }\n  void setBlockDecl(BlockDecl *BD) { TheBlock = BD; }\n\n  // Convenience functions for probing the underlying BlockDecl.\n  SourceLocation getCaretLocation() const;\n  const Stmt *getBody() const;\n  Stmt *getBody();\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCaretLocation();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getBody()->getEndLoc();\n  }\n\n  /// getFunctionType - Return the underlying function type for this block.\n  const FunctionProtoType *getFunctionType() const;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == BlockExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// Copy initialization expr of a __block variable and a boolean flag that\n/// indicates whether the expression can throw.\nstruct BlockVarCopyInit {\n  BlockVarCopyInit() = default;\n  BlockVarCopyInit(Expr *CopyExpr, bool CanThrow)\n      : ExprAndFlag(CopyExpr, CanThrow) {}\n  void setExprAndFlag(Expr *CopyExpr, bool CanThrow) {\n    ExprAndFlag.setPointerAndInt(CopyExpr, CanThrow);\n  }\n  Expr *getCopyExpr() const { return ExprAndFlag.getPointer(); }\n  bool canThrow() const { return ExprAndFlag.getInt(); }\n  llvm::PointerIntPair<Expr *, 1, bool> ExprAndFlag;\n};\n\n/// AsTypeExpr - Clang builtin function __builtin_astype [OpenCL 6.2.4.2]\n/// This AST node provides support for reinterpreting a type to another\n/// type of the same size.\nclass AsTypeExpr : public Expr {\nprivate:\n  Stmt *SrcExpr;\n  SourceLocation BuiltinLoc, RParenLoc;\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  explicit AsTypeExpr(EmptyShell Empty) : Expr(AsTypeExprClass, Empty) {}\n\npublic:\n  AsTypeExpr(Expr *SrcExpr, QualType DstType, ExprValueKind VK,\n             ExprObjectKind OK, SourceLocation BuiltinLoc,\n             SourceLocation RParenLoc)\n      : Expr(AsTypeExprClass, DstType, VK, OK), SrcExpr(SrcExpr),\n        BuiltinLoc(BuiltinLoc), RParenLoc(RParenLoc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// getSrcExpr - Return the Expr to be converted.\n  Expr *getSrcExpr() const { return cast<Expr>(SrcExpr); }\n\n  /// getBuiltinLoc - Return the location of the __builtin_astype token.\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n\n  /// getRParenLoc - Return the location of final right parenthesis.\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == AsTypeExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&SrcExpr, &SrcExpr+1); }\n  const_child_range children() const {\n    return const_child_range(&SrcExpr, &SrcExpr + 1);\n  }\n};\n\n/// PseudoObjectExpr - An expression which accesses a pseudo-object\n/// l-value.  A pseudo-object is an abstract object, accesses to which\n/// are translated to calls.  The pseudo-object expression has a\n/// syntactic form, which shows how the expression was actually\n/// written in the source code, and a semantic form, which is a series\n/// of expressions to be executed in order which detail how the\n/// operation is actually evaluated.  Optionally, one of the semantic\n/// forms may also provide a result value for the expression.\n///\n/// If any of the semantic-form expressions is an OpaqueValueExpr,\n/// that OVE is required to have a source expression, and it is bound\n/// to the result of that source expression.  Such OVEs may appear\n/// only in subsequent semantic-form expressions and as\n/// sub-expressions of the syntactic form.\n///\n/// PseudoObjectExpr should be used only when an operation can be\n/// usefully described in terms of fairly simple rewrite rules on\n/// objects and functions that are meant to be used by end-developers.\n/// For example, under the Itanium ABI, dynamic casts are implemented\n/// as a call to a runtime function called __dynamic_cast; using this\n/// class to describe that would be inappropriate because that call is\n/// not really part of the user-visible semantics, and instead the\n/// cast is properly reflected in the AST and IR-generation has been\n/// taught to generate the call as necessary.  In contrast, an\n/// Objective-C property access is semantically defined to be\n/// equivalent to a particular message send, and this is very much\n/// part of the user model.  The name of this class encourages this\n/// modelling design.\nclass PseudoObjectExpr final\n    : public Expr,\n      private llvm::TrailingObjects<PseudoObjectExpr, Expr *> {\n  // PseudoObjectExprBits.NumSubExprs - The number of sub-expressions.\n  // Always at least two, because the first sub-expression is the\n  // syntactic form.\n\n  // PseudoObjectExprBits.ResultIndex - The index of the\n  // sub-expression holding the result.  0 means the result is void,\n  // which is unambiguous because it's the index of the syntactic\n  // form.  Note that this is therefore 1 higher than the value passed\n  // in to Create, which is an index within the semantic forms.\n  // Note also that ASTStmtWriter assumes this encoding.\n\n  Expr **getSubExprsBuffer() { return getTrailingObjects<Expr *>(); }\n  const Expr * const *getSubExprsBuffer() const {\n    return getTrailingObjects<Expr *>();\n  }\n\n  PseudoObjectExpr(QualType type, ExprValueKind VK,\n                   Expr *syntactic, ArrayRef<Expr*> semantic,\n                   unsigned resultIndex);\n\n  PseudoObjectExpr(EmptyShell shell, unsigned numSemanticExprs);\n\n  unsigned getNumSubExprs() const {\n    return PseudoObjectExprBits.NumSubExprs;\n  }\n\npublic:\n  /// NoResult - A value for the result index indicating that there is\n  /// no semantic result.\n  enum : unsigned { NoResult = ~0U };\n\n  static PseudoObjectExpr *Create(const ASTContext &Context, Expr *syntactic,\n                                  ArrayRef<Expr*> semantic,\n                                  unsigned resultIndex);\n\n  static PseudoObjectExpr *Create(const ASTContext &Context, EmptyShell shell,\n                                  unsigned numSemanticExprs);\n\n  /// Return the syntactic form of this expression, i.e. the\n  /// expression it actually looks like.  Likely to be expressed in\n  /// terms of OpaqueValueExprs bound in the semantic form.\n  Expr *getSyntacticForm() { return getSubExprsBuffer()[0]; }\n  const Expr *getSyntacticForm() const { return getSubExprsBuffer()[0]; }\n\n  /// Return the index of the result-bearing expression into the semantics\n  /// expressions, or PseudoObjectExpr::NoResult if there is none.\n  unsigned getResultExprIndex() const {\n    if (PseudoObjectExprBits.ResultIndex == 0) return NoResult;\n    return PseudoObjectExprBits.ResultIndex - 1;\n  }\n\n  /// Return the result-bearing expression, or null if there is none.\n  Expr *getResultExpr() {\n    if (PseudoObjectExprBits.ResultIndex == 0)\n      return nullptr;\n    return getSubExprsBuffer()[PseudoObjectExprBits.ResultIndex];\n  }\n  const Expr *getResultExpr() const {\n    return const_cast<PseudoObjectExpr*>(this)->getResultExpr();\n  }\n\n  unsigned getNumSemanticExprs() const { return getNumSubExprs() - 1; }\n\n  typedef Expr * const *semantics_iterator;\n  typedef const Expr * const *const_semantics_iterator;\n  semantics_iterator semantics_begin() {\n    return getSubExprsBuffer() + 1;\n  }\n  const_semantics_iterator semantics_begin() const {\n    return getSubExprsBuffer() + 1;\n  }\n  semantics_iterator semantics_end() {\n    return getSubExprsBuffer() + getNumSubExprs();\n  }\n  const_semantics_iterator semantics_end() const {\n    return getSubExprsBuffer() + getNumSubExprs();\n  }\n\n  llvm::iterator_range<semantics_iterator> semantics() {\n    return llvm::make_range(semantics_begin(), semantics_end());\n  }\n  llvm::iterator_range<const_semantics_iterator> semantics() const {\n    return llvm::make_range(semantics_begin(), semantics_end());\n  }\n\n  Expr *getSemanticExpr(unsigned index) {\n    assert(index + 1 < getNumSubExprs());\n    return getSubExprsBuffer()[index + 1];\n  }\n  const Expr *getSemanticExpr(unsigned index) const {\n    return const_cast<PseudoObjectExpr*>(this)->getSemanticExpr(index);\n  }\n\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return getSyntacticForm()->getExprLoc();\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getSyntacticForm()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getSyntacticForm()->getEndLoc();\n  }\n\n  child_range children() {\n    const_child_range CCR =\n        const_cast<const PseudoObjectExpr *>(this)->children();\n    return child_range(cast_away_const(CCR.begin()),\n                       cast_away_const(CCR.end()));\n  }\n  const_child_range children() const {\n    Stmt *const *cs = const_cast<Stmt *const *>(\n        reinterpret_cast<const Stmt *const *>(getSubExprsBuffer()));\n    return const_child_range(cs, cs + getNumSubExprs());\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == PseudoObjectExprClass;\n  }\n\n  friend TrailingObjects;\n  friend class ASTStmtReader;\n};\n\n/// AtomicExpr - Variadic atomic builtins: __atomic_exchange, __atomic_fetch_*,\n/// __atomic_load, __atomic_store, and __atomic_compare_exchange_*, for the\n/// similarly-named C++11 instructions, and __c11 variants for <stdatomic.h>,\n/// and corresponding __opencl_atomic_* for OpenCL 2.0.\n/// All of these instructions take one primary pointer, at least one memory\n/// order. The instructions for which getScopeModel returns non-null value\n/// take one synch scope.\nclass AtomicExpr : public Expr {\npublic:\n  enum AtomicOp {\n#define BUILTIN(ID, TYPE, ATTRS)\n#define ATOMIC_BUILTIN(ID, TYPE, ATTRS) AO ## ID,\n#include \"clang/Basic/Builtins.def\"\n    // Avoid trailing comma\n    BI_First = 0\n  };\n\nprivate:\n  /// Location of sub-expressions.\n  /// The location of Scope sub-expression is NumSubExprs - 1, which is\n  /// not fixed, therefore is not defined in enum.\n  enum { PTR, ORDER, VAL1, ORDER_FAIL, VAL2, WEAK, END_EXPR };\n  Stmt *SubExprs[END_EXPR + 1];\n  unsigned NumSubExprs;\n  SourceLocation BuiltinLoc, RParenLoc;\n  AtomicOp Op;\n\n  friend class ASTStmtReader;\npublic:\n  AtomicExpr(SourceLocation BLoc, ArrayRef<Expr*> args, QualType t,\n             AtomicOp op, SourceLocation RP);\n\n  /// Determine the number of arguments the specified atomic builtin\n  /// should have.\n  static unsigned getNumSubExprs(AtomicOp Op);\n\n  /// Build an empty AtomicExpr.\n  explicit AtomicExpr(EmptyShell Empty) : Expr(AtomicExprClass, Empty) { }\n\n  Expr *getPtr() const {\n    return cast<Expr>(SubExprs[PTR]);\n  }\n  Expr *getOrder() const {\n    return cast<Expr>(SubExprs[ORDER]);\n  }\n  Expr *getScope() const {\n    assert(getScopeModel() && \"No scope\");\n    return cast<Expr>(SubExprs[NumSubExprs - 1]);\n  }\n  Expr *getVal1() const {\n    if (Op == AO__c11_atomic_init || Op == AO__opencl_atomic_init)\n      return cast<Expr>(SubExprs[ORDER]);\n    assert(NumSubExprs > VAL1);\n    return cast<Expr>(SubExprs[VAL1]);\n  }\n  Expr *getOrderFail() const {\n    assert(NumSubExprs > ORDER_FAIL);\n    return cast<Expr>(SubExprs[ORDER_FAIL]);\n  }\n  Expr *getVal2() const {\n    if (Op == AO__atomic_exchange)\n      return cast<Expr>(SubExprs[ORDER_FAIL]);\n    assert(NumSubExprs > VAL2);\n    return cast<Expr>(SubExprs[VAL2]);\n  }\n  Expr *getWeak() const {\n    assert(NumSubExprs > WEAK);\n    return cast<Expr>(SubExprs[WEAK]);\n  }\n  QualType getValueType() const;\n\n  AtomicOp getOp() const { return Op; }\n  unsigned getNumSubExprs() const { return NumSubExprs; }\n\n  Expr **getSubExprs() { return reinterpret_cast<Expr **>(SubExprs); }\n  const Expr * const *getSubExprs() const {\n    return reinterpret_cast<Expr * const *>(SubExprs);\n  }\n\n  bool isVolatile() const {\n    return getPtr()->getType()->getPointeeType().isVolatileQualified();\n  }\n\n  bool isCmpXChg() const {\n    return getOp() == AO__c11_atomic_compare_exchange_strong ||\n           getOp() == AO__c11_atomic_compare_exchange_weak ||\n           getOp() == AO__opencl_atomic_compare_exchange_strong ||\n           getOp() == AO__opencl_atomic_compare_exchange_weak ||\n           getOp() == AO__atomic_compare_exchange ||\n           getOp() == AO__atomic_compare_exchange_n;\n  }\n\n  bool isOpenCL() const {\n    return getOp() >= AO__opencl_atomic_init &&\n           getOp() <= AO__opencl_atomic_fetch_max;\n  }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == AtomicExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(SubExprs, SubExprs+NumSubExprs);\n  }\n  const_child_range children() const {\n    return const_child_range(SubExprs, SubExprs + NumSubExprs);\n  }\n\n  /// Get atomic scope model for the atomic op code.\n  /// \\return empty atomic scope model if the atomic op code does not have\n  ///   scope operand.\n  static std::unique_ptr<AtomicScopeModel> getScopeModel(AtomicOp Op) {\n    auto Kind =\n        (Op >= AO__opencl_atomic_load && Op <= AO__opencl_atomic_fetch_max)\n            ? AtomicScopeModelKind::OpenCL\n            : AtomicScopeModelKind::None;\n    return AtomicScopeModel::create(Kind);\n  }\n\n  /// Get atomic scope model.\n  /// \\return empty atomic scope model if this atomic expression does not have\n  ///   scope operand.\n  std::unique_ptr<AtomicScopeModel> getScopeModel() const {\n    return getScopeModel(getOp());\n  }\n};\n\n/// TypoExpr - Internal placeholder for expressions where typo correction\n/// still needs to be performed and/or an error diagnostic emitted.\nclass TypoExpr : public Expr {\n  // The location for the typo name.\n  SourceLocation TypoLoc;\n\npublic:\n  TypoExpr(QualType T, SourceLocation TypoLoc)\n      : Expr(TypoExprClass, T, VK_LValue, OK_Ordinary), TypoLoc(TypoLoc) {\n    assert(T->isDependentType() && \"TypoExpr given a non-dependent type\");\n    setDependence(ExprDependence::TypeValueInstantiation |\n                  ExprDependence::Error);\n  }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return TypoLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return TypoLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == TypoExprClass;\n  }\n\n};\n\n/// Frontend produces RecoveryExprs on semantic errors that prevent creating\n/// other well-formed expressions. E.g. when type-checking of a binary operator\n/// fails, we cannot produce a BinaryOperator expression. Instead, we can choose\n/// to produce a recovery expression storing left and right operands.\n///\n/// RecoveryExpr does not have any semantic meaning in C++, it is only useful to\n/// preserve expressions in AST that would otherwise be dropped. It captures\n/// subexpressions of some expression that we could not construct and source\n/// range covered by the expression.\n///\n/// By default, RecoveryExpr uses dependence-bits to take advantage of existing\n/// machinery to deal with dependent code in C++, e.g. RecoveryExpr is preserved\n/// in `decltype(<broken-expr>)` as part of the `DependentDecltypeType`. In\n/// addition to that, clang does not report most errors on dependent\n/// expressions, so we get rid of bogus errors for free. However, note that\n/// unlike other dependent expressions, RecoveryExpr can be produced in\n/// non-template contexts.\n///\n/// We will preserve the type in RecoveryExpr when the type is known, e.g.\n/// preserving the return type for a broken non-overloaded function call, a\n/// overloaded call where all candidates have the same return type. In this\n/// case, the expression is not type-dependent (unless the known type is itself\n/// dependent)\n///\n/// One can also reliably suppress all bogus errors on expressions containing\n/// recovery expressions by examining results of Expr::containsErrors().\nclass RecoveryExpr final : public Expr,\n                           private llvm::TrailingObjects<RecoveryExpr, Expr *> {\npublic:\n  static RecoveryExpr *Create(ASTContext &Ctx, QualType T,\n                              SourceLocation BeginLoc, SourceLocation EndLoc,\n                              ArrayRef<Expr *> SubExprs);\n  static RecoveryExpr *CreateEmpty(ASTContext &Ctx, unsigned NumSubExprs);\n\n  ArrayRef<Expr *> subExpressions() {\n    auto *B = getTrailingObjects<Expr *>();\n    return llvm::makeArrayRef(B, B + NumExprs);\n  }\n\n  ArrayRef<const Expr *> subExpressions() const {\n    return const_cast<RecoveryExpr *>(this)->subExpressions();\n  }\n\n  child_range children() {\n    Stmt **B = reinterpret_cast<Stmt **>(getTrailingObjects<Expr *>());\n    return child_range(B, B + NumExprs);\n  }\n\n  SourceLocation getBeginLoc() const { return BeginLoc; }\n  SourceLocation getEndLoc() const { return EndLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == RecoveryExprClass;\n  }\n\nprivate:\n  RecoveryExpr(ASTContext &Ctx, QualType T, SourceLocation BeginLoc,\n               SourceLocation EndLoc, ArrayRef<Expr *> SubExprs);\n  RecoveryExpr(EmptyShell Empty, unsigned NumSubExprs)\n      : Expr(RecoveryExprClass, Empty), NumExprs(NumSubExprs) {}\n\n  size_t numTrailingObjects(OverloadToken<Stmt *>) const { return NumExprs; }\n\n  SourceLocation BeginLoc, EndLoc;\n  unsigned NumExprs;\n  friend TrailingObjects;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n} // end namespace clang\n\n#endif // LLVM_CLANG_AST_EXPR_H\n"}, "24": {"id": 24, "path": "/home/vsts/work/1/llvm-project/clang/include/clang/AST/Type.h", "content": "//===- Type.h - C Language Family Type Representation -----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file\n/// C Language Family Type Representation\n///\n/// This file defines the clang::Type interface and subclasses, used to\n/// represent types for languages in the C family.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_AST_TYPE_H\n#define LLVM_CLANG_AST_TYPE_H\n\n#include \"clang/AST/DependenceFlags.h\"\n#include \"clang/AST/NestedNameSpecifier.h\"\n#include \"clang/AST/TemplateName.h\"\n#include \"clang/Basic/AddressSpaces.h\"\n#include \"clang/Basic/AttrKinds.h\"\n#include \"clang/Basic/Diagnostic.h\"\n#include \"clang/Basic/ExceptionSpecificationType.h\"\n#include \"clang/Basic/LLVM.h\"\n#include \"clang/Basic/Linkage.h\"\n#include \"clang/Basic/PartialDiagnostic.h\"\n#include \"clang/Basic/SourceLocation.h\"\n#include \"clang/Basic/Specifiers.h\"\n#include \"clang/Basic/Visibility.h\"\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/APSInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/PointerIntPair.h\"\n#include \"llvm/ADT/PointerUnion.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/PointerLikeTypeTraits.h\"\n#include \"llvm/Support/TrailingObjects.h\"\n#include \"llvm/Support/type_traits.h\"\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <cstring>\n#include <string>\n#include <type_traits>\n#include <utility>\n\nnamespace clang {\n\nclass ExtQuals;\nclass QualType;\nclass ConceptDecl;\nclass TagDecl;\nclass TemplateParameterList;\nclass Type;\n\nenum {\n  TypeAlignmentInBits = 4,\n  TypeAlignment = 1 << TypeAlignmentInBits\n};\n\nnamespace serialization {\n  template <class T> class AbstractTypeReader;\n  template <class T> class AbstractTypeWriter;\n}\n\n} // namespace clang\n\nnamespace llvm {\n\n  template <typename T>\n  struct PointerLikeTypeTraits;\n  template<>\n  struct PointerLikeTypeTraits< ::clang::Type*> {\n    static inline void *getAsVoidPointer(::clang::Type *P) { return P; }\n\n    static inline ::clang::Type *getFromVoidPointer(void *P) {\n      return static_cast< ::clang::Type*>(P);\n    }\n\n    static constexpr int NumLowBitsAvailable = clang::TypeAlignmentInBits;\n  };\n\n  template<>\n  struct PointerLikeTypeTraits< ::clang::ExtQuals*> {\n    static inline void *getAsVoidPointer(::clang::ExtQuals *P) { return P; }\n\n    static inline ::clang::ExtQuals *getFromVoidPointer(void *P) {\n      return static_cast< ::clang::ExtQuals*>(P);\n    }\n\n    static constexpr int NumLowBitsAvailable = clang::TypeAlignmentInBits;\n  };\n\n} // namespace llvm\n\nnamespace clang {\n\nclass ASTContext;\ntemplate <typename> class CanQual;\nclass CXXRecordDecl;\nclass DeclContext;\nclass EnumDecl;\nclass Expr;\nclass ExtQualsTypeCommonBase;\nclass FunctionDecl;\nclass IdentifierInfo;\nclass NamedDecl;\nclass ObjCInterfaceDecl;\nclass ObjCProtocolDecl;\nclass ObjCTypeParamDecl;\nstruct PrintingPolicy;\nclass RecordDecl;\nclass Stmt;\nclass TagDecl;\nclass TemplateArgument;\nclass TemplateArgumentListInfo;\nclass TemplateArgumentLoc;\nclass TemplateTypeParmDecl;\nclass TypedefNameDecl;\nclass UnresolvedUsingTypenameDecl;\n\nusing CanQualType = CanQual<Type>;\n\n// Provide forward declarations for all of the *Type classes.\n#define TYPE(Class, Base) class Class##Type;\n#include \"clang/AST/TypeNodes.inc\"\n\n/// The collection of all-type qualifiers we support.\n/// Clang supports five independent qualifiers:\n/// * C99: const, volatile, and restrict\n/// * MS: __unaligned\n/// * Embedded C (TR18037): address spaces\n/// * Objective C: the GC attributes (none, weak, or strong)\nclass Qualifiers {\npublic:\n  enum TQ { // NOTE: These flags must be kept in sync with DeclSpec::TQ.\n    Const    = 0x1,\n    Restrict = 0x2,\n    Volatile = 0x4,\n    CVRMask = Const | Volatile | Restrict\n  };\n\n  enum GC {\n    GCNone = 0,\n    Weak,\n    Strong\n  };\n\n  enum ObjCLifetime {\n    /// There is no lifetime qualification on this type.\n    OCL_None,\n\n    /// This object can be modified without requiring retains or\n    /// releases.\n    OCL_ExplicitNone,\n\n    /// Assigning into this object requires the old value to be\n    /// released and the new value to be retained.  The timing of the\n    /// release of the old value is inexact: it may be moved to\n    /// immediately after the last known point where the value is\n    /// live.\n    OCL_Strong,\n\n    /// Reading or writing from this object requires a barrier call.\n    OCL_Weak,\n\n    /// Assigning into this object requires a lifetime extension.\n    OCL_Autoreleasing\n  };\n\n  enum {\n    /// The maximum supported address space number.\n    /// 23 bits should be enough for anyone.\n    MaxAddressSpace = 0x7fffffu,\n\n    /// The width of the \"fast\" qualifier mask.\n    FastWidth = 3,\n\n    /// The fast qualifier mask.\n    FastMask = (1 << FastWidth) - 1\n  };\n\n  /// Returns the common set of qualifiers while removing them from\n  /// the given sets.\n  static Qualifiers removeCommonQualifiers(Qualifiers &L, Qualifiers &R) {\n    // If both are only CVR-qualified, bit operations are sufficient.\n    if (!(L.Mask & ~CVRMask) && !(R.Mask & ~CVRMask)) {\n      Qualifiers Q;\n      Q.Mask = L.Mask & R.Mask;\n      L.Mask &= ~Q.Mask;\n      R.Mask &= ~Q.Mask;\n      return Q;\n    }\n\n    Qualifiers Q;\n    unsigned CommonCRV = L.getCVRQualifiers() & R.getCVRQualifiers();\n    Q.addCVRQualifiers(CommonCRV);\n    L.removeCVRQualifiers(CommonCRV);\n    R.removeCVRQualifiers(CommonCRV);\n\n    if (L.getObjCGCAttr() == R.getObjCGCAttr()) {\n      Q.setObjCGCAttr(L.getObjCGCAttr());\n      L.removeObjCGCAttr();\n      R.removeObjCGCAttr();\n    }\n\n    if (L.getObjCLifetime() == R.getObjCLifetime()) {\n      Q.setObjCLifetime(L.getObjCLifetime());\n      L.removeObjCLifetime();\n      R.removeObjCLifetime();\n    }\n\n    if (L.getAddressSpace() == R.getAddressSpace()) {\n      Q.setAddressSpace(L.getAddressSpace());\n      L.removeAddressSpace();\n      R.removeAddressSpace();\n    }\n    return Q;\n  }\n\n  static Qualifiers fromFastMask(unsigned Mask) {\n    Qualifiers Qs;\n    Qs.addFastQualifiers(Mask);\n    return Qs;\n  }\n\n  static Qualifiers fromCVRMask(unsigned CVR) {\n    Qualifiers Qs;\n    Qs.addCVRQualifiers(CVR);\n    return Qs;\n  }\n\n  static Qualifiers fromCVRUMask(unsigned CVRU) {\n    Qualifiers Qs;\n    Qs.addCVRUQualifiers(CVRU);\n    return Qs;\n  }\n\n  // Deserialize qualifiers from an opaque representation.\n  static Qualifiers fromOpaqueValue(unsigned opaque) {\n    Qualifiers Qs;\n    Qs.Mask = opaque;\n    return Qs;\n  }\n\n  // Serialize these qualifiers into an opaque representation.\n  unsigned getAsOpaqueValue() const {\n    return Mask;\n  }\n\n  bool hasConst() const { return Mask & Const; }\n  bool hasOnlyConst() const { return Mask == Const; }\n  void removeConst() { Mask &= ~Const; }\n  void addConst() { Mask |= Const; }\n\n  bool hasVolatile() const { return Mask & Volatile; }\n  bool hasOnlyVolatile() const { return Mask == Volatile; }\n  void removeVolatile() { Mask &= ~Volatile; }\n  void addVolatile() { Mask |= Volatile; }\n\n  bool hasRestrict() const { return Mask & Restrict; }\n  bool hasOnlyRestrict() const { return Mask == Restrict; }\n  void removeRestrict() { Mask &= ~Restrict; }\n  void addRestrict() { Mask |= Restrict; }\n\n  bool hasCVRQualifiers() const { return getCVRQualifiers(); }\n  unsigned getCVRQualifiers() const { return Mask & CVRMask; }\n  unsigned getCVRUQualifiers() const { return Mask & (CVRMask | UMask); }\n\n  void setCVRQualifiers(unsigned mask) {\n    assert(!(mask & ~CVRMask) && \"bitmask contains non-CVR bits\");\n    Mask = (Mask & ~CVRMask) | mask;\n  }\n  void removeCVRQualifiers(unsigned mask) {\n    assert(!(mask & ~CVRMask) && \"bitmask contains non-CVR bits\");\n    Mask &= ~mask;\n  }\n  void removeCVRQualifiers() {\n    removeCVRQualifiers(CVRMask);\n  }\n  void addCVRQualifiers(unsigned mask) {\n    assert(!(mask & ~CVRMask) && \"bitmask contains non-CVR bits\");\n    Mask |= mask;\n  }\n  void addCVRUQualifiers(unsigned mask) {\n    assert(!(mask & ~CVRMask & ~UMask) && \"bitmask contains non-CVRU bits\");\n    Mask |= mask;\n  }\n\n  bool hasUnaligned() const { return Mask & UMask; }\n  void setUnaligned(bool flag) {\n    Mask = (Mask & ~UMask) | (flag ? UMask : 0);\n  }\n  void removeUnaligned() { Mask &= ~UMask; }\n  void addUnaligned() { Mask |= UMask; }\n\n  bool hasObjCGCAttr() const { return Mask & GCAttrMask; }\n  GC getObjCGCAttr() const { return GC((Mask & GCAttrMask) >> GCAttrShift); }\n  void setObjCGCAttr(GC type) {\n    Mask = (Mask & ~GCAttrMask) | (type << GCAttrShift);\n  }\n  void removeObjCGCAttr() { setObjCGCAttr(GCNone); }\n  void addObjCGCAttr(GC type) {\n    assert(type);\n    setObjCGCAttr(type);\n  }\n  Qualifiers withoutObjCGCAttr() const {\n    Qualifiers qs = *this;\n    qs.removeObjCGCAttr();\n    return qs;\n  }\n  Qualifiers withoutObjCLifetime() const {\n    Qualifiers qs = *this;\n    qs.removeObjCLifetime();\n    return qs;\n  }\n  Qualifiers withoutAddressSpace() const {\n    Qualifiers qs = *this;\n    qs.removeAddressSpace();\n    return qs;\n  }\n\n  bool hasObjCLifetime() const { return Mask & LifetimeMask; }\n  ObjCLifetime getObjCLifetime() const {\n    return ObjCLifetime((Mask & LifetimeMask) >> LifetimeShift);\n  }\n  void setObjCLifetime(ObjCLifetime type) {\n    Mask = (Mask & ~LifetimeMask) | (type << LifetimeShift);\n  }\n  void removeObjCLifetime() { setObjCLifetime(OCL_None); }\n  void addObjCLifetime(ObjCLifetime type) {\n    assert(type);\n    assert(!hasObjCLifetime());\n    Mask |= (type << LifetimeShift);\n  }\n\n  /// True if the lifetime is neither None or ExplicitNone.\n  bool hasNonTrivialObjCLifetime() const {\n    ObjCLifetime lifetime = getObjCLifetime();\n    return (lifetime > OCL_ExplicitNone);\n  }\n\n  /// True if the lifetime is either strong or weak.\n  bool hasStrongOrWeakObjCLifetime() const {\n    ObjCLifetime lifetime = getObjCLifetime();\n    return (lifetime == OCL_Strong || lifetime == OCL_Weak);\n  }\n\n  bool hasAddressSpace() const { return Mask & AddressSpaceMask; }\n  LangAS getAddressSpace() const {\n    return static_cast<LangAS>(Mask >> AddressSpaceShift);\n  }\n  bool hasTargetSpecificAddressSpace() const {\n    return isTargetAddressSpace(getAddressSpace());\n  }\n  /// Get the address space attribute value to be printed by diagnostics.\n  unsigned getAddressSpaceAttributePrintValue() const {\n    auto Addr = getAddressSpace();\n    // This function is not supposed to be used with language specific\n    // address spaces. If that happens, the diagnostic message should consider\n    // printing the QualType instead of the address space value.\n    assert(Addr == LangAS::Default || hasTargetSpecificAddressSpace());\n    if (Addr != LangAS::Default)\n      return toTargetAddressSpace(Addr);\n    // TODO: The diagnostic messages where Addr may be 0 should be fixed\n    // since it cannot differentiate the situation where 0 denotes the default\n    // address space or user specified __attribute__((address_space(0))).\n    return 0;\n  }\n  void setAddressSpace(LangAS space) {\n    assert((unsigned)space <= MaxAddressSpace);\n    Mask = (Mask & ~AddressSpaceMask)\n         | (((uint32_t) space) << AddressSpaceShift);\n  }\n  void removeAddressSpace() { setAddressSpace(LangAS::Default); }\n  void addAddressSpace(LangAS space) {\n    assert(space != LangAS::Default);\n    setAddressSpace(space);\n  }\n\n  // Fast qualifiers are those that can be allocated directly\n  // on a QualType object.\n  bool hasFastQualifiers() const { return getFastQualifiers(); }\n  unsigned getFastQualifiers() const { return Mask & FastMask; }\n  void setFastQualifiers(unsigned mask) {\n    assert(!(mask & ~FastMask) && \"bitmask contains non-fast qualifier bits\");\n    Mask = (Mask & ~FastMask) | mask;\n  }\n  void removeFastQualifiers(unsigned mask) {\n    assert(!(mask & ~FastMask) && \"bitmask contains non-fast qualifier bits\");\n    Mask &= ~mask;\n  }\n  void removeFastQualifiers() {\n    removeFastQualifiers(FastMask);\n  }\n  void addFastQualifiers(unsigned mask) {\n    assert(!(mask & ~FastMask) && \"bitmask contains non-fast qualifier bits\");\n    Mask |= mask;\n  }\n\n  /// Return true if the set contains any qualifiers which require an ExtQuals\n  /// node to be allocated.\n  bool hasNonFastQualifiers() const { return Mask & ~FastMask; }\n  Qualifiers getNonFastQualifiers() const {\n    Qualifiers Quals = *this;\n    Quals.setFastQualifiers(0);\n    return Quals;\n  }\n\n  /// Return true if the set contains any qualifiers.\n  bool hasQualifiers() const { return Mask; }\n  bool empty() const { return !Mask; }\n\n  /// Add the qualifiers from the given set to this set.\n  void addQualifiers(Qualifiers Q) {\n    // If the other set doesn't have any non-boolean qualifiers, just\n    // bit-or it in.\n    if (!(Q.Mask & ~CVRMask))\n      Mask |= Q.Mask;\n    else {\n      Mask |= (Q.Mask & CVRMask);\n      if (Q.hasAddressSpace())\n        addAddressSpace(Q.getAddressSpace());\n      if (Q.hasObjCGCAttr())\n        addObjCGCAttr(Q.getObjCGCAttr());\n      if (Q.hasObjCLifetime())\n        addObjCLifetime(Q.getObjCLifetime());\n    }\n  }\n\n  /// Remove the qualifiers from the given set from this set.\n  void removeQualifiers(Qualifiers Q) {\n    // If the other set doesn't have any non-boolean qualifiers, just\n    // bit-and the inverse in.\n    if (!(Q.Mask & ~CVRMask))\n      Mask &= ~Q.Mask;\n    else {\n      Mask &= ~(Q.Mask & CVRMask);\n      if (getObjCGCAttr() == Q.getObjCGCAttr())\n        removeObjCGCAttr();\n      if (getObjCLifetime() == Q.getObjCLifetime())\n        removeObjCLifetime();\n      if (getAddressSpace() == Q.getAddressSpace())\n        removeAddressSpace();\n    }\n  }\n\n  /// Add the qualifiers from the given set to this set, given that\n  /// they don't conflict.\n  void addConsistentQualifiers(Qualifiers qs) {\n    assert(getAddressSpace() == qs.getAddressSpace() ||\n           !hasAddressSpace() || !qs.hasAddressSpace());\n    assert(getObjCGCAttr() == qs.getObjCGCAttr() ||\n           !hasObjCGCAttr() || !qs.hasObjCGCAttr());\n    assert(getObjCLifetime() == qs.getObjCLifetime() ||\n           !hasObjCLifetime() || !qs.hasObjCLifetime());\n    Mask |= qs.Mask;\n  }\n\n  /// Returns true if address space A is equal to or a superset of B.\n  /// OpenCL v2.0 defines conversion rules (OpenCLC v2.0 s6.5.5) and notion of\n  /// overlapping address spaces.\n  /// CL1.1 or CL1.2:\n  ///   every address space is a superset of itself.\n  /// CL2.0 adds:\n  ///   __generic is a superset of any address space except for __constant.\n  static bool isAddressSpaceSupersetOf(LangAS A, LangAS B) {\n    // Address spaces must match exactly.\n    return A == B ||\n           // Otherwise in OpenCLC v2.0 s6.5.5: every address space except\n           // for __constant can be used as __generic.\n           (A == LangAS::opencl_generic && B != LangAS::opencl_constant) ||\n           // We also define global_device and global_host address spaces,\n           // to distinguish global pointers allocated on host from pointers\n           // allocated on device, which are a subset of __global.\n           (A == LangAS::opencl_global && (B == LangAS::opencl_global_device ||\n                                           B == LangAS::opencl_global_host)) ||\n           // Consider pointer size address spaces to be equivalent to default.\n           ((isPtrSizeAddressSpace(A) || A == LangAS::Default) &&\n            (isPtrSizeAddressSpace(B) || B == LangAS::Default));\n  }\n\n  /// Returns true if the address space in these qualifiers is equal to or\n  /// a superset of the address space in the argument qualifiers.\n  bool isAddressSpaceSupersetOf(Qualifiers other) const {\n    return isAddressSpaceSupersetOf(getAddressSpace(), other.getAddressSpace());\n  }\n\n  /// Determines if these qualifiers compatibly include another set.\n  /// Generally this answers the question of whether an object with the other\n  /// qualifiers can be safely used as an object with these qualifiers.\n  bool compatiblyIncludes(Qualifiers other) const {\n    return isAddressSpaceSupersetOf(other) &&\n           // ObjC GC qualifiers can match, be added, or be removed, but can't\n           // be changed.\n           (getObjCGCAttr() == other.getObjCGCAttr() || !hasObjCGCAttr() ||\n            !other.hasObjCGCAttr()) &&\n           // ObjC lifetime qualifiers must match exactly.\n           getObjCLifetime() == other.getObjCLifetime() &&\n           // CVR qualifiers may subset.\n           (((Mask & CVRMask) | (other.Mask & CVRMask)) == (Mask & CVRMask)) &&\n           // U qualifier may superset.\n           (!other.hasUnaligned() || hasUnaligned());\n  }\n\n  /// Determines if these qualifiers compatibly include another set of\n  /// qualifiers from the narrow perspective of Objective-C ARC lifetime.\n  ///\n  /// One set of Objective-C lifetime qualifiers compatibly includes the other\n  /// if the lifetime qualifiers match, or if both are non-__weak and the\n  /// including set also contains the 'const' qualifier, or both are non-__weak\n  /// and one is None (which can only happen in non-ARC modes).\n  bool compatiblyIncludesObjCLifetime(Qualifiers other) const {\n    if (getObjCLifetime() == other.getObjCLifetime())\n      return true;\n\n    if (getObjCLifetime() == OCL_Weak || other.getObjCLifetime() == OCL_Weak)\n      return false;\n\n    if (getObjCLifetime() == OCL_None || other.getObjCLifetime() == OCL_None)\n      return true;\n\n    return hasConst();\n  }\n\n  /// Determine whether this set of qualifiers is a strict superset of\n  /// another set of qualifiers, not considering qualifier compatibility.\n  bool isStrictSupersetOf(Qualifiers Other) const;\n\n  bool operator==(Qualifiers Other) const { return Mask == Other.Mask; }\n  bool operator!=(Qualifiers Other) const { return Mask != Other.Mask; }\n\n  explicit operator bool() const { return hasQualifiers(); }\n\n  Qualifiers &operator+=(Qualifiers R) {\n    addQualifiers(R);\n    return *this;\n  }\n\n  // Union two qualifier sets.  If an enumerated qualifier appears\n  // in both sets, use the one from the right.\n  friend Qualifiers operator+(Qualifiers L, Qualifiers R) {\n    L += R;\n    return L;\n  }\n\n  Qualifiers &operator-=(Qualifiers R) {\n    removeQualifiers(R);\n    return *this;\n  }\n\n  /// Compute the difference between two qualifier sets.\n  friend Qualifiers operator-(Qualifiers L, Qualifiers R) {\n    L -= R;\n    return L;\n  }\n\n  std::string getAsString() const;\n  std::string getAsString(const PrintingPolicy &Policy) const;\n\n  static std::string getAddrSpaceAsString(LangAS AS);\n\n  bool isEmptyWhenPrinted(const PrintingPolicy &Policy) const;\n  void print(raw_ostream &OS, const PrintingPolicy &Policy,\n             bool appendSpaceIfNonEmpty = false) const;\n\n  void Profile(llvm::FoldingSetNodeID &ID) const {\n    ID.AddInteger(Mask);\n  }\n\nprivate:\n  // bits:     |0 1 2|3|4 .. 5|6  ..  8|9   ...   31|\n  //           |C R V|U|GCAttr|Lifetime|AddressSpace|\n  uint32_t Mask = 0;\n\n  static const uint32_t UMask = 0x8;\n  static const uint32_t UShift = 3;\n  static const uint32_t GCAttrMask = 0x30;\n  static const uint32_t GCAttrShift = 4;\n  static const uint32_t LifetimeMask = 0x1C0;\n  static const uint32_t LifetimeShift = 6;\n  static const uint32_t AddressSpaceMask =\n      ~(CVRMask | UMask | GCAttrMask | LifetimeMask);\n  static const uint32_t AddressSpaceShift = 9;\n};\n\n/// A std::pair-like structure for storing a qualified type split\n/// into its local qualifiers and its locally-unqualified type.\nstruct SplitQualType {\n  /// The locally-unqualified type.\n  const Type *Ty = nullptr;\n\n  /// The local qualifiers.\n  Qualifiers Quals;\n\n  SplitQualType() = default;\n  SplitQualType(const Type *ty, Qualifiers qs) : Ty(ty), Quals(qs) {}\n\n  SplitQualType getSingleStepDesugaredType() const; // end of this file\n\n  // Make std::tie work.\n  std::pair<const Type *,Qualifiers> asPair() const {\n    return std::pair<const Type *, Qualifiers>(Ty, Quals);\n  }\n\n  friend bool operator==(SplitQualType a, SplitQualType b) {\n    return a.Ty == b.Ty && a.Quals == b.Quals;\n  }\n  friend bool operator!=(SplitQualType a, SplitQualType b) {\n    return a.Ty != b.Ty || a.Quals != b.Quals;\n  }\n};\n\n/// The kind of type we are substituting Objective-C type arguments into.\n///\n/// The kind of substitution affects the replacement of type parameters when\n/// no concrete type information is provided, e.g., when dealing with an\n/// unspecialized type.\nenum class ObjCSubstitutionContext {\n  /// An ordinary type.\n  Ordinary,\n\n  /// The result type of a method or function.\n  Result,\n\n  /// The parameter type of a method or function.\n  Parameter,\n\n  /// The type of a property.\n  Property,\n\n  /// The superclass of a type.\n  Superclass,\n};\n\n/// A (possibly-)qualified type.\n///\n/// For efficiency, we don't store CV-qualified types as nodes on their\n/// own: instead each reference to a type stores the qualifiers.  This\n/// greatly reduces the number of nodes we need to allocate for types (for\n/// example we only need one for 'int', 'const int', 'volatile int',\n/// 'const volatile int', etc).\n///\n/// As an added efficiency bonus, instead of making this a pair, we\n/// just store the two bits we care about in the low bits of the\n/// pointer.  To handle the packing/unpacking, we make QualType be a\n/// simple wrapper class that acts like a smart pointer.  A third bit\n/// indicates whether there are extended qualifiers present, in which\n/// case the pointer points to a special structure.\nclass QualType {\n  friend class QualifierCollector;\n\n  // Thankfully, these are efficiently composable.\n  llvm::PointerIntPair<llvm::PointerUnion<const Type *, const ExtQuals *>,\n                       Qualifiers::FastWidth> Value;\n\n  const ExtQuals *getExtQualsUnsafe() const {\n    return Value.getPointer().get<const ExtQuals*>();\n  }\n\n  const Type *getTypePtrUnsafe() const {\n    return Value.getPointer().get<const Type*>();\n  }\n\n  const ExtQualsTypeCommonBase *getCommonPtr() const {\n    assert(!isNull() && \"Cannot retrieve a NULL type pointer\");\n    auto CommonPtrVal = reinterpret_cast<uintptr_t>(Value.getOpaqueValue());\n    CommonPtrVal &= ~(uintptr_t)((1 << TypeAlignmentInBits) - 1);\n    return reinterpret_cast<ExtQualsTypeCommonBase*>(CommonPtrVal);\n  }\n\npublic:\n  QualType() = default;\n  QualType(const Type *Ptr, unsigned Quals) : Value(Ptr, Quals) {}\n  QualType(const ExtQuals *Ptr, unsigned Quals) : Value(Ptr, Quals) {}\n\n  unsigned getLocalFastQualifiers() const { return Value.getInt(); }\n  void setLocalFastQualifiers(unsigned Quals) { Value.setInt(Quals); }\n\n  /// Retrieves a pointer to the underlying (unqualified) type.\n  ///\n  /// This function requires that the type not be NULL. If the type might be\n  /// NULL, use the (slightly less efficient) \\c getTypePtrOrNull().\n  const Type *getTypePtr() const;\n\n  const Type *getTypePtrOrNull() const;\n\n  /// Retrieves a pointer to the name of the base type.\n  const IdentifierInfo *getBaseTypeIdentifier() const;\n\n  /// Divides a QualType into its unqualified type and a set of local\n  /// qualifiers.\n  SplitQualType split() const;\n\n  void *getAsOpaquePtr() const { return Value.getOpaqueValue(); }\n\n  static QualType getFromOpaquePtr(const void *Ptr) {\n    QualType T;\n    T.Value.setFromOpaqueValue(const_cast<void*>(Ptr));\n    return T;\n  }\n\n  const Type &operator*() const {\n    return *getTypePtr();\n  }\n\n  const Type *operator->() const {\n    return getTypePtr();\n  }\n\n  bool isCanonical() const;\n  bool isCanonicalAsParam() const;\n\n  /// Return true if this QualType doesn't point to a type yet.\n  bool isNull() const {\n    return Value.getPointer().isNull();\n  }\n\n  /// Determine whether this particular QualType instance has the\n  /// \"const\" qualifier set, without looking through typedefs that may have\n  /// added \"const\" at a different level.\n  bool isLocalConstQualified() const {\n    return (getLocalFastQualifiers() & Qualifiers::Const);\n  }\n\n  /// Determine whether this type is const-qualified.\n  bool isConstQualified() const;\n\n  /// Determine whether this particular QualType instance has the\n  /// \"restrict\" qualifier set, without looking through typedefs that may have\n  /// added \"restrict\" at a different level.\n  bool isLocalRestrictQualified() const {\n    return (getLocalFastQualifiers() & Qualifiers::Restrict);\n  }\n\n  /// Determine whether this type is restrict-qualified.\n  bool isRestrictQualified() const;\n\n  /// Determine whether this particular QualType instance has the\n  /// \"volatile\" qualifier set, without looking through typedefs that may have\n  /// added \"volatile\" at a different level.\n  bool isLocalVolatileQualified() const {\n    return (getLocalFastQualifiers() & Qualifiers::Volatile);\n  }\n\n  /// Determine whether this type is volatile-qualified.\n  bool isVolatileQualified() const;\n\n  /// Determine whether this particular QualType instance has any\n  /// qualifiers, without looking through any typedefs that might add\n  /// qualifiers at a different level.\n  bool hasLocalQualifiers() const {\n    return getLocalFastQualifiers() || hasLocalNonFastQualifiers();\n  }\n\n  /// Determine whether this type has any qualifiers.\n  bool hasQualifiers() const;\n\n  /// Determine whether this particular QualType instance has any\n  /// \"non-fast\" qualifiers, e.g., those that are stored in an ExtQualType\n  /// instance.\n  bool hasLocalNonFastQualifiers() const {\n    return Value.getPointer().is<const ExtQuals*>();\n  }\n\n  /// Retrieve the set of qualifiers local to this particular QualType\n  /// instance, not including any qualifiers acquired through typedefs or\n  /// other sugar.\n  Qualifiers getLocalQualifiers() const;\n\n  /// Retrieve the set of qualifiers applied to this type.\n  Qualifiers getQualifiers() const;\n\n  /// Retrieve the set of CVR (const-volatile-restrict) qualifiers\n  /// local to this particular QualType instance, not including any qualifiers\n  /// acquired through typedefs or other sugar.\n  unsigned getLocalCVRQualifiers() const {\n    return getLocalFastQualifiers();\n  }\n\n  /// Retrieve the set of CVR (const-volatile-restrict) qualifiers\n  /// applied to this type.\n  unsigned getCVRQualifiers() const;\n\n  bool isConstant(const ASTContext& Ctx) const {\n    return QualType::isConstant(*this, Ctx);\n  }\n\n  /// Determine whether this is a Plain Old Data (POD) type (C++ 3.9p10).\n  bool isPODType(const ASTContext &Context) const;\n\n  /// Return true if this is a POD type according to the rules of the C++98\n  /// standard, regardless of the current compilation's language.\n  bool isCXX98PODType(const ASTContext &Context) const;\n\n  /// Return true if this is a POD type according to the more relaxed rules\n  /// of the C++11 standard, regardless of the current compilation's language.\n  /// (C++0x [basic.types]p9). Note that, unlike\n  /// CXXRecordDecl::isCXX11StandardLayout, this takes DRs into account.\n  bool isCXX11PODType(const ASTContext &Context) const;\n\n  /// Return true if this is a trivial type per (C++0x [basic.types]p9)\n  bool isTrivialType(const ASTContext &Context) const;\n\n  /// Return true if this is a trivially copyable type (C++0x [basic.types]p9)\n  bool isTriviallyCopyableType(const ASTContext &Context) const;\n\n\n  /// Returns true if it is a class and it might be dynamic.\n  bool mayBeDynamicClass() const;\n\n  /// Returns true if it is not a class or if the class might not be dynamic.\n  bool mayBeNotDynamicClass() const;\n\n  // Don't promise in the API that anything besides 'const' can be\n  // easily added.\n\n  /// Add the `const` type qualifier to this QualType.\n  void addConst() {\n    addFastQualifiers(Qualifiers::Const);\n  }\n  QualType withConst() const {\n    return withFastQualifiers(Qualifiers::Const);\n  }\n\n  /// Add the `volatile` type qualifier to this QualType.\n  void addVolatile() {\n    addFastQualifiers(Qualifiers::Volatile);\n  }\n  QualType withVolatile() const {\n    return withFastQualifiers(Qualifiers::Volatile);\n  }\n\n  /// Add the `restrict` qualifier to this QualType.\n  void addRestrict() {\n    addFastQualifiers(Qualifiers::Restrict);\n  }\n  QualType withRestrict() const {\n    return withFastQualifiers(Qualifiers::Restrict);\n  }\n\n  QualType withCVRQualifiers(unsigned CVR) const {\n    return withFastQualifiers(CVR);\n  }\n\n  void addFastQualifiers(unsigned TQs) {\n    assert(!(TQs & ~Qualifiers::FastMask)\n           && \"non-fast qualifier bits set in mask!\");\n    Value.setInt(Value.getInt() | TQs);\n  }\n\n  void removeLocalConst();\n  void removeLocalVolatile();\n  void removeLocalRestrict();\n  void removeLocalCVRQualifiers(unsigned Mask);\n\n  void removeLocalFastQualifiers() { Value.setInt(0); }\n  void removeLocalFastQualifiers(unsigned Mask) {\n    assert(!(Mask & ~Qualifiers::FastMask) && \"mask has non-fast qualifiers\");\n    Value.setInt(Value.getInt() & ~Mask);\n  }\n\n  // Creates a type with the given qualifiers in addition to any\n  // qualifiers already on this type.\n  QualType withFastQualifiers(unsigned TQs) const {\n    QualType T = *this;\n    T.addFastQualifiers(TQs);\n    return T;\n  }\n\n  // Creates a type with exactly the given fast qualifiers, removing\n  // any existing fast qualifiers.\n  QualType withExactLocalFastQualifiers(unsigned TQs) const {\n    return withoutLocalFastQualifiers().withFastQualifiers(TQs);\n  }\n\n  // Removes fast qualifiers, but leaves any extended qualifiers in place.\n  QualType withoutLocalFastQualifiers() const {\n    QualType T = *this;\n    T.removeLocalFastQualifiers();\n    return T;\n  }\n\n  QualType getCanonicalType() const;\n\n  /// Return this type with all of the instance-specific qualifiers\n  /// removed, but without removing any qualifiers that may have been applied\n  /// through typedefs.\n  QualType getLocalUnqualifiedType() const { return QualType(getTypePtr(), 0); }\n\n  /// Retrieve the unqualified variant of the given type,\n  /// removing as little sugar as possible.\n  ///\n  /// This routine looks through various kinds of sugar to find the\n  /// least-desugared type that is unqualified. For example, given:\n  ///\n  /// \\code\n  /// typedef int Integer;\n  /// typedef const Integer CInteger;\n  /// typedef CInteger DifferenceType;\n  /// \\endcode\n  ///\n  /// Executing \\c getUnqualifiedType() on the type \\c DifferenceType will\n  /// desugar until we hit the type \\c Integer, which has no qualifiers on it.\n  ///\n  /// The resulting type might still be qualified if it's sugar for an array\n  /// type.  To strip qualifiers even from within a sugared array type, use\n  /// ASTContext::getUnqualifiedArrayType.\n  inline QualType getUnqualifiedType() const;\n\n  /// Retrieve the unqualified variant of the given type, removing as little\n  /// sugar as possible.\n  ///\n  /// Like getUnqualifiedType(), but also returns the set of\n  /// qualifiers that were built up.\n  ///\n  /// The resulting type might still be qualified if it's sugar for an array\n  /// type.  To strip qualifiers even from within a sugared array type, use\n  /// ASTContext::getUnqualifiedArrayType.\n  inline SplitQualType getSplitUnqualifiedType() const;\n\n  /// Determine whether this type is more qualified than the other\n  /// given type, requiring exact equality for non-CVR qualifiers.\n  bool isMoreQualifiedThan(QualType Other) const;\n\n  /// Determine whether this type is at least as qualified as the other\n  /// given type, requiring exact equality for non-CVR qualifiers.\n  bool isAtLeastAsQualifiedAs(QualType Other) const;\n\n  QualType getNonReferenceType() const;\n\n  /// Determine the type of a (typically non-lvalue) expression with the\n  /// specified result type.\n  ///\n  /// This routine should be used for expressions for which the return type is\n  /// explicitly specified (e.g., in a cast or call) and isn't necessarily\n  /// an lvalue. It removes a top-level reference (since there are no\n  /// expressions of reference type) and deletes top-level cvr-qualifiers\n  /// from non-class types (in C++) or all types (in C).\n  QualType getNonLValueExprType(const ASTContext &Context) const;\n\n  /// Remove an outer pack expansion type (if any) from this type. Used as part\n  /// of converting the type of a declaration to the type of an expression that\n  /// references that expression. It's meaningless for an expression to have a\n  /// pack expansion type.\n  QualType getNonPackExpansionType() const;\n\n  /// Return the specified type with any \"sugar\" removed from\n  /// the type.  This takes off typedefs, typeof's etc.  If the outer level of\n  /// the type is already concrete, it returns it unmodified.  This is similar\n  /// to getting the canonical type, but it doesn't remove *all* typedefs.  For\n  /// example, it returns \"T*\" as \"T*\", (not as \"int*\"), because the pointer is\n  /// concrete.\n  ///\n  /// Qualifiers are left in place.\n  QualType getDesugaredType(const ASTContext &Context) const {\n    return getDesugaredType(*this, Context);\n  }\n\n  SplitQualType getSplitDesugaredType() const {\n    return getSplitDesugaredType(*this);\n  }\n\n  /// Return the specified type with one level of \"sugar\" removed from\n  /// the type.\n  ///\n  /// This routine takes off the first typedef, typeof, etc. If the outer level\n  /// of the type is already concrete, it returns it unmodified.\n  QualType getSingleStepDesugaredType(const ASTContext &Context) const {\n    return getSingleStepDesugaredTypeImpl(*this, Context);\n  }\n\n  /// Returns the specified type after dropping any\n  /// outer-level parentheses.\n  QualType IgnoreParens() const {\n    if (isa<ParenType>(*this))\n      return QualType::IgnoreParens(*this);\n    return *this;\n  }\n\n  /// Indicate whether the specified types and qualifiers are identical.\n  friend bool operator==(const QualType &LHS, const QualType &RHS) {\n    return LHS.Value == RHS.Value;\n  }\n  friend bool operator!=(const QualType &LHS, const QualType &RHS) {\n    return LHS.Value != RHS.Value;\n  }\n  friend bool operator<(const QualType &LHS, const QualType &RHS) {\n    return LHS.Value < RHS.Value;\n  }\n\n  static std::string getAsString(SplitQualType split,\n                                 const PrintingPolicy &Policy) {\n    return getAsString(split.Ty, split.Quals, Policy);\n  }\n  static std::string getAsString(const Type *ty, Qualifiers qs,\n                                 const PrintingPolicy &Policy);\n\n  std::string getAsString() const;\n  std::string getAsString(const PrintingPolicy &Policy) const;\n\n  void print(raw_ostream &OS, const PrintingPolicy &Policy,\n             const Twine &PlaceHolder = Twine(),\n             unsigned Indentation = 0) const;\n\n  static void print(SplitQualType split, raw_ostream &OS,\n                    const PrintingPolicy &policy, const Twine &PlaceHolder,\n                    unsigned Indentation = 0) {\n    return print(split.Ty, split.Quals, OS, policy, PlaceHolder, Indentation);\n  }\n\n  static void print(const Type *ty, Qualifiers qs,\n                    raw_ostream &OS, const PrintingPolicy &policy,\n                    const Twine &PlaceHolder,\n                    unsigned Indentation = 0);\n\n  void getAsStringInternal(std::string &Str,\n                           const PrintingPolicy &Policy) const;\n\n  static void getAsStringInternal(SplitQualType split, std::string &out,\n                                  const PrintingPolicy &policy) {\n    return getAsStringInternal(split.Ty, split.Quals, out, policy);\n  }\n\n  static void getAsStringInternal(const Type *ty, Qualifiers qs,\n                                  std::string &out,\n                                  const PrintingPolicy &policy);\n\n  class StreamedQualTypeHelper {\n    const QualType &T;\n    const PrintingPolicy &Policy;\n    const Twine &PlaceHolder;\n    unsigned Indentation;\n\n  public:\n    StreamedQualTypeHelper(const QualType &T, const PrintingPolicy &Policy,\n                           const Twine &PlaceHolder, unsigned Indentation)\n        : T(T), Policy(Policy), PlaceHolder(PlaceHolder),\n          Indentation(Indentation) {}\n\n    friend raw_ostream &operator<<(raw_ostream &OS,\n                                   const StreamedQualTypeHelper &SQT) {\n      SQT.T.print(OS, SQT.Policy, SQT.PlaceHolder, SQT.Indentation);\n      return OS;\n    }\n  };\n\n  StreamedQualTypeHelper stream(const PrintingPolicy &Policy,\n                                const Twine &PlaceHolder = Twine(),\n                                unsigned Indentation = 0) const {\n    return StreamedQualTypeHelper(*this, Policy, PlaceHolder, Indentation);\n  }\n\n  void dump(const char *s) const;\n  void dump() const;\n  void dump(llvm::raw_ostream &OS, const ASTContext &Context) const;\n\n  void Profile(llvm::FoldingSetNodeID &ID) const {\n    ID.AddPointer(getAsOpaquePtr());\n  }\n\n  /// Check if this type has any address space qualifier.\n  inline bool hasAddressSpace() const;\n\n  /// Return the address space of this type.\n  inline LangAS getAddressSpace() const;\n\n  /// Returns true if address space qualifiers overlap with T address space\n  /// qualifiers.\n  /// OpenCL C defines conversion rules for pointers to different address spaces\n  /// and notion of overlapping address spaces.\n  /// CL1.1 or CL1.2:\n  ///   address spaces overlap iff they are they same.\n  /// OpenCL C v2.0 s6.5.5 adds:\n  ///   __generic overlaps with any address space except for __constant.\n  bool isAddressSpaceOverlapping(QualType T) const {\n    Qualifiers Q = getQualifiers();\n    Qualifiers TQ = T.getQualifiers();\n    // Address spaces overlap if at least one of them is a superset of another\n    return Q.isAddressSpaceSupersetOf(TQ) || TQ.isAddressSpaceSupersetOf(Q);\n  }\n\n  /// Returns gc attribute of this type.\n  inline Qualifiers::GC getObjCGCAttr() const;\n\n  /// true when Type is objc's weak.\n  bool isObjCGCWeak() const {\n    return getObjCGCAttr() == Qualifiers::Weak;\n  }\n\n  /// true when Type is objc's strong.\n  bool isObjCGCStrong() const {\n    return getObjCGCAttr() == Qualifiers::Strong;\n  }\n\n  /// Returns lifetime attribute of this type.\n  Qualifiers::ObjCLifetime getObjCLifetime() const {\n    return getQualifiers().getObjCLifetime();\n  }\n\n  bool hasNonTrivialObjCLifetime() const {\n    return getQualifiers().hasNonTrivialObjCLifetime();\n  }\n\n  bool hasStrongOrWeakObjCLifetime() const {\n    return getQualifiers().hasStrongOrWeakObjCLifetime();\n  }\n\n  // true when Type is objc's weak and weak is enabled but ARC isn't.\n  bool isNonWeakInMRRWithObjCWeak(const ASTContext &Context) const;\n\n  enum PrimitiveDefaultInitializeKind {\n    /// The type does not fall into any of the following categories. Note that\n    /// this case is zero-valued so that values of this enum can be used as a\n    /// boolean condition for non-triviality.\n    PDIK_Trivial,\n\n    /// The type is an Objective-C retainable pointer type that is qualified\n    /// with the ARC __strong qualifier.\n    PDIK_ARCStrong,\n\n    /// The type is an Objective-C retainable pointer type that is qualified\n    /// with the ARC __weak qualifier.\n    PDIK_ARCWeak,\n\n    /// The type is a struct containing a field whose type is not PCK_Trivial.\n    PDIK_Struct\n  };\n\n  /// Functions to query basic properties of non-trivial C struct types.\n\n  /// Check if this is a non-trivial type that would cause a C struct\n  /// transitively containing this type to be non-trivial to default initialize\n  /// and return the kind.\n  PrimitiveDefaultInitializeKind\n  isNonTrivialToPrimitiveDefaultInitialize() const;\n\n  enum PrimitiveCopyKind {\n    /// The type does not fall into any of the following categories. Note that\n    /// this case is zero-valued so that values of this enum can be used as a\n    /// boolean condition for non-triviality.\n    PCK_Trivial,\n\n    /// The type would be trivial except that it is volatile-qualified. Types\n    /// that fall into one of the other non-trivial cases may additionally be\n    /// volatile-qualified.\n    PCK_VolatileTrivial,\n\n    /// The type is an Objective-C retainable pointer type that is qualified\n    /// with the ARC __strong qualifier.\n    PCK_ARCStrong,\n\n    /// The type is an Objective-C retainable pointer type that is qualified\n    /// with the ARC __weak qualifier.\n    PCK_ARCWeak,\n\n    /// The type is a struct containing a field whose type is neither\n    /// PCK_Trivial nor PCK_VolatileTrivial.\n    /// Note that a C++ struct type does not necessarily match this; C++ copying\n    /// semantics are too complex to express here, in part because they depend\n    /// on the exact constructor or assignment operator that is chosen by\n    /// overload resolution to do the copy.\n    PCK_Struct\n  };\n\n  /// Check if this is a non-trivial type that would cause a C struct\n  /// transitively containing this type to be non-trivial to copy and return the\n  /// kind.\n  PrimitiveCopyKind isNonTrivialToPrimitiveCopy() const;\n\n  /// Check if this is a non-trivial type that would cause a C struct\n  /// transitively containing this type to be non-trivial to destructively\n  /// move and return the kind. Destructive move in this context is a C++-style\n  /// move in which the source object is placed in a valid but unspecified state\n  /// after it is moved, as opposed to a truly destructive move in which the\n  /// source object is placed in an uninitialized state.\n  PrimitiveCopyKind isNonTrivialToPrimitiveDestructiveMove() const;\n\n  enum DestructionKind {\n    DK_none,\n    DK_cxx_destructor,\n    DK_objc_strong_lifetime,\n    DK_objc_weak_lifetime,\n    DK_nontrivial_c_struct\n  };\n\n  /// Returns a nonzero value if objects of this type require\n  /// non-trivial work to clean up after.  Non-zero because it's\n  /// conceivable that qualifiers (objc_gc(weak)?) could make\n  /// something require destruction.\n  DestructionKind isDestructedType() const {\n    return isDestructedTypeImpl(*this);\n  }\n\n  /// Check if this is or contains a C union that is non-trivial to\n  /// default-initialize, which is a union that has a member that is non-trivial\n  /// to default-initialize. If this returns true,\n  /// isNonTrivialToPrimitiveDefaultInitialize returns PDIK_Struct.\n  bool hasNonTrivialToPrimitiveDefaultInitializeCUnion() const;\n\n  /// Check if this is or contains a C union that is non-trivial to destruct,\n  /// which is a union that has a member that is non-trivial to destruct. If\n  /// this returns true, isDestructedType returns DK_nontrivial_c_struct.\n  bool hasNonTrivialToPrimitiveDestructCUnion() const;\n\n  /// Check if this is or contains a C union that is non-trivial to copy, which\n  /// is a union that has a member that is non-trivial to copy. If this returns\n  /// true, isNonTrivialToPrimitiveCopy returns PCK_Struct.\n  bool hasNonTrivialToPrimitiveCopyCUnion() const;\n\n  /// Determine whether expressions of the given type are forbidden\n  /// from being lvalues in C.\n  ///\n  /// The expression types that are forbidden to be lvalues are:\n  ///   - 'void', but not qualified void\n  ///   - function types\n  ///\n  /// The exact rule here is C99 6.3.2.1:\n  ///   An lvalue is an expression with an object type or an incomplete\n  ///   type other than void.\n  bool isCForbiddenLValueType() const;\n\n  /// Substitute type arguments for the Objective-C type parameters used in the\n  /// subject type.\n  ///\n  /// \\param ctx ASTContext in which the type exists.\n  ///\n  /// \\param typeArgs The type arguments that will be substituted for the\n  /// Objective-C type parameters in the subject type, which are generally\n  /// computed via \\c Type::getObjCSubstitutions. If empty, the type\n  /// parameters will be replaced with their bounds or id/Class, as appropriate\n  /// for the context.\n  ///\n  /// \\param context The context in which the subject type was written.\n  ///\n  /// \\returns the resulting type.\n  QualType substObjCTypeArgs(ASTContext &ctx,\n                             ArrayRef<QualType> typeArgs,\n                             ObjCSubstitutionContext context) const;\n\n  /// Substitute type arguments from an object type for the Objective-C type\n  /// parameters used in the subject type.\n  ///\n  /// This operation combines the computation of type arguments for\n  /// substitution (\\c Type::getObjCSubstitutions) with the actual process of\n  /// substitution (\\c QualType::substObjCTypeArgs) for the convenience of\n  /// callers that need to perform a single substitution in isolation.\n  ///\n  /// \\param objectType The type of the object whose member type we're\n  /// substituting into. For example, this might be the receiver of a message\n  /// or the base of a property access.\n  ///\n  /// \\param dc The declaration context from which the subject type was\n  /// retrieved, which indicates (for example) which type parameters should\n  /// be substituted.\n  ///\n  /// \\param context The context in which the subject type was written.\n  ///\n  /// \\returns the subject type after replacing all of the Objective-C type\n  /// parameters with their corresponding arguments.\n  QualType substObjCMemberType(QualType objectType,\n                               const DeclContext *dc,\n                               ObjCSubstitutionContext context) const;\n\n  /// Strip Objective-C \"__kindof\" types from the given type.\n  QualType stripObjCKindOfType(const ASTContext &ctx) const;\n\n  /// Remove all qualifiers including _Atomic.\n  QualType getAtomicUnqualifiedType() const;\n\nprivate:\n  // These methods are implemented in a separate translation unit;\n  // \"static\"-ize them to avoid creating temporary QualTypes in the\n  // caller.\n  static bool isConstant(QualType T, const ASTContext& Ctx);\n  static QualType getDesugaredType(QualType T, const ASTContext &Context);\n  static SplitQualType getSplitDesugaredType(QualType T);\n  static SplitQualType getSplitUnqualifiedTypeImpl(QualType type);\n  static QualType getSingleStepDesugaredTypeImpl(QualType type,\n                                                 const ASTContext &C);\n  static QualType IgnoreParens(QualType T);\n  static DestructionKind isDestructedTypeImpl(QualType type);\n\n  /// Check if \\param RD is or contains a non-trivial C union.\n  static bool hasNonTrivialToPrimitiveDefaultInitializeCUnion(const RecordDecl *RD);\n  static bool hasNonTrivialToPrimitiveDestructCUnion(const RecordDecl *RD);\n  static bool hasNonTrivialToPrimitiveCopyCUnion(const RecordDecl *RD);\n};\n\n} // namespace clang\n\nnamespace llvm {\n\n/// Implement simplify_type for QualType, so that we can dyn_cast from QualType\n/// to a specific Type class.\ntemplate<> struct simplify_type< ::clang::QualType> {\n  using SimpleType = const ::clang::Type *;\n\n  static SimpleType getSimplifiedValue(::clang::QualType Val) {\n    return Val.getTypePtr();\n  }\n};\n\n// Teach SmallPtrSet that QualType is \"basically a pointer\".\ntemplate<>\nstruct PointerLikeTypeTraits<clang::QualType> {\n  static inline void *getAsVoidPointer(clang::QualType P) {\n    return P.getAsOpaquePtr();\n  }\n\n  static inline clang::QualType getFromVoidPointer(void *P) {\n    return clang::QualType::getFromOpaquePtr(P);\n  }\n\n  // Various qualifiers go in low bits.\n  static constexpr int NumLowBitsAvailable = 0;\n};\n\n} // namespace llvm\n\nnamespace clang {\n\n/// Base class that is common to both the \\c ExtQuals and \\c Type\n/// classes, which allows \\c QualType to access the common fields between the\n/// two.\nclass ExtQualsTypeCommonBase {\n  friend class ExtQuals;\n  friend class QualType;\n  friend class Type;\n\n  /// The \"base\" type of an extended qualifiers type (\\c ExtQuals) or\n  /// a self-referential pointer (for \\c Type).\n  ///\n  /// This pointer allows an efficient mapping from a QualType to its\n  /// underlying type pointer.\n  const Type *const BaseType;\n\n  /// The canonical type of this type.  A QualType.\n  QualType CanonicalType;\n\n  ExtQualsTypeCommonBase(const Type *baseType, QualType canon)\n      : BaseType(baseType), CanonicalType(canon) {}\n};\n\n/// We can encode up to four bits in the low bits of a\n/// type pointer, but there are many more type qualifiers that we want\n/// to be able to apply to an arbitrary type.  Therefore we have this\n/// struct, intended to be heap-allocated and used by QualType to\n/// store qualifiers.\n///\n/// The current design tags the 'const', 'restrict', and 'volatile' qualifiers\n/// in three low bits on the QualType pointer; a fourth bit records whether\n/// the pointer is an ExtQuals node. The extended qualifiers (address spaces,\n/// Objective-C GC attributes) are much more rare.\nclass ExtQuals : public ExtQualsTypeCommonBase, public llvm::FoldingSetNode {\n  // NOTE: changing the fast qualifiers should be straightforward as\n  // long as you don't make 'const' non-fast.\n  // 1. Qualifiers:\n  //    a) Modify the bitmasks (Qualifiers::TQ and DeclSpec::TQ).\n  //       Fast qualifiers must occupy the low-order bits.\n  //    b) Update Qualifiers::FastWidth and FastMask.\n  // 2. QualType:\n  //    a) Update is{Volatile,Restrict}Qualified(), defined inline.\n  //    b) Update remove{Volatile,Restrict}, defined near the end of\n  //       this header.\n  // 3. ASTContext:\n  //    a) Update get{Volatile,Restrict}Type.\n\n  /// The immutable set of qualifiers applied by this node. Always contains\n  /// extended qualifiers.\n  Qualifiers Quals;\n\n  ExtQuals *this_() { return this; }\n\npublic:\n  ExtQuals(const Type *baseType, QualType canon, Qualifiers quals)\n      : ExtQualsTypeCommonBase(baseType,\n                               canon.isNull() ? QualType(this_(), 0) : canon),\n        Quals(quals) {\n    assert(Quals.hasNonFastQualifiers()\n           && \"ExtQuals created with no fast qualifiers\");\n    assert(!Quals.hasFastQualifiers()\n           && \"ExtQuals created with fast qualifiers\");\n  }\n\n  Qualifiers getQualifiers() const { return Quals; }\n\n  bool hasObjCGCAttr() const { return Quals.hasObjCGCAttr(); }\n  Qualifiers::GC getObjCGCAttr() const { return Quals.getObjCGCAttr(); }\n\n  bool hasObjCLifetime() const { return Quals.hasObjCLifetime(); }\n  Qualifiers::ObjCLifetime getObjCLifetime() const {\n    return Quals.getObjCLifetime();\n  }\n\n  bool hasAddressSpace() const { return Quals.hasAddressSpace(); }\n  LangAS getAddressSpace() const { return Quals.getAddressSpace(); }\n\n  const Type *getBaseType() const { return BaseType; }\n\npublic:\n  void Profile(llvm::FoldingSetNodeID &ID) const {\n    Profile(ID, getBaseType(), Quals);\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      const Type *BaseType,\n                      Qualifiers Quals) {\n    assert(!Quals.hasFastQualifiers() && \"fast qualifiers in ExtQuals hash!\");\n    ID.AddPointer(BaseType);\n    Quals.Profile(ID);\n  }\n};\n\n/// The kind of C++11 ref-qualifier associated with a function type.\n/// This determines whether a member function's \"this\" object can be an\n/// lvalue, rvalue, or neither.\nenum RefQualifierKind {\n  /// No ref-qualifier was provided.\n  RQ_None = 0,\n\n  /// An lvalue ref-qualifier was provided (\\c &).\n  RQ_LValue,\n\n  /// An rvalue ref-qualifier was provided (\\c &&).\n  RQ_RValue\n};\n\n/// Which keyword(s) were used to create an AutoType.\nenum class AutoTypeKeyword {\n  /// auto\n  Auto,\n\n  /// decltype(auto)\n  DecltypeAuto,\n\n  /// __auto_type (GNU extension)\n  GNUAutoType\n};\n\n/// The base class of the type hierarchy.\n///\n/// A central concept with types is that each type always has a canonical\n/// type.  A canonical type is the type with any typedef names stripped out\n/// of it or the types it references.  For example, consider:\n///\n///  typedef int  foo;\n///  typedef foo* bar;\n///    'int *'    'foo *'    'bar'\n///\n/// There will be a Type object created for 'int'.  Since int is canonical, its\n/// CanonicalType pointer points to itself.  There is also a Type for 'foo' (a\n/// TypedefType).  Its CanonicalType pointer points to the 'int' Type.  Next\n/// there is a PointerType that represents 'int*', which, like 'int', is\n/// canonical.  Finally, there is a PointerType type for 'foo*' whose canonical\n/// type is 'int*', and there is a TypedefType for 'bar', whose canonical type\n/// is also 'int*'.\n///\n/// Non-canonical types are useful for emitting diagnostics, without losing\n/// information about typedefs being used.  Canonical types are useful for type\n/// comparisons (they allow by-pointer equality tests) and useful for reasoning\n/// about whether something has a particular form (e.g. is a function type),\n/// because they implicitly, recursively, strip all typedefs out of a type.\n///\n/// Types, once created, are immutable.\n///\nclass alignas(8) Type : public ExtQualsTypeCommonBase {\npublic:\n  enum TypeClass {\n#define TYPE(Class, Base) Class,\n#define LAST_TYPE(Class) TypeLast = Class\n#define ABSTRACT_TYPE(Class, Base)\n#include \"clang/AST/TypeNodes.inc\"\n  };\n\nprivate:\n  /// Bitfields required by the Type class.\n  class TypeBitfields {\n    friend class Type;\n    template <class T> friend class TypePropertyCache;\n\n    /// TypeClass bitfield - Enum that specifies what subclass this belongs to.\n    unsigned TC : 8;\n\n    /// Store information on the type dependency.\n    unsigned Dependence : llvm::BitWidth<TypeDependence>;\n\n    /// True if the cache (i.e. the bitfields here starting with\n    /// 'Cache') is valid.\n    mutable unsigned CacheValid : 1;\n\n    /// Linkage of this type.\n    mutable unsigned CachedLinkage : 3;\n\n    /// Whether this type involves and local or unnamed types.\n    mutable unsigned CachedLocalOrUnnamed : 1;\n\n    /// Whether this type comes from an AST file.\n    mutable unsigned FromAST : 1;\n\n    bool isCacheValid() const {\n      return CacheValid;\n    }\n\n    Linkage getLinkage() const {\n      assert(isCacheValid() && \"getting linkage from invalid cache\");\n      return static_cast<Linkage>(CachedLinkage);\n    }\n\n    bool hasLocalOrUnnamedType() const {\n      assert(isCacheValid() && \"getting linkage from invalid cache\");\n      return CachedLocalOrUnnamed;\n    }\n  };\n  enum { NumTypeBits = 8 + llvm::BitWidth<TypeDependence> + 6 };\n\nprotected:\n  // These classes allow subclasses to somewhat cleanly pack bitfields\n  // into Type.\n\n  class ArrayTypeBitfields {\n    friend class ArrayType;\n\n    unsigned : NumTypeBits;\n\n    /// CVR qualifiers from declarations like\n    /// 'int X[static restrict 4]'. For function parameters only.\n    unsigned IndexTypeQuals : 3;\n\n    /// Storage class qualifiers from declarations like\n    /// 'int X[static restrict 4]'. For function parameters only.\n    /// Actually an ArrayType::ArraySizeModifier.\n    unsigned SizeModifier : 3;\n  };\n\n  class ConstantArrayTypeBitfields {\n    friend class ConstantArrayType;\n\n    unsigned : NumTypeBits + 3 + 3;\n\n    /// Whether we have a stored size expression.\n    unsigned HasStoredSizeExpr : 1;\n  };\n\n  class BuiltinTypeBitfields {\n    friend class BuiltinType;\n\n    unsigned : NumTypeBits;\n\n    /// The kind (BuiltinType::Kind) of builtin type this is.\n    unsigned Kind : 8;\n  };\n\n  /// FunctionTypeBitfields store various bits belonging to FunctionProtoType.\n  /// Only common bits are stored here. Additional uncommon bits are stored\n  /// in a trailing object after FunctionProtoType.\n  class FunctionTypeBitfields {\n    friend class FunctionProtoType;\n    friend class FunctionType;\n\n    unsigned : NumTypeBits;\n\n    /// Extra information which affects how the function is called, like\n    /// regparm and the calling convention.\n    unsigned ExtInfo : 13;\n\n    /// The ref-qualifier associated with a \\c FunctionProtoType.\n    ///\n    /// This is a value of type \\c RefQualifierKind.\n    unsigned RefQualifier : 2;\n\n    /// Used only by FunctionProtoType, put here to pack with the\n    /// other bitfields.\n    /// The qualifiers are part of FunctionProtoType because...\n    ///\n    /// C++ 8.3.5p4: The return type, the parameter type list and the\n    /// cv-qualifier-seq, [...], are part of the function type.\n    unsigned FastTypeQuals : Qualifiers::FastWidth;\n    /// Whether this function has extended Qualifiers.\n    unsigned HasExtQuals : 1;\n\n    /// The number of parameters this function has, not counting '...'.\n    /// According to [implimits] 8 bits should be enough here but this is\n    /// somewhat easy to exceed with metaprogramming and so we would like to\n    /// keep NumParams as wide as reasonably possible.\n    unsigned NumParams : 16;\n\n    /// The type of exception specification this function has.\n    unsigned ExceptionSpecType : 4;\n\n    /// Whether this function has extended parameter information.\n    unsigned HasExtParameterInfos : 1;\n\n    /// Whether the function is variadic.\n    unsigned Variadic : 1;\n\n    /// Whether this function has a trailing return type.\n    unsigned HasTrailingReturn : 1;\n  };\n\n  class ObjCObjectTypeBitfields {\n    friend class ObjCObjectType;\n\n    unsigned : NumTypeBits;\n\n    /// The number of type arguments stored directly on this object type.\n    unsigned NumTypeArgs : 7;\n\n    /// The number of protocols stored directly on this object type.\n    unsigned NumProtocols : 6;\n\n    /// Whether this is a \"kindof\" type.\n    unsigned IsKindOf : 1;\n  };\n\n  class ReferenceTypeBitfields {\n    friend class ReferenceType;\n\n    unsigned : NumTypeBits;\n\n    /// True if the type was originally spelled with an lvalue sigil.\n    /// This is never true of rvalue references but can also be false\n    /// on lvalue references because of C++0x [dcl.typedef]p9,\n    /// as follows:\n    ///\n    ///   typedef int &ref;    // lvalue, spelled lvalue\n    ///   typedef int &&rvref; // rvalue\n    ///   ref &a;              // lvalue, inner ref, spelled lvalue\n    ///   ref &&a;             // lvalue, inner ref\n    ///   rvref &a;            // lvalue, inner ref, spelled lvalue\n    ///   rvref &&a;           // rvalue, inner ref\n    unsigned SpelledAsLValue : 1;\n\n    /// True if the inner type is a reference type.  This only happens\n    /// in non-canonical forms.\n    unsigned InnerRef : 1;\n  };\n\n  class TypeWithKeywordBitfields {\n    friend class TypeWithKeyword;\n\n    unsigned : NumTypeBits;\n\n    /// An ElaboratedTypeKeyword.  8 bits for efficient access.\n    unsigned Keyword : 8;\n  };\n\n  enum { NumTypeWithKeywordBits = 8 };\n\n  class ElaboratedTypeBitfields {\n    friend class ElaboratedType;\n\n    unsigned : NumTypeBits;\n    unsigned : NumTypeWithKeywordBits;\n\n    /// Whether the ElaboratedType has a trailing OwnedTagDecl.\n    unsigned HasOwnedTagDecl : 1;\n  };\n\n  class VectorTypeBitfields {\n    friend class VectorType;\n    friend class DependentVectorType;\n\n    unsigned : NumTypeBits;\n\n    /// The kind of vector, either a generic vector type or some\n    /// target-specific vector type such as for AltiVec or Neon.\n    unsigned VecKind : 3;\n    /// The number of elements in the vector.\n    uint32_t NumElements;\n  };\n\n  class AttributedTypeBitfields {\n    friend class AttributedType;\n\n    unsigned : NumTypeBits;\n\n    /// An AttributedType::Kind\n    unsigned AttrKind : 32 - NumTypeBits;\n  };\n\n  class AutoTypeBitfields {\n    friend class AutoType;\n\n    unsigned : NumTypeBits;\n\n    /// Was this placeholder type spelled as 'auto', 'decltype(auto)',\n    /// or '__auto_type'?  AutoTypeKeyword value.\n    unsigned Keyword : 2;\n\n    /// The number of template arguments in the type-constraints, which is\n    /// expected to be able to hold at least 1024 according to [implimits].\n    /// However as this limit is somewhat easy to hit with template\n    /// metaprogramming we'd prefer to keep it as large as possible.\n    /// At the moment it has been left as a non-bitfield since this type\n    /// safely fits in 64 bits as an unsigned, so there is no reason to\n    /// introduce the performance impact of a bitfield.\n    unsigned NumArgs;\n  };\n\n  class SubstTemplateTypeParmPackTypeBitfields {\n    friend class SubstTemplateTypeParmPackType;\n\n    unsigned : NumTypeBits;\n\n    /// The number of template arguments in \\c Arguments, which is\n    /// expected to be able to hold at least 1024 according to [implimits].\n    /// However as this limit is somewhat easy to hit with template\n    /// metaprogramming we'd prefer to keep it as large as possible.\n    /// At the moment it has been left as a non-bitfield since this type\n    /// safely fits in 64 bits as an unsigned, so there is no reason to\n    /// introduce the performance impact of a bitfield.\n    unsigned NumArgs;\n  };\n\n  class TemplateSpecializationTypeBitfields {\n    friend class TemplateSpecializationType;\n\n    unsigned : NumTypeBits;\n\n    /// Whether this template specialization type is a substituted type alias.\n    unsigned TypeAlias : 1;\n\n    /// The number of template arguments named in this class template\n    /// specialization, which is expected to be able to hold at least 1024\n    /// according to [implimits]. However, as this limit is somewhat easy to\n    /// hit with template metaprogramming we'd prefer to keep it as large\n    /// as possible. At the moment it has been left as a non-bitfield since\n    /// this type safely fits in 64 bits as an unsigned, so there is no reason\n    /// to introduce the performance impact of a bitfield.\n    unsigned NumArgs;\n  };\n\n  class DependentTemplateSpecializationTypeBitfields {\n    friend class DependentTemplateSpecializationType;\n\n    unsigned : NumTypeBits;\n    unsigned : NumTypeWithKeywordBits;\n\n    /// The number of template arguments named in this class template\n    /// specialization, which is expected to be able to hold at least 1024\n    /// according to [implimits]. However, as this limit is somewhat easy to\n    /// hit with template metaprogramming we'd prefer to keep it as large\n    /// as possible. At the moment it has been left as a non-bitfield since\n    /// this type safely fits in 64 bits as an unsigned, so there is no reason\n    /// to introduce the performance impact of a bitfield.\n    unsigned NumArgs;\n  };\n\n  class PackExpansionTypeBitfields {\n    friend class PackExpansionType;\n\n    unsigned : NumTypeBits;\n\n    /// The number of expansions that this pack expansion will\n    /// generate when substituted (+1), which is expected to be able to\n    /// hold at least 1024 according to [implimits]. However, as this limit\n    /// is somewhat easy to hit with template metaprogramming we'd prefer to\n    /// keep it as large as possible. At the moment it has been left as a\n    /// non-bitfield since this type safely fits in 64 bits as an unsigned, so\n    /// there is no reason to introduce the performance impact of a bitfield.\n    ///\n    /// This field will only have a non-zero value when some of the parameter\n    /// packs that occur within the pattern have been substituted but others\n    /// have not.\n    unsigned NumExpansions;\n  };\n\n  union {\n    TypeBitfields TypeBits;\n    ArrayTypeBitfields ArrayTypeBits;\n    ConstantArrayTypeBitfields ConstantArrayTypeBits;\n    AttributedTypeBitfields AttributedTypeBits;\n    AutoTypeBitfields AutoTypeBits;\n    BuiltinTypeBitfields BuiltinTypeBits;\n    FunctionTypeBitfields FunctionTypeBits;\n    ObjCObjectTypeBitfields ObjCObjectTypeBits;\n    ReferenceTypeBitfields ReferenceTypeBits;\n    TypeWithKeywordBitfields TypeWithKeywordBits;\n    ElaboratedTypeBitfields ElaboratedTypeBits;\n    VectorTypeBitfields VectorTypeBits;\n    SubstTemplateTypeParmPackTypeBitfields SubstTemplateTypeParmPackTypeBits;\n    TemplateSpecializationTypeBitfields TemplateSpecializationTypeBits;\n    DependentTemplateSpecializationTypeBitfields\n      DependentTemplateSpecializationTypeBits;\n    PackExpansionTypeBitfields PackExpansionTypeBits;\n  };\n\nprivate:\n  template <class T> friend class TypePropertyCache;\n\n  /// Set whether this type comes from an AST file.\n  void setFromAST(bool V = true) const {\n    TypeBits.FromAST = V;\n  }\n\nprotected:\n  friend class ASTContext;\n\n  Type(TypeClass tc, QualType canon, TypeDependence Dependence)\n      : ExtQualsTypeCommonBase(this,\n                               canon.isNull() ? QualType(this_(), 0) : canon) {\n    static_assert(sizeof(*this) <= 8 + sizeof(ExtQualsTypeCommonBase),\n                  \"changing bitfields changed sizeof(Type)!\");\n    static_assert(alignof(decltype(*this)) % sizeof(void *) == 0,\n                  \"Insufficient alignment!\");\n    TypeBits.TC = tc;\n    TypeBits.Dependence = static_cast<unsigned>(Dependence);\n    TypeBits.CacheValid = false;\n    TypeBits.CachedLocalOrUnnamed = false;\n    TypeBits.CachedLinkage = NoLinkage;\n    TypeBits.FromAST = false;\n  }\n\n  // silence VC++ warning C4355: 'this' : used in base member initializer list\n  Type *this_() { return this; }\n\n  void setDependence(TypeDependence D) {\n    TypeBits.Dependence = static_cast<unsigned>(D);\n  }\n\n  void addDependence(TypeDependence D) { setDependence(getDependence() | D); }\n\npublic:\n  friend class ASTReader;\n  friend class ASTWriter;\n  template <class T> friend class serialization::AbstractTypeReader;\n  template <class T> friend class serialization::AbstractTypeWriter;\n\n  Type(const Type &) = delete;\n  Type(Type &&) = delete;\n  Type &operator=(const Type &) = delete;\n  Type &operator=(Type &&) = delete;\n\n  TypeClass getTypeClass() const { return static_cast<TypeClass>(TypeBits.TC); }\n\n  /// Whether this type comes from an AST file.\n  bool isFromAST() const { return TypeBits.FromAST; }\n\n  /// Whether this type is or contains an unexpanded parameter\n  /// pack, used to support C++0x variadic templates.\n  ///\n  /// A type that contains a parameter pack shall be expanded by the\n  /// ellipsis operator at some point. For example, the typedef in the\n  /// following example contains an unexpanded parameter pack 'T':\n  ///\n  /// \\code\n  /// template<typename ...T>\n  /// struct X {\n  ///   typedef T* pointer_types; // ill-formed; T is a parameter pack.\n  /// };\n  /// \\endcode\n  ///\n  /// Note that this routine does not specify which\n  bool containsUnexpandedParameterPack() const {\n    return getDependence() & TypeDependence::UnexpandedPack;\n  }\n\n  /// Determines if this type would be canonical if it had no further\n  /// qualification.\n  bool isCanonicalUnqualified() const {\n    return CanonicalType == QualType(this, 0);\n  }\n\n  /// Pull a single level of sugar off of this locally-unqualified type.\n  /// Users should generally prefer SplitQualType::getSingleStepDesugaredType()\n  /// or QualType::getSingleStepDesugaredType(const ASTContext&).\n  QualType getLocallyUnqualifiedSingleStepDesugaredType() const;\n\n  /// As an extension, we classify types as one of \"sized\" or \"sizeless\";\n  /// every type is one or the other.  Standard types are all sized;\n  /// sizeless types are purely an extension.\n  ///\n  /// Sizeless types contain data with no specified size, alignment,\n  /// or layout.\n  bool isSizelessType() const;\n  bool isSizelessBuiltinType() const;\n\n  /// Determines if this is a sizeless type supported by the\n  /// 'arm_sve_vector_bits' type attribute, which can be applied to a single\n  /// SVE vector or predicate, excluding tuple types such as svint32x4_t.\n  bool isVLSTBuiltinType() const;\n\n  /// Returns the representative type for the element of an SVE builtin type.\n  /// This is used to represent fixed-length SVE vectors created with the\n  /// 'arm_sve_vector_bits' type attribute as VectorType.\n  QualType getSveEltType(const ASTContext &Ctx) const;\n\n  /// Types are partitioned into 3 broad categories (C99 6.2.5p1):\n  /// object types, function types, and incomplete types.\n\n  /// Return true if this is an incomplete type.\n  /// A type that can describe objects, but which lacks information needed to\n  /// determine its size (e.g. void, or a fwd declared struct). Clients of this\n  /// routine will need to determine if the size is actually required.\n  ///\n  /// Def If non-null, and the type refers to some kind of declaration\n  /// that can be completed (such as a C struct, C++ class, or Objective-C\n  /// class), will be set to the declaration.\n  bool isIncompleteType(NamedDecl **Def = nullptr) const;\n\n  /// Return true if this is an incomplete or object\n  /// type, in other words, not a function type.\n  bool isIncompleteOrObjectType() const {\n    return !isFunctionType();\n  }\n\n  /// Determine whether this type is an object type.\n  bool isObjectType() const {\n    // C++ [basic.types]p8:\n    //   An object type is a (possibly cv-qualified) type that is not a\n    //   function type, not a reference type, and not a void type.\n    return !isReferenceType() && !isFunctionType() && !isVoidType();\n  }\n\n  /// Return true if this is a literal type\n  /// (C++11 [basic.types]p10)\n  bool isLiteralType(const ASTContext &Ctx) const;\n\n  /// Determine if this type is a structural type, per C++20 [temp.param]p7.\n  bool isStructuralType() const;\n\n  /// Test if this type is a standard-layout type.\n  /// (C++0x [basic.type]p9)\n  bool isStandardLayoutType() const;\n\n  /// Helper methods to distinguish type categories. All type predicates\n  /// operate on the canonical type, ignoring typedefs and qualifiers.\n\n  /// Returns true if the type is a builtin type.\n  bool isBuiltinType() const;\n\n  /// Test for a particular builtin type.\n  bool isSpecificBuiltinType(unsigned K) const;\n\n  /// Test for a type which does not represent an actual type-system type but\n  /// is instead used as a placeholder for various convenient purposes within\n  /// Clang.  All such types are BuiltinTypes.\n  bool isPlaceholderType() const;\n  const BuiltinType *getAsPlaceholderType() const;\n\n  /// Test for a specific placeholder type.\n  bool isSpecificPlaceholderType(unsigned K) const;\n\n  /// Test for a placeholder type other than Overload; see\n  /// BuiltinType::isNonOverloadPlaceholderType.\n  bool isNonOverloadPlaceholderType() const;\n\n  /// isIntegerType() does *not* include complex integers (a GCC extension).\n  /// isComplexIntegerType() can be used to test for complex integers.\n  bool isIntegerType() const;     // C99 6.2.5p17 (int, char, bool, enum)\n  bool isEnumeralType() const;\n\n  /// Determine whether this type is a scoped enumeration type.\n  bool isScopedEnumeralType() const;\n  bool isBooleanType() const;\n  bool isCharType() const;\n  bool isWideCharType() const;\n  bool isChar8Type() const;\n  bool isChar16Type() const;\n  bool isChar32Type() const;\n  bool isAnyCharacterType() const;\n  bool isIntegralType(const ASTContext &Ctx) const;\n\n  /// Determine whether this type is an integral or enumeration type.\n  bool isIntegralOrEnumerationType() const;\n\n  /// Determine whether this type is an integral or unscoped enumeration type.\n  bool isIntegralOrUnscopedEnumerationType() const;\n  bool isUnscopedEnumerationType() const;\n\n  /// Floating point categories.\n  bool isRealFloatingType() const; // C99 6.2.5p10 (float, double, long double)\n  /// isComplexType() does *not* include complex integers (a GCC extension).\n  /// isComplexIntegerType() can be used to test for complex integers.\n  bool isComplexType() const;      // C99 6.2.5p11 (complex)\n  bool isAnyComplexType() const;   // C99 6.2.5p11 (complex) + Complex Int.\n  bool isFloatingType() const;     // C99 6.2.5p11 (real floating + complex)\n  bool isHalfType() const;         // OpenCL 6.1.1.1, NEON (IEEE 754-2008 half)\n  bool isFloat16Type() const;      // C11 extension ISO/IEC TS 18661\n  bool isBFloat16Type() const;\n  bool isFloat128Type() const;\n  bool isRealType() const;         // C99 6.2.5p17 (real floating + integer)\n  bool isArithmeticType() const;   // C99 6.2.5p18 (integer + floating)\n  bool isVoidType() const;         // C99 6.2.5p19\n  bool isScalarType() const;       // C99 6.2.5p21 (arithmetic + pointers)\n  bool isAggregateType() const;\n  bool isFundamentalType() const;\n  bool isCompoundType() const;\n\n  // Type Predicates: Check to see if this type is structurally the specified\n  // type, ignoring typedefs and qualifiers.\n  bool isFunctionType() const;\n  bool isFunctionNoProtoType() const { return getAs<FunctionNoProtoType>(); }\n  bool isFunctionProtoType() const { return getAs<FunctionProtoType>(); }\n  bool isPointerType() const;\n  bool isAnyPointerType() const;   // Any C pointer or ObjC object pointer\n  bool isBlockPointerType() const;\n  bool isVoidPointerType() const;\n  bool isReferenceType() const;\n  bool isLValueReferenceType() const;\n  bool isRValueReferenceType() const;\n  bool isObjectPointerType() const;\n  bool isFunctionPointerType() const;\n  bool isFunctionReferenceType() const;\n  bool isMemberPointerType() const;\n  bool isMemberFunctionPointerType() const;\n  bool isMemberDataPointerType() const;\n  bool isArrayType() const;\n  bool isConstantArrayType() const;\n  bool isIncompleteArrayType() const;\n  bool isVariableArrayType() const;\n  bool isDependentSizedArrayType() const;\n  bool isRecordType() const;\n  bool isClassType() const;\n  bool isStructureType() const;\n  bool isObjCBoxableRecordType() const;\n  bool isInterfaceType() const;\n  bool isStructureOrClassType() const;\n  bool isUnionType() const;\n  bool isComplexIntegerType() const;            // GCC _Complex integer type.\n  bool isVectorType() const;                    // GCC vector type.\n  bool isExtVectorType() const;                 // Extended vector type.\n  bool isMatrixType() const;                    // Matrix type.\n  bool isConstantMatrixType() const;            // Constant matrix type.\n  bool isDependentAddressSpaceType() const;     // value-dependent address space qualifier\n  bool isObjCObjectPointerType() const;         // pointer to ObjC object\n  bool isObjCRetainableType() const;            // ObjC object or block pointer\n  bool isObjCLifetimeType() const;              // (array of)* retainable type\n  bool isObjCIndirectLifetimeType() const;      // (pointer to)* lifetime type\n  bool isObjCNSObjectType() const;              // __attribute__((NSObject))\n  bool isObjCIndependentClassType() const;      // __attribute__((objc_independent_class))\n  // FIXME: change this to 'raw' interface type, so we can used 'interface' type\n  // for the common case.\n  bool isObjCObjectType() const;                // NSString or typeof(*(id)0)\n  bool isObjCQualifiedInterfaceType() const;    // NSString<foo>\n  bool isObjCQualifiedIdType() const;           // id<foo>\n  bool isObjCQualifiedClassType() const;        // Class<foo>\n  bool isObjCObjectOrInterfaceType() const;\n  bool isObjCIdType() const;                    // id\n  bool isDecltypeType() const;\n  /// Was this type written with the special inert-in-ARC __unsafe_unretained\n  /// qualifier?\n  ///\n  /// This approximates the answer to the following question: if this\n  /// translation unit were compiled in ARC, would this type be qualified\n  /// with __unsafe_unretained?\n  bool isObjCInertUnsafeUnretainedType() const {\n    return hasAttr(attr::ObjCInertUnsafeUnretained);\n  }\n\n  /// Whether the type is Objective-C 'id' or a __kindof type of an\n  /// object type, e.g., __kindof NSView * or __kindof id\n  /// <NSCopying>.\n  ///\n  /// \\param bound Will be set to the bound on non-id subtype types,\n  /// which will be (possibly specialized) Objective-C class type, or\n  /// null for 'id.\n  bool isObjCIdOrObjectKindOfType(const ASTContext &ctx,\n                                  const ObjCObjectType *&bound) const;\n\n  bool isObjCClassType() const;                 // Class\n\n  /// Whether the type is Objective-C 'Class' or a __kindof type of an\n  /// Class type, e.g., __kindof Class <NSCopying>.\n  ///\n  /// Unlike \\c isObjCIdOrObjectKindOfType, there is no relevant bound\n  /// here because Objective-C's type system cannot express \"a class\n  /// object for a subclass of NSFoo\".\n  bool isObjCClassOrClassKindOfType() const;\n\n  bool isBlockCompatibleObjCPointerType(ASTContext &ctx) const;\n  bool isObjCSelType() const;                 // Class\n  bool isObjCBuiltinType() const;               // 'id' or 'Class'\n  bool isObjCARCBridgableType() const;\n  bool isCARCBridgableType() const;\n  bool isTemplateTypeParmType() const;          // C++ template type parameter\n  bool isNullPtrType() const;                   // C++11 std::nullptr_t\n  bool isNothrowT() const;                      // C++   std::nothrow_t\n  bool isAlignValT() const;                     // C++17 std::align_val_t\n  bool isStdByteType() const;                   // C++17 std::byte\n  bool isAtomicType() const;                    // C11 _Atomic()\n  bool isUndeducedAutoType() const;             // C++11 auto or\n                                                // C++14 decltype(auto)\n  bool isTypedefNameType() const;               // typedef or alias template\n\n#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \\\n  bool is##Id##Type() const;\n#include \"clang/Basic/OpenCLImageTypes.def\"\n\n  bool isImageType() const;                     // Any OpenCL image type\n\n  bool isSamplerT() const;                      // OpenCL sampler_t\n  bool isEventT() const;                        // OpenCL event_t\n  bool isClkEventT() const;                     // OpenCL clk_event_t\n  bool isQueueT() const;                        // OpenCL queue_t\n  bool isReserveIDT() const;                    // OpenCL reserve_id_t\n\n#define EXT_OPAQUE_TYPE(ExtType, Id, Ext) \\\n  bool is##Id##Type() const;\n#include \"clang/Basic/OpenCLExtensionTypes.def\"\n  // Type defined in cl_intel_device_side_avc_motion_estimation OpenCL extension\n  bool isOCLIntelSubgroupAVCType() const;\n  bool isOCLExtOpaqueType() const;              // Any OpenCL extension type\n\n  bool isPipeType() const;                      // OpenCL pipe type\n  bool isExtIntType() const;                    // Extended Int Type\n  bool isOpenCLSpecificType() const;            // Any OpenCL specific type\n\n  /// Determines if this type, which must satisfy\n  /// isObjCLifetimeType(), is implicitly __unsafe_unretained rather\n  /// than implicitly __strong.\n  bool isObjCARCImplicitlyUnretainedType() const;\n\n  /// Check if the type is the CUDA device builtin surface type.\n  bool isCUDADeviceBuiltinSurfaceType() const;\n  /// Check if the type is the CUDA device builtin texture type.\n  bool isCUDADeviceBuiltinTextureType() const;\n\n  /// Return the implicit lifetime for this type, which must not be dependent.\n  Qualifiers::ObjCLifetime getObjCARCImplicitLifetime() const;\n\n  enum ScalarTypeKind {\n    STK_CPointer,\n    STK_BlockPointer,\n    STK_ObjCObjectPointer,\n    STK_MemberPointer,\n    STK_Bool,\n    STK_Integral,\n    STK_Floating,\n    STK_IntegralComplex,\n    STK_FloatingComplex,\n    STK_FixedPoint\n  };\n\n  /// Given that this is a scalar type, classify it.\n  ScalarTypeKind getScalarTypeKind() const;\n\n  TypeDependence getDependence() const {\n    return static_cast<TypeDependence>(TypeBits.Dependence);\n  }\n\n  /// Whether this type is an error type.\n  bool containsErrors() const {\n    return getDependence() & TypeDependence::Error;\n  }\n\n  /// Whether this type is a dependent type, meaning that its definition\n  /// somehow depends on a template parameter (C++ [temp.dep.type]).\n  bool isDependentType() const {\n    return getDependence() & TypeDependence::Dependent;\n  }\n\n  /// Determine whether this type is an instantiation-dependent type,\n  /// meaning that the type involves a template parameter (even if the\n  /// definition does not actually depend on the type substituted for that\n  /// template parameter).\n  bool isInstantiationDependentType() const {\n    return getDependence() & TypeDependence::Instantiation;\n  }\n\n  /// Determine whether this type is an undeduced type, meaning that\n  /// it somehow involves a C++11 'auto' type or similar which has not yet been\n  /// deduced.\n  bool isUndeducedType() const;\n\n  /// Whether this type is a variably-modified type (C99 6.7.5).\n  bool isVariablyModifiedType() const {\n    return getDependence() & TypeDependence::VariablyModified;\n  }\n\n  /// Whether this type involves a variable-length array type\n  /// with a definite size.\n  bool hasSizedVLAType() const;\n\n  /// Whether this type is or contains a local or unnamed type.\n  bool hasUnnamedOrLocalType() const;\n\n  bool isOverloadableType() const;\n\n  /// Determine wither this type is a C++ elaborated-type-specifier.\n  bool isElaboratedTypeSpecifier() const;\n\n  bool canDecayToPointerType() const;\n\n  /// Whether this type is represented natively as a pointer.  This includes\n  /// pointers, references, block pointers, and Objective-C interface,\n  /// qualified id, and qualified interface types, as well as nullptr_t.\n  bool hasPointerRepresentation() const;\n\n  /// Whether this type can represent an objective pointer type for the\n  /// purpose of GC'ability\n  bool hasObjCPointerRepresentation() const;\n\n  /// Determine whether this type has an integer representation\n  /// of some sort, e.g., it is an integer type or a vector.\n  bool hasIntegerRepresentation() const;\n\n  /// Determine whether this type has an signed integer representation\n  /// of some sort, e.g., it is an signed integer type or a vector.\n  bool hasSignedIntegerRepresentation() const;\n\n  /// Determine whether this type has an unsigned integer representation\n  /// of some sort, e.g., it is an unsigned integer type or a vector.\n  bool hasUnsignedIntegerRepresentation() const;\n\n  /// Determine whether this type has a floating-point representation\n  /// of some sort, e.g., it is a floating-point type or a vector thereof.\n  bool hasFloatingRepresentation() const;\n\n  // Type Checking Functions: Check to see if this type is structurally the\n  // specified type, ignoring typedefs and qualifiers, and return a pointer to\n  // the best type we can.\n  const RecordType *getAsStructureType() const;\n  /// NOTE: getAs*ArrayType are methods on ASTContext.\n  const RecordType *getAsUnionType() const;\n  const ComplexType *getAsComplexIntegerType() const; // GCC complex int type.\n  const ObjCObjectType *getAsObjCInterfaceType() const;\n\n  // The following is a convenience method that returns an ObjCObjectPointerType\n  // for object declared using an interface.\n  const ObjCObjectPointerType *getAsObjCInterfacePointerType() const;\n  const ObjCObjectPointerType *getAsObjCQualifiedIdType() const;\n  const ObjCObjectPointerType *getAsObjCQualifiedClassType() const;\n  const ObjCObjectType *getAsObjCQualifiedInterfaceType() const;\n\n  /// Retrieves the CXXRecordDecl that this type refers to, either\n  /// because the type is a RecordType or because it is the injected-class-name\n  /// type of a class template or class template partial specialization.\n  CXXRecordDecl *getAsCXXRecordDecl() const;\n\n  /// Retrieves the RecordDecl this type refers to.\n  RecordDecl *getAsRecordDecl() const;\n\n  /// Retrieves the TagDecl that this type refers to, either\n  /// because the type is a TagType or because it is the injected-class-name\n  /// type of a class template or class template partial specialization.\n  TagDecl *getAsTagDecl() const;\n\n  /// If this is a pointer or reference to a RecordType, return the\n  /// CXXRecordDecl that the type refers to.\n  ///\n  /// If this is not a pointer or reference, or the type being pointed to does\n  /// not refer to a CXXRecordDecl, returns NULL.\n  const CXXRecordDecl *getPointeeCXXRecordDecl() const;\n\n  /// Get the DeducedType whose type will be deduced for a variable with\n  /// an initializer of this type. This looks through declarators like pointer\n  /// types, but not through decltype or typedefs.\n  DeducedType *getContainedDeducedType() const;\n\n  /// Get the AutoType whose type will be deduced for a variable with\n  /// an initializer of this type. This looks through declarators like pointer\n  /// types, but not through decltype or typedefs.\n  AutoType *getContainedAutoType() const {\n    return dyn_cast_or_null<AutoType>(getContainedDeducedType());\n  }\n\n  /// Determine whether this type was written with a leading 'auto'\n  /// corresponding to a trailing return type (possibly for a nested\n  /// function type within a pointer to function type or similar).\n  bool hasAutoForTrailingReturnType() const;\n\n  /// Member-template getAs<specific type>'.  Look through sugar for\n  /// an instance of \\<specific type>.   This scheme will eventually\n  /// replace the specific getAsXXXX methods above.\n  ///\n  /// There are some specializations of this member template listed\n  /// immediately following this class.\n  template <typename T> const T *getAs() const;\n\n  /// Member-template getAsAdjusted<specific type>. Look through specific kinds\n  /// of sugar (parens, attributes, etc) for an instance of \\<specific type>.\n  /// This is used when you need to walk over sugar nodes that represent some\n  /// kind of type adjustment from a type that was written as a \\<specific type>\n  /// to another type that is still canonically a \\<specific type>.\n  template <typename T> const T *getAsAdjusted() const;\n\n  /// A variant of getAs<> for array types which silently discards\n  /// qualifiers from the outermost type.\n  const ArrayType *getAsArrayTypeUnsafe() const;\n\n  /// Member-template castAs<specific type>.  Look through sugar for\n  /// the underlying instance of \\<specific type>.\n  ///\n  /// This method has the same relationship to getAs<T> as cast<T> has\n  /// to dyn_cast<T>; which is to say, the underlying type *must*\n  /// have the intended type, and this method will never return null.\n  template <typename T> const T *castAs() const;\n\n  /// A variant of castAs<> for array type which silently discards\n  /// qualifiers from the outermost type.\n  const ArrayType *castAsArrayTypeUnsafe() const;\n\n  /// Determine whether this type had the specified attribute applied to it\n  /// (looking through top-level type sugar).\n  bool hasAttr(attr::Kind AK) const;\n\n  /// Get the base element type of this type, potentially discarding type\n  /// qualifiers.  This should never be used when type qualifiers\n  /// are meaningful.\n  const Type *getBaseElementTypeUnsafe() const;\n\n  /// If this is an array type, return the element type of the array,\n  /// potentially with type qualifiers missing.\n  /// This should never be used when type qualifiers are meaningful.\n  const Type *getArrayElementTypeNoTypeQual() const;\n\n  /// If this is a pointer type, return the pointee type.\n  /// If this is an array type, return the array element type.\n  /// This should never be used when type qualifiers are meaningful.\n  const Type *getPointeeOrArrayElementType() const;\n\n  /// If this is a pointer, ObjC object pointer, or block\n  /// pointer, this returns the respective pointee.\n  QualType getPointeeType() const;\n\n  /// Return the specified type with any \"sugar\" removed from the type,\n  /// removing any typedefs, typeofs, etc., as well as any qualifiers.\n  const Type *getUnqualifiedDesugaredType() const;\n\n  /// More type predicates useful for type checking/promotion\n  bool isPromotableIntegerType() const; // C99 6.3.1.1p2\n\n  /// Return true if this is an integer type that is\n  /// signed, according to C99 6.2.5p4 [char, signed char, short, int, long..],\n  /// or an enum decl which has a signed representation.\n  bool isSignedIntegerType() const;\n\n  /// Return true if this is an integer type that is\n  /// unsigned, according to C99 6.2.5p6 [which returns true for _Bool],\n  /// or an enum decl which has an unsigned representation.\n  bool isUnsignedIntegerType() const;\n\n  /// Determines whether this is an integer type that is signed or an\n  /// enumeration types whose underlying type is a signed integer type.\n  bool isSignedIntegerOrEnumerationType() const;\n\n  /// Determines whether this is an integer type that is unsigned or an\n  /// enumeration types whose underlying type is a unsigned integer type.\n  bool isUnsignedIntegerOrEnumerationType() const;\n\n  /// Return true if this is a fixed point type according to\n  /// ISO/IEC JTC1 SC22 WG14 N1169.\n  bool isFixedPointType() const;\n\n  /// Return true if this is a fixed point or integer type.\n  bool isFixedPointOrIntegerType() const;\n\n  /// Return true if this is a saturated fixed point type according to\n  /// ISO/IEC JTC1 SC22 WG14 N1169. This type can be signed or unsigned.\n  bool isSaturatedFixedPointType() const;\n\n  /// Return true if this is a saturated fixed point type according to\n  /// ISO/IEC JTC1 SC22 WG14 N1169. This type can be signed or unsigned.\n  bool isUnsaturatedFixedPointType() const;\n\n  /// Return true if this is a fixed point type that is signed according\n  /// to ISO/IEC JTC1 SC22 WG14 N1169. This type can also be saturated.\n  bool isSignedFixedPointType() const;\n\n  /// Return true if this is a fixed point type that is unsigned according\n  /// to ISO/IEC JTC1 SC22 WG14 N1169. This type can also be saturated.\n  bool isUnsignedFixedPointType() const;\n\n  /// Return true if this is not a variable sized type,\n  /// according to the rules of C99 6.7.5p3.  It is not legal to call this on\n  /// incomplete types.\n  bool isConstantSizeType() const;\n\n  /// Returns true if this type can be represented by some\n  /// set of type specifiers.\n  bool isSpecifierType() const;\n\n  /// Determine the linkage of this type.\n  Linkage getLinkage() const;\n\n  /// Determine the visibility of this type.\n  Visibility getVisibility() const {\n    return getLinkageAndVisibility().getVisibility();\n  }\n\n  /// Return true if the visibility was explicitly set is the code.\n  bool isVisibilityExplicit() const {\n    return getLinkageAndVisibility().isVisibilityExplicit();\n  }\n\n  /// Determine the linkage and visibility of this type.\n  LinkageInfo getLinkageAndVisibility() const;\n\n  /// True if the computed linkage is valid. Used for consistency\n  /// checking. Should always return true.\n  bool isLinkageValid() const;\n\n  /// Determine the nullability of the given type.\n  ///\n  /// Note that nullability is only captured as sugar within the type\n  /// system, not as part of the canonical type, so nullability will\n  /// be lost by canonicalization and desugaring.\n  Optional<NullabilityKind> getNullability(const ASTContext &context) const;\n\n  /// Determine whether the given type can have a nullability\n  /// specifier applied to it, i.e., if it is any kind of pointer type.\n  ///\n  /// \\param ResultIfUnknown The value to return if we don't yet know whether\n  ///        this type can have nullability because it is dependent.\n  bool canHaveNullability(bool ResultIfUnknown = true) const;\n\n  /// Retrieve the set of substitutions required when accessing a member\n  /// of the Objective-C receiver type that is declared in the given context.\n  ///\n  /// \\c *this is the type of the object we're operating on, e.g., the\n  /// receiver for a message send or the base of a property access, and is\n  /// expected to be of some object or object pointer type.\n  ///\n  /// \\param dc The declaration context for which we are building up a\n  /// substitution mapping, which should be an Objective-C class, extension,\n  /// category, or method within.\n  ///\n  /// \\returns an array of type arguments that can be substituted for\n  /// the type parameters of the given declaration context in any type described\n  /// within that context, or an empty optional to indicate that no\n  /// substitution is required.\n  Optional<ArrayRef<QualType>>\n  getObjCSubstitutions(const DeclContext *dc) const;\n\n  /// Determines if this is an ObjC interface type that may accept type\n  /// parameters.\n  bool acceptsObjCTypeParams() const;\n\n  const char *getTypeClassName() const;\n\n  QualType getCanonicalTypeInternal() const {\n    return CanonicalType;\n  }\n\n  CanQualType getCanonicalTypeUnqualified() const; // in CanonicalType.h\n  void dump() const;\n  void dump(llvm::raw_ostream &OS, const ASTContext &Context) const;\n};\n\n/// This will check for a TypedefType by removing any existing sugar\n/// until it reaches a TypedefType or a non-sugared type.\ntemplate <> const TypedefType *Type::getAs() const;\n\n/// This will check for a TemplateSpecializationType by removing any\n/// existing sugar until it reaches a TemplateSpecializationType or a\n/// non-sugared type.\ntemplate <> const TemplateSpecializationType *Type::getAs() const;\n\n/// This will check for an AttributedType by removing any existing sugar\n/// until it reaches an AttributedType or a non-sugared type.\ntemplate <> const AttributedType *Type::getAs() const;\n\n// We can do canonical leaf types faster, because we don't have to\n// worry about preserving child type decoration.\n#define TYPE(Class, Base)\n#define LEAF_TYPE(Class) \\\ntemplate <> inline const Class##Type *Type::getAs() const { \\\n  return dyn_cast<Class##Type>(CanonicalType); \\\n} \\\ntemplate <> inline const Class##Type *Type::castAs() const { \\\n  return cast<Class##Type>(CanonicalType); \\\n}\n#include \"clang/AST/TypeNodes.inc\"\n\n/// This class is used for builtin types like 'int'.  Builtin\n/// types are always canonical and have a literal name field.\nclass BuiltinType : public Type {\npublic:\n  enum Kind {\n// OpenCL image types\n#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) Id,\n#include \"clang/Basic/OpenCLImageTypes.def\"\n// OpenCL extension types\n#define EXT_OPAQUE_TYPE(ExtType, Id, Ext) Id,\n#include \"clang/Basic/OpenCLExtensionTypes.def\"\n// SVE Types\n#define SVE_TYPE(Name, Id, SingletonId) Id,\n#include \"clang/Basic/AArch64SVEACLETypes.def\"\n// PPC MMA Types\n#define PPC_VECTOR_TYPE(Name, Id, Size) Id,\n#include \"clang/Basic/PPCTypes.def\"\n// RVV Types\n#define RVV_TYPE(Name, Id, SingletonId) Id,\n#include \"clang/Basic/RISCVVTypes.def\"\n// All other builtin types\n#define BUILTIN_TYPE(Id, SingletonId) Id,\n#define LAST_BUILTIN_TYPE(Id) LastKind = Id\n#include \"clang/AST/BuiltinTypes.def\"\n  };\n\nprivate:\n  friend class ASTContext; // ASTContext creates these.\n\n  BuiltinType(Kind K)\n      : Type(Builtin, QualType(),\n             K == Dependent ? TypeDependence::DependentInstantiation\n                            : TypeDependence::None) {\n    BuiltinTypeBits.Kind = K;\n  }\n\npublic:\n  Kind getKind() const { return static_cast<Kind>(BuiltinTypeBits.Kind); }\n  StringRef getName(const PrintingPolicy &Policy) const;\n\n  const char *getNameAsCString(const PrintingPolicy &Policy) const {\n    // The StringRef is null-terminated.\n    StringRef str = getName(Policy);\n    assert(!str.empty() && str.data()[str.size()] == '\\0');\n    return str.data();\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  bool isInteger() const {\n    return getKind() >= Bool && getKind() <= Int128;\n  }\n\n  bool isSignedInteger() const {\n    return getKind() >= Char_S && getKind() <= Int128;\n  }\n\n  bool isUnsignedInteger() const {\n    return getKind() >= Bool && getKind() <= UInt128;\n  }\n\n  bool isFloatingPoint() const {\n    return getKind() >= Half && getKind() <= Float128;\n  }\n\n  /// Determines whether the given kind corresponds to a placeholder type.\n  static bool isPlaceholderTypeKind(Kind K) {\n    return K >= Overload;\n  }\n\n  /// Determines whether this type is a placeholder type, i.e. a type\n  /// which cannot appear in arbitrary positions in a fully-formed\n  /// expression.\n  bool isPlaceholderType() const {\n    return isPlaceholderTypeKind(getKind());\n  }\n\n  /// Determines whether this type is a placeholder type other than\n  /// Overload.  Most placeholder types require only syntactic\n  /// information about their context in order to be resolved (e.g.\n  /// whether it is a call expression), which means they can (and\n  /// should) be resolved in an earlier \"phase\" of analysis.\n  /// Overload expressions sometimes pick up further information\n  /// from their context, like whether the context expects a\n  /// specific function-pointer type, and so frequently need\n  /// special treatment.\n  bool isNonOverloadPlaceholderType() const {\n    return getKind() > Overload;\n  }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Builtin; }\n};\n\n/// Complex values, per C99 6.2.5p11.  This supports the C99 complex\n/// types (_Complex float etc) as well as the GCC integer complex extensions.\nclass ComplexType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType ElementType;\n\n  ComplexType(QualType Element, QualType CanonicalPtr)\n      : Type(Complex, CanonicalPtr, Element->getDependence()),\n        ElementType(Element) {}\n\npublic:\n  QualType getElementType() const { return ElementType; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getElementType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Element) {\n    ID.AddPointer(Element.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Complex; }\n};\n\n/// Sugar for parentheses used when specifying types.\nclass ParenType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType Inner;\n\n  ParenType(QualType InnerType, QualType CanonType)\n      : Type(Paren, CanonType, InnerType->getDependence()), Inner(InnerType) {}\n\npublic:\n  QualType getInnerType() const { return Inner; }\n\n  bool isSugared() const { return true; }\n  QualType desugar() const { return getInnerType(); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getInnerType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Inner) {\n    Inner.Profile(ID);\n  }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Paren; }\n};\n\n/// PointerType - C99 6.7.5.1 - Pointer Declarators.\nclass PointerType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType PointeeType;\n\n  PointerType(QualType Pointee, QualType CanonicalPtr)\n      : Type(Pointer, CanonicalPtr, Pointee->getDependence()),\n        PointeeType(Pointee) {}\n\npublic:\n  QualType getPointeeType() const { return PointeeType; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getPointeeType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Pointee) {\n    ID.AddPointer(Pointee.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Pointer; }\n};\n\n/// Represents a type which was implicitly adjusted by the semantic\n/// engine for arbitrary reasons.  For example, array and function types can\n/// decay, and function types can have their calling conventions adjusted.\nclass AdjustedType : public Type, public llvm::FoldingSetNode {\n  QualType OriginalTy;\n  QualType AdjustedTy;\n\nprotected:\n  friend class ASTContext; // ASTContext creates these.\n\n  AdjustedType(TypeClass TC, QualType OriginalTy, QualType AdjustedTy,\n               QualType CanonicalPtr)\n      : Type(TC, CanonicalPtr, OriginalTy->getDependence()),\n        OriginalTy(OriginalTy), AdjustedTy(AdjustedTy) {}\n\npublic:\n  QualType getOriginalType() const { return OriginalTy; }\n  QualType getAdjustedType() const { return AdjustedTy; }\n\n  bool isSugared() const { return true; }\n  QualType desugar() const { return AdjustedTy; }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, OriginalTy, AdjustedTy);\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Orig, QualType New) {\n    ID.AddPointer(Orig.getAsOpaquePtr());\n    ID.AddPointer(New.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Adjusted || T->getTypeClass() == Decayed;\n  }\n};\n\n/// Represents a pointer type decayed from an array or function type.\nclass DecayedType : public AdjustedType {\n  friend class ASTContext; // ASTContext creates these.\n\n  inline\n  DecayedType(QualType OriginalType, QualType Decayed, QualType Canonical);\n\npublic:\n  QualType getDecayedType() const { return getAdjustedType(); }\n\n  inline QualType getPointeeType() const;\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Decayed; }\n};\n\n/// Pointer to a block type.\n/// This type is to represent types syntactically represented as\n/// \"void (^)(int)\", etc. Pointee is required to always be a function type.\nclass BlockPointerType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  // Block is some kind of pointer type\n  QualType PointeeType;\n\n  BlockPointerType(QualType Pointee, QualType CanonicalCls)\n      : Type(BlockPointer, CanonicalCls, Pointee->getDependence()),\n        PointeeType(Pointee) {}\n\npublic:\n  // Get the pointee type. Pointee is required to always be a function type.\n  QualType getPointeeType() const { return PointeeType; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n      Profile(ID, getPointeeType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Pointee) {\n      ID.AddPointer(Pointee.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == BlockPointer;\n  }\n};\n\n/// Base for LValueReferenceType and RValueReferenceType\nclass ReferenceType : public Type, public llvm::FoldingSetNode {\n  QualType PointeeType;\n\nprotected:\n  ReferenceType(TypeClass tc, QualType Referencee, QualType CanonicalRef,\n                bool SpelledAsLValue)\n      : Type(tc, CanonicalRef, Referencee->getDependence()),\n        PointeeType(Referencee) {\n    ReferenceTypeBits.SpelledAsLValue = SpelledAsLValue;\n    ReferenceTypeBits.InnerRef = Referencee->isReferenceType();\n  }\n\npublic:\n  bool isSpelledAsLValue() const { return ReferenceTypeBits.SpelledAsLValue; }\n  bool isInnerRef() const { return ReferenceTypeBits.InnerRef; }\n\n  QualType getPointeeTypeAsWritten() const { return PointeeType; }\n\n  QualType getPointeeType() const {\n    // FIXME: this might strip inner qualifiers; okay?\n    const ReferenceType *T = this;\n    while (T->isInnerRef())\n      T = T->PointeeType->castAs<ReferenceType>();\n    return T->PointeeType;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, PointeeType, isSpelledAsLValue());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      QualType Referencee,\n                      bool SpelledAsLValue) {\n    ID.AddPointer(Referencee.getAsOpaquePtr());\n    ID.AddBoolean(SpelledAsLValue);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == LValueReference ||\n           T->getTypeClass() == RValueReference;\n  }\n};\n\n/// An lvalue reference type, per C++11 [dcl.ref].\nclass LValueReferenceType : public ReferenceType {\n  friend class ASTContext; // ASTContext creates these\n\n  LValueReferenceType(QualType Referencee, QualType CanonicalRef,\n                      bool SpelledAsLValue)\n      : ReferenceType(LValueReference, Referencee, CanonicalRef,\n                      SpelledAsLValue) {}\n\npublic:\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == LValueReference;\n  }\n};\n\n/// An rvalue reference type, per C++11 [dcl.ref].\nclass RValueReferenceType : public ReferenceType {\n  friend class ASTContext; // ASTContext creates these\n\n  RValueReferenceType(QualType Referencee, QualType CanonicalRef)\n       : ReferenceType(RValueReference, Referencee, CanonicalRef, false) {}\n\npublic:\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == RValueReference;\n  }\n};\n\n/// A pointer to member type per C++ 8.3.3 - Pointers to members.\n///\n/// This includes both pointers to data members and pointer to member functions.\nclass MemberPointerType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType PointeeType;\n\n  /// The class of which the pointee is a member. Must ultimately be a\n  /// RecordType, but could be a typedef or a template parameter too.\n  const Type *Class;\n\n  MemberPointerType(QualType Pointee, const Type *Cls, QualType CanonicalPtr)\n      : Type(MemberPointer, CanonicalPtr,\n             (Cls->getDependence() & ~TypeDependence::VariablyModified) |\n                 Pointee->getDependence()),\n        PointeeType(Pointee), Class(Cls) {}\n\npublic:\n  QualType getPointeeType() const { return PointeeType; }\n\n  /// Returns true if the member type (i.e. the pointee type) is a\n  /// function type rather than a data-member type.\n  bool isMemberFunctionPointer() const {\n    return PointeeType->isFunctionProtoType();\n  }\n\n  /// Returns true if the member type (i.e. the pointee type) is a\n  /// data type rather than a function type.\n  bool isMemberDataPointer() const {\n    return !PointeeType->isFunctionProtoType();\n  }\n\n  const Type *getClass() const { return Class; }\n  CXXRecordDecl *getMostRecentCXXRecordDecl() const;\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getPointeeType(), getClass());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Pointee,\n                      const Type *Class) {\n    ID.AddPointer(Pointee.getAsOpaquePtr());\n    ID.AddPointer(Class);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == MemberPointer;\n  }\n};\n\n/// Represents an array type, per C99 6.7.5.2 - Array Declarators.\nclass ArrayType : public Type, public llvm::FoldingSetNode {\npublic:\n  /// Capture whether this is a normal array (e.g. int X[4])\n  /// an array with a static size (e.g. int X[static 4]), or an array\n  /// with a star size (e.g. int X[*]).\n  /// 'static' is only allowed on function parameters.\n  enum ArraySizeModifier {\n    Normal, Static, Star\n  };\n\nprivate:\n  /// The element type of the array.\n  QualType ElementType;\n\nprotected:\n  friend class ASTContext; // ASTContext creates these.\n\n  ArrayType(TypeClass tc, QualType et, QualType can, ArraySizeModifier sm,\n            unsigned tq, const Expr *sz = nullptr);\n\npublic:\n  QualType getElementType() const { return ElementType; }\n\n  ArraySizeModifier getSizeModifier() const {\n    return ArraySizeModifier(ArrayTypeBits.SizeModifier);\n  }\n\n  Qualifiers getIndexTypeQualifiers() const {\n    return Qualifiers::fromCVRMask(getIndexTypeCVRQualifiers());\n  }\n\n  unsigned getIndexTypeCVRQualifiers() const {\n    return ArrayTypeBits.IndexTypeQuals;\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ConstantArray ||\n           T->getTypeClass() == VariableArray ||\n           T->getTypeClass() == IncompleteArray ||\n           T->getTypeClass() == DependentSizedArray;\n  }\n};\n\n/// Represents the canonical version of C arrays with a specified constant size.\n/// For example, the canonical type for 'int A[4 + 4*100]' is a\n/// ConstantArrayType where the element type is 'int' and the size is 404.\nclass ConstantArrayType final\n    : public ArrayType,\n      private llvm::TrailingObjects<ConstantArrayType, const Expr *> {\n  friend class ASTContext; // ASTContext creates these.\n  friend TrailingObjects;\n\n  llvm::APInt Size; // Allows us to unique the type.\n\n  ConstantArrayType(QualType et, QualType can, const llvm::APInt &size,\n                    const Expr *sz, ArraySizeModifier sm, unsigned tq)\n      : ArrayType(ConstantArray, et, can, sm, tq, sz), Size(size) {\n    ConstantArrayTypeBits.HasStoredSizeExpr = sz != nullptr;\n    if (ConstantArrayTypeBits.HasStoredSizeExpr) {\n      assert(!can.isNull() && \"canonical constant array should not have size\");\n      *getTrailingObjects<const Expr*>() = sz;\n    }\n  }\n\n  unsigned numTrailingObjects(OverloadToken<const Expr*>) const {\n    return ConstantArrayTypeBits.HasStoredSizeExpr;\n  }\n\npublic:\n  const llvm::APInt &getSize() const { return Size; }\n  const Expr *getSizeExpr() const {\n    return ConstantArrayTypeBits.HasStoredSizeExpr\n               ? *getTrailingObjects<const Expr *>()\n               : nullptr;\n  }\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  /// Determine the number of bits required to address a member of\n  // an array with the given element type and number of elements.\n  static unsigned getNumAddressingBits(const ASTContext &Context,\n                                       QualType ElementType,\n                                       const llvm::APInt &NumElements);\n\n  /// Determine the maximum number of active bits that an array's size\n  /// can require, which limits the maximum size of the array.\n  static unsigned getMaxSizeBits(const ASTContext &Context);\n\n  void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Ctx) {\n    Profile(ID, Ctx, getElementType(), getSize(), getSizeExpr(),\n            getSizeModifier(), getIndexTypeCVRQualifiers());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Ctx,\n                      QualType ET, const llvm::APInt &ArraySize,\n                      const Expr *SizeExpr, ArraySizeModifier SizeMod,\n                      unsigned TypeQuals);\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ConstantArray;\n  }\n};\n\n/// Represents a C array with an unspecified size.  For example 'int A[]' has\n/// an IncompleteArrayType where the element type is 'int' and the size is\n/// unspecified.\nclass IncompleteArrayType : public ArrayType {\n  friend class ASTContext; // ASTContext creates these.\n\n  IncompleteArrayType(QualType et, QualType can,\n                      ArraySizeModifier sm, unsigned tq)\n      : ArrayType(IncompleteArray, et, can, sm, tq) {}\n\npublic:\n  friend class StmtIteratorBase;\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == IncompleteArray;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getElementType(), getSizeModifier(),\n            getIndexTypeCVRQualifiers());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType ET,\n                      ArraySizeModifier SizeMod, unsigned TypeQuals) {\n    ID.AddPointer(ET.getAsOpaquePtr());\n    ID.AddInteger(SizeMod);\n    ID.AddInteger(TypeQuals);\n  }\n};\n\n/// Represents a C array with a specified size that is not an\n/// integer-constant-expression.  For example, 'int s[x+foo()]'.\n/// Since the size expression is an arbitrary expression, we store it as such.\n///\n/// Note: VariableArrayType's aren't uniqued (since the expressions aren't) and\n/// should not be: two lexically equivalent variable array types could mean\n/// different things, for example, these variables do not have the same type\n/// dynamically:\n///\n/// void foo(int x) {\n///   int Y[x];\n///   ++x;\n///   int Z[x];\n/// }\nclass VariableArrayType : public ArrayType {\n  friend class ASTContext; // ASTContext creates these.\n\n  /// An assignment-expression. VLA's are only permitted within\n  /// a function block.\n  Stmt *SizeExpr;\n\n  /// The range spanned by the left and right array brackets.\n  SourceRange Brackets;\n\n  VariableArrayType(QualType et, QualType can, Expr *e,\n                    ArraySizeModifier sm, unsigned tq,\n                    SourceRange brackets)\n      : ArrayType(VariableArray, et, can, sm, tq, e),\n        SizeExpr((Stmt*) e), Brackets(brackets) {}\n\npublic:\n  friend class StmtIteratorBase;\n\n  Expr *getSizeExpr() const {\n    // We use C-style casts instead of cast<> here because we do not wish\n    // to have a dependency of Type.h on Stmt.h/Expr.h.\n    return (Expr*) SizeExpr;\n  }\n\n  SourceRange getBracketsRange() const { return Brackets; }\n  SourceLocation getLBracketLoc() const { return Brackets.getBegin(); }\n  SourceLocation getRBracketLoc() const { return Brackets.getEnd(); }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == VariableArray;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    llvm_unreachable(\"Cannot unique VariableArrayTypes.\");\n  }\n};\n\n/// Represents an array type in C++ whose size is a value-dependent expression.\n///\n/// For example:\n/// \\code\n/// template<typename T, int Size>\n/// class array {\n///   T data[Size];\n/// };\n/// \\endcode\n///\n/// For these types, we won't actually know what the array bound is\n/// until template instantiation occurs, at which point this will\n/// become either a ConstantArrayType or a VariableArrayType.\nclass DependentSizedArrayType : public ArrayType {\n  friend class ASTContext; // ASTContext creates these.\n\n  const ASTContext &Context;\n\n  /// An assignment expression that will instantiate to the\n  /// size of the array.\n  ///\n  /// The expression itself might be null, in which case the array\n  /// type will have its size deduced from an initializer.\n  Stmt *SizeExpr;\n\n  /// The range spanned by the left and right array brackets.\n  SourceRange Brackets;\n\n  DependentSizedArrayType(const ASTContext &Context, QualType et, QualType can,\n                          Expr *e, ArraySizeModifier sm, unsigned tq,\n                          SourceRange brackets);\n\npublic:\n  friend class StmtIteratorBase;\n\n  Expr *getSizeExpr() const {\n    // We use C-style casts instead of cast<> here because we do not wish\n    // to have a dependency of Type.h on Stmt.h/Expr.h.\n    return (Expr*) SizeExpr;\n  }\n\n  SourceRange getBracketsRange() const { return Brackets; }\n  SourceLocation getLBracketLoc() const { return Brackets.getBegin(); }\n  SourceLocation getRBracketLoc() const { return Brackets.getEnd(); }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentSizedArray;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, getElementType(),\n            getSizeModifier(), getIndexTypeCVRQualifiers(), getSizeExpr());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      QualType ET, ArraySizeModifier SizeMod,\n                      unsigned TypeQuals, Expr *E);\n};\n\n/// Represents an extended address space qualifier where the input address space\n/// value is dependent. Non-dependent address spaces are not represented with a\n/// special Type subclass; they are stored on an ExtQuals node as part of a QualType.\n///\n/// For example:\n/// \\code\n/// template<typename T, int AddrSpace>\n/// class AddressSpace {\n///   typedef T __attribute__((address_space(AddrSpace))) type;\n/// }\n/// \\endcode\nclass DependentAddressSpaceType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext;\n\n  const ASTContext &Context;\n  Expr *AddrSpaceExpr;\n  QualType PointeeType;\n  SourceLocation loc;\n\n  DependentAddressSpaceType(const ASTContext &Context, QualType PointeeType,\n                            QualType can, Expr *AddrSpaceExpr,\n                            SourceLocation loc);\n\npublic:\n  Expr *getAddrSpaceExpr() const { return AddrSpaceExpr; }\n  QualType getPointeeType() const { return PointeeType; }\n  SourceLocation getAttributeLoc() const { return loc; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentAddressSpace;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, getPointeeType(), getAddrSpaceExpr());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      QualType PointeeType, Expr *AddrSpaceExpr);\n};\n\n/// Represents an extended vector type where either the type or size is\n/// dependent.\n///\n/// For example:\n/// \\code\n/// template<typename T, int Size>\n/// class vector {\n///   typedef T __attribute__((ext_vector_type(Size))) type;\n/// }\n/// \\endcode\nclass DependentSizedExtVectorType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext;\n\n  const ASTContext &Context;\n  Expr *SizeExpr;\n\n  /// The element type of the array.\n  QualType ElementType;\n\n  SourceLocation loc;\n\n  DependentSizedExtVectorType(const ASTContext &Context, QualType ElementType,\n                              QualType can, Expr *SizeExpr, SourceLocation loc);\n\npublic:\n  Expr *getSizeExpr() const { return SizeExpr; }\n  QualType getElementType() const { return ElementType; }\n  SourceLocation getAttributeLoc() const { return loc; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentSizedExtVector;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, getElementType(), getSizeExpr());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      QualType ElementType, Expr *SizeExpr);\n};\n\n\n/// Represents a GCC generic vector type. This type is created using\n/// __attribute__((vector_size(n)), where \"n\" specifies the vector size in\n/// bytes; or from an Altivec __vector or vector declaration.\n/// Since the constructor takes the number of vector elements, the\n/// client is responsible for converting the size into the number of elements.\nclass VectorType : public Type, public llvm::FoldingSetNode {\npublic:\n  enum VectorKind {\n    /// not a target-specific vector type\n    GenericVector,\n\n    /// is AltiVec vector\n    AltiVecVector,\n\n    /// is AltiVec 'vector Pixel'\n    AltiVecPixel,\n\n    /// is AltiVec 'vector bool ...'\n    AltiVecBool,\n\n    /// is ARM Neon vector\n    NeonVector,\n\n    /// is ARM Neon polynomial vector\n    NeonPolyVector,\n\n    /// is AArch64 SVE fixed-length data vector\n    SveFixedLengthDataVector,\n\n    /// is AArch64 SVE fixed-length predicate vector\n    SveFixedLengthPredicateVector\n  };\n\nprotected:\n  friend class ASTContext; // ASTContext creates these.\n\n  /// The element type of the vector.\n  QualType ElementType;\n\n  VectorType(QualType vecType, unsigned nElements, QualType canonType,\n             VectorKind vecKind);\n\n  VectorType(TypeClass tc, QualType vecType, unsigned nElements,\n             QualType canonType, VectorKind vecKind);\n\npublic:\n  QualType getElementType() const { return ElementType; }\n  unsigned getNumElements() const { return VectorTypeBits.NumElements; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  VectorKind getVectorKind() const {\n    return VectorKind(VectorTypeBits.VecKind);\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getElementType(), getNumElements(),\n            getTypeClass(), getVectorKind());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType ElementType,\n                      unsigned NumElements, TypeClass TypeClass,\n                      VectorKind VecKind) {\n    ID.AddPointer(ElementType.getAsOpaquePtr());\n    ID.AddInteger(NumElements);\n    ID.AddInteger(TypeClass);\n    ID.AddInteger(VecKind);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Vector || T->getTypeClass() == ExtVector;\n  }\n};\n\n/// Represents a vector type where either the type or size is dependent.\n////\n/// For example:\n/// \\code\n/// template<typename T, int Size>\n/// class vector {\n///   typedef T __attribute__((vector_size(Size))) type;\n/// }\n/// \\endcode\nclass DependentVectorType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext;\n\n  const ASTContext &Context;\n  QualType ElementType;\n  Expr *SizeExpr;\n  SourceLocation Loc;\n\n  DependentVectorType(const ASTContext &Context, QualType ElementType,\n                           QualType CanonType, Expr *SizeExpr,\n                           SourceLocation Loc, VectorType::VectorKind vecKind);\n\npublic:\n  Expr *getSizeExpr() const { return SizeExpr; }\n  QualType getElementType() const { return ElementType; }\n  SourceLocation getAttributeLoc() const { return Loc; }\n  VectorType::VectorKind getVectorKind() const {\n    return VectorType::VectorKind(VectorTypeBits.VecKind);\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentVector;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, getElementType(), getSizeExpr(), getVectorKind());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      QualType ElementType, const Expr *SizeExpr,\n                      VectorType::VectorKind VecKind);\n};\n\n/// ExtVectorType - Extended vector type. This type is created using\n/// __attribute__((ext_vector_type(n)), where \"n\" is the number of elements.\n/// Unlike vector_size, ext_vector_type is only allowed on typedef's. This\n/// class enables syntactic extensions, like Vector Components for accessing\n/// points (as .xyzw), colors (as .rgba), and textures (modeled after OpenGL\n/// Shading Language).\nclass ExtVectorType : public VectorType {\n  friend class ASTContext; // ASTContext creates these.\n\n  ExtVectorType(QualType vecType, unsigned nElements, QualType canonType)\n      : VectorType(ExtVector, vecType, nElements, canonType, GenericVector) {}\n\npublic:\n  static int getPointAccessorIdx(char c) {\n    switch (c) {\n    default: return -1;\n    case 'x': case 'r': return 0;\n    case 'y': case 'g': return 1;\n    case 'z': case 'b': return 2;\n    case 'w': case 'a': return 3;\n    }\n  }\n\n  static int getNumericAccessorIdx(char c) {\n    switch (c) {\n      default: return -1;\n      case '0': return 0;\n      case '1': return 1;\n      case '2': return 2;\n      case '3': return 3;\n      case '4': return 4;\n      case '5': return 5;\n      case '6': return 6;\n      case '7': return 7;\n      case '8': return 8;\n      case '9': return 9;\n      case 'A':\n      case 'a': return 10;\n      case 'B':\n      case 'b': return 11;\n      case 'C':\n      case 'c': return 12;\n      case 'D':\n      case 'd': return 13;\n      case 'E':\n      case 'e': return 14;\n      case 'F':\n      case 'f': return 15;\n    }\n  }\n\n  static int getAccessorIdx(char c, bool isNumericAccessor) {\n    if (isNumericAccessor)\n      return getNumericAccessorIdx(c);\n    else\n      return getPointAccessorIdx(c);\n  }\n\n  bool isAccessorWithinNumElements(char c, bool isNumericAccessor) const {\n    if (int idx = getAccessorIdx(c, isNumericAccessor)+1)\n      return unsigned(idx-1) < getNumElements();\n    return false;\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ExtVector;\n  }\n};\n\n/// Represents a matrix type, as defined in the Matrix Types clang extensions.\n/// __attribute__((matrix_type(rows, columns))), where \"rows\" specifies\n/// number of rows and \"columns\" specifies the number of columns.\nclass MatrixType : public Type, public llvm::FoldingSetNode {\nprotected:\n  friend class ASTContext;\n\n  /// The element type of the matrix.\n  QualType ElementType;\n\n  MatrixType(QualType ElementTy, QualType CanonElementTy);\n\n  MatrixType(TypeClass TypeClass, QualType ElementTy, QualType CanonElementTy,\n             const Expr *RowExpr = nullptr, const Expr *ColumnExpr = nullptr);\n\npublic:\n  /// Returns type of the elements being stored in the matrix\n  QualType getElementType() const { return ElementType; }\n\n  /// Valid elements types are the following:\n  /// * an integer type (as in C2x 6.2.5p19), but excluding enumerated types\n  ///   and _Bool\n  /// * the standard floating types float or double\n  /// * a half-precision floating point type, if one is supported on the target\n  static bool isValidElementType(QualType T) {\n    return T->isDependentType() ||\n           (T->isRealType() && !T->isBooleanType() && !T->isEnumeralType());\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ConstantMatrix ||\n           T->getTypeClass() == DependentSizedMatrix;\n  }\n};\n\n/// Represents a concrete matrix type with constant number of rows and columns\nclass ConstantMatrixType final : public MatrixType {\nprotected:\n  friend class ASTContext;\n\n  /// The element type of the matrix.\n  // FIXME: Appears to be unused? There is also MatrixType::ElementType...\n  QualType ElementType;\n\n  /// Number of rows and columns.\n  unsigned NumRows;\n  unsigned NumColumns;\n\n  static constexpr unsigned MaxElementsPerDimension = (1 << 20) - 1;\n\n  ConstantMatrixType(QualType MatrixElementType, unsigned NRows,\n                     unsigned NColumns, QualType CanonElementType);\n\n  ConstantMatrixType(TypeClass typeClass, QualType MatrixType, unsigned NRows,\n                     unsigned NColumns, QualType CanonElementType);\n\npublic:\n  /// Returns the number of rows in the matrix.\n  unsigned getNumRows() const { return NumRows; }\n\n  /// Returns the number of columns in the matrix.\n  unsigned getNumColumns() const { return NumColumns; }\n\n  /// Returns the number of elements required to embed the matrix into a vector.\n  unsigned getNumElementsFlattened() const {\n    return getNumRows() * getNumColumns();\n  }\n\n  /// Returns true if \\p NumElements is a valid matrix dimension.\n  static constexpr bool isDimensionValid(size_t NumElements) {\n    return NumElements > 0 && NumElements <= MaxElementsPerDimension;\n  }\n\n  /// Returns the maximum number of elements per dimension.\n  static constexpr unsigned getMaxElementsPerDimension() {\n    return MaxElementsPerDimension;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getElementType(), getNumRows(), getNumColumns(),\n            getTypeClass());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType ElementType,\n                      unsigned NumRows, unsigned NumColumns,\n                      TypeClass TypeClass) {\n    ID.AddPointer(ElementType.getAsOpaquePtr());\n    ID.AddInteger(NumRows);\n    ID.AddInteger(NumColumns);\n    ID.AddInteger(TypeClass);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ConstantMatrix;\n  }\n};\n\n/// Represents a matrix type where the type and the number of rows and columns\n/// is dependent on a template.\nclass DependentSizedMatrixType final : public MatrixType {\n  friend class ASTContext;\n\n  const ASTContext &Context;\n  Expr *RowExpr;\n  Expr *ColumnExpr;\n\n  SourceLocation loc;\n\n  DependentSizedMatrixType(const ASTContext &Context, QualType ElementType,\n                           QualType CanonicalType, Expr *RowExpr,\n                           Expr *ColumnExpr, SourceLocation loc);\n\npublic:\n  QualType getElementType() const { return ElementType; }\n  Expr *getRowExpr() const { return RowExpr; }\n  Expr *getColumnExpr() const { return ColumnExpr; }\n  SourceLocation getAttributeLoc() const { return loc; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentSizedMatrix;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, getElementType(), getRowExpr(), getColumnExpr());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      QualType ElementType, Expr *RowExpr, Expr *ColumnExpr);\n};\n\n/// FunctionType - C99 6.7.5.3 - Function Declarators.  This is the common base\n/// class of FunctionNoProtoType and FunctionProtoType.\nclass FunctionType : public Type {\n  // The type returned by the function.\n  QualType ResultType;\n\npublic:\n  /// Interesting information about a specific parameter that can't simply\n  /// be reflected in parameter's type. This is only used by FunctionProtoType\n  /// but is in FunctionType to make this class available during the\n  /// specification of the bases of FunctionProtoType.\n  ///\n  /// It makes sense to model language features this way when there's some\n  /// sort of parameter-specific override (such as an attribute) that\n  /// affects how the function is called.  For example, the ARC ns_consumed\n  /// attribute changes whether a parameter is passed at +0 (the default)\n  /// or +1 (ns_consumed).  This must be reflected in the function type,\n  /// but isn't really a change to the parameter type.\n  ///\n  /// One serious disadvantage of modelling language features this way is\n  /// that they generally do not work with language features that attempt\n  /// to destructure types.  For example, template argument deduction will\n  /// not be able to match a parameter declared as\n  ///   T (*)(U)\n  /// against an argument of type\n  ///   void (*)(__attribute__((ns_consumed)) id)\n  /// because the substitution of T=void, U=id into the former will\n  /// not produce the latter.\n  class ExtParameterInfo {\n    enum {\n      ABIMask = 0x0F,\n      IsConsumed = 0x10,\n      HasPassObjSize = 0x20,\n      IsNoEscape = 0x40,\n    };\n    unsigned char Data = 0;\n\n  public:\n    ExtParameterInfo() = default;\n\n    /// Return the ABI treatment of this parameter.\n    ParameterABI getABI() const { return ParameterABI(Data & ABIMask); }\n    ExtParameterInfo withABI(ParameterABI kind) const {\n      ExtParameterInfo copy = *this;\n      copy.Data = (copy.Data & ~ABIMask) | unsigned(kind);\n      return copy;\n    }\n\n    /// Is this parameter considered \"consumed\" by Objective-C ARC?\n    /// Consumed parameters must have retainable object type.\n    bool isConsumed() const { return (Data & IsConsumed); }\n    ExtParameterInfo withIsConsumed(bool consumed) const {\n      ExtParameterInfo copy = *this;\n      if (consumed)\n        copy.Data |= IsConsumed;\n      else\n        copy.Data &= ~IsConsumed;\n      return copy;\n    }\n\n    bool hasPassObjectSize() const { return Data & HasPassObjSize; }\n    ExtParameterInfo withHasPassObjectSize() const {\n      ExtParameterInfo Copy = *this;\n      Copy.Data |= HasPassObjSize;\n      return Copy;\n    }\n\n    bool isNoEscape() const { return Data & IsNoEscape; }\n    ExtParameterInfo withIsNoEscape(bool NoEscape) const {\n      ExtParameterInfo Copy = *this;\n      if (NoEscape)\n        Copy.Data |= IsNoEscape;\n      else\n        Copy.Data &= ~IsNoEscape;\n      return Copy;\n    }\n\n    unsigned char getOpaqueValue() const { return Data; }\n    static ExtParameterInfo getFromOpaqueValue(unsigned char data) {\n      ExtParameterInfo result;\n      result.Data = data;\n      return result;\n    }\n\n    friend bool operator==(ExtParameterInfo lhs, ExtParameterInfo rhs) {\n      return lhs.Data == rhs.Data;\n    }\n\n    friend bool operator!=(ExtParameterInfo lhs, ExtParameterInfo rhs) {\n      return lhs.Data != rhs.Data;\n    }\n  };\n\n  /// A class which abstracts out some details necessary for\n  /// making a call.\n  ///\n  /// It is not actually used directly for storing this information in\n  /// a FunctionType, although FunctionType does currently use the\n  /// same bit-pattern.\n  ///\n  // If you add a field (say Foo), other than the obvious places (both,\n  // constructors, compile failures), what you need to update is\n  // * Operator==\n  // * getFoo\n  // * withFoo\n  // * functionType. Add Foo, getFoo.\n  // * ASTContext::getFooType\n  // * ASTContext::mergeFunctionTypes\n  // * FunctionNoProtoType::Profile\n  // * FunctionProtoType::Profile\n  // * TypePrinter::PrintFunctionProto\n  // * AST read and write\n  // * Codegen\n  class ExtInfo {\n    friend class FunctionType;\n\n    // Feel free to rearrange or add bits, but if you go over 16, you'll need to\n    // adjust the Bits field below, and if you add bits, you'll need to adjust\n    // Type::FunctionTypeBitfields::ExtInfo as well.\n\n    // |  CC  |noreturn|produces|nocallersavedregs|regparm|nocfcheck|cmsenscall|\n    // |0 .. 4|   5    |    6   |       7         |8 .. 10|    11   |    12    |\n    //\n    // regparm is either 0 (no regparm attribute) or the regparm value+1.\n    enum { CallConvMask = 0x1F };\n    enum { NoReturnMask = 0x20 };\n    enum { ProducesResultMask = 0x40 };\n    enum { NoCallerSavedRegsMask = 0x80 };\n    enum {\n      RegParmMask =  0x700,\n      RegParmOffset = 8\n    };\n    enum { NoCfCheckMask = 0x800 };\n    enum { CmseNSCallMask = 0x1000 };\n    uint16_t Bits = CC_C;\n\n    ExtInfo(unsigned Bits) : Bits(static_cast<uint16_t>(Bits)) {}\n\n  public:\n    // Constructor with no defaults. Use this when you know that you\n    // have all the elements (when reading an AST file for example).\n    ExtInfo(bool noReturn, bool hasRegParm, unsigned regParm, CallingConv cc,\n            bool producesResult, bool noCallerSavedRegs, bool NoCfCheck,\n            bool cmseNSCall) {\n      assert((!hasRegParm || regParm < 7) && \"Invalid regparm value\");\n      Bits = ((unsigned)cc) | (noReturn ? NoReturnMask : 0) |\n             (producesResult ? ProducesResultMask : 0) |\n             (noCallerSavedRegs ? NoCallerSavedRegsMask : 0) |\n             (hasRegParm ? ((regParm + 1) << RegParmOffset) : 0) |\n             (NoCfCheck ? NoCfCheckMask : 0) |\n             (cmseNSCall ? CmseNSCallMask : 0);\n    }\n\n    // Constructor with all defaults. Use when for example creating a\n    // function known to use defaults.\n    ExtInfo() = default;\n\n    // Constructor with just the calling convention, which is an important part\n    // of the canonical type.\n    ExtInfo(CallingConv CC) : Bits(CC) {}\n\n    bool getNoReturn() const { return Bits & NoReturnMask; }\n    bool getProducesResult() const { return Bits & ProducesResultMask; }\n    bool getCmseNSCall() const { return Bits & CmseNSCallMask; }\n    bool getNoCallerSavedRegs() const { return Bits & NoCallerSavedRegsMask; }\n    bool getNoCfCheck() const { return Bits & NoCfCheckMask; }\n    bool getHasRegParm() const { return ((Bits & RegParmMask) >> RegParmOffset) != 0; }\n\n    unsigned getRegParm() const {\n      unsigned RegParm = (Bits & RegParmMask) >> RegParmOffset;\n      if (RegParm > 0)\n        --RegParm;\n      return RegParm;\n    }\n\n    CallingConv getCC() const { return CallingConv(Bits & CallConvMask); }\n\n    bool operator==(ExtInfo Other) const {\n      return Bits == Other.Bits;\n    }\n    bool operator!=(ExtInfo Other) const {\n      return Bits != Other.Bits;\n    }\n\n    // Note that we don't have setters. That is by design, use\n    // the following with methods instead of mutating these objects.\n\n    ExtInfo withNoReturn(bool noReturn) const {\n      if (noReturn)\n        return ExtInfo(Bits | NoReturnMask);\n      else\n        return ExtInfo(Bits & ~NoReturnMask);\n    }\n\n    ExtInfo withProducesResult(bool producesResult) const {\n      if (producesResult)\n        return ExtInfo(Bits | ProducesResultMask);\n      else\n        return ExtInfo(Bits & ~ProducesResultMask);\n    }\n\n    ExtInfo withCmseNSCall(bool cmseNSCall) const {\n      if (cmseNSCall)\n        return ExtInfo(Bits | CmseNSCallMask);\n      else\n        return ExtInfo(Bits & ~CmseNSCallMask);\n    }\n\n    ExtInfo withNoCallerSavedRegs(bool noCallerSavedRegs) const {\n      if (noCallerSavedRegs)\n        return ExtInfo(Bits | NoCallerSavedRegsMask);\n      else\n        return ExtInfo(Bits & ~NoCallerSavedRegsMask);\n    }\n\n    ExtInfo withNoCfCheck(bool noCfCheck) const {\n      if (noCfCheck)\n        return ExtInfo(Bits | NoCfCheckMask);\n      else\n        return ExtInfo(Bits & ~NoCfCheckMask);\n    }\n\n    ExtInfo withRegParm(unsigned RegParm) const {\n      assert(RegParm < 7 && \"Invalid regparm value\");\n      return ExtInfo((Bits & ~RegParmMask) |\n                     ((RegParm + 1) << RegParmOffset));\n    }\n\n    ExtInfo withCallingConv(CallingConv cc) const {\n      return ExtInfo((Bits & ~CallConvMask) | (unsigned) cc);\n    }\n\n    void Profile(llvm::FoldingSetNodeID &ID) const {\n      ID.AddInteger(Bits);\n    }\n  };\n\n  /// A simple holder for a QualType representing a type in an\n  /// exception specification. Unfortunately needed by FunctionProtoType\n  /// because TrailingObjects cannot handle repeated types.\n  struct ExceptionType { QualType Type; };\n\n  /// A simple holder for various uncommon bits which do not fit in\n  /// FunctionTypeBitfields. Aligned to alignof(void *) to maintain the\n  /// alignment of subsequent objects in TrailingObjects. You must update\n  /// hasExtraBitfields in FunctionProtoType after adding extra data here.\n  struct alignas(void *) FunctionTypeExtraBitfields {\n    /// The number of types in the exception specification.\n    /// A whole unsigned is not needed here and according to\n    /// [implimits] 8 bits would be enough here.\n    unsigned NumExceptionType;\n  };\n\nprotected:\n  FunctionType(TypeClass tc, QualType res, QualType Canonical,\n               TypeDependence Dependence, ExtInfo Info)\n      : Type(tc, Canonical, Dependence), ResultType(res) {\n    FunctionTypeBits.ExtInfo = Info.Bits;\n  }\n\n  Qualifiers getFastTypeQuals() const {\n    return Qualifiers::fromFastMask(FunctionTypeBits.FastTypeQuals);\n  }\n\npublic:\n  QualType getReturnType() const { return ResultType; }\n\n  bool getHasRegParm() const { return getExtInfo().getHasRegParm(); }\n  unsigned getRegParmType() const { return getExtInfo().getRegParm(); }\n\n  /// Determine whether this function type includes the GNU noreturn\n  /// attribute. The C++11 [[noreturn]] attribute does not affect the function\n  /// type.\n  bool getNoReturnAttr() const { return getExtInfo().getNoReturn(); }\n\n  bool getCmseNSCallAttr() const { return getExtInfo().getCmseNSCall(); }\n  CallingConv getCallConv() const { return getExtInfo().getCC(); }\n  ExtInfo getExtInfo() const { return ExtInfo(FunctionTypeBits.ExtInfo); }\n\n  static_assert((~Qualifiers::FastMask & Qualifiers::CVRMask) == 0,\n                \"Const, volatile and restrict are assumed to be a subset of \"\n                \"the fast qualifiers.\");\n\n  bool isConst() const { return getFastTypeQuals().hasConst(); }\n  bool isVolatile() const { return getFastTypeQuals().hasVolatile(); }\n  bool isRestrict() const { return getFastTypeQuals().hasRestrict(); }\n\n  /// Determine the type of an expression that calls a function of\n  /// this type.\n  QualType getCallResultType(const ASTContext &Context) const {\n    return getReturnType().getNonLValueExprType(Context);\n  }\n\n  static StringRef getNameForCallConv(CallingConv CC);\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == FunctionNoProto ||\n           T->getTypeClass() == FunctionProto;\n  }\n};\n\n/// Represents a K&R-style 'int foo()' function, which has\n/// no information available about its arguments.\nclass FunctionNoProtoType : public FunctionType, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  FunctionNoProtoType(QualType Result, QualType Canonical, ExtInfo Info)\n      : FunctionType(FunctionNoProto, Result, Canonical,\n                     Result->getDependence() &\n                         ~(TypeDependence::DependentInstantiation |\n                           TypeDependence::UnexpandedPack),\n                     Info) {}\n\npublic:\n  // No additional state past what FunctionType provides.\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getReturnType(), getExtInfo());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType ResultType,\n                      ExtInfo Info) {\n    Info.Profile(ID);\n    ID.AddPointer(ResultType.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == FunctionNoProto;\n  }\n};\n\n/// Represents a prototype with parameter type info, e.g.\n/// 'int foo(int)' or 'int foo(void)'.  'void' is represented as having no\n/// parameters, not as having a single void parameter. Such a type can have\n/// an exception specification, but this specification is not part of the\n/// canonical type. FunctionProtoType has several trailing objects, some of\n/// which optional. For more information about the trailing objects see\n/// the first comment inside FunctionProtoType.\nclass FunctionProtoType final\n    : public FunctionType,\n      public llvm::FoldingSetNode,\n      private llvm::TrailingObjects<\n          FunctionProtoType, QualType, SourceLocation,\n          FunctionType::FunctionTypeExtraBitfields, FunctionType::ExceptionType,\n          Expr *, FunctionDecl *, FunctionType::ExtParameterInfo, Qualifiers> {\n  friend class ASTContext; // ASTContext creates these.\n  friend TrailingObjects;\n\n  // FunctionProtoType is followed by several trailing objects, some of\n  // which optional. They are in order:\n  //\n  // * An array of getNumParams() QualType holding the parameter types.\n  //   Always present. Note that for the vast majority of FunctionProtoType,\n  //   these will be the only trailing objects.\n  //\n  // * Optionally if the function is variadic, the SourceLocation of the\n  //   ellipsis.\n  //\n  // * Optionally if some extra data is stored in FunctionTypeExtraBitfields\n  //   (see FunctionTypeExtraBitfields and FunctionTypeBitfields):\n  //   a single FunctionTypeExtraBitfields. Present if and only if\n  //   hasExtraBitfields() is true.\n  //\n  // * Optionally exactly one of:\n  //   * an array of getNumExceptions() ExceptionType,\n  //   * a single Expr *,\n  //   * a pair of FunctionDecl *,\n  //   * a single FunctionDecl *\n  //   used to store information about the various types of exception\n  //   specification. See getExceptionSpecSize for the details.\n  //\n  // * Optionally an array of getNumParams() ExtParameterInfo holding\n  //   an ExtParameterInfo for each of the parameters. Present if and\n  //   only if hasExtParameterInfos() is true.\n  //\n  // * Optionally a Qualifiers object to represent extra qualifiers that can't\n  //   be represented by FunctionTypeBitfields.FastTypeQuals. Present if and only\n  //   if hasExtQualifiers() is true.\n  //\n  // The optional FunctionTypeExtraBitfields has to be before the data\n  // related to the exception specification since it contains the number\n  // of exception types.\n  //\n  // We put the ExtParameterInfos last.  If all were equal, it would make\n  // more sense to put these before the exception specification, because\n  // it's much easier to skip past them compared to the elaborate switch\n  // required to skip the exception specification.  However, all is not\n  // equal; ExtParameterInfos are used to model very uncommon features,\n  // and it's better not to burden the more common paths.\n\npublic:\n  /// Holds information about the various types of exception specification.\n  /// ExceptionSpecInfo is not stored as such in FunctionProtoType but is\n  /// used to group together the various bits of information about the\n  /// exception specification.\n  struct ExceptionSpecInfo {\n    /// The kind of exception specification this is.\n    ExceptionSpecificationType Type = EST_None;\n\n    /// Explicitly-specified list of exception types.\n    ArrayRef<QualType> Exceptions;\n\n    /// Noexcept expression, if this is a computed noexcept specification.\n    Expr *NoexceptExpr = nullptr;\n\n    /// The function whose exception specification this is, for\n    /// EST_Unevaluated and EST_Uninstantiated.\n    FunctionDecl *SourceDecl = nullptr;\n\n    /// The function template whose exception specification this is instantiated\n    /// from, for EST_Uninstantiated.\n    FunctionDecl *SourceTemplate = nullptr;\n\n    ExceptionSpecInfo() = default;\n\n    ExceptionSpecInfo(ExceptionSpecificationType EST) : Type(EST) {}\n  };\n\n  /// Extra information about a function prototype. ExtProtoInfo is not\n  /// stored as such in FunctionProtoType but is used to group together\n  /// the various bits of extra information about a function prototype.\n  struct ExtProtoInfo {\n    FunctionType::ExtInfo ExtInfo;\n    bool Variadic : 1;\n    bool HasTrailingReturn : 1;\n    Qualifiers TypeQuals;\n    RefQualifierKind RefQualifier = RQ_None;\n    ExceptionSpecInfo ExceptionSpec;\n    const ExtParameterInfo *ExtParameterInfos = nullptr;\n    SourceLocation EllipsisLoc;\n\n    ExtProtoInfo() : Variadic(false), HasTrailingReturn(false) {}\n\n    ExtProtoInfo(CallingConv CC)\n        : ExtInfo(CC), Variadic(false), HasTrailingReturn(false) {}\n\n    ExtProtoInfo withExceptionSpec(const ExceptionSpecInfo &ESI) {\n      ExtProtoInfo Result(*this);\n      Result.ExceptionSpec = ESI;\n      return Result;\n    }\n  };\n\nprivate:\n  unsigned numTrailingObjects(OverloadToken<QualType>) const {\n    return getNumParams();\n  }\n\n  unsigned numTrailingObjects(OverloadToken<SourceLocation>) const {\n    return isVariadic();\n  }\n\n  unsigned numTrailingObjects(OverloadToken<FunctionTypeExtraBitfields>) const {\n    return hasExtraBitfields();\n  }\n\n  unsigned numTrailingObjects(OverloadToken<ExceptionType>) const {\n    return getExceptionSpecSize().NumExceptionType;\n  }\n\n  unsigned numTrailingObjects(OverloadToken<Expr *>) const {\n    return getExceptionSpecSize().NumExprPtr;\n  }\n\n  unsigned numTrailingObjects(OverloadToken<FunctionDecl *>) const {\n    return getExceptionSpecSize().NumFunctionDeclPtr;\n  }\n\n  unsigned numTrailingObjects(OverloadToken<ExtParameterInfo>) const {\n    return hasExtParameterInfos() ? getNumParams() : 0;\n  }\n\n  /// Determine whether there are any argument types that\n  /// contain an unexpanded parameter pack.\n  static bool containsAnyUnexpandedParameterPack(const QualType *ArgArray,\n                                                 unsigned numArgs) {\n    for (unsigned Idx = 0; Idx < numArgs; ++Idx)\n      if (ArgArray[Idx]->containsUnexpandedParameterPack())\n        return true;\n\n    return false;\n  }\n\n  FunctionProtoType(QualType result, ArrayRef<QualType> params,\n                    QualType canonical, const ExtProtoInfo &epi);\n\n  /// This struct is returned by getExceptionSpecSize and is used to\n  /// translate an ExceptionSpecificationType to the number and kind\n  /// of trailing objects related to the exception specification.\n  struct ExceptionSpecSizeHolder {\n    unsigned NumExceptionType;\n    unsigned NumExprPtr;\n    unsigned NumFunctionDeclPtr;\n  };\n\n  /// Return the number and kind of trailing objects\n  /// related to the exception specification.\n  static ExceptionSpecSizeHolder\n  getExceptionSpecSize(ExceptionSpecificationType EST, unsigned NumExceptions) {\n    switch (EST) {\n    case EST_None:\n    case EST_DynamicNone:\n    case EST_MSAny:\n    case EST_BasicNoexcept:\n    case EST_Unparsed:\n    case EST_NoThrow:\n      return {0, 0, 0};\n\n    case EST_Dynamic:\n      return {NumExceptions, 0, 0};\n\n    case EST_DependentNoexcept:\n    case EST_NoexceptFalse:\n    case EST_NoexceptTrue:\n      return {0, 1, 0};\n\n    case EST_Uninstantiated:\n      return {0, 0, 2};\n\n    case EST_Unevaluated:\n      return {0, 0, 1};\n    }\n    llvm_unreachable(\"bad exception specification kind\");\n  }\n\n  /// Return the number and kind of trailing objects\n  /// related to the exception specification.\n  ExceptionSpecSizeHolder getExceptionSpecSize() const {\n    return getExceptionSpecSize(getExceptionSpecType(), getNumExceptions());\n  }\n\n  /// Whether the trailing FunctionTypeExtraBitfields is present.\n  static bool hasExtraBitfields(ExceptionSpecificationType EST) {\n    // If the exception spec type is EST_Dynamic then we have > 0 exception\n    // types and the exact number is stored in FunctionTypeExtraBitfields.\n    return EST == EST_Dynamic;\n  }\n\n  /// Whether the trailing FunctionTypeExtraBitfields is present.\n  bool hasExtraBitfields() const {\n    return hasExtraBitfields(getExceptionSpecType());\n  }\n\n  bool hasExtQualifiers() const {\n    return FunctionTypeBits.HasExtQuals;\n  }\n\npublic:\n  unsigned getNumParams() const { return FunctionTypeBits.NumParams; }\n\n  QualType getParamType(unsigned i) const {\n    assert(i < getNumParams() && \"invalid parameter index\");\n    return param_type_begin()[i];\n  }\n\n  ArrayRef<QualType> getParamTypes() const {\n    return llvm::makeArrayRef(param_type_begin(), param_type_end());\n  }\n\n  ExtProtoInfo getExtProtoInfo() const {\n    ExtProtoInfo EPI;\n    EPI.ExtInfo = getExtInfo();\n    EPI.Variadic = isVariadic();\n    EPI.EllipsisLoc = getEllipsisLoc();\n    EPI.HasTrailingReturn = hasTrailingReturn();\n    EPI.ExceptionSpec = getExceptionSpecInfo();\n    EPI.TypeQuals = getMethodQuals();\n    EPI.RefQualifier = getRefQualifier();\n    EPI.ExtParameterInfos = getExtParameterInfosOrNull();\n    return EPI;\n  }\n\n  /// Get the kind of exception specification on this function.\n  ExceptionSpecificationType getExceptionSpecType() const {\n    return static_cast<ExceptionSpecificationType>(\n        FunctionTypeBits.ExceptionSpecType);\n  }\n\n  /// Return whether this function has any kind of exception spec.\n  bool hasExceptionSpec() const { return getExceptionSpecType() != EST_None; }\n\n  /// Return whether this function has a dynamic (throw) exception spec.\n  bool hasDynamicExceptionSpec() const {\n    return isDynamicExceptionSpec(getExceptionSpecType());\n  }\n\n  /// Return whether this function has a noexcept exception spec.\n  bool hasNoexceptExceptionSpec() const {\n    return isNoexceptExceptionSpec(getExceptionSpecType());\n  }\n\n  /// Return whether this function has a dependent exception spec.\n  bool hasDependentExceptionSpec() const;\n\n  /// Return whether this function has an instantiation-dependent exception\n  /// spec.\n  bool hasInstantiationDependentExceptionSpec() const;\n\n  /// Return all the available information about this type's exception spec.\n  ExceptionSpecInfo getExceptionSpecInfo() const {\n    ExceptionSpecInfo Result;\n    Result.Type = getExceptionSpecType();\n    if (Result.Type == EST_Dynamic) {\n      Result.Exceptions = exceptions();\n    } else if (isComputedNoexcept(Result.Type)) {\n      Result.NoexceptExpr = getNoexceptExpr();\n    } else if (Result.Type == EST_Uninstantiated) {\n      Result.SourceDecl = getExceptionSpecDecl();\n      Result.SourceTemplate = getExceptionSpecTemplate();\n    } else if (Result.Type == EST_Unevaluated) {\n      Result.SourceDecl = getExceptionSpecDecl();\n    }\n    return Result;\n  }\n\n  /// Return the number of types in the exception specification.\n  unsigned getNumExceptions() const {\n    return getExceptionSpecType() == EST_Dynamic\n               ? getTrailingObjects<FunctionTypeExtraBitfields>()\n                     ->NumExceptionType\n               : 0;\n  }\n\n  /// Return the ith exception type, where 0 <= i < getNumExceptions().\n  QualType getExceptionType(unsigned i) const {\n    assert(i < getNumExceptions() && \"Invalid exception number!\");\n    return exception_begin()[i];\n  }\n\n  /// Return the expression inside noexcept(expression), or a null pointer\n  /// if there is none (because the exception spec is not of this form).\n  Expr *getNoexceptExpr() const {\n    if (!isComputedNoexcept(getExceptionSpecType()))\n      return nullptr;\n    return *getTrailingObjects<Expr *>();\n  }\n\n  /// If this function type has an exception specification which hasn't\n  /// been determined yet (either because it has not been evaluated or because\n  /// it has not been instantiated), this is the function whose exception\n  /// specification is represented by this type.\n  FunctionDecl *getExceptionSpecDecl() const {\n    if (getExceptionSpecType() != EST_Uninstantiated &&\n        getExceptionSpecType() != EST_Unevaluated)\n      return nullptr;\n    return getTrailingObjects<FunctionDecl *>()[0];\n  }\n\n  /// If this function type has an uninstantiated exception\n  /// specification, this is the function whose exception specification\n  /// should be instantiated to find the exception specification for\n  /// this type.\n  FunctionDecl *getExceptionSpecTemplate() const {\n    if (getExceptionSpecType() != EST_Uninstantiated)\n      return nullptr;\n    return getTrailingObjects<FunctionDecl *>()[1];\n  }\n\n  /// Determine whether this function type has a non-throwing exception\n  /// specification.\n  CanThrowResult canThrow() const;\n\n  /// Determine whether this function type has a non-throwing exception\n  /// specification. If this depends on template arguments, returns\n  /// \\c ResultIfDependent.\n  bool isNothrow(bool ResultIfDependent = false) const {\n    return ResultIfDependent ? canThrow() != CT_Can : canThrow() == CT_Cannot;\n  }\n\n  /// Whether this function prototype is variadic.\n  bool isVariadic() const { return FunctionTypeBits.Variadic; }\n\n  SourceLocation getEllipsisLoc() const {\n    return isVariadic() ? *getTrailingObjects<SourceLocation>()\n                        : SourceLocation();\n  }\n\n  /// Determines whether this function prototype contains a\n  /// parameter pack at the end.\n  ///\n  /// A function template whose last parameter is a parameter pack can be\n  /// called with an arbitrary number of arguments, much like a variadic\n  /// function.\n  bool isTemplateVariadic() const;\n\n  /// Whether this function prototype has a trailing return type.\n  bool hasTrailingReturn() const { return FunctionTypeBits.HasTrailingReturn; }\n\n  Qualifiers getMethodQuals() const {\n    if (hasExtQualifiers())\n      return *getTrailingObjects<Qualifiers>();\n    else\n      return getFastTypeQuals();\n  }\n\n  /// Retrieve the ref-qualifier associated with this function type.\n  RefQualifierKind getRefQualifier() const {\n    return static_cast<RefQualifierKind>(FunctionTypeBits.RefQualifier);\n  }\n\n  using param_type_iterator = const QualType *;\n  using param_type_range = llvm::iterator_range<param_type_iterator>;\n\n  param_type_range param_types() const {\n    return param_type_range(param_type_begin(), param_type_end());\n  }\n\n  param_type_iterator param_type_begin() const {\n    return getTrailingObjects<QualType>();\n  }\n\n  param_type_iterator param_type_end() const {\n    return param_type_begin() + getNumParams();\n  }\n\n  using exception_iterator = const QualType *;\n\n  ArrayRef<QualType> exceptions() const {\n    return llvm::makeArrayRef(exception_begin(), exception_end());\n  }\n\n  exception_iterator exception_begin() const {\n    return reinterpret_cast<exception_iterator>(\n        getTrailingObjects<ExceptionType>());\n  }\n\n  exception_iterator exception_end() const {\n    return exception_begin() + getNumExceptions();\n  }\n\n  /// Is there any interesting extra information for any of the parameters\n  /// of this function type?\n  bool hasExtParameterInfos() const {\n    return FunctionTypeBits.HasExtParameterInfos;\n  }\n\n  ArrayRef<ExtParameterInfo> getExtParameterInfos() const {\n    assert(hasExtParameterInfos());\n    return ArrayRef<ExtParameterInfo>(getTrailingObjects<ExtParameterInfo>(),\n                                      getNumParams());\n  }\n\n  /// Return a pointer to the beginning of the array of extra parameter\n  /// information, if present, or else null if none of the parameters\n  /// carry it.  This is equivalent to getExtProtoInfo().ExtParameterInfos.\n  const ExtParameterInfo *getExtParameterInfosOrNull() const {\n    if (!hasExtParameterInfos())\n      return nullptr;\n    return getTrailingObjects<ExtParameterInfo>();\n  }\n\n  ExtParameterInfo getExtParameterInfo(unsigned I) const {\n    assert(I < getNumParams() && \"parameter index out of range\");\n    if (hasExtParameterInfos())\n      return getTrailingObjects<ExtParameterInfo>()[I];\n    return ExtParameterInfo();\n  }\n\n  ParameterABI getParameterABI(unsigned I) const {\n    assert(I < getNumParams() && \"parameter index out of range\");\n    if (hasExtParameterInfos())\n      return getTrailingObjects<ExtParameterInfo>()[I].getABI();\n    return ParameterABI::Ordinary;\n  }\n\n  bool isParamConsumed(unsigned I) const {\n    assert(I < getNumParams() && \"parameter index out of range\");\n    if (hasExtParameterInfos())\n      return getTrailingObjects<ExtParameterInfo>()[I].isConsumed();\n    return false;\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void printExceptionSpecification(raw_ostream &OS,\n                                   const PrintingPolicy &Policy) const;\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == FunctionProto;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Ctx);\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Result,\n                      param_type_iterator ArgTys, unsigned NumArgs,\n                      const ExtProtoInfo &EPI, const ASTContext &Context,\n                      bool Canonical);\n};\n\n/// Represents the dependent type named by a dependently-scoped\n/// typename using declaration, e.g.\n///   using typename Base<T>::foo;\n///\n/// Template instantiation turns these into the underlying type.\nclass UnresolvedUsingType : public Type {\n  friend class ASTContext; // ASTContext creates these.\n\n  UnresolvedUsingTypenameDecl *Decl;\n\n  UnresolvedUsingType(const UnresolvedUsingTypenameDecl *D)\n      : Type(UnresolvedUsing, QualType(),\n             TypeDependence::DependentInstantiation),\n        Decl(const_cast<UnresolvedUsingTypenameDecl *>(D)) {}\n\npublic:\n  UnresolvedUsingTypenameDecl *getDecl() const { return Decl; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == UnresolvedUsing;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    return Profile(ID, Decl);\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      UnresolvedUsingTypenameDecl *D) {\n    ID.AddPointer(D);\n  }\n};\n\nclass TypedefType : public Type {\n  TypedefNameDecl *Decl;\n\nprivate:\n  friend class ASTContext; // ASTContext creates these.\n\n  TypedefType(TypeClass tc, const TypedefNameDecl *D, QualType underlying,\n              QualType can);\n\npublic:\n  TypedefNameDecl *getDecl() const { return Decl; }\n\n  bool isSugared() const { return true; }\n  QualType desugar() const;\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Typedef; }\n};\n\n/// Sugar type that represents a type that was qualified by a qualifier written\n/// as a macro invocation.\nclass MacroQualifiedType : public Type {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType UnderlyingTy;\n  const IdentifierInfo *MacroII;\n\n  MacroQualifiedType(QualType UnderlyingTy, QualType CanonTy,\n                     const IdentifierInfo *MacroII)\n      : Type(MacroQualified, CanonTy, UnderlyingTy->getDependence()),\n        UnderlyingTy(UnderlyingTy), MacroII(MacroII) {\n    assert(isa<AttributedType>(UnderlyingTy) &&\n           \"Expected a macro qualified type to only wrap attributed types.\");\n  }\n\npublic:\n  const IdentifierInfo *getMacroIdentifier() const { return MacroII; }\n  QualType getUnderlyingType() const { return UnderlyingTy; }\n\n  /// Return this attributed type's modified type with no qualifiers attached to\n  /// it.\n  QualType getModifiedType() const;\n\n  bool isSugared() const { return true; }\n  QualType desugar() const;\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == MacroQualified;\n  }\n};\n\n/// Represents a `typeof` (or __typeof__) expression (a GCC extension).\nclass TypeOfExprType : public Type {\n  Expr *TOExpr;\n\nprotected:\n  friend class ASTContext; // ASTContext creates these.\n\n  TypeOfExprType(Expr *E, QualType can = QualType());\n\npublic:\n  Expr *getUnderlyingExpr() const { return TOExpr; }\n\n  /// Remove a single level of sugar.\n  QualType desugar() const;\n\n  /// Returns whether this type directly provides sugar.\n  bool isSugared() const;\n\n  static bool classof(const Type *T) { return T->getTypeClass() == TypeOfExpr; }\n};\n\n/// Internal representation of canonical, dependent\n/// `typeof(expr)` types.\n///\n/// This class is used internally by the ASTContext to manage\n/// canonical, dependent types, only. Clients will only see instances\n/// of this class via TypeOfExprType nodes.\nclass DependentTypeOfExprType\n  : public TypeOfExprType, public llvm::FoldingSetNode {\n  const ASTContext &Context;\n\npublic:\n  DependentTypeOfExprType(const ASTContext &Context, Expr *E)\n      : TypeOfExprType(E), Context(Context) {}\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, getUnderlyingExpr());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      Expr *E);\n};\n\n/// Represents `typeof(type)`, a GCC extension.\nclass TypeOfType : public Type {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType TOType;\n\n  TypeOfType(QualType T, QualType can)\n      : Type(TypeOf, can, T->getDependence()), TOType(T) {\n    assert(!isa<TypedefType>(can) && \"Invalid canonical type\");\n  }\n\npublic:\n  QualType getUnderlyingType() const { return TOType; }\n\n  /// Remove a single level of sugar.\n  QualType desugar() const { return getUnderlyingType(); }\n\n  /// Returns whether this type directly provides sugar.\n  bool isSugared() const { return true; }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == TypeOf; }\n};\n\n/// Represents the type `decltype(expr)` (C++11).\nclass DecltypeType : public Type {\n  Expr *E;\n  QualType UnderlyingType;\n\nprotected:\n  friend class ASTContext; // ASTContext creates these.\n\n  DecltypeType(Expr *E, QualType underlyingType, QualType can = QualType());\n\npublic:\n  Expr *getUnderlyingExpr() const { return E; }\n  QualType getUnderlyingType() const { return UnderlyingType; }\n\n  /// Remove a single level of sugar.\n  QualType desugar() const;\n\n  /// Returns whether this type directly provides sugar.\n  bool isSugared() const;\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Decltype; }\n};\n\n/// Internal representation of canonical, dependent\n/// decltype(expr) types.\n///\n/// This class is used internally by the ASTContext to manage\n/// canonical, dependent types, only. Clients will only see instances\n/// of this class via DecltypeType nodes.\nclass DependentDecltypeType : public DecltypeType, public llvm::FoldingSetNode {\n  const ASTContext &Context;\n\npublic:\n  DependentDecltypeType(const ASTContext &Context, Expr *E);\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, getUnderlyingExpr());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      Expr *E);\n};\n\n/// A unary type transform, which is a type constructed from another.\nclass UnaryTransformType : public Type {\npublic:\n  enum UTTKind {\n    EnumUnderlyingType\n  };\n\nprivate:\n  /// The untransformed type.\n  QualType BaseType;\n\n  /// The transformed type if not dependent, otherwise the same as BaseType.\n  QualType UnderlyingType;\n\n  UTTKind UKind;\n\nprotected:\n  friend class ASTContext;\n\n  UnaryTransformType(QualType BaseTy, QualType UnderlyingTy, UTTKind UKind,\n                     QualType CanonicalTy);\n\npublic:\n  bool isSugared() const { return !isDependentType(); }\n  QualType desugar() const { return UnderlyingType; }\n\n  QualType getUnderlyingType() const { return UnderlyingType; }\n  QualType getBaseType() const { return BaseType; }\n\n  UTTKind getUTTKind() const { return UKind; }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == UnaryTransform;\n  }\n};\n\n/// Internal representation of canonical, dependent\n/// __underlying_type(type) types.\n///\n/// This class is used internally by the ASTContext to manage\n/// canonical, dependent types, only. Clients will only see instances\n/// of this class via UnaryTransformType nodes.\nclass DependentUnaryTransformType : public UnaryTransformType,\n                                    public llvm::FoldingSetNode {\npublic:\n  DependentUnaryTransformType(const ASTContext &C, QualType BaseType,\n                              UTTKind UKind);\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getBaseType(), getUTTKind());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType BaseType,\n                      UTTKind UKind) {\n    ID.AddPointer(BaseType.getAsOpaquePtr());\n    ID.AddInteger((unsigned)UKind);\n  }\n};\n\nclass TagType : public Type {\n  friend class ASTReader;\n  template <class T> friend class serialization::AbstractTypeReader;\n\n  /// Stores the TagDecl associated with this type. The decl may point to any\n  /// TagDecl that declares the entity.\n  TagDecl *decl;\n\nprotected:\n  TagType(TypeClass TC, const TagDecl *D, QualType can);\n\npublic:\n  TagDecl *getDecl() const;\n\n  /// Determines whether this type is in the process of being defined.\n  bool isBeingDefined() const;\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Enum || T->getTypeClass() == Record;\n  }\n};\n\n/// A helper class that allows the use of isa/cast/dyncast\n/// to detect TagType objects of structs/unions/classes.\nclass RecordType : public TagType {\nprotected:\n  friend class ASTContext; // ASTContext creates these.\n\n  explicit RecordType(const RecordDecl *D)\n      : TagType(Record, reinterpret_cast<const TagDecl*>(D), QualType()) {}\n  explicit RecordType(TypeClass TC, RecordDecl *D)\n      : TagType(TC, reinterpret_cast<const TagDecl*>(D), QualType()) {}\n\npublic:\n  RecordDecl *getDecl() const {\n    return reinterpret_cast<RecordDecl*>(TagType::getDecl());\n  }\n\n  /// Recursively check all fields in the record for const-ness. If any field\n  /// is declared const, return true. Otherwise, return false.\n  bool hasConstFields() const;\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Record; }\n};\n\n/// A helper class that allows the use of isa/cast/dyncast\n/// to detect TagType objects of enums.\nclass EnumType : public TagType {\n  friend class ASTContext; // ASTContext creates these.\n\n  explicit EnumType(const EnumDecl *D)\n      : TagType(Enum, reinterpret_cast<const TagDecl*>(D), QualType()) {}\n\npublic:\n  EnumDecl *getDecl() const {\n    return reinterpret_cast<EnumDecl*>(TagType::getDecl());\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Enum; }\n};\n\n/// An attributed type is a type to which a type attribute has been applied.\n///\n/// The \"modified type\" is the fully-sugared type to which the attributed\n/// type was applied; generally it is not canonically equivalent to the\n/// attributed type. The \"equivalent type\" is the minimally-desugared type\n/// which the type is canonically equivalent to.\n///\n/// For example, in the following attributed type:\n///     int32_t __attribute__((vector_size(16)))\n///   - the modified type is the TypedefType for int32_t\n///   - the equivalent type is VectorType(16, int32_t)\n///   - the canonical type is VectorType(16, int)\nclass AttributedType : public Type, public llvm::FoldingSetNode {\npublic:\n  using Kind = attr::Kind;\n\nprivate:\n  friend class ASTContext; // ASTContext creates these\n\n  QualType ModifiedType;\n  QualType EquivalentType;\n\n  AttributedType(QualType canon, attr::Kind attrKind, QualType modified,\n                 QualType equivalent)\n      : Type(Attributed, canon, equivalent->getDependence()),\n        ModifiedType(modified), EquivalentType(equivalent) {\n    AttributedTypeBits.AttrKind = attrKind;\n  }\n\npublic:\n  Kind getAttrKind() const {\n    return static_cast<Kind>(AttributedTypeBits.AttrKind);\n  }\n\n  QualType getModifiedType() const { return ModifiedType; }\n  QualType getEquivalentType() const { return EquivalentType; }\n\n  bool isSugared() const { return true; }\n  QualType desugar() const { return getEquivalentType(); }\n\n  /// Does this attribute behave like a type qualifier?\n  ///\n  /// A type qualifier adjusts a type to provide specialized rules for\n  /// a specific object, like the standard const and volatile qualifiers.\n  /// This includes attributes controlling things like nullability,\n  /// address spaces, and ARC ownership.  The value of the object is still\n  /// largely described by the modified type.\n  ///\n  /// In contrast, many type attributes \"rewrite\" their modified type to\n  /// produce a fundamentally different type, not necessarily related in any\n  /// formalizable way to the original type.  For example, calling convention\n  /// and vector attributes are not simple type qualifiers.\n  ///\n  /// Type qualifiers are often, but not always, reflected in the canonical\n  /// type.\n  bool isQualifier() const;\n\n  bool isMSTypeSpec() const;\n\n  bool isCallingConv() const;\n\n  llvm::Optional<NullabilityKind> getImmediateNullability() const;\n\n  /// Retrieve the attribute kind corresponding to the given\n  /// nullability kind.\n  static Kind getNullabilityAttrKind(NullabilityKind kind) {\n    switch (kind) {\n    case NullabilityKind::NonNull:\n      return attr::TypeNonNull;\n\n    case NullabilityKind::Nullable:\n      return attr::TypeNullable;\n\n    case NullabilityKind::NullableResult:\n      return attr::TypeNullableResult;\n\n    case NullabilityKind::Unspecified:\n      return attr::TypeNullUnspecified;\n    }\n    llvm_unreachable(\"Unknown nullability kind.\");\n  }\n\n  /// Strip off the top-level nullability annotation on the given\n  /// type, if it's there.\n  ///\n  /// \\param T The type to strip. If the type is exactly an\n  /// AttributedType specifying nullability (without looking through\n  /// type sugar), the nullability is returned and this type changed\n  /// to the underlying modified type.\n  ///\n  /// \\returns the top-level nullability, if present.\n  static Optional<NullabilityKind> stripOuterNullability(QualType &T);\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getAttrKind(), ModifiedType, EquivalentType);\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, Kind attrKind,\n                      QualType modified, QualType equivalent) {\n    ID.AddInteger(attrKind);\n    ID.AddPointer(modified.getAsOpaquePtr());\n    ID.AddPointer(equivalent.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Attributed;\n  }\n};\n\nclass TemplateTypeParmType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these\n\n  // Helper data collector for canonical types.\n  struct CanonicalTTPTInfo {\n    unsigned Depth : 15;\n    unsigned ParameterPack : 1;\n    unsigned Index : 16;\n  };\n\n  union {\n    // Info for the canonical type.\n    CanonicalTTPTInfo CanTTPTInfo;\n\n    // Info for the non-canonical type.\n    TemplateTypeParmDecl *TTPDecl;\n  };\n\n  /// Build a non-canonical type.\n  TemplateTypeParmType(TemplateTypeParmDecl *TTPDecl, QualType Canon)\n      : Type(TemplateTypeParm, Canon,\n             TypeDependence::DependentInstantiation |\n                 (Canon->getDependence() & TypeDependence::UnexpandedPack)),\n        TTPDecl(TTPDecl) {}\n\n  /// Build the canonical type.\n  TemplateTypeParmType(unsigned D, unsigned I, bool PP)\n      : Type(TemplateTypeParm, QualType(this, 0),\n             TypeDependence::DependentInstantiation |\n                 (PP ? TypeDependence::UnexpandedPack : TypeDependence::None)) {\n    CanTTPTInfo.Depth = D;\n    CanTTPTInfo.Index = I;\n    CanTTPTInfo.ParameterPack = PP;\n  }\n\n  const CanonicalTTPTInfo& getCanTTPTInfo() const {\n    QualType Can = getCanonicalTypeInternal();\n    return Can->castAs<TemplateTypeParmType>()->CanTTPTInfo;\n  }\n\npublic:\n  unsigned getDepth() const { return getCanTTPTInfo().Depth; }\n  unsigned getIndex() const { return getCanTTPTInfo().Index; }\n  bool isParameterPack() const { return getCanTTPTInfo().ParameterPack; }\n\n  TemplateTypeParmDecl *getDecl() const {\n    return isCanonicalUnqualified() ? nullptr : TTPDecl;\n  }\n\n  IdentifierInfo *getIdentifier() const;\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getDepth(), getIndex(), isParameterPack(), getDecl());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, unsigned Depth,\n                      unsigned Index, bool ParameterPack,\n                      TemplateTypeParmDecl *TTPDecl) {\n    ID.AddInteger(Depth);\n    ID.AddInteger(Index);\n    ID.AddBoolean(ParameterPack);\n    ID.AddPointer(TTPDecl);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == TemplateTypeParm;\n  }\n};\n\n/// Represents the result of substituting a type for a template\n/// type parameter.\n///\n/// Within an instantiated template, all template type parameters have\n/// been replaced with these.  They are used solely to record that a\n/// type was originally written as a template type parameter;\n/// therefore they are never canonical.\nclass SubstTemplateTypeParmType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext;\n\n  // The original type parameter.\n  const TemplateTypeParmType *Replaced;\n\n  SubstTemplateTypeParmType(const TemplateTypeParmType *Param, QualType Canon)\n      : Type(SubstTemplateTypeParm, Canon, Canon->getDependence()),\n        Replaced(Param) {}\n\npublic:\n  /// Gets the template parameter that was substituted for.\n  const TemplateTypeParmType *getReplacedParameter() const {\n    return Replaced;\n  }\n\n  /// Gets the type that was substituted for the template\n  /// parameter.\n  QualType getReplacementType() const {\n    return getCanonicalTypeInternal();\n  }\n\n  bool isSugared() const { return true; }\n  QualType desugar() const { return getReplacementType(); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getReplacedParameter(), getReplacementType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      const TemplateTypeParmType *Replaced,\n                      QualType Replacement) {\n    ID.AddPointer(Replaced);\n    ID.AddPointer(Replacement.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == SubstTemplateTypeParm;\n  }\n};\n\n/// Represents the result of substituting a set of types for a template\n/// type parameter pack.\n///\n/// When a pack expansion in the source code contains multiple parameter packs\n/// and those parameter packs correspond to different levels of template\n/// parameter lists, this type node is used to represent a template type\n/// parameter pack from an outer level, which has already had its argument pack\n/// substituted but that still lives within a pack expansion that itself\n/// could not be instantiated. When actually performing a substitution into\n/// that pack expansion (e.g., when all template parameters have corresponding\n/// arguments), this type will be replaced with the \\c SubstTemplateTypeParmType\n/// at the current pack substitution index.\nclass SubstTemplateTypeParmPackType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext;\n\n  /// The original type parameter.\n  const TemplateTypeParmType *Replaced;\n\n  /// A pointer to the set of template arguments that this\n  /// parameter pack is instantiated with.\n  const TemplateArgument *Arguments;\n\n  SubstTemplateTypeParmPackType(const TemplateTypeParmType *Param,\n                                QualType Canon,\n                                const TemplateArgument &ArgPack);\n\npublic:\n  IdentifierInfo *getIdentifier() const { return Replaced->getIdentifier(); }\n\n  /// Gets the template parameter that was substituted for.\n  const TemplateTypeParmType *getReplacedParameter() const {\n    return Replaced;\n  }\n\n  unsigned getNumArgs() const {\n    return SubstTemplateTypeParmPackTypeBits.NumArgs;\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  TemplateArgument getArgumentPack() const;\n\n  void Profile(llvm::FoldingSetNodeID &ID);\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      const TemplateTypeParmType *Replaced,\n                      const TemplateArgument &ArgPack);\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == SubstTemplateTypeParmPack;\n  }\n};\n\n/// Common base class for placeholders for types that get replaced by\n/// placeholder type deduction: C++11 auto, C++14 decltype(auto), C++17 deduced\n/// class template types, and constrained type names.\n///\n/// These types are usually a placeholder for a deduced type. However, before\n/// the initializer is attached, or (usually) if the initializer is\n/// type-dependent, there is no deduced type and the type is canonical. In\n/// the latter case, it is also a dependent type.\nclass DeducedType : public Type {\nprotected:\n  DeducedType(TypeClass TC, QualType DeducedAsType,\n              TypeDependence ExtraDependence)\n      : Type(TC,\n             // FIXME: Retain the sugared deduced type?\n             DeducedAsType.isNull() ? QualType(this, 0)\n                                    : DeducedAsType.getCanonicalType(),\n             ExtraDependence | (DeducedAsType.isNull()\n                                    ? TypeDependence::None\n                                    : DeducedAsType->getDependence() &\n                                          ~TypeDependence::VariablyModified)) {}\n\npublic:\n  bool isSugared() const { return !isCanonicalUnqualified(); }\n  QualType desugar() const { return getCanonicalTypeInternal(); }\n\n  /// Get the type deduced for this placeholder type, or null if it's\n  /// either not been deduced or was deduced to a dependent type.\n  QualType getDeducedType() const {\n    return !isCanonicalUnqualified() ? getCanonicalTypeInternal() : QualType();\n  }\n  bool isDeduced() const {\n    return !isCanonicalUnqualified() || isDependentType();\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Auto ||\n           T->getTypeClass() == DeducedTemplateSpecialization;\n  }\n};\n\n/// Represents a C++11 auto or C++14 decltype(auto) type, possibly constrained\n/// by a type-constraint.\nclass alignas(8) AutoType : public DeducedType, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these\n\n  ConceptDecl *TypeConstraintConcept;\n\n  AutoType(QualType DeducedAsType, AutoTypeKeyword Keyword,\n           TypeDependence ExtraDependence, ConceptDecl *CD,\n           ArrayRef<TemplateArgument> TypeConstraintArgs);\n\n  const TemplateArgument *getArgBuffer() const {\n    return reinterpret_cast<const TemplateArgument*>(this+1);\n  }\n\n  TemplateArgument *getArgBuffer() {\n    return reinterpret_cast<TemplateArgument*>(this+1);\n  }\n\npublic:\n  /// Retrieve the template arguments.\n  const TemplateArgument *getArgs() const {\n    return getArgBuffer();\n  }\n\n  /// Retrieve the number of template arguments.\n  unsigned getNumArgs() const {\n    return AutoTypeBits.NumArgs;\n  }\n\n  const TemplateArgument &getArg(unsigned Idx) const; // in TemplateBase.h\n\n  ArrayRef<TemplateArgument> getTypeConstraintArguments() const {\n    return {getArgs(), getNumArgs()};\n  }\n\n  ConceptDecl *getTypeConstraintConcept() const {\n    return TypeConstraintConcept;\n  }\n\n  bool isConstrained() const {\n    return TypeConstraintConcept != nullptr;\n  }\n\n  bool isDecltypeAuto() const {\n    return getKeyword() == AutoTypeKeyword::DecltypeAuto;\n  }\n\n  AutoTypeKeyword getKeyword() const {\n    return (AutoTypeKeyword)AutoTypeBits.Keyword;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context) {\n    Profile(ID, Context, getDeducedType(), getKeyword(), isDependentType(),\n            getTypeConstraintConcept(), getTypeConstraintArguments());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      QualType Deduced, AutoTypeKeyword Keyword,\n                      bool IsDependent, ConceptDecl *CD,\n                      ArrayRef<TemplateArgument> Arguments);\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Auto;\n  }\n};\n\n/// Represents a C++17 deduced template specialization type.\nclass DeducedTemplateSpecializationType : public DeducedType,\n                                          public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these\n\n  /// The name of the template whose arguments will be deduced.\n  TemplateName Template;\n\n  DeducedTemplateSpecializationType(TemplateName Template,\n                                    QualType DeducedAsType,\n                                    bool IsDeducedAsDependent)\n      : DeducedType(DeducedTemplateSpecialization, DeducedAsType,\n                    toTypeDependence(Template.getDependence()) |\n                        (IsDeducedAsDependent\n                             ? TypeDependence::DependentInstantiation\n                             : TypeDependence::None)),\n        Template(Template) {}\n\npublic:\n  /// Retrieve the name of the template that we are deducing.\n  TemplateName getTemplateName() const { return Template;}\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getTemplateName(), getDeducedType(), isDependentType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, TemplateName Template,\n                      QualType Deduced, bool IsDependent) {\n    Template.Profile(ID);\n    ID.AddPointer(Deduced.getAsOpaquePtr());\n    ID.AddBoolean(IsDependent);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DeducedTemplateSpecialization;\n  }\n};\n\n/// Represents a type template specialization; the template\n/// must be a class template, a type alias template, or a template\n/// template parameter.  A template which cannot be resolved to one of\n/// these, e.g. because it is written with a dependent scope\n/// specifier, is instead represented as a\n/// @c DependentTemplateSpecializationType.\n///\n/// A non-dependent template specialization type is always \"sugar\",\n/// typically for a \\c RecordType.  For example, a class template\n/// specialization type of \\c vector<int> will refer to a tag type for\n/// the instantiation \\c std::vector<int, std::allocator<int>>\n///\n/// Template specializations are dependent if either the template or\n/// any of the template arguments are dependent, in which case the\n/// type may also be canonical.\n///\n/// Instances of this type are allocated with a trailing array of\n/// TemplateArguments, followed by a QualType representing the\n/// non-canonical aliased type when the template is a type alias\n/// template.\nclass alignas(8) TemplateSpecializationType\n    : public Type,\n      public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these\n\n  /// The name of the template being specialized.  This is\n  /// either a TemplateName::Template (in which case it is a\n  /// ClassTemplateDecl*, a TemplateTemplateParmDecl*, or a\n  /// TypeAliasTemplateDecl*), a\n  /// TemplateName::SubstTemplateTemplateParmPack, or a\n  /// TemplateName::SubstTemplateTemplateParm (in which case the\n  /// replacement must, recursively, be one of these).\n  TemplateName Template;\n\n  TemplateSpecializationType(TemplateName T,\n                             ArrayRef<TemplateArgument> Args,\n                             QualType Canon,\n                             QualType Aliased);\n\npublic:\n  /// Determine whether any of the given template arguments are dependent.\n  ///\n  /// The converted arguments should be supplied when known; whether an\n  /// argument is dependent can depend on the conversions performed on it\n  /// (for example, a 'const int' passed as a template argument might be\n  /// dependent if the parameter is a reference but non-dependent if the\n  /// parameter is an int).\n  ///\n  /// Note that the \\p Args parameter is unused: this is intentional, to remind\n  /// the caller that they need to pass in the converted arguments, not the\n  /// specified arguments.\n  static bool\n  anyDependentTemplateArguments(ArrayRef<TemplateArgumentLoc> Args,\n                                ArrayRef<TemplateArgument> Converted);\n  static bool\n  anyDependentTemplateArguments(const TemplateArgumentListInfo &,\n                                ArrayRef<TemplateArgument> Converted);\n  static bool anyInstantiationDependentTemplateArguments(\n      ArrayRef<TemplateArgumentLoc> Args);\n\n  /// True if this template specialization type matches a current\n  /// instantiation in the context in which it is found.\n  bool isCurrentInstantiation() const {\n    return isa<InjectedClassNameType>(getCanonicalTypeInternal());\n  }\n\n  /// Determine if this template specialization type is for a type alias\n  /// template that has been substituted.\n  ///\n  /// Nearly every template specialization type whose template is an alias\n  /// template will be substituted. However, this is not the case when\n  /// the specialization contains a pack expansion but the template alias\n  /// does not have a corresponding parameter pack, e.g.,\n  ///\n  /// \\code\n  /// template<typename T, typename U, typename V> struct S;\n  /// template<typename T, typename U> using A = S<T, int, U>;\n  /// template<typename... Ts> struct X {\n  ///   typedef A<Ts...> type; // not a type alias\n  /// };\n  /// \\endcode\n  bool isTypeAlias() const { return TemplateSpecializationTypeBits.TypeAlias; }\n\n  /// Get the aliased type, if this is a specialization of a type alias\n  /// template.\n  QualType getAliasedType() const {\n    assert(isTypeAlias() && \"not a type alias template specialization\");\n    return *reinterpret_cast<const QualType*>(end());\n  }\n\n  using iterator = const TemplateArgument *;\n\n  iterator begin() const { return getArgs(); }\n  iterator end() const; // defined inline in TemplateBase.h\n\n  /// Retrieve the name of the template that we are specializing.\n  TemplateName getTemplateName() const { return Template; }\n\n  /// Retrieve the template arguments.\n  const TemplateArgument *getArgs() const {\n    return reinterpret_cast<const TemplateArgument *>(this + 1);\n  }\n\n  /// Retrieve the number of template arguments.\n  unsigned getNumArgs() const {\n    return TemplateSpecializationTypeBits.NumArgs;\n  }\n\n  /// Retrieve a specific template argument as a type.\n  /// \\pre \\c isArgType(Arg)\n  const TemplateArgument &getArg(unsigned Idx) const; // in TemplateBase.h\n\n  ArrayRef<TemplateArgument> template_arguments() const {\n    return {getArgs(), getNumArgs()};\n  }\n\n  bool isSugared() const {\n    return !isDependentType() || isCurrentInstantiation() || isTypeAlias();\n  }\n\n  QualType desugar() const {\n    return isTypeAlias() ? getAliasedType() : getCanonicalTypeInternal();\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Ctx) {\n    Profile(ID, Template, template_arguments(), Ctx);\n    if (isTypeAlias())\n      getAliasedType().Profile(ID);\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, TemplateName T,\n                      ArrayRef<TemplateArgument> Args,\n                      const ASTContext &Context);\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == TemplateSpecialization;\n  }\n};\n\n/// Print a template argument list, including the '<' and '>'\n/// enclosing the template arguments.\nvoid printTemplateArgumentList(raw_ostream &OS,\n                               ArrayRef<TemplateArgument> Args,\n                               const PrintingPolicy &Policy,\n                               const TemplateParameterList *TPL = nullptr);\n\nvoid printTemplateArgumentList(raw_ostream &OS,\n                               ArrayRef<TemplateArgumentLoc> Args,\n                               const PrintingPolicy &Policy,\n                               const TemplateParameterList *TPL = nullptr);\n\nvoid printTemplateArgumentList(raw_ostream &OS,\n                               const TemplateArgumentListInfo &Args,\n                               const PrintingPolicy &Policy,\n                               const TemplateParameterList *TPL = nullptr);\n\n/// The injected class name of a C++ class template or class\n/// template partial specialization.  Used to record that a type was\n/// spelled with a bare identifier rather than as a template-id; the\n/// equivalent for non-templated classes is just RecordType.\n///\n/// Injected class name types are always dependent.  Template\n/// instantiation turns these into RecordTypes.\n///\n/// Injected class name types are always canonical.  This works\n/// because it is impossible to compare an injected class name type\n/// with the corresponding non-injected template type, for the same\n/// reason that it is impossible to directly compare template\n/// parameters from different dependent contexts: injected class name\n/// types can only occur within the scope of a particular templated\n/// declaration, and within that scope every template specialization\n/// will canonicalize to the injected class name (when appropriate\n/// according to the rules of the language).\nclass InjectedClassNameType : public Type {\n  friend class ASTContext; // ASTContext creates these.\n  friend class ASTNodeImporter;\n  friend class ASTReader; // FIXME: ASTContext::getInjectedClassNameType is not\n                          // currently suitable for AST reading, too much\n                          // interdependencies.\n  template <class T> friend class serialization::AbstractTypeReader;\n\n  CXXRecordDecl *Decl;\n\n  /// The template specialization which this type represents.\n  /// For example, in\n  ///   template <class T> class A { ... };\n  /// this is A<T>, whereas in\n  ///   template <class X, class Y> class A<B<X,Y> > { ... };\n  /// this is A<B<X,Y> >.\n  ///\n  /// It is always unqualified, always a template specialization type,\n  /// and always dependent.\n  QualType InjectedType;\n\n  InjectedClassNameType(CXXRecordDecl *D, QualType TST)\n      : Type(InjectedClassName, QualType(),\n             TypeDependence::DependentInstantiation),\n        Decl(D), InjectedType(TST) {\n    assert(isa<TemplateSpecializationType>(TST));\n    assert(!TST.hasQualifiers());\n    assert(TST->isDependentType());\n  }\n\npublic:\n  QualType getInjectedSpecializationType() const { return InjectedType; }\n\n  const TemplateSpecializationType *getInjectedTST() const {\n    return cast<TemplateSpecializationType>(InjectedType.getTypePtr());\n  }\n\n  TemplateName getTemplateName() const {\n    return getInjectedTST()->getTemplateName();\n  }\n\n  CXXRecordDecl *getDecl() const;\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == InjectedClassName;\n  }\n};\n\n/// The kind of a tag type.\nenum TagTypeKind {\n  /// The \"struct\" keyword.\n  TTK_Struct,\n\n  /// The \"__interface\" keyword.\n  TTK_Interface,\n\n  /// The \"union\" keyword.\n  TTK_Union,\n\n  /// The \"class\" keyword.\n  TTK_Class,\n\n  /// The \"enum\" keyword.\n  TTK_Enum\n};\n\n/// The elaboration keyword that precedes a qualified type name or\n/// introduces an elaborated-type-specifier.\nenum ElaboratedTypeKeyword {\n  /// The \"struct\" keyword introduces the elaborated-type-specifier.\n  ETK_Struct,\n\n  /// The \"__interface\" keyword introduces the elaborated-type-specifier.\n  ETK_Interface,\n\n  /// The \"union\" keyword introduces the elaborated-type-specifier.\n  ETK_Union,\n\n  /// The \"class\" keyword introduces the elaborated-type-specifier.\n  ETK_Class,\n\n  /// The \"enum\" keyword introduces the elaborated-type-specifier.\n  ETK_Enum,\n\n  /// The \"typename\" keyword precedes the qualified type name, e.g.,\n  /// \\c typename T::type.\n  ETK_Typename,\n\n  /// No keyword precedes the qualified type name.\n  ETK_None\n};\n\n/// A helper class for Type nodes having an ElaboratedTypeKeyword.\n/// The keyword in stored in the free bits of the base class.\n/// Also provides a few static helpers for converting and printing\n/// elaborated type keyword and tag type kind enumerations.\nclass TypeWithKeyword : public Type {\nprotected:\n  TypeWithKeyword(ElaboratedTypeKeyword Keyword, TypeClass tc,\n                  QualType Canonical, TypeDependence Dependence)\n      : Type(tc, Canonical, Dependence) {\n    TypeWithKeywordBits.Keyword = Keyword;\n  }\n\npublic:\n  ElaboratedTypeKeyword getKeyword() const {\n    return static_cast<ElaboratedTypeKeyword>(TypeWithKeywordBits.Keyword);\n  }\n\n  /// Converts a type specifier (DeclSpec::TST) into an elaborated type keyword.\n  static ElaboratedTypeKeyword getKeywordForTypeSpec(unsigned TypeSpec);\n\n  /// Converts a type specifier (DeclSpec::TST) into a tag type kind.\n  /// It is an error to provide a type specifier which *isn't* a tag kind here.\n  static TagTypeKind getTagTypeKindForTypeSpec(unsigned TypeSpec);\n\n  /// Converts a TagTypeKind into an elaborated type keyword.\n  static ElaboratedTypeKeyword getKeywordForTagTypeKind(TagTypeKind Tag);\n\n  /// Converts an elaborated type keyword into a TagTypeKind.\n  /// It is an error to provide an elaborated type keyword\n  /// which *isn't* a tag kind here.\n  static TagTypeKind getTagTypeKindForKeyword(ElaboratedTypeKeyword Keyword);\n\n  static bool KeywordIsTagTypeKind(ElaboratedTypeKeyword Keyword);\n\n  static StringRef getKeywordName(ElaboratedTypeKeyword Keyword);\n\n  static StringRef getTagTypeKindName(TagTypeKind Kind) {\n    return getKeywordName(getKeywordForTagTypeKind(Kind));\n  }\n\n  class CannotCastToThisType {};\n  static CannotCastToThisType classof(const Type *);\n};\n\n/// Represents a type that was referred to using an elaborated type\n/// keyword, e.g., struct S, or via a qualified name, e.g., N::M::type,\n/// or both.\n///\n/// This type is used to keep track of a type name as written in the\n/// source code, including tag keywords and any nested-name-specifiers.\n/// The type itself is always \"sugar\", used to express what was written\n/// in the source code but containing no additional semantic information.\nclass ElaboratedType final\n    : public TypeWithKeyword,\n      public llvm::FoldingSetNode,\n      private llvm::TrailingObjects<ElaboratedType, TagDecl *> {\n  friend class ASTContext; // ASTContext creates these\n  friend TrailingObjects;\n\n  /// The nested name specifier containing the qualifier.\n  NestedNameSpecifier *NNS;\n\n  /// The type that this qualified name refers to.\n  QualType NamedType;\n\n  /// The (re)declaration of this tag type owned by this occurrence is stored\n  /// as a trailing object if there is one. Use getOwnedTagDecl to obtain\n  /// it, or obtain a null pointer if there is none.\n\n  ElaboratedType(ElaboratedTypeKeyword Keyword, NestedNameSpecifier *NNS,\n                 QualType NamedType, QualType CanonType, TagDecl *OwnedTagDecl)\n      : TypeWithKeyword(Keyword, Elaborated, CanonType,\n                        // Any semantic dependence on the qualifier will have\n                        // been incorporated into NamedType. We still need to\n                        // track syntactic (instantiation / error / pack)\n                        // dependence on the qualifier.\n                        NamedType->getDependence() |\n                            (NNS ? toSyntacticDependence(\n                                       toTypeDependence(NNS->getDependence()))\n                                 : TypeDependence::None)),\n        NNS(NNS), NamedType(NamedType) {\n    ElaboratedTypeBits.HasOwnedTagDecl = false;\n    if (OwnedTagDecl) {\n      ElaboratedTypeBits.HasOwnedTagDecl = true;\n      *getTrailingObjects<TagDecl *>() = OwnedTagDecl;\n    }\n    assert(!(Keyword == ETK_None && NNS == nullptr) &&\n           \"ElaboratedType cannot have elaborated type keyword \"\n           \"and name qualifier both null.\");\n  }\n\npublic:\n  /// Retrieve the qualification on this type.\n  NestedNameSpecifier *getQualifier() const { return NNS; }\n\n  /// Retrieve the type named by the qualified-id.\n  QualType getNamedType() const { return NamedType; }\n\n  /// Remove a single level of sugar.\n  QualType desugar() const { return getNamedType(); }\n\n  /// Returns whether this type directly provides sugar.\n  bool isSugared() const { return true; }\n\n  /// Return the (re)declaration of this type owned by this occurrence of this\n  /// type, or nullptr if there is none.\n  TagDecl *getOwnedTagDecl() const {\n    return ElaboratedTypeBits.HasOwnedTagDecl ? *getTrailingObjects<TagDecl *>()\n                                              : nullptr;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getKeyword(), NNS, NamedType, getOwnedTagDecl());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, ElaboratedTypeKeyword Keyword,\n                      NestedNameSpecifier *NNS, QualType NamedType,\n                      TagDecl *OwnedTagDecl) {\n    ID.AddInteger(Keyword);\n    ID.AddPointer(NNS);\n    NamedType.Profile(ID);\n    ID.AddPointer(OwnedTagDecl);\n  }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == Elaborated; }\n};\n\n/// Represents a qualified type name for which the type name is\n/// dependent.\n///\n/// DependentNameType represents a class of dependent types that involve a\n/// possibly dependent nested-name-specifier (e.g., \"T::\") followed by a\n/// name of a type. The DependentNameType may start with a \"typename\" (for a\n/// typename-specifier), \"class\", \"struct\", \"union\", or \"enum\" (for a\n/// dependent elaborated-type-specifier), or nothing (in contexts where we\n/// know that we must be referring to a type, e.g., in a base class specifier).\n/// Typically the nested-name-specifier is dependent, but in MSVC compatibility\n/// mode, this type is used with non-dependent names to delay name lookup until\n/// instantiation.\nclass DependentNameType : public TypeWithKeyword, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these\n\n  /// The nested name specifier containing the qualifier.\n  NestedNameSpecifier *NNS;\n\n  /// The type that this typename specifier refers to.\n  const IdentifierInfo *Name;\n\n  DependentNameType(ElaboratedTypeKeyword Keyword, NestedNameSpecifier *NNS,\n                    const IdentifierInfo *Name, QualType CanonType)\n      : TypeWithKeyword(Keyword, DependentName, CanonType,\n                        TypeDependence::DependentInstantiation |\n                            toTypeDependence(NNS->getDependence())),\n        NNS(NNS), Name(Name) {}\n\npublic:\n  /// Retrieve the qualification on this type.\n  NestedNameSpecifier *getQualifier() const { return NNS; }\n\n  /// Retrieve the type named by the typename specifier as an identifier.\n  ///\n  /// This routine will return a non-NULL identifier pointer when the\n  /// form of the original typename was terminated by an identifier,\n  /// e.g., \"typename T::type\".\n  const IdentifierInfo *getIdentifier() const {\n    return Name;\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getKeyword(), NNS, Name);\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, ElaboratedTypeKeyword Keyword,\n                      NestedNameSpecifier *NNS, const IdentifierInfo *Name) {\n    ID.AddInteger(Keyword);\n    ID.AddPointer(NNS);\n    ID.AddPointer(Name);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentName;\n  }\n};\n\n/// Represents a template specialization type whose template cannot be\n/// resolved, e.g.\n///   A<T>::template B<T>\nclass alignas(8) DependentTemplateSpecializationType\n    : public TypeWithKeyword,\n      public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these\n\n  /// The nested name specifier containing the qualifier.\n  NestedNameSpecifier *NNS;\n\n  /// The identifier of the template.\n  const IdentifierInfo *Name;\n\n  DependentTemplateSpecializationType(ElaboratedTypeKeyword Keyword,\n                                      NestedNameSpecifier *NNS,\n                                      const IdentifierInfo *Name,\n                                      ArrayRef<TemplateArgument> Args,\n                                      QualType Canon);\n\n  const TemplateArgument *getArgBuffer() const {\n    return reinterpret_cast<const TemplateArgument*>(this+1);\n  }\n\n  TemplateArgument *getArgBuffer() {\n    return reinterpret_cast<TemplateArgument*>(this+1);\n  }\n\npublic:\n  NestedNameSpecifier *getQualifier() const { return NNS; }\n  const IdentifierInfo *getIdentifier() const { return Name; }\n\n  /// Retrieve the template arguments.\n  const TemplateArgument *getArgs() const {\n    return getArgBuffer();\n  }\n\n  /// Retrieve the number of template arguments.\n  unsigned getNumArgs() const {\n    return DependentTemplateSpecializationTypeBits.NumArgs;\n  }\n\n  const TemplateArgument &getArg(unsigned Idx) const; // in TemplateBase.h\n\n  ArrayRef<TemplateArgument> template_arguments() const {\n    return {getArgs(), getNumArgs()};\n  }\n\n  using iterator = const TemplateArgument *;\n\n  iterator begin() const { return getArgs(); }\n  iterator end() const; // inline in TemplateBase.h\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context) {\n    Profile(ID, Context, getKeyword(), NNS, Name, {getArgs(), getNumArgs()});\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      const ASTContext &Context,\n                      ElaboratedTypeKeyword Keyword,\n                      NestedNameSpecifier *Qualifier,\n                      const IdentifierInfo *Name,\n                      ArrayRef<TemplateArgument> Args);\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentTemplateSpecialization;\n  }\n};\n\n/// Represents a pack expansion of types.\n///\n/// Pack expansions are part of C++11 variadic templates. A pack\n/// expansion contains a pattern, which itself contains one or more\n/// \"unexpanded\" parameter packs. When instantiated, a pack expansion\n/// produces a series of types, each instantiated from the pattern of\n/// the expansion, where the Ith instantiation of the pattern uses the\n/// Ith arguments bound to each of the unexpanded parameter packs. The\n/// pack expansion is considered to \"expand\" these unexpanded\n/// parameter packs.\n///\n/// \\code\n/// template<typename ...Types> struct tuple;\n///\n/// template<typename ...Types>\n/// struct tuple_of_references {\n///   typedef tuple<Types&...> type;\n/// };\n/// \\endcode\n///\n/// Here, the pack expansion \\c Types&... is represented via a\n/// PackExpansionType whose pattern is Types&.\nclass PackExpansionType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these\n\n  /// The pattern of the pack expansion.\n  QualType Pattern;\n\n  PackExpansionType(QualType Pattern, QualType Canon,\n                    Optional<unsigned> NumExpansions)\n      : Type(PackExpansion, Canon,\n             (Pattern->getDependence() | TypeDependence::Dependent |\n              TypeDependence::Instantiation) &\n                 ~TypeDependence::UnexpandedPack),\n        Pattern(Pattern) {\n    PackExpansionTypeBits.NumExpansions =\n        NumExpansions ? *NumExpansions + 1 : 0;\n  }\n\npublic:\n  /// Retrieve the pattern of this pack expansion, which is the\n  /// type that will be repeatedly instantiated when instantiating the\n  /// pack expansion itself.\n  QualType getPattern() const { return Pattern; }\n\n  /// Retrieve the number of expansions that this pack expansion will\n  /// generate, if known.\n  Optional<unsigned> getNumExpansions() const {\n    if (PackExpansionTypeBits.NumExpansions)\n      return PackExpansionTypeBits.NumExpansions - 1;\n    return None;\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getPattern(), getNumExpansions());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType Pattern,\n                      Optional<unsigned> NumExpansions) {\n    ID.AddPointer(Pattern.getAsOpaquePtr());\n    ID.AddBoolean(NumExpansions.hasValue());\n    if (NumExpansions)\n      ID.AddInteger(*NumExpansions);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == PackExpansion;\n  }\n};\n\n/// This class wraps the list of protocol qualifiers. For types that can\n/// take ObjC protocol qualifers, they can subclass this class.\ntemplate <class T>\nclass ObjCProtocolQualifiers {\nprotected:\n  ObjCProtocolQualifiers() = default;\n\n  ObjCProtocolDecl * const *getProtocolStorage() const {\n    return const_cast<ObjCProtocolQualifiers*>(this)->getProtocolStorage();\n  }\n\n  ObjCProtocolDecl **getProtocolStorage() {\n    return static_cast<T*>(this)->getProtocolStorageImpl();\n  }\n\n  void setNumProtocols(unsigned N) {\n    static_cast<T*>(this)->setNumProtocolsImpl(N);\n  }\n\n  void initialize(ArrayRef<ObjCProtocolDecl *> protocols) {\n    setNumProtocols(protocols.size());\n    assert(getNumProtocols() == protocols.size() &&\n           \"bitfield overflow in protocol count\");\n    if (!protocols.empty())\n      memcpy(getProtocolStorage(), protocols.data(),\n             protocols.size() * sizeof(ObjCProtocolDecl*));\n  }\n\npublic:\n  using qual_iterator = ObjCProtocolDecl * const *;\n  using qual_range = llvm::iterator_range<qual_iterator>;\n\n  qual_range quals() const { return qual_range(qual_begin(), qual_end()); }\n  qual_iterator qual_begin() const { return getProtocolStorage(); }\n  qual_iterator qual_end() const { return qual_begin() + getNumProtocols(); }\n\n  bool qual_empty() const { return getNumProtocols() == 0; }\n\n  /// Return the number of qualifying protocols in this type, or 0 if\n  /// there are none.\n  unsigned getNumProtocols() const {\n    return static_cast<const T*>(this)->getNumProtocolsImpl();\n  }\n\n  /// Fetch a protocol by index.\n  ObjCProtocolDecl *getProtocol(unsigned I) const {\n    assert(I < getNumProtocols() && \"Out-of-range protocol access\");\n    return qual_begin()[I];\n  }\n\n  /// Retrieve all of the protocol qualifiers.\n  ArrayRef<ObjCProtocolDecl *> getProtocols() const {\n    return ArrayRef<ObjCProtocolDecl *>(qual_begin(), getNumProtocols());\n  }\n};\n\n/// Represents a type parameter type in Objective C. It can take\n/// a list of protocols.\nclass ObjCTypeParamType : public Type,\n                          public ObjCProtocolQualifiers<ObjCTypeParamType>,\n                          public llvm::FoldingSetNode {\n  friend class ASTContext;\n  friend class ObjCProtocolQualifiers<ObjCTypeParamType>;\n\n  /// The number of protocols stored on this type.\n  unsigned NumProtocols : 6;\n\n  ObjCTypeParamDecl *OTPDecl;\n\n  /// The protocols are stored after the ObjCTypeParamType node. In the\n  /// canonical type, the list of protocols are sorted alphabetically\n  /// and uniqued.\n  ObjCProtocolDecl **getProtocolStorageImpl();\n\n  /// Return the number of qualifying protocols in this interface type,\n  /// or 0 if there are none.\n  unsigned getNumProtocolsImpl() const {\n    return NumProtocols;\n  }\n\n  void setNumProtocolsImpl(unsigned N) {\n    NumProtocols = N;\n  }\n\n  ObjCTypeParamType(const ObjCTypeParamDecl *D,\n                    QualType can,\n                    ArrayRef<ObjCProtocolDecl *> protocols);\n\npublic:\n  bool isSugared() const { return true; }\n  QualType desugar() const { return getCanonicalTypeInternal(); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ObjCTypeParam;\n  }\n\n  void Profile(llvm::FoldingSetNodeID &ID);\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      const ObjCTypeParamDecl *OTPDecl,\n                      QualType CanonicalType,\n                      ArrayRef<ObjCProtocolDecl *> protocols);\n\n  ObjCTypeParamDecl *getDecl() const { return OTPDecl; }\n};\n\n/// Represents a class type in Objective C.\n///\n/// Every Objective C type is a combination of a base type, a set of\n/// type arguments (optional, for parameterized classes) and a list of\n/// protocols.\n///\n/// Given the following declarations:\n/// \\code\n///   \\@class C<T>;\n///   \\@protocol P;\n/// \\endcode\n///\n/// 'C' is an ObjCInterfaceType C.  It is sugar for an ObjCObjectType\n/// with base C and no protocols.\n///\n/// 'C<P>' is an unspecialized ObjCObjectType with base C and protocol list [P].\n/// 'C<C*>' is a specialized ObjCObjectType with type arguments 'C*' and no\n/// protocol list.\n/// 'C<C*><P>' is a specialized ObjCObjectType with base C, type arguments 'C*',\n/// and protocol list [P].\n///\n/// 'id' is a TypedefType which is sugar for an ObjCObjectPointerType whose\n/// pointee is an ObjCObjectType with base BuiltinType::ObjCIdType\n/// and no protocols.\n///\n/// 'id<P>' is an ObjCObjectPointerType whose pointee is an ObjCObjectType\n/// with base BuiltinType::ObjCIdType and protocol list [P].  Eventually\n/// this should get its own sugar class to better represent the source.\nclass ObjCObjectType : public Type,\n                       public ObjCProtocolQualifiers<ObjCObjectType> {\n  friend class ObjCProtocolQualifiers<ObjCObjectType>;\n\n  // ObjCObjectType.NumTypeArgs - the number of type arguments stored\n  // after the ObjCObjectPointerType node.\n  // ObjCObjectType.NumProtocols - the number of protocols stored\n  // after the type arguments of ObjCObjectPointerType node.\n  //\n  // These protocols are those written directly on the type.  If\n  // protocol qualifiers ever become additive, the iterators will need\n  // to get kindof complicated.\n  //\n  // In the canonical object type, these are sorted alphabetically\n  // and uniqued.\n\n  /// Either a BuiltinType or an InterfaceType or sugar for either.\n  QualType BaseType;\n\n  /// Cached superclass type.\n  mutable llvm::PointerIntPair<const ObjCObjectType *, 1, bool>\n    CachedSuperClassType;\n\n  QualType *getTypeArgStorage();\n  const QualType *getTypeArgStorage() const {\n    return const_cast<ObjCObjectType *>(this)->getTypeArgStorage();\n  }\n\n  ObjCProtocolDecl **getProtocolStorageImpl();\n  /// Return the number of qualifying protocols in this interface type,\n  /// or 0 if there are none.\n  unsigned getNumProtocolsImpl() const {\n    return ObjCObjectTypeBits.NumProtocols;\n  }\n  void setNumProtocolsImpl(unsigned N) {\n    ObjCObjectTypeBits.NumProtocols = N;\n  }\n\nprotected:\n  enum Nonce_ObjCInterface { Nonce_ObjCInterface };\n\n  ObjCObjectType(QualType Canonical, QualType Base,\n                 ArrayRef<QualType> typeArgs,\n                 ArrayRef<ObjCProtocolDecl *> protocols,\n                 bool isKindOf);\n\n  ObjCObjectType(enum Nonce_ObjCInterface)\n      : Type(ObjCInterface, QualType(), TypeDependence::None),\n        BaseType(QualType(this_(), 0)) {\n    ObjCObjectTypeBits.NumProtocols = 0;\n    ObjCObjectTypeBits.NumTypeArgs = 0;\n    ObjCObjectTypeBits.IsKindOf = 0;\n  }\n\n  void computeSuperClassTypeSlow() const;\n\npublic:\n  /// Gets the base type of this object type.  This is always (possibly\n  /// sugar for) one of:\n  ///  - the 'id' builtin type (as opposed to the 'id' type visible to the\n  ///    user, which is a typedef for an ObjCObjectPointerType)\n  ///  - the 'Class' builtin type (same caveat)\n  ///  - an ObjCObjectType (currently always an ObjCInterfaceType)\n  QualType getBaseType() const { return BaseType; }\n\n  bool isObjCId() const {\n    return getBaseType()->isSpecificBuiltinType(BuiltinType::ObjCId);\n  }\n\n  bool isObjCClass() const {\n    return getBaseType()->isSpecificBuiltinType(BuiltinType::ObjCClass);\n  }\n\n  bool isObjCUnqualifiedId() const { return qual_empty() && isObjCId(); }\n  bool isObjCUnqualifiedClass() const { return qual_empty() && isObjCClass(); }\n  bool isObjCUnqualifiedIdOrClass() const {\n    if (!qual_empty()) return false;\n    if (const BuiltinType *T = getBaseType()->getAs<BuiltinType>())\n      return T->getKind() == BuiltinType::ObjCId ||\n             T->getKind() == BuiltinType::ObjCClass;\n    return false;\n  }\n  bool isObjCQualifiedId() const { return !qual_empty() && isObjCId(); }\n  bool isObjCQualifiedClass() const { return !qual_empty() && isObjCClass(); }\n\n  /// Gets the interface declaration for this object type, if the base type\n  /// really is an interface.\n  ObjCInterfaceDecl *getInterface() const;\n\n  /// Determine whether this object type is \"specialized\", meaning\n  /// that it has type arguments.\n  bool isSpecialized() const;\n\n  /// Determine whether this object type was written with type arguments.\n  bool isSpecializedAsWritten() const {\n    return ObjCObjectTypeBits.NumTypeArgs > 0;\n  }\n\n  /// Determine whether this object type is \"unspecialized\", meaning\n  /// that it has no type arguments.\n  bool isUnspecialized() const { return !isSpecialized(); }\n\n  /// Determine whether this object type is \"unspecialized\" as\n  /// written, meaning that it has no type arguments.\n  bool isUnspecializedAsWritten() const { return !isSpecializedAsWritten(); }\n\n  /// Retrieve the type arguments of this object type (semantically).\n  ArrayRef<QualType> getTypeArgs() const;\n\n  /// Retrieve the type arguments of this object type as they were\n  /// written.\n  ArrayRef<QualType> getTypeArgsAsWritten() const {\n    return llvm::makeArrayRef(getTypeArgStorage(),\n                              ObjCObjectTypeBits.NumTypeArgs);\n  }\n\n  /// Whether this is a \"__kindof\" type as written.\n  bool isKindOfTypeAsWritten() const { return ObjCObjectTypeBits.IsKindOf; }\n\n  /// Whether this ia a \"__kindof\" type (semantically).\n  bool isKindOfType() const;\n\n  /// Retrieve the type of the superclass of this object type.\n  ///\n  /// This operation substitutes any type arguments into the\n  /// superclass of the current class type, potentially producing a\n  /// specialization of the superclass type. Produces a null type if\n  /// there is no superclass.\n  QualType getSuperClassType() const {\n    if (!CachedSuperClassType.getInt())\n      computeSuperClassTypeSlow();\n\n    assert(CachedSuperClassType.getInt() && \"Superclass not set?\");\n    return QualType(CachedSuperClassType.getPointer(), 0);\n  }\n\n  /// Strip off the Objective-C \"kindof\" type and (with it) any\n  /// protocol qualifiers.\n  QualType stripObjCKindOfTypeAndQuals(const ASTContext &ctx) const;\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ObjCObject ||\n           T->getTypeClass() == ObjCInterface;\n  }\n};\n\n/// A class providing a concrete implementation\n/// of ObjCObjectType, so as to not increase the footprint of\n/// ObjCInterfaceType.  Code outside of ASTContext and the core type\n/// system should not reference this type.\nclass ObjCObjectTypeImpl : public ObjCObjectType, public llvm::FoldingSetNode {\n  friend class ASTContext;\n\n  // If anyone adds fields here, ObjCObjectType::getProtocolStorage()\n  // will need to be modified.\n\n  ObjCObjectTypeImpl(QualType Canonical, QualType Base,\n                     ArrayRef<QualType> typeArgs,\n                     ArrayRef<ObjCProtocolDecl *> protocols,\n                     bool isKindOf)\n      : ObjCObjectType(Canonical, Base, typeArgs, protocols, isKindOf) {}\n\npublic:\n  void Profile(llvm::FoldingSetNodeID &ID);\n  static void Profile(llvm::FoldingSetNodeID &ID,\n                      QualType Base,\n                      ArrayRef<QualType> typeArgs,\n                      ArrayRef<ObjCProtocolDecl *> protocols,\n                      bool isKindOf);\n};\n\ninline QualType *ObjCObjectType::getTypeArgStorage() {\n  return reinterpret_cast<QualType *>(static_cast<ObjCObjectTypeImpl*>(this)+1);\n}\n\ninline ObjCProtocolDecl **ObjCObjectType::getProtocolStorageImpl() {\n    return reinterpret_cast<ObjCProtocolDecl**>(\n             getTypeArgStorage() + ObjCObjectTypeBits.NumTypeArgs);\n}\n\ninline ObjCProtocolDecl **ObjCTypeParamType::getProtocolStorageImpl() {\n    return reinterpret_cast<ObjCProtocolDecl**>(\n             static_cast<ObjCTypeParamType*>(this)+1);\n}\n\n/// Interfaces are the core concept in Objective-C for object oriented design.\n/// They basically correspond to C++ classes.  There are two kinds of interface\n/// types: normal interfaces like `NSString`, and qualified interfaces, which\n/// are qualified with a protocol list like `NSString<NSCopyable, NSAmazing>`.\n///\n/// ObjCInterfaceType guarantees the following properties when considered\n/// as a subtype of its superclass, ObjCObjectType:\n///   - There are no protocol qualifiers.  To reinforce this, code which\n///     tries to invoke the protocol methods via an ObjCInterfaceType will\n///     fail to compile.\n///   - It is its own base type.  That is, if T is an ObjCInterfaceType*,\n///     T->getBaseType() == QualType(T, 0).\nclass ObjCInterfaceType : public ObjCObjectType {\n  friend class ASTContext; // ASTContext creates these.\n  friend class ASTReader;\n  friend class ObjCInterfaceDecl;\n  template <class T> friend class serialization::AbstractTypeReader;\n\n  mutable ObjCInterfaceDecl *Decl;\n\n  ObjCInterfaceType(const ObjCInterfaceDecl *D)\n      : ObjCObjectType(Nonce_ObjCInterface),\n        Decl(const_cast<ObjCInterfaceDecl*>(D)) {}\n\npublic:\n  /// Get the declaration of this interface.\n  ObjCInterfaceDecl *getDecl() const { return Decl; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ObjCInterface;\n  }\n\n  // Nonsense to \"hide\" certain members of ObjCObjectType within this\n  // class.  People asking for protocols on an ObjCInterfaceType are\n  // not going to get what they want: ObjCInterfaceTypes are\n  // guaranteed to have no protocols.\n  enum {\n    qual_iterator,\n    qual_begin,\n    qual_end,\n    getNumProtocols,\n    getProtocol\n  };\n};\n\ninline ObjCInterfaceDecl *ObjCObjectType::getInterface() const {\n  QualType baseType = getBaseType();\n  while (const auto *ObjT = baseType->getAs<ObjCObjectType>()) {\n    if (const auto *T = dyn_cast<ObjCInterfaceType>(ObjT))\n      return T->getDecl();\n\n    baseType = ObjT->getBaseType();\n  }\n\n  return nullptr;\n}\n\n/// Represents a pointer to an Objective C object.\n///\n/// These are constructed from pointer declarators when the pointee type is\n/// an ObjCObjectType (or sugar for one).  In addition, the 'id' and 'Class'\n/// types are typedefs for these, and the protocol-qualified types 'id<P>'\n/// and 'Class<P>' are translated into these.\n///\n/// Pointers to pointers to Objective C objects are still PointerTypes;\n/// only the first level of pointer gets it own type implementation.\nclass ObjCObjectPointerType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType PointeeType;\n\n  ObjCObjectPointerType(QualType Canonical, QualType Pointee)\n      : Type(ObjCObjectPointer, Canonical, Pointee->getDependence()),\n        PointeeType(Pointee) {}\n\npublic:\n  /// Gets the type pointed to by this ObjC pointer.\n  /// The result will always be an ObjCObjectType or sugar thereof.\n  QualType getPointeeType() const { return PointeeType; }\n\n  /// Gets the type pointed to by this ObjC pointer.  Always returns non-null.\n  ///\n  /// This method is equivalent to getPointeeType() except that\n  /// it discards any typedefs (or other sugar) between this\n  /// type and the \"outermost\" object type.  So for:\n  /// \\code\n  ///   \\@class A; \\@protocol P; \\@protocol Q;\n  ///   typedef A<P> AP;\n  ///   typedef A A1;\n  ///   typedef A1<P> A1P;\n  ///   typedef A1P<Q> A1PQ;\n  /// \\endcode\n  /// For 'A*', getObjectType() will return 'A'.\n  /// For 'A<P>*', getObjectType() will return 'A<P>'.\n  /// For 'AP*', getObjectType() will return 'A<P>'.\n  /// For 'A1*', getObjectType() will return 'A'.\n  /// For 'A1<P>*', getObjectType() will return 'A1<P>'.\n  /// For 'A1P*', getObjectType() will return 'A1<P>'.\n  /// For 'A1PQ*', getObjectType() will return 'A1<Q>', because\n  ///   adding protocols to a protocol-qualified base discards the\n  ///   old qualifiers (for now).  But if it didn't, getObjectType()\n  ///   would return 'A1P<Q>' (and we'd have to make iterating over\n  ///   qualifiers more complicated).\n  const ObjCObjectType *getObjectType() const {\n    return PointeeType->castAs<ObjCObjectType>();\n  }\n\n  /// If this pointer points to an Objective C\n  /// \\@interface type, gets the type for that interface.  Any protocol\n  /// qualifiers on the interface are ignored.\n  ///\n  /// \\return null if the base type for this pointer is 'id' or 'Class'\n  const ObjCInterfaceType *getInterfaceType() const;\n\n  /// If this pointer points to an Objective \\@interface\n  /// type, gets the declaration for that interface.\n  ///\n  /// \\return null if the base type for this pointer is 'id' or 'Class'\n  ObjCInterfaceDecl *getInterfaceDecl() const {\n    return getObjectType()->getInterface();\n  }\n\n  /// True if this is equivalent to the 'id' type, i.e. if\n  /// its object type is the primitive 'id' type with no protocols.\n  bool isObjCIdType() const {\n    return getObjectType()->isObjCUnqualifiedId();\n  }\n\n  /// True if this is equivalent to the 'Class' type,\n  /// i.e. if its object tive is the primitive 'Class' type with no protocols.\n  bool isObjCClassType() const {\n    return getObjectType()->isObjCUnqualifiedClass();\n  }\n\n  /// True if this is equivalent to the 'id' or 'Class' type,\n  bool isObjCIdOrClassType() const {\n    return getObjectType()->isObjCUnqualifiedIdOrClass();\n  }\n\n  /// True if this is equivalent to 'id<P>' for some non-empty set of\n  /// protocols.\n  bool isObjCQualifiedIdType() const {\n    return getObjectType()->isObjCQualifiedId();\n  }\n\n  /// True if this is equivalent to 'Class<P>' for some non-empty set of\n  /// protocols.\n  bool isObjCQualifiedClassType() const {\n    return getObjectType()->isObjCQualifiedClass();\n  }\n\n  /// Whether this is a \"__kindof\" type.\n  bool isKindOfType() const { return getObjectType()->isKindOfType(); }\n\n  /// Whether this type is specialized, meaning that it has type arguments.\n  bool isSpecialized() const { return getObjectType()->isSpecialized(); }\n\n  /// Whether this type is specialized, meaning that it has type arguments.\n  bool isSpecializedAsWritten() const {\n    return getObjectType()->isSpecializedAsWritten();\n  }\n\n  /// Whether this type is unspecialized, meaning that is has no type arguments.\n  bool isUnspecialized() const { return getObjectType()->isUnspecialized(); }\n\n  /// Determine whether this object type is \"unspecialized\" as\n  /// written, meaning that it has no type arguments.\n  bool isUnspecializedAsWritten() const { return !isSpecializedAsWritten(); }\n\n  /// Retrieve the type arguments for this type.\n  ArrayRef<QualType> getTypeArgs() const {\n    return getObjectType()->getTypeArgs();\n  }\n\n  /// Retrieve the type arguments for this type.\n  ArrayRef<QualType> getTypeArgsAsWritten() const {\n    return getObjectType()->getTypeArgsAsWritten();\n  }\n\n  /// An iterator over the qualifiers on the object type.  Provided\n  /// for convenience.  This will always iterate over the full set of\n  /// protocols on a type, not just those provided directly.\n  using qual_iterator = ObjCObjectType::qual_iterator;\n  using qual_range = llvm::iterator_range<qual_iterator>;\n\n  qual_range quals() const { return qual_range(qual_begin(), qual_end()); }\n\n  qual_iterator qual_begin() const {\n    return getObjectType()->qual_begin();\n  }\n\n  qual_iterator qual_end() const {\n    return getObjectType()->qual_end();\n  }\n\n  bool qual_empty() const { return getObjectType()->qual_empty(); }\n\n  /// Return the number of qualifying protocols on the object type.\n  unsigned getNumProtocols() const {\n    return getObjectType()->getNumProtocols();\n  }\n\n  /// Retrieve a qualifying protocol by index on the object type.\n  ObjCProtocolDecl *getProtocol(unsigned I) const {\n    return getObjectType()->getProtocol(I);\n  }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  /// Retrieve the type of the superclass of this object pointer type.\n  ///\n  /// This operation substitutes any type arguments into the\n  /// superclass of the current class type, potentially producing a\n  /// pointer to a specialization of the superclass type. Produces a\n  /// null type if there is no superclass.\n  QualType getSuperClassType() const;\n\n  /// Strip off the Objective-C \"kindof\" type and (with it) any\n  /// protocol qualifiers.\n  const ObjCObjectPointerType *stripObjCKindOfTypeAndQuals(\n                                 const ASTContext &ctx) const;\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getPointeeType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType T) {\n    ID.AddPointer(T.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == ObjCObjectPointer;\n  }\n};\n\nclass AtomicType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType ValueType;\n\n  AtomicType(QualType ValTy, QualType Canonical)\n      : Type(Atomic, Canonical, ValTy->getDependence()), ValueType(ValTy) {}\n\npublic:\n  /// Gets the type contained by this atomic type, i.e.\n  /// the type returned by performing an atomic load of this atomic type.\n  QualType getValueType() const { return ValueType; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getValueType());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType T) {\n    ID.AddPointer(T.getAsOpaquePtr());\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Atomic;\n  }\n};\n\n/// PipeType - OpenCL20.\nclass PipeType : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext; // ASTContext creates these.\n\n  QualType ElementType;\n  bool isRead;\n\n  PipeType(QualType elemType, QualType CanonicalPtr, bool isRead)\n      : Type(Pipe, CanonicalPtr, elemType->getDependence()),\n        ElementType(elemType), isRead(isRead) {}\n\npublic:\n  QualType getElementType() const { return ElementType; }\n\n  bool isSugared() const { return false; }\n\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, getElementType(), isReadOnly());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, QualType T, bool isRead) {\n    ID.AddPointer(T.getAsOpaquePtr());\n    ID.AddBoolean(isRead);\n  }\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == Pipe;\n  }\n\n  bool isReadOnly() const { return isRead; }\n};\n\n/// A fixed int type of a specified bitwidth.\nclass ExtIntType final : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext;\n  unsigned IsUnsigned : 1;\n  unsigned NumBits : 24;\n\nprotected:\n  ExtIntType(bool isUnsigned, unsigned NumBits);\n\npublic:\n  bool isUnsigned() const { return IsUnsigned; }\n  bool isSigned() const { return !IsUnsigned; }\n  unsigned getNumBits() const { return NumBits; }\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, isUnsigned(), getNumBits());\n  }\n\n  static void Profile(llvm::FoldingSetNodeID &ID, bool IsUnsigned,\n                      unsigned NumBits) {\n    ID.AddBoolean(IsUnsigned);\n    ID.AddInteger(NumBits);\n  }\n\n  static bool classof(const Type *T) { return T->getTypeClass() == ExtInt; }\n};\n\nclass DependentExtIntType final : public Type, public llvm::FoldingSetNode {\n  friend class ASTContext;\n  const ASTContext &Context;\n  llvm::PointerIntPair<Expr*, 1, bool> ExprAndUnsigned;\n\nprotected:\n  DependentExtIntType(const ASTContext &Context, bool IsUnsigned,\n                      Expr *NumBits);\n\npublic:\n  bool isUnsigned() const;\n  bool isSigned() const { return !isUnsigned(); }\n  Expr *getNumBitsExpr() const;\n\n  bool isSugared() const { return false; }\n  QualType desugar() const { return QualType(this, 0); }\n\n  void Profile(llvm::FoldingSetNodeID &ID) {\n    Profile(ID, Context, isUnsigned(), getNumBitsExpr());\n  }\n  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,\n                      bool IsUnsigned, Expr *NumBitsExpr);\n\n  static bool classof(const Type *T) {\n    return T->getTypeClass() == DependentExtInt;\n  }\n};\n\n/// A qualifier set is used to build a set of qualifiers.\nclass QualifierCollector : public Qualifiers {\npublic:\n  QualifierCollector(Qualifiers Qs = Qualifiers()) : Qualifiers(Qs) {}\n\n  /// Collect any qualifiers on the given type and return an\n  /// unqualified type.  The qualifiers are assumed to be consistent\n  /// with those already in the type.\n  const Type *strip(QualType type) {\n    addFastQualifiers(type.getLocalFastQualifiers());\n    if (!type.hasLocalNonFastQualifiers())\n      return type.getTypePtrUnsafe();\n\n    const ExtQuals *extQuals = type.getExtQualsUnsafe();\n    addConsistentQualifiers(extQuals->getQualifiers());\n    return extQuals->getBaseType();\n  }\n\n  /// Apply the collected qualifiers to the given type.\n  QualType apply(const ASTContext &Context, QualType QT) const;\n\n  /// Apply the collected qualifiers to the given type.\n  QualType apply(const ASTContext &Context, const Type* T) const;\n};\n\n/// A container of type source information.\n///\n/// A client can read the relevant info using TypeLoc wrappers, e.g:\n/// @code\n/// TypeLoc TL = TypeSourceInfo->getTypeLoc();\n/// TL.getBeginLoc().print(OS, SrcMgr);\n/// @endcode\nclass alignas(8) TypeSourceInfo {\n  // Contains a memory block after the class, used for type source information,\n  // allocated by ASTContext.\n  friend class ASTContext;\n\n  QualType Ty;\n\n  TypeSourceInfo(QualType ty) : Ty(ty) {}\n\npublic:\n  /// Return the type wrapped by this type source info.\n  QualType getType() const { return Ty; }\n\n  /// Return the TypeLoc wrapper for the type source info.\n  TypeLoc getTypeLoc() const; // implemented in TypeLoc.h\n\n  /// Override the type stored in this TypeSourceInfo. Use with caution!\n  void overrideType(QualType T) { Ty = T; }\n};\n\n// Inline function definitions.\n\ninline SplitQualType SplitQualType::getSingleStepDesugaredType() const {\n  SplitQualType desugar =\n    Ty->getLocallyUnqualifiedSingleStepDesugaredType().split();\n  desugar.Quals.addConsistentQualifiers(Quals);\n  return desugar;\n}\n\ninline const Type *QualType::getTypePtr() const {\n  return getCommonPtr()->BaseType;\n}\n\ninline const Type *QualType::getTypePtrOrNull() const {\n  return (isNull() ? nullptr : getCommonPtr()->BaseType);\n}\n\ninline SplitQualType QualType::split() const {\n  if (!hasLocalNonFastQualifiers())\n    return SplitQualType(getTypePtrUnsafe(),\n                         Qualifiers::fromFastMask(getLocalFastQualifiers()));\n\n  const ExtQuals *eq = getExtQualsUnsafe();\n  Qualifiers qs = eq->getQualifiers();\n  qs.addFastQualifiers(getLocalFastQualifiers());\n  return SplitQualType(eq->getBaseType(), qs);\n}\n\ninline Qualifiers QualType::getLocalQualifiers() const {\n  Qualifiers Quals;\n  if (hasLocalNonFastQualifiers())\n    Quals = getExtQualsUnsafe()->getQualifiers();\n  Quals.addFastQualifiers(getLocalFastQualifiers());\n  return Quals;\n}\n\ninline Qualifiers QualType::getQualifiers() const {\n  Qualifiers quals = getCommonPtr()->CanonicalType.getLocalQualifiers();\n  quals.addFastQualifiers(getLocalFastQualifiers());\n  return quals;\n}\n\ninline unsigned QualType::getCVRQualifiers() const {\n  unsigned cvr = getCommonPtr()->CanonicalType.getLocalCVRQualifiers();\n  cvr |= getLocalCVRQualifiers();\n  return cvr;\n}\n\ninline QualType QualType::getCanonicalType() const {\n  QualType canon = getCommonPtr()->CanonicalType;\n  return canon.withFastQualifiers(getLocalFastQualifiers());\n}\n\ninline bool QualType::isCanonical() const {\n  return getTypePtr()->isCanonicalUnqualified();\n}\n\ninline bool QualType::isCanonicalAsParam() const {\n  if (!isCanonical()) return false;\n  if (hasLocalQualifiers()) return false;\n\n  const Type *T = getTypePtr();\n  if (T->isVariablyModifiedType() && T->hasSizedVLAType())\n    return false;\n\n  return !isa<FunctionType>(T) && !isa<ArrayType>(T);\n}\n\ninline bool QualType::isConstQualified() const {\n  return isLocalConstQualified() ||\n         getCommonPtr()->CanonicalType.isLocalConstQualified();\n}\n\ninline bool QualType::isRestrictQualified() const {\n  return isLocalRestrictQualified() ||\n         getCommonPtr()->CanonicalType.isLocalRestrictQualified();\n}\n\n\ninline bool QualType::isVolatileQualified() const {\n  return isLocalVolatileQualified() ||\n         getCommonPtr()->CanonicalType.isLocalVolatileQualified();\n}\n\ninline bool QualType::hasQualifiers() const {\n  return hasLocalQualifiers() ||\n         getCommonPtr()->CanonicalType.hasLocalQualifiers();\n}\n\ninline QualType QualType::getUnqualifiedType() const {\n  if (!getTypePtr()->getCanonicalTypeInternal().hasLocalQualifiers())\n    return QualType(getTypePtr(), 0);\n\n  return QualType(getSplitUnqualifiedTypeImpl(*this).Ty, 0);\n}\n\ninline SplitQualType QualType::getSplitUnqualifiedType() const {\n  if (!getTypePtr()->getCanonicalTypeInternal().hasLocalQualifiers())\n    return split();\n\n  return getSplitUnqualifiedTypeImpl(*this);\n}\n\ninline void QualType::removeLocalConst() {\n  removeLocalFastQualifiers(Qualifiers::Const);\n}\n\ninline void QualType::removeLocalRestrict() {\n  removeLocalFastQualifiers(Qualifiers::Restrict);\n}\n\ninline void QualType::removeLocalVolatile() {\n  removeLocalFastQualifiers(Qualifiers::Volatile);\n}\n\ninline void QualType::removeLocalCVRQualifiers(unsigned Mask) {\n  assert(!(Mask & ~Qualifiers::CVRMask) && \"mask has non-CVR bits\");\n  static_assert((int)Qualifiers::CVRMask == (int)Qualifiers::FastMask,\n                \"Fast bits differ from CVR bits!\");\n\n  // Fast path: we don't need to touch the slow qualifiers.\n  removeLocalFastQualifiers(Mask);\n}\n\n/// Check if this type has any address space qualifier.\ninline bool QualType::hasAddressSpace() const {\n  return getQualifiers().hasAddressSpace();\n}\n\n/// Return the address space of this type.\ninline LangAS QualType::getAddressSpace() const {\n  return getQualifiers().getAddressSpace();\n}\n\n/// Return the gc attribute of this type.\ninline Qualifiers::GC QualType::getObjCGCAttr() const {\n  return getQualifiers().getObjCGCAttr();\n}\n\ninline bool QualType::hasNonTrivialToPrimitiveDefaultInitializeCUnion() const {\n  if (auto *RD = getTypePtr()->getBaseElementTypeUnsafe()->getAsRecordDecl())\n    return hasNonTrivialToPrimitiveDefaultInitializeCUnion(RD);\n  return false;\n}\n\ninline bool QualType::hasNonTrivialToPrimitiveDestructCUnion() const {\n  if (auto *RD = getTypePtr()->getBaseElementTypeUnsafe()->getAsRecordDecl())\n    return hasNonTrivialToPrimitiveDestructCUnion(RD);\n  return false;\n}\n\ninline bool QualType::hasNonTrivialToPrimitiveCopyCUnion() const {\n  if (auto *RD = getTypePtr()->getBaseElementTypeUnsafe()->getAsRecordDecl())\n    return hasNonTrivialToPrimitiveCopyCUnion(RD);\n  return false;\n}\n\ninline FunctionType::ExtInfo getFunctionExtInfo(const Type &t) {\n  if (const auto *PT = t.getAs<PointerType>()) {\n    if (const auto *FT = PT->getPointeeType()->getAs<FunctionType>())\n      return FT->getExtInfo();\n  } else if (const auto *FT = t.getAs<FunctionType>())\n    return FT->getExtInfo();\n\n  return FunctionType::ExtInfo();\n}\n\ninline FunctionType::ExtInfo getFunctionExtInfo(QualType t) {\n  return getFunctionExtInfo(*t);\n}\n\n/// Determine whether this type is more\n/// qualified than the Other type. For example, \"const volatile int\"\n/// is more qualified than \"const int\", \"volatile int\", and\n/// \"int\". However, it is not more qualified than \"const volatile\n/// int\".\ninline bool QualType::isMoreQualifiedThan(QualType other) const {\n  Qualifiers MyQuals = getQualifiers();\n  Qualifiers OtherQuals = other.getQualifiers();\n  return (MyQuals != OtherQuals && MyQuals.compatiblyIncludes(OtherQuals));\n}\n\n/// Determine whether this type is at last\n/// as qualified as the Other type. For example, \"const volatile\n/// int\" is at least as qualified as \"const int\", \"volatile int\",\n/// \"int\", and \"const volatile int\".\ninline bool QualType::isAtLeastAsQualifiedAs(QualType other) const {\n  Qualifiers OtherQuals = other.getQualifiers();\n\n  // Ignore __unaligned qualifier if this type is a void.\n  if (getUnqualifiedType()->isVoidType())\n    OtherQuals.removeUnaligned();\n\n  return getQualifiers().compatiblyIncludes(OtherQuals);\n}\n\n/// If Type is a reference type (e.g., const\n/// int&), returns the type that the reference refers to (\"const\n/// int\"). Otherwise, returns the type itself. This routine is used\n/// throughout Sema to implement C++ 5p6:\n///\n///   If an expression initially has the type \"reference to T\" (8.3.2,\n///   8.5.3), the type is adjusted to \"T\" prior to any further\n///   analysis, the expression designates the object or function\n///   denoted by the reference, and the expression is an lvalue.\ninline QualType QualType::getNonReferenceType() const {\n  if (const auto *RefType = (*this)->getAs<ReferenceType>())\n    return RefType->getPointeeType();\n  else\n    return *this;\n}\n\ninline bool QualType::isCForbiddenLValueType() const {\n  return ((getTypePtr()->isVoidType() && !hasQualifiers()) ||\n          getTypePtr()->isFunctionType());\n}\n\n/// Tests whether the type is categorized as a fundamental type.\n///\n/// \\returns True for types specified in C++0x [basic.fundamental].\ninline bool Type::isFundamentalType() const {\n  return isVoidType() ||\n         isNullPtrType() ||\n         // FIXME: It's really annoying that we don't have an\n         // 'isArithmeticType()' which agrees with the standard definition.\n         (isArithmeticType() && !isEnumeralType());\n}\n\n/// Tests whether the type is categorized as a compound type.\n///\n/// \\returns True for types specified in C++0x [basic.compound].\ninline bool Type::isCompoundType() const {\n  // C++0x [basic.compound]p1:\n  //   Compound types can be constructed in the following ways:\n  //    -- arrays of objects of a given type [...];\n  return isArrayType() ||\n  //    -- functions, which have parameters of given types [...];\n         isFunctionType() ||\n  //    -- pointers to void or objects or functions [...];\n         isPointerType() ||\n  //    -- references to objects or functions of a given type. [...]\n         isReferenceType() ||\n  //    -- classes containing a sequence of objects of various types, [...];\n         isRecordType() ||\n  //    -- unions, which are classes capable of containing objects of different\n  //               types at different times;\n         isUnionType() ||\n  //    -- enumerations, which comprise a set of named constant values. [...];\n         isEnumeralType() ||\n  //    -- pointers to non-static class members, [...].\n         isMemberPointerType();\n}\n\ninline bool Type::isFunctionType() const {\n  return isa<FunctionType>(CanonicalType);\n}\n\ninline bool Type::isPointerType() const {\n  return isa<PointerType>(CanonicalType);\n}\n\ninline bool Type::isAnyPointerType() const {\n  return isPointerType() || isObjCObjectPointerType();\n}\n\ninline bool Type::isBlockPointerType() const {\n  return isa<BlockPointerType>(CanonicalType);\n}\n\ninline bool Type::isReferenceType() const {\n  return isa<ReferenceType>(CanonicalType);\n}\n\ninline bool Type::isLValueReferenceType() const {\n  return isa<LValueReferenceType>(CanonicalType);\n}\n\ninline bool Type::isRValueReferenceType() const {\n  return isa<RValueReferenceType>(CanonicalType);\n}\n\ninline bool Type::isObjectPointerType() const {\n  // Note: an \"object pointer type\" is not the same thing as a pointer to an\n  // object type; rather, it is a pointer to an object type or a pointer to cv\n  // void.\n  if (const auto *T = getAs<PointerType>())\n    return !T->getPointeeType()->isFunctionType();\n  else\n    return false;\n}\n\ninline bool Type::isFunctionPointerType() const {\n  if (const auto *T = getAs<PointerType>())\n    return T->getPointeeType()->isFunctionType();\n  else\n    return false;\n}\n\ninline bool Type::isFunctionReferenceType() const {\n  if (const auto *T = getAs<ReferenceType>())\n    return T->getPointeeType()->isFunctionType();\n  else\n    return false;\n}\n\ninline bool Type::isMemberPointerType() const {\n  return isa<MemberPointerType>(CanonicalType);\n}\n\ninline bool Type::isMemberFunctionPointerType() const {\n  if (const auto *T = getAs<MemberPointerType>())\n    return T->isMemberFunctionPointer();\n  else\n    return false;\n}\n\ninline bool Type::isMemberDataPointerType() const {\n  if (const auto *T = getAs<MemberPointerType>())\n    return T->isMemberDataPointer();\n  else\n    return false;\n}\n\ninline bool Type::isArrayType() const {\n  return isa<ArrayType>(CanonicalType);\n}\n\ninline bool Type::isConstantArrayType() const {\n  return isa<ConstantArrayType>(CanonicalType);\n}\n\ninline bool Type::isIncompleteArrayType() const {\n  return isa<IncompleteArrayType>(CanonicalType);\n}\n\ninline bool Type::isVariableArrayType() const {\n  return isa<VariableArrayType>(CanonicalType);\n}\n\ninline bool Type::isDependentSizedArrayType() const {\n  return isa<DependentSizedArrayType>(CanonicalType);\n}\n\ninline bool Type::isBuiltinType() const {\n  return isa<BuiltinType>(CanonicalType);\n}\n\ninline bool Type::isRecordType() const {\n  return isa<RecordType>(CanonicalType);\n}\n\ninline bool Type::isEnumeralType() const {\n  return isa<EnumType>(CanonicalType);\n}\n\ninline bool Type::isAnyComplexType() const {\n  return isa<ComplexType>(CanonicalType);\n}\n\ninline bool Type::isVectorType() const {\n  return isa<VectorType>(CanonicalType);\n}\n\ninline bool Type::isExtVectorType() const {\n  return isa<ExtVectorType>(CanonicalType);\n}\n\ninline bool Type::isMatrixType() const {\n  return isa<MatrixType>(CanonicalType);\n}\n\ninline bool Type::isConstantMatrixType() const {\n  return isa<ConstantMatrixType>(CanonicalType);\n}\n\ninline bool Type::isDependentAddressSpaceType() const {\n  return isa<DependentAddressSpaceType>(CanonicalType);\n}\n\ninline bool Type::isObjCObjectPointerType() const {\n  return isa<ObjCObjectPointerType>(CanonicalType);\n}\n\ninline bool Type::isObjCObjectType() const {\n  return isa<ObjCObjectType>(CanonicalType);\n}\n\ninline bool Type::isObjCObjectOrInterfaceType() const {\n  return isa<ObjCInterfaceType>(CanonicalType) ||\n    isa<ObjCObjectType>(CanonicalType);\n}\n\ninline bool Type::isAtomicType() const {\n  return isa<AtomicType>(CanonicalType);\n}\n\ninline bool Type::isUndeducedAutoType() const {\n  return isa<AutoType>(CanonicalType);\n}\n\ninline bool Type::isObjCQualifiedIdType() const {\n  if (const auto *OPT = getAs<ObjCObjectPointerType>())\n    return OPT->isObjCQualifiedIdType();\n  return false;\n}\n\ninline bool Type::isObjCQualifiedClassType() const {\n  if (const auto *OPT = getAs<ObjCObjectPointerType>())\n    return OPT->isObjCQualifiedClassType();\n  return false;\n}\n\ninline bool Type::isObjCIdType() const {\n  if (const auto *OPT = getAs<ObjCObjectPointerType>())\n    return OPT->isObjCIdType();\n  return false;\n}\n\ninline bool Type::isObjCClassType() const {\n  if (const auto *OPT = getAs<ObjCObjectPointerType>())\n    return OPT->isObjCClassType();\n  return false;\n}\n\ninline bool Type::isObjCSelType() const {\n  if (const auto *OPT = getAs<PointerType>())\n    return OPT->getPointeeType()->isSpecificBuiltinType(BuiltinType::ObjCSel);\n  return false;\n}\n\ninline bool Type::isObjCBuiltinType() const {\n  return isObjCIdType() || isObjCClassType() || isObjCSelType();\n}\n\ninline bool Type::isDecltypeType() const {\n  return isa<DecltypeType>(this);\n}\n\n#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \\\n  inline bool Type::is##Id##Type() const { \\\n    return isSpecificBuiltinType(BuiltinType::Id); \\\n  }\n#include \"clang/Basic/OpenCLImageTypes.def\"\n\ninline bool Type::isSamplerT() const {\n  return isSpecificBuiltinType(BuiltinType::OCLSampler);\n}\n\ninline bool Type::isEventT() const {\n  return isSpecificBuiltinType(BuiltinType::OCLEvent);\n}\n\ninline bool Type::isClkEventT() const {\n  return isSpecificBuiltinType(BuiltinType::OCLClkEvent);\n}\n\ninline bool Type::isQueueT() const {\n  return isSpecificBuiltinType(BuiltinType::OCLQueue);\n}\n\ninline bool Type::isReserveIDT() const {\n  return isSpecificBuiltinType(BuiltinType::OCLReserveID);\n}\n\ninline bool Type::isImageType() const {\n#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) is##Id##Type() ||\n  return\n#include \"clang/Basic/OpenCLImageTypes.def\"\n      false; // end boolean or operation\n}\n\ninline bool Type::isPipeType() const {\n  return isa<PipeType>(CanonicalType);\n}\n\ninline bool Type::isExtIntType() const {\n  return isa<ExtIntType>(CanonicalType);\n}\n\n#define EXT_OPAQUE_TYPE(ExtType, Id, Ext) \\\n  inline bool Type::is##Id##Type() const { \\\n    return isSpecificBuiltinType(BuiltinType::Id); \\\n  }\n#include \"clang/Basic/OpenCLExtensionTypes.def\"\n\ninline bool Type::isOCLIntelSubgroupAVCType() const {\n#define INTEL_SUBGROUP_AVC_TYPE(ExtType, Id) \\\n  isOCLIntelSubgroupAVC##Id##Type() ||\n  return\n#include \"clang/Basic/OpenCLExtensionTypes.def\"\n    false; // end of boolean or operation\n}\n\ninline bool Type::isOCLExtOpaqueType() const {\n#define EXT_OPAQUE_TYPE(ExtType, Id, Ext) is##Id##Type() ||\n  return\n#include \"clang/Basic/OpenCLExtensionTypes.def\"\n    false; // end of boolean or operation\n}\n\ninline bool Type::isOpenCLSpecificType() const {\n  return isSamplerT() || isEventT() || isImageType() || isClkEventT() ||\n         isQueueT() || isReserveIDT() || isPipeType() || isOCLExtOpaqueType();\n}\n\ninline bool Type::isTemplateTypeParmType() const {\n  return isa<TemplateTypeParmType>(CanonicalType);\n}\n\ninline bool Type::isSpecificBuiltinType(unsigned K) const {\n  if (const BuiltinType *BT = getAs<BuiltinType>()) {\n    return BT->getKind() == static_cast<BuiltinType::Kind>(K);\n  }\n  return false;\n}\n\ninline bool Type::isPlaceholderType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(this))\n    return BT->isPlaceholderType();\n  return false;\n}\n\ninline const BuiltinType *Type::getAsPlaceholderType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(this))\n    if (BT->isPlaceholderType())\n      return BT;\n  return nullptr;\n}\n\ninline bool Type::isSpecificPlaceholderType(unsigned K) const {\n  assert(BuiltinType::isPlaceholderTypeKind((BuiltinType::Kind) K));\n  return isSpecificBuiltinType(K);\n}\n\ninline bool Type::isNonOverloadPlaceholderType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(this))\n    return BT->isNonOverloadPlaceholderType();\n  return false;\n}\n\ninline bool Type::isVoidType() const {\n  return isSpecificBuiltinType(BuiltinType::Void);\n}\n\ninline bool Type::isHalfType() const {\n  // FIXME: Should we allow complex __fp16? Probably not.\n  return isSpecificBuiltinType(BuiltinType::Half);\n}\n\ninline bool Type::isFloat16Type() const {\n  return isSpecificBuiltinType(BuiltinType::Float16);\n}\n\ninline bool Type::isBFloat16Type() const {\n  return isSpecificBuiltinType(BuiltinType::BFloat16);\n}\n\ninline bool Type::isFloat128Type() const {\n  return isSpecificBuiltinType(BuiltinType::Float128);\n}\n\ninline bool Type::isNullPtrType() const {\n  return isSpecificBuiltinType(BuiltinType::NullPtr);\n}\n\nbool IsEnumDeclComplete(EnumDecl *);\nbool IsEnumDeclScoped(EnumDecl *);\n\ninline bool Type::isIntegerType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType))\n    return BT->getKind() >= BuiltinType::Bool &&\n           BT->getKind() <= BuiltinType::Int128;\n  if (const EnumType *ET = dyn_cast<EnumType>(CanonicalType)) {\n    // Incomplete enum types are not treated as integer types.\n    // FIXME: In C++, enum types are never integer types.\n    return IsEnumDeclComplete(ET->getDecl()) &&\n      !IsEnumDeclScoped(ET->getDecl());\n  }\n  return isExtIntType();\n}\n\ninline bool Type::isFixedPointType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType)) {\n    return BT->getKind() >= BuiltinType::ShortAccum &&\n           BT->getKind() <= BuiltinType::SatULongFract;\n  }\n  return false;\n}\n\ninline bool Type::isFixedPointOrIntegerType() const {\n  return isFixedPointType() || isIntegerType();\n}\n\ninline bool Type::isSaturatedFixedPointType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType)) {\n    return BT->getKind() >= BuiltinType::SatShortAccum &&\n           BT->getKind() <= BuiltinType::SatULongFract;\n  }\n  return false;\n}\n\ninline bool Type::isUnsaturatedFixedPointType() const {\n  return isFixedPointType() && !isSaturatedFixedPointType();\n}\n\ninline bool Type::isSignedFixedPointType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType)) {\n    return ((BT->getKind() >= BuiltinType::ShortAccum &&\n             BT->getKind() <= BuiltinType::LongAccum) ||\n            (BT->getKind() >= BuiltinType::ShortFract &&\n             BT->getKind() <= BuiltinType::LongFract) ||\n            (BT->getKind() >= BuiltinType::SatShortAccum &&\n             BT->getKind() <= BuiltinType::SatLongAccum) ||\n            (BT->getKind() >= BuiltinType::SatShortFract &&\n             BT->getKind() <= BuiltinType::SatLongFract));\n  }\n  return false;\n}\n\ninline bool Type::isUnsignedFixedPointType() const {\n  return isFixedPointType() && !isSignedFixedPointType();\n}\n\ninline bool Type::isScalarType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType))\n    return BT->getKind() > BuiltinType::Void &&\n           BT->getKind() <= BuiltinType::NullPtr;\n  if (const EnumType *ET = dyn_cast<EnumType>(CanonicalType))\n    // Enums are scalar types, but only if they are defined.  Incomplete enums\n    // are not treated as scalar types.\n    return IsEnumDeclComplete(ET->getDecl());\n  return isa<PointerType>(CanonicalType) ||\n         isa<BlockPointerType>(CanonicalType) ||\n         isa<MemberPointerType>(CanonicalType) ||\n         isa<ComplexType>(CanonicalType) ||\n         isa<ObjCObjectPointerType>(CanonicalType) ||\n         isExtIntType();\n}\n\ninline bool Type::isIntegralOrEnumerationType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType))\n    return BT->getKind() >= BuiltinType::Bool &&\n           BT->getKind() <= BuiltinType::Int128;\n\n  // Check for a complete enum type; incomplete enum types are not properly an\n  // enumeration type in the sense required here.\n  if (const auto *ET = dyn_cast<EnumType>(CanonicalType))\n    return IsEnumDeclComplete(ET->getDecl());\n\n  return isExtIntType();\n}\n\ninline bool Type::isBooleanType() const {\n  if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType))\n    return BT->getKind() == BuiltinType::Bool;\n  return false;\n}\n\ninline bool Type::isUndeducedType() const {\n  auto *DT = getContainedDeducedType();\n  return DT && !DT->isDeduced();\n}\n\n/// Determines whether this is a type for which one can define\n/// an overloaded operator.\ninline bool Type::isOverloadableType() const {\n  return isDependentType() || isRecordType() || isEnumeralType();\n}\n\n/// Determines whether this type is written as a typedef-name.\ninline bool Type::isTypedefNameType() const {\n  if (getAs<TypedefType>())\n    return true;\n  if (auto *TST = getAs<TemplateSpecializationType>())\n    return TST->isTypeAlias();\n  return false;\n}\n\n/// Determines whether this type can decay to a pointer type.\ninline bool Type::canDecayToPointerType() const {\n  return isFunctionType() || isArrayType();\n}\n\ninline bool Type::hasPointerRepresentation() const {\n  return (isPointerType() || isReferenceType() || isBlockPointerType() ||\n          isObjCObjectPointerType() || isNullPtrType());\n}\n\ninline bool Type::hasObjCPointerRepresentation() const {\n  return isObjCObjectPointerType();\n}\n\ninline const Type *Type::getBaseElementTypeUnsafe() const {\n  const Type *type = this;\n  while (const ArrayType *arrayType = type->getAsArrayTypeUnsafe())\n    type = arrayType->getElementType().getTypePtr();\n  return type;\n}\n\ninline const Type *Type::getPointeeOrArrayElementType() const {\n  const Type *type = this;\n  if (type->isAnyPointerType())\n    return type->getPointeeType().getTypePtr();\n  else if (type->isArrayType())\n    return type->getBaseElementTypeUnsafe();\n  return type;\n}\n/// Insertion operator for partial diagnostics. This allows sending adress\n/// spaces into a diagnostic with <<.\ninline const StreamingDiagnostic &operator<<(const StreamingDiagnostic &PD,\n                                             LangAS AS) {\n  PD.AddTaggedVal(static_cast<std::underlying_type_t<LangAS>>(AS),\n                  DiagnosticsEngine::ArgumentKind::ak_addrspace);\n  return PD;\n}\n\n/// Insertion operator for partial diagnostics. This allows sending Qualifiers\n/// into a diagnostic with <<.\ninline const StreamingDiagnostic &operator<<(const StreamingDiagnostic &PD,\n                                             Qualifiers Q) {\n  PD.AddTaggedVal(Q.getAsOpaqueValue(),\n                  DiagnosticsEngine::ArgumentKind::ak_qual);\n  return PD;\n}\n\n/// Insertion operator for partial diagnostics.  This allows sending QualType's\n/// into a diagnostic with <<.\ninline const StreamingDiagnostic &operator<<(const StreamingDiagnostic &PD,\n                                             QualType T) {\n  PD.AddTaggedVal(reinterpret_cast<intptr_t>(T.getAsOpaquePtr()),\n                  DiagnosticsEngine::ak_qualtype);\n  return PD;\n}\n\n// Helper class template that is used by Type::getAs to ensure that one does\n// not try to look through a qualified type to get to an array type.\ntemplate <typename T>\nusing TypeIsArrayType =\n    std::integral_constant<bool, std::is_same<T, ArrayType>::value ||\n                                     std::is_base_of<ArrayType, T>::value>;\n\n// Member-template getAs<specific type>'.\ntemplate <typename T> const T *Type::getAs() const {\n  static_assert(!TypeIsArrayType<T>::value,\n                \"ArrayType cannot be used with getAs!\");\n\n  // If this is directly a T type, return it.\n  if (const auto *Ty = dyn_cast<T>(this))\n    return Ty;\n\n  // If the canonical form of this type isn't the right kind, reject it.\n  if (!isa<T>(CanonicalType))\n    return nullptr;\n\n  // If this is a typedef for the type, strip the typedef off without\n  // losing all typedef information.\n  return cast<T>(getUnqualifiedDesugaredType());\n}\n\ntemplate <typename T> const T *Type::getAsAdjusted() const {\n  static_assert(!TypeIsArrayType<T>::value, \"ArrayType cannot be used with getAsAdjusted!\");\n\n  // If this is directly a T type, return it.\n  if (const auto *Ty = dyn_cast<T>(this))\n    return Ty;\n\n  // If the canonical form of this type isn't the right kind, reject it.\n  if (!isa<T>(CanonicalType))\n    return nullptr;\n\n  // Strip off type adjustments that do not modify the underlying nature of the\n  // type.\n  const Type *Ty = this;\n  while (Ty) {\n    if (const auto *A = dyn_cast<AttributedType>(Ty))\n      Ty = A->getModifiedType().getTypePtr();\n    else if (const auto *E = dyn_cast<ElaboratedType>(Ty))\n      Ty = E->desugar().getTypePtr();\n    else if (const auto *P = dyn_cast<ParenType>(Ty))\n      Ty = P->desugar().getTypePtr();\n    else if (const auto *A = dyn_cast<AdjustedType>(Ty))\n      Ty = A->desugar().getTypePtr();\n    else if (const auto *M = dyn_cast<MacroQualifiedType>(Ty))\n      Ty = M->desugar().getTypePtr();\n    else\n      break;\n  }\n\n  // Just because the canonical type is correct does not mean we can use cast<>,\n  // since we may not have stripped off all the sugar down to the base type.\n  return dyn_cast<T>(Ty);\n}\n\ninline const ArrayType *Type::getAsArrayTypeUnsafe() const {\n  // If this is directly an array type, return it.\n  if (const auto *arr = dyn_cast<ArrayType>(this))\n    return arr;\n\n  // If the canonical form of this type isn't the right kind, reject it.\n  if (!isa<ArrayType>(CanonicalType))\n    return nullptr;\n\n  // If this is a typedef for the type, strip the typedef off without\n  // losing all typedef information.\n  return cast<ArrayType>(getUnqualifiedDesugaredType());\n}\n\ntemplate <typename T> const T *Type::castAs() const {\n  static_assert(!TypeIsArrayType<T>::value,\n                \"ArrayType cannot be used with castAs!\");\n\n  if (const auto *ty = dyn_cast<T>(this)) return ty;\n  assert(isa<T>(CanonicalType));\n  return cast<T>(getUnqualifiedDesugaredType());\n}\n\ninline const ArrayType *Type::castAsArrayTypeUnsafe() const {\n  assert(isa<ArrayType>(CanonicalType));\n  if (const auto *arr = dyn_cast<ArrayType>(this)) return arr;\n  return cast<ArrayType>(getUnqualifiedDesugaredType());\n}\n\nDecayedType::DecayedType(QualType OriginalType, QualType DecayedPtr,\n                         QualType CanonicalPtr)\n    : AdjustedType(Decayed, OriginalType, DecayedPtr, CanonicalPtr) {\n#ifndef NDEBUG\n  QualType Adjusted = getAdjustedType();\n  (void)AttributedType::stripOuterNullability(Adjusted);\n  assert(isa<PointerType>(Adjusted));\n#endif\n}\n\nQualType DecayedType::getPointeeType() const {\n  QualType Decayed = getDecayedType();\n  (void)AttributedType::stripOuterNullability(Decayed);\n  return cast<PointerType>(Decayed)->getPointeeType();\n}\n\n// Get the decimal string representation of a fixed point type, represented\n// as a scaled integer.\n// TODO: At some point, we should change the arguments to instead just accept an\n// APFixedPoint instead of APSInt and scale.\nvoid FixedPointValueToString(SmallVectorImpl<char> &Str, llvm::APSInt Val,\n                             unsigned Scale);\n\n} // namespace clang\n\n#endif // LLVM_CLANG_AST_TYPE_H\n"}, "38": {"id": 38, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/Address.h", "content": "//===-- Address.h - An aligned address -------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This class provides a simple wrapper for a pair of a pointer and an\n// alignment.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_ADDRESS_H\n#define LLVM_CLANG_LIB_CODEGEN_ADDRESS_H\n\n#include \"llvm/IR/Constants.h\"\n#include \"clang/AST/CharUnits.h\"\n\nnamespace clang {\nnamespace CodeGen {\n\n/// An aligned address.\nclass Address {\n  llvm::Value *Pointer;\n  CharUnits Alignment;\npublic:\n  Address(llvm::Value *pointer, CharUnits alignment)\n      : Pointer(pointer), Alignment(alignment) {\n    assert((!alignment.isZero() || pointer == nullptr) &&\n           \"creating valid address with invalid alignment\");\n  }\n\n  static Address invalid() { return Address(nullptr, CharUnits()); }\n  bool isValid() const { return Pointer != nullptr; }\n\n  llvm::Value *getPointer() const {\n    assert(isValid());\n    return Pointer;\n  }\n\n  /// Return the type of the pointer value.\n  llvm::PointerType *getType() const {\n    return llvm::cast<llvm::PointerType>(getPointer()->getType());\n  }\n\n  /// Return the type of the values stored in this address.\n  ///\n  /// When IR pointer types lose their element type, we should simply\n  /// store it in Address instead for the convenience of writing code.\n  llvm::Type *getElementType() const {\n    return getType()->getElementType();\n  }\n\n  /// Return the address space that this address resides in.\n  unsigned getAddressSpace() const {\n    return getType()->getAddressSpace();\n  }\n\n  /// Return the IR name of the pointer value.\n  llvm::StringRef getName() const {\n    return getPointer()->getName();\n  }\n\n  /// Return the alignment of this pointer.\n  CharUnits getAlignment() const {\n    assert(isValid());\n    return Alignment;\n  }\n};\n\n/// A specialization of Address that requires the address to be an\n/// LLVM Constant.\nclass ConstantAddress : public Address {\npublic:\n  ConstantAddress(llvm::Constant *pointer, CharUnits alignment)\n    : Address(pointer, alignment) {}\n\n  static ConstantAddress invalid() {\n    return ConstantAddress(nullptr, CharUnits());\n  }\n\n  llvm::Constant *getPointer() const {\n    return llvm::cast<llvm::Constant>(Address::getPointer());\n  }\n\n  ConstantAddress getBitCast(llvm::Type *ty) const {\n    return ConstantAddress(llvm::ConstantExpr::getBitCast(getPointer(), ty),\n                           getAlignment());\n  }\n\n  ConstantAddress getElementBitCast(llvm::Type *ty) const {\n    return getBitCast(ty->getPointerTo(getAddressSpace()));\n  }\n\n  static bool isaImpl(Address addr) {\n    return llvm::isa<llvm::Constant>(addr.getPointer());\n  }\n  static ConstantAddress castImpl(Address addr) {\n    return ConstantAddress(llvm::cast<llvm::Constant>(addr.getPointer()),\n                           addr.getAlignment());\n  }\n};\n\n}\n\n// Present a minimal LLVM-like casting interface.\ntemplate <class U> inline U cast(CodeGen::Address addr) {\n  return U::castImpl(addr);\n}\ntemplate <class U> inline bool isa(CodeGen::Address addr) {\n  return U::isaImpl(addr);\n}\n\n}\n\n#endif\n"}, "40": {"id": 40, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCleanup.h", "content": "//===-- CGCleanup.h - Classes for cleanups IR generation --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// These classes support the generation of LLVM IR for cleanups.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CGCLEANUP_H\n#define LLVM_CLANG_LIB_CODEGEN_CGCLEANUP_H\n\n#include \"EHScopeStack.h\"\n\n#include \"Address.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n\nnamespace llvm {\nclass BasicBlock;\nclass Value;\nclass ConstantInt;\nclass AllocaInst;\n}\n\nnamespace clang {\nclass FunctionDecl;\nnamespace CodeGen {\nclass CodeGenModule;\nclass CodeGenFunction;\n\n/// The MS C++ ABI needs a pointer to RTTI data plus some flags to describe the\n/// type of a catch handler, so we use this wrapper.\nstruct CatchTypeInfo {\n  llvm::Constant *RTTI;\n  unsigned Flags;\n};\n\n/// A protected scope for zero-cost EH handling.\nclass EHScope {\n  llvm::BasicBlock *CachedLandingPad;\n  llvm::BasicBlock *CachedEHDispatchBlock;\n\n  EHScopeStack::stable_iterator EnclosingEHScope;\n\n  class CommonBitFields {\n    friend class EHScope;\n    unsigned Kind : 3;\n  };\n  enum { NumCommonBits = 3 };\n\nprotected:\n  class CatchBitFields {\n    friend class EHCatchScope;\n    unsigned : NumCommonBits;\n\n    unsigned NumHandlers : 32 - NumCommonBits;\n  };\n\n  class CleanupBitFields {\n    friend class EHCleanupScope;\n    unsigned : NumCommonBits;\n\n    /// Whether this cleanup needs to be run along normal edges.\n    unsigned IsNormalCleanup : 1;\n\n    /// Whether this cleanup needs to be run along exception edges.\n    unsigned IsEHCleanup : 1;\n\n    /// Whether this cleanup is currently active.\n    unsigned IsActive : 1;\n\n    /// Whether this cleanup is a lifetime marker\n    unsigned IsLifetimeMarker : 1;\n\n    /// Whether the normal cleanup should test the activation flag.\n    unsigned TestFlagInNormalCleanup : 1;\n\n    /// Whether the EH cleanup should test the activation flag.\n    unsigned TestFlagInEHCleanup : 1;\n\n    /// The amount of extra storage needed by the Cleanup.\n    /// Always a multiple of the scope-stack alignment.\n    unsigned CleanupSize : 12;\n  };\n\n  class FilterBitFields {\n    friend class EHFilterScope;\n    unsigned : NumCommonBits;\n\n    unsigned NumFilters : 32 - NumCommonBits;\n  };\n\n  union {\n    CommonBitFields CommonBits;\n    CatchBitFields CatchBits;\n    CleanupBitFields CleanupBits;\n    FilterBitFields FilterBits;\n  };\n\npublic:\n  enum Kind { Cleanup, Catch, Terminate, Filter };\n\n  EHScope(Kind kind, EHScopeStack::stable_iterator enclosingEHScope)\n    : CachedLandingPad(nullptr), CachedEHDispatchBlock(nullptr),\n      EnclosingEHScope(enclosingEHScope) {\n    CommonBits.Kind = kind;\n  }\n\n  Kind getKind() const { return static_cast<Kind>(CommonBits.Kind); }\n\n  llvm::BasicBlock *getCachedLandingPad() const {\n    return CachedLandingPad;\n  }\n\n  void setCachedLandingPad(llvm::BasicBlock *block) {\n    CachedLandingPad = block;\n  }\n\n  llvm::BasicBlock *getCachedEHDispatchBlock() const {\n    return CachedEHDispatchBlock;\n  }\n\n  void setCachedEHDispatchBlock(llvm::BasicBlock *block) {\n    CachedEHDispatchBlock = block;\n  }\n\n  bool hasEHBranches() const {\n    if (llvm::BasicBlock *block = getCachedEHDispatchBlock())\n      return !block->use_empty();\n    return false;\n  }\n\n  EHScopeStack::stable_iterator getEnclosingEHScope() const {\n    return EnclosingEHScope;\n  }\n};\n\n/// A scope which attempts to handle some, possibly all, types of\n/// exceptions.\n///\n/// Objective C \\@finally blocks are represented using a cleanup scope\n/// after the catch scope.\nclass EHCatchScope : public EHScope {\n  // In effect, we have a flexible array member\n  //   Handler Handlers[0];\n  // But that's only standard in C99, not C++, so we have to do\n  // annoying pointer arithmetic instead.\n\npublic:\n  struct Handler {\n    /// A type info value, or null (C++ null, not an LLVM null pointer)\n    /// for a catch-all.\n    CatchTypeInfo Type;\n\n    /// The catch handler for this type.\n    llvm::BasicBlock *Block;\n\n    bool isCatchAll() const { return Type.RTTI == nullptr; }\n  };\n\nprivate:\n  friend class EHScopeStack;\n\n  Handler *getHandlers() {\n    return reinterpret_cast<Handler*>(this+1);\n  }\n\n  const Handler *getHandlers() const {\n    return reinterpret_cast<const Handler*>(this+1);\n  }\n\npublic:\n  static size_t getSizeForNumHandlers(unsigned N) {\n    return sizeof(EHCatchScope) + N * sizeof(Handler);\n  }\n\n  EHCatchScope(unsigned numHandlers,\n               EHScopeStack::stable_iterator enclosingEHScope)\n    : EHScope(Catch, enclosingEHScope) {\n    CatchBits.NumHandlers = numHandlers;\n    assert(CatchBits.NumHandlers == numHandlers && \"NumHandlers overflow?\");\n  }\n\n  unsigned getNumHandlers() const {\n    return CatchBits.NumHandlers;\n  }\n\n  void setCatchAllHandler(unsigned I, llvm::BasicBlock *Block) {\n    setHandler(I, CatchTypeInfo{nullptr, 0}, Block);\n  }\n\n  void setHandler(unsigned I, llvm::Constant *Type, llvm::BasicBlock *Block) {\n    assert(I < getNumHandlers());\n    getHandlers()[I].Type = CatchTypeInfo{Type, 0};\n    getHandlers()[I].Block = Block;\n  }\n\n  void setHandler(unsigned I, CatchTypeInfo Type, llvm::BasicBlock *Block) {\n    assert(I < getNumHandlers());\n    getHandlers()[I].Type = Type;\n    getHandlers()[I].Block = Block;\n  }\n\n  const Handler &getHandler(unsigned I) const {\n    assert(I < getNumHandlers());\n    return getHandlers()[I];\n  }\n\n  // Clear all handler blocks.\n  // FIXME: it's better to always call clearHandlerBlocks in DTOR and have a\n  // 'takeHandler' or some such function which removes ownership from the\n  // EHCatchScope object if the handlers should live longer than EHCatchScope.\n  void clearHandlerBlocks() {\n    for (unsigned I = 0, N = getNumHandlers(); I != N; ++I)\n      delete getHandler(I).Block;\n  }\n\n  typedef const Handler *iterator;\n  iterator begin() const { return getHandlers(); }\n  iterator end() const { return getHandlers() + getNumHandlers(); }\n\n  static bool classof(const EHScope *Scope) {\n    return Scope->getKind() == Catch;\n  }\n};\n\n/// A cleanup scope which generates the cleanup blocks lazily.\nclass alignas(8) EHCleanupScope : public EHScope {\n  /// The nearest normal cleanup scope enclosing this one.\n  EHScopeStack::stable_iterator EnclosingNormal;\n\n  /// The nearest EH scope enclosing this one.\n  EHScopeStack::stable_iterator EnclosingEH;\n\n  /// The dual entry/exit block along the normal edge.  This is lazily\n  /// created if needed before the cleanup is popped.\n  llvm::BasicBlock *NormalBlock;\n\n  /// An optional i1 variable indicating whether this cleanup has been\n  /// activated yet.\n  llvm::AllocaInst *ActiveFlag;\n\n  /// Extra information required for cleanups that have resolved\n  /// branches through them.  This has to be allocated on the side\n  /// because everything on the cleanup stack has be trivially\n  /// movable.\n  struct ExtInfo {\n    /// The destinations of normal branch-afters and branch-throughs.\n    llvm::SmallPtrSet<llvm::BasicBlock*, 4> Branches;\n\n    /// Normal branch-afters.\n    SmallVector<std::pair<llvm::BasicBlock*,llvm::ConstantInt*>, 4>\n      BranchAfters;\n  };\n  mutable struct ExtInfo *ExtInfo;\n\n  /// The number of fixups required by enclosing scopes (not including\n  /// this one).  If this is the top cleanup scope, all the fixups\n  /// from this index onwards belong to this scope.\n  unsigned FixupDepth;\n\n  struct ExtInfo &getExtInfo() {\n    if (!ExtInfo) ExtInfo = new struct ExtInfo();\n    return *ExtInfo;\n  }\n\n  const struct ExtInfo &getExtInfo() const {\n    if (!ExtInfo) ExtInfo = new struct ExtInfo();\n    return *ExtInfo;\n  }\n\npublic:\n  /// Gets the size required for a lazy cleanup scope with the given\n  /// cleanup-data requirements.\n  static size_t getSizeForCleanupSize(size_t Size) {\n    return sizeof(EHCleanupScope) + Size;\n  }\n\n  size_t getAllocatedSize() const {\n    return sizeof(EHCleanupScope) + CleanupBits.CleanupSize;\n  }\n\n  EHCleanupScope(bool isNormal, bool isEH, unsigned cleanupSize,\n                 unsigned fixupDepth,\n                 EHScopeStack::stable_iterator enclosingNormal,\n                 EHScopeStack::stable_iterator enclosingEH)\n      : EHScope(EHScope::Cleanup, enclosingEH),\n        EnclosingNormal(enclosingNormal), NormalBlock(nullptr),\n        ActiveFlag(nullptr), ExtInfo(nullptr), FixupDepth(fixupDepth) {\n    CleanupBits.IsNormalCleanup = isNormal;\n    CleanupBits.IsEHCleanup = isEH;\n    CleanupBits.IsActive = true;\n    CleanupBits.IsLifetimeMarker = false;\n    CleanupBits.TestFlagInNormalCleanup = false;\n    CleanupBits.TestFlagInEHCleanup = false;\n    CleanupBits.CleanupSize = cleanupSize;\n\n    assert(CleanupBits.CleanupSize == cleanupSize && \"cleanup size overflow\");\n  }\n\n  void Destroy() {\n    delete ExtInfo;\n  }\n  // Objects of EHCleanupScope are not destructed. Use Destroy().\n  ~EHCleanupScope() = delete;\n\n  bool isNormalCleanup() const { return CleanupBits.IsNormalCleanup; }\n  llvm::BasicBlock *getNormalBlock() const { return NormalBlock; }\n  void setNormalBlock(llvm::BasicBlock *BB) { NormalBlock = BB; }\n\n  bool isEHCleanup() const { return CleanupBits.IsEHCleanup; }\n\n  bool isActive() const { return CleanupBits.IsActive; }\n  void setActive(bool A) { CleanupBits.IsActive = A; }\n\n  bool isLifetimeMarker() const { return CleanupBits.IsLifetimeMarker; }\n  void setLifetimeMarker() { CleanupBits.IsLifetimeMarker = true; }\n\n  bool hasActiveFlag() const { return ActiveFlag != nullptr; }\n  Address getActiveFlag() const {\n    return Address(ActiveFlag, CharUnits::One());\n  }\n  void setActiveFlag(Address Var) {\n    assert(Var.getAlignment().isOne());\n    ActiveFlag = cast<llvm::AllocaInst>(Var.getPointer());\n  }\n\n  void setTestFlagInNormalCleanup() {\n    CleanupBits.TestFlagInNormalCleanup = true;\n  }\n  bool shouldTestFlagInNormalCleanup() const {\n    return CleanupBits.TestFlagInNormalCleanup;\n  }\n\n  void setTestFlagInEHCleanup() {\n    CleanupBits.TestFlagInEHCleanup = true;\n  }\n  bool shouldTestFlagInEHCleanup() const {\n    return CleanupBits.TestFlagInEHCleanup;\n  }\n\n  unsigned getFixupDepth() const { return FixupDepth; }\n  EHScopeStack::stable_iterator getEnclosingNormalCleanup() const {\n    return EnclosingNormal;\n  }\n\n  size_t getCleanupSize() const { return CleanupBits.CleanupSize; }\n  void *getCleanupBuffer() { return this + 1; }\n\n  EHScopeStack::Cleanup *getCleanup() {\n    return reinterpret_cast<EHScopeStack::Cleanup*>(getCleanupBuffer());\n  }\n\n  /// True if this cleanup scope has any branch-afters or branch-throughs.\n  bool hasBranches() const { return ExtInfo && !ExtInfo->Branches.empty(); }\n\n  /// Add a branch-after to this cleanup scope.  A branch-after is a\n  /// branch from a point protected by this (normal) cleanup to a\n  /// point in the normal cleanup scope immediately containing it.\n  /// For example,\n  ///   for (;;) { A a; break; }\n  /// contains a branch-after.\n  ///\n  /// Branch-afters each have their own destination out of the\n  /// cleanup, guaranteed distinct from anything else threaded through\n  /// it.  Therefore branch-afters usually force a switch after the\n  /// cleanup.\n  void addBranchAfter(llvm::ConstantInt *Index,\n                      llvm::BasicBlock *Block) {\n    struct ExtInfo &ExtInfo = getExtInfo();\n    if (ExtInfo.Branches.insert(Block).second)\n      ExtInfo.BranchAfters.push_back(std::make_pair(Block, Index));\n  }\n\n  /// Return the number of unique branch-afters on this scope.\n  unsigned getNumBranchAfters() const {\n    return ExtInfo ? ExtInfo->BranchAfters.size() : 0;\n  }\n\n  llvm::BasicBlock *getBranchAfterBlock(unsigned I) const {\n    assert(I < getNumBranchAfters());\n    return ExtInfo->BranchAfters[I].first;\n  }\n\n  llvm::ConstantInt *getBranchAfterIndex(unsigned I) const {\n    assert(I < getNumBranchAfters());\n    return ExtInfo->BranchAfters[I].second;\n  }\n\n  /// Add a branch-through to this cleanup scope.  A branch-through is\n  /// a branch from a scope protected by this (normal) cleanup to an\n  /// enclosing scope other than the immediately-enclosing normal\n  /// cleanup scope.\n  ///\n  /// In the following example, the branch through B's scope is a\n  /// branch-through, while the branch through A's scope is a\n  /// branch-after:\n  ///   for (;;) { A a; B b; break; }\n  ///\n  /// All branch-throughs have a common destination out of the\n  /// cleanup, one possibly shared with the fall-through.  Therefore\n  /// branch-throughs usually don't force a switch after the cleanup.\n  ///\n  /// \\return true if the branch-through was new to this scope\n  bool addBranchThrough(llvm::BasicBlock *Block) {\n    return getExtInfo().Branches.insert(Block).second;\n  }\n\n  /// Determines if this cleanup scope has any branch throughs.\n  bool hasBranchThroughs() const {\n    if (!ExtInfo) return false;\n    return (ExtInfo->BranchAfters.size() != ExtInfo->Branches.size());\n  }\n\n  static bool classof(const EHScope *Scope) {\n    return (Scope->getKind() == Cleanup);\n  }\n};\n// NOTE: there's a bunch of different data classes tacked on after an\n// EHCleanupScope. It is asserted (in EHScopeStack::pushCleanup*) that\n// they don't require greater alignment than ScopeStackAlignment. So,\n// EHCleanupScope ought to have alignment equal to that -- not more\n// (would be misaligned by the stack allocator), and not less (would\n// break the appended classes).\nstatic_assert(alignof(EHCleanupScope) == EHScopeStack::ScopeStackAlignment,\n              \"EHCleanupScope expected alignment\");\n\n/// An exceptions scope which filters exceptions thrown through it.\n/// Only exceptions matching the filter types will be permitted to be\n/// thrown.\n///\n/// This is used to implement C++ exception specifications.\nclass EHFilterScope : public EHScope {\n  // Essentially ends in a flexible array member:\n  // llvm::Value *FilterTypes[0];\n\n  llvm::Value **getFilters() {\n    return reinterpret_cast<llvm::Value**>(this+1);\n  }\n\n  llvm::Value * const *getFilters() const {\n    return reinterpret_cast<llvm::Value* const *>(this+1);\n  }\n\npublic:\n  EHFilterScope(unsigned numFilters)\n    : EHScope(Filter, EHScopeStack::stable_end()) {\n    FilterBits.NumFilters = numFilters;\n    assert(FilterBits.NumFilters == numFilters && \"NumFilters overflow\");\n  }\n\n  static size_t getSizeForNumFilters(unsigned numFilters) {\n    return sizeof(EHFilterScope) + numFilters * sizeof(llvm::Value*);\n  }\n\n  unsigned getNumFilters() const { return FilterBits.NumFilters; }\n\n  void setFilter(unsigned i, llvm::Value *filterValue) {\n    assert(i < getNumFilters());\n    getFilters()[i] = filterValue;\n  }\n\n  llvm::Value *getFilter(unsigned i) const {\n    assert(i < getNumFilters());\n    return getFilters()[i];\n  }\n\n  static bool classof(const EHScope *scope) {\n    return scope->getKind() == Filter;\n  }\n};\n\n/// An exceptions scope which calls std::terminate if any exception\n/// reaches it.\nclass EHTerminateScope : public EHScope {\npublic:\n  EHTerminateScope(EHScopeStack::stable_iterator enclosingEHScope)\n    : EHScope(Terminate, enclosingEHScope) {}\n  static size_t getSize() { return sizeof(EHTerminateScope); }\n\n  static bool classof(const EHScope *scope) {\n    return scope->getKind() == Terminate;\n  }\n};\n\n/// A non-stable pointer into the scope stack.\nclass EHScopeStack::iterator {\n  char *Ptr;\n\n  friend class EHScopeStack;\n  explicit iterator(char *Ptr) : Ptr(Ptr) {}\n\npublic:\n  iterator() : Ptr(nullptr) {}\n\n  EHScope *get() const {\n    return reinterpret_cast<EHScope*>(Ptr);\n  }\n\n  EHScope *operator->() const { return get(); }\n  EHScope &operator*() const { return *get(); }\n\n  iterator &operator++() {\n    size_t Size;\n    switch (get()->getKind()) {\n    case EHScope::Catch:\n      Size = EHCatchScope::getSizeForNumHandlers(\n          static_cast<const EHCatchScope *>(get())->getNumHandlers());\n      break;\n\n    case EHScope::Filter:\n      Size = EHFilterScope::getSizeForNumFilters(\n          static_cast<const EHFilterScope *>(get())->getNumFilters());\n      break;\n\n    case EHScope::Cleanup:\n      Size = static_cast<const EHCleanupScope *>(get())->getAllocatedSize();\n      break;\n\n    case EHScope::Terminate:\n      Size = EHTerminateScope::getSize();\n      break;\n    }\n    Ptr += llvm::alignTo(Size, ScopeStackAlignment);\n    return *this;\n  }\n\n  iterator next() {\n    iterator copy = *this;\n    ++copy;\n    return copy;\n  }\n\n  iterator operator++(int) {\n    iterator copy = *this;\n    operator++();\n    return copy;\n  }\n\n  bool encloses(iterator other) const { return Ptr >= other.Ptr; }\n  bool strictlyEncloses(iterator other) const { return Ptr > other.Ptr; }\n\n  bool operator==(iterator other) const { return Ptr == other.Ptr; }\n  bool operator!=(iterator other) const { return Ptr != other.Ptr; }\n};\n\ninline EHScopeStack::iterator EHScopeStack::begin() const {\n  return iterator(StartOfData);\n}\n\ninline EHScopeStack::iterator EHScopeStack::end() const {\n  return iterator(EndOfBuffer);\n}\n\ninline void EHScopeStack::popCatch() {\n  assert(!empty() && \"popping exception stack when not empty\");\n\n  EHCatchScope &scope = cast<EHCatchScope>(*begin());\n  InnermostEHScope = scope.getEnclosingEHScope();\n  deallocate(EHCatchScope::getSizeForNumHandlers(scope.getNumHandlers()));\n}\n\ninline void EHScopeStack::popTerminate() {\n  assert(!empty() && \"popping exception stack when not empty\");\n\n  EHTerminateScope &scope = cast<EHTerminateScope>(*begin());\n  InnermostEHScope = scope.getEnclosingEHScope();\n  deallocate(EHTerminateScope::getSize());\n}\n\ninline EHScopeStack::iterator EHScopeStack::find(stable_iterator sp) const {\n  assert(sp.isValid() && \"finding invalid savepoint\");\n  assert(sp.Size <= stable_begin().Size && \"finding savepoint after pop\");\n  return iterator(EndOfBuffer - sp.Size);\n}\n\ninline EHScopeStack::stable_iterator\nEHScopeStack::stabilize(iterator ir) const {\n  assert(StartOfData <= ir.Ptr && ir.Ptr <= EndOfBuffer);\n  return stable_iterator(EndOfBuffer - ir.Ptr);\n}\n\n/// The exceptions personality for a function.\nstruct EHPersonality {\n  const char *PersonalityFn;\n\n  // If this is non-null, this personality requires a non-standard\n  // function for rethrowing an exception after a catchall cleanup.\n  // This function must have prototype void(void*).\n  const char *CatchallRethrowFn;\n\n  static const EHPersonality &get(CodeGenModule &CGM, const FunctionDecl *FD);\n  static const EHPersonality &get(CodeGenFunction &CGF);\n\n  static const EHPersonality GNU_C;\n  static const EHPersonality GNU_C_SJLJ;\n  static const EHPersonality GNU_C_SEH;\n  static const EHPersonality GNU_ObjC;\n  static const EHPersonality GNU_ObjC_SJLJ;\n  static const EHPersonality GNU_ObjC_SEH;\n  static const EHPersonality GNUstep_ObjC;\n  static const EHPersonality GNU_ObjCXX;\n  static const EHPersonality NeXT_ObjC;\n  static const EHPersonality GNU_CPlusPlus;\n  static const EHPersonality GNU_CPlusPlus_SJLJ;\n  static const EHPersonality GNU_CPlusPlus_SEH;\n  static const EHPersonality MSVC_except_handler;\n  static const EHPersonality MSVC_C_specific_handler;\n  static const EHPersonality MSVC_CxxFrameHandler3;\n  static const EHPersonality GNU_Wasm_CPlusPlus;\n  static const EHPersonality XL_CPlusPlus;\n\n  /// Does this personality use landingpads or the family of pad instructions\n  /// designed to form funclets?\n  bool usesFuncletPads() const {\n    return isMSVCPersonality() || isWasmPersonality();\n  }\n\n  bool isMSVCPersonality() const {\n    return this == &MSVC_except_handler || this == &MSVC_C_specific_handler ||\n           this == &MSVC_CxxFrameHandler3;\n  }\n\n  bool isWasmPersonality() const { return this == &GNU_Wasm_CPlusPlus; }\n\n  bool isMSVCXXPersonality() const { return this == &MSVC_CxxFrameHandler3; }\n};\n}\n}\n\n#endif\n"}, "41": {"id": 41, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "content": "//===--- CGExprScalar.cpp - Emit LLVM Code for Scalar Exprs ---------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This contains code to emit Expr nodes with scalar LLVM types as LLVM code.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"CGCXXABI.h\"\n#include \"CGCleanup.h\"\n#include \"CGDebugInfo.h\"\n#include \"CGObjCRuntime.h\"\n#include \"CGOpenMPRuntime.h\"\n#include \"CodeGenFunction.h\"\n#include \"CodeGenModule.h\"\n#include \"ConstantEmitter.h\"\n#include \"TargetInfo.h\"\n#include \"clang/AST/ASTContext.h\"\n#include \"clang/AST/Attr.h\"\n#include \"clang/AST/DeclObjC.h\"\n#include \"clang/AST/Expr.h\"\n#include \"clang/AST/RecordLayout.h\"\n#include \"clang/AST/StmtVisitor.h\"\n#include \"clang/Basic/CodeGenOptions.h\"\n#include \"clang/Basic/TargetInfo.h\"\n#include \"llvm/ADT/APFixedPoint.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/IR/CFG.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/FixedPointBuilder.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/GetElementPtrTypeIterator.h\"\n#include \"llvm/IR/GlobalVariable.h\"\n#include \"llvm/IR/Intrinsics.h\"\n#include \"llvm/IR/IntrinsicsPowerPC.h\"\n#include \"llvm/IR/MatrixBuilder.h\"\n#include \"llvm/IR/Module.h\"\n#include <cstdarg>\n\nusing namespace clang;\nusing namespace CodeGen;\nusing llvm::Value;\n\n//===----------------------------------------------------------------------===//\n//                         Scalar Expression Emitter\n//===----------------------------------------------------------------------===//\n\nnamespace {\n\n/// Determine whether the given binary operation may overflow.\n/// Sets \\p Result to the value of the operation for BO_Add, BO_Sub, BO_Mul,\n/// and signed BO_{Div,Rem}. For these opcodes, and for unsigned BO_{Div,Rem},\n/// the returned overflow check is precise. The returned value is 'true' for\n/// all other opcodes, to be conservative.\nbool mayHaveIntegerOverflow(llvm::ConstantInt *LHS, llvm::ConstantInt *RHS,\n                             BinaryOperator::Opcode Opcode, bool Signed,\n                             llvm::APInt &Result) {\n  // Assume overflow is possible, unless we can prove otherwise.\n  bool Overflow = true;\n  const auto &LHSAP = LHS->getValue();\n  const auto &RHSAP = RHS->getValue();\n  if (Opcode == BO_Add) {\n    if (Signed)\n      Result = LHSAP.sadd_ov(RHSAP, Overflow);\n    else\n      Result = LHSAP.uadd_ov(RHSAP, Overflow);\n  } else if (Opcode == BO_Sub) {\n    if (Signed)\n      Result = LHSAP.ssub_ov(RHSAP, Overflow);\n    else\n      Result = LHSAP.usub_ov(RHSAP, Overflow);\n  } else if (Opcode == BO_Mul) {\n    if (Signed)\n      Result = LHSAP.smul_ov(RHSAP, Overflow);\n    else\n      Result = LHSAP.umul_ov(RHSAP, Overflow);\n  } else if (Opcode == BO_Div || Opcode == BO_Rem) {\n    if (Signed && !RHS->isZero())\n      Result = LHSAP.sdiv_ov(RHSAP, Overflow);\n    else\n      return false;\n  }\n  return Overflow;\n}\n\nstruct BinOpInfo {\n  Value *LHS;\n  Value *RHS;\n  QualType Ty;  // Computation Type.\n  BinaryOperator::Opcode Opcode; // Opcode of BinOp to perform\n  FPOptions FPFeatures;\n  const Expr *E;      // Entire expr, for error unsupported.  May not be binop.\n\n  /// Check if the binop can result in integer overflow.\n  bool mayHaveIntegerOverflow() const {\n    // Without constant input, we can't rule out overflow.\n    auto *LHSCI = dyn_cast<llvm::ConstantInt>(LHS);\n    auto *RHSCI = dyn_cast<llvm::ConstantInt>(RHS);\n    if (!LHSCI || !RHSCI)\n      return true;\n\n    llvm::APInt Result;\n    return ::mayHaveIntegerOverflow(\n        LHSCI, RHSCI, Opcode, Ty->hasSignedIntegerRepresentation(), Result);\n  }\n\n  /// Check if the binop computes a division or a remainder.\n  bool isDivremOp() const {\n    return Opcode == BO_Div || Opcode == BO_Rem || Opcode == BO_DivAssign ||\n           Opcode == BO_RemAssign;\n  }\n\n  /// Check if the binop can result in an integer division by zero.\n  bool mayHaveIntegerDivisionByZero() const {\n    if (isDivremOp())\n      if (auto *CI = dyn_cast<llvm::ConstantInt>(RHS))\n        return CI->isZero();\n    return true;\n  }\n\n  /// Check if the binop can result in a float division by zero.\n  bool mayHaveFloatDivisionByZero() const {\n    if (isDivremOp())\n      if (auto *CFP = dyn_cast<llvm::ConstantFP>(RHS))\n        return CFP->isZero();\n    return true;\n  }\n\n  /// Check if at least one operand is a fixed point type. In such cases, this\n  /// operation did not follow usual arithmetic conversion and both operands\n  /// might not be of the same type.\n  bool isFixedPointOp() const {\n    // We cannot simply check the result type since comparison operations return\n    // an int.\n    if (const auto *BinOp = dyn_cast<BinaryOperator>(E)) {\n      QualType LHSType = BinOp->getLHS()->getType();\n      QualType RHSType = BinOp->getRHS()->getType();\n      return LHSType->isFixedPointType() || RHSType->isFixedPointType();\n    }\n    if (const auto *UnOp = dyn_cast<UnaryOperator>(E))\n      return UnOp->getSubExpr()->getType()->isFixedPointType();\n    return false;\n  }\n};\n\nstatic bool MustVisitNullValue(const Expr *E) {\n  // If a null pointer expression's type is the C++0x nullptr_t, then\n  // it's not necessarily a simple constant and it must be evaluated\n  // for its potential side effects.\n  return E->getType()->isNullPtrType();\n}\n\n/// If \\p E is a widened promoted integer, get its base (unpromoted) type.\nstatic llvm::Optional<QualType> getUnwidenedIntegerType(const ASTContext &Ctx,\n                                                        const Expr *E) {\n  const Expr *Base = E->IgnoreImpCasts();\n  if (E == Base)\n    return llvm::None;\n\n  QualType BaseTy = Base->getType();\n  if (!BaseTy->isPromotableIntegerType() ||\n      Ctx.getTypeSize(BaseTy) >= Ctx.getTypeSize(E->getType()))\n    return llvm::None;\n\n  return BaseTy;\n}\n\n/// Check if \\p E is a widened promoted integer.\nstatic bool IsWidenedIntegerOp(const ASTContext &Ctx, const Expr *E) {\n  return getUnwidenedIntegerType(Ctx, E).hasValue();\n}\n\n/// Check if we can skip the overflow check for \\p Op.\nstatic bool CanElideOverflowCheck(const ASTContext &Ctx, const BinOpInfo &Op) {\n  assert((isa<UnaryOperator>(Op.E) || isa<BinaryOperator>(Op.E)) &&\n         \"Expected a unary or binary operator\");\n\n  // If the binop has constant inputs and we can prove there is no overflow,\n  // we can elide the overflow check.\n  if (!Op.mayHaveIntegerOverflow())\n    return true;\n\n  // If a unary op has a widened operand, the op cannot overflow.\n  if (const auto *UO = dyn_cast<UnaryOperator>(Op.E))\n    return !UO->canOverflow();\n\n  // We usually don't need overflow checks for binops with widened operands.\n  // Multiplication with promoted unsigned operands is a special case.\n  const auto *BO = cast<BinaryOperator>(Op.E);\n  auto OptionalLHSTy = getUnwidenedIntegerType(Ctx, BO->getLHS());\n  if (!OptionalLHSTy)\n    return false;\n\n  auto OptionalRHSTy = getUnwidenedIntegerType(Ctx, BO->getRHS());\n  if (!OptionalRHSTy)\n    return false;\n\n  QualType LHSTy = *OptionalLHSTy;\n  QualType RHSTy = *OptionalRHSTy;\n\n  // This is the simple case: binops without unsigned multiplication, and with\n  // widened operands. No overflow check is needed here.\n  if ((Op.Opcode != BO_Mul && Op.Opcode != BO_MulAssign) ||\n      !LHSTy->isUnsignedIntegerType() || !RHSTy->isUnsignedIntegerType())\n    return true;\n\n  // For unsigned multiplication the overflow check can be elided if either one\n  // of the unpromoted types are less than half the size of the promoted type.\n  unsigned PromotedSize = Ctx.getTypeSize(Op.E->getType());\n  return (2 * Ctx.getTypeSize(LHSTy)) < PromotedSize ||\n         (2 * Ctx.getTypeSize(RHSTy)) < PromotedSize;\n}\n\nclass ScalarExprEmitter\n  : public StmtVisitor<ScalarExprEmitter, Value*> {\n  CodeGenFunction &CGF;\n  CGBuilderTy &Builder;\n  bool IgnoreResultAssign;\n  llvm::LLVMContext &VMContext;\npublic:\n\n  ScalarExprEmitter(CodeGenFunction &cgf, bool ira=false)\n    : CGF(cgf), Builder(CGF.Builder), IgnoreResultAssign(ira),\n      VMContext(cgf.getLLVMContext()) {\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                               Utilities\n  //===--------------------------------------------------------------------===//\n\n  bool TestAndClearIgnoreResultAssign() {\n    bool I = IgnoreResultAssign;\n    IgnoreResultAssign = false;\n    return I;\n  }\n\n  llvm::Type *ConvertType(QualType T) { return CGF.ConvertType(T); }\n  LValue EmitLValue(const Expr *E) { return CGF.EmitLValue(E); }\n  LValue EmitCheckedLValue(const Expr *E, CodeGenFunction::TypeCheckKind TCK) {\n    return CGF.EmitCheckedLValue(E, TCK);\n  }\n\n  void EmitBinOpCheck(ArrayRef<std::pair<Value *, SanitizerMask>> Checks,\n                      const BinOpInfo &Info);\n\n  Value *EmitLoadOfLValue(LValue LV, SourceLocation Loc) {\n    return CGF.EmitLoadOfLValue(LV, Loc).getScalarVal();\n  }\n\n  void EmitLValueAlignmentAssumption(const Expr *E, Value *V) {\n    const AlignValueAttr *AVAttr = nullptr;\n    if (const auto *DRE = dyn_cast<DeclRefExpr>(E)) {\n      const ValueDecl *VD = DRE->getDecl();\n\n      if (VD->getType()->isReferenceType()) {\n        if (const auto *TTy =\n            dyn_cast<TypedefType>(VD->getType().getNonReferenceType()))\n          AVAttr = TTy->getDecl()->getAttr<AlignValueAttr>();\n      } else {\n        // Assumptions for function parameters are emitted at the start of the\n        // function, so there is no need to repeat that here,\n        // unless the alignment-assumption sanitizer is enabled,\n        // then we prefer the assumption over alignment attribute\n        // on IR function param.\n        if (isa<ParmVarDecl>(VD) && !CGF.SanOpts.has(SanitizerKind::Alignment))\n          return;\n\n        AVAttr = VD->getAttr<AlignValueAttr>();\n      }\n    }\n\n    if (!AVAttr)\n      if (const auto *TTy =\n          dyn_cast<TypedefType>(E->getType()))\n        AVAttr = TTy->getDecl()->getAttr<AlignValueAttr>();\n\n    if (!AVAttr)\n      return;\n\n    Value *AlignmentValue = CGF.EmitScalarExpr(AVAttr->getAlignment());\n    llvm::ConstantInt *AlignmentCI = cast<llvm::ConstantInt>(AlignmentValue);\n    CGF.emitAlignmentAssumption(V, E, AVAttr->getLocation(), AlignmentCI);\n  }\n\n  /// EmitLoadOfLValue - Given an expression with complex type that represents a\n  /// value l-value, this method emits the address of the l-value, then loads\n  /// and returns the result.\n  Value *EmitLoadOfLValue(const Expr *E) {\n    Value *V = EmitLoadOfLValue(EmitCheckedLValue(E, CodeGenFunction::TCK_Load),\n                                E->getExprLoc());\n\n    EmitLValueAlignmentAssumption(E, V);\n    return V;\n  }\n\n  /// EmitConversionToBool - Convert the specified expression value to a\n  /// boolean (i1) truth value.  This is equivalent to \"Val != 0\".\n  Value *EmitConversionToBool(Value *Src, QualType DstTy);\n\n  /// Emit a check that a conversion from a floating-point type does not\n  /// overflow.\n  void EmitFloatConversionCheck(Value *OrigSrc, QualType OrigSrcType,\n                                Value *Src, QualType SrcType, QualType DstType,\n                                llvm::Type *DstTy, SourceLocation Loc);\n\n  /// Known implicit conversion check kinds.\n  /// Keep in sync with the enum of the same name in ubsan_handlers.h\n  enum ImplicitConversionCheckKind : unsigned char {\n    ICCK_IntegerTruncation = 0, // Legacy, was only used by clang 7.\n    ICCK_UnsignedIntegerTruncation = 1,\n    ICCK_SignedIntegerTruncation = 2,\n    ICCK_IntegerSignChange = 3,\n    ICCK_SignedIntegerTruncationOrSignChange = 4,\n  };\n\n  /// Emit a check that an [implicit] truncation of an integer  does not\n  /// discard any bits. It is not UB, so we use the value after truncation.\n  void EmitIntegerTruncationCheck(Value *Src, QualType SrcType, Value *Dst,\n                                  QualType DstType, SourceLocation Loc);\n\n  /// Emit a check that an [implicit] conversion of an integer does not change\n  /// the sign of the value. It is not UB, so we use the value after conversion.\n  /// NOTE: Src and Dst may be the exact same value! (point to the same thing)\n  void EmitIntegerSignChangeCheck(Value *Src, QualType SrcType, Value *Dst,\n                                  QualType DstType, SourceLocation Loc);\n\n  /// Emit a conversion from the specified type to the specified destination\n  /// type, both of which are LLVM scalar types.\n  struct ScalarConversionOpts {\n    bool TreatBooleanAsSigned;\n    bool EmitImplicitIntegerTruncationChecks;\n    bool EmitImplicitIntegerSignChangeChecks;\n\n    ScalarConversionOpts()\n        : TreatBooleanAsSigned(false),\n          EmitImplicitIntegerTruncationChecks(false),\n          EmitImplicitIntegerSignChangeChecks(false) {}\n\n    ScalarConversionOpts(clang::SanitizerSet SanOpts)\n        : TreatBooleanAsSigned(false),\n          EmitImplicitIntegerTruncationChecks(\n              SanOpts.hasOneOf(SanitizerKind::ImplicitIntegerTruncation)),\n          EmitImplicitIntegerSignChangeChecks(\n              SanOpts.has(SanitizerKind::ImplicitIntegerSignChange)) {}\n  };\n  Value *\n  EmitScalarConversion(Value *Src, QualType SrcTy, QualType DstTy,\n                       SourceLocation Loc,\n                       ScalarConversionOpts Opts = ScalarConversionOpts());\n\n  /// Convert between either a fixed point and other fixed point or fixed point\n  /// and an integer.\n  Value *EmitFixedPointConversion(Value *Src, QualType SrcTy, QualType DstTy,\n                                  SourceLocation Loc);\n\n  /// Emit a conversion from the specified complex type to the specified\n  /// destination type, where the destination type is an LLVM scalar type.\n  Value *EmitComplexToScalarConversion(CodeGenFunction::ComplexPairTy Src,\n                                       QualType SrcTy, QualType DstTy,\n                                       SourceLocation Loc);\n\n  /// EmitNullValue - Emit a value that corresponds to null for the given type.\n  Value *EmitNullValue(QualType Ty);\n\n  /// EmitFloatToBoolConversion - Perform an FP to boolean conversion.\n  Value *EmitFloatToBoolConversion(Value *V) {\n    // Compare against 0.0 for fp scalars.\n    llvm::Value *Zero = llvm::Constant::getNullValue(V->getType());\n    return Builder.CreateFCmpUNE(V, Zero, \"tobool\");\n  }\n\n  /// EmitPointerToBoolConversion - Perform a pointer to boolean conversion.\n  Value *EmitPointerToBoolConversion(Value *V, QualType QT) {\n    Value *Zero = CGF.CGM.getNullPointer(cast<llvm::PointerType>(V->getType()), QT);\n\n    return Builder.CreateICmpNE(V, Zero, \"tobool\");\n  }\n\n  Value *EmitIntToBoolConversion(Value *V) {\n    // Because of the type rules of C, we often end up computing a\n    // logical value, then zero extending it to int, then wanting it\n    // as a logical value again.  Optimize this common case.\n    if (llvm::ZExtInst *ZI = dyn_cast<llvm::ZExtInst>(V)) {\n      if (ZI->getOperand(0)->getType() == Builder.getInt1Ty()) {\n        Value *Result = ZI->getOperand(0);\n        // If there aren't any more uses, zap the instruction to save space.\n        // Note that there can be more uses, for example if this\n        // is the result of an assignment.\n        if (ZI->use_empty())\n          ZI->eraseFromParent();\n        return Result;\n      }\n    }\n\n    return Builder.CreateIsNotNull(V, \"tobool\");\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                            Visitor Methods\n  //===--------------------------------------------------------------------===//\n\n  Value *Visit(Expr *E) {\n    ApplyDebugLocation DL(CGF, E);\n    return StmtVisitor<ScalarExprEmitter, Value*>::Visit(E);\n  }\n\n  Value *VisitStmt(Stmt *S) {\n    S->dump(llvm::errs(), CGF.getContext());\n    llvm_unreachable(\"Stmt can't have complex result type!\");\n  }\n  Value *VisitExpr(Expr *S);\n\n  Value *VisitConstantExpr(ConstantExpr *E) {\n    if (Value *Result = ConstantEmitter(CGF).tryEmitConstantExpr(E)) {\n      if (E->isGLValue())\n        return CGF.Builder.CreateLoad(Address(\n            Result, CGF.getContext().getTypeAlignInChars(E->getType())));\n      return Result;\n    }\n    return Visit(E->getSubExpr());\n  }\n  Value *VisitParenExpr(ParenExpr *PE) {\n    return Visit(PE->getSubExpr());\n  }\n  Value *VisitSubstNonTypeTemplateParmExpr(SubstNonTypeTemplateParmExpr *E) {\n    return Visit(E->getReplacement());\n  }\n  Value *VisitGenericSelectionExpr(GenericSelectionExpr *GE) {\n    return Visit(GE->getResultExpr());\n  }\n  Value *VisitCoawaitExpr(CoawaitExpr *S) {\n    return CGF.EmitCoawaitExpr(*S).getScalarVal();\n  }\n  Value *VisitCoyieldExpr(CoyieldExpr *S) {\n    return CGF.EmitCoyieldExpr(*S).getScalarVal();\n  }\n  Value *VisitUnaryCoawait(const UnaryOperator *E) {\n    return Visit(E->getSubExpr());\n  }\n\n  // Leaves.\n  Value *VisitIntegerLiteral(const IntegerLiteral *E) {\n    return Builder.getInt(E->getValue());\n  }\n  Value *VisitFixedPointLiteral(const FixedPointLiteral *E) {\n    return Builder.getInt(E->getValue());\n  }\n  Value *VisitFloatingLiteral(const FloatingLiteral *E) {\n    return llvm::ConstantFP::get(VMContext, E->getValue());\n  }\n  Value *VisitCharacterLiteral(const CharacterLiteral *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n  Value *VisitObjCBoolLiteralExpr(const ObjCBoolLiteralExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n  Value *VisitCXXBoolLiteralExpr(const CXXBoolLiteralExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n  Value *VisitCXXScalarValueInitExpr(const CXXScalarValueInitExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n  Value *VisitGNUNullExpr(const GNUNullExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n  Value *VisitOffsetOfExpr(OffsetOfExpr *E);\n  Value *VisitUnaryExprOrTypeTraitExpr(const UnaryExprOrTypeTraitExpr *E);\n  Value *VisitAddrLabelExpr(const AddrLabelExpr *E) {\n    llvm::Value *V = CGF.GetAddrOfLabel(E->getLabel());\n    return Builder.CreateBitCast(V, ConvertType(E->getType()));\n  }\n\n  Value *VisitSizeOfPackExpr(SizeOfPackExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()),E->getPackLength());\n  }\n\n  Value *VisitPseudoObjectExpr(PseudoObjectExpr *E) {\n    return CGF.EmitPseudoObjectRValue(E).getScalarVal();\n  }\n\n  Value *VisitOpaqueValueExpr(OpaqueValueExpr *E) {\n    if (E->isGLValue())\n      return EmitLoadOfLValue(CGF.getOrCreateOpaqueLValueMapping(E),\n                              E->getExprLoc());\n\n    // Otherwise, assume the mapping is the scalar directly.\n    return CGF.getOrCreateOpaqueRValueMapping(E).getScalarVal();\n  }\n\n  // l-values.\n  Value *VisitDeclRefExpr(DeclRefExpr *E) {\n    if (CodeGenFunction::ConstantEmission Constant = CGF.tryEmitAsConstant(E))\n      return CGF.emitScalarConstant(Constant, E);\n    return EmitLoadOfLValue(E);\n  }\n\n  Value *VisitObjCSelectorExpr(ObjCSelectorExpr *E) {\n    return CGF.EmitObjCSelectorExpr(E);\n  }\n  Value *VisitObjCProtocolExpr(ObjCProtocolExpr *E) {\n    return CGF.EmitObjCProtocolExpr(E);\n  }\n  Value *VisitObjCIvarRefExpr(ObjCIvarRefExpr *E) {\n    return EmitLoadOfLValue(E);\n  }\n  Value *VisitObjCMessageExpr(ObjCMessageExpr *E) {\n    if (E->getMethodDecl() &&\n        E->getMethodDecl()->getReturnType()->isReferenceType())\n      return EmitLoadOfLValue(E);\n    return CGF.EmitObjCMessageExpr(E).getScalarVal();\n  }\n\n  Value *VisitObjCIsaExpr(ObjCIsaExpr *E) {\n    LValue LV = CGF.EmitObjCIsaExpr(E);\n    Value *V = CGF.EmitLoadOfLValue(LV, E->getExprLoc()).getScalarVal();\n    return V;\n  }\n\n  Value *VisitObjCAvailabilityCheckExpr(ObjCAvailabilityCheckExpr *E) {\n    VersionTuple Version = E->getVersion();\n\n    // If we're checking for a platform older than our minimum deployment\n    // target, we can fold the check away.\n    if (Version <= CGF.CGM.getTarget().getPlatformMinVersion())\n      return llvm::ConstantInt::get(Builder.getInt1Ty(), 1);\n\n    return CGF.EmitBuiltinAvailable(Version);\n  }\n\n  Value *VisitArraySubscriptExpr(ArraySubscriptExpr *E);\n  Value *VisitMatrixSubscriptExpr(MatrixSubscriptExpr *E);\n  Value *VisitShuffleVectorExpr(ShuffleVectorExpr *E);\n  Value *VisitConvertVectorExpr(ConvertVectorExpr *E);\n  Value *VisitMemberExpr(MemberExpr *E);\n  Value *VisitExtVectorElementExpr(Expr *E) { return EmitLoadOfLValue(E); }\n  Value *VisitCompoundLiteralExpr(CompoundLiteralExpr *E) {\n    // Strictly speaking, we shouldn't be calling EmitLoadOfLValue, which\n    // transitively calls EmitCompoundLiteralLValue, here in C++ since compound\n    // literals aren't l-values in C++. We do so simply because that's the\n    // cleanest way to handle compound literals in C++.\n    // See the discussion here: https://reviews.llvm.org/D64464\n    return EmitLoadOfLValue(E);\n  }\n\n  Value *VisitInitListExpr(InitListExpr *E);\n\n  Value *VisitArrayInitIndexExpr(ArrayInitIndexExpr *E) {\n    assert(CGF.getArrayInitIndex() &&\n           \"ArrayInitIndexExpr not inside an ArrayInitLoopExpr?\");\n    return CGF.getArrayInitIndex();\n  }\n\n  Value *VisitImplicitValueInitExpr(const ImplicitValueInitExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n  Value *VisitExplicitCastExpr(ExplicitCastExpr *E) {\n    CGF.CGM.EmitExplicitCastExprType(E, &CGF);\n    return VisitCastExpr(E);\n  }\n  Value *VisitCastExpr(CastExpr *E);\n\n  Value *VisitCallExpr(const CallExpr *E) {\n    if (E->getCallReturnType(CGF.getContext())->isReferenceType())\n      return EmitLoadOfLValue(E);\n\n    Value *V = CGF.EmitCallExpr(E).getScalarVal();\n\n    EmitLValueAlignmentAssumption(E, V);\n    return V;\n  }\n\n  Value *VisitStmtExpr(const StmtExpr *E);\n\n  // Unary Operators.\n  Value *VisitUnaryPostDec(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, false, false);\n  }\n  Value *VisitUnaryPostInc(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, true, false);\n  }\n  Value *VisitUnaryPreDec(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, false, true);\n  }\n  Value *VisitUnaryPreInc(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, true, true);\n  }\n\n  llvm::Value *EmitIncDecConsiderOverflowBehavior(const UnaryOperator *E,\n                                                  llvm::Value *InVal,\n                                                  bool IsInc);\n\n  llvm::Value *EmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                       bool isInc, bool isPre);\n\n\n  Value *VisitUnaryAddrOf(const UnaryOperator *E) {\n    if (isa<MemberPointerType>(E->getType())) // never sugared\n      return CGF.CGM.getMemberPointerConstant(E);\n\n    return EmitLValue(E->getSubExpr()).getPointer(CGF);\n  }\n  Value *VisitUnaryDeref(const UnaryOperator *E) {\n    if (E->getType()->isVoidType())\n      return Visit(E->getSubExpr()); // the actual value should be unused\n    return EmitLoadOfLValue(E);\n  }\n  Value *VisitUnaryPlus(const UnaryOperator *E) {\n    // This differs from gcc, though, most likely due to a bug in gcc.\n    TestAndClearIgnoreResultAssign();\n    return Visit(E->getSubExpr());\n  }\n  Value *VisitUnaryMinus    (const UnaryOperator *E);\n  Value *VisitUnaryNot      (const UnaryOperator *E);\n  Value *VisitUnaryLNot     (const UnaryOperator *E);\n  Value *VisitUnaryReal     (const UnaryOperator *E);\n  Value *VisitUnaryImag     (const UnaryOperator *E);\n  Value *VisitUnaryExtension(const UnaryOperator *E) {\n    return Visit(E->getSubExpr());\n  }\n\n  // C++\n  Value *VisitMaterializeTemporaryExpr(const MaterializeTemporaryExpr *E) {\n    return EmitLoadOfLValue(E);\n  }\n  Value *VisitSourceLocExpr(SourceLocExpr *SLE) {\n    auto &Ctx = CGF.getContext();\n    APValue Evaluated =\n        SLE->EvaluateInContext(Ctx, CGF.CurSourceLocExprScope.getDefaultExpr());\n    return ConstantEmitter(CGF).emitAbstract(SLE->getLocation(), Evaluated,\n                                             SLE->getType());\n  }\n\n  Value *VisitCXXDefaultArgExpr(CXXDefaultArgExpr *DAE) {\n    CodeGenFunction::CXXDefaultArgExprScope Scope(CGF, DAE);\n    return Visit(DAE->getExpr());\n  }\n  Value *VisitCXXDefaultInitExpr(CXXDefaultInitExpr *DIE) {\n    CodeGenFunction::CXXDefaultInitExprScope Scope(CGF, DIE);\n    return Visit(DIE->getExpr());\n  }\n  Value *VisitCXXThisExpr(CXXThisExpr *TE) {\n    return CGF.LoadCXXThis();\n  }\n\n  Value *VisitExprWithCleanups(ExprWithCleanups *E);\n  Value *VisitCXXNewExpr(const CXXNewExpr *E) {\n    return CGF.EmitCXXNewExpr(E);\n  }\n  Value *VisitCXXDeleteExpr(const CXXDeleteExpr *E) {\n    CGF.EmitCXXDeleteExpr(E);\n    return nullptr;\n  }\n\n  Value *VisitTypeTraitExpr(const TypeTraitExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n\n  Value *VisitConceptSpecializationExpr(const ConceptSpecializationExpr *E) {\n    return Builder.getInt1(E->isSatisfied());\n  }\n\n  Value *VisitRequiresExpr(const RequiresExpr *E) {\n    return Builder.getInt1(E->isSatisfied());\n  }\n\n  Value *VisitArrayTypeTraitExpr(const ArrayTypeTraitExpr *E) {\n    return llvm::ConstantInt::get(Builder.getInt32Ty(), E->getValue());\n  }\n\n  Value *VisitExpressionTraitExpr(const ExpressionTraitExpr *E) {\n    return llvm::ConstantInt::get(Builder.getInt1Ty(), E->getValue());\n  }\n\n  Value *VisitCXXPseudoDestructorExpr(const CXXPseudoDestructorExpr *E) {\n    // C++ [expr.pseudo]p1:\n    //   The result shall only be used as the operand for the function call\n    //   operator (), and the result of such a call has type void. The only\n    //   effect is the evaluation of the postfix-expression before the dot or\n    //   arrow.\n    CGF.EmitScalarExpr(E->getBase());\n    return nullptr;\n  }\n\n  Value *VisitCXXNullPtrLiteralExpr(const CXXNullPtrLiteralExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n\n  Value *VisitCXXThrowExpr(const CXXThrowExpr *E) {\n    CGF.EmitCXXThrowExpr(E);\n    return nullptr;\n  }\n\n  Value *VisitCXXNoexceptExpr(const CXXNoexceptExpr *E) {\n    return Builder.getInt1(E->getValue());\n  }\n\n  // Binary Operators.\n  Value *EmitMul(const BinOpInfo &Ops) {\n    if (Ops.Ty->isSignedIntegerOrEnumerationType()) {\n      switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n      case LangOptions::SOB_Defined:\n        return Builder.CreateMul(Ops.LHS, Ops.RHS, \"mul\");\n      case LangOptions::SOB_Undefined:\n        if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n          return Builder.CreateNSWMul(Ops.LHS, Ops.RHS, \"mul\");\n        LLVM_FALLTHROUGH;\n      case LangOptions::SOB_Trapping:\n        if (CanElideOverflowCheck(CGF.getContext(), Ops))\n          return Builder.CreateNSWMul(Ops.LHS, Ops.RHS, \"mul\");\n        return EmitOverflowCheckedBinOp(Ops);\n      }\n    }\n\n    if (Ops.Ty->isConstantMatrixType()) {\n      llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n      // We need to check the types of the operands of the operator to get the\n      // correct matrix dimensions.\n      auto *BO = cast<BinaryOperator>(Ops.E);\n      auto *LHSMatTy = dyn_cast<ConstantMatrixType>(\n          BO->getLHS()->getType().getCanonicalType());\n      auto *RHSMatTy = dyn_cast<ConstantMatrixType>(\n          BO->getRHS()->getType().getCanonicalType());\n      if (LHSMatTy && RHSMatTy)\n        return MB.CreateMatrixMultiply(Ops.LHS, Ops.RHS, LHSMatTy->getNumRows(),\n                                       LHSMatTy->getNumColumns(),\n                                       RHSMatTy->getNumColumns());\n      return MB.CreateScalarMultiply(Ops.LHS, Ops.RHS);\n    }\n\n    if (Ops.Ty->isUnsignedIntegerType() &&\n        CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow) &&\n        !CanElideOverflowCheck(CGF.getContext(), Ops))\n      return EmitOverflowCheckedBinOp(Ops);\n\n    if (Ops.LHS->getType()->isFPOrFPVectorTy()) {\n      //  Preserve the old values\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, Ops.FPFeatures);\n      return Builder.CreateFMul(Ops.LHS, Ops.RHS, \"mul\");\n    }\n    if (Ops.isFixedPointOp())\n      return EmitFixedPointBinOp(Ops);\n    return Builder.CreateMul(Ops.LHS, Ops.RHS, \"mul\");\n  }\n  /// Create a binary op that checks for overflow.\n  /// Currently only supports +, - and *.\n  Value *EmitOverflowCheckedBinOp(const BinOpInfo &Ops);\n\n  // Check for undefined division and modulus behaviors.\n  void EmitUndefinedBehaviorIntegerDivAndRemCheck(const BinOpInfo &Ops,\n                                                  llvm::Value *Zero,bool isDiv);\n  // Common helper for getting how wide LHS of shift is.\n  static Value *GetWidthMinusOneValue(Value* LHS,Value* RHS);\n\n  // Used for shifting constraints for OpenCL, do mask for powers of 2, URem for\n  // non powers of two.\n  Value *ConstrainShiftValue(Value *LHS, Value *RHS, const Twine &Name);\n\n  Value *EmitDiv(const BinOpInfo &Ops);\n  Value *EmitRem(const BinOpInfo &Ops);\n  Value *EmitAdd(const BinOpInfo &Ops);\n  Value *EmitSub(const BinOpInfo &Ops);\n  Value *EmitShl(const BinOpInfo &Ops);\n  Value *EmitShr(const BinOpInfo &Ops);\n  Value *EmitAnd(const BinOpInfo &Ops) {\n    return Builder.CreateAnd(Ops.LHS, Ops.RHS, \"and\");\n  }\n  Value *EmitXor(const BinOpInfo &Ops) {\n    return Builder.CreateXor(Ops.LHS, Ops.RHS, \"xor\");\n  }\n  Value *EmitOr (const BinOpInfo &Ops) {\n    return Builder.CreateOr(Ops.LHS, Ops.RHS, \"or\");\n  }\n\n  // Helper functions for fixed point binary operations.\n  Value *EmitFixedPointBinOp(const BinOpInfo &Ops);\n\n  BinOpInfo EmitBinOps(const BinaryOperator *E);\n  LValue EmitCompoundAssignLValue(const CompoundAssignOperator *E,\n                            Value *(ScalarExprEmitter::*F)(const BinOpInfo &),\n                                  Value *&Result);\n\n  Value *EmitCompoundAssign(const CompoundAssignOperator *E,\n                            Value *(ScalarExprEmitter::*F)(const BinOpInfo &));\n\n  // Binary operators and binary compound assignment operators.\n#define HANDLEBINOP(OP) \\\n  Value *VisitBin ## OP(const BinaryOperator *E) {                         \\\n    return Emit ## OP(EmitBinOps(E));                                      \\\n  }                                                                        \\\n  Value *VisitBin ## OP ## Assign(const CompoundAssignOperator *E) {       \\\n    return EmitCompoundAssign(E, &ScalarExprEmitter::Emit ## OP);          \\\n  }\n  HANDLEBINOP(Mul)\n  HANDLEBINOP(Div)\n  HANDLEBINOP(Rem)\n  HANDLEBINOP(Add)\n  HANDLEBINOP(Sub)\n  HANDLEBINOP(Shl)\n  HANDLEBINOP(Shr)\n  HANDLEBINOP(And)\n  HANDLEBINOP(Xor)\n  HANDLEBINOP(Or)\n#undef HANDLEBINOP\n\n  // Comparisons.\n  Value *EmitCompare(const BinaryOperator *E, llvm::CmpInst::Predicate UICmpOpc,\n                     llvm::CmpInst::Predicate SICmpOpc,\n                     llvm::CmpInst::Predicate FCmpOpc, bool IsSignaling);\n#define VISITCOMP(CODE, UI, SI, FP, SIG) \\\n    Value *VisitBin##CODE(const BinaryOperator *E) { \\\n      return EmitCompare(E, llvm::ICmpInst::UI, llvm::ICmpInst::SI, \\\n                         llvm::FCmpInst::FP, SIG); }\n  VISITCOMP(LT, ICMP_ULT, ICMP_SLT, FCMP_OLT, true)\n  VISITCOMP(GT, ICMP_UGT, ICMP_SGT, FCMP_OGT, true)\n  VISITCOMP(LE, ICMP_ULE, ICMP_SLE, FCMP_OLE, true)\n  VISITCOMP(GE, ICMP_UGE, ICMP_SGE, FCMP_OGE, true)\n  VISITCOMP(EQ, ICMP_EQ , ICMP_EQ , FCMP_OEQ, false)\n  VISITCOMP(NE, ICMP_NE , ICMP_NE , FCMP_UNE, false)\n#undef VISITCOMP\n\n  Value *VisitBinAssign     (const BinaryOperator *E);\n\n  Value *VisitBinLAnd       (const BinaryOperator *E);\n  Value *VisitBinLOr        (const BinaryOperator *E);\n  Value *VisitBinComma      (const BinaryOperator *E);\n\n  Value *VisitBinPtrMemD(const Expr *E) { return EmitLoadOfLValue(E); }\n  Value *VisitBinPtrMemI(const Expr *E) { return EmitLoadOfLValue(E); }\n\n  Value *VisitCXXRewrittenBinaryOperator(CXXRewrittenBinaryOperator *E) {\n    return Visit(E->getSemanticForm());\n  }\n\n  // Other Operators.\n  Value *VisitBlockExpr(const BlockExpr *BE);\n  Value *VisitAbstractConditionalOperator(const AbstractConditionalOperator *);\n  Value *VisitChooseExpr(ChooseExpr *CE);\n  Value *VisitVAArgExpr(VAArgExpr *VE);\n  Value *VisitObjCStringLiteral(const ObjCStringLiteral *E) {\n    return CGF.EmitObjCStringLiteral(E);\n  }\n  Value *VisitObjCBoxedExpr(ObjCBoxedExpr *E) {\n    return CGF.EmitObjCBoxedExpr(E);\n  }\n  Value *VisitObjCArrayLiteral(ObjCArrayLiteral *E) {\n    return CGF.EmitObjCArrayLiteral(E);\n  }\n  Value *VisitObjCDictionaryLiteral(ObjCDictionaryLiteral *E) {\n    return CGF.EmitObjCDictionaryLiteral(E);\n  }\n  Value *VisitAsTypeExpr(AsTypeExpr *CE);\n  Value *VisitAtomicExpr(AtomicExpr *AE);\n};\n}  // end anonymous namespace.\n\n//===----------------------------------------------------------------------===//\n//                                Utilities\n//===----------------------------------------------------------------------===//\n\n/// EmitConversionToBool - Convert the specified expression value to a\n/// boolean (i1) truth value.  This is equivalent to \"Val != 0\".\nValue *ScalarExprEmitter::EmitConversionToBool(Value *Src, QualType SrcType) {\n  assert(SrcType.isCanonical() && \"EmitScalarConversion strips typedefs\");\n\n  if (SrcType->isRealFloatingType())\n    return EmitFloatToBoolConversion(Src);\n\n  if (const MemberPointerType *MPT = dyn_cast<MemberPointerType>(SrcType))\n    return CGF.CGM.getCXXABI().EmitMemberPointerIsNotNull(CGF, Src, MPT);\n\n  assert((SrcType->isIntegerType() || isa<llvm::PointerType>(Src->getType())) &&\n         \"Unknown scalar type to convert\");\n\n  if (isa<llvm::IntegerType>(Src->getType()))\n    return EmitIntToBoolConversion(Src);\n\n  assert(isa<llvm::PointerType>(Src->getType()));\n  return EmitPointerToBoolConversion(Src, SrcType);\n}\n\nvoid ScalarExprEmitter::EmitFloatConversionCheck(\n    Value *OrigSrc, QualType OrigSrcType, Value *Src, QualType SrcType,\n    QualType DstType, llvm::Type *DstTy, SourceLocation Loc) {\n  assert(SrcType->isFloatingType() && \"not a conversion from floating point\");\n  if (!isa<llvm::IntegerType>(DstTy))\n    return;\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n  using llvm::APFloat;\n  using llvm::APSInt;\n\n  llvm::Value *Check = nullptr;\n  const llvm::fltSemantics &SrcSema =\n    CGF.getContext().getFloatTypeSemantics(OrigSrcType);\n\n  // Floating-point to integer. This has undefined behavior if the source is\n  // +-Inf, NaN, or doesn't fit into the destination type (after truncation\n  // to an integer).\n  unsigned Width = CGF.getContext().getIntWidth(DstType);\n  bool Unsigned = DstType->isUnsignedIntegerOrEnumerationType();\n\n  APSInt Min = APSInt::getMinValue(Width, Unsigned);\n  APFloat MinSrc(SrcSema, APFloat::uninitialized);\n  if (MinSrc.convertFromAPInt(Min, !Unsigned, APFloat::rmTowardZero) &\n      APFloat::opOverflow)\n    // Don't need an overflow check for lower bound. Just check for\n    // -Inf/NaN.\n    MinSrc = APFloat::getInf(SrcSema, true);\n  else\n    // Find the largest value which is too small to represent (before\n    // truncation toward zero).\n    MinSrc.subtract(APFloat(SrcSema, 1), APFloat::rmTowardNegative);\n\n  APSInt Max = APSInt::getMaxValue(Width, Unsigned);\n  APFloat MaxSrc(SrcSema, APFloat::uninitialized);\n  if (MaxSrc.convertFromAPInt(Max, !Unsigned, APFloat::rmTowardZero) &\n      APFloat::opOverflow)\n    // Don't need an overflow check for upper bound. Just check for\n    // +Inf/NaN.\n    MaxSrc = APFloat::getInf(SrcSema, false);\n  else\n    // Find the smallest value which is too large to represent (before\n    // truncation toward zero).\n    MaxSrc.add(APFloat(SrcSema, 1), APFloat::rmTowardPositive);\n\n  // If we're converting from __half, convert the range to float to match\n  // the type of src.\n  if (OrigSrcType->isHalfType()) {\n    const llvm::fltSemantics &Sema =\n      CGF.getContext().getFloatTypeSemantics(SrcType);\n    bool IsInexact;\n    MinSrc.convert(Sema, APFloat::rmTowardZero, &IsInexact);\n    MaxSrc.convert(Sema, APFloat::rmTowardZero, &IsInexact);\n  }\n\n  llvm::Value *GE =\n    Builder.CreateFCmpOGT(Src, llvm::ConstantFP::get(VMContext, MinSrc));\n  llvm::Value *LE =\n    Builder.CreateFCmpOLT(Src, llvm::ConstantFP::get(VMContext, MaxSrc));\n  Check = Builder.CreateAnd(GE, LE);\n\n  llvm::Constant *StaticArgs[] = {CGF.EmitCheckSourceLocation(Loc),\n                                  CGF.EmitCheckTypeDescriptor(OrigSrcType),\n                                  CGF.EmitCheckTypeDescriptor(DstType)};\n  CGF.EmitCheck(std::make_pair(Check, SanitizerKind::FloatCastOverflow),\n                SanitizerHandler::FloatCastOverflow, StaticArgs, OrigSrc);\n}\n\n// Should be called within CodeGenFunction::SanitizerScope RAII scope.\n// Returns 'i1 false' when the truncation Src -> Dst was lossy.\nstatic std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n                 std::pair<llvm::Value *, SanitizerMask>>\nEmitIntegerTruncationCheckHelper(Value *Src, QualType SrcType, Value *Dst,\n                                 QualType DstType, CGBuilderTy &Builder) {\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = Dst->getType();\n  (void)DstTy; // Only used in assert()\n\n  // This should be truncation of integral types.\n  assert(Src != Dst);\n  assert(SrcTy->getScalarSizeInBits() > Dst->getType()->getScalarSizeInBits());\n  assert(isa<llvm::IntegerType>(SrcTy) && isa<llvm::IntegerType>(DstTy) &&\n         \"non-integer llvm type\");\n\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n\n  // If both (src and dst) types are unsigned, then it's an unsigned truncation.\n  // Else, it is a signed truncation.\n  ScalarExprEmitter::ImplicitConversionCheckKind Kind;\n  SanitizerMask Mask;\n  if (!SrcSigned && !DstSigned) {\n    Kind = ScalarExprEmitter::ICCK_UnsignedIntegerTruncation;\n    Mask = SanitizerKind::ImplicitUnsignedIntegerTruncation;\n  } else {\n    Kind = ScalarExprEmitter::ICCK_SignedIntegerTruncation;\n    Mask = SanitizerKind::ImplicitSignedIntegerTruncation;\n  }\n\n  llvm::Value *Check = nullptr;\n  // 1. Extend the truncated value back to the same width as the Src.\n  Check = Builder.CreateIntCast(Dst, SrcTy, DstSigned, \"anyext\");\n  // 2. Equality-compare with the original source value\n  Check = Builder.CreateICmpEQ(Check, Src, \"truncheck\");\n  // If the comparison result is 'i1 false', then the truncation was lossy.\n  return std::make_pair(Kind, std::make_pair(Check, Mask));\n}\n\nstatic bool PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(\n    QualType SrcType, QualType DstType) {\n  return SrcType->isIntegerType() && DstType->isIntegerType();\n}\n\nvoid ScalarExprEmitter::EmitIntegerTruncationCheck(Value *Src, QualType SrcType,\n                                                   Value *Dst, QualType DstType,\n                                                   SourceLocation Loc) {\n  if (!CGF.SanOpts.hasOneOf(SanitizerKind::ImplicitIntegerTruncation))\n    return;\n\n  // We only care about int->int conversions here.\n  // We ignore conversions to/from pointer and/or bool.\n  if (!PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(SrcType,\n                                                                       DstType))\n    return;\n\n  unsigned SrcBits = Src->getType()->getScalarSizeInBits();\n  unsigned DstBits = Dst->getType()->getScalarSizeInBits();\n  // This must be truncation. Else we do not care.\n  if (SrcBits <= DstBits)\n    return;\n\n  assert(!DstType->isBooleanType() && \"we should not get here with booleans.\");\n\n  // If the integer sign change sanitizer is enabled,\n  // and we are truncating from larger unsigned type to smaller signed type,\n  // let that next sanitizer deal with it.\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n  if (CGF.SanOpts.has(SanitizerKind::ImplicitIntegerSignChange) &&\n      (!SrcSigned && DstSigned))\n    return;\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n\n  std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n            std::pair<llvm::Value *, SanitizerMask>>\n      Check =\n          EmitIntegerTruncationCheckHelper(Src, SrcType, Dst, DstType, Builder);\n  // If the comparison result is 'i1 false', then the truncation was lossy.\n\n  // Do we care about this type of truncation?\n  if (!CGF.SanOpts.has(Check.second.second))\n    return;\n\n  llvm::Constant *StaticArgs[] = {\n      CGF.EmitCheckSourceLocation(Loc), CGF.EmitCheckTypeDescriptor(SrcType),\n      CGF.EmitCheckTypeDescriptor(DstType),\n      llvm::ConstantInt::get(Builder.getInt8Ty(), Check.first)};\n  CGF.EmitCheck(Check.second, SanitizerHandler::ImplicitConversion, StaticArgs,\n                {Src, Dst});\n}\n\n// Should be called within CodeGenFunction::SanitizerScope RAII scope.\n// Returns 'i1 false' when the conversion Src -> Dst changed the sign.\nstatic std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n                 std::pair<llvm::Value *, SanitizerMask>>\nEmitIntegerSignChangeCheckHelper(Value *Src, QualType SrcType, Value *Dst,\n                                 QualType DstType, CGBuilderTy &Builder) {\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = Dst->getType();\n\n  assert(isa<llvm::IntegerType>(SrcTy) && isa<llvm::IntegerType>(DstTy) &&\n         \"non-integer llvm type\");\n\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n  (void)SrcSigned; // Only used in assert()\n  (void)DstSigned; // Only used in assert()\n  unsigned SrcBits = SrcTy->getScalarSizeInBits();\n  unsigned DstBits = DstTy->getScalarSizeInBits();\n  (void)SrcBits; // Only used in assert()\n  (void)DstBits; // Only used in assert()\n\n  assert(((SrcBits != DstBits) || (SrcSigned != DstSigned)) &&\n         \"either the widths should be different, or the signednesses.\");\n\n  // NOTE: zero value is considered to be non-negative.\n  auto EmitIsNegativeTest = [&Builder](Value *V, QualType VType,\n                                       const char *Name) -> Value * {\n    // Is this value a signed type?\n    bool VSigned = VType->isSignedIntegerOrEnumerationType();\n    llvm::Type *VTy = V->getType();\n    if (!VSigned) {\n      // If the value is unsigned, then it is never negative.\n      // FIXME: can we encounter non-scalar VTy here?\n      return llvm::ConstantInt::getFalse(VTy->getContext());\n    }\n    // Get the zero of the same type with which we will be comparing.\n    llvm::Constant *Zero = llvm::ConstantInt::get(VTy, 0);\n    // %V.isnegative = icmp slt %V, 0\n    // I.e is %V *strictly* less than zero, does it have negative value?\n    return Builder.CreateICmp(llvm::ICmpInst::ICMP_SLT, V, Zero,\n                              llvm::Twine(Name) + \".\" + V->getName() +\n                                  \".negativitycheck\");\n  };\n\n  // 1. Was the old Value negative?\n  llvm::Value *SrcIsNegative = EmitIsNegativeTest(Src, SrcType, \"src\");\n  // 2. Is the new Value negative?\n  llvm::Value *DstIsNegative = EmitIsNegativeTest(Dst, DstType, \"dst\");\n  // 3. Now, was the 'negativity status' preserved during the conversion?\n  //    NOTE: conversion from negative to zero is considered to change the sign.\n  //    (We want to get 'false' when the conversion changed the sign)\n  //    So we should just equality-compare the negativity statuses.\n  llvm::Value *Check = nullptr;\n  Check = Builder.CreateICmpEQ(SrcIsNegative, DstIsNegative, \"signchangecheck\");\n  // If the comparison result is 'false', then the conversion changed the sign.\n  return std::make_pair(\n      ScalarExprEmitter::ICCK_IntegerSignChange,\n      std::make_pair(Check, SanitizerKind::ImplicitIntegerSignChange));\n}\n\nvoid ScalarExprEmitter::EmitIntegerSignChangeCheck(Value *Src, QualType SrcType,\n                                                   Value *Dst, QualType DstType,\n                                                   SourceLocation Loc) {\n  if (!CGF.SanOpts.has(SanitizerKind::ImplicitIntegerSignChange))\n    return;\n\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = Dst->getType();\n\n  // We only care about int->int conversions here.\n  // We ignore conversions to/from pointer and/or bool.\n  if (!PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(SrcType,\n                                                                       DstType))\n    return;\n\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n  unsigned SrcBits = SrcTy->getScalarSizeInBits();\n  unsigned DstBits = DstTy->getScalarSizeInBits();\n\n  // Now, we do not need to emit the check in *all* of the cases.\n  // We can avoid emitting it in some obvious cases where it would have been\n  // dropped by the opt passes (instcombine) always anyways.\n  // If it's a cast between effectively the same type, no check.\n  // NOTE: this is *not* equivalent to checking the canonical types.\n  if (SrcSigned == DstSigned && SrcBits == DstBits)\n    return;\n  // At least one of the values needs to have signed type.\n  // If both are unsigned, then obviously, neither of them can be negative.\n  if (!SrcSigned && !DstSigned)\n    return;\n  // If the conversion is to *larger* *signed* type, then no check is needed.\n  // Because either sign-extension happens (so the sign will remain),\n  // or zero-extension will happen (the sign bit will be zero.)\n  if ((DstBits > SrcBits) && DstSigned)\n    return;\n  if (CGF.SanOpts.has(SanitizerKind::ImplicitSignedIntegerTruncation) &&\n      (SrcBits > DstBits) && SrcSigned) {\n    // If the signed integer truncation sanitizer is enabled,\n    // and this is a truncation from signed type, then no check is needed.\n    // Because here sign change check is interchangeable with truncation check.\n    return;\n  }\n  // That's it. We can't rule out any more cases with the data we have.\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n\n  std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n            std::pair<llvm::Value *, SanitizerMask>>\n      Check;\n\n  // Each of these checks needs to return 'false' when an issue was detected.\n  ImplicitConversionCheckKind CheckKind;\n  llvm::SmallVector<std::pair<llvm::Value *, SanitizerMask>, 2> Checks;\n  // So we can 'and' all the checks together, and still get 'false',\n  // if at least one of the checks detected an issue.\n\n  Check = EmitIntegerSignChangeCheckHelper(Src, SrcType, Dst, DstType, Builder);\n  CheckKind = Check.first;\n  Checks.emplace_back(Check.second);\n\n  if (CGF.SanOpts.has(SanitizerKind::ImplicitSignedIntegerTruncation) &&\n      (SrcBits > DstBits) && !SrcSigned && DstSigned) {\n    // If the signed integer truncation sanitizer was enabled,\n    // and we are truncating from larger unsigned type to smaller signed type,\n    // let's handle the case we skipped in that check.\n    Check =\n        EmitIntegerTruncationCheckHelper(Src, SrcType, Dst, DstType, Builder);\n    CheckKind = ICCK_SignedIntegerTruncationOrSignChange;\n    Checks.emplace_back(Check.second);\n    // If the comparison result is 'i1 false', then the truncation was lossy.\n  }\n\n  llvm::Constant *StaticArgs[] = {\n      CGF.EmitCheckSourceLocation(Loc), CGF.EmitCheckTypeDescriptor(SrcType),\n      CGF.EmitCheckTypeDescriptor(DstType),\n      llvm::ConstantInt::get(Builder.getInt8Ty(), CheckKind)};\n  // EmitCheck() will 'and' all the checks together.\n  CGF.EmitCheck(Checks, SanitizerHandler::ImplicitConversion, StaticArgs,\n                {Src, Dst});\n}\n\n/// Emit a conversion from the specified type to the specified destination type,\n/// both of which are LLVM scalar types.\nValue *ScalarExprEmitter::EmitScalarConversion(Value *Src, QualType SrcType,\n                                               QualType DstType,\n                                               SourceLocation Loc,\n                                               ScalarConversionOpts Opts) {\n  // All conversions involving fixed point types should be handled by the\n  // EmitFixedPoint family functions. This is done to prevent bloating up this\n  // function more, and although fixed point numbers are represented by\n  // integers, we do not want to follow any logic that assumes they should be\n  // treated as integers.\n  // TODO(leonardchan): When necessary, add another if statement checking for\n  // conversions to fixed point types from other types.\n  if (SrcType->isFixedPointType()) {\n    if (DstType->isBooleanType())\n      // It is important that we check this before checking if the dest type is\n      // an integer because booleans are technically integer types.\n      // We do not need to check the padding bit on unsigned types if unsigned\n      // padding is enabled because overflow into this bit is undefined\n      // behavior.\n      return Builder.CreateIsNotNull(Src, \"tobool\");\n    if (DstType->isFixedPointType() || DstType->isIntegerType() ||\n        DstType->isRealFloatingType())\n      return EmitFixedPointConversion(Src, SrcType, DstType, Loc);\n\n    llvm_unreachable(\n        \"Unhandled scalar conversion from a fixed point type to another type.\");\n  } else if (DstType->isFixedPointType()) {\n    if (SrcType->isIntegerType() || SrcType->isRealFloatingType())\n      // This also includes converting booleans and enums to fixed point types.\n      return EmitFixedPointConversion(Src, SrcType, DstType, Loc);\n\n    llvm_unreachable(\n        \"Unhandled scalar conversion to a fixed point type from another type.\");\n  }\n\n  QualType NoncanonicalSrcType = SrcType;\n  QualType NoncanonicalDstType = DstType;\n\n  SrcType = CGF.getContext().getCanonicalType(SrcType);\n  DstType = CGF.getContext().getCanonicalType(DstType);\n  if (SrcType == DstType) return Src;\n\n  if (DstType->isVoidType()) return nullptr;\n\n  llvm::Value *OrigSrc = Src;\n  QualType OrigSrcType = SrcType;\n  llvm::Type *SrcTy = Src->getType();\n\n  // Handle conversions to bool first, they are special: comparisons against 0.\n  if (DstType->isBooleanType())\n    return EmitConversionToBool(Src, SrcType);\n\n  llvm::Type *DstTy = ConvertType(DstType);\n\n  // Cast from half through float if half isn't a native type.\n  if (SrcType->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n    // Cast to FP using the intrinsic if the half type itself isn't supported.\n    if (DstTy->isFloatingPointTy()) {\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics())\n        return Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_from_fp16, DstTy),\n            Src);\n    } else {\n      // Cast to other types through float, using either the intrinsic or FPExt,\n      // depending on whether the half type itself is supported\n      // (as opposed to operations on half, available with NativeHalfType).\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n        Src = Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_from_fp16,\n                                 CGF.CGM.FloatTy),\n            Src);\n      } else {\n        Src = Builder.CreateFPExt(Src, CGF.CGM.FloatTy, \"conv\");\n      }\n      SrcType = CGF.getContext().FloatTy;\n      SrcTy = CGF.FloatTy;\n    }\n  }\n\n  // Ignore conversions like int -> uint.\n  if (SrcTy == DstTy) {\n    if (Opts.EmitImplicitIntegerSignChangeChecks)\n      EmitIntegerSignChangeCheck(Src, NoncanonicalSrcType, Src,\n                                 NoncanonicalDstType, Loc);\n\n    return Src;\n  }\n\n  // Handle pointer conversions next: pointers can only be converted to/from\n  // other pointers and integers. Check for pointer types in terms of LLVM, as\n  // some native types (like Obj-C id) may map to a pointer type.\n  if (auto DstPT = dyn_cast<llvm::PointerType>(DstTy)) {\n    // The source value may be an integer, or a pointer.\n    if (isa<llvm::PointerType>(SrcTy))\n      return Builder.CreateBitCast(Src, DstTy, \"conv\");\n\n    assert(SrcType->isIntegerType() && \"Not ptr->ptr or int->ptr conversion?\");\n    // First, convert to the correct width so that we control the kind of\n    // extension.\n    llvm::Type *MiddleTy = CGF.CGM.getDataLayout().getIntPtrType(DstPT);\n    bool InputSigned = SrcType->isSignedIntegerOrEnumerationType();\n    llvm::Value* IntResult =\n        Builder.CreateIntCast(Src, MiddleTy, InputSigned, \"conv\");\n    // Then, cast to pointer.\n    return Builder.CreateIntToPtr(IntResult, DstTy, \"conv\");\n  }\n\n  if (isa<llvm::PointerType>(SrcTy)) {\n    // Must be an ptr to int cast.\n    assert(isa<llvm::IntegerType>(DstTy) && \"not ptr->int?\");\n    return Builder.CreatePtrToInt(Src, DstTy, \"conv\");\n  }\n\n  // A scalar can be splatted to an extended vector of the same element type\n  if (DstType->isExtVectorType() && !SrcType->isVectorType()) {\n    // Sema should add casts to make sure that the source expression's type is\n    // the same as the vector's element type (sans qualifiers)\n    assert(DstType->castAs<ExtVectorType>()->getElementType().getTypePtr() ==\n               SrcType.getTypePtr() &&\n           \"Splatted expr doesn't match with vector element type?\");\n\n    // Splat the element across to all elements\n    unsigned NumElements = cast<llvm::FixedVectorType>(DstTy)->getNumElements();\n    return Builder.CreateVectorSplat(NumElements, Src, \"splat\");\n  }\n\n  if (isa<llvm::VectorType>(SrcTy) || isa<llvm::VectorType>(DstTy)) {\n    // Allow bitcast from vector to integer/fp of the same size.\n    unsigned SrcSize = SrcTy->getPrimitiveSizeInBits();\n    unsigned DstSize = DstTy->getPrimitiveSizeInBits();\n    if (SrcSize == DstSize)\n      return Builder.CreateBitCast(Src, DstTy, \"conv\");\n\n    // Conversions between vectors of different sizes are not allowed except\n    // when vectors of half are involved. Operations on storage-only half\n    // vectors require promoting half vector operands to float vectors and\n    // truncating the result, which is either an int or float vector, to a\n    // short or half vector.\n\n    // Source and destination are both expected to be vectors.\n    llvm::Type *SrcElementTy = cast<llvm::VectorType>(SrcTy)->getElementType();\n    llvm::Type *DstElementTy = cast<llvm::VectorType>(DstTy)->getElementType();\n    (void)DstElementTy;\n\n    assert(((SrcElementTy->isIntegerTy() &&\n             DstElementTy->isIntegerTy()) ||\n            (SrcElementTy->isFloatingPointTy() &&\n             DstElementTy->isFloatingPointTy())) &&\n           \"unexpected conversion between a floating-point vector and an \"\n           \"integer vector\");\n\n    // Truncate an i32 vector to an i16 vector.\n    if (SrcElementTy->isIntegerTy())\n      return Builder.CreateIntCast(Src, DstTy, false, \"conv\");\n\n    // Truncate a float vector to a half vector.\n    if (SrcSize > DstSize)\n      return Builder.CreateFPTrunc(Src, DstTy, \"conv\");\n\n    // Promote a half vector to a float vector.\n    return Builder.CreateFPExt(Src, DstTy, \"conv\");\n  }\n\n  // Finally, we have the arithmetic types: real int/float.\n  Value *Res = nullptr;\n  llvm::Type *ResTy = DstTy;\n\n  // An overflowing conversion has undefined behavior if either the source type\n  // or the destination type is a floating-point type. However, we consider the\n  // range of representable values for all floating-point types to be\n  // [-inf,+inf], so no overflow can ever happen when the destination type is a\n  // floating-point type.\n  if (CGF.SanOpts.has(SanitizerKind::FloatCastOverflow) &&\n      OrigSrcType->isFloatingType())\n    EmitFloatConversionCheck(OrigSrc, OrigSrcType, Src, SrcType, DstType, DstTy,\n                             Loc);\n\n  // Cast to half through float if half isn't a native type.\n  if (DstType->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n    // Make sure we cast in a single step if from another FP type.\n    if (SrcTy->isFloatingPointTy()) {\n      // Use the intrinsic if the half type itself isn't supported\n      // (as opposed to operations on half, available with NativeHalfType).\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics())\n        return Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_to_fp16, SrcTy), Src);\n      // If the half type is supported, just use an fptrunc.\n      return Builder.CreateFPTrunc(Src, DstTy);\n    }\n    DstTy = CGF.FloatTy;\n  }\n\n  if (isa<llvm::IntegerType>(SrcTy)) {\n    bool InputSigned = SrcType->isSignedIntegerOrEnumerationType();\n    if (SrcType->isBooleanType() && Opts.TreatBooleanAsSigned) {\n      InputSigned = true;\n    }\n    if (isa<llvm::IntegerType>(DstTy))\n      Res = Builder.CreateIntCast(Src, DstTy, InputSigned, \"conv\");\n    else if (InputSigned)\n      Res = Builder.CreateSIToFP(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateUIToFP(Src, DstTy, \"conv\");\n  } else if (isa<llvm::IntegerType>(DstTy)) {\n    assert(SrcTy->isFloatingPointTy() && \"Unknown real conversion\");\n    if (DstType->isSignedIntegerOrEnumerationType())\n      Res = Builder.CreateFPToSI(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPToUI(Src, DstTy, \"conv\");\n  } else {\n    assert(SrcTy->isFloatingPointTy() && DstTy->isFloatingPointTy() &&\n           \"Unknown real conversion\");\n    if (DstTy->getTypeID() < SrcTy->getTypeID())\n      Res = Builder.CreateFPTrunc(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPExt(Src, DstTy, \"conv\");\n  }\n\n  if (DstTy != ResTy) {\n    if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n      assert(ResTy->isIntegerTy(16) && \"Only half FP requires extra conversion\");\n      Res = Builder.CreateCall(\n        CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_to_fp16, CGF.CGM.FloatTy),\n        Res);\n    } else {\n      Res = Builder.CreateFPTrunc(Res, ResTy, \"conv\");\n    }\n  }\n\n  if (Opts.EmitImplicitIntegerTruncationChecks)\n    EmitIntegerTruncationCheck(Src, NoncanonicalSrcType, Res,\n                               NoncanonicalDstType, Loc);\n\n  if (Opts.EmitImplicitIntegerSignChangeChecks)\n    EmitIntegerSignChangeCheck(Src, NoncanonicalSrcType, Res,\n                               NoncanonicalDstType, Loc);\n\n  return Res;\n}\n\nValue *ScalarExprEmitter::EmitFixedPointConversion(Value *Src, QualType SrcTy,\n                                                   QualType DstTy,\n                                                   SourceLocation Loc) {\n  llvm::FixedPointBuilder<CGBuilderTy> FPBuilder(Builder);\n  llvm::Value *Result;\n  if (SrcTy->isRealFloatingType())\n    Result = FPBuilder.CreateFloatingToFixed(Src,\n        CGF.getContext().getFixedPointSemantics(DstTy));\n  else if (DstTy->isRealFloatingType())\n    Result = FPBuilder.CreateFixedToFloating(Src,\n        CGF.getContext().getFixedPointSemantics(SrcTy),\n        ConvertType(DstTy));\n  else {\n    auto SrcFPSema = CGF.getContext().getFixedPointSemantics(SrcTy);\n    auto DstFPSema = CGF.getContext().getFixedPointSemantics(DstTy);\n\n    if (DstTy->isIntegerType())\n      Result = FPBuilder.CreateFixedToInteger(Src, SrcFPSema,\n                                              DstFPSema.getWidth(),\n                                              DstFPSema.isSigned());\n    else if (SrcTy->isIntegerType())\n      Result =  FPBuilder.CreateIntegerToFixed(Src, SrcFPSema.isSigned(),\n                                               DstFPSema);\n    else\n      Result = FPBuilder.CreateFixedToFixed(Src, SrcFPSema, DstFPSema);\n  }\n  return Result;\n}\n\n/// Emit a conversion from the specified complex type to the specified\n/// destination type, where the destination type is an LLVM scalar type.\nValue *ScalarExprEmitter::EmitComplexToScalarConversion(\n    CodeGenFunction::ComplexPairTy Src, QualType SrcTy, QualType DstTy,\n    SourceLocation Loc) {\n  // Get the source element type.\n  SrcTy = SrcTy->castAs<ComplexType>()->getElementType();\n\n  // Handle conversions to bool first, they are special: comparisons against 0.\n  if (DstTy->isBooleanType()) {\n    //  Complex != 0  -> (Real != 0) | (Imag != 0)\n    Src.first = EmitScalarConversion(Src.first, SrcTy, DstTy, Loc);\n    Src.second = EmitScalarConversion(Src.second, SrcTy, DstTy, Loc);\n    return Builder.CreateOr(Src.first, Src.second, \"tobool\");\n  }\n\n  // C99 6.3.1.7p2: \"When a value of complex type is converted to a real type,\n  // the imaginary part of the complex value is discarded and the value of the\n  // real part is converted according to the conversion rules for the\n  // corresponding real type.\n  return EmitScalarConversion(Src.first, SrcTy, DstTy, Loc);\n}\n\nValue *ScalarExprEmitter::EmitNullValue(QualType Ty) {\n  return CGF.EmitFromMemory(CGF.CGM.EmitNullConstant(Ty), Ty);\n}\n\n/// Emit a sanitization check for the given \"binary\" operation (which\n/// might actually be a unary increment which has been lowered to a binary\n/// operation). The check passes if all values in \\p Checks (which are \\c i1),\n/// are \\c true.\nvoid ScalarExprEmitter::EmitBinOpCheck(\n    ArrayRef<std::pair<Value *, SanitizerMask>> Checks, const BinOpInfo &Info) {\n  assert(CGF.IsSanitizerScope);\n  SanitizerHandler Check;\n  SmallVector<llvm::Constant *, 4> StaticData;\n  SmallVector<llvm::Value *, 2> DynamicData;\n\n  BinaryOperatorKind Opcode = Info.Opcode;\n  if (BinaryOperator::isCompoundAssignmentOp(Opcode))\n    Opcode = BinaryOperator::getOpForCompoundAssignment(Opcode);\n\n  StaticData.push_back(CGF.EmitCheckSourceLocation(Info.E->getExprLoc()));\n  const UnaryOperator *UO = dyn_cast<UnaryOperator>(Info.E);\n  if (UO && UO->getOpcode() == UO_Minus) {\n    Check = SanitizerHandler::NegateOverflow;\n    StaticData.push_back(CGF.EmitCheckTypeDescriptor(UO->getType()));\n    DynamicData.push_back(Info.RHS);\n  } else {\n    if (BinaryOperator::isShiftOp(Opcode)) {\n      // Shift LHS negative or too large, or RHS out of bounds.\n      Check = SanitizerHandler::ShiftOutOfBounds;\n      const BinaryOperator *BO = cast<BinaryOperator>(Info.E);\n      StaticData.push_back(\n        CGF.EmitCheckTypeDescriptor(BO->getLHS()->getType()));\n      StaticData.push_back(\n        CGF.EmitCheckTypeDescriptor(BO->getRHS()->getType()));\n    } else if (Opcode == BO_Div || Opcode == BO_Rem) {\n      // Divide or modulo by zero, or signed overflow (eg INT_MAX / -1).\n      Check = SanitizerHandler::DivremOverflow;\n      StaticData.push_back(CGF.EmitCheckTypeDescriptor(Info.Ty));\n    } else {\n      // Arithmetic overflow (+, -, *).\n      switch (Opcode) {\n      case BO_Add: Check = SanitizerHandler::AddOverflow; break;\n      case BO_Sub: Check = SanitizerHandler::SubOverflow; break;\n      case BO_Mul: Check = SanitizerHandler::MulOverflow; break;\n      default: llvm_unreachable(\"unexpected opcode for bin op check\");\n      }\n      StaticData.push_back(CGF.EmitCheckTypeDescriptor(Info.Ty));\n    }\n    DynamicData.push_back(Info.LHS);\n    DynamicData.push_back(Info.RHS);\n  }\n\n  CGF.EmitCheck(Checks, Check, StaticData, DynamicData);\n}\n\n//===----------------------------------------------------------------------===//\n//                            Visitor Methods\n//===----------------------------------------------------------------------===//\n\nValue *ScalarExprEmitter::VisitExpr(Expr *E) {\n  CGF.ErrorUnsupported(E, \"scalar expression\");\n  if (E->getType()->isVoidType())\n    return nullptr;\n  return llvm::UndefValue::get(CGF.ConvertType(E->getType()));\n}\n\nValue *ScalarExprEmitter::VisitShuffleVectorExpr(ShuffleVectorExpr *E) {\n  // Vector Mask Case\n  if (E->getNumSubExprs() == 2) {\n    Value *LHS = CGF.EmitScalarExpr(E->getExpr(0));\n    Value *RHS = CGF.EmitScalarExpr(E->getExpr(1));\n    Value *Mask;\n\n    auto *LTy = cast<llvm::FixedVectorType>(LHS->getType());\n    unsigned LHSElts = LTy->getNumElements();\n\n    Mask = RHS;\n\n    auto *MTy = cast<llvm::FixedVectorType>(Mask->getType());\n\n    // Mask off the high bits of each shuffle index.\n    Value *MaskBits =\n        llvm::ConstantInt::get(MTy, llvm::NextPowerOf2(LHSElts - 1) - 1);\n    Mask = Builder.CreateAnd(Mask, MaskBits, \"mask\");\n\n    // newv = undef\n    // mask = mask & maskbits\n    // for each elt\n    //   n = extract mask i\n    //   x = extract val n\n    //   newv = insert newv, x, i\n    auto *RTy = llvm::FixedVectorType::get(LTy->getElementType(),\n                                           MTy->getNumElements());\n    Value* NewV = llvm::UndefValue::get(RTy);\n    for (unsigned i = 0, e = MTy->getNumElements(); i != e; ++i) {\n      Value *IIndx = llvm::ConstantInt::get(CGF.SizeTy, i);\n      Value *Indx = Builder.CreateExtractElement(Mask, IIndx, \"shuf_idx\");\n\n      Value *VExt = Builder.CreateExtractElement(LHS, Indx, \"shuf_elt\");\n      NewV = Builder.CreateInsertElement(NewV, VExt, IIndx, \"shuf_ins\");\n    }\n    return NewV;\n  }\n\n  Value* V1 = CGF.EmitScalarExpr(E->getExpr(0));\n  Value* V2 = CGF.EmitScalarExpr(E->getExpr(1));\n\n  SmallVector<int, 32> Indices;\n  for (unsigned i = 2; i < E->getNumSubExprs(); ++i) {\n    llvm::APSInt Idx = E->getShuffleMaskIdx(CGF.getContext(), i-2);\n    // Check for -1 and output it as undef in the IR.\n    if (Idx.isSigned() && Idx.isAllOnesValue())\n      Indices.push_back(-1);\n    else\n      Indices.push_back(Idx.getZExtValue());\n  }\n\n  return Builder.CreateShuffleVector(V1, V2, Indices, \"shuffle\");\n}\n\nValue *ScalarExprEmitter::VisitConvertVectorExpr(ConvertVectorExpr *E) {\n  QualType SrcType = E->getSrcExpr()->getType(),\n           DstType = E->getType();\n\n  Value *Src  = CGF.EmitScalarExpr(E->getSrcExpr());\n\n  SrcType = CGF.getContext().getCanonicalType(SrcType);\n  DstType = CGF.getContext().getCanonicalType(DstType);\n  if (SrcType == DstType) return Src;\n\n  assert(SrcType->isVectorType() &&\n         \"ConvertVector source type must be a vector\");\n  assert(DstType->isVectorType() &&\n         \"ConvertVector destination type must be a vector\");\n\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = ConvertType(DstType);\n\n  // Ignore conversions like int -> uint.\n  if (SrcTy == DstTy)\n    return Src;\n\n  QualType SrcEltType = SrcType->castAs<VectorType>()->getElementType(),\n           DstEltType = DstType->castAs<VectorType>()->getElementType();\n\n  assert(SrcTy->isVectorTy() &&\n         \"ConvertVector source IR type must be a vector\");\n  assert(DstTy->isVectorTy() &&\n         \"ConvertVector destination IR type must be a vector\");\n\n  llvm::Type *SrcEltTy = cast<llvm::VectorType>(SrcTy)->getElementType(),\n             *DstEltTy = cast<llvm::VectorType>(DstTy)->getElementType();\n\n  if (DstEltType->isBooleanType()) {\n    assert((SrcEltTy->isFloatingPointTy() ||\n            isa<llvm::IntegerType>(SrcEltTy)) && \"Unknown boolean conversion\");\n\n    llvm::Value *Zero = llvm::Constant::getNullValue(SrcTy);\n    if (SrcEltTy->isFloatingPointTy()) {\n      return Builder.CreateFCmpUNE(Src, Zero, \"tobool\");\n    } else {\n      return Builder.CreateICmpNE(Src, Zero, \"tobool\");\n    }\n  }\n\n  // We have the arithmetic types: real int/float.\n  Value *Res = nullptr;\n\n  if (isa<llvm::IntegerType>(SrcEltTy)) {\n    bool InputSigned = SrcEltType->isSignedIntegerOrEnumerationType();\n    if (isa<llvm::IntegerType>(DstEltTy))\n      Res = Builder.CreateIntCast(Src, DstTy, InputSigned, \"conv\");\n    else if (InputSigned)\n      Res = Builder.CreateSIToFP(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateUIToFP(Src, DstTy, \"conv\");\n  } else if (isa<llvm::IntegerType>(DstEltTy)) {\n    assert(SrcEltTy->isFloatingPointTy() && \"Unknown real conversion\");\n    if (DstEltType->isSignedIntegerOrEnumerationType())\n      Res = Builder.CreateFPToSI(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPToUI(Src, DstTy, \"conv\");\n  } else {\n    assert(SrcEltTy->isFloatingPointTy() && DstEltTy->isFloatingPointTy() &&\n           \"Unknown real conversion\");\n    if (DstEltTy->getTypeID() < SrcEltTy->getTypeID())\n      Res = Builder.CreateFPTrunc(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPExt(Src, DstTy, \"conv\");\n  }\n\n  return Res;\n}\n\nValue *ScalarExprEmitter::VisitMemberExpr(MemberExpr *E) {\n  if (CodeGenFunction::ConstantEmission Constant = CGF.tryEmitAsConstant(E)) {\n    CGF.EmitIgnoredExpr(E->getBase());\n    return CGF.emitScalarConstant(Constant, E);\n  } else {\n    Expr::EvalResult Result;\n    if (E->EvaluateAsInt(Result, CGF.getContext(), Expr::SE_AllowSideEffects)) {\n      llvm::APSInt Value = Result.Val.getInt();\n      CGF.EmitIgnoredExpr(E->getBase());\n      return Builder.getInt(Value);\n    }\n  }\n\n  return EmitLoadOfLValue(E);\n}\n\nValue *ScalarExprEmitter::VisitArraySubscriptExpr(ArraySubscriptExpr *E) {\n  TestAndClearIgnoreResultAssign();\n\n  // Emit subscript expressions in rvalue context's.  For most cases, this just\n  // loads the lvalue formed by the subscript expr.  However, we have to be\n  // careful, because the base of a vector subscript is occasionally an rvalue,\n  // so we can't get it as an lvalue.\n  if (!E->getBase()->getType()->isVectorType())\n    return EmitLoadOfLValue(E);\n\n  // Handle the vector case.  The base must be a vector, the index must be an\n  // integer value.\n  Value *Base = Visit(E->getBase());\n  Value *Idx  = Visit(E->getIdx());\n  QualType IdxTy = E->getIdx()->getType();\n\n  if (CGF.SanOpts.has(SanitizerKind::ArrayBounds))\n    CGF.EmitBoundsCheck(E, E->getBase(), Idx, IdxTy, /*Accessed*/true);\n\n  return Builder.CreateExtractElement(Base, Idx, \"vecext\");\n}\n\nValue *ScalarExprEmitter::VisitMatrixSubscriptExpr(MatrixSubscriptExpr *E) {\n  TestAndClearIgnoreResultAssign();\n\n  // Handle the vector case.  The base must be a vector, the index must be an\n  // integer value.\n  Value *RowIdx = Visit(E->getRowIdx());\n  Value *ColumnIdx = Visit(E->getColumnIdx());\n  Value *Matrix = Visit(E->getBase());\n\n  // TODO: Should we emit bounds checks with SanitizerKind::ArrayBounds?\n  llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n  return MB.CreateExtractElement(\n      Matrix, RowIdx, ColumnIdx,\n      E->getBase()->getType()->getAs<ConstantMatrixType>()->getNumRows());\n}\n\nstatic int getMaskElt(llvm::ShuffleVectorInst *SVI, unsigned Idx,\n                      unsigned Off) {\n  int MV = SVI->getMaskValue(Idx);\n  if (MV == -1)\n    return -1;\n  return Off + MV;\n}\n\nstatic int getAsInt32(llvm::ConstantInt *C, llvm::Type *I32Ty) {\n  assert(llvm::ConstantInt::isValueValidForType(I32Ty, C->getZExtValue()) &&\n         \"Index operand too large for shufflevector mask!\");\n  return C->getZExtValue();\n}\n\nValue *ScalarExprEmitter::VisitInitListExpr(InitListExpr *E) {\n  bool Ignore = TestAndClearIgnoreResultAssign();\n  (void)Ignore;\n  assert (Ignore == false && \"init list ignored\");\n  unsigned NumInitElements = E->getNumInits();\n\n  if (E->hadArrayRangeDesignator())\n    CGF.ErrorUnsupported(E, \"GNU array range designator extension\");\n\n  llvm::VectorType *VType =\n    dyn_cast<llvm::VectorType>(ConvertType(E->getType()));\n\n  if (!VType) {\n    if (NumInitElements == 0) {\n      // C++11 value-initialization for the scalar.\n      return EmitNullValue(E->getType());\n    }\n    // We have a scalar in braces. Just use the first element.\n    return Visit(E->getInit(0));\n  }\n\n  unsigned ResElts = cast<llvm::FixedVectorType>(VType)->getNumElements();\n\n  // Loop over initializers collecting the Value for each, and remembering\n  // whether the source was swizzle (ExtVectorElementExpr).  This will allow\n  // us to fold the shuffle for the swizzle into the shuffle for the vector\n  // initializer, since LLVM optimizers generally do not want to touch\n  // shuffles.\n  unsigned CurIdx = 0;\n  bool VIsUndefShuffle = false;\n  llvm::Value *V = llvm::UndefValue::get(VType);\n  for (unsigned i = 0; i != NumInitElements; ++i) {\n    Expr *IE = E->getInit(i);\n    Value *Init = Visit(IE);\n    SmallVector<int, 16> Args;\n\n    llvm::VectorType *VVT = dyn_cast<llvm::VectorType>(Init->getType());\n\n    // Handle scalar elements.  If the scalar initializer is actually one\n    // element of a different vector of the same width, use shuffle instead of\n    // extract+insert.\n    if (!VVT) {\n      if (isa<ExtVectorElementExpr>(IE)) {\n        llvm::ExtractElementInst *EI = cast<llvm::ExtractElementInst>(Init);\n\n        if (cast<llvm::FixedVectorType>(EI->getVectorOperandType())\n                ->getNumElements() == ResElts) {\n          llvm::ConstantInt *C = cast<llvm::ConstantInt>(EI->getIndexOperand());\n          Value *LHS = nullptr, *RHS = nullptr;\n          if (CurIdx == 0) {\n            // insert into undef -> shuffle (src, undef)\n            // shufflemask must use an i32\n            Args.push_back(getAsInt32(C, CGF.Int32Ty));\n            Args.resize(ResElts, -1);\n\n            LHS = EI->getVectorOperand();\n            RHS = V;\n            VIsUndefShuffle = true;\n          } else if (VIsUndefShuffle) {\n            // insert into undefshuffle && size match -> shuffle (v, src)\n            llvm::ShuffleVectorInst *SVV = cast<llvm::ShuffleVectorInst>(V);\n            for (unsigned j = 0; j != CurIdx; ++j)\n              Args.push_back(getMaskElt(SVV, j, 0));\n            Args.push_back(ResElts + C->getZExtValue());\n            Args.resize(ResElts, -1);\n\n            LHS = cast<llvm::ShuffleVectorInst>(V)->getOperand(0);\n            RHS = EI->getVectorOperand();\n            VIsUndefShuffle = false;\n          }\n          if (!Args.empty()) {\n            V = Builder.CreateShuffleVector(LHS, RHS, Args);\n            ++CurIdx;\n            continue;\n          }\n        }\n      }\n      V = Builder.CreateInsertElement(V, Init, Builder.getInt32(CurIdx),\n                                      \"vecinit\");\n      VIsUndefShuffle = false;\n      ++CurIdx;\n      continue;\n    }\n\n    unsigned InitElts = cast<llvm::FixedVectorType>(VVT)->getNumElements();\n\n    // If the initializer is an ExtVecEltExpr (a swizzle), and the swizzle's\n    // input is the same width as the vector being constructed, generate an\n    // optimized shuffle of the swizzle input into the result.\n    unsigned Offset = (CurIdx == 0) ? 0 : ResElts;\n    if (isa<ExtVectorElementExpr>(IE)) {\n      llvm::ShuffleVectorInst *SVI = cast<llvm::ShuffleVectorInst>(Init);\n      Value *SVOp = SVI->getOperand(0);\n      auto *OpTy = cast<llvm::FixedVectorType>(SVOp->getType());\n\n      if (OpTy->getNumElements() == ResElts) {\n        for (unsigned j = 0; j != CurIdx; ++j) {\n          // If the current vector initializer is a shuffle with undef, merge\n          // this shuffle directly into it.\n          if (VIsUndefShuffle) {\n            Args.push_back(getMaskElt(cast<llvm::ShuffleVectorInst>(V), j, 0));\n          } else {\n            Args.push_back(j);\n          }\n        }\n        for (unsigned j = 0, je = InitElts; j != je; ++j)\n          Args.push_back(getMaskElt(SVI, j, Offset));\n        Args.resize(ResElts, -1);\n\n        if (VIsUndefShuffle)\n          V = cast<llvm::ShuffleVectorInst>(V)->getOperand(0);\n\n        Init = SVOp;\n      }\n    }\n\n    // Extend init to result vector length, and then shuffle its contribution\n    // to the vector initializer into V.\n    if (Args.empty()) {\n      for (unsigned j = 0; j != InitElts; ++j)\n        Args.push_back(j);\n      Args.resize(ResElts, -1);\n      Init = Builder.CreateShuffleVector(Init, Args, \"vext\");\n\n      Args.clear();\n      for (unsigned j = 0; j != CurIdx; ++j)\n        Args.push_back(j);\n      for (unsigned j = 0; j != InitElts; ++j)\n        Args.push_back(j + Offset);\n      Args.resize(ResElts, -1);\n    }\n\n    // If V is undef, make sure it ends up on the RHS of the shuffle to aid\n    // merging subsequent shuffles into this one.\n    if (CurIdx == 0)\n      std::swap(V, Init);\n    V = Builder.CreateShuffleVector(V, Init, Args, \"vecinit\");\n    VIsUndefShuffle = isa<llvm::UndefValue>(Init);\n    CurIdx += InitElts;\n  }\n\n  // FIXME: evaluate codegen vs. shuffling against constant null vector.\n  // Emit remaining default initializers.\n  llvm::Type *EltTy = VType->getElementType();\n\n  // Emit remaining default initializers\n  for (/* Do not initialize i*/; CurIdx < ResElts; ++CurIdx) {\n    Value *Idx = Builder.getInt32(CurIdx);\n    llvm::Value *Init = llvm::Constant::getNullValue(EltTy);\n    V = Builder.CreateInsertElement(V, Init, Idx, \"vecinit\");\n  }\n  return V;\n}\n\nbool CodeGenFunction::ShouldNullCheckClassCastValue(const CastExpr *CE) {\n  const Expr *E = CE->getSubExpr();\n\n  if (CE->getCastKind() == CK_UncheckedDerivedToBase)\n    return false;\n\n  if (isa<CXXThisExpr>(E->IgnoreParens())) {\n    // We always assume that 'this' is never null.\n    return false;\n  }\n\n  if (const ImplicitCastExpr *ICE = dyn_cast<ImplicitCastExpr>(CE)) {\n    // And that glvalue casts are never null.\n    if (ICE->getValueKind() != VK_RValue)\n      return false;\n  }\n\n  return true;\n}\n\n// VisitCastExpr - Emit code for an explicit or implicit cast.  Implicit casts\n// have to handle a more broad range of conversions than explicit casts, as they\n// handle things like function to ptr-to-function decay etc.\nValue *ScalarExprEmitter::VisitCastExpr(CastExpr *CE) {\n  Expr *E = CE->getSubExpr();\n  QualType DestTy = CE->getType();\n  CastKind Kind = CE->getCastKind();\n\n  // These cases are generally not written to ignore the result of\n  // evaluating their sub-expressions, so we clear this now.\n  bool Ignored = TestAndClearIgnoreResultAssign();\n\n  // Since almost all cast kinds apply to scalars, this switch doesn't have\n  // a default case, so the compiler will warn on a missing case.  The cases\n  // are in the same order as in the CastKind enum.\n  switch (Kind) {\n  case CK_Dependent: llvm_unreachable(\"dependent cast kind in IR gen!\");\n  case CK_BuiltinFnToFnPtr:\n    llvm_unreachable(\"builtin functions are handled elsewhere\");\n\n  case CK_LValueBitCast:\n  case CK_ObjCObjectLValueCast: {\n    Address Addr = EmitLValue(E).getAddress(CGF);\n    Addr = Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(DestTy));\n    LValue LV = CGF.MakeAddrLValue(Addr, DestTy);\n    return EmitLoadOfLValue(LV, CE->getExprLoc());\n  }\n\n  case CK_LValueToRValueBitCast: {\n    LValue SourceLVal = CGF.EmitLValue(E);\n    Address Addr = Builder.CreateElementBitCast(SourceLVal.getAddress(CGF),\n                                                CGF.ConvertTypeForMem(DestTy));\n    LValue DestLV = CGF.MakeAddrLValue(Addr, DestTy);\n    DestLV.setTBAAInfo(TBAAAccessInfo::getMayAliasInfo());\n    return EmitLoadOfLValue(DestLV, CE->getExprLoc());\n  }\n\n  case CK_CPointerToObjCPointerCast:\n  case CK_BlockPointerToObjCPointerCast:\n  case CK_AnyPointerToBlockPointerCast:\n  case CK_BitCast: {\n    Value *Src = Visit(const_cast<Expr*>(E));\n    llvm::Type *SrcTy = Src->getType();\n    llvm::Type *DstTy = ConvertType(DestTy);\n    if (SrcTy->isPtrOrPtrVectorTy() && DstTy->isPtrOrPtrVectorTy() &&\n        SrcTy->getPointerAddressSpace() != DstTy->getPointerAddressSpace()) {\n      llvm_unreachable(\"wrong cast for pointers in different address spaces\"\n                       \"(must be an address space cast)!\");\n    }\n\n    if (CGF.SanOpts.has(SanitizerKind::CFIUnrelatedCast)) {\n      if (auto PT = DestTy->getAs<PointerType>())\n        CGF.EmitVTablePtrCheckForCast(PT->getPointeeType(), Src,\n                                      /*MayBeNull=*/true,\n                                      CodeGenFunction::CFITCK_UnrelatedCast,\n                                      CE->getBeginLoc());\n    }\n\n    if (CGF.CGM.getCodeGenOpts().StrictVTablePointers) {\n      const QualType SrcType = E->getType();\n\n      if (SrcType.mayBeNotDynamicClass() && DestTy.mayBeDynamicClass()) {\n        // Casting to pointer that could carry dynamic information (provided by\n        // invariant.group) requires launder.\n        Src = Builder.CreateLaunderInvariantGroup(Src);\n      } else if (SrcType.mayBeDynamicClass() && DestTy.mayBeNotDynamicClass()) {\n        // Casting to pointer that does not carry dynamic information (provided\n        // by invariant.group) requires stripping it.  Note that we don't do it\n        // if the source could not be dynamic type and destination could be\n        // dynamic because dynamic information is already laundered.  It is\n        // because launder(strip(src)) == launder(src), so there is no need to\n        // add extra strip before launder.\n        Src = Builder.CreateStripInvariantGroup(Src);\n      }\n    }\n\n    // Update heapallocsite metadata when there is an explicit pointer cast.\n    if (auto *CI = dyn_cast<llvm::CallBase>(Src)) {\n      if (CI->getMetadata(\"heapallocsite\") && isa<ExplicitCastExpr>(CE)) {\n        QualType PointeeType = DestTy->getPointeeType();\n        if (!PointeeType.isNull())\n          CGF.getDebugInfo()->addHeapAllocSiteMetadata(CI, PointeeType,\n                                                       CE->getExprLoc());\n      }\n    }\n\n    // If Src is a fixed vector and Dst is a scalable vector, and both have the\n    // same element type, use the llvm.experimental.vector.insert intrinsic to\n    // perform the bitcast.\n    if (const auto *FixedSrc = dyn_cast<llvm::FixedVectorType>(SrcTy)) {\n      if (const auto *ScalableDst = dyn_cast<llvm::ScalableVectorType>(DstTy)) {\n        if (FixedSrc->getElementType() == ScalableDst->getElementType()) {\n          llvm::Value *UndefVec = llvm::UndefValue::get(DstTy);\n          llvm::Value *Zero = llvm::Constant::getNullValue(CGF.CGM.Int64Ty);\n          return Builder.CreateInsertVector(DstTy, UndefVec, Src, Zero,\n                                            \"castScalableSve\");\n        }\n      }\n    }\n\n    // If Src is a scalable vector and Dst is a fixed vector, and both have the\n    // same element type, use the llvm.experimental.vector.extract intrinsic to\n    // perform the bitcast.\n    if (const auto *ScalableSrc = dyn_cast<llvm::ScalableVectorType>(SrcTy)) {\n      if (const auto *FixedDst = dyn_cast<llvm::FixedVectorType>(DstTy)) {\n        if (ScalableSrc->getElementType() == FixedDst->getElementType()) {\n          llvm::Value *Zero = llvm::Constant::getNullValue(CGF.CGM.Int64Ty);\n          return Builder.CreateExtractVector(DstTy, Src, Zero, \"castFixedSve\");\n        }\n      }\n    }\n\n    // Perform VLAT <-> VLST bitcast through memory.\n    // TODO: since the llvm.experimental.vector.{insert,extract} intrinsics\n    //       require the element types of the vectors to be the same, we\n    //       need to keep this around for casting between predicates, or more\n    //       generally for bitcasts between VLAT <-> VLST where the element\n    //       types of the vectors are not the same, until we figure out a better\n    //       way of doing these casts.\n    if ((isa<llvm::FixedVectorType>(SrcTy) &&\n         isa<llvm::ScalableVectorType>(DstTy)) ||\n        (isa<llvm::ScalableVectorType>(SrcTy) &&\n         isa<llvm::FixedVectorType>(DstTy))) {\n      if (const CallExpr *CE = dyn_cast<CallExpr>(E)) {\n        // Call expressions can't have a scalar return unless the return type\n        // is a reference type so an lvalue can't be emitted. Create a temp\n        // alloca to store the call, bitcast the address then load.\n        QualType RetTy = CE->getCallReturnType(CGF.getContext());\n        Address Addr =\n            CGF.CreateDefaultAlignTempAlloca(SrcTy, \"saved-call-rvalue\");\n        LValue LV = CGF.MakeAddrLValue(Addr, RetTy);\n        CGF.EmitStoreOfScalar(Src, LV);\n        Addr = Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(DestTy),\n                                            \"castFixedSve\");\n        LValue DestLV = CGF.MakeAddrLValue(Addr, DestTy);\n        DestLV.setTBAAInfo(TBAAAccessInfo::getMayAliasInfo());\n        return EmitLoadOfLValue(DestLV, CE->getExprLoc());\n      }\n\n      Address Addr = EmitLValue(E).getAddress(CGF);\n      Addr = Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(DestTy));\n      LValue DestLV = CGF.MakeAddrLValue(Addr, DestTy);\n      DestLV.setTBAAInfo(TBAAAccessInfo::getMayAliasInfo());\n      return EmitLoadOfLValue(DestLV, CE->getExprLoc());\n    }\n\n    return Builder.CreateBitCast(Src, DstTy);\n  }\n  case CK_AddressSpaceConversion: {\n    Expr::EvalResult Result;\n    if (E->EvaluateAsRValue(Result, CGF.getContext()) &&\n        Result.Val.isNullPointer()) {\n      // If E has side effect, it is emitted even if its final result is a\n      // null pointer. In that case, a DCE pass should be able to\n      // eliminate the useless instructions emitted during translating E.\n      if (Result.HasSideEffects)\n        Visit(E);\n      return CGF.CGM.getNullPointer(cast<llvm::PointerType>(\n          ConvertType(DestTy)), DestTy);\n    }\n    // Since target may map different address spaces in AST to the same address\n    // space, an address space conversion may end up as a bitcast.\n    return CGF.CGM.getTargetCodeGenInfo().performAddrSpaceCast(\n        CGF, Visit(E), E->getType()->getPointeeType().getAddressSpace(),\n        DestTy->getPointeeType().getAddressSpace(), ConvertType(DestTy));\n  }\n  case CK_AtomicToNonAtomic:\n  case CK_NonAtomicToAtomic:\n  case CK_NoOp:\n  case CK_UserDefinedConversion:\n    return Visit(const_cast<Expr*>(E));\n\n  case CK_BaseToDerived: {\n    const CXXRecordDecl *DerivedClassDecl = DestTy->getPointeeCXXRecordDecl();\n    assert(DerivedClassDecl && \"BaseToDerived arg isn't a C++ object pointer!\");\n\n    Address Base = CGF.EmitPointerWithAlignment(E);\n    Address Derived =\n      CGF.GetAddressOfDerivedClass(Base, DerivedClassDecl,\n                                   CE->path_begin(), CE->path_end(),\n                                   CGF.ShouldNullCheckClassCastValue(CE));\n\n    // C++11 [expr.static.cast]p11: Behavior is undefined if a downcast is\n    // performed and the object is not of the derived type.\n    if (CGF.sanitizePerformTypeCheck())\n      CGF.EmitTypeCheck(CodeGenFunction::TCK_DowncastPointer, CE->getExprLoc(),\n                        Derived.getPointer(), DestTy->getPointeeType());\n\n    if (CGF.SanOpts.has(SanitizerKind::CFIDerivedCast))\n      CGF.EmitVTablePtrCheckForCast(\n          DestTy->getPointeeType(), Derived.getPointer(),\n          /*MayBeNull=*/true, CodeGenFunction::CFITCK_DerivedCast,\n          CE->getBeginLoc());\n\n    return Derived.getPointer();\n  }\n  case CK_UncheckedDerivedToBase:\n  case CK_DerivedToBase: {\n    // The EmitPointerWithAlignment path does this fine; just discard\n    // the alignment.\n    return CGF.EmitPointerWithAlignment(CE).getPointer();\n  }\n\n  case CK_Dynamic: {\n    Address V = CGF.EmitPointerWithAlignment(E);\n    const CXXDynamicCastExpr *DCE = cast<CXXDynamicCastExpr>(CE);\n    return CGF.EmitDynamicCast(V, DCE);\n  }\n\n  case CK_ArrayToPointerDecay:\n    return CGF.EmitArrayToPointerDecay(E).getPointer();\n  case CK_FunctionToPointerDecay:\n    return EmitLValue(E).getPointer(CGF);\n\n  case CK_NullToPointer:\n    if (MustVisitNullValue(E))\n      CGF.EmitIgnoredExpr(E);\n\n    return CGF.CGM.getNullPointer(cast<llvm::PointerType>(ConvertType(DestTy)),\n                              DestTy);\n\n  case CK_NullToMemberPointer: {\n    if (MustVisitNullValue(E))\n      CGF.EmitIgnoredExpr(E);\n\n    const MemberPointerType *MPT = CE->getType()->getAs<MemberPointerType>();\n    return CGF.CGM.getCXXABI().EmitNullMemberPointer(MPT);\n  }\n\n  case CK_ReinterpretMemberPointer:\n  case CK_BaseToDerivedMemberPointer:\n  case CK_DerivedToBaseMemberPointer: {\n    Value *Src = Visit(E);\n\n    // Note that the AST doesn't distinguish between checked and\n    // unchecked member pointer conversions, so we always have to\n    // implement checked conversions here.  This is inefficient when\n    // actual control flow may be required in order to perform the\n    // check, which it is for data member pointers (but not member\n    // function pointers on Itanium and ARM).\n    return CGF.CGM.getCXXABI().EmitMemberPointerConversion(CGF, CE, Src);\n  }\n\n  case CK_ARCProduceObject:\n    return CGF.EmitARCRetainScalarExpr(E);\n  case CK_ARCConsumeObject:\n    return CGF.EmitObjCConsumeObject(E->getType(), Visit(E));\n  case CK_ARCReclaimReturnedObject:\n    return CGF.EmitARCReclaimReturnedObject(E, /*allowUnsafe*/ Ignored);\n  case CK_ARCExtendBlockObject:\n    return CGF.EmitARCExtendBlockObject(E);\n\n  case CK_CopyAndAutoreleaseBlockObject:\n    return CGF.EmitBlockCopyAndAutorelease(Visit(E), E->getType());\n\n  case CK_FloatingRealToComplex:\n  case CK_FloatingComplexCast:\n  case CK_IntegralRealToComplex:\n  case CK_IntegralComplexCast:\n  case CK_IntegralComplexToFloatingComplex:\n  case CK_FloatingComplexToIntegralComplex:\n  case CK_ConstructorConversion:\n  case CK_ToUnion:\n    llvm_unreachable(\"scalar cast to non-scalar value\");\n\n  case CK_LValueToRValue:\n    assert(CGF.getContext().hasSameUnqualifiedType(E->getType(), DestTy));\n    assert(E->isGLValue() && \"lvalue-to-rvalue applied to r-value!\");\n    return Visit(const_cast<Expr*>(E));\n\n  case CK_IntegralToPointer: {\n    Value *Src = Visit(const_cast<Expr*>(E));\n\n    // First, convert to the correct width so that we control the kind of\n    // extension.\n    auto DestLLVMTy = ConvertType(DestTy);\n    llvm::Type *MiddleTy = CGF.CGM.getDataLayout().getIntPtrType(DestLLVMTy);\n    bool InputSigned = E->getType()->isSignedIntegerOrEnumerationType();\n    llvm::Value* IntResult =\n      Builder.CreateIntCast(Src, MiddleTy, InputSigned, \"conv\");\n\n    auto *IntToPtr = Builder.CreateIntToPtr(IntResult, DestLLVMTy);\n\n    if (CGF.CGM.getCodeGenOpts().StrictVTablePointers) {\n      // Going from integer to pointer that could be dynamic requires reloading\n      // dynamic information from invariant.group.\n      if (DestTy.mayBeDynamicClass())\n        IntToPtr = Builder.CreateLaunderInvariantGroup(IntToPtr);\n    }\n    return IntToPtr;\n  }\n  case CK_PointerToIntegral: {\n    assert(!DestTy->isBooleanType() && \"bool should use PointerToBool\");\n    auto *PtrExpr = Visit(E);\n\n    if (CGF.CGM.getCodeGenOpts().StrictVTablePointers) {\n      const QualType SrcType = E->getType();\n\n      // Casting to integer requires stripping dynamic information as it does\n      // not carries it.\n      if (SrcType.mayBeDynamicClass())\n        PtrExpr = Builder.CreateStripInvariantGroup(PtrExpr);\n    }\n\n    return Builder.CreatePtrToInt(PtrExpr, ConvertType(DestTy));\n  }\n  case CK_ToVoid: {\n    CGF.EmitIgnoredExpr(E);\n    return nullptr;\n  }\n  case CK_VectorSplat: {\n    llvm::Type *DstTy = ConvertType(DestTy);\n    Value *Elt = Visit(const_cast<Expr*>(E));\n    // Splat the element across to all elements\n    unsigned NumElements = cast<llvm::FixedVectorType>(DstTy)->getNumElements();\n    return Builder.CreateVectorSplat(NumElements, Elt, \"splat\");\n  }\n\n  case CK_FixedPointCast:\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_FixedPointToBoolean:\n    assert(E->getType()->isFixedPointType() &&\n           \"Expected src type to be fixed point type\");\n    assert(DestTy->isBooleanType() && \"Expected dest type to be boolean type\");\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_FixedPointToIntegral:\n    assert(E->getType()->isFixedPointType() &&\n           \"Expected src type to be fixed point type\");\n    assert(DestTy->isIntegerType() && \"Expected dest type to be an integer\");\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_IntegralToFixedPoint:\n    assert(E->getType()->isIntegerType() &&\n           \"Expected src type to be an integer\");\n    assert(DestTy->isFixedPointType() &&\n           \"Expected dest type to be fixed point type\");\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_IntegralCast: {\n    ScalarConversionOpts Opts;\n    if (auto *ICE = dyn_cast<ImplicitCastExpr>(CE)) {\n      if (!ICE->isPartOfExplicitCast())\n        Opts = ScalarConversionOpts(CGF.SanOpts);\n    }\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc(), Opts);\n  }\n  case CK_IntegralToFloating:\n  case CK_FloatingToIntegral:\n  case CK_FloatingCast:\n  case CK_FixedPointToFloating:\n  case CK_FloatingToFixedPoint: {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, CE);\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n  }\n  case CK_BooleanToSignedIntegral: {\n    ScalarConversionOpts Opts;\n    Opts.TreatBooleanAsSigned = true;\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc(), Opts);\n  }\n  case CK_IntegralToBoolean:\n    return EmitIntToBoolConversion(Visit(E));\n  case CK_PointerToBoolean:\n    return EmitPointerToBoolConversion(Visit(E), E->getType());\n  case CK_FloatingToBoolean: {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, CE);\n    return EmitFloatToBoolConversion(Visit(E));\n  }\n  case CK_MemberPointerToBoolean: {\n    llvm::Value *MemPtr = Visit(E);\n    const MemberPointerType *MPT = E->getType()->getAs<MemberPointerType>();\n    return CGF.CGM.getCXXABI().EmitMemberPointerIsNotNull(CGF, MemPtr, MPT);\n  }\n\n  case CK_FloatingComplexToReal:\n  case CK_IntegralComplexToReal:\n    return CGF.EmitComplexExpr(E, false, true).first;\n\n  case CK_FloatingComplexToBoolean:\n  case CK_IntegralComplexToBoolean: {\n    CodeGenFunction::ComplexPairTy V = CGF.EmitComplexExpr(E);\n\n    // TODO: kill this function off, inline appropriate case here\n    return EmitComplexToScalarConversion(V, E->getType(), DestTy,\n                                         CE->getExprLoc());\n  }\n\n  case CK_ZeroToOCLOpaqueType: {\n    assert((DestTy->isEventT() || DestTy->isQueueT() ||\n            DestTy->isOCLIntelSubgroupAVCType()) &&\n           \"CK_ZeroToOCLEvent cast on non-event type\");\n    return llvm::Constant::getNullValue(ConvertType(DestTy));\n  }\n\n  case CK_IntToOCLSampler:\n    return CGF.CGM.createOpenCLIntToSamplerConversion(E, CGF);\n\n  } // end of switch\n\n  llvm_unreachable(\"unknown scalar cast\");\n}\n\nValue *ScalarExprEmitter::VisitStmtExpr(const StmtExpr *E) {\n  CodeGenFunction::StmtExprEvaluation eval(CGF);\n  Address RetAlloca = CGF.EmitCompoundStmt(*E->getSubStmt(),\n                                           !E->getType()->isVoidType());\n  if (!RetAlloca.isValid())\n    return nullptr;\n  return CGF.EmitLoadOfScalar(CGF.MakeAddrLValue(RetAlloca, E->getType()),\n                              E->getExprLoc());\n}\n\nValue *ScalarExprEmitter::VisitExprWithCleanups(ExprWithCleanups *E) {\n  CodeGenFunction::RunCleanupsScope Scope(CGF);\n  Value *V = Visit(E->getSubExpr());\n  // Defend against dominance problems caused by jumps out of expression\n  // evaluation through the shared cleanup block.\n  Scope.ForceCleanup({&V});\n  return V;\n}\n\n//===----------------------------------------------------------------------===//\n//                             Unary Operators\n//===----------------------------------------------------------------------===//\n\nstatic BinOpInfo createBinOpInfoFromIncDec(const UnaryOperator *E,\n                                           llvm::Value *InVal, bool IsInc,\n                                           FPOptions FPFeatures) {\n  BinOpInfo BinOp;\n  BinOp.LHS = InVal;\n  BinOp.RHS = llvm::ConstantInt::get(InVal->getType(), 1, false);\n  BinOp.Ty = E->getType();\n  BinOp.Opcode = IsInc ? BO_Add : BO_Sub;\n  BinOp.FPFeatures = FPFeatures;\n  BinOp.E = E;\n  return BinOp;\n}\n\nllvm::Value *ScalarExprEmitter::EmitIncDecConsiderOverflowBehavior(\n    const UnaryOperator *E, llvm::Value *InVal, bool IsInc) {\n  llvm::Value *Amount =\n      llvm::ConstantInt::get(InVal->getType(), IsInc ? 1 : -1, true);\n  StringRef Name = IsInc ? \"inc\" : \"dec\";\n  switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n  case LangOptions::SOB_Defined:\n    return Builder.CreateAdd(InVal, Amount, Name);\n  case LangOptions::SOB_Undefined:\n    if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n      return Builder.CreateNSWAdd(InVal, Amount, Name);\n    LLVM_FALLTHROUGH;\n  case LangOptions::SOB_Trapping:\n    if (!E->canOverflow())\n      return Builder.CreateNSWAdd(InVal, Amount, Name);\n    return EmitOverflowCheckedBinOp(createBinOpInfoFromIncDec(\n        E, InVal, IsInc, E->getFPFeaturesInEffect(CGF.getLangOpts())));\n  }\n  llvm_unreachable(\"Unknown SignedOverflowBehaviorTy\");\n}\n\nnamespace {\n/// Handles check and update for lastprivate conditional variables.\nclass OMPLastprivateConditionalUpdateRAII {\nprivate:\n  CodeGenFunction &CGF;\n  const UnaryOperator *E;\n\npublic:\n  OMPLastprivateConditionalUpdateRAII(CodeGenFunction &CGF,\n                                      const UnaryOperator *E)\n      : CGF(CGF), E(E) {}\n  ~OMPLastprivateConditionalUpdateRAII() {\n    if (CGF.getLangOpts().OpenMP)\n      CGF.CGM.getOpenMPRuntime().checkAndEmitLastprivateConditional(\n          CGF, E->getSubExpr());\n  }\n};\n} // namespace\n\nllvm::Value *\nScalarExprEmitter::EmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                           bool isInc, bool isPre) {\n  OMPLastprivateConditionalUpdateRAII OMPRegion(CGF, E);\n  QualType type = E->getSubExpr()->getType();\n  llvm::PHINode *atomicPHI = nullptr;\n  llvm::Value *value;\n  llvm::Value *input;\n\n  int amount = (isInc ? 1 : -1);\n  bool isSubtraction = !isInc;\n\n  if (const AtomicType *atomicTy = type->getAs<AtomicType>()) {\n    type = atomicTy->getValueType();\n    if (isInc && type->isBooleanType()) {\n      llvm::Value *True = CGF.EmitToMemory(Builder.getTrue(), type);\n      if (isPre) {\n        Builder.CreateStore(True, LV.getAddress(CGF), LV.isVolatileQualified())\n            ->setAtomic(llvm::AtomicOrdering::SequentiallyConsistent);\n        return Builder.getTrue();\n      }\n      // For atomic bool increment, we just store true and return it for\n      // preincrement, do an atomic swap with true for postincrement\n      return Builder.CreateAtomicRMW(\n          llvm::AtomicRMWInst::Xchg, LV.getPointer(CGF), True,\n          llvm::AtomicOrdering::SequentiallyConsistent);\n    }\n    // Special case for atomic increment / decrement on integers, emit\n    // atomicrmw instructions.  We skip this if we want to be doing overflow\n    // checking, and fall into the slow path with the atomic cmpxchg loop.\n    if (!type->isBooleanType() && type->isIntegerType() &&\n        !(type->isUnsignedIntegerType() &&\n          CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow)) &&\n        CGF.getLangOpts().getSignedOverflowBehavior() !=\n            LangOptions::SOB_Trapping) {\n      llvm::AtomicRMWInst::BinOp aop = isInc ? llvm::AtomicRMWInst::Add :\n        llvm::AtomicRMWInst::Sub;\n      llvm::Instruction::BinaryOps op = isInc ? llvm::Instruction::Add :\n        llvm::Instruction::Sub;\n      llvm::Value *amt = CGF.EmitToMemory(\n          llvm::ConstantInt::get(ConvertType(type), 1, true), type);\n      llvm::Value *old =\n          Builder.CreateAtomicRMW(aop, LV.getPointer(CGF), amt,\n                                  llvm::AtomicOrdering::SequentiallyConsistent);\n      return isPre ? Builder.CreateBinOp(op, old, amt) : old;\n    }\n    value = EmitLoadOfLValue(LV, E->getExprLoc());\n    input = value;\n    // For every other atomic operation, we need to emit a load-op-cmpxchg loop\n    llvm::BasicBlock *startBB = Builder.GetInsertBlock();\n    llvm::BasicBlock *opBB = CGF.createBasicBlock(\"atomic_op\", CGF.CurFn);\n    value = CGF.EmitToMemory(value, type);\n    Builder.CreateBr(opBB);\n    Builder.SetInsertPoint(opBB);\n    atomicPHI = Builder.CreatePHI(value->getType(), 2);\n    atomicPHI->addIncoming(value, startBB);\n    value = atomicPHI;\n  } else {\n    value = EmitLoadOfLValue(LV, E->getExprLoc());\n    input = value;\n  }\n\n  // Special case of integer increment that we have to check first: bool++.\n  // Due to promotion rules, we get:\n  //   bool++ -> bool = bool + 1\n  //          -> bool = (int)bool + 1\n  //          -> bool = ((int)bool + 1 != 0)\n  // An interesting aspect of this is that increment is always true.\n  // Decrement does not have this property.\n  if (isInc && type->isBooleanType()) {\n    value = Builder.getTrue();\n\n  // Most common case by far: integer increment.\n  } else if (type->isIntegerType()) {\n    QualType promotedType;\n    bool canPerformLossyDemotionCheck = false;\n    if (type->isPromotableIntegerType()) {\n      promotedType = CGF.getContext().getPromotedIntegerType(type);\n      assert(promotedType != type && \"Shouldn't promote to the same type.\");\n      canPerformLossyDemotionCheck = true;\n      canPerformLossyDemotionCheck &=\n          CGF.getContext().getCanonicalType(type) !=\n          CGF.getContext().getCanonicalType(promotedType);\n      canPerformLossyDemotionCheck &=\n          PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(\n              type, promotedType);\n      assert((!canPerformLossyDemotionCheck ||\n              type->isSignedIntegerOrEnumerationType() ||\n              promotedType->isSignedIntegerOrEnumerationType() ||\n              ConvertType(type)->getScalarSizeInBits() ==\n                  ConvertType(promotedType)->getScalarSizeInBits()) &&\n             \"The following check expects that if we do promotion to different \"\n             \"underlying canonical type, at least one of the types (either \"\n             \"base or promoted) will be signed, or the bitwidths will match.\");\n    }\n    if (CGF.SanOpts.hasOneOf(\n            SanitizerKind::ImplicitIntegerArithmeticValueChange) &&\n        canPerformLossyDemotionCheck) {\n      // While `x += 1` (for `x` with width less than int) is modeled as\n      // promotion+arithmetics+demotion, and we can catch lossy demotion with\n      // ease; inc/dec with width less than int can't overflow because of\n      // promotion rules, so we omit promotion+demotion, which means that we can\n      // not catch lossy \"demotion\". Because we still want to catch these cases\n      // when the sanitizer is enabled, we perform the promotion, then perform\n      // the increment/decrement in the wider type, and finally\n      // perform the demotion. This will catch lossy demotions.\n\n      value = EmitScalarConversion(value, type, promotedType, E->getExprLoc());\n      Value *amt = llvm::ConstantInt::get(value->getType(), amount, true);\n      value = Builder.CreateAdd(value, amt, isInc ? \"inc\" : \"dec\");\n      // Do pass non-default ScalarConversionOpts so that sanitizer check is\n      // emitted.\n      value = EmitScalarConversion(value, promotedType, type, E->getExprLoc(),\n                                   ScalarConversionOpts(CGF.SanOpts));\n\n      // Note that signed integer inc/dec with width less than int can't\n      // overflow because of promotion rules; we're just eliding a few steps\n      // here.\n    } else if (E->canOverflow() && type->isSignedIntegerOrEnumerationType()) {\n      value = EmitIncDecConsiderOverflowBehavior(E, value, isInc);\n    } else if (E->canOverflow() && type->isUnsignedIntegerType() &&\n               CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow)) {\n      value = EmitOverflowCheckedBinOp(createBinOpInfoFromIncDec(\n          E, value, isInc, E->getFPFeaturesInEffect(CGF.getLangOpts())));\n    } else {\n      llvm::Value *amt = llvm::ConstantInt::get(value->getType(), amount, true);\n      value = Builder.CreateAdd(value, amt, isInc ? \"inc\" : \"dec\");\n    }\n\n  // Next most common: pointer increment.\n  } else if (const PointerType *ptr = type->getAs<PointerType>()) {\n    QualType type = ptr->getPointeeType();\n\n    // VLA types don't have constant size.\n    if (const VariableArrayType *vla\n          = CGF.getContext().getAsVariableArrayType(type)) {\n      llvm::Value *numElts = CGF.getVLASize(vla).NumElts;\n      if (!isInc) numElts = Builder.CreateNSWNeg(numElts, \"vla.negsize\");\n      if (CGF.getLangOpts().isSignedOverflowDefined())\n        value = Builder.CreateGEP(value, numElts, \"vla.inc\");\n      else\n        value = CGF.EmitCheckedInBoundsGEP(\n            value, numElts, /*SignedIndices=*/false, isSubtraction,\n            E->getExprLoc(), \"vla.inc\");\n\n    // Arithmetic on function pointers (!) is just +-1.\n    } else if (type->isFunctionType()) {\n      llvm::Value *amt = Builder.getInt32(amount);\n\n      value = CGF.EmitCastToVoidPtr(value);\n      if (CGF.getLangOpts().isSignedOverflowDefined())\n        value = Builder.CreateGEP(value, amt, \"incdec.funcptr\");\n      else\n        value = CGF.EmitCheckedInBoundsGEP(value, amt, /*SignedIndices=*/false,\n                                           isSubtraction, E->getExprLoc(),\n                                           \"incdec.funcptr\");\n      value = Builder.CreateBitCast(value, input->getType());\n\n    // For everything else, we can just do a simple increment.\n    } else {\n      llvm::Value *amt = Builder.getInt32(amount);\n      if (CGF.getLangOpts().isSignedOverflowDefined())\n        value = Builder.CreateGEP(value, amt, \"incdec.ptr\");\n      else\n        value = CGF.EmitCheckedInBoundsGEP(value, amt, /*SignedIndices=*/false,\n                                           isSubtraction, E->getExprLoc(),\n                                           \"incdec.ptr\");\n    }\n\n  // Vector increment/decrement.\n  } else if (type->isVectorType()) {\n    if (type->hasIntegerRepresentation()) {\n      llvm::Value *amt = llvm::ConstantInt::get(value->getType(), amount);\n\n      value = Builder.CreateAdd(value, amt, isInc ? \"inc\" : \"dec\");\n    } else {\n      value = Builder.CreateFAdd(\n                  value,\n                  llvm::ConstantFP::get(value->getType(), amount),\n                  isInc ? \"inc\" : \"dec\");\n    }\n\n  // Floating point.\n  } else if (type->isRealFloatingType()) {\n    // Add the inc/dec to the real part.\n    llvm::Value *amt;\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, E);\n\n    if (type->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n      // Another special case: half FP increment should be done via float\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n        value = Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_from_fp16,\n                                 CGF.CGM.FloatTy),\n            input, \"incdec.conv\");\n      } else {\n        value = Builder.CreateFPExt(input, CGF.CGM.FloatTy, \"incdec.conv\");\n      }\n    }\n\n    if (value->getType()->isFloatTy())\n      amt = llvm::ConstantFP::get(VMContext,\n                                  llvm::APFloat(static_cast<float>(amount)));\n    else if (value->getType()->isDoubleTy())\n      amt = llvm::ConstantFP::get(VMContext,\n                                  llvm::APFloat(static_cast<double>(amount)));\n    else {\n      // Remaining types are Half, LongDouble or __float128. Convert from float.\n      llvm::APFloat F(static_cast<float>(amount));\n      bool ignored;\n      const llvm::fltSemantics *FS;\n      // Don't use getFloatTypeSemantics because Half isn't\n      // necessarily represented using the \"half\" LLVM type.\n      if (value->getType()->isFP128Ty())\n        FS = &CGF.getTarget().getFloat128Format();\n      else if (value->getType()->isHalfTy())\n        FS = &CGF.getTarget().getHalfFormat();\n      else\n        FS = &CGF.getTarget().getLongDoubleFormat();\n      F.convert(*FS, llvm::APFloat::rmTowardZero, &ignored);\n      amt = llvm::ConstantFP::get(VMContext, F);\n    }\n    value = Builder.CreateFAdd(value, amt, isInc ? \"inc\" : \"dec\");\n\n    if (type->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n        value = Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_to_fp16,\n                                 CGF.CGM.FloatTy),\n            value, \"incdec.conv\");\n      } else {\n        value = Builder.CreateFPTrunc(value, input->getType(), \"incdec.conv\");\n      }\n    }\n\n  // Fixed-point types.\n  } else if (type->isFixedPointType()) {\n    // Fixed-point types are tricky. In some cases, it isn't possible to\n    // represent a 1 or a -1 in the type at all. Piggyback off of\n    // EmitFixedPointBinOp to avoid having to reimplement saturation.\n    BinOpInfo Info;\n    Info.E = E;\n    Info.Ty = E->getType();\n    Info.Opcode = isInc ? BO_Add : BO_Sub;\n    Info.LHS = value;\n    Info.RHS = llvm::ConstantInt::get(value->getType(), 1, false);\n    // If the type is signed, it's better to represent this as +(-1) or -(-1),\n    // since -1 is guaranteed to be representable.\n    if (type->isSignedFixedPointType()) {\n      Info.Opcode = isInc ? BO_Sub : BO_Add;\n      Info.RHS = Builder.CreateNeg(Info.RHS);\n    }\n    // Now, convert from our invented integer literal to the type of the unary\n    // op. This will upscale and saturate if necessary. This value can become\n    // undef in some cases.\n    llvm::FixedPointBuilder<CGBuilderTy> FPBuilder(Builder);\n    auto DstSema = CGF.getContext().getFixedPointSemantics(Info.Ty);\n    Info.RHS = FPBuilder.CreateIntegerToFixed(Info.RHS, true, DstSema);\n    value = EmitFixedPointBinOp(Info);\n\n  // Objective-C pointer types.\n  } else {\n    const ObjCObjectPointerType *OPT = type->castAs<ObjCObjectPointerType>();\n    value = CGF.EmitCastToVoidPtr(value);\n\n    CharUnits size = CGF.getContext().getTypeSizeInChars(OPT->getObjectType());\n    if (!isInc) size = -size;\n    llvm::Value *sizeValue =\n      llvm::ConstantInt::get(CGF.SizeTy, size.getQuantity());\n\n    if (CGF.getLangOpts().isSignedOverflowDefined())\n      value = Builder.CreateGEP(value, sizeValue, \"incdec.objptr\");\n    else\n      value = CGF.EmitCheckedInBoundsGEP(value, sizeValue,\n                                         /*SignedIndices=*/false, isSubtraction,\n                                         E->getExprLoc(), \"incdec.objptr\");\n    value = Builder.CreateBitCast(value, input->getType());\n  }\n\n  if (atomicPHI) {\n    llvm::BasicBlock *curBlock = Builder.GetInsertBlock();\n    llvm::BasicBlock *contBB = CGF.createBasicBlock(\"atomic_cont\", CGF.CurFn);\n    auto Pair = CGF.EmitAtomicCompareExchange(\n        LV, RValue::get(atomicPHI), RValue::get(value), E->getExprLoc());\n    llvm::Value *old = CGF.EmitToMemory(Pair.first.getScalarVal(), type);\n    llvm::Value *success = Pair.second;\n    atomicPHI->addIncoming(old, curBlock);\n    Builder.CreateCondBr(success, contBB, atomicPHI->getParent());\n    Builder.SetInsertPoint(contBB);\n    return isPre ? value : input;\n  }\n\n  // Store the updated result through the lvalue.\n  if (LV.isBitField())\n    CGF.EmitStoreThroughBitfieldLValue(RValue::get(value), LV, &value);\n  else\n    CGF.EmitStoreThroughLValue(RValue::get(value), LV);\n\n  // If this is a postinc, return the value read from memory, otherwise use the\n  // updated value.\n  return isPre ? value : input;\n}\n\n\n\nValue *ScalarExprEmitter::VisitUnaryMinus(const UnaryOperator *E) {\n  TestAndClearIgnoreResultAssign();\n  Value *Op = Visit(E->getSubExpr());\n\n  // Generate a unary FNeg for FP ops.\n  if (Op->getType()->isFPOrFPVectorTy())\n    return Builder.CreateFNeg(Op, \"fneg\");\n\n  // Emit unary minus with EmitSub so we handle overflow cases etc.\n  BinOpInfo BinOp;\n  BinOp.RHS = Op;\n  BinOp.LHS = llvm::Constant::getNullValue(BinOp.RHS->getType());\n  BinOp.Ty = E->getType();\n  BinOp.Opcode = BO_Sub;\n  BinOp.FPFeatures = E->getFPFeaturesInEffect(CGF.getLangOpts());\n  BinOp.E = E;\n  return EmitSub(BinOp);\n}\n\nValue *ScalarExprEmitter::VisitUnaryNot(const UnaryOperator *E) {\n  TestAndClearIgnoreResultAssign();\n  Value *Op = Visit(E->getSubExpr());\n  return Builder.CreateNot(Op, \"neg\");\n}\n\nValue *ScalarExprEmitter::VisitUnaryLNot(const UnaryOperator *E) {\n  // Perform vector logical not on comparison with zero vector.\n  if (E->getType()->isVectorType() &&\n      E->getType()->castAs<VectorType>()->getVectorKind() ==\n          VectorType::GenericVector) {\n    Value *Oper = Visit(E->getSubExpr());\n    Value *Zero = llvm::Constant::getNullValue(Oper->getType());\n    Value *Result;\n    if (Oper->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(\n          CGF, E->getFPFeaturesInEffect(CGF.getLangOpts()));\n      Result = Builder.CreateFCmp(llvm::CmpInst::FCMP_OEQ, Oper, Zero, \"cmp\");\n    } else\n      Result = Builder.CreateICmp(llvm::CmpInst::ICMP_EQ, Oper, Zero, \"cmp\");\n    return Builder.CreateSExt(Result, ConvertType(E->getType()), \"sext\");\n  }\n\n  // Compare operand to zero.\n  Value *BoolVal = CGF.EvaluateExprAsBool(E->getSubExpr());\n\n  // Invert value.\n  // TODO: Could dynamically modify easy computations here.  For example, if\n  // the operand is an icmp ne, turn into icmp eq.\n  BoolVal = Builder.CreateNot(BoolVal, \"lnot\");\n\n  // ZExt result to the expr type.\n  return Builder.CreateZExt(BoolVal, ConvertType(E->getType()), \"lnot.ext\");\n}\n\nValue *ScalarExprEmitter::VisitOffsetOfExpr(OffsetOfExpr *E) {\n  // Try folding the offsetof to a constant.\n  Expr::EvalResult EVResult;\n  if (E->EvaluateAsInt(EVResult, CGF.getContext())) {\n    llvm::APSInt Value = EVResult.Val.getInt();\n    return Builder.getInt(Value);\n  }\n\n  // Loop over the components of the offsetof to compute the value.\n  unsigned n = E->getNumComponents();\n  llvm::Type* ResultType = ConvertType(E->getType());\n  llvm::Value* Result = llvm::Constant::getNullValue(ResultType);\n  QualType CurrentType = E->getTypeSourceInfo()->getType();\n  for (unsigned i = 0; i != n; ++i) {\n    OffsetOfNode ON = E->getComponent(i);\n    llvm::Value *Offset = nullptr;\n    switch (ON.getKind()) {\n    case OffsetOfNode::Array: {\n      // Compute the index\n      Expr *IdxExpr = E->getIndexExpr(ON.getArrayExprIndex());\n      llvm::Value* Idx = CGF.EmitScalarExpr(IdxExpr);\n      bool IdxSigned = IdxExpr->getType()->isSignedIntegerOrEnumerationType();\n      Idx = Builder.CreateIntCast(Idx, ResultType, IdxSigned, \"conv\");\n\n      // Save the element type\n      CurrentType =\n          CGF.getContext().getAsArrayType(CurrentType)->getElementType();\n\n      // Compute the element size\n      llvm::Value* ElemSize = llvm::ConstantInt::get(ResultType,\n          CGF.getContext().getTypeSizeInChars(CurrentType).getQuantity());\n\n      // Multiply out to compute the result\n      Offset = Builder.CreateMul(Idx, ElemSize);\n      break;\n    }\n\n    case OffsetOfNode::Field: {\n      FieldDecl *MemberDecl = ON.getField();\n      RecordDecl *RD = CurrentType->castAs<RecordType>()->getDecl();\n      const ASTRecordLayout &RL = CGF.getContext().getASTRecordLayout(RD);\n\n      // Compute the index of the field in its parent.\n      unsigned i = 0;\n      // FIXME: It would be nice if we didn't have to loop here!\n      for (RecordDecl::field_iterator Field = RD->field_begin(),\n                                      FieldEnd = RD->field_end();\n           Field != FieldEnd; ++Field, ++i) {\n        if (*Field == MemberDecl)\n          break;\n      }\n      assert(i < RL.getFieldCount() && \"offsetof field in wrong type\");\n\n      // Compute the offset to the field\n      int64_t OffsetInt = RL.getFieldOffset(i) /\n                          CGF.getContext().getCharWidth();\n      Offset = llvm::ConstantInt::get(ResultType, OffsetInt);\n\n      // Save the element type.\n      CurrentType = MemberDecl->getType();\n      break;\n    }\n\n    case OffsetOfNode::Identifier:\n      llvm_unreachable(\"dependent __builtin_offsetof\");\n\n    case OffsetOfNode::Base: {\n      if (ON.getBase()->isVirtual()) {\n        CGF.ErrorUnsupported(E, \"virtual base in offsetof\");\n        continue;\n      }\n\n      RecordDecl *RD = CurrentType->castAs<RecordType>()->getDecl();\n      const ASTRecordLayout &RL = CGF.getContext().getASTRecordLayout(RD);\n\n      // Save the element type.\n      CurrentType = ON.getBase()->getType();\n\n      // Compute the offset to the base.\n      const RecordType *BaseRT = CurrentType->getAs<RecordType>();\n      CXXRecordDecl *BaseRD = cast<CXXRecordDecl>(BaseRT->getDecl());\n      CharUnits OffsetInt = RL.getBaseClassOffset(BaseRD);\n      Offset = llvm::ConstantInt::get(ResultType, OffsetInt.getQuantity());\n      break;\n    }\n    }\n    Result = Builder.CreateAdd(Result, Offset);\n  }\n  return Result;\n}\n\n/// VisitUnaryExprOrTypeTraitExpr - Return the size or alignment of the type of\n/// argument of the sizeof expression as an integer.\nValue *\nScalarExprEmitter::VisitUnaryExprOrTypeTraitExpr(\n                              const UnaryExprOrTypeTraitExpr *E) {\n  QualType TypeToSize = E->getTypeOfArgument();\n  if (E->getKind() == UETT_SizeOf) {\n    if (const VariableArrayType *VAT =\n          CGF.getContext().getAsVariableArrayType(TypeToSize)) {\n      if (E->isArgumentType()) {\n        // sizeof(type) - make sure to emit the VLA size.\n        CGF.EmitVariablyModifiedType(TypeToSize);\n      } else {\n        // C99 6.5.3.4p2: If the argument is an expression of type\n        // VLA, it is evaluated.\n        CGF.EmitIgnoredExpr(E->getArgumentExpr());\n      }\n\n      auto VlaSize = CGF.getVLASize(VAT);\n      llvm::Value *size = VlaSize.NumElts;\n\n      // Scale the number of non-VLA elements by the non-VLA element size.\n      CharUnits eltSize = CGF.getContext().getTypeSizeInChars(VlaSize.Type);\n      if (!eltSize.isOne())\n        size = CGF.Builder.CreateNUWMul(CGF.CGM.getSize(eltSize), size);\n\n      return size;\n    }\n  } else if (E->getKind() == UETT_OpenMPRequiredSimdAlign) {\n    auto Alignment =\n        CGF.getContext()\n            .toCharUnitsFromBits(CGF.getContext().getOpenMPDefaultSimdAlign(\n                E->getTypeOfArgument()->getPointeeType()))\n            .getQuantity();\n    return llvm::ConstantInt::get(CGF.SizeTy, Alignment);\n  }\n\n  // If this isn't sizeof(vla), the result must be constant; use the constant\n  // folding logic so we don't have to duplicate it here.\n  return Builder.getInt(E->EvaluateKnownConstInt(CGF.getContext()));\n}\n\nValue *ScalarExprEmitter::VisitUnaryReal(const UnaryOperator *E) {\n  Expr *Op = E->getSubExpr();\n  if (Op->getType()->isAnyComplexType()) {\n    // If it's an l-value, load through the appropriate subobject l-value.\n    // Note that we have to ask E because Op might be an l-value that\n    // this won't work for, e.g. an Obj-C property.\n    if (E->isGLValue())\n      return CGF.EmitLoadOfLValue(CGF.EmitLValue(E),\n                                  E->getExprLoc()).getScalarVal();\n\n    // Otherwise, calculate and project.\n    return CGF.EmitComplexExpr(Op, false, true).first;\n  }\n\n  return Visit(Op);\n}\n\nValue *ScalarExprEmitter::VisitUnaryImag(const UnaryOperator *E) {\n  Expr *Op = E->getSubExpr();\n  if (Op->getType()->isAnyComplexType()) {\n    // If it's an l-value, load through the appropriate subobject l-value.\n    // Note that we have to ask E because Op might be an l-value that\n    // this won't work for, e.g. an Obj-C property.\n    if (Op->isGLValue())\n      return CGF.EmitLoadOfLValue(CGF.EmitLValue(E),\n                                  E->getExprLoc()).getScalarVal();\n\n    // Otherwise, calculate and project.\n    return CGF.EmitComplexExpr(Op, true, false).second;\n  }\n\n  // __imag on a scalar returns zero.  Emit the subexpr to ensure side\n  // effects are evaluated, but not the actual value.\n  if (Op->isGLValue())\n    CGF.EmitLValue(Op);\n  else\n    CGF.EmitScalarExpr(Op, true);\n  return llvm::Constant::getNullValue(ConvertType(E->getType()));\n}\n\n//===----------------------------------------------------------------------===//\n//                           Binary Operators\n//===----------------------------------------------------------------------===//\n\nBinOpInfo ScalarExprEmitter::EmitBinOps(const BinaryOperator *E) {\n  TestAndClearIgnoreResultAssign();\n  BinOpInfo Result;\n  Result.LHS = Visit(E->getLHS());\n  Result.RHS = Visit(E->getRHS());\n  Result.Ty  = E->getType();\n  Result.Opcode = E->getOpcode();\n  Result.FPFeatures = E->getFPFeaturesInEffect(CGF.getLangOpts());\n  Result.E = E;\n  return Result;\n}\n\nLValue ScalarExprEmitter::EmitCompoundAssignLValue(\n                                              const CompoundAssignOperator *E,\n                        Value *(ScalarExprEmitter::*Func)(const BinOpInfo &),\n                                                   Value *&Result) {\n  QualType LHSTy = E->getLHS()->getType();\n  BinOpInfo OpInfo;\n\n  if (E->getComputationResultType()->isAnyComplexType())\n    return CGF.EmitScalarCompoundAssignWithComplex(E, Result);\n\n  // Emit the RHS first.  __block variables need to have the rhs evaluated\n  // first, plus this should improve codegen a little.\n  OpInfo.RHS = Visit(E->getRHS());\n  OpInfo.Ty = E->getComputationResultType();\n  OpInfo.Opcode = E->getOpcode();\n  OpInfo.FPFeatures = E->getFPFeaturesInEffect(CGF.getLangOpts());\n  OpInfo.E = E;\n  // Load/convert the LHS.\n  LValue LHSLV = EmitCheckedLValue(E->getLHS(), CodeGenFunction::TCK_Store);\n\n  llvm::PHINode *atomicPHI = nullptr;\n  if (const AtomicType *atomicTy = LHSTy->getAs<AtomicType>()) {\n    QualType type = atomicTy->getValueType();\n    if (!type->isBooleanType() && type->isIntegerType() &&\n        !(type->isUnsignedIntegerType() &&\n          CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow)) &&\n        CGF.getLangOpts().getSignedOverflowBehavior() !=\n            LangOptions::SOB_Trapping) {\n      llvm::AtomicRMWInst::BinOp AtomicOp = llvm::AtomicRMWInst::BAD_BINOP;\n      llvm::Instruction::BinaryOps Op;\n      switch (OpInfo.Opcode) {\n        // We don't have atomicrmw operands for *, %, /, <<, >>\n        case BO_MulAssign: case BO_DivAssign:\n        case BO_RemAssign:\n        case BO_ShlAssign:\n        case BO_ShrAssign:\n          break;\n        case BO_AddAssign:\n          AtomicOp = llvm::AtomicRMWInst::Add;\n          Op = llvm::Instruction::Add;\n          break;\n        case BO_SubAssign:\n          AtomicOp = llvm::AtomicRMWInst::Sub;\n          Op = llvm::Instruction::Sub;\n          break;\n        case BO_AndAssign:\n          AtomicOp = llvm::AtomicRMWInst::And;\n          Op = llvm::Instruction::And;\n          break;\n        case BO_XorAssign:\n          AtomicOp = llvm::AtomicRMWInst::Xor;\n          Op = llvm::Instruction::Xor;\n          break;\n        case BO_OrAssign:\n          AtomicOp = llvm::AtomicRMWInst::Or;\n          Op = llvm::Instruction::Or;\n          break;\n        default:\n          llvm_unreachable(\"Invalid compound assignment type\");\n      }\n      if (AtomicOp != llvm::AtomicRMWInst::BAD_BINOP) {\n        llvm::Value *Amt = CGF.EmitToMemory(\n            EmitScalarConversion(OpInfo.RHS, E->getRHS()->getType(), LHSTy,\n                                 E->getExprLoc()),\n            LHSTy);\n        Value *OldVal = Builder.CreateAtomicRMW(\n            AtomicOp, LHSLV.getPointer(CGF), Amt,\n            llvm::AtomicOrdering::SequentiallyConsistent);\n\n        // Since operation is atomic, the result type is guaranteed to be the\n        // same as the input in LLVM terms.\n        Result = Builder.CreateBinOp(Op, OldVal, Amt);\n        return LHSLV;\n      }\n    }\n    // FIXME: For floating point types, we should be saving and restoring the\n    // floating point environment in the loop.\n    llvm::BasicBlock *startBB = Builder.GetInsertBlock();\n    llvm::BasicBlock *opBB = CGF.createBasicBlock(\"atomic_op\", CGF.CurFn);\n    OpInfo.LHS = EmitLoadOfLValue(LHSLV, E->getExprLoc());\n    OpInfo.LHS = CGF.EmitToMemory(OpInfo.LHS, type);\n    Builder.CreateBr(opBB);\n    Builder.SetInsertPoint(opBB);\n    atomicPHI = Builder.CreatePHI(OpInfo.LHS->getType(), 2);\n    atomicPHI->addIncoming(OpInfo.LHS, startBB);\n    OpInfo.LHS = atomicPHI;\n  }\n  else\n    OpInfo.LHS = EmitLoadOfLValue(LHSLV, E->getExprLoc());\n\n  CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, OpInfo.FPFeatures);\n  SourceLocation Loc = E->getExprLoc();\n  OpInfo.LHS =\n      EmitScalarConversion(OpInfo.LHS, LHSTy, E->getComputationLHSType(), Loc);\n\n  // Expand the binary operator.\n  Result = (this->*Func)(OpInfo);\n\n  // Convert the result back to the LHS type,\n  // potentially with Implicit Conversion sanitizer check.\n  Result = EmitScalarConversion(Result, E->getComputationResultType(), LHSTy,\n                                Loc, ScalarConversionOpts(CGF.SanOpts));\n\n  if (atomicPHI) {\n    llvm::BasicBlock *curBlock = Builder.GetInsertBlock();\n    llvm::BasicBlock *contBB = CGF.createBasicBlock(\"atomic_cont\", CGF.CurFn);\n    auto Pair = CGF.EmitAtomicCompareExchange(\n        LHSLV, RValue::get(atomicPHI), RValue::get(Result), E->getExprLoc());\n    llvm::Value *old = CGF.EmitToMemory(Pair.first.getScalarVal(), LHSTy);\n    llvm::Value *success = Pair.second;\n    atomicPHI->addIncoming(old, curBlock);\n    Builder.CreateCondBr(success, contBB, atomicPHI->getParent());\n    Builder.SetInsertPoint(contBB);\n    return LHSLV;\n  }\n\n  // Store the result value into the LHS lvalue. Bit-fields are handled\n  // specially because the result is altered by the store, i.e., [C99 6.5.16p1]\n  // 'An assignment expression has the value of the left operand after the\n  // assignment...'.\n  if (LHSLV.isBitField())\n    CGF.EmitStoreThroughBitfieldLValue(RValue::get(Result), LHSLV, &Result);\n  else\n    CGF.EmitStoreThroughLValue(RValue::get(Result), LHSLV);\n\n  if (CGF.getLangOpts().OpenMP)\n    CGF.CGM.getOpenMPRuntime().checkAndEmitLastprivateConditional(CGF,\n                                                                  E->getLHS());\n  return LHSLV;\n}\n\nValue *ScalarExprEmitter::EmitCompoundAssign(const CompoundAssignOperator *E,\n                      Value *(ScalarExprEmitter::*Func)(const BinOpInfo &)) {\n  bool Ignore = TestAndClearIgnoreResultAssign();\n  Value *RHS = nullptr;\n  LValue LHS = EmitCompoundAssignLValue(E, Func, RHS);\n\n  // If the result is clearly ignored, return now.\n  if (Ignore)\n    return nullptr;\n\n  // The result of an assignment in C is the assigned r-value.\n  if (!CGF.getLangOpts().CPlusPlus)\n    return RHS;\n\n  // If the lvalue is non-volatile, return the computed value of the assignment.\n  if (!LHS.isVolatileQualified())\n    return RHS;\n\n  // Otherwise, reload the value.\n  return EmitLoadOfLValue(LHS, E->getExprLoc());\n}\n\nvoid ScalarExprEmitter::EmitUndefinedBehaviorIntegerDivAndRemCheck(\n    const BinOpInfo &Ops, llvm::Value *Zero, bool isDiv) {\n  SmallVector<std::pair<llvm::Value *, SanitizerMask>, 2> Checks;\n\n  if (CGF.SanOpts.has(SanitizerKind::IntegerDivideByZero)) {\n    Checks.push_back(std::make_pair(Builder.CreateICmpNE(Ops.RHS, Zero),\n                                    SanitizerKind::IntegerDivideByZero));\n  }\n\n  const auto *BO = cast<BinaryOperator>(Ops.E);\n  if (CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow) &&\n      Ops.Ty->hasSignedIntegerRepresentation() &&\n      !IsWidenedIntegerOp(CGF.getContext(), BO->getLHS()) &&\n      Ops.mayHaveIntegerOverflow()) {\n    llvm::IntegerType *Ty = cast<llvm::IntegerType>(Zero->getType());\n\n    llvm::Value *IntMin =\n      Builder.getInt(llvm::APInt::getSignedMinValue(Ty->getBitWidth()));\n    llvm::Value *NegOne = llvm::ConstantInt::get(Ty, -1ULL);\n\n    llvm::Value *LHSCmp = Builder.CreateICmpNE(Ops.LHS, IntMin);\n    llvm::Value *RHSCmp = Builder.CreateICmpNE(Ops.RHS, NegOne);\n    llvm::Value *NotOverflow = Builder.CreateOr(LHSCmp, RHSCmp, \"or\");\n    Checks.push_back(\n        std::make_pair(NotOverflow, SanitizerKind::SignedIntegerOverflow));\n  }\n\n  if (Checks.size() > 0)\n    EmitBinOpCheck(Checks, Ops);\n}\n\nValue *ScalarExprEmitter::EmitDiv(const BinOpInfo &Ops) {\n  {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    if ((CGF.SanOpts.has(SanitizerKind::IntegerDivideByZero) ||\n         CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow)) &&\n        Ops.Ty->isIntegerType() &&\n        (Ops.mayHaveIntegerDivisionByZero() || Ops.mayHaveIntegerOverflow())) {\n      llvm::Value *Zero = llvm::Constant::getNullValue(ConvertType(Ops.Ty));\n      EmitUndefinedBehaviorIntegerDivAndRemCheck(Ops, Zero, true);\n    } else if (CGF.SanOpts.has(SanitizerKind::FloatDivideByZero) &&\n               Ops.Ty->isRealFloatingType() &&\n               Ops.mayHaveFloatDivisionByZero()) {\n      llvm::Value *Zero = llvm::Constant::getNullValue(ConvertType(Ops.Ty));\n      llvm::Value *NonZero = Builder.CreateFCmpUNE(Ops.RHS, Zero);\n      EmitBinOpCheck(std::make_pair(NonZero, SanitizerKind::FloatDivideByZero),\n                     Ops);\n    }\n  }\n\n  if (Ops.LHS->getType()->isFPOrFPVectorTy()) {\n    llvm::Value *Val;\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, Ops.FPFeatures);\n    Val = Builder.CreateFDiv(Ops.LHS, Ops.RHS, \"div\");\n    if (CGF.getLangOpts().OpenCL &&\n        !CGF.CGM.getCodeGenOpts().CorrectlyRoundedDivSqrt) {\n      // OpenCL v1.1 s7.4: minimum accuracy of single precision / is 2.5ulp\n      // OpenCL v1.2 s5.6.4.2: The -cl-fp32-correctly-rounded-divide-sqrt\n      // build option allows an application to specify that single precision\n      // floating-point divide (x/y and 1/x) and sqrt used in the program\n      // source are correctly rounded.\n      llvm::Type *ValTy = Val->getType();\n      if (ValTy->isFloatTy() ||\n          (isa<llvm::VectorType>(ValTy) &&\n           cast<llvm::VectorType>(ValTy)->getElementType()->isFloatTy()))\n        CGF.SetFPAccuracy(Val, 2.5);\n    }\n    return Val;\n  }\n  else if (Ops.isFixedPointOp())\n    return EmitFixedPointBinOp(Ops);\n  else if (Ops.Ty->hasUnsignedIntegerRepresentation())\n    return Builder.CreateUDiv(Ops.LHS, Ops.RHS, \"div\");\n  else\n    return Builder.CreateSDiv(Ops.LHS, Ops.RHS, \"div\");\n}\n\nValue *ScalarExprEmitter::EmitRem(const BinOpInfo &Ops) {\n  // Rem in C can't be a floating point type: C99 6.5.5p2.\n  if ((CGF.SanOpts.has(SanitizerKind::IntegerDivideByZero) ||\n       CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow)) &&\n      Ops.Ty->isIntegerType() &&\n      (Ops.mayHaveIntegerDivisionByZero() || Ops.mayHaveIntegerOverflow())) {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    llvm::Value *Zero = llvm::Constant::getNullValue(ConvertType(Ops.Ty));\n    EmitUndefinedBehaviorIntegerDivAndRemCheck(Ops, Zero, false);\n  }\n\n  if (Ops.Ty->hasUnsignedIntegerRepresentation())\n    return Builder.CreateURem(Ops.LHS, Ops.RHS, \"rem\");\n  else\n    return Builder.CreateSRem(Ops.LHS, Ops.RHS, \"rem\");\n}\n\nValue *ScalarExprEmitter::EmitOverflowCheckedBinOp(const BinOpInfo &Ops) {\n  unsigned IID;\n  unsigned OpID = 0;\n  SanitizerHandler OverflowKind;\n\n  bool isSigned = Ops.Ty->isSignedIntegerOrEnumerationType();\n  switch (Ops.Opcode) {\n  case BO_Add:\n  case BO_AddAssign:\n    OpID = 1;\n    IID = isSigned ? llvm::Intrinsic::sadd_with_overflow :\n                     llvm::Intrinsic::uadd_with_overflow;\n    OverflowKind = SanitizerHandler::AddOverflow;\n    break;\n  case BO_Sub:\n  case BO_SubAssign:\n    OpID = 2;\n    IID = isSigned ? llvm::Intrinsic::ssub_with_overflow :\n                     llvm::Intrinsic::usub_with_overflow;\n    OverflowKind = SanitizerHandler::SubOverflow;\n    break;\n  case BO_Mul:\n  case BO_MulAssign:\n    OpID = 3;\n    IID = isSigned ? llvm::Intrinsic::smul_with_overflow :\n                     llvm::Intrinsic::umul_with_overflow;\n    OverflowKind = SanitizerHandler::MulOverflow;\n    break;\n  default:\n    llvm_unreachable(\"Unsupported operation for overflow detection\");\n  }\n  OpID <<= 1;\n  if (isSigned)\n    OpID |= 1;\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n  llvm::Type *opTy = CGF.CGM.getTypes().ConvertType(Ops.Ty);\n\n  llvm::Function *intrinsic = CGF.CGM.getIntrinsic(IID, opTy);\n\n  Value *resultAndOverflow = Builder.CreateCall(intrinsic, {Ops.LHS, Ops.RHS});\n  Value *result = Builder.CreateExtractValue(resultAndOverflow, 0);\n  Value *overflow = Builder.CreateExtractValue(resultAndOverflow, 1);\n\n  // Handle overflow with llvm.trap if no custom handler has been specified.\n  const std::string *handlerName =\n    &CGF.getLangOpts().OverflowHandler;\n  if (handlerName->empty()) {\n    // If the signed-integer-overflow sanitizer is enabled, emit a call to its\n    // runtime. Otherwise, this is a -ftrapv check, so just emit a trap.\n    if (!isSigned || CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow)) {\n      llvm::Value *NotOverflow = Builder.CreateNot(overflow);\n      SanitizerMask Kind = isSigned ? SanitizerKind::SignedIntegerOverflow\n                              : SanitizerKind::UnsignedIntegerOverflow;\n      EmitBinOpCheck(std::make_pair(NotOverflow, Kind), Ops);\n    } else\n      CGF.EmitTrapCheck(Builder.CreateNot(overflow), OverflowKind);\n    return result;\n  }\n\n  // Branch in case of overflow.\n  llvm::BasicBlock *initialBB = Builder.GetInsertBlock();\n  llvm::BasicBlock *continueBB =\n      CGF.createBasicBlock(\"nooverflow\", CGF.CurFn, initialBB->getNextNode());\n  llvm::BasicBlock *overflowBB = CGF.createBasicBlock(\"overflow\", CGF.CurFn);\n\n  Builder.CreateCondBr(overflow, overflowBB, continueBB);\n\n  // If an overflow handler is set, then we want to call it and then use its\n  // result, if it returns.\n  Builder.SetInsertPoint(overflowBB);\n\n  // Get the overflow handler.\n  llvm::Type *Int8Ty = CGF.Int8Ty;\n  llvm::Type *argTypes[] = { CGF.Int64Ty, CGF.Int64Ty, Int8Ty, Int8Ty };\n  llvm::FunctionType *handlerTy =\n      llvm::FunctionType::get(CGF.Int64Ty, argTypes, true);\n  llvm::FunctionCallee handler =\n      CGF.CGM.CreateRuntimeFunction(handlerTy, *handlerName);\n\n  // Sign extend the args to 64-bit, so that we can use the same handler for\n  // all types of overflow.\n  llvm::Value *lhs = Builder.CreateSExt(Ops.LHS, CGF.Int64Ty);\n  llvm::Value *rhs = Builder.CreateSExt(Ops.RHS, CGF.Int64Ty);\n\n  // Call the handler with the two arguments, the operation, and the size of\n  // the result.\n  llvm::Value *handlerArgs[] = {\n    lhs,\n    rhs,\n    Builder.getInt8(OpID),\n    Builder.getInt8(cast<llvm::IntegerType>(opTy)->getBitWidth())\n  };\n  llvm::Value *handlerResult =\n    CGF.EmitNounwindRuntimeCall(handler, handlerArgs);\n\n  // Truncate the result back to the desired size.\n  handlerResult = Builder.CreateTrunc(handlerResult, opTy);\n  Builder.CreateBr(continueBB);\n\n  Builder.SetInsertPoint(continueBB);\n  llvm::PHINode *phi = Builder.CreatePHI(opTy, 2);\n  phi->addIncoming(result, initialBB);\n  phi->addIncoming(handlerResult, overflowBB);\n\n  return phi;\n}\n\n/// Emit pointer + index arithmetic.\nstatic Value *emitPointerArithmetic(CodeGenFunction &CGF,\n                                    const BinOpInfo &op,\n                                    bool isSubtraction) {\n  // Must have binary (not unary) expr here.  Unary pointer\n  // increment/decrement doesn't use this path.\n  const BinaryOperator *expr = cast<BinaryOperator>(op.E);\n\n  Value *pointer = op.LHS;\n  Expr *pointerOperand = expr->getLHS();\n  Value *index = op.RHS;\n  Expr *indexOperand = expr->getRHS();\n\n  // In a subtraction, the LHS is always the pointer.\n  if (!isSubtraction && !pointer->getType()->isPointerTy()) {\n    std::swap(pointer, index);\n    std::swap(pointerOperand, indexOperand);\n  }\n\n  bool isSigned = indexOperand->getType()->isSignedIntegerOrEnumerationType();\n\n  unsigned width = cast<llvm::IntegerType>(index->getType())->getBitWidth();\n  auto &DL = CGF.CGM.getDataLayout();\n  auto PtrTy = cast<llvm::PointerType>(pointer->getType());\n\n  // Some versions of glibc and gcc use idioms (particularly in their malloc\n  // routines) that add a pointer-sized integer (known to be a pointer value)\n  // to a null pointer in order to cast the value back to an integer or as\n  // part of a pointer alignment algorithm.  This is undefined behavior, but\n  // we'd like to be able to compile programs that use it.\n  //\n  // Normally, we'd generate a GEP with a null-pointer base here in response\n  // to that code, but it's also UB to dereference a pointer created that\n  // way.  Instead (as an acknowledged hack to tolerate the idiom) we will\n  // generate a direct cast of the integer value to a pointer.\n  //\n  // The idiom (p = nullptr + N) is not met if any of the following are true:\n  //\n  //   The operation is subtraction.\n  //   The index is not pointer-sized.\n  //   The pointer type is not byte-sized.\n  //\n  if (BinaryOperator::isNullPointerArithmeticExtension(CGF.getContext(),\n                                                       op.Opcode,\n                                                       expr->getLHS(),\n                                                       expr->getRHS()))\n    return CGF.Builder.CreateIntToPtr(index, pointer->getType());\n\n  if (width != DL.getIndexTypeSizeInBits(PtrTy)) {\n    // Zero-extend or sign-extend the pointer value according to\n    // whether the index is signed or not.\n    index = CGF.Builder.CreateIntCast(index, DL.getIndexType(PtrTy), isSigned,\n                                      \"idx.ext\");\n  }\n\n  // If this is subtraction, negate the index.\n  if (isSubtraction)\n    index = CGF.Builder.CreateNeg(index, \"idx.neg\");\n\n  if (CGF.SanOpts.has(SanitizerKind::ArrayBounds))\n    CGF.EmitBoundsCheck(op.E, pointerOperand, index, indexOperand->getType(),\n                        /*Accessed*/ false);\n\n  const PointerType *pointerType\n    = pointerOperand->getType()->getAs<PointerType>();\n  if (!pointerType) {\n    QualType objectType = pointerOperand->getType()\n                                        ->castAs<ObjCObjectPointerType>()\n                                        ->getPointeeType();\n    llvm::Value *objectSize\n      = CGF.CGM.getSize(CGF.getContext().getTypeSizeInChars(objectType));\n\n    index = CGF.Builder.CreateMul(index, objectSize);\n\n    Value *result = CGF.Builder.CreateBitCast(pointer, CGF.VoidPtrTy);\n    result = CGF.Builder.CreateGEP(result, index, \"add.ptr\");\n    return CGF.Builder.CreateBitCast(result, pointer->getType());\n  }\n\n  QualType elementType = pointerType->getPointeeType();\n  if (const VariableArrayType *vla\n        = CGF.getContext().getAsVariableArrayType(elementType)) {\n    // The element count here is the total number of non-VLA elements.\n    llvm::Value *numElements = CGF.getVLASize(vla).NumElts;\n\n    // Effectively, the multiply by the VLA size is part of the GEP.\n    // GEP indexes are signed, and scaling an index isn't permitted to\n    // signed-overflow, so we use the same semantics for our explicit\n    // multiply.  We suppress this if overflow is not undefined behavior.\n    if (CGF.getLangOpts().isSignedOverflowDefined()) {\n      index = CGF.Builder.CreateMul(index, numElements, \"vla.index\");\n      pointer = CGF.Builder.CreateGEP(pointer, index, \"add.ptr\");\n    } else {\n      index = CGF.Builder.CreateNSWMul(index, numElements, \"vla.index\");\n      pointer =\n          CGF.EmitCheckedInBoundsGEP(pointer, index, isSigned, isSubtraction,\n                                     op.E->getExprLoc(), \"add.ptr\");\n    }\n    return pointer;\n  }\n\n  // Explicitly handle GNU void* and function pointer arithmetic extensions. The\n  // GNU void* casts amount to no-ops since our void* type is i8*, but this is\n  // future proof.\n  if (elementType->isVoidType() || elementType->isFunctionType()) {\n    Value *result = CGF.EmitCastToVoidPtr(pointer);\n    result = CGF.Builder.CreateGEP(result, index, \"add.ptr\");\n    return CGF.Builder.CreateBitCast(result, pointer->getType());\n  }\n\n  if (CGF.getLangOpts().isSignedOverflowDefined())\n    return CGF.Builder.CreateGEP(pointer, index, \"add.ptr\");\n\n  return CGF.EmitCheckedInBoundsGEP(pointer, index, isSigned, isSubtraction,\n                                    op.E->getExprLoc(), \"add.ptr\");\n}\n\n// Construct an fmuladd intrinsic to represent a fused mul-add of MulOp and\n// Addend. Use negMul and negAdd to negate the first operand of the Mul or\n// the add operand respectively. This allows fmuladd to represent a*b-c, or\n// c-a*b. Patterns in LLVM should catch the negated forms and translate them to\n// efficient operations.\nstatic Value* buildFMulAdd(llvm::Instruction *MulOp, Value *Addend,\n                           const CodeGenFunction &CGF, CGBuilderTy &Builder,\n                           bool negMul, bool negAdd) {\n  assert(!(negMul && negAdd) && \"Only one of negMul and negAdd should be set.\");\n\n  Value *MulOp0 = MulOp->getOperand(0);\n  Value *MulOp1 = MulOp->getOperand(1);\n  if (negMul)\n    MulOp0 = Builder.CreateFNeg(MulOp0, \"neg\");\n  if (negAdd)\n    Addend = Builder.CreateFNeg(Addend, \"neg\");\n\n  Value *FMulAdd = nullptr;\n  if (Builder.getIsFPConstrained()) {\n    assert(isa<llvm::ConstrainedFPIntrinsic>(MulOp) &&\n           \"Only constrained operation should be created when Builder is in FP \"\n           \"constrained mode\");\n    FMulAdd = Builder.CreateConstrainedFPCall(\n        CGF.CGM.getIntrinsic(llvm::Intrinsic::experimental_constrained_fmuladd,\n                             Addend->getType()),\n        {MulOp0, MulOp1, Addend});\n  } else {\n    FMulAdd = Builder.CreateCall(\n        CGF.CGM.getIntrinsic(llvm::Intrinsic::fmuladd, Addend->getType()),\n        {MulOp0, MulOp1, Addend});\n  }\n  MulOp->eraseFromParent();\n\n  return FMulAdd;\n}\n\n// Check whether it would be legal to emit an fmuladd intrinsic call to\n// represent op and if so, build the fmuladd.\n//\n// Checks that (a) the operation is fusable, and (b) -ffp-contract=on.\n// Does NOT check the type of the operation - it's assumed that this function\n// will be called from contexts where it's known that the type is contractable.\nstatic Value* tryEmitFMulAdd(const BinOpInfo &op,\n                         const CodeGenFunction &CGF, CGBuilderTy &Builder,\n                         bool isSub=false) {\n\n  assert((op.Opcode == BO_Add || op.Opcode == BO_AddAssign ||\n          op.Opcode == BO_Sub || op.Opcode == BO_SubAssign) &&\n         \"Only fadd/fsub can be the root of an fmuladd.\");\n\n  // Check whether this op is marked as fusable.\n  if (!op.FPFeatures.allowFPContractWithinStatement())\n    return nullptr;\n\n  // We have a potentially fusable op. Look for a mul on one of the operands.\n  // Also, make sure that the mul result isn't used directly. In that case,\n  // there's no point creating a muladd operation.\n  if (auto *LHSBinOp = dyn_cast<llvm::BinaryOperator>(op.LHS)) {\n    if (LHSBinOp->getOpcode() == llvm::Instruction::FMul &&\n        LHSBinOp->use_empty())\n      return buildFMulAdd(LHSBinOp, op.RHS, CGF, Builder, false, isSub);\n  }\n  if (auto *RHSBinOp = dyn_cast<llvm::BinaryOperator>(op.RHS)) {\n    if (RHSBinOp->getOpcode() == llvm::Instruction::FMul &&\n        RHSBinOp->use_empty())\n      return buildFMulAdd(RHSBinOp, op.LHS, CGF, Builder, isSub, false);\n  }\n\n  if (auto *LHSBinOp = dyn_cast<llvm::CallBase>(op.LHS)) {\n    if (LHSBinOp->getIntrinsicID() ==\n            llvm::Intrinsic::experimental_constrained_fmul &&\n        LHSBinOp->use_empty())\n      return buildFMulAdd(LHSBinOp, op.RHS, CGF, Builder, false, isSub);\n  }\n  if (auto *RHSBinOp = dyn_cast<llvm::CallBase>(op.RHS)) {\n    if (RHSBinOp->getIntrinsicID() ==\n            llvm::Intrinsic::experimental_constrained_fmul &&\n        RHSBinOp->use_empty())\n      return buildFMulAdd(RHSBinOp, op.LHS, CGF, Builder, isSub, false);\n  }\n\n  return nullptr;\n}\n\nValue *ScalarExprEmitter::EmitAdd(const BinOpInfo &op) {\n  if (op.LHS->getType()->isPointerTy() ||\n      op.RHS->getType()->isPointerTy())\n    return emitPointerArithmetic(CGF, op, CodeGenFunction::NotSubtraction);\n\n  if (op.Ty->isSignedIntegerOrEnumerationType()) {\n    switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n    case LangOptions::SOB_Defined:\n      return Builder.CreateAdd(op.LHS, op.RHS, \"add\");\n    case LangOptions::SOB_Undefined:\n      if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n        return Builder.CreateNSWAdd(op.LHS, op.RHS, \"add\");\n      LLVM_FALLTHROUGH;\n    case LangOptions::SOB_Trapping:\n      if (CanElideOverflowCheck(CGF.getContext(), op))\n        return Builder.CreateNSWAdd(op.LHS, op.RHS, \"add\");\n      return EmitOverflowCheckedBinOp(op);\n    }\n  }\n\n  if (op.Ty->isConstantMatrixType()) {\n    llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n    return MB.CreateAdd(op.LHS, op.RHS);\n  }\n\n  if (op.Ty->isUnsignedIntegerType() &&\n      CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow) &&\n      !CanElideOverflowCheck(CGF.getContext(), op))\n    return EmitOverflowCheckedBinOp(op);\n\n  if (op.LHS->getType()->isFPOrFPVectorTy()) {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, op.FPFeatures);\n    // Try to form an fmuladd.\n    if (Value *FMulAdd = tryEmitFMulAdd(op, CGF, Builder))\n      return FMulAdd;\n\n    return Builder.CreateFAdd(op.LHS, op.RHS, \"add\");\n  }\n\n  if (op.isFixedPointOp())\n    return EmitFixedPointBinOp(op);\n\n  return Builder.CreateAdd(op.LHS, op.RHS, \"add\");\n}\n\n/// The resulting value must be calculated with exact precision, so the operands\n/// may not be the same type.\nValue *ScalarExprEmitter::EmitFixedPointBinOp(const BinOpInfo &op) {\n  using llvm::APSInt;\n  using llvm::ConstantInt;\n\n  // This is either a binary operation where at least one of the operands is\n  // a fixed-point type, or a unary operation where the operand is a fixed-point\n  // type. The result type of a binary operation is determined by\n  // Sema::handleFixedPointConversions().\n  QualType ResultTy = op.Ty;\n  QualType LHSTy, RHSTy;\n  if (const auto *BinOp = dyn_cast<BinaryOperator>(op.E)) {\n    RHSTy = BinOp->getRHS()->getType();\n    if (const auto *CAO = dyn_cast<CompoundAssignOperator>(BinOp)) {\n      // For compound assignment, the effective type of the LHS at this point\n      // is the computation LHS type, not the actual LHS type, and the final\n      // result type is not the type of the expression but rather the\n      // computation result type.\n      LHSTy = CAO->getComputationLHSType();\n      ResultTy = CAO->getComputationResultType();\n    } else\n      LHSTy = BinOp->getLHS()->getType();\n  } else if (const auto *UnOp = dyn_cast<UnaryOperator>(op.E)) {\n    LHSTy = UnOp->getSubExpr()->getType();\n    RHSTy = UnOp->getSubExpr()->getType();\n  }\n  ASTContext &Ctx = CGF.getContext();\n  Value *LHS = op.LHS;\n  Value *RHS = op.RHS;\n\n  auto LHSFixedSema = Ctx.getFixedPointSemantics(LHSTy);\n  auto RHSFixedSema = Ctx.getFixedPointSemantics(RHSTy);\n  auto ResultFixedSema = Ctx.getFixedPointSemantics(ResultTy);\n  auto CommonFixedSema = LHSFixedSema.getCommonSemantics(RHSFixedSema);\n\n  // Perform the actual operation.\n  Value *Result;\n  llvm::FixedPointBuilder<CGBuilderTy> FPBuilder(Builder);\n  switch (op.Opcode) {\n  case BO_AddAssign:\n  case BO_Add:\n    Result = FPBuilder.CreateAdd(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_SubAssign:\n  case BO_Sub:\n    Result = FPBuilder.CreateSub(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_MulAssign:\n  case BO_Mul:\n    Result = FPBuilder.CreateMul(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_DivAssign:\n  case BO_Div:\n    Result = FPBuilder.CreateDiv(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_ShlAssign:\n  case BO_Shl:\n    Result = FPBuilder.CreateShl(LHS, LHSFixedSema, RHS);\n    break;\n  case BO_ShrAssign:\n  case BO_Shr:\n    Result = FPBuilder.CreateShr(LHS, LHSFixedSema, RHS);\n    break;\n  case BO_LT:\n    return FPBuilder.CreateLT(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_GT:\n    return FPBuilder.CreateGT(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_LE:\n    return FPBuilder.CreateLE(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_GE:\n    return FPBuilder.CreateGE(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_EQ:\n    // For equality operations, we assume any padding bits on unsigned types are\n    // zero'd out. They could be overwritten through non-saturating operations\n    // that cause overflow, but this leads to undefined behavior.\n    return FPBuilder.CreateEQ(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_NE:\n    return FPBuilder.CreateNE(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_Cmp:\n  case BO_LAnd:\n  case BO_LOr:\n    llvm_unreachable(\"Found unimplemented fixed point binary operation\");\n  case BO_PtrMemD:\n  case BO_PtrMemI:\n  case BO_Rem:\n  case BO_Xor:\n  case BO_And:\n  case BO_Or:\n  case BO_Assign:\n  case BO_RemAssign:\n  case BO_AndAssign:\n  case BO_XorAssign:\n  case BO_OrAssign:\n  case BO_Comma:\n    llvm_unreachable(\"Found unsupported binary operation for fixed point types.\");\n  }\n\n  bool IsShift = BinaryOperator::isShiftOp(op.Opcode) ||\n                 BinaryOperator::isShiftAssignOp(op.Opcode);\n  // Convert to the result type.\n  return FPBuilder.CreateFixedToFixed(Result, IsShift ? LHSFixedSema\n                                                      : CommonFixedSema,\n                                      ResultFixedSema);\n}\n\nValue *ScalarExprEmitter::EmitSub(const BinOpInfo &op) {\n  // The LHS is always a pointer if either side is.\n  if (!op.LHS->getType()->isPointerTy()) {\n    if (op.Ty->isSignedIntegerOrEnumerationType()) {\n      switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n      case LangOptions::SOB_Defined:\n        return Builder.CreateSub(op.LHS, op.RHS, \"sub\");\n      case LangOptions::SOB_Undefined:\n        if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n          return Builder.CreateNSWSub(op.LHS, op.RHS, \"sub\");\n        LLVM_FALLTHROUGH;\n      case LangOptions::SOB_Trapping:\n        if (CanElideOverflowCheck(CGF.getContext(), op))\n          return Builder.CreateNSWSub(op.LHS, op.RHS, \"sub\");\n        return EmitOverflowCheckedBinOp(op);\n      }\n    }\n\n    if (op.Ty->isConstantMatrixType()) {\n      llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n      return MB.CreateSub(op.LHS, op.RHS);\n    }\n\n    if (op.Ty->isUnsignedIntegerType() &&\n        CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow) &&\n        !CanElideOverflowCheck(CGF.getContext(), op))\n      return EmitOverflowCheckedBinOp(op);\n\n    if (op.LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, op.FPFeatures);\n      // Try to form an fmuladd.\n      if (Value *FMulAdd = tryEmitFMulAdd(op, CGF, Builder, true))\n        return FMulAdd;\n      return Builder.CreateFSub(op.LHS, op.RHS, \"sub\");\n    }\n\n    if (op.isFixedPointOp())\n      return EmitFixedPointBinOp(op);\n\n    return Builder.CreateSub(op.LHS, op.RHS, \"sub\");\n  }\n\n  // If the RHS is not a pointer, then we have normal pointer\n  // arithmetic.\n  if (!op.RHS->getType()->isPointerTy())\n    return emitPointerArithmetic(CGF, op, CodeGenFunction::IsSubtraction);\n\n  // Otherwise, this is a pointer subtraction.\n\n  // Do the raw subtraction part.\n  llvm::Value *LHS\n    = Builder.CreatePtrToInt(op.LHS, CGF.PtrDiffTy, \"sub.ptr.lhs.cast\");\n  llvm::Value *RHS\n    = Builder.CreatePtrToInt(op.RHS, CGF.PtrDiffTy, \"sub.ptr.rhs.cast\");\n  Value *diffInChars = Builder.CreateSub(LHS, RHS, \"sub.ptr.sub\");\n\n  // Okay, figure out the element size.\n  const BinaryOperator *expr = cast<BinaryOperator>(op.E);\n  QualType elementType = expr->getLHS()->getType()->getPointeeType();\n\n  llvm::Value *divisor = nullptr;\n\n  // For a variable-length array, this is going to be non-constant.\n  if (const VariableArrayType *vla\n        = CGF.getContext().getAsVariableArrayType(elementType)) {\n    auto VlaSize = CGF.getVLASize(vla);\n    elementType = VlaSize.Type;\n    divisor = VlaSize.NumElts;\n\n    // Scale the number of non-VLA elements by the non-VLA element size.\n    CharUnits eltSize = CGF.getContext().getTypeSizeInChars(elementType);\n    if (!eltSize.isOne())\n      divisor = CGF.Builder.CreateNUWMul(CGF.CGM.getSize(eltSize), divisor);\n\n  // For everything elese, we can just compute it, safe in the\n  // assumption that Sema won't let anything through that we can't\n  // safely compute the size of.\n  } else {\n    CharUnits elementSize;\n    // Handle GCC extension for pointer arithmetic on void* and\n    // function pointer types.\n    if (elementType->isVoidType() || elementType->isFunctionType())\n      elementSize = CharUnits::One();\n    else\n      elementSize = CGF.getContext().getTypeSizeInChars(elementType);\n\n    // Don't even emit the divide for element size of 1.\n    if (elementSize.isOne())\n      return diffInChars;\n\n    divisor = CGF.CGM.getSize(elementSize);\n  }\n\n  // Otherwise, do a full sdiv. This uses the \"exact\" form of sdiv, since\n  // pointer difference in C is only defined in the case where both operands\n  // are pointing to elements of an array.\n  return Builder.CreateExactSDiv(diffInChars, divisor, \"sub.ptr.div\");\n}\n\nValue *ScalarExprEmitter::GetWidthMinusOneValue(Value* LHS,Value* RHS) {\n  llvm::IntegerType *Ty;\n  if (llvm::VectorType *VT = dyn_cast<llvm::VectorType>(LHS->getType()))\n    Ty = cast<llvm::IntegerType>(VT->getElementType());\n  else\n    Ty = cast<llvm::IntegerType>(LHS->getType());\n  return llvm::ConstantInt::get(RHS->getType(), Ty->getBitWidth() - 1);\n}\n\nValue *ScalarExprEmitter::ConstrainShiftValue(Value *LHS, Value *RHS,\n                                              const Twine &Name) {\n  llvm::IntegerType *Ty;\n  if (auto *VT = dyn_cast<llvm::VectorType>(LHS->getType()))\n    Ty = cast<llvm::IntegerType>(VT->getElementType());\n  else\n    Ty = cast<llvm::IntegerType>(LHS->getType());\n\n  if (llvm::isPowerOf2_64(Ty->getBitWidth()))\n        return Builder.CreateAnd(RHS, GetWidthMinusOneValue(LHS, RHS), Name);\n\n  return Builder.CreateURem(\n      RHS, llvm::ConstantInt::get(RHS->getType(), Ty->getBitWidth()), Name);\n}\n\nValue *ScalarExprEmitter::EmitShl(const BinOpInfo &Ops) {\n  // TODO: This misses out on the sanitizer check below.\n  if (Ops.isFixedPointOp())\n    return EmitFixedPointBinOp(Ops);\n\n  // LLVM requires the LHS and RHS to be the same type: promote or truncate the\n  // RHS to the same size as the LHS.\n  Value *RHS = Ops.RHS;\n  if (Ops.LHS->getType() != RHS->getType())\n    RHS = Builder.CreateIntCast(RHS, Ops.LHS->getType(), false, \"sh_prom\");\n\n  bool SanitizeSignedBase = CGF.SanOpts.has(SanitizerKind::ShiftBase) &&\n                            Ops.Ty->hasSignedIntegerRepresentation() &&\n                            !CGF.getLangOpts().isSignedOverflowDefined() &&\n                            !CGF.getLangOpts().CPlusPlus20;\n  bool SanitizeUnsignedBase =\n      CGF.SanOpts.has(SanitizerKind::UnsignedShiftBase) &&\n      Ops.Ty->hasUnsignedIntegerRepresentation();\n  bool SanitizeBase = SanitizeSignedBase || SanitizeUnsignedBase;\n  bool SanitizeExponent = CGF.SanOpts.has(SanitizerKind::ShiftExponent);\n  // OpenCL 6.3j: shift values are effectively % word size of LHS.\n  if (CGF.getLangOpts().OpenCL)\n    RHS = ConstrainShiftValue(Ops.LHS, RHS, \"shl.mask\");\n  else if ((SanitizeBase || SanitizeExponent) &&\n           isa<llvm::IntegerType>(Ops.LHS->getType())) {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    SmallVector<std::pair<Value *, SanitizerMask>, 2> Checks;\n    llvm::Value *WidthMinusOne = GetWidthMinusOneValue(Ops.LHS, Ops.RHS);\n    llvm::Value *ValidExponent = Builder.CreateICmpULE(Ops.RHS, WidthMinusOne);\n\n    if (SanitizeExponent) {\n      Checks.push_back(\n          std::make_pair(ValidExponent, SanitizerKind::ShiftExponent));\n    }\n\n    if (SanitizeBase) {\n      // Check whether we are shifting any non-zero bits off the top of the\n      // integer. We only emit this check if exponent is valid - otherwise\n      // instructions below will have undefined behavior themselves.\n      llvm::BasicBlock *Orig = Builder.GetInsertBlock();\n      llvm::BasicBlock *Cont = CGF.createBasicBlock(\"cont\");\n      llvm::BasicBlock *CheckShiftBase = CGF.createBasicBlock(\"check\");\n      Builder.CreateCondBr(ValidExponent, CheckShiftBase, Cont);\n      llvm::Value *PromotedWidthMinusOne =\n          (RHS == Ops.RHS) ? WidthMinusOne\n                           : GetWidthMinusOneValue(Ops.LHS, RHS);\n      CGF.EmitBlock(CheckShiftBase);\n      llvm::Value *BitsShiftedOff = Builder.CreateLShr(\n          Ops.LHS, Builder.CreateSub(PromotedWidthMinusOne, RHS, \"shl.zeros\",\n                                     /*NUW*/ true, /*NSW*/ true),\n          \"shl.check\");\n      if (SanitizeUnsignedBase || CGF.getLangOpts().CPlusPlus) {\n        // In C99, we are not permitted to shift a 1 bit into the sign bit.\n        // Under C++11's rules, shifting a 1 bit into the sign bit is\n        // OK, but shifting a 1 bit out of it is not. (C89 and C++03 don't\n        // define signed left shifts, so we use the C99 and C++11 rules there).\n        // Unsigned shifts can always shift into the top bit.\n        llvm::Value *One = llvm::ConstantInt::get(BitsShiftedOff->getType(), 1);\n        BitsShiftedOff = Builder.CreateLShr(BitsShiftedOff, One);\n      }\n      llvm::Value *Zero = llvm::ConstantInt::get(BitsShiftedOff->getType(), 0);\n      llvm::Value *ValidBase = Builder.CreateICmpEQ(BitsShiftedOff, Zero);\n      CGF.EmitBlock(Cont);\n      llvm::PHINode *BaseCheck = Builder.CreatePHI(ValidBase->getType(), 2);\n      BaseCheck->addIncoming(Builder.getTrue(), Orig);\n      BaseCheck->addIncoming(ValidBase, CheckShiftBase);\n      Checks.push_back(std::make_pair(\n          BaseCheck, SanitizeSignedBase ? SanitizerKind::ShiftBase\n                                        : SanitizerKind::UnsignedShiftBase));\n    }\n\n    assert(!Checks.empty());\n    EmitBinOpCheck(Checks, Ops);\n  }\n\n  return Builder.CreateShl(Ops.LHS, RHS, \"shl\");\n}\n\nValue *ScalarExprEmitter::EmitShr(const BinOpInfo &Ops) {\n  // TODO: This misses out on the sanitizer check below.\n  if (Ops.isFixedPointOp())\n    return EmitFixedPointBinOp(Ops);\n\n  // LLVM requires the LHS and RHS to be the same type: promote or truncate the\n  // RHS to the same size as the LHS.\n  Value *RHS = Ops.RHS;\n  if (Ops.LHS->getType() != RHS->getType())\n    RHS = Builder.CreateIntCast(RHS, Ops.LHS->getType(), false, \"sh_prom\");\n\n  // OpenCL 6.3j: shift values are effectively % word size of LHS.\n  if (CGF.getLangOpts().OpenCL)\n    RHS = ConstrainShiftValue(Ops.LHS, RHS, \"shr.mask\");\n  else if (CGF.SanOpts.has(SanitizerKind::ShiftExponent) &&\n           isa<llvm::IntegerType>(Ops.LHS->getType())) {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    llvm::Value *Valid =\n        Builder.CreateICmpULE(RHS, GetWidthMinusOneValue(Ops.LHS, RHS));\n    EmitBinOpCheck(std::make_pair(Valid, SanitizerKind::ShiftExponent), Ops);\n  }\n\n  if (Ops.Ty->hasUnsignedIntegerRepresentation())\n    return Builder.CreateLShr(Ops.LHS, RHS, \"shr\");\n  return Builder.CreateAShr(Ops.LHS, RHS, \"shr\");\n}\n\nenum IntrinsicType { VCMPEQ, VCMPGT };\n// return corresponding comparison intrinsic for given vector type\nstatic llvm::Intrinsic::ID GetIntrinsic(IntrinsicType IT,\n                                        BuiltinType::Kind ElemKind) {\n  switch (ElemKind) {\n  default: llvm_unreachable(\"unexpected element type\");\n  case BuiltinType::Char_U:\n  case BuiltinType::UChar:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequb_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtub_p;\n  case BuiltinType::Char_S:\n  case BuiltinType::SChar:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequb_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsb_p;\n  case BuiltinType::UShort:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequh_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtuh_p;\n  case BuiltinType::Short:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequh_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsh_p;\n  case BuiltinType::UInt:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequw_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtuw_p;\n  case BuiltinType::Int:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequw_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsw_p;\n  case BuiltinType::ULong:\n  case BuiltinType::ULongLong:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequd_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtud_p;\n  case BuiltinType::Long:\n  case BuiltinType::LongLong:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequd_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsd_p;\n  case BuiltinType::Float:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpeqfp_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtfp_p;\n  case BuiltinType::Double:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_vsx_xvcmpeqdp_p :\n                            llvm::Intrinsic::ppc_vsx_xvcmpgtdp_p;\n  case BuiltinType::UInt128:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequq_p\n                          : llvm::Intrinsic::ppc_altivec_vcmpgtuq_p;\n  case BuiltinType::Int128:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequq_p\n                          : llvm::Intrinsic::ppc_altivec_vcmpgtsq_p;\n  }\n}\n\nValue *ScalarExprEmitter::EmitCompare(const BinaryOperator *E,\n                                      llvm::CmpInst::Predicate UICmpOpc,\n                                      llvm::CmpInst::Predicate SICmpOpc,\n                                      llvm::CmpInst::Predicate FCmpOpc,\n                                      bool IsSignaling) {\n  TestAndClearIgnoreResultAssign();\n  Value *Result;\n  QualType LHSTy = E->getLHS()->getType();\n  QualType RHSTy = E->getRHS()->getType();\n  if (const MemberPointerType *MPT = LHSTy->getAs<MemberPointerType>()) {\n    assert(E->getOpcode() == BO_EQ ||\n           E->getOpcode() == BO_NE);\n    Value *LHS = CGF.EmitScalarExpr(E->getLHS());\n    Value *RHS = CGF.EmitScalarExpr(E->getRHS());\n    Result = CGF.CGM.getCXXABI().EmitMemberPointerComparison(\n                   CGF, LHS, RHS, MPT, E->getOpcode() == BO_NE);\n  } else if (!LHSTy->isAnyComplexType() && !RHSTy->isAnyComplexType()) {\n    BinOpInfo BOInfo = EmitBinOps(E);\n    Value *LHS = BOInfo.LHS;\n    Value *RHS = BOInfo.RHS;\n\n    // If AltiVec, the comparison results in a numeric type, so we use\n    // intrinsics comparing vectors and giving 0 or 1 as a result\n    if (LHSTy->isVectorType() && !E->getType()->isVectorType()) {\n      // constants for mapping CR6 register bits to predicate result\n      enum { CR6_EQ=0, CR6_EQ_REV, CR6_LT, CR6_LT_REV } CR6;\n\n      llvm::Intrinsic::ID ID = llvm::Intrinsic::not_intrinsic;\n\n      // in several cases vector arguments order will be reversed\n      Value *FirstVecArg = LHS,\n            *SecondVecArg = RHS;\n\n      QualType ElTy = LHSTy->castAs<VectorType>()->getElementType();\n      BuiltinType::Kind ElementKind = ElTy->castAs<BuiltinType>()->getKind();\n\n      switch(E->getOpcode()) {\n      default: llvm_unreachable(\"is not a comparison operation\");\n      case BO_EQ:\n        CR6 = CR6_LT;\n        ID = GetIntrinsic(VCMPEQ, ElementKind);\n        break;\n      case BO_NE:\n        CR6 = CR6_EQ;\n        ID = GetIntrinsic(VCMPEQ, ElementKind);\n        break;\n      case BO_LT:\n        CR6 = CR6_LT;\n        ID = GetIntrinsic(VCMPGT, ElementKind);\n        std::swap(FirstVecArg, SecondVecArg);\n        break;\n      case BO_GT:\n        CR6 = CR6_LT;\n        ID = GetIntrinsic(VCMPGT, ElementKind);\n        break;\n      case BO_LE:\n        if (ElementKind == BuiltinType::Float) {\n          CR6 = CR6_LT;\n          ID = llvm::Intrinsic::ppc_altivec_vcmpgefp_p;\n          std::swap(FirstVecArg, SecondVecArg);\n        }\n        else {\n          CR6 = CR6_EQ;\n          ID = GetIntrinsic(VCMPGT, ElementKind);\n        }\n        break;\n      case BO_GE:\n        if (ElementKind == BuiltinType::Float) {\n          CR6 = CR6_LT;\n          ID = llvm::Intrinsic::ppc_altivec_vcmpgefp_p;\n        }\n        else {\n          CR6 = CR6_EQ;\n          ID = GetIntrinsic(VCMPGT, ElementKind);\n          std::swap(FirstVecArg, SecondVecArg);\n        }\n        break;\n      }\n\n      Value *CR6Param = Builder.getInt32(CR6);\n      llvm::Function *F = CGF.CGM.getIntrinsic(ID);\n      Result = Builder.CreateCall(F, {CR6Param, FirstVecArg, SecondVecArg});\n\n      // The result type of intrinsic may not be same as E->getType().\n      // If E->getType() is not BoolTy, EmitScalarConversion will do the\n      // conversion work. If E->getType() is BoolTy, EmitScalarConversion will\n      // do nothing, if ResultTy is not i1 at the same time, it will cause\n      // crash later.\n      llvm::IntegerType *ResultTy = cast<llvm::IntegerType>(Result->getType());\n      if (ResultTy->getBitWidth() > 1 &&\n          E->getType() == CGF.getContext().BoolTy)\n        Result = Builder.CreateTrunc(Result, Builder.getInt1Ty());\n      return EmitScalarConversion(Result, CGF.getContext().BoolTy, E->getType(),\n                                  E->getExprLoc());\n    }\n\n    if (BOInfo.isFixedPointOp()) {\n      Result = EmitFixedPointBinOp(BOInfo);\n    } else if (LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, BOInfo.FPFeatures);\n      if (!IsSignaling)\n        Result = Builder.CreateFCmp(FCmpOpc, LHS, RHS, \"cmp\");\n      else\n        Result = Builder.CreateFCmpS(FCmpOpc, LHS, RHS, \"cmp\");\n    } else if (LHSTy->hasSignedIntegerRepresentation()) {\n      Result = Builder.CreateICmp(SICmpOpc, LHS, RHS, \"cmp\");\n    } else {\n      // Unsigned integers and pointers.\n\n      if (CGF.CGM.getCodeGenOpts().StrictVTablePointers &&\n          !isa<llvm::ConstantPointerNull>(LHS) &&\n          !isa<llvm::ConstantPointerNull>(RHS)) {\n\n        // Dynamic information is required to be stripped for comparisons,\n        // because it could leak the dynamic information.  Based on comparisons\n        // of pointers to dynamic objects, the optimizer can replace one pointer\n        // with another, which might be incorrect in presence of invariant\n        // groups. Comparison with null is safe because null does not carry any\n        // dynamic information.\n        if (LHSTy.mayBeDynamicClass())\n          LHS = Builder.CreateStripInvariantGroup(LHS);\n        if (RHSTy.mayBeDynamicClass())\n          RHS = Builder.CreateStripInvariantGroup(RHS);\n      }\n\n      Result = Builder.CreateICmp(UICmpOpc, LHS, RHS, \"cmp\");\n    }\n\n    // If this is a vector comparison, sign extend the result to the appropriate\n    // vector integer type and return it (don't convert to bool).\n    if (LHSTy->isVectorType())\n      return Builder.CreateSExt(Result, ConvertType(E->getType()), \"sext\");\n\n  } else {\n    // Complex Comparison: can only be an equality comparison.\n    CodeGenFunction::ComplexPairTy LHS, RHS;\n    QualType CETy;\n    if (auto *CTy = LHSTy->getAs<ComplexType>()) {\n      LHS = CGF.EmitComplexExpr(E->getLHS());\n      CETy = CTy->getElementType();\n    } else {\n      LHS.first = Visit(E->getLHS());\n      LHS.second = llvm::Constant::getNullValue(LHS.first->getType());\n      CETy = LHSTy;\n    }\n    if (auto *CTy = RHSTy->getAs<ComplexType>()) {\n      RHS = CGF.EmitComplexExpr(E->getRHS());\n      assert(CGF.getContext().hasSameUnqualifiedType(CETy,\n                                                     CTy->getElementType()) &&\n             \"The element types must always match.\");\n      (void)CTy;\n    } else {\n      RHS.first = Visit(E->getRHS());\n      RHS.second = llvm::Constant::getNullValue(RHS.first->getType());\n      assert(CGF.getContext().hasSameUnqualifiedType(CETy, RHSTy) &&\n             \"The element types must always match.\");\n    }\n\n    Value *ResultR, *ResultI;\n    if (CETy->isRealFloatingType()) {\n      // As complex comparisons can only be equality comparisons, they\n      // are never signaling comparisons.\n      ResultR = Builder.CreateFCmp(FCmpOpc, LHS.first, RHS.first, \"cmp.r\");\n      ResultI = Builder.CreateFCmp(FCmpOpc, LHS.second, RHS.second, \"cmp.i\");\n    } else {\n      // Complex comparisons can only be equality comparisons.  As such, signed\n      // and unsigned opcodes are the same.\n      ResultR = Builder.CreateICmp(UICmpOpc, LHS.first, RHS.first, \"cmp.r\");\n      ResultI = Builder.CreateICmp(UICmpOpc, LHS.second, RHS.second, \"cmp.i\");\n    }\n\n    if (E->getOpcode() == BO_EQ) {\n      Result = Builder.CreateAnd(ResultR, ResultI, \"and.ri\");\n    } else {\n      assert(E->getOpcode() == BO_NE &&\n             \"Complex comparison other than == or != ?\");\n      Result = Builder.CreateOr(ResultR, ResultI, \"or.ri\");\n    }\n  }\n\n  return EmitScalarConversion(Result, CGF.getContext().BoolTy, E->getType(),\n                              E->getExprLoc());\n}\n\nValue *ScalarExprEmitter::VisitBinAssign(const BinaryOperator *E) {\n  bool Ignore = TestAndClearIgnoreResultAssign();\n\n  Value *RHS;\n  LValue LHS;\n\n  switch (E->getLHS()->getType().getObjCLifetime()) {\n  case Qualifiers::OCL_Strong:\n    std::tie(LHS, RHS) = CGF.EmitARCStoreStrong(E, Ignore);\n    break;\n\n  case Qualifiers::OCL_Autoreleasing:\n    std::tie(LHS, RHS) = CGF.EmitARCStoreAutoreleasing(E);\n    break;\n\n  case Qualifiers::OCL_ExplicitNone:\n    std::tie(LHS, RHS) = CGF.EmitARCStoreUnsafeUnretained(E, Ignore);\n    break;\n\n  case Qualifiers::OCL_Weak:\n    RHS = Visit(E->getRHS());\n    LHS = EmitCheckedLValue(E->getLHS(), CodeGenFunction::TCK_Store);\n    RHS = CGF.EmitARCStoreWeak(LHS.getAddress(CGF), RHS, Ignore);\n    break;\n\n  case Qualifiers::OCL_None:\n    // __block variables need to have the rhs evaluated first, plus\n    // this should improve codegen just a little.\n    RHS = Visit(E->getRHS());\n    LHS = EmitCheckedLValue(E->getLHS(), CodeGenFunction::TCK_Store);\n\n    // Store the value into the LHS.  Bit-fields are handled specially\n    // because the result is altered by the store, i.e., [C99 6.5.16p1]\n    // 'An assignment expression has the value of the left operand after\n    // the assignment...'.\n    if (LHS.isBitField()) {\n      CGF.EmitStoreThroughBitfieldLValue(RValue::get(RHS), LHS, &RHS);\n    } else {\n      CGF.EmitNullabilityCheck(LHS, RHS, E->getExprLoc());\n      CGF.EmitStoreThroughLValue(RValue::get(RHS), LHS);\n    }\n  }\n\n  // If the result is clearly ignored, return now.\n  if (Ignore)\n    return nullptr;\n\n  // The result of an assignment in C is the assigned r-value.\n  if (!CGF.getLangOpts().CPlusPlus)\n    return RHS;\n\n  // If the lvalue is non-volatile, return the computed value of the assignment.\n  if (!LHS.isVolatileQualified())\n    return RHS;\n\n  // Otherwise, reload the value.\n  return EmitLoadOfLValue(LHS, E->getExprLoc());\n}\n\nValue *ScalarExprEmitter::VisitBinLAnd(const BinaryOperator *E) {\n  // Perform vector logical and on comparisons with zero vectors.\n  if (E->getType()->isVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    Value *LHS = Visit(E->getLHS());\n    Value *RHS = Visit(E->getRHS());\n    Value *Zero = llvm::ConstantAggregateZero::get(LHS->getType());\n    if (LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(\n          CGF, E->getFPFeaturesInEffect(CGF.getLangOpts()));\n      LHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, RHS, Zero, \"cmp\");\n    } else {\n      LHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, RHS, Zero, \"cmp\");\n    }\n    Value *And = Builder.CreateAnd(LHS, RHS);\n    return Builder.CreateSExt(And, ConvertType(E->getType()), \"sext\");\n  }\n\n  bool InstrumentRegions = CGF.CGM.getCodeGenOpts().hasProfileClangInstr();\n  llvm::Type *ResTy = ConvertType(E->getType());\n\n  // If we have 0 && RHS, see if we can elide RHS, if so, just return 0.\n  // If we have 1 && X, just emit X without inserting the control flow.\n  bool LHSCondVal;\n  if (CGF.ConstantFoldsToSimpleInteger(E->getLHS(), LHSCondVal)) {\n    if (LHSCondVal) { // If we have 1 && X, just emit X.\n      CGF.incrementProfileCounter(E);\n\n      Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n\n      // If we're generating for profiling or coverage, generate a branch to a\n      // block that increments the RHS counter needed to track branch condition\n      // coverage. In this case, use \"FBlock\" as both the final \"TrueBlock\" and\n      // \"FalseBlock\" after the increment is done.\n      if (InstrumentRegions &&\n          CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n        llvm::BasicBlock *FBlock = CGF.createBasicBlock(\"land.end\");\n        llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"land.rhscnt\");\n        Builder.CreateCondBr(RHSCond, RHSBlockCnt, FBlock);\n        CGF.EmitBlock(RHSBlockCnt);\n        CGF.incrementProfileCounter(E->getRHS());\n        CGF.EmitBranch(FBlock);\n        CGF.EmitBlock(FBlock);\n      }\n\n      // ZExt result to int or bool.\n      return Builder.CreateZExtOrBitCast(RHSCond, ResTy, \"land.ext\");\n    }\n\n    // 0 && RHS: If it is safe, just elide the RHS, and return 0/false.\n    if (!CGF.ContainsLabel(E->getRHS()))\n      return llvm::Constant::getNullValue(ResTy);\n  }\n\n  llvm::BasicBlock *ContBlock = CGF.createBasicBlock(\"land.end\");\n  llvm::BasicBlock *RHSBlock  = CGF.createBasicBlock(\"land.rhs\");\n\n  CodeGenFunction::ConditionalEvaluation eval(CGF);\n\n  // Branch on the LHS first.  If it is false, go to the failure (cont) block.\n  CGF.EmitBranchOnBoolExpr(E->getLHS(), RHSBlock, ContBlock,\n                           CGF.getProfileCount(E->getRHS()));\n\n  // Any edges into the ContBlock are now from an (indeterminate number of)\n  // edges from this first condition.  All of these values will be false.  Start\n  // setting up the PHI node in the Cont Block for this.\n  llvm::PHINode *PN = llvm::PHINode::Create(llvm::Type::getInt1Ty(VMContext), 2,\n                                            \"\", ContBlock);\n  for (llvm::pred_iterator PI = pred_begin(ContBlock), PE = pred_end(ContBlock);\n       PI != PE; ++PI)\n    PN->addIncoming(llvm::ConstantInt::getFalse(VMContext), *PI);\n\n  eval.begin(CGF);\n  CGF.EmitBlock(RHSBlock);\n  CGF.incrementProfileCounter(E);\n  Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n  eval.end(CGF);\n\n  // Reaquire the RHS block, as there may be subblocks inserted.\n  RHSBlock = Builder.GetInsertBlock();\n\n  // If we're generating for profiling or coverage, generate a branch on the\n  // RHS to a block that increments the RHS true counter needed to track branch\n  // condition coverage.\n  if (InstrumentRegions &&\n      CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n    llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"land.rhscnt\");\n    Builder.CreateCondBr(RHSCond, RHSBlockCnt, ContBlock);\n    CGF.EmitBlock(RHSBlockCnt);\n    CGF.incrementProfileCounter(E->getRHS());\n    CGF.EmitBranch(ContBlock);\n    PN->addIncoming(RHSCond, RHSBlockCnt);\n  }\n\n  // Emit an unconditional branch from this block to ContBlock.\n  {\n    // There is no need to emit line number for unconditional branch.\n    auto NL = ApplyDebugLocation::CreateEmpty(CGF);\n    CGF.EmitBlock(ContBlock);\n  }\n  // Insert an entry into the phi node for the edge with the value of RHSCond.\n  PN->addIncoming(RHSCond, RHSBlock);\n\n  // Artificial location to preserve the scope information\n  {\n    auto NL = ApplyDebugLocation::CreateArtificial(CGF);\n    PN->setDebugLoc(Builder.getCurrentDebugLocation());\n  }\n\n  // ZExt result to int.\n  return Builder.CreateZExtOrBitCast(PN, ResTy, \"land.ext\");\n}\n\nValue *ScalarExprEmitter::VisitBinLOr(const BinaryOperator *E) {\n  // Perform vector logical or on comparisons with zero vectors.\n  if (E->getType()->isVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    Value *LHS = Visit(E->getLHS());\n    Value *RHS = Visit(E->getRHS());\n    Value *Zero = llvm::ConstantAggregateZero::get(LHS->getType());\n    if (LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(\n          CGF, E->getFPFeaturesInEffect(CGF.getLangOpts()));\n      LHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, RHS, Zero, \"cmp\");\n    } else {\n      LHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, RHS, Zero, \"cmp\");\n    }\n    Value *Or = Builder.CreateOr(LHS, RHS);\n    return Builder.CreateSExt(Or, ConvertType(E->getType()), \"sext\");\n  }\n\n  bool InstrumentRegions = CGF.CGM.getCodeGenOpts().hasProfileClangInstr();\n  llvm::Type *ResTy = ConvertType(E->getType());\n\n  // If we have 1 || RHS, see if we can elide RHS, if so, just return 1.\n  // If we have 0 || X, just emit X without inserting the control flow.\n  bool LHSCondVal;\n  if (CGF.ConstantFoldsToSimpleInteger(E->getLHS(), LHSCondVal)) {\n    if (!LHSCondVal) { // If we have 0 || X, just emit X.\n      CGF.incrementProfileCounter(E);\n\n      Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n\n      // If we're generating for profiling or coverage, generate a branch to a\n      // block that increments the RHS counter need to track branch condition\n      // coverage. In this case, use \"FBlock\" as both the final \"TrueBlock\" and\n      // \"FalseBlock\" after the increment is done.\n      if (InstrumentRegions &&\n          CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n        llvm::BasicBlock *FBlock = CGF.createBasicBlock(\"lor.end\");\n        llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"lor.rhscnt\");\n        Builder.CreateCondBr(RHSCond, FBlock, RHSBlockCnt);\n        CGF.EmitBlock(RHSBlockCnt);\n        CGF.incrementProfileCounter(E->getRHS());\n        CGF.EmitBranch(FBlock);\n        CGF.EmitBlock(FBlock);\n      }\n\n      // ZExt result to int or bool.\n      return Builder.CreateZExtOrBitCast(RHSCond, ResTy, \"lor.ext\");\n    }\n\n    // 1 || RHS: If it is safe, just elide the RHS, and return 1/true.\n    if (!CGF.ContainsLabel(E->getRHS()))\n      return llvm::ConstantInt::get(ResTy, 1);\n  }\n\n  llvm::BasicBlock *ContBlock = CGF.createBasicBlock(\"lor.end\");\n  llvm::BasicBlock *RHSBlock = CGF.createBasicBlock(\"lor.rhs\");\n\n  CodeGenFunction::ConditionalEvaluation eval(CGF);\n\n  // Branch on the LHS first.  If it is true, go to the success (cont) block.\n  CGF.EmitBranchOnBoolExpr(E->getLHS(), ContBlock, RHSBlock,\n                           CGF.getCurrentProfileCount() -\n                               CGF.getProfileCount(E->getRHS()));\n\n  // Any edges into the ContBlock are now from an (indeterminate number of)\n  // edges from this first condition.  All of these values will be true.  Start\n  // setting up the PHI node in the Cont Block for this.\n  llvm::PHINode *PN = llvm::PHINode::Create(llvm::Type::getInt1Ty(VMContext), 2,\n                                            \"\", ContBlock);\n  for (llvm::pred_iterator PI = pred_begin(ContBlock), PE = pred_end(ContBlock);\n       PI != PE; ++PI)\n    PN->addIncoming(llvm::ConstantInt::getTrue(VMContext), *PI);\n\n  eval.begin(CGF);\n\n  // Emit the RHS condition as a bool value.\n  CGF.EmitBlock(RHSBlock);\n  CGF.incrementProfileCounter(E);\n  Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n\n  eval.end(CGF);\n\n  // Reaquire the RHS block, as there may be subblocks inserted.\n  RHSBlock = Builder.GetInsertBlock();\n\n  // If we're generating for profiling or coverage, generate a branch on the\n  // RHS to a block that increments the RHS true counter needed to track branch\n  // condition coverage.\n  if (InstrumentRegions &&\n      CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n    llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"lor.rhscnt\");\n    Builder.CreateCondBr(RHSCond, ContBlock, RHSBlockCnt);\n    CGF.EmitBlock(RHSBlockCnt);\n    CGF.incrementProfileCounter(E->getRHS());\n    CGF.EmitBranch(ContBlock);\n    PN->addIncoming(RHSCond, RHSBlockCnt);\n  }\n\n  // Emit an unconditional branch from this block to ContBlock.  Insert an entry\n  // into the phi node for the edge with the value of RHSCond.\n  CGF.EmitBlock(ContBlock);\n  PN->addIncoming(RHSCond, RHSBlock);\n\n  // ZExt result to int.\n  return Builder.CreateZExtOrBitCast(PN, ResTy, \"lor.ext\");\n}\n\nValue *ScalarExprEmitter::VisitBinComma(const BinaryOperator *E) {\n  CGF.EmitIgnoredExpr(E->getLHS());\n  CGF.EnsureInsertPoint();\n  return Visit(E->getRHS());\n}\n\n//===----------------------------------------------------------------------===//\n//                             Other Operators\n//===----------------------------------------------------------------------===//\n\n/// isCheapEnoughToEvaluateUnconditionally - Return true if the specified\n/// expression is cheap enough and side-effect-free enough to evaluate\n/// unconditionally instead of conditionally.  This is used to convert control\n/// flow into selects in some cases.\nstatic bool isCheapEnoughToEvaluateUnconditionally(const Expr *E,\n                                                   CodeGenFunction &CGF) {\n  // Anything that is an integer or floating point constant is fine.\n  return E->IgnoreParens()->isEvaluatable(CGF.getContext());\n\n  // Even non-volatile automatic variables can't be evaluated unconditionally.\n  // Referencing a thread_local may cause non-trivial initialization work to\n  // occur. If we're inside a lambda and one of the variables is from the scope\n  // outside the lambda, that function may have returned already. Reading its\n  // locals is a bad idea. Also, these reads may introduce races there didn't\n  // exist in the source-level program.\n}\n\n\nValue *ScalarExprEmitter::\nVisitAbstractConditionalOperator(const AbstractConditionalOperator *E) {\n  TestAndClearIgnoreResultAssign();\n\n  // Bind the common expression if necessary.\n  CodeGenFunction::OpaqueValueMapping binding(CGF, E);\n\n  Expr *condExpr = E->getCond();\n  Expr *lhsExpr = E->getTrueExpr();\n  Expr *rhsExpr = E->getFalseExpr();\n\n  // If the condition constant folds and can be elided, try to avoid emitting\n  // the condition and the dead arm.\n  bool CondExprBool;\n  if (CGF.ConstantFoldsToSimpleInteger(condExpr, CondExprBool)) {\n    Expr *live = lhsExpr, *dead = rhsExpr;\n    if (!CondExprBool) std::swap(live, dead);\n\n    // If the dead side doesn't have labels we need, just emit the Live part.\n    if (!CGF.ContainsLabel(dead)) {\n      if (CondExprBool)\n        CGF.incrementProfileCounter(E);\n      Value *Result = Visit(live);\n\n      // If the live part is a throw expression, it acts like it has a void\n      // type, so evaluating it returns a null Value*.  However, a conditional\n      // with non-void type must return a non-null Value*.\n      if (!Result && !E->getType()->isVoidType())\n        Result = llvm::UndefValue::get(CGF.ConvertType(E->getType()));\n\n      return Result;\n    }\n  }\n\n  // OpenCL: If the condition is a vector, we can treat this condition like\n  // the select function.\n  if ((CGF.getLangOpts().OpenCL && condExpr->getType()->isVectorType()) ||\n      condExpr->getType()->isExtVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    llvm::Value *CondV = CGF.EmitScalarExpr(condExpr);\n    llvm::Value *LHS = Visit(lhsExpr);\n    llvm::Value *RHS = Visit(rhsExpr);\n\n    llvm::Type *condType = ConvertType(condExpr->getType());\n    auto *vecTy = cast<llvm::FixedVectorType>(condType);\n\n    unsigned numElem = vecTy->getNumElements();\n    llvm::Type *elemType = vecTy->getElementType();\n\n    llvm::Value *zeroVec = llvm::Constant::getNullValue(vecTy);\n    llvm::Value *TestMSB = Builder.CreateICmpSLT(CondV, zeroVec);\n    llvm::Value *tmp = Builder.CreateSExt(\n        TestMSB, llvm::FixedVectorType::get(elemType, numElem), \"sext\");\n    llvm::Value *tmp2 = Builder.CreateNot(tmp);\n\n    // Cast float to int to perform ANDs if necessary.\n    llvm::Value *RHSTmp = RHS;\n    llvm::Value *LHSTmp = LHS;\n    bool wasCast = false;\n    llvm::VectorType *rhsVTy = cast<llvm::VectorType>(RHS->getType());\n    if (rhsVTy->getElementType()->isFloatingPointTy()) {\n      RHSTmp = Builder.CreateBitCast(RHS, tmp2->getType());\n      LHSTmp = Builder.CreateBitCast(LHS, tmp->getType());\n      wasCast = true;\n    }\n\n    llvm::Value *tmp3 = Builder.CreateAnd(RHSTmp, tmp2);\n    llvm::Value *tmp4 = Builder.CreateAnd(LHSTmp, tmp);\n    llvm::Value *tmp5 = Builder.CreateOr(tmp3, tmp4, \"cond\");\n    if (wasCast)\n      tmp5 = Builder.CreateBitCast(tmp5, RHS->getType());\n\n    return tmp5;\n  }\n\n  if (condExpr->getType()->isVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    llvm::Value *CondV = CGF.EmitScalarExpr(condExpr);\n    llvm::Value *LHS = Visit(lhsExpr);\n    llvm::Value *RHS = Visit(rhsExpr);\n\n    llvm::Type *CondType = ConvertType(condExpr->getType());\n    auto *VecTy = cast<llvm::VectorType>(CondType);\n    llvm::Value *ZeroVec = llvm::Constant::getNullValue(VecTy);\n\n    CondV = Builder.CreateICmpNE(CondV, ZeroVec, \"vector_cond\");\n    return Builder.CreateSelect(CondV, LHS, RHS, \"vector_select\");\n  }\n\n  // If this is a really simple expression (like x ? 4 : 5), emit this as a\n  // select instead of as control flow.  We can only do this if it is cheap and\n  // safe to evaluate the LHS and RHS unconditionally.\n  if (isCheapEnoughToEvaluateUnconditionally(lhsExpr, CGF) &&\n      isCheapEnoughToEvaluateUnconditionally(rhsExpr, CGF)) {\n    llvm::Value *CondV = CGF.EvaluateExprAsBool(condExpr);\n    llvm::Value *StepV = Builder.CreateZExtOrBitCast(CondV, CGF.Int64Ty);\n\n    CGF.incrementProfileCounter(E, StepV);\n\n    llvm::Value *LHS = Visit(lhsExpr);\n    llvm::Value *RHS = Visit(rhsExpr);\n    if (!LHS) {\n      // If the conditional has void type, make sure we return a null Value*.\n      assert(!RHS && \"LHS and RHS types must match\");\n      return nullptr;\n    }\n    return Builder.CreateSelect(CondV, LHS, RHS, \"cond\");\n  }\n\n  llvm::BasicBlock *LHSBlock = CGF.createBasicBlock(\"cond.true\");\n  llvm::BasicBlock *RHSBlock = CGF.createBasicBlock(\"cond.false\");\n  llvm::BasicBlock *ContBlock = CGF.createBasicBlock(\"cond.end\");\n\n  CodeGenFunction::ConditionalEvaluation eval(CGF);\n  CGF.EmitBranchOnBoolExpr(condExpr, LHSBlock, RHSBlock,\n                           CGF.getProfileCount(lhsExpr));\n\n  CGF.EmitBlock(LHSBlock);\n  CGF.incrementProfileCounter(E);\n  eval.begin(CGF);\n  Value *LHS = Visit(lhsExpr);\n  eval.end(CGF);\n\n  LHSBlock = Builder.GetInsertBlock();\n  Builder.CreateBr(ContBlock);\n\n  CGF.EmitBlock(RHSBlock);\n  eval.begin(CGF);\n  Value *RHS = Visit(rhsExpr);\n  eval.end(CGF);\n\n  RHSBlock = Builder.GetInsertBlock();\n  CGF.EmitBlock(ContBlock);\n\n  // If the LHS or RHS is a throw expression, it will be legitimately null.\n  if (!LHS)\n    return RHS;\n  if (!RHS)\n    return LHS;\n\n  // Create a PHI node for the real part.\n  llvm::PHINode *PN = Builder.CreatePHI(LHS->getType(), 2, \"cond\");\n  PN->addIncoming(LHS, LHSBlock);\n  PN->addIncoming(RHS, RHSBlock);\n  return PN;\n}\n\nValue *ScalarExprEmitter::VisitChooseExpr(ChooseExpr *E) {\n  return Visit(E->getChosenSubExpr());\n}\n\nValue *ScalarExprEmitter::VisitVAArgExpr(VAArgExpr *VE) {\n  QualType Ty = VE->getType();\n\n  if (Ty->isVariablyModifiedType())\n    CGF.EmitVariablyModifiedType(Ty);\n\n  Address ArgValue = Address::invalid();\n  Address ArgPtr = CGF.EmitVAArg(VE, ArgValue);\n\n  llvm::Type *ArgTy = ConvertType(VE->getType());\n\n  // If EmitVAArg fails, emit an error.\n  if (!ArgPtr.isValid()) {\n    CGF.ErrorUnsupported(VE, \"va_arg expression\");\n    return llvm::UndefValue::get(ArgTy);\n  }\n\n  // FIXME Volatility.\n  llvm::Value *Val = Builder.CreateLoad(ArgPtr);\n\n  // If EmitVAArg promoted the type, we must truncate it.\n  if (ArgTy != Val->getType()) {\n    if (ArgTy->isPointerTy() && !Val->getType()->isPointerTy())\n      Val = Builder.CreateIntToPtr(Val, ArgTy);\n    else\n      Val = Builder.CreateTrunc(Val, ArgTy);\n  }\n\n  return Val;\n}\n\nValue *ScalarExprEmitter::VisitBlockExpr(const BlockExpr *block) {\n  return CGF.EmitBlockLiteral(block);\n}\n\n// Convert a vec3 to vec4, or vice versa.\nstatic Value *ConvertVec3AndVec4(CGBuilderTy &Builder, CodeGenFunction &CGF,\n                                 Value *Src, unsigned NumElementsDst) {\n  static constexpr int Mask[] = {0, 1, 2, -1};\n  return Builder.CreateShuffleVector(Src,\n                                     llvm::makeArrayRef(Mask, NumElementsDst));\n}\n\n// Create cast instructions for converting LLVM value \\p Src to LLVM type \\p\n// DstTy. \\p Src has the same size as \\p DstTy. Both are single value types\n// but could be scalar or vectors of different lengths, and either can be\n// pointer.\n// There are 4 cases:\n// 1. non-pointer -> non-pointer  : needs 1 bitcast\n// 2. pointer -> pointer          : needs 1 bitcast or addrspacecast\n// 3. pointer -> non-pointer\n//   a) pointer -> intptr_t       : needs 1 ptrtoint\n//   b) pointer -> non-intptr_t   : needs 1 ptrtoint then 1 bitcast\n// 4. non-pointer -> pointer\n//   a) intptr_t -> pointer       : needs 1 inttoptr\n//   b) non-intptr_t -> pointer   : needs 1 bitcast then 1 inttoptr\n// Note: for cases 3b and 4b two casts are required since LLVM casts do not\n// allow casting directly between pointer types and non-integer non-pointer\n// types.\nstatic Value *createCastsForTypeOfSameSize(CGBuilderTy &Builder,\n                                           const llvm::DataLayout &DL,\n                                           Value *Src, llvm::Type *DstTy,\n                                           StringRef Name = \"\") {\n  auto SrcTy = Src->getType();\n\n  // Case 1.\n  if (!SrcTy->isPointerTy() && !DstTy->isPointerTy())\n    return Builder.CreateBitCast(Src, DstTy, Name);\n\n  // Case 2.\n  if (SrcTy->isPointerTy() && DstTy->isPointerTy())\n    return Builder.CreatePointerBitCastOrAddrSpaceCast(Src, DstTy, Name);\n\n  // Case 3.\n  if (SrcTy->isPointerTy() && !DstTy->isPointerTy()) {\n    // Case 3b.\n    if (!DstTy->isIntegerTy())\n      Src = Builder.CreatePtrToInt(Src, DL.getIntPtrType(SrcTy));\n    // Cases 3a and 3b.\n    return Builder.CreateBitOrPointerCast(Src, DstTy, Name);\n  }\n\n  // Case 4b.\n  if (!SrcTy->isIntegerTy())\n    Src = Builder.CreateBitCast(Src, DL.getIntPtrType(DstTy));\n  // Cases 4a and 4b.\n  return Builder.CreateIntToPtr(Src, DstTy, Name);\n}\n\nValue *ScalarExprEmitter::VisitAsTypeExpr(AsTypeExpr *E) {\n  Value *Src  = CGF.EmitScalarExpr(E->getSrcExpr());\n  llvm::Type *DstTy = ConvertType(E->getType());\n\n  llvm::Type *SrcTy = Src->getType();\n  unsigned NumElementsSrc =\n      isa<llvm::VectorType>(SrcTy)\n          ? cast<llvm::FixedVectorType>(SrcTy)->getNumElements()\n          : 0;\n  unsigned NumElementsDst =\n      isa<llvm::VectorType>(DstTy)\n          ? cast<llvm::FixedVectorType>(DstTy)->getNumElements()\n          : 0;\n\n  // Going from vec3 to non-vec3 is a special case and requires a shuffle\n  // vector to get a vec4, then a bitcast if the target type is different.\n  if (NumElementsSrc == 3 && NumElementsDst != 3) {\n    Src = ConvertVec3AndVec4(Builder, CGF, Src, 4);\n\n    if (!CGF.CGM.getCodeGenOpts().PreserveVec3Type) {\n      Src = createCastsForTypeOfSameSize(Builder, CGF.CGM.getDataLayout(), Src,\n                                         DstTy);\n    }\n\n    Src->setName(\"astype\");\n    return Src;\n  }\n\n  // Going from non-vec3 to vec3 is a special case and requires a bitcast\n  // to vec4 if the original type is not vec4, then a shuffle vector to\n  // get a vec3.\n  if (NumElementsSrc != 3 && NumElementsDst == 3) {\n    if (!CGF.CGM.getCodeGenOpts().PreserveVec3Type) {\n      auto *Vec4Ty = llvm::FixedVectorType::get(\n          cast<llvm::VectorType>(DstTy)->getElementType(), 4);\n      Src = createCastsForTypeOfSameSize(Builder, CGF.CGM.getDataLayout(), Src,\n                                         Vec4Ty);\n    }\n\n    Src = ConvertVec3AndVec4(Builder, CGF, Src, 3);\n    Src->setName(\"astype\");\n    return Src;\n  }\n\n  return createCastsForTypeOfSameSize(Builder, CGF.CGM.getDataLayout(),\n                                      Src, DstTy, \"astype\");\n}\n\nValue *ScalarExprEmitter::VisitAtomicExpr(AtomicExpr *E) {\n  return CGF.EmitAtomicExpr(E).getScalarVal();\n}\n\n//===----------------------------------------------------------------------===//\n//                         Entry Point into this File\n//===----------------------------------------------------------------------===//\n\n/// Emit the computation of the specified expression of scalar type, ignoring\n/// the result.\nValue *CodeGenFunction::EmitScalarExpr(const Expr *E, bool IgnoreResultAssign) {\n  assert(E && hasScalarEvaluationKind(E->getType()) &&\n         \"Invalid scalar expression to emit\");\n\n  return ScalarExprEmitter(*this, IgnoreResultAssign)\n      .Visit(const_cast<Expr *>(E));\n}\n\n/// Emit a conversion from the specified type to the specified destination type,\n/// both of which are LLVM scalar types.\nValue *CodeGenFunction::EmitScalarConversion(Value *Src, QualType SrcTy,\n                                             QualType DstTy,\n                                             SourceLocation Loc) {\n  assert(hasScalarEvaluationKind(SrcTy) && hasScalarEvaluationKind(DstTy) &&\n         \"Invalid scalar expression to emit\");\n  return ScalarExprEmitter(*this).EmitScalarConversion(Src, SrcTy, DstTy, Loc);\n}\n\n/// Emit a conversion from the specified complex type to the specified\n/// destination type, where the destination type is an LLVM scalar type.\nValue *CodeGenFunction::EmitComplexToScalarConversion(ComplexPairTy Src,\n                                                      QualType SrcTy,\n                                                      QualType DstTy,\n                                                      SourceLocation Loc) {\n  assert(SrcTy->isAnyComplexType() && hasScalarEvaluationKind(DstTy) &&\n         \"Invalid complex -> scalar conversion\");\n  return ScalarExprEmitter(*this)\n      .EmitComplexToScalarConversion(Src, SrcTy, DstTy, Loc);\n}\n\n\nllvm::Value *CodeGenFunction::\nEmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                        bool isInc, bool isPre) {\n  return ScalarExprEmitter(*this).EmitScalarPrePostIncDec(E, LV, isInc, isPre);\n}\n\nLValue CodeGenFunction::EmitObjCIsaExpr(const ObjCIsaExpr *E) {\n  // object->isa or (*object).isa\n  // Generate code as for: *(Class*)object\n\n  Expr *BaseExpr = E->getBase();\n  Address Addr = Address::invalid();\n  if (BaseExpr->isRValue()) {\n    Addr = Address(EmitScalarExpr(BaseExpr), getPointerAlign());\n  } else {\n    Addr = EmitLValue(BaseExpr).getAddress(*this);\n  }\n\n  // Cast the address to Class*.\n  Addr = Builder.CreateElementBitCast(Addr, ConvertType(E->getType()));\n  return MakeAddrLValue(Addr, E->getType());\n}\n\n\nLValue CodeGenFunction::EmitCompoundAssignmentLValue(\n                                            const CompoundAssignOperator *E) {\n  ScalarExprEmitter Scalar(*this);\n  Value *Result = nullptr;\n  switch (E->getOpcode()) {\n#define COMPOUND_OP(Op)                                                       \\\n    case BO_##Op##Assign:                                                     \\\n      return Scalar.EmitCompoundAssignLValue(E, &ScalarExprEmitter::Emit##Op, \\\n                                             Result)\n  COMPOUND_OP(Mul);\n  COMPOUND_OP(Div);\n  COMPOUND_OP(Rem);\n  COMPOUND_OP(Add);\n  COMPOUND_OP(Sub);\n  COMPOUND_OP(Shl);\n  COMPOUND_OP(Shr);\n  COMPOUND_OP(And);\n  COMPOUND_OP(Xor);\n  COMPOUND_OP(Or);\n#undef COMPOUND_OP\n\n  case BO_PtrMemD:\n  case BO_PtrMemI:\n  case BO_Mul:\n  case BO_Div:\n  case BO_Rem:\n  case BO_Add:\n  case BO_Sub:\n  case BO_Shl:\n  case BO_Shr:\n  case BO_LT:\n  case BO_GT:\n  case BO_LE:\n  case BO_GE:\n  case BO_EQ:\n  case BO_NE:\n  case BO_Cmp:\n  case BO_And:\n  case BO_Xor:\n  case BO_Or:\n  case BO_LAnd:\n  case BO_LOr:\n  case BO_Assign:\n  case BO_Comma:\n    llvm_unreachable(\"Not valid compound assignment operators\");\n  }\n\n  llvm_unreachable(\"Unhandled compound assignment operator\");\n}\n\nstruct GEPOffsetAndOverflow {\n  // The total (signed) byte offset for the GEP.\n  llvm::Value *TotalOffset;\n  // The offset overflow flag - true if the total offset overflows.\n  llvm::Value *OffsetOverflows;\n};\n\n/// Evaluate given GEPVal, which is either an inbounds GEP, or a constant,\n/// and compute the total offset it applies from it's base pointer BasePtr.\n/// Returns offset in bytes and a boolean flag whether an overflow happened\n/// during evaluation.\nstatic GEPOffsetAndOverflow EmitGEPOffsetInBytes(Value *BasePtr, Value *GEPVal,\n                                                 llvm::LLVMContext &VMContext,\n                                                 CodeGenModule &CGM,\n                                                 CGBuilderTy &Builder) {\n  const auto &DL = CGM.getDataLayout();\n\n  // The total (signed) byte offset for the GEP.\n  llvm::Value *TotalOffset = nullptr;\n\n  // Was the GEP already reduced to a constant?\n  if (isa<llvm::Constant>(GEPVal)) {\n    // Compute the offset by casting both pointers to integers and subtracting:\n    // GEPVal = BasePtr + ptr(Offset) <--> Offset = int(GEPVal) - int(BasePtr)\n    Value *BasePtr_int =\n        Builder.CreatePtrToInt(BasePtr, DL.getIntPtrType(BasePtr->getType()));\n    Value *GEPVal_int =\n        Builder.CreatePtrToInt(GEPVal, DL.getIntPtrType(GEPVal->getType()));\n    TotalOffset = Builder.CreateSub(GEPVal_int, BasePtr_int);\n    return {TotalOffset, /*OffsetOverflows=*/Builder.getFalse()};\n  }\n\n  auto *GEP = cast<llvm::GEPOperator>(GEPVal);\n  assert(GEP->getPointerOperand() == BasePtr &&\n         \"BasePtr must be the the base of the GEP.\");\n  assert(GEP->isInBounds() && \"Expected inbounds GEP\");\n\n  auto *IntPtrTy = DL.getIntPtrType(GEP->getPointerOperandType());\n\n  // Grab references to the signed add/mul overflow intrinsics for intptr_t.\n  auto *Zero = llvm::ConstantInt::getNullValue(IntPtrTy);\n  auto *SAddIntrinsic =\n      CGM.getIntrinsic(llvm::Intrinsic::sadd_with_overflow, IntPtrTy);\n  auto *SMulIntrinsic =\n      CGM.getIntrinsic(llvm::Intrinsic::smul_with_overflow, IntPtrTy);\n\n  // The offset overflow flag - true if the total offset overflows.\n  llvm::Value *OffsetOverflows = Builder.getFalse();\n\n  /// Return the result of the given binary operation.\n  auto eval = [&](BinaryOperator::Opcode Opcode, llvm::Value *LHS,\n                  llvm::Value *RHS) -> llvm::Value * {\n    assert((Opcode == BO_Add || Opcode == BO_Mul) && \"Can't eval binop\");\n\n    // If the operands are constants, return a constant result.\n    if (auto *LHSCI = dyn_cast<llvm::ConstantInt>(LHS)) {\n      if (auto *RHSCI = dyn_cast<llvm::ConstantInt>(RHS)) {\n        llvm::APInt N;\n        bool HasOverflow = mayHaveIntegerOverflow(LHSCI, RHSCI, Opcode,\n                                                  /*Signed=*/true, N);\n        if (HasOverflow)\n          OffsetOverflows = Builder.getTrue();\n        return llvm::ConstantInt::get(VMContext, N);\n      }\n    }\n\n    // Otherwise, compute the result with checked arithmetic.\n    auto *ResultAndOverflow = Builder.CreateCall(\n        (Opcode == BO_Add) ? SAddIntrinsic : SMulIntrinsic, {LHS, RHS});\n    OffsetOverflows = Builder.CreateOr(\n        Builder.CreateExtractValue(ResultAndOverflow, 1), OffsetOverflows);\n    return Builder.CreateExtractValue(ResultAndOverflow, 0);\n  };\n\n  // Determine the total byte offset by looking at each GEP operand.\n  for (auto GTI = llvm::gep_type_begin(GEP), GTE = llvm::gep_type_end(GEP);\n       GTI != GTE; ++GTI) {\n    llvm::Value *LocalOffset;\n    auto *Index = GTI.getOperand();\n    // Compute the local offset contributed by this indexing step:\n    if (auto *STy = GTI.getStructTypeOrNull()) {\n      // For struct indexing, the local offset is the byte position of the\n      // specified field.\n      unsigned FieldNo = cast<llvm::ConstantInt>(Index)->getZExtValue();\n      LocalOffset = llvm::ConstantInt::get(\n          IntPtrTy, DL.getStructLayout(STy)->getElementOffset(FieldNo));\n    } else {\n      // Otherwise this is array-like indexing. The local offset is the index\n      // multiplied by the element size.\n      auto *ElementSize = llvm::ConstantInt::get(\n          IntPtrTy, DL.getTypeAllocSize(GTI.getIndexedType()));\n      auto *IndexS = Builder.CreateIntCast(Index, IntPtrTy, /*isSigned=*/true);\n      LocalOffset = eval(BO_Mul, ElementSize, IndexS);\n    }\n\n    // If this is the first offset, set it as the total offset. Otherwise, add\n    // the local offset into the running total.\n    if (!TotalOffset || TotalOffset == Zero)\n      TotalOffset = LocalOffset;\n    else\n      TotalOffset = eval(BO_Add, TotalOffset, LocalOffset);\n  }\n\n  return {TotalOffset, OffsetOverflows};\n}\n\nValue *\nCodeGenFunction::EmitCheckedInBoundsGEP(Value *Ptr, ArrayRef<Value *> IdxList,\n                                        bool SignedIndices, bool IsSubtraction,\n                                        SourceLocation Loc, const Twine &Name) {\n  Value *GEPVal = Builder.CreateInBoundsGEP(Ptr, IdxList, Name);\n\n  // If the pointer overflow sanitizer isn't enabled, do nothing.\n  if (!SanOpts.has(SanitizerKind::PointerOverflow))\n    return GEPVal;\n\n  llvm::Type *PtrTy = Ptr->getType();\n\n  // Perform nullptr-and-offset check unless the nullptr is defined.\n  bool PerformNullCheck = !NullPointerIsDefined(\n      Builder.GetInsertBlock()->getParent(), PtrTy->getPointerAddressSpace());\n  // Check for overflows unless the GEP got constant-folded,\n  // and only in the default address space\n  bool PerformOverflowCheck =\n      !isa<llvm::Constant>(GEPVal) && PtrTy->getPointerAddressSpace() == 0;\n\n  if (!(PerformNullCheck || PerformOverflowCheck))\n    return GEPVal;\n\n  const auto &DL = CGM.getDataLayout();\n\n  SanitizerScope SanScope(this);\n  llvm::Type *IntPtrTy = DL.getIntPtrType(PtrTy);\n\n  GEPOffsetAndOverflow EvaluatedGEP =\n      EmitGEPOffsetInBytes(Ptr, GEPVal, getLLVMContext(), CGM, Builder);\n\n  assert((!isa<llvm::Constant>(EvaluatedGEP.TotalOffset) ||\n          EvaluatedGEP.OffsetOverflows == Builder.getFalse()) &&\n         \"If the offset got constant-folded, we don't expect that there was an \"\n         \"overflow.\");\n\n  auto *Zero = llvm::ConstantInt::getNullValue(IntPtrTy);\n\n  // Common case: if the total offset is zero, and we are using C++ semantics,\n  // where nullptr+0 is defined, don't emit a check.\n  if (EvaluatedGEP.TotalOffset == Zero && CGM.getLangOpts().CPlusPlus)\n    return GEPVal;\n\n  // Now that we've computed the total offset, add it to the base pointer (with\n  // wrapping semantics).\n  auto *IntPtr = Builder.CreatePtrToInt(Ptr, IntPtrTy);\n  auto *ComputedGEP = Builder.CreateAdd(IntPtr, EvaluatedGEP.TotalOffset);\n\n  llvm::SmallVector<std::pair<llvm::Value *, SanitizerMask>, 2> Checks;\n\n  if (PerformNullCheck) {\n    // In C++, if the base pointer evaluates to a null pointer value,\n    // the only valid  pointer this inbounds GEP can produce is also\n    // a null pointer, so the offset must also evaluate to zero.\n    // Likewise, if we have non-zero base pointer, we can not get null pointer\n    // as a result, so the offset can not be -intptr_t(BasePtr).\n    // In other words, both pointers are either null, or both are non-null,\n    // or the behaviour is undefined.\n    //\n    // C, however, is more strict in this regard, and gives more\n    // optimization opportunities: in C, additionally, nullptr+0 is undefined.\n    // So both the input to the 'gep inbounds' AND the output must not be null.\n    auto *BaseIsNotNullptr = Builder.CreateIsNotNull(Ptr);\n    auto *ResultIsNotNullptr = Builder.CreateIsNotNull(ComputedGEP);\n    auto *Valid =\n        CGM.getLangOpts().CPlusPlus\n            ? Builder.CreateICmpEQ(BaseIsNotNullptr, ResultIsNotNullptr)\n            : Builder.CreateAnd(BaseIsNotNullptr, ResultIsNotNullptr);\n    Checks.emplace_back(Valid, SanitizerKind::PointerOverflow);\n  }\n\n  if (PerformOverflowCheck) {\n    // The GEP is valid if:\n    // 1) The total offset doesn't overflow, and\n    // 2) The sign of the difference between the computed address and the base\n    // pointer matches the sign of the total offset.\n    llvm::Value *ValidGEP;\n    auto *NoOffsetOverflow = Builder.CreateNot(EvaluatedGEP.OffsetOverflows);\n    if (SignedIndices) {\n      // GEP is computed as `unsigned base + signed offset`, therefore:\n      // * If offset was positive, then the computed pointer can not be\n      //   [unsigned] less than the base pointer, unless it overflowed.\n      // * If offset was negative, then the computed pointer can not be\n      //   [unsigned] greater than the bas pointere, unless it overflowed.\n      auto *PosOrZeroValid = Builder.CreateICmpUGE(ComputedGEP, IntPtr);\n      auto *PosOrZeroOffset =\n          Builder.CreateICmpSGE(EvaluatedGEP.TotalOffset, Zero);\n      llvm::Value *NegValid = Builder.CreateICmpULT(ComputedGEP, IntPtr);\n      ValidGEP =\n          Builder.CreateSelect(PosOrZeroOffset, PosOrZeroValid, NegValid);\n    } else if (!IsSubtraction) {\n      // GEP is computed as `unsigned base + unsigned offset`,  therefore the\n      // computed pointer can not be [unsigned] less than base pointer,\n      // unless there was an overflow.\n      // Equivalent to `@llvm.uadd.with.overflow(%base, %offset)`.\n      ValidGEP = Builder.CreateICmpUGE(ComputedGEP, IntPtr);\n    } else {\n      // GEP is computed as `unsigned base - unsigned offset`, therefore the\n      // computed pointer can not be [unsigned] greater than base pointer,\n      // unless there was an overflow.\n      // Equivalent to `@llvm.usub.with.overflow(%base, sub(0, %offset))`.\n      ValidGEP = Builder.CreateICmpULE(ComputedGEP, IntPtr);\n    }\n    ValidGEP = Builder.CreateAnd(ValidGEP, NoOffsetOverflow);\n    Checks.emplace_back(ValidGEP, SanitizerKind::PointerOverflow);\n  }\n\n  assert(!Checks.empty() && \"Should have produced some checks.\");\n\n  llvm::Constant *StaticArgs[] = {EmitCheckSourceLocation(Loc)};\n  // Pass the computed GEP to the runtime to avoid emitting poisoned arguments.\n  llvm::Value *DynamicArgs[] = {IntPtr, ComputedGEP};\n  EmitCheck(Checks, SanitizerHandler::PointerOverflow, StaticArgs, DynamicArgs);\n\n  return GEPVal;\n}\n"}, "42": {"id": 42, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "content": "//===-- CodeGenFunction.h - Per-Function state for LLVM CodeGen -*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This is the internal per-function state used for llvm translation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CODEGENFUNCTION_H\n#define LLVM_CLANG_LIB_CODEGEN_CODEGENFUNCTION_H\n\n#include \"CGBuilder.h\"\n#include \"CGDebugInfo.h\"\n#include \"CGLoopInfo.h\"\n#include \"CGValue.h\"\n#include \"CodeGenModule.h\"\n#include \"CodeGenPGO.h\"\n#include \"EHScopeStack.h\"\n#include \"VarBypassDetector.h\"\n#include \"clang/AST/CharUnits.h\"\n#include \"clang/AST/CurrentSourceLocExprScope.h\"\n#include \"clang/AST/ExprCXX.h\"\n#include \"clang/AST/ExprObjC.h\"\n#include \"clang/AST/ExprOpenMP.h\"\n#include \"clang/AST/StmtOpenMP.h\"\n#include \"clang/AST/Type.h\"\n#include \"clang/Basic/ABI.h\"\n#include \"clang/Basic/CapturedStmt.h\"\n#include \"clang/Basic/CodeGenOptions.h\"\n#include \"clang/Basic/OpenMPKinds.h\"\n#include \"clang/Basic/TargetInfo.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/MapVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/Frontend/OpenMP/OMPIRBuilder.h\"\n#include \"llvm/IR/ValueHandle.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Transforms/Utils/SanitizerStats.h\"\n\nnamespace llvm {\nclass BasicBlock;\nclass LLVMContext;\nclass MDNode;\nclass Module;\nclass SwitchInst;\nclass Twine;\nclass Value;\nclass CanonicalLoopInfo;\n}\n\nnamespace clang {\nclass ASTContext;\nclass BlockDecl;\nclass CXXDestructorDecl;\nclass CXXForRangeStmt;\nclass CXXTryStmt;\nclass Decl;\nclass LabelDecl;\nclass EnumConstantDecl;\nclass FunctionDecl;\nclass FunctionProtoType;\nclass LabelStmt;\nclass ObjCContainerDecl;\nclass ObjCInterfaceDecl;\nclass ObjCIvarDecl;\nclass ObjCMethodDecl;\nclass ObjCImplementationDecl;\nclass ObjCPropertyImplDecl;\nclass TargetInfo;\nclass VarDecl;\nclass ObjCForCollectionStmt;\nclass ObjCAtTryStmt;\nclass ObjCAtThrowStmt;\nclass ObjCAtSynchronizedStmt;\nclass ObjCAutoreleasePoolStmt;\nclass OMPUseDevicePtrClause;\nclass OMPUseDeviceAddrClause;\nclass ReturnsNonNullAttr;\nclass SVETypeFlags;\nclass OMPExecutableDirective;\n\nnamespace analyze_os_log {\nclass OSLogBufferLayout;\n}\n\nnamespace CodeGen {\nclass CodeGenTypes;\nclass CGCallee;\nclass CGFunctionInfo;\nclass CGRecordLayout;\nclass CGBlockInfo;\nclass CGCXXABI;\nclass BlockByrefHelpers;\nclass BlockByrefInfo;\nclass BlockFlags;\nclass BlockFieldFlags;\nclass RegionCodeGenTy;\nclass TargetCodeGenInfo;\nstruct OMPTaskDataTy;\nstruct CGCoroData;\n\n/// The kind of evaluation to perform on values of a particular\n/// type.  Basically, is the code in CGExprScalar, CGExprComplex, or\n/// CGExprAgg?\n///\n/// TODO: should vectors maybe be split out into their own thing?\nenum TypeEvaluationKind {\n  TEK_Scalar,\n  TEK_Complex,\n  TEK_Aggregate\n};\n\n#define LIST_SANITIZER_CHECKS                                                  \\\n  SANITIZER_CHECK(AddOverflow, add_overflow, 0)                                \\\n  SANITIZER_CHECK(BuiltinUnreachable, builtin_unreachable, 0)                  \\\n  SANITIZER_CHECK(CFICheckFail, cfi_check_fail, 0)                             \\\n  SANITIZER_CHECK(DivremOverflow, divrem_overflow, 0)                          \\\n  SANITIZER_CHECK(DynamicTypeCacheMiss, dynamic_type_cache_miss, 0)            \\\n  SANITIZER_CHECK(FloatCastOverflow, float_cast_overflow, 0)                   \\\n  SANITIZER_CHECK(FunctionTypeMismatch, function_type_mismatch, 1)             \\\n  SANITIZER_CHECK(ImplicitConversion, implicit_conversion, 0)                  \\\n  SANITIZER_CHECK(InvalidBuiltin, invalid_builtin, 0)                          \\\n  SANITIZER_CHECK(InvalidObjCCast, invalid_objc_cast, 0)                       \\\n  SANITIZER_CHECK(LoadInvalidValue, load_invalid_value, 0)                     \\\n  SANITIZER_CHECK(MissingReturn, missing_return, 0)                            \\\n  SANITIZER_CHECK(MulOverflow, mul_overflow, 0)                                \\\n  SANITIZER_CHECK(NegateOverflow, negate_overflow, 0)                          \\\n  SANITIZER_CHECK(NullabilityArg, nullability_arg, 0)                          \\\n  SANITIZER_CHECK(NullabilityReturn, nullability_return, 1)                    \\\n  SANITIZER_CHECK(NonnullArg, nonnull_arg, 0)                                  \\\n  SANITIZER_CHECK(NonnullReturn, nonnull_return, 1)                            \\\n  SANITIZER_CHECK(OutOfBounds, out_of_bounds, 0)                               \\\n  SANITIZER_CHECK(PointerOverflow, pointer_overflow, 0)                        \\\n  SANITIZER_CHECK(ShiftOutOfBounds, shift_out_of_bounds, 0)                    \\\n  SANITIZER_CHECK(SubOverflow, sub_overflow, 0)                                \\\n  SANITIZER_CHECK(TypeMismatch, type_mismatch, 1)                              \\\n  SANITIZER_CHECK(AlignmentAssumption, alignment_assumption, 0)                \\\n  SANITIZER_CHECK(VLABoundNotPositive, vla_bound_not_positive, 0)\n\nenum SanitizerHandler {\n#define SANITIZER_CHECK(Enum, Name, Version) Enum,\n  LIST_SANITIZER_CHECKS\n#undef SANITIZER_CHECK\n};\n\n/// Helper class with most of the code for saving a value for a\n/// conditional expression cleanup.\nstruct DominatingLLVMValue {\n  typedef llvm::PointerIntPair<llvm::Value*, 1, bool> saved_type;\n\n  /// Answer whether the given value needs extra work to be saved.\n  static bool needsSaving(llvm::Value *value) {\n    // If it's not an instruction, we don't need to save.\n    if (!isa<llvm::Instruction>(value)) return false;\n\n    // If it's an instruction in the entry block, we don't need to save.\n    llvm::BasicBlock *block = cast<llvm::Instruction>(value)->getParent();\n    return (block != &block->getParent()->getEntryBlock());\n  }\n\n  static saved_type save(CodeGenFunction &CGF, llvm::Value *value);\n  static llvm::Value *restore(CodeGenFunction &CGF, saved_type value);\n};\n\n/// A partial specialization of DominatingValue for llvm::Values that\n/// might be llvm::Instructions.\ntemplate <class T> struct DominatingPointer<T,true> : DominatingLLVMValue {\n  typedef T *type;\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return static_cast<T*>(DominatingLLVMValue::restore(CGF, value));\n  }\n};\n\n/// A specialization of DominatingValue for Address.\ntemplate <> struct DominatingValue<Address> {\n  typedef Address type;\n\n  struct saved_type {\n    DominatingLLVMValue::saved_type SavedValue;\n    CharUnits Alignment;\n  };\n\n  static bool needsSaving(type value) {\n    return DominatingLLVMValue::needsSaving(value.getPointer());\n  }\n  static saved_type save(CodeGenFunction &CGF, type value) {\n    return { DominatingLLVMValue::save(CGF, value.getPointer()),\n             value.getAlignment() };\n  }\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return Address(DominatingLLVMValue::restore(CGF, value.SavedValue),\n                   value.Alignment);\n  }\n};\n\n/// A specialization of DominatingValue for RValue.\ntemplate <> struct DominatingValue<RValue> {\n  typedef RValue type;\n  class saved_type {\n    enum Kind { ScalarLiteral, ScalarAddress, AggregateLiteral,\n                AggregateAddress, ComplexAddress };\n\n    llvm::Value *Value;\n    unsigned K : 3;\n    unsigned Align : 29;\n    saved_type(llvm::Value *v, Kind k, unsigned a = 0)\n      : Value(v), K(k), Align(a) {}\n\n  public:\n    static bool needsSaving(RValue value);\n    static saved_type save(CodeGenFunction &CGF, RValue value);\n    RValue restore(CodeGenFunction &CGF);\n\n    // implementations in CGCleanup.cpp\n  };\n\n  static bool needsSaving(type value) {\n    return saved_type::needsSaving(value);\n  }\n  static saved_type save(CodeGenFunction &CGF, type value) {\n    return saved_type::save(CGF, value);\n  }\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return value.restore(CGF);\n  }\n};\n\n/// CodeGenFunction - This class organizes the per-function state that is used\n/// while generating LLVM code.\nclass CodeGenFunction : public CodeGenTypeCache {\n  CodeGenFunction(const CodeGenFunction &) = delete;\n  void operator=(const CodeGenFunction &) = delete;\n\n  friend class CGCXXABI;\npublic:\n  /// A jump destination is an abstract label, branching to which may\n  /// require a jump out through normal cleanups.\n  struct JumpDest {\n    JumpDest() : Block(nullptr), ScopeDepth(), Index(0) {}\n    JumpDest(llvm::BasicBlock *Block,\n             EHScopeStack::stable_iterator Depth,\n             unsigned Index)\n      : Block(Block), ScopeDepth(Depth), Index(Index) {}\n\n    bool isValid() const { return Block != nullptr; }\n    llvm::BasicBlock *getBlock() const { return Block; }\n    EHScopeStack::stable_iterator getScopeDepth() const { return ScopeDepth; }\n    unsigned getDestIndex() const { return Index; }\n\n    // This should be used cautiously.\n    void setScopeDepth(EHScopeStack::stable_iterator depth) {\n      ScopeDepth = depth;\n    }\n\n  private:\n    llvm::BasicBlock *Block;\n    EHScopeStack::stable_iterator ScopeDepth;\n    unsigned Index;\n  };\n\n  CodeGenModule &CGM;  // Per-module state.\n  const TargetInfo &Target;\n\n  // For EH/SEH outlined funclets, this field points to parent's CGF\n  CodeGenFunction *ParentCGF = nullptr;\n\n  typedef std::pair<llvm::Value *, llvm::Value *> ComplexPairTy;\n  LoopInfoStack LoopStack;\n  CGBuilderTy Builder;\n\n  // Stores variables for which we can't generate correct lifetime markers\n  // because of jumps.\n  VarBypassDetector Bypasses;\n\n  /// List of recently emitted OMPCanonicalLoops.\n  ///\n  /// Since OMPCanonicalLoops are nested inside other statements (in particular\n  /// CapturedStmt generated by OMPExecutableDirective and non-perfectly nested\n  /// loops), we cannot directly call OMPEmitOMPCanonicalLoop and receive its\n  /// llvm::CanonicalLoopInfo. Instead, we call EmitStmt and any\n  /// OMPEmitOMPCanonicalLoop called by it will add its CanonicalLoopInfo to\n  /// this stack when done. Entering a new loop requires clearing this list; it\n  /// either means we start parsing a new loop nest (in which case the previous\n  /// loop nest goes out of scope) or a second loop in the same level in which\n  /// case it would be ambiguous into which of the two (or more) loops the loop\n  /// nest would extend.\n  SmallVector<llvm::CanonicalLoopInfo *, 4> OMPLoopNestStack;\n\n  // CodeGen lambda for loops and support for ordered clause\n  typedef llvm::function_ref<void(CodeGenFunction &, const OMPLoopDirective &,\n                                  JumpDest)>\n      CodeGenLoopTy;\n  typedef llvm::function_ref<void(CodeGenFunction &, SourceLocation,\n                                  const unsigned, const bool)>\n      CodeGenOrderedTy;\n\n  // Codegen lambda for loop bounds in worksharing loop constructs\n  typedef llvm::function_ref<std::pair<LValue, LValue>(\n      CodeGenFunction &, const OMPExecutableDirective &S)>\n      CodeGenLoopBoundsTy;\n\n  // Codegen lambda for loop bounds in dispatch-based loop implementation\n  typedef llvm::function_ref<std::pair<llvm::Value *, llvm::Value *>(\n      CodeGenFunction &, const OMPExecutableDirective &S, Address LB,\n      Address UB)>\n      CodeGenDispatchBoundsTy;\n\n  /// CGBuilder insert helper. This function is called after an\n  /// instruction is created using Builder.\n  void InsertHelper(llvm::Instruction *I, const llvm::Twine &Name,\n                    llvm::BasicBlock *BB,\n                    llvm::BasicBlock::iterator InsertPt) const;\n\n  /// CurFuncDecl - Holds the Decl for the current outermost\n  /// non-closure context.\n  const Decl *CurFuncDecl;\n  /// CurCodeDecl - This is the inner-most code context, which includes blocks.\n  const Decl *CurCodeDecl;\n  const CGFunctionInfo *CurFnInfo;\n  QualType FnRetTy;\n  llvm::Function *CurFn = nullptr;\n\n  // Holds coroutine data if the current function is a coroutine. We use a\n  // wrapper to manage its lifetime, so that we don't have to define CGCoroData\n  // in this header.\n  struct CGCoroInfo {\n    std::unique_ptr<CGCoroData> Data;\n    CGCoroInfo();\n    ~CGCoroInfo();\n  };\n  CGCoroInfo CurCoro;\n\n  bool isCoroutine() const {\n    return CurCoro.Data != nullptr;\n  }\n\n  /// CurGD - The GlobalDecl for the current function being compiled.\n  GlobalDecl CurGD;\n\n  /// PrologueCleanupDepth - The cleanup depth enclosing all the\n  /// cleanups associated with the parameters.\n  EHScopeStack::stable_iterator PrologueCleanupDepth;\n\n  /// ReturnBlock - Unified return block.\n  JumpDest ReturnBlock;\n\n  /// ReturnValue - The temporary alloca to hold the return\n  /// value. This is invalid iff the function has no return value.\n  Address ReturnValue = Address::invalid();\n\n  /// ReturnValuePointer - The temporary alloca to hold a pointer to sret.\n  /// This is invalid if sret is not in use.\n  Address ReturnValuePointer = Address::invalid();\n\n  /// If a return statement is being visited, this holds the return statment's\n  /// result expression.\n  const Expr *RetExpr = nullptr;\n\n  /// Return true if a label was seen in the current scope.\n  bool hasLabelBeenSeenInCurrentScope() const {\n    if (CurLexicalScope)\n      return CurLexicalScope->hasLabels();\n    return !LabelMap.empty();\n  }\n\n  /// AllocaInsertPoint - This is an instruction in the entry block before which\n  /// we prefer to insert allocas.\n  llvm::AssertingVH<llvm::Instruction> AllocaInsertPt;\n\n  /// API for captured statement code generation.\n  class CGCapturedStmtInfo {\n  public:\n    explicit CGCapturedStmtInfo(CapturedRegionKind K = CR_Default)\n        : Kind(K), ThisValue(nullptr), CXXThisFieldDecl(nullptr) {}\n    explicit CGCapturedStmtInfo(const CapturedStmt &S,\n                                CapturedRegionKind K = CR_Default)\n      : Kind(K), ThisValue(nullptr), CXXThisFieldDecl(nullptr) {\n\n      RecordDecl::field_iterator Field =\n        S.getCapturedRecordDecl()->field_begin();\n      for (CapturedStmt::const_capture_iterator I = S.capture_begin(),\n                                                E = S.capture_end();\n           I != E; ++I, ++Field) {\n        if (I->capturesThis())\n          CXXThisFieldDecl = *Field;\n        else if (I->capturesVariable())\n          CaptureFields[I->getCapturedVar()->getCanonicalDecl()] = *Field;\n        else if (I->capturesVariableByCopy())\n          CaptureFields[I->getCapturedVar()->getCanonicalDecl()] = *Field;\n      }\n    }\n\n    virtual ~CGCapturedStmtInfo();\n\n    CapturedRegionKind getKind() const { return Kind; }\n\n    virtual void setContextValue(llvm::Value *V) { ThisValue = V; }\n    // Retrieve the value of the context parameter.\n    virtual llvm::Value *getContextValue() const { return ThisValue; }\n\n    /// Lookup the captured field decl for a variable.\n    virtual const FieldDecl *lookup(const VarDecl *VD) const {\n      return CaptureFields.lookup(VD->getCanonicalDecl());\n    }\n\n    bool isCXXThisExprCaptured() const { return getThisFieldDecl() != nullptr; }\n    virtual FieldDecl *getThisFieldDecl() const { return CXXThisFieldDecl; }\n\n    static bool classof(const CGCapturedStmtInfo *) {\n      return true;\n    }\n\n    /// Emit the captured statement body.\n    virtual void EmitBody(CodeGenFunction &CGF, const Stmt *S) {\n      CGF.incrementProfileCounter(S);\n      CGF.EmitStmt(S);\n    }\n\n    /// Get the name of the capture helper.\n    virtual StringRef getHelperName() const { return \"__captured_stmt\"; }\n\n  private:\n    /// The kind of captured statement being generated.\n    CapturedRegionKind Kind;\n\n    /// Keep the map between VarDecl and FieldDecl.\n    llvm::SmallDenseMap<const VarDecl *, FieldDecl *> CaptureFields;\n\n    /// The base address of the captured record, passed in as the first\n    /// argument of the parallel region function.\n    llvm::Value *ThisValue;\n\n    /// Captured 'this' type.\n    FieldDecl *CXXThisFieldDecl;\n  };\n  CGCapturedStmtInfo *CapturedStmtInfo = nullptr;\n\n  /// RAII for correct setting/restoring of CapturedStmtInfo.\n  class CGCapturedStmtRAII {\n  private:\n    CodeGenFunction &CGF;\n    CGCapturedStmtInfo *PrevCapturedStmtInfo;\n  public:\n    CGCapturedStmtRAII(CodeGenFunction &CGF,\n                       CGCapturedStmtInfo *NewCapturedStmtInfo)\n        : CGF(CGF), PrevCapturedStmtInfo(CGF.CapturedStmtInfo) {\n      CGF.CapturedStmtInfo = NewCapturedStmtInfo;\n    }\n    ~CGCapturedStmtRAII() { CGF.CapturedStmtInfo = PrevCapturedStmtInfo; }\n  };\n\n  /// An abstract representation of regular/ObjC call/message targets.\n  class AbstractCallee {\n    /// The function declaration of the callee.\n    const Decl *CalleeDecl;\n\n  public:\n    AbstractCallee() : CalleeDecl(nullptr) {}\n    AbstractCallee(const FunctionDecl *FD) : CalleeDecl(FD) {}\n    AbstractCallee(const ObjCMethodDecl *OMD) : CalleeDecl(OMD) {}\n    bool hasFunctionDecl() const {\n      return dyn_cast_or_null<FunctionDecl>(CalleeDecl);\n    }\n    const Decl *getDecl() const { return CalleeDecl; }\n    unsigned getNumParams() const {\n      if (const auto *FD = dyn_cast<FunctionDecl>(CalleeDecl))\n        return FD->getNumParams();\n      return cast<ObjCMethodDecl>(CalleeDecl)->param_size();\n    }\n    const ParmVarDecl *getParamDecl(unsigned I) const {\n      if (const auto *FD = dyn_cast<FunctionDecl>(CalleeDecl))\n        return FD->getParamDecl(I);\n      return *(cast<ObjCMethodDecl>(CalleeDecl)->param_begin() + I);\n    }\n  };\n\n  /// Sanitizers enabled for this function.\n  SanitizerSet SanOpts;\n\n  /// True if CodeGen currently emits code implementing sanitizer checks.\n  bool IsSanitizerScope = false;\n\n  /// RAII object to set/unset CodeGenFunction::IsSanitizerScope.\n  class SanitizerScope {\n    CodeGenFunction *CGF;\n  public:\n    SanitizerScope(CodeGenFunction *CGF);\n    ~SanitizerScope();\n  };\n\n  /// In C++, whether we are code generating a thunk.  This controls whether we\n  /// should emit cleanups.\n  bool CurFuncIsThunk = false;\n\n  /// In ARC, whether we should autorelease the return value.\n  bool AutoreleaseResult = false;\n\n  /// Whether we processed a Microsoft-style asm block during CodeGen. These can\n  /// potentially set the return value.\n  bool SawAsmBlock = false;\n\n  const NamedDecl *CurSEHParent = nullptr;\n\n  /// True if the current function is an outlined SEH helper. This can be a\n  /// finally block or filter expression.\n  bool IsOutlinedSEHHelper = false;\n\n  /// True if CodeGen currently emits code inside presereved access index\n  /// region.\n  bool IsInPreservedAIRegion = false;\n\n  /// True if the current statement has nomerge attribute.\n  bool InNoMergeAttributedStmt = false;\n\n  /// True if the current function should be marked mustprogress.\n  bool FnIsMustProgress = false;\n\n  /// True if the C++ Standard Requires Progress.\n  bool CPlusPlusWithProgress() {\n    if (CGM.getCodeGenOpts().getFiniteLoops() ==\n        CodeGenOptions::FiniteLoopsKind::Never)\n      return false;\n\n    return getLangOpts().CPlusPlus11 || getLangOpts().CPlusPlus14 ||\n           getLangOpts().CPlusPlus17 || getLangOpts().CPlusPlus20;\n  }\n\n  /// True if the C Standard Requires Progress.\n  bool CWithProgress() {\n    if (CGM.getCodeGenOpts().getFiniteLoops() ==\n        CodeGenOptions::FiniteLoopsKind::Always)\n      return true;\n    if (CGM.getCodeGenOpts().getFiniteLoops() ==\n        CodeGenOptions::FiniteLoopsKind::Never)\n      return false;\n\n    return getLangOpts().C11 || getLangOpts().C17 || getLangOpts().C2x;\n  }\n\n  /// True if the language standard requires progress in functions or\n  /// in infinite loops with non-constant conditionals.\n  bool LanguageRequiresProgress() {\n    return CWithProgress() || CPlusPlusWithProgress();\n  }\n\n  const CodeGen::CGBlockInfo *BlockInfo = nullptr;\n  llvm::Value *BlockPointer = nullptr;\n\n  llvm::DenseMap<const VarDecl *, FieldDecl *> LambdaCaptureFields;\n  FieldDecl *LambdaThisCaptureField = nullptr;\n\n  /// A mapping from NRVO variables to the flags used to indicate\n  /// when the NRVO has been applied to this variable.\n  llvm::DenseMap<const VarDecl *, llvm::Value *> NRVOFlags;\n\n  EHScopeStack EHStack;\n  llvm::SmallVector<char, 256> LifetimeExtendedCleanupStack;\n  llvm::SmallVector<const JumpDest *, 2> SEHTryEpilogueStack;\n\n  llvm::Instruction *CurrentFuncletPad = nullptr;\n\n  class CallLifetimeEnd final : public EHScopeStack::Cleanup {\n    llvm::Value *Addr;\n    llvm::Value *Size;\n\n  public:\n    CallLifetimeEnd(Address addr, llvm::Value *size)\n        : Addr(addr.getPointer()), Size(size) {}\n\n    void Emit(CodeGenFunction &CGF, Flags flags) override {\n      CGF.EmitLifetimeEnd(Size, Addr);\n    }\n  };\n\n  /// Header for data within LifetimeExtendedCleanupStack.\n  struct LifetimeExtendedCleanupHeader {\n    /// The size of the following cleanup object.\n    unsigned Size;\n    /// The kind of cleanup to push: a value from the CleanupKind enumeration.\n    unsigned Kind : 31;\n    /// Whether this is a conditional cleanup.\n    unsigned IsConditional : 1;\n\n    size_t getSize() const { return Size; }\n    CleanupKind getKind() const { return (CleanupKind)Kind; }\n    bool isConditional() const { return IsConditional; }\n  };\n\n  /// i32s containing the indexes of the cleanup destinations.\n  Address NormalCleanupDest = Address::invalid();\n\n  unsigned NextCleanupDestIndex = 1;\n\n  /// EHResumeBlock - Unified block containing a call to llvm.eh.resume.\n  llvm::BasicBlock *EHResumeBlock = nullptr;\n\n  /// The exception slot.  All landing pads write the current exception pointer\n  /// into this alloca.\n  llvm::Value *ExceptionSlot = nullptr;\n\n  /// The selector slot.  Under the MandatoryCleanup model, all landing pads\n  /// write the current selector value into this alloca.\n  llvm::AllocaInst *EHSelectorSlot = nullptr;\n\n  /// A stack of exception code slots. Entering an __except block pushes a slot\n  /// on the stack and leaving pops one. The __exception_code() intrinsic loads\n  /// a value from the top of the stack.\n  SmallVector<Address, 1> SEHCodeSlotStack;\n\n  /// Value returned by __exception_info intrinsic.\n  llvm::Value *SEHInfo = nullptr;\n\n  /// Emits a landing pad for the current EH stack.\n  llvm::BasicBlock *EmitLandingPad();\n\n  llvm::BasicBlock *getInvokeDestImpl();\n\n  /// Parent loop-based directive for scan directive.\n  const OMPExecutableDirective *OMPParentLoopDirectiveForScan = nullptr;\n  llvm::BasicBlock *OMPBeforeScanBlock = nullptr;\n  llvm::BasicBlock *OMPAfterScanBlock = nullptr;\n  llvm::BasicBlock *OMPScanExitBlock = nullptr;\n  llvm::BasicBlock *OMPScanDispatch = nullptr;\n  bool OMPFirstScanLoop = false;\n\n  /// Manages parent directive for scan directives.\n  class ParentLoopDirectiveForScanRegion {\n    CodeGenFunction &CGF;\n    const OMPExecutableDirective *ParentLoopDirectiveForScan;\n\n  public:\n    ParentLoopDirectiveForScanRegion(\n        CodeGenFunction &CGF,\n        const OMPExecutableDirective &ParentLoopDirectiveForScan)\n        : CGF(CGF),\n          ParentLoopDirectiveForScan(CGF.OMPParentLoopDirectiveForScan) {\n      CGF.OMPParentLoopDirectiveForScan = &ParentLoopDirectiveForScan;\n    }\n    ~ParentLoopDirectiveForScanRegion() {\n      CGF.OMPParentLoopDirectiveForScan = ParentLoopDirectiveForScan;\n    }\n  };\n\n  template <class T>\n  typename DominatingValue<T>::saved_type saveValueInCond(T value) {\n    return DominatingValue<T>::save(*this, value);\n  }\n\n  class CGFPOptionsRAII {\n  public:\n    CGFPOptionsRAII(CodeGenFunction &CGF, FPOptions FPFeatures);\n    CGFPOptionsRAII(CodeGenFunction &CGF, const Expr *E);\n    ~CGFPOptionsRAII();\n\n  private:\n    void ConstructorHelper(FPOptions FPFeatures);\n    CodeGenFunction &CGF;\n    FPOptions OldFPFeatures;\n    llvm::fp::ExceptionBehavior OldExcept;\n    llvm::RoundingMode OldRounding;\n    Optional<CGBuilderTy::FastMathFlagGuard> FMFGuard;\n  };\n  FPOptions CurFPFeatures;\n\npublic:\n  /// ObjCEHValueStack - Stack of Objective-C exception values, used for\n  /// rethrows.\n  SmallVector<llvm::Value*, 8> ObjCEHValueStack;\n\n  /// A class controlling the emission of a finally block.\n  class FinallyInfo {\n    /// Where the catchall's edge through the cleanup should go.\n    JumpDest RethrowDest;\n\n    /// A function to call to enter the catch.\n    llvm::FunctionCallee BeginCatchFn;\n\n    /// An i1 variable indicating whether or not the @finally is\n    /// running for an exception.\n    llvm::AllocaInst *ForEHVar;\n\n    /// An i8* variable into which the exception pointer to rethrow\n    /// has been saved.\n    llvm::AllocaInst *SavedExnVar;\n\n  public:\n    void enter(CodeGenFunction &CGF, const Stmt *Finally,\n               llvm::FunctionCallee beginCatchFn,\n               llvm::FunctionCallee endCatchFn, llvm::FunctionCallee rethrowFn);\n    void exit(CodeGenFunction &CGF);\n  };\n\n  /// Returns true inside SEH __try blocks.\n  bool isSEHTryScope() const { return !SEHTryEpilogueStack.empty(); }\n\n  /// Returns true while emitting a cleanuppad.\n  bool isCleanupPadScope() const {\n    return CurrentFuncletPad && isa<llvm::CleanupPadInst>(CurrentFuncletPad);\n  }\n\n  /// pushFullExprCleanup - Push a cleanup to be run at the end of the\n  /// current full-expression.  Safe against the possibility that\n  /// we're currently inside a conditionally-evaluated expression.\n  template <class T, class... As>\n  void pushFullExprCleanup(CleanupKind kind, As... A) {\n    // If we're not in a conditional branch, or if none of the\n    // arguments requires saving, then use the unconditional cleanup.\n    if (!isInConditionalBranch())\n      return EHStack.pushCleanup<T>(kind, A...);\n\n    // Stash values in a tuple so we can guarantee the order of saves.\n    typedef std::tuple<typename DominatingValue<As>::saved_type...> SavedTuple;\n    SavedTuple Saved{saveValueInCond(A)...};\n\n    typedef EHScopeStack::ConditionalCleanup<T, As...> CleanupType;\n    EHStack.pushCleanupTuple<CleanupType>(kind, Saved);\n    initFullExprCleanup();\n  }\n\n  /// Queue a cleanup to be pushed after finishing the current full-expression,\n  /// potentially with an active flag.\n  template <class T, class... As>\n  void pushCleanupAfterFullExpr(CleanupKind Kind, As... A) {\n    if (!isInConditionalBranch())\n      return pushCleanupAfterFullExprWithActiveFlag<T>(Kind, Address::invalid(),\n                                                       A...);\n\n    Address ActiveFlag = createCleanupActiveFlag();\n    assert(!DominatingValue<Address>::needsSaving(ActiveFlag) &&\n           \"cleanup active flag should never need saving\");\n\n    typedef std::tuple<typename DominatingValue<As>::saved_type...> SavedTuple;\n    SavedTuple Saved{saveValueInCond(A)...};\n\n    typedef EHScopeStack::ConditionalCleanup<T, As...> CleanupType;\n    pushCleanupAfterFullExprWithActiveFlag<CleanupType>(Kind, ActiveFlag, Saved);\n  }\n\n  template <class T, class... As>\n  void pushCleanupAfterFullExprWithActiveFlag(CleanupKind Kind,\n                                              Address ActiveFlag, As... A) {\n    LifetimeExtendedCleanupHeader Header = {sizeof(T), Kind,\n                                            ActiveFlag.isValid()};\n\n    size_t OldSize = LifetimeExtendedCleanupStack.size();\n    LifetimeExtendedCleanupStack.resize(\n        LifetimeExtendedCleanupStack.size() + sizeof(Header) + Header.Size +\n        (Header.IsConditional ? sizeof(ActiveFlag) : 0));\n\n    static_assert(sizeof(Header) % alignof(T) == 0,\n                  \"Cleanup will be allocated on misaligned address\");\n    char *Buffer = &LifetimeExtendedCleanupStack[OldSize];\n    new (Buffer) LifetimeExtendedCleanupHeader(Header);\n    new (Buffer + sizeof(Header)) T(A...);\n    if (Header.IsConditional)\n      new (Buffer + sizeof(Header) + sizeof(T)) Address(ActiveFlag);\n  }\n\n  /// Set up the last cleanup that was pushed as a conditional\n  /// full-expression cleanup.\n  void initFullExprCleanup() {\n    initFullExprCleanupWithFlag(createCleanupActiveFlag());\n  }\n\n  void initFullExprCleanupWithFlag(Address ActiveFlag);\n  Address createCleanupActiveFlag();\n\n  /// PushDestructorCleanup - Push a cleanup to call the\n  /// complete-object destructor of an object of the given type at the\n  /// given address.  Does nothing if T is not a C++ class type with a\n  /// non-trivial destructor.\n  void PushDestructorCleanup(QualType T, Address Addr);\n\n  /// PushDestructorCleanup - Push a cleanup to call the\n  /// complete-object variant of the given destructor on the object at\n  /// the given address.\n  void PushDestructorCleanup(const CXXDestructorDecl *Dtor, QualType T,\n                             Address Addr);\n\n  /// PopCleanupBlock - Will pop the cleanup entry on the stack and\n  /// process all branch fixups.\n  void PopCleanupBlock(bool FallThroughIsBranchThrough = false);\n\n  /// DeactivateCleanupBlock - Deactivates the given cleanup block.\n  /// The block cannot be reactivated.  Pops it if it's the top of the\n  /// stack.\n  ///\n  /// \\param DominatingIP - An instruction which is known to\n  ///   dominate the current IP (if set) and which lies along\n  ///   all paths of execution between the current IP and the\n  ///   the point at which the cleanup comes into scope.\n  void DeactivateCleanupBlock(EHScopeStack::stable_iterator Cleanup,\n                              llvm::Instruction *DominatingIP);\n\n  /// ActivateCleanupBlock - Activates an initially-inactive cleanup.\n  /// Cannot be used to resurrect a deactivated cleanup.\n  ///\n  /// \\param DominatingIP - An instruction which is known to\n  ///   dominate the current IP (if set) and which lies along\n  ///   all paths of execution between the current IP and the\n  ///   the point at which the cleanup comes into scope.\n  void ActivateCleanupBlock(EHScopeStack::stable_iterator Cleanup,\n                            llvm::Instruction *DominatingIP);\n\n  /// Enters a new scope for capturing cleanups, all of which\n  /// will be executed once the scope is exited.\n  class RunCleanupsScope {\n    EHScopeStack::stable_iterator CleanupStackDepth, OldCleanupScopeDepth;\n    size_t LifetimeExtendedCleanupStackSize;\n    bool OldDidCallStackSave;\n  protected:\n    bool PerformCleanup;\n  private:\n\n    RunCleanupsScope(const RunCleanupsScope &) = delete;\n    void operator=(const RunCleanupsScope &) = delete;\n\n  protected:\n    CodeGenFunction& CGF;\n\n  public:\n    /// Enter a new cleanup scope.\n    explicit RunCleanupsScope(CodeGenFunction &CGF)\n      : PerformCleanup(true), CGF(CGF)\n    {\n      CleanupStackDepth = CGF.EHStack.stable_begin();\n      LifetimeExtendedCleanupStackSize =\n          CGF.LifetimeExtendedCleanupStack.size();\n      OldDidCallStackSave = CGF.DidCallStackSave;\n      CGF.DidCallStackSave = false;\n      OldCleanupScopeDepth = CGF.CurrentCleanupScopeDepth;\n      CGF.CurrentCleanupScopeDepth = CleanupStackDepth;\n    }\n\n    /// Exit this cleanup scope, emitting any accumulated cleanups.\n    ~RunCleanupsScope() {\n      if (PerformCleanup)\n        ForceCleanup();\n    }\n\n    /// Determine whether this scope requires any cleanups.\n    bool requiresCleanups() const {\n      return CGF.EHStack.stable_begin() != CleanupStackDepth;\n    }\n\n    /// Force the emission of cleanups now, instead of waiting\n    /// until this object is destroyed.\n    /// \\param ValuesToReload - A list of values that need to be available at\n    /// the insertion point after cleanup emission. If cleanup emission created\n    /// a shared cleanup block, these value pointers will be rewritten.\n    /// Otherwise, they not will be modified.\n    void ForceCleanup(std::initializer_list<llvm::Value**> ValuesToReload = {}) {\n      assert(PerformCleanup && \"Already forced cleanup\");\n      CGF.DidCallStackSave = OldDidCallStackSave;\n      CGF.PopCleanupBlocks(CleanupStackDepth, LifetimeExtendedCleanupStackSize,\n                           ValuesToReload);\n      PerformCleanup = false;\n      CGF.CurrentCleanupScopeDepth = OldCleanupScopeDepth;\n    }\n  };\n\n  // Cleanup stack depth of the RunCleanupsScope that was pushed most recently.\n  EHScopeStack::stable_iterator CurrentCleanupScopeDepth =\n      EHScopeStack::stable_end();\n\n  class LexicalScope : public RunCleanupsScope {\n    SourceRange Range;\n    SmallVector<const LabelDecl*, 4> Labels;\n    LexicalScope *ParentScope;\n\n    LexicalScope(const LexicalScope &) = delete;\n    void operator=(const LexicalScope &) = delete;\n\n  public:\n    /// Enter a new cleanup scope.\n    explicit LexicalScope(CodeGenFunction &CGF, SourceRange Range)\n      : RunCleanupsScope(CGF), Range(Range), ParentScope(CGF.CurLexicalScope) {\n      CGF.CurLexicalScope = this;\n      if (CGDebugInfo *DI = CGF.getDebugInfo())\n        DI->EmitLexicalBlockStart(CGF.Builder, Range.getBegin());\n    }\n\n    void addLabel(const LabelDecl *label) {\n      assert(PerformCleanup && \"adding label to dead scope?\");\n      Labels.push_back(label);\n    }\n\n    /// Exit this cleanup scope, emitting any accumulated\n    /// cleanups.\n    ~LexicalScope() {\n      if (CGDebugInfo *DI = CGF.getDebugInfo())\n        DI->EmitLexicalBlockEnd(CGF.Builder, Range.getEnd());\n\n      // If we should perform a cleanup, force them now.  Note that\n      // this ends the cleanup scope before rescoping any labels.\n      if (PerformCleanup) {\n        ApplyDebugLocation DL(CGF, Range.getEnd());\n        ForceCleanup();\n      }\n    }\n\n    /// Force the emission of cleanups now, instead of waiting\n    /// until this object is destroyed.\n    void ForceCleanup() {\n      CGF.CurLexicalScope = ParentScope;\n      RunCleanupsScope::ForceCleanup();\n\n      if (!Labels.empty())\n        rescopeLabels();\n    }\n\n    bool hasLabels() const {\n      return !Labels.empty();\n    }\n\n    void rescopeLabels();\n  };\n\n  typedef llvm::DenseMap<const Decl *, Address> DeclMapTy;\n\n  /// The class used to assign some variables some temporarily addresses.\n  class OMPMapVars {\n    DeclMapTy SavedLocals;\n    DeclMapTy SavedTempAddresses;\n    OMPMapVars(const OMPMapVars &) = delete;\n    void operator=(const OMPMapVars &) = delete;\n\n  public:\n    explicit OMPMapVars() = default;\n    ~OMPMapVars() {\n      assert(SavedLocals.empty() && \"Did not restored original addresses.\");\n    };\n\n    /// Sets the address of the variable \\p LocalVD to be \\p TempAddr in\n    /// function \\p CGF.\n    /// \\return true if at least one variable was set already, false otherwise.\n    bool setVarAddr(CodeGenFunction &CGF, const VarDecl *LocalVD,\n                    Address TempAddr) {\n      LocalVD = LocalVD->getCanonicalDecl();\n      // Only save it once.\n      if (SavedLocals.count(LocalVD)) return false;\n\n      // Copy the existing local entry to SavedLocals.\n      auto it = CGF.LocalDeclMap.find(LocalVD);\n      if (it != CGF.LocalDeclMap.end())\n        SavedLocals.try_emplace(LocalVD, it->second);\n      else\n        SavedLocals.try_emplace(LocalVD, Address::invalid());\n\n      // Generate the private entry.\n      QualType VarTy = LocalVD->getType();\n      if (VarTy->isReferenceType()) {\n        Address Temp = CGF.CreateMemTemp(VarTy);\n        CGF.Builder.CreateStore(TempAddr.getPointer(), Temp);\n        TempAddr = Temp;\n      }\n      SavedTempAddresses.try_emplace(LocalVD, TempAddr);\n\n      return true;\n    }\n\n    /// Applies new addresses to the list of the variables.\n    /// \\return true if at least one variable is using new address, false\n    /// otherwise.\n    bool apply(CodeGenFunction &CGF) {\n      copyInto(SavedTempAddresses, CGF.LocalDeclMap);\n      SavedTempAddresses.clear();\n      return !SavedLocals.empty();\n    }\n\n    /// Restores original addresses of the variables.\n    void restore(CodeGenFunction &CGF) {\n      if (!SavedLocals.empty()) {\n        copyInto(SavedLocals, CGF.LocalDeclMap);\n        SavedLocals.clear();\n      }\n    }\n\n  private:\n    /// Copy all the entries in the source map over the corresponding\n    /// entries in the destination, which must exist.\n    static void copyInto(const DeclMapTy &Src, DeclMapTy &Dest) {\n      for (auto &Pair : Src) {\n        if (!Pair.second.isValid()) {\n          Dest.erase(Pair.first);\n          continue;\n        }\n\n        auto I = Dest.find(Pair.first);\n        if (I != Dest.end())\n          I->second = Pair.second;\n        else\n          Dest.insert(Pair);\n      }\n    }\n  };\n\n  /// The scope used to remap some variables as private in the OpenMP loop body\n  /// (or other captured region emitted without outlining), and to restore old\n  /// vars back on exit.\n  class OMPPrivateScope : public RunCleanupsScope {\n    OMPMapVars MappedVars;\n    OMPPrivateScope(const OMPPrivateScope &) = delete;\n    void operator=(const OMPPrivateScope &) = delete;\n\n  public:\n    /// Enter a new OpenMP private scope.\n    explicit OMPPrivateScope(CodeGenFunction &CGF) : RunCleanupsScope(CGF) {}\n\n    /// Registers \\p LocalVD variable as a private and apply \\p PrivateGen\n    /// function for it to generate corresponding private variable. \\p\n    /// PrivateGen returns an address of the generated private variable.\n    /// \\return true if the variable is registered as private, false if it has\n    /// been privatized already.\n    bool addPrivate(const VarDecl *LocalVD,\n                    const llvm::function_ref<Address()> PrivateGen) {\n      assert(PerformCleanup && \"adding private to dead scope\");\n      return MappedVars.setVarAddr(CGF, LocalVD, PrivateGen());\n    }\n\n    /// Privatizes local variables previously registered as private.\n    /// Registration is separate from the actual privatization to allow\n    /// initializers use values of the original variables, not the private one.\n    /// This is important, for example, if the private variable is a class\n    /// variable initialized by a constructor that references other private\n    /// variables. But at initialization original variables must be used, not\n    /// private copies.\n    /// \\return true if at least one variable was privatized, false otherwise.\n    bool Privatize() { return MappedVars.apply(CGF); }\n\n    void ForceCleanup() {\n      RunCleanupsScope::ForceCleanup();\n      MappedVars.restore(CGF);\n    }\n\n    /// Exit scope - all the mapped variables are restored.\n    ~OMPPrivateScope() {\n      if (PerformCleanup)\n        ForceCleanup();\n    }\n\n    /// Checks if the global variable is captured in current function.\n    bool isGlobalVarCaptured(const VarDecl *VD) const {\n      VD = VD->getCanonicalDecl();\n      return !VD->isLocalVarDeclOrParm() && CGF.LocalDeclMap.count(VD) > 0;\n    }\n  };\n\n  /// Save/restore original map of previously emitted local vars in case when we\n  /// need to duplicate emission of the same code several times in the same\n  /// function for OpenMP code.\n  class OMPLocalDeclMapRAII {\n    CodeGenFunction &CGF;\n    DeclMapTy SavedMap;\n\n  public:\n    OMPLocalDeclMapRAII(CodeGenFunction &CGF)\n        : CGF(CGF), SavedMap(CGF.LocalDeclMap) {}\n    ~OMPLocalDeclMapRAII() { SavedMap.swap(CGF.LocalDeclMap); }\n  };\n\n  /// Takes the old cleanup stack size and emits the cleanup blocks\n  /// that have been added.\n  void\n  PopCleanupBlocks(EHScopeStack::stable_iterator OldCleanupStackSize,\n                   std::initializer_list<llvm::Value **> ValuesToReload = {});\n\n  /// Takes the old cleanup stack size and emits the cleanup blocks\n  /// that have been added, then adds all lifetime-extended cleanups from\n  /// the given position to the stack.\n  void\n  PopCleanupBlocks(EHScopeStack::stable_iterator OldCleanupStackSize,\n                   size_t OldLifetimeExtendedStackSize,\n                   std::initializer_list<llvm::Value **> ValuesToReload = {});\n\n  void ResolveBranchFixups(llvm::BasicBlock *Target);\n\n  /// The given basic block lies in the current EH scope, but may be a\n  /// target of a potentially scope-crossing jump; get a stable handle\n  /// to which we can perform this jump later.\n  JumpDest getJumpDestInCurrentScope(llvm::BasicBlock *Target) {\n    return JumpDest(Target,\n                    EHStack.getInnermostNormalCleanup(),\n                    NextCleanupDestIndex++);\n  }\n\n  /// The given basic block lies in the current EH scope, but may be a\n  /// target of a potentially scope-crossing jump; get a stable handle\n  /// to which we can perform this jump later.\n  JumpDest getJumpDestInCurrentScope(StringRef Name = StringRef()) {\n    return getJumpDestInCurrentScope(createBasicBlock(Name));\n  }\n\n  /// EmitBranchThroughCleanup - Emit a branch from the current insert\n  /// block through the normal cleanup handling code (if any) and then\n  /// on to \\arg Dest.\n  void EmitBranchThroughCleanup(JumpDest Dest);\n\n  /// isObviouslyBranchWithoutCleanups - Return true if a branch to the\n  /// specified destination obviously has no cleanups to run.  'false' is always\n  /// a conservatively correct answer for this method.\n  bool isObviouslyBranchWithoutCleanups(JumpDest Dest) const;\n\n  /// popCatchScope - Pops the catch scope at the top of the EHScope\n  /// stack, emitting any required code (other than the catch handlers\n  /// themselves).\n  void popCatchScope();\n\n  llvm::BasicBlock *getEHResumeBlock(bool isCleanup);\n  llvm::BasicBlock *getEHDispatchBlock(EHScopeStack::stable_iterator scope);\n  llvm::BasicBlock *\n  getFuncletEHDispatchBlock(EHScopeStack::stable_iterator scope);\n\n  /// An object to manage conditionally-evaluated expressions.\n  class ConditionalEvaluation {\n    llvm::BasicBlock *StartBB;\n\n  public:\n    ConditionalEvaluation(CodeGenFunction &CGF)\n      : StartBB(CGF.Builder.GetInsertBlock()) {}\n\n    void begin(CodeGenFunction &CGF) {\n      assert(CGF.OutermostConditional != this);\n      if (!CGF.OutermostConditional)\n        CGF.OutermostConditional = this;\n    }\n\n    void end(CodeGenFunction &CGF) {\n      assert(CGF.OutermostConditional != nullptr);\n      if (CGF.OutermostConditional == this)\n        CGF.OutermostConditional = nullptr;\n    }\n\n    /// Returns a block which will be executed prior to each\n    /// evaluation of the conditional code.\n    llvm::BasicBlock *getStartingBlock() const {\n      return StartBB;\n    }\n  };\n\n  /// isInConditionalBranch - Return true if we're currently emitting\n  /// one branch or the other of a conditional expression.\n  bool isInConditionalBranch() const { return OutermostConditional != nullptr; }\n\n  void setBeforeOutermostConditional(llvm::Value *value, Address addr) {\n    assert(isInConditionalBranch());\n    llvm::BasicBlock *block = OutermostConditional->getStartingBlock();\n    auto store = new llvm::StoreInst(value, addr.getPointer(), &block->back());\n    store->setAlignment(addr.getAlignment().getAsAlign());\n  }\n\n  /// An RAII object to record that we're evaluating a statement\n  /// expression.\n  class StmtExprEvaluation {\n    CodeGenFunction &CGF;\n\n    /// We have to save the outermost conditional: cleanups in a\n    /// statement expression aren't conditional just because the\n    /// StmtExpr is.\n    ConditionalEvaluation *SavedOutermostConditional;\n\n  public:\n    StmtExprEvaluation(CodeGenFunction &CGF)\n      : CGF(CGF), SavedOutermostConditional(CGF.OutermostConditional) {\n      CGF.OutermostConditional = nullptr;\n    }\n\n    ~StmtExprEvaluation() {\n      CGF.OutermostConditional = SavedOutermostConditional;\n      CGF.EnsureInsertPoint();\n    }\n  };\n\n  /// An object which temporarily prevents a value from being\n  /// destroyed by aggressive peephole optimizations that assume that\n  /// all uses of a value have been realized in the IR.\n  class PeepholeProtection {\n    llvm::Instruction *Inst;\n    friend class CodeGenFunction;\n\n  public:\n    PeepholeProtection() : Inst(nullptr) {}\n  };\n\n  /// A non-RAII class containing all the information about a bound\n  /// opaque value.  OpaqueValueMapping, below, is a RAII wrapper for\n  /// this which makes individual mappings very simple; using this\n  /// class directly is useful when you have a variable number of\n  /// opaque values or don't want the RAII functionality for some\n  /// reason.\n  class OpaqueValueMappingData {\n    const OpaqueValueExpr *OpaqueValue;\n    bool BoundLValue;\n    CodeGenFunction::PeepholeProtection Protection;\n\n    OpaqueValueMappingData(const OpaqueValueExpr *ov,\n                           bool boundLValue)\n      : OpaqueValue(ov), BoundLValue(boundLValue) {}\n  public:\n    OpaqueValueMappingData() : OpaqueValue(nullptr) {}\n\n    static bool shouldBindAsLValue(const Expr *expr) {\n      // gl-values should be bound as l-values for obvious reasons.\n      // Records should be bound as l-values because IR generation\n      // always keeps them in memory.  Expressions of function type\n      // act exactly like l-values but are formally required to be\n      // r-values in C.\n      return expr->isGLValue() ||\n             expr->getType()->isFunctionType() ||\n             hasAggregateEvaluationKind(expr->getType());\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const Expr *e) {\n      if (shouldBindAsLValue(ov))\n        return bind(CGF, ov, CGF.EmitLValue(e));\n      return bind(CGF, ov, CGF.EmitAnyExpr(e));\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const LValue &lv) {\n      assert(shouldBindAsLValue(ov));\n      CGF.OpaqueLValues.insert(std::make_pair(ov, lv));\n      return OpaqueValueMappingData(ov, true);\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const RValue &rv) {\n      assert(!shouldBindAsLValue(ov));\n      CGF.OpaqueRValues.insert(std::make_pair(ov, rv));\n\n      OpaqueValueMappingData data(ov, false);\n\n      // Work around an extremely aggressive peephole optimization in\n      // EmitScalarConversion which assumes that all other uses of a\n      // value are extant.\n      data.Protection = CGF.protectFromPeepholes(rv);\n\n      return data;\n    }\n\n    bool isValid() const { return OpaqueValue != nullptr; }\n    void clear() { OpaqueValue = nullptr; }\n\n    void unbind(CodeGenFunction &CGF) {\n      assert(OpaqueValue && \"no data to unbind!\");\n\n      if (BoundLValue) {\n        CGF.OpaqueLValues.erase(OpaqueValue);\n      } else {\n        CGF.OpaqueRValues.erase(OpaqueValue);\n        CGF.unprotectFromPeepholes(Protection);\n      }\n    }\n  };\n\n  /// An RAII object to set (and then clear) a mapping for an OpaqueValueExpr.\n  class OpaqueValueMapping {\n    CodeGenFunction &CGF;\n    OpaqueValueMappingData Data;\n\n  public:\n    static bool shouldBindAsLValue(const Expr *expr) {\n      return OpaqueValueMappingData::shouldBindAsLValue(expr);\n    }\n\n    /// Build the opaque value mapping for the given conditional\n    /// operator if it's the GNU ?: extension.  This is a common\n    /// enough pattern that the convenience operator is really\n    /// helpful.\n    ///\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const AbstractConditionalOperator *op) : CGF(CGF) {\n      if (isa<ConditionalOperator>(op))\n        // Leave Data empty.\n        return;\n\n      const BinaryConditionalOperator *e = cast<BinaryConditionalOperator>(op);\n      Data = OpaqueValueMappingData::bind(CGF, e->getOpaqueValue(),\n                                          e->getCommon());\n    }\n\n    /// Build the opaque value mapping for an OpaqueValueExpr whose source\n    /// expression is set to the expression the OVE represents.\n    OpaqueValueMapping(CodeGenFunction &CGF, const OpaqueValueExpr *OV)\n        : CGF(CGF) {\n      if (OV) {\n        assert(OV->getSourceExpr() && \"wrong form of OpaqueValueMapping used \"\n                                      \"for OVE with no source expression\");\n        Data = OpaqueValueMappingData::bind(CGF, OV, OV->getSourceExpr());\n      }\n    }\n\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const OpaqueValueExpr *opaqueValue,\n                       LValue lvalue)\n      : CGF(CGF), Data(OpaqueValueMappingData::bind(CGF, opaqueValue, lvalue)) {\n    }\n\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const OpaqueValueExpr *opaqueValue,\n                       RValue rvalue)\n      : CGF(CGF), Data(OpaqueValueMappingData::bind(CGF, opaqueValue, rvalue)) {\n    }\n\n    void pop() {\n      Data.unbind(CGF);\n      Data.clear();\n    }\n\n    ~OpaqueValueMapping() {\n      if (Data.isValid()) Data.unbind(CGF);\n    }\n  };\n\nprivate:\n  CGDebugInfo *DebugInfo;\n  /// Used to create unique names for artificial VLA size debug info variables.\n  unsigned VLAExprCounter = 0;\n  bool DisableDebugInfo = false;\n\n  /// DidCallStackSave - Whether llvm.stacksave has been called. Used to avoid\n  /// calling llvm.stacksave for multiple VLAs in the same scope.\n  bool DidCallStackSave = false;\n\n  /// IndirectBranch - The first time an indirect goto is seen we create a block\n  /// with an indirect branch.  Every time we see the address of a label taken,\n  /// we add the label to the indirect goto.  Every subsequent indirect goto is\n  /// codegen'd as a jump to the IndirectBranch's basic block.\n  llvm::IndirectBrInst *IndirectBranch = nullptr;\n\n  /// LocalDeclMap - This keeps track of the LLVM allocas or globals for local C\n  /// decls.\n  DeclMapTy LocalDeclMap;\n\n  // Keep track of the cleanups for callee-destructed parameters pushed to the\n  // cleanup stack so that they can be deactivated later.\n  llvm::DenseMap<const ParmVarDecl *, EHScopeStack::stable_iterator>\n      CalleeDestructedParamCleanups;\n\n  /// SizeArguments - If a ParmVarDecl had the pass_object_size attribute, this\n  /// will contain a mapping from said ParmVarDecl to its implicit \"object_size\"\n  /// parameter.\n  llvm::SmallDenseMap<const ParmVarDecl *, const ImplicitParamDecl *, 2>\n      SizeArguments;\n\n  /// Track escaped local variables with auto storage. Used during SEH\n  /// outlining to produce a call to llvm.localescape.\n  llvm::DenseMap<llvm::AllocaInst *, int> EscapedLocals;\n\n  /// LabelMap - This keeps track of the LLVM basic block for each C label.\n  llvm::DenseMap<const LabelDecl*, JumpDest> LabelMap;\n\n  // BreakContinueStack - This keeps track of where break and continue\n  // statements should jump to.\n  struct BreakContinue {\n    BreakContinue(JumpDest Break, JumpDest Continue)\n      : BreakBlock(Break), ContinueBlock(Continue) {}\n\n    JumpDest BreakBlock;\n    JumpDest ContinueBlock;\n  };\n  SmallVector<BreakContinue, 8> BreakContinueStack;\n\n  /// Handles cancellation exit points in OpenMP-related constructs.\n  class OpenMPCancelExitStack {\n    /// Tracks cancellation exit point and join point for cancel-related exit\n    /// and normal exit.\n    struct CancelExit {\n      CancelExit() = default;\n      CancelExit(OpenMPDirectiveKind Kind, JumpDest ExitBlock,\n                 JumpDest ContBlock)\n          : Kind(Kind), ExitBlock(ExitBlock), ContBlock(ContBlock) {}\n      OpenMPDirectiveKind Kind = llvm::omp::OMPD_unknown;\n      /// true if the exit block has been emitted already by the special\n      /// emitExit() call, false if the default codegen is used.\n      bool HasBeenEmitted = false;\n      JumpDest ExitBlock;\n      JumpDest ContBlock;\n    };\n\n    SmallVector<CancelExit, 8> Stack;\n\n  public:\n    OpenMPCancelExitStack() : Stack(1) {}\n    ~OpenMPCancelExitStack() = default;\n    /// Fetches the exit block for the current OpenMP construct.\n    JumpDest getExitBlock() const { return Stack.back().ExitBlock; }\n    /// Emits exit block with special codegen procedure specific for the related\n    /// OpenMP construct + emits code for normal construct cleanup.\n    void emitExit(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,\n                  const llvm::function_ref<void(CodeGenFunction &)> CodeGen) {\n      if (Stack.back().Kind == Kind && getExitBlock().isValid()) {\n        assert(CGF.getOMPCancelDestination(Kind).isValid());\n        assert(CGF.HaveInsertPoint());\n        assert(!Stack.back().HasBeenEmitted);\n        auto IP = CGF.Builder.saveAndClearIP();\n        CGF.EmitBlock(Stack.back().ExitBlock.getBlock());\n        CodeGen(CGF);\n        CGF.EmitBranch(Stack.back().ContBlock.getBlock());\n        CGF.Builder.restoreIP(IP);\n        Stack.back().HasBeenEmitted = true;\n      }\n      CodeGen(CGF);\n    }\n    /// Enter the cancel supporting \\a Kind construct.\n    /// \\param Kind OpenMP directive that supports cancel constructs.\n    /// \\param HasCancel true, if the construct has inner cancel directive,\n    /// false otherwise.\n    void enter(CodeGenFunction &CGF, OpenMPDirectiveKind Kind, bool HasCancel) {\n      Stack.push_back({Kind,\n                       HasCancel ? CGF.getJumpDestInCurrentScope(\"cancel.exit\")\n                                 : JumpDest(),\n                       HasCancel ? CGF.getJumpDestInCurrentScope(\"cancel.cont\")\n                                 : JumpDest()});\n    }\n    /// Emits default exit point for the cancel construct (if the special one\n    /// has not be used) + join point for cancel/normal exits.\n    void exit(CodeGenFunction &CGF) {\n      if (getExitBlock().isValid()) {\n        assert(CGF.getOMPCancelDestination(Stack.back().Kind).isValid());\n        bool HaveIP = CGF.HaveInsertPoint();\n        if (!Stack.back().HasBeenEmitted) {\n          if (HaveIP)\n            CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);\n          CGF.EmitBlock(Stack.back().ExitBlock.getBlock());\n          CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);\n        }\n        CGF.EmitBlock(Stack.back().ContBlock.getBlock());\n        if (!HaveIP) {\n          CGF.Builder.CreateUnreachable();\n          CGF.Builder.ClearInsertionPoint();\n        }\n      }\n      Stack.pop_back();\n    }\n  };\n  OpenMPCancelExitStack OMPCancelStack;\n\n  /// Calculate branch weights for the likelihood attribute\n  llvm::MDNode *createBranchWeights(Stmt::Likelihood LH) const;\n\n  CodeGenPGO PGO;\n\n  /// Calculate branch weights appropriate for PGO data\n  llvm::MDNode *createProfileWeights(uint64_t TrueCount,\n                                     uint64_t FalseCount) const;\n  llvm::MDNode *createProfileWeights(ArrayRef<uint64_t> Weights) const;\n  llvm::MDNode *createProfileWeightsForLoop(const Stmt *Cond,\n                                            uint64_t LoopCount) const;\n\n  /// Calculate the branch weight for PGO data or the likelihood attribute.\n  /// The function tries to get the weight of \\ref createProfileWeightsForLoop.\n  /// If that fails it gets the weight of \\ref createBranchWeights.\n  llvm::MDNode *createProfileOrBranchWeightsForLoop(const Stmt *Cond,\n                                                    uint64_t LoopCount,\n                                                    const Stmt *Body) const;\n\npublic:\n  /// Increment the profiler's counter for the given statement by \\p StepV.\n  /// If \\p StepV is null, the default increment is 1.\n  void incrementProfileCounter(const Stmt *S, llvm::Value *StepV = nullptr) {\n    if (CGM.getCodeGenOpts().hasProfileClangInstr() &&\n        !CurFn->hasFnAttribute(llvm::Attribute::NoProfile))\n      PGO.emitCounterIncrement(Builder, S, StepV);\n    PGO.setCurrentStmt(S);\n  }\n\n  /// Get the profiler's count for the given statement.\n  uint64_t getProfileCount(const Stmt *S) {\n    Optional<uint64_t> Count = PGO.getStmtCount(S);\n    if (!Count.hasValue())\n      return 0;\n    return *Count;\n  }\n\n  /// Set the profiler's current count.\n  void setCurrentProfileCount(uint64_t Count) {\n    PGO.setCurrentRegionCount(Count);\n  }\n\n  /// Get the profiler's current count. This is generally the count for the most\n  /// recently incremented counter.\n  uint64_t getCurrentProfileCount() {\n    return PGO.getCurrentRegionCount();\n  }\n\nprivate:\n\n  /// SwitchInsn - This is nearest current switch instruction. It is null if\n  /// current context is not in a switch.\n  llvm::SwitchInst *SwitchInsn = nullptr;\n  /// The branch weights of SwitchInsn when doing instrumentation based PGO.\n  SmallVector<uint64_t, 16> *SwitchWeights = nullptr;\n\n  /// The likelihood attributes of the SwitchCase.\n  SmallVector<Stmt::Likelihood, 16> *SwitchLikelihood = nullptr;\n\n  /// CaseRangeBlock - This block holds if condition check for last case\n  /// statement range in current switch instruction.\n  llvm::BasicBlock *CaseRangeBlock = nullptr;\n\n  /// OpaqueLValues - Keeps track of the current set of opaque value\n  /// expressions.\n  llvm::DenseMap<const OpaqueValueExpr *, LValue> OpaqueLValues;\n  llvm::DenseMap<const OpaqueValueExpr *, RValue> OpaqueRValues;\n\n  // VLASizeMap - This keeps track of the associated size for each VLA type.\n  // We track this by the size expression rather than the type itself because\n  // in certain situations, like a const qualifier applied to an VLA typedef,\n  // multiple VLA types can share the same size expression.\n  // FIXME: Maybe this could be a stack of maps that is pushed/popped as we\n  // enter/leave scopes.\n  llvm::DenseMap<const Expr*, llvm::Value*> VLASizeMap;\n\n  /// A block containing a single 'unreachable' instruction.  Created\n  /// lazily by getUnreachableBlock().\n  llvm::BasicBlock *UnreachableBlock = nullptr;\n\n  /// Counts of the number return expressions in the function.\n  unsigned NumReturnExprs = 0;\n\n  /// Count the number of simple (constant) return expressions in the function.\n  unsigned NumSimpleReturnExprs = 0;\n\n  /// The last regular (non-return) debug location (breakpoint) in the function.\n  SourceLocation LastStopPoint;\n\npublic:\n  /// Source location information about the default argument or member\n  /// initializer expression we're evaluating, if any.\n  CurrentSourceLocExprScope CurSourceLocExprScope;\n  using SourceLocExprScopeGuard =\n      CurrentSourceLocExprScope::SourceLocExprScopeGuard;\n\n  /// A scope within which we are constructing the fields of an object which\n  /// might use a CXXDefaultInitExpr. This stashes away a 'this' value to use\n  /// if we need to evaluate a CXXDefaultInitExpr within the evaluation.\n  class FieldConstructionScope {\n  public:\n    FieldConstructionScope(CodeGenFunction &CGF, Address This)\n        : CGF(CGF), OldCXXDefaultInitExprThis(CGF.CXXDefaultInitExprThis) {\n      CGF.CXXDefaultInitExprThis = This;\n    }\n    ~FieldConstructionScope() {\n      CGF.CXXDefaultInitExprThis = OldCXXDefaultInitExprThis;\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    Address OldCXXDefaultInitExprThis;\n  };\n\n  /// The scope of a CXXDefaultInitExpr. Within this scope, the value of 'this'\n  /// is overridden to be the object under construction.\n  class CXXDefaultInitExprScope  {\n  public:\n    CXXDefaultInitExprScope(CodeGenFunction &CGF, const CXXDefaultInitExpr *E)\n        : CGF(CGF), OldCXXThisValue(CGF.CXXThisValue),\n          OldCXXThisAlignment(CGF.CXXThisAlignment),\n          SourceLocScope(E, CGF.CurSourceLocExprScope) {\n      CGF.CXXThisValue = CGF.CXXDefaultInitExprThis.getPointer();\n      CGF.CXXThisAlignment = CGF.CXXDefaultInitExprThis.getAlignment();\n    }\n    ~CXXDefaultInitExprScope() {\n      CGF.CXXThisValue = OldCXXThisValue;\n      CGF.CXXThisAlignment = OldCXXThisAlignment;\n    }\n\n  public:\n    CodeGenFunction &CGF;\n    llvm::Value *OldCXXThisValue;\n    CharUnits OldCXXThisAlignment;\n    SourceLocExprScopeGuard SourceLocScope;\n  };\n\n  struct CXXDefaultArgExprScope : SourceLocExprScopeGuard {\n    CXXDefaultArgExprScope(CodeGenFunction &CGF, const CXXDefaultArgExpr *E)\n        : SourceLocExprScopeGuard(E, CGF.CurSourceLocExprScope) {}\n  };\n\n  /// The scope of an ArrayInitLoopExpr. Within this scope, the value of the\n  /// current loop index is overridden.\n  class ArrayInitLoopExprScope {\n  public:\n    ArrayInitLoopExprScope(CodeGenFunction &CGF, llvm::Value *Index)\n      : CGF(CGF), OldArrayInitIndex(CGF.ArrayInitIndex) {\n      CGF.ArrayInitIndex = Index;\n    }\n    ~ArrayInitLoopExprScope() {\n      CGF.ArrayInitIndex = OldArrayInitIndex;\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    llvm::Value *OldArrayInitIndex;\n  };\n\n  class InlinedInheritingConstructorScope {\n  public:\n    InlinedInheritingConstructorScope(CodeGenFunction &CGF, GlobalDecl GD)\n        : CGF(CGF), OldCurGD(CGF.CurGD), OldCurFuncDecl(CGF.CurFuncDecl),\n          OldCurCodeDecl(CGF.CurCodeDecl),\n          OldCXXABIThisDecl(CGF.CXXABIThisDecl),\n          OldCXXABIThisValue(CGF.CXXABIThisValue),\n          OldCXXThisValue(CGF.CXXThisValue),\n          OldCXXABIThisAlignment(CGF.CXXABIThisAlignment),\n          OldCXXThisAlignment(CGF.CXXThisAlignment),\n          OldReturnValue(CGF.ReturnValue), OldFnRetTy(CGF.FnRetTy),\n          OldCXXInheritedCtorInitExprArgs(\n              std::move(CGF.CXXInheritedCtorInitExprArgs)) {\n      CGF.CurGD = GD;\n      CGF.CurFuncDecl = CGF.CurCodeDecl =\n          cast<CXXConstructorDecl>(GD.getDecl());\n      CGF.CXXABIThisDecl = nullptr;\n      CGF.CXXABIThisValue = nullptr;\n      CGF.CXXThisValue = nullptr;\n      CGF.CXXABIThisAlignment = CharUnits();\n      CGF.CXXThisAlignment = CharUnits();\n      CGF.ReturnValue = Address::invalid();\n      CGF.FnRetTy = QualType();\n      CGF.CXXInheritedCtorInitExprArgs.clear();\n    }\n    ~InlinedInheritingConstructorScope() {\n      CGF.CurGD = OldCurGD;\n      CGF.CurFuncDecl = OldCurFuncDecl;\n      CGF.CurCodeDecl = OldCurCodeDecl;\n      CGF.CXXABIThisDecl = OldCXXABIThisDecl;\n      CGF.CXXABIThisValue = OldCXXABIThisValue;\n      CGF.CXXThisValue = OldCXXThisValue;\n      CGF.CXXABIThisAlignment = OldCXXABIThisAlignment;\n      CGF.CXXThisAlignment = OldCXXThisAlignment;\n      CGF.ReturnValue = OldReturnValue;\n      CGF.FnRetTy = OldFnRetTy;\n      CGF.CXXInheritedCtorInitExprArgs =\n          std::move(OldCXXInheritedCtorInitExprArgs);\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    GlobalDecl OldCurGD;\n    const Decl *OldCurFuncDecl;\n    const Decl *OldCurCodeDecl;\n    ImplicitParamDecl *OldCXXABIThisDecl;\n    llvm::Value *OldCXXABIThisValue;\n    llvm::Value *OldCXXThisValue;\n    CharUnits OldCXXABIThisAlignment;\n    CharUnits OldCXXThisAlignment;\n    Address OldReturnValue;\n    QualType OldFnRetTy;\n    CallArgList OldCXXInheritedCtorInitExprArgs;\n  };\n\n  // Helper class for the OpenMP IR Builder. Allows reusability of code used for\n  // region body, and finalization codegen callbacks. This will class will also\n  // contain privatization functions used by the privatization call backs\n  //\n  // TODO: this is temporary class for things that are being moved out of\n  // CGOpenMPRuntime, new versions of current CodeGenFunction methods, or\n  // utility function for use with the OMPBuilder. Once that move to use the\n  // OMPBuilder is done, everything here will either become part of CodeGenFunc.\n  // directly, or a new helper class that will contain functions used by both\n  // this and the OMPBuilder\n\n  struct OMPBuilderCBHelpers {\n\n    OMPBuilderCBHelpers() = delete;\n    OMPBuilderCBHelpers(const OMPBuilderCBHelpers &) = delete;\n    OMPBuilderCBHelpers &operator=(const OMPBuilderCBHelpers &) = delete;\n\n    using InsertPointTy = llvm::OpenMPIRBuilder::InsertPointTy;\n\n    /// Cleanup action for allocate support.\n    class OMPAllocateCleanupTy final : public EHScopeStack::Cleanup {\n\n    private:\n      llvm::CallInst *RTLFnCI;\n\n    public:\n      OMPAllocateCleanupTy(llvm::CallInst *RLFnCI) : RTLFnCI(RLFnCI) {\n        RLFnCI->removeFromParent();\n      }\n\n      void Emit(CodeGenFunction &CGF, Flags /*flags*/) override {\n        if (!CGF.HaveInsertPoint())\n          return;\n        CGF.Builder.Insert(RTLFnCI);\n      }\n    };\n\n    /// Returns address of the threadprivate variable for the current\n    /// thread. This Also create any necessary OMP runtime calls.\n    ///\n    /// \\param VD VarDecl for Threadprivate variable.\n    /// \\param VDAddr Address of the Vardecl\n    /// \\param Loc  The location where the barrier directive was encountered\n    static Address getAddrOfThreadPrivate(CodeGenFunction &CGF,\n                                          const VarDecl *VD, Address VDAddr,\n                                          SourceLocation Loc);\n\n    /// Gets the OpenMP-specific address of the local variable /p VD.\n    static Address getAddressOfLocalVariable(CodeGenFunction &CGF,\n                                             const VarDecl *VD);\n    /// Get the platform-specific name separator.\n    /// \\param Parts different parts of the final name that needs separation\n    /// \\param FirstSeparator First separator used between the initial two\n    ///        parts of the name.\n    /// \\param Separator separator used between all of the rest consecutinve\n    ///        parts of the name\n    static std::string getNameWithSeparators(ArrayRef<StringRef> Parts,\n                                             StringRef FirstSeparator = \".\",\n                                             StringRef Separator = \".\");\n    /// Emit the Finalization for an OMP region\n    /// \\param CGF\tThe Codegen function this belongs to\n    /// \\param IP\tInsertion point for generating the finalization code.\n    static void FinalizeOMPRegion(CodeGenFunction &CGF, InsertPointTy IP) {\n      CGBuilderTy::InsertPointGuard IPG(CGF.Builder);\n      assert(IP.getBlock()->end() != IP.getPoint() &&\n             \"OpenMP IR Builder should cause terminated block!\");\n\n      llvm::BasicBlock *IPBB = IP.getBlock();\n      llvm::BasicBlock *DestBB = IPBB->getUniqueSuccessor();\n      assert(DestBB && \"Finalization block should have one successor!\");\n\n      // erase and replace with cleanup branch.\n      IPBB->getTerminator()->eraseFromParent();\n      CGF.Builder.SetInsertPoint(IPBB);\n      CodeGenFunction::JumpDest Dest = CGF.getJumpDestInCurrentScope(DestBB);\n      CGF.EmitBranchThroughCleanup(Dest);\n    }\n\n    /// Emit the body of an OMP region\n    /// \\param CGF\tThe Codegen function this belongs to\n    /// \\param RegionBodyStmt\tThe body statement for the OpenMP region being\n    /// \t\t\t generated\n    /// \\param CodeGenIP\tInsertion point for generating the body code.\n    /// \\param FiniBB\tThe finalization basic block\n    static void EmitOMPRegionBody(CodeGenFunction &CGF,\n                                  const Stmt *RegionBodyStmt,\n                                  InsertPointTy CodeGenIP,\n                                  llvm::BasicBlock &FiniBB) {\n      llvm::BasicBlock *CodeGenIPBB = CodeGenIP.getBlock();\n      if (llvm::Instruction *CodeGenIPBBTI = CodeGenIPBB->getTerminator())\n        CodeGenIPBBTI->eraseFromParent();\n\n      CGF.Builder.SetInsertPoint(CodeGenIPBB);\n\n      CGF.EmitStmt(RegionBodyStmt);\n\n      if (CGF.Builder.saveIP().isSet())\n        CGF.Builder.CreateBr(&FiniBB);\n    }\n\n    /// RAII for preserving necessary info during Outlined region body codegen.\n    class OutlinedRegionBodyRAII {\n\n      llvm::AssertingVH<llvm::Instruction> OldAllocaIP;\n      CodeGenFunction::JumpDest OldReturnBlock;\n      CGBuilderTy::InsertPoint IP;\n      CodeGenFunction &CGF;\n\n    public:\n      OutlinedRegionBodyRAII(CodeGenFunction &cgf, InsertPointTy &AllocaIP,\n                             llvm::BasicBlock &RetBB)\n          : CGF(cgf) {\n        assert(AllocaIP.isSet() &&\n               \"Must specify Insertion point for allocas of outlined function\");\n        OldAllocaIP = CGF.AllocaInsertPt;\n        CGF.AllocaInsertPt = &*AllocaIP.getPoint();\n        IP = CGF.Builder.saveIP();\n\n        OldReturnBlock = CGF.ReturnBlock;\n        CGF.ReturnBlock = CGF.getJumpDestInCurrentScope(&RetBB);\n      }\n\n      ~OutlinedRegionBodyRAII() {\n        CGF.AllocaInsertPt = OldAllocaIP;\n        CGF.ReturnBlock = OldReturnBlock;\n        CGF.Builder.restoreIP(IP);\n      }\n    };\n\n    /// RAII for preserving necessary info during inlined region body codegen.\n    class InlinedRegionBodyRAII {\n\n      llvm::AssertingVH<llvm::Instruction> OldAllocaIP;\n      CodeGenFunction &CGF;\n\n    public:\n      InlinedRegionBodyRAII(CodeGenFunction &cgf, InsertPointTy &AllocaIP,\n                            llvm::BasicBlock &FiniBB)\n          : CGF(cgf) {\n        // Alloca insertion block should be in the entry block of the containing\n        // function so it expects an empty AllocaIP in which case will reuse the\n        // old alloca insertion point, or a new AllocaIP in the same block as\n        // the old one\n        assert((!AllocaIP.isSet() ||\n                CGF.AllocaInsertPt->getParent() == AllocaIP.getBlock()) &&\n               \"Insertion point should be in the entry block of containing \"\n               \"function!\");\n        OldAllocaIP = CGF.AllocaInsertPt;\n        if (AllocaIP.isSet())\n          CGF.AllocaInsertPt = &*AllocaIP.getPoint();\n\n        // TODO: Remove the call, after making sure the counter is not used by\n        //       the EHStack.\n        // Since this is an inlined region, it should not modify the\n        // ReturnBlock, and should reuse the one for the enclosing outlined\n        // region. So, the JumpDest being return by the function is discarded\n        (void)CGF.getJumpDestInCurrentScope(&FiniBB);\n      }\n\n      ~InlinedRegionBodyRAII() { CGF.AllocaInsertPt = OldAllocaIP; }\n    };\n  };\n\nprivate:\n  /// CXXThisDecl - When generating code for a C++ member function,\n  /// this will hold the implicit 'this' declaration.\n  ImplicitParamDecl *CXXABIThisDecl = nullptr;\n  llvm::Value *CXXABIThisValue = nullptr;\n  llvm::Value *CXXThisValue = nullptr;\n  CharUnits CXXABIThisAlignment;\n  CharUnits CXXThisAlignment;\n\n  /// The value of 'this' to use when evaluating CXXDefaultInitExprs within\n  /// this expression.\n  Address CXXDefaultInitExprThis = Address::invalid();\n\n  /// The current array initialization index when evaluating an\n  /// ArrayInitIndexExpr within an ArrayInitLoopExpr.\n  llvm::Value *ArrayInitIndex = nullptr;\n\n  /// The values of function arguments to use when evaluating\n  /// CXXInheritedCtorInitExprs within this context.\n  CallArgList CXXInheritedCtorInitExprArgs;\n\n  /// CXXStructorImplicitParamDecl - When generating code for a constructor or\n  /// destructor, this will hold the implicit argument (e.g. VTT).\n  ImplicitParamDecl *CXXStructorImplicitParamDecl = nullptr;\n  llvm::Value *CXXStructorImplicitParamValue = nullptr;\n\n  /// OutermostConditional - Points to the outermost active\n  /// conditional control.  This is used so that we know if a\n  /// temporary should be destroyed conditionally.\n  ConditionalEvaluation *OutermostConditional = nullptr;\n\n  /// The current lexical scope.\n  LexicalScope *CurLexicalScope = nullptr;\n\n  /// The current source location that should be used for exception\n  /// handling code.\n  SourceLocation CurEHLocation;\n\n  /// BlockByrefInfos - For each __block variable, contains\n  /// information about the layout of the variable.\n  llvm::DenseMap<const ValueDecl *, BlockByrefInfo> BlockByrefInfos;\n\n  /// Used by -fsanitize=nullability-return to determine whether the return\n  /// value can be checked.\n  llvm::Value *RetValNullabilityPrecondition = nullptr;\n\n  /// Check if -fsanitize=nullability-return instrumentation is required for\n  /// this function.\n  bool requiresReturnValueNullabilityCheck() const {\n    return RetValNullabilityPrecondition;\n  }\n\n  /// Used to store precise source locations for return statements by the\n  /// runtime return value checks.\n  Address ReturnLocation = Address::invalid();\n\n  /// Check if the return value of this function requires sanitization.\n  bool requiresReturnValueCheck() const;\n\n  llvm::BasicBlock *TerminateLandingPad = nullptr;\n  llvm::BasicBlock *TerminateHandler = nullptr;\n  llvm::SmallVector<llvm::BasicBlock *, 2> TrapBBs;\n\n  /// Terminate funclets keyed by parent funclet pad.\n  llvm::MapVector<llvm::Value *, llvm::BasicBlock *> TerminateFunclets;\n\n  /// Largest vector width used in ths function. Will be used to create a\n  /// function attribute.\n  unsigned LargestVectorWidth = 0;\n\n  /// True if we need emit the life-time markers.\n  const bool ShouldEmitLifetimeMarkers;\n\n  /// Add OpenCL kernel arg metadata and the kernel attribute metadata to\n  /// the function metadata.\n  void EmitOpenCLKernelMetadata(const FunctionDecl *FD,\n                                llvm::Function *Fn);\n\npublic:\n  CodeGenFunction(CodeGenModule &cgm, bool suppressNewContext=false);\n  ~CodeGenFunction();\n\n  CodeGenTypes &getTypes() const { return CGM.getTypes(); }\n  ASTContext &getContext() const { return CGM.getContext(); }\n  CGDebugInfo *getDebugInfo() {\n    if (DisableDebugInfo)\n      return nullptr;\n    return DebugInfo;\n  }\n  void disableDebugInfo() { DisableDebugInfo = true; }\n  void enableDebugInfo() { DisableDebugInfo = false; }\n\n  bool shouldUseFusedARCCalls() {\n    return CGM.getCodeGenOpts().OptimizationLevel == 0;\n  }\n\n  const LangOptions &getLangOpts() const { return CGM.getLangOpts(); }\n\n  /// Returns a pointer to the function's exception object and selector slot,\n  /// which is assigned in every landing pad.\n  Address getExceptionSlot();\n  Address getEHSelectorSlot();\n\n  /// Returns the contents of the function's exception object and selector\n  /// slots.\n  llvm::Value *getExceptionFromSlot();\n  llvm::Value *getSelectorFromSlot();\n\n  Address getNormalCleanupDestSlot();\n\n  llvm::BasicBlock *getUnreachableBlock() {\n    if (!UnreachableBlock) {\n      UnreachableBlock = createBasicBlock(\"unreachable\");\n      new llvm::UnreachableInst(getLLVMContext(), UnreachableBlock);\n    }\n    return UnreachableBlock;\n  }\n\n  llvm::BasicBlock *getInvokeDest() {\n    if (!EHStack.requiresLandingPad()) return nullptr;\n    return getInvokeDestImpl();\n  }\n\n  bool currentFunctionUsesSEHTry() const { return CurSEHParent != nullptr; }\n\n  const TargetInfo &getTarget() const { return Target; }\n  llvm::LLVMContext &getLLVMContext() { return CGM.getLLVMContext(); }\n  const TargetCodeGenInfo &getTargetHooks() const {\n    return CGM.getTargetCodeGenInfo();\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                                  Cleanups\n  //===--------------------------------------------------------------------===//\n\n  typedef void Destroyer(CodeGenFunction &CGF, Address addr, QualType ty);\n\n  void pushIrregularPartialArrayCleanup(llvm::Value *arrayBegin,\n                                        Address arrayEndPointer,\n                                        QualType elementType,\n                                        CharUnits elementAlignment,\n                                        Destroyer *destroyer);\n  void pushRegularPartialArrayCleanup(llvm::Value *arrayBegin,\n                                      llvm::Value *arrayEnd,\n                                      QualType elementType,\n                                      CharUnits elementAlignment,\n                                      Destroyer *destroyer);\n\n  void pushDestroy(QualType::DestructionKind dtorKind,\n                   Address addr, QualType type);\n  void pushEHDestroy(QualType::DestructionKind dtorKind,\n                     Address addr, QualType type);\n  void pushDestroy(CleanupKind kind, Address addr, QualType type,\n                   Destroyer *destroyer, bool useEHCleanupForArray);\n  void pushLifetimeExtendedDestroy(CleanupKind kind, Address addr,\n                                   QualType type, Destroyer *destroyer,\n                                   bool useEHCleanupForArray);\n  void pushCallObjectDeleteCleanup(const FunctionDecl *OperatorDelete,\n                                   llvm::Value *CompletePtr,\n                                   QualType ElementType);\n  void pushStackRestore(CleanupKind kind, Address SPMem);\n  void emitDestroy(Address addr, QualType type, Destroyer *destroyer,\n                   bool useEHCleanupForArray);\n  llvm::Function *generateDestroyHelper(Address addr, QualType type,\n                                        Destroyer *destroyer,\n                                        bool useEHCleanupForArray,\n                                        const VarDecl *VD);\n  void emitArrayDestroy(llvm::Value *begin, llvm::Value *end,\n                        QualType elementType, CharUnits elementAlign,\n                        Destroyer *destroyer,\n                        bool checkZeroLength, bool useEHCleanup);\n\n  Destroyer *getDestroyer(QualType::DestructionKind destructionKind);\n\n  /// Determines whether an EH cleanup is required to destroy a type\n  /// with the given destruction kind.\n  bool needsEHCleanup(QualType::DestructionKind kind) {\n    switch (kind) {\n    case QualType::DK_none:\n      return false;\n    case QualType::DK_cxx_destructor:\n    case QualType::DK_objc_weak_lifetime:\n    case QualType::DK_nontrivial_c_struct:\n      return getLangOpts().Exceptions;\n    case QualType::DK_objc_strong_lifetime:\n      return getLangOpts().Exceptions &&\n             CGM.getCodeGenOpts().ObjCAutoRefCountExceptions;\n    }\n    llvm_unreachable(\"bad destruction kind\");\n  }\n\n  CleanupKind getCleanupKind(QualType::DestructionKind kind) {\n    return (needsEHCleanup(kind) ? NormalAndEHCleanup : NormalCleanup);\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                                  Objective-C\n  //===--------------------------------------------------------------------===//\n\n  void GenerateObjCMethod(const ObjCMethodDecl *OMD);\n\n  void StartObjCMethod(const ObjCMethodDecl *MD, const ObjCContainerDecl *CD);\n\n  /// GenerateObjCGetter - Synthesize an Objective-C property getter function.\n  void GenerateObjCGetter(ObjCImplementationDecl *IMP,\n                          const ObjCPropertyImplDecl *PID);\n  void generateObjCGetterBody(const ObjCImplementationDecl *classImpl,\n                              const ObjCPropertyImplDecl *propImpl,\n                              const ObjCMethodDecl *GetterMothodDecl,\n                              llvm::Constant *AtomicHelperFn);\n\n  void GenerateObjCCtorDtorMethod(ObjCImplementationDecl *IMP,\n                                  ObjCMethodDecl *MD, bool ctor);\n\n  /// GenerateObjCSetter - Synthesize an Objective-C property setter function\n  /// for the given property.\n  void GenerateObjCSetter(ObjCImplementationDecl *IMP,\n                          const ObjCPropertyImplDecl *PID);\n  void generateObjCSetterBody(const ObjCImplementationDecl *classImpl,\n                              const ObjCPropertyImplDecl *propImpl,\n                              llvm::Constant *AtomicHelperFn);\n\n  //===--------------------------------------------------------------------===//\n  //                                  Block Bits\n  //===--------------------------------------------------------------------===//\n\n  /// Emit block literal.\n  /// \\return an LLVM value which is a pointer to a struct which contains\n  /// information about the block, including the block invoke function, the\n  /// captured variables, etc.\n  llvm::Value *EmitBlockLiteral(const BlockExpr *);\n\n  llvm::Function *GenerateBlockFunction(GlobalDecl GD,\n                                        const CGBlockInfo &Info,\n                                        const DeclMapTy &ldm,\n                                        bool IsLambdaConversionToBlock,\n                                        bool BuildGlobalBlock);\n\n  /// Check if \\p T is a C++ class that has a destructor that can throw.\n  static bool cxxDestructorCanThrow(QualType T);\n\n  llvm::Constant *GenerateCopyHelperFunction(const CGBlockInfo &blockInfo);\n  llvm::Constant *GenerateDestroyHelperFunction(const CGBlockInfo &blockInfo);\n  llvm::Constant *GenerateObjCAtomicSetterCopyHelperFunction(\n                                             const ObjCPropertyImplDecl *PID);\n  llvm::Constant *GenerateObjCAtomicGetterCopyHelperFunction(\n                                             const ObjCPropertyImplDecl *PID);\n  llvm::Value *EmitBlockCopyAndAutorelease(llvm::Value *Block, QualType Ty);\n\n  void BuildBlockRelease(llvm::Value *DeclPtr, BlockFieldFlags flags,\n                         bool CanThrow);\n\n  class AutoVarEmission;\n\n  void emitByrefStructureInit(const AutoVarEmission &emission);\n\n  /// Enter a cleanup to destroy a __block variable.  Note that this\n  /// cleanup should be a no-op if the variable hasn't left the stack\n  /// yet; if a cleanup is required for the variable itself, that needs\n  /// to be done externally.\n  ///\n  /// \\param Kind Cleanup kind.\n  ///\n  /// \\param Addr When \\p LoadBlockVarAddr is false, the address of the __block\n  /// structure that will be passed to _Block_object_dispose. When\n  /// \\p LoadBlockVarAddr is true, the address of the field of the block\n  /// structure that holds the address of the __block structure.\n  ///\n  /// \\param Flags The flag that will be passed to _Block_object_dispose.\n  ///\n  /// \\param LoadBlockVarAddr Indicates whether we need to emit a load from\n  /// \\p Addr to get the address of the __block structure.\n  void enterByrefCleanup(CleanupKind Kind, Address Addr, BlockFieldFlags Flags,\n                         bool LoadBlockVarAddr, bool CanThrow);\n\n  void setBlockContextParameter(const ImplicitParamDecl *D, unsigned argNum,\n                                llvm::Value *ptr);\n\n  Address LoadBlockStruct();\n  Address GetAddrOfBlockDecl(const VarDecl *var);\n\n  /// BuildBlockByrefAddress - Computes the location of the\n  /// data in a variable which is declared as __block.\n  Address emitBlockByrefAddress(Address baseAddr, const VarDecl *V,\n                                bool followForward = true);\n  Address emitBlockByrefAddress(Address baseAddr,\n                                const BlockByrefInfo &info,\n                                bool followForward,\n                                const llvm::Twine &name);\n\n  const BlockByrefInfo &getBlockByrefInfo(const VarDecl *var);\n\n  QualType BuildFunctionArgList(GlobalDecl GD, FunctionArgList &Args);\n\n  void GenerateCode(GlobalDecl GD, llvm::Function *Fn,\n                    const CGFunctionInfo &FnInfo);\n\n  /// Annotate the function with an attribute that disables TSan checking at\n  /// runtime.\n  void markAsIgnoreThreadCheckingAtRuntime(llvm::Function *Fn);\n\n  /// Emit code for the start of a function.\n  /// \\param Loc       The location to be associated with the function.\n  /// \\param StartLoc  The location of the function body.\n  void StartFunction(GlobalDecl GD,\n                     QualType RetTy,\n                     llvm::Function *Fn,\n                     const CGFunctionInfo &FnInfo,\n                     const FunctionArgList &Args,\n                     SourceLocation Loc = SourceLocation(),\n                     SourceLocation StartLoc = SourceLocation());\n\n  static bool IsConstructorDelegationValid(const CXXConstructorDecl *Ctor);\n\n  void EmitConstructorBody(FunctionArgList &Args);\n  void EmitDestructorBody(FunctionArgList &Args);\n  void emitImplicitAssignmentOperatorBody(FunctionArgList &Args);\n  void EmitFunctionBody(const Stmt *Body);\n  void EmitBlockWithFallThrough(llvm::BasicBlock *BB, const Stmt *S);\n\n  void EmitForwardingCallToLambda(const CXXMethodDecl *LambdaCallOperator,\n                                  CallArgList &CallArgs);\n  void EmitLambdaBlockInvokeBody();\n  void EmitLambdaDelegatingInvokeBody(const CXXMethodDecl *MD);\n  void EmitLambdaStaticInvokeBody(const CXXMethodDecl *MD);\n  void EmitLambdaVLACapture(const VariableArrayType *VAT, LValue LV) {\n    EmitStoreThroughLValue(RValue::get(VLASizeMap[VAT->getSizeExpr()]), LV);\n  }\n  void EmitAsanPrologueOrEpilogue(bool Prologue);\n\n  /// Emit the unified return block, trying to avoid its emission when\n  /// possible.\n  /// \\return The debug location of the user written return statement if the\n  /// return block is is avoided.\n  llvm::DebugLoc EmitReturnBlock();\n\n  /// FinishFunction - Complete IR generation of the current function. It is\n  /// legal to call this function even if there is no current insertion point.\n  void FinishFunction(SourceLocation EndLoc=SourceLocation());\n\n  void StartThunk(llvm::Function *Fn, GlobalDecl GD,\n                  const CGFunctionInfo &FnInfo, bool IsUnprototyped);\n\n  void EmitCallAndReturnForThunk(llvm::FunctionCallee Callee,\n                                 const ThunkInfo *Thunk, bool IsUnprototyped);\n\n  void FinishThunk();\n\n  /// Emit a musttail call for a thunk with a potentially adjusted this pointer.\n  void EmitMustTailThunk(GlobalDecl GD, llvm::Value *AdjustedThisPtr,\n                         llvm::FunctionCallee Callee);\n\n  /// Generate a thunk for the given method.\n  void generateThunk(llvm::Function *Fn, const CGFunctionInfo &FnInfo,\n                     GlobalDecl GD, const ThunkInfo &Thunk,\n                     bool IsUnprototyped);\n\n  llvm::Function *GenerateVarArgsThunk(llvm::Function *Fn,\n                                       const CGFunctionInfo &FnInfo,\n                                       GlobalDecl GD, const ThunkInfo &Thunk);\n\n  void EmitCtorPrologue(const CXXConstructorDecl *CD, CXXCtorType Type,\n                        FunctionArgList &Args);\n\n  void EmitInitializerForField(FieldDecl *Field, LValue LHS, Expr *Init);\n\n  /// Struct with all information about dynamic [sub]class needed to set vptr.\n  struct VPtr {\n    BaseSubobject Base;\n    const CXXRecordDecl *NearestVBase;\n    CharUnits OffsetFromNearestVBase;\n    const CXXRecordDecl *VTableClass;\n  };\n\n  /// Initialize the vtable pointer of the given subobject.\n  void InitializeVTablePointer(const VPtr &vptr);\n\n  typedef llvm::SmallVector<VPtr, 4> VPtrsVector;\n\n  typedef llvm::SmallPtrSet<const CXXRecordDecl *, 4> VisitedVirtualBasesSetTy;\n  VPtrsVector getVTablePointers(const CXXRecordDecl *VTableClass);\n\n  void getVTablePointers(BaseSubobject Base, const CXXRecordDecl *NearestVBase,\n                         CharUnits OffsetFromNearestVBase,\n                         bool BaseIsNonVirtualPrimaryBase,\n                         const CXXRecordDecl *VTableClass,\n                         VisitedVirtualBasesSetTy &VBases, VPtrsVector &vptrs);\n\n  void InitializeVTablePointers(const CXXRecordDecl *ClassDecl);\n\n  /// GetVTablePtr - Return the Value of the vtable pointer member pointed\n  /// to by This.\n  llvm::Value *GetVTablePtr(Address This, llvm::Type *VTableTy,\n                            const CXXRecordDecl *VTableClass);\n\n  enum CFITypeCheckKind {\n    CFITCK_VCall,\n    CFITCK_NVCall,\n    CFITCK_DerivedCast,\n    CFITCK_UnrelatedCast,\n    CFITCK_ICall,\n    CFITCK_NVMFCall,\n    CFITCK_VMFCall,\n  };\n\n  /// Derived is the presumed address of an object of type T after a\n  /// cast. If T is a polymorphic class type, emit a check that the virtual\n  /// table for Derived belongs to a class derived from T.\n  void EmitVTablePtrCheckForCast(QualType T, llvm::Value *Derived,\n                                 bool MayBeNull, CFITypeCheckKind TCK,\n                                 SourceLocation Loc);\n\n  /// EmitVTablePtrCheckForCall - Virtual method MD is being called via VTable.\n  /// If vptr CFI is enabled, emit a check that VTable is valid.\n  void EmitVTablePtrCheckForCall(const CXXRecordDecl *RD, llvm::Value *VTable,\n                                 CFITypeCheckKind TCK, SourceLocation Loc);\n\n  /// EmitVTablePtrCheck - Emit a check that VTable is a valid virtual table for\n  /// RD using llvm.type.test.\n  void EmitVTablePtrCheck(const CXXRecordDecl *RD, llvm::Value *VTable,\n                          CFITypeCheckKind TCK, SourceLocation Loc);\n\n  /// If whole-program virtual table optimization is enabled, emit an assumption\n  /// that VTable is a member of RD's type identifier. Or, if vptr CFI is\n  /// enabled, emit a check that VTable is a member of RD's type identifier.\n  void EmitTypeMetadataCodeForVCall(const CXXRecordDecl *RD,\n                                    llvm::Value *VTable, SourceLocation Loc);\n\n  /// Returns whether we should perform a type checked load when loading a\n  /// virtual function for virtual calls to members of RD. This is generally\n  /// true when both vcall CFI and whole-program-vtables are enabled.\n  bool ShouldEmitVTableTypeCheckedLoad(const CXXRecordDecl *RD);\n\n  /// Emit a type checked load from the given vtable.\n  llvm::Value *EmitVTableTypeCheckedLoad(const CXXRecordDecl *RD, llvm::Value *VTable,\n                                         uint64_t VTableByteOffset);\n\n  /// EnterDtorCleanups - Enter the cleanups necessary to complete the\n  /// given phase of destruction for a destructor.  The end result\n  /// should call destructors on members and base classes in reverse\n  /// order of their construction.\n  void EnterDtorCleanups(const CXXDestructorDecl *Dtor, CXXDtorType Type);\n\n  /// ShouldInstrumentFunction - Return true if the current function should be\n  /// instrumented with __cyg_profile_func_* calls\n  bool ShouldInstrumentFunction();\n\n  /// ShouldXRayInstrument - Return true if the current function should be\n  /// instrumented with XRay nop sleds.\n  bool ShouldXRayInstrumentFunction() const;\n\n  /// AlwaysEmitXRayCustomEvents - Return true if we must unconditionally emit\n  /// XRay custom event handling calls.\n  bool AlwaysEmitXRayCustomEvents() const;\n\n  /// AlwaysEmitXRayTypedEvents - Return true if clang must unconditionally emit\n  /// XRay typed event handling calls.\n  bool AlwaysEmitXRayTypedEvents() const;\n\n  /// Encode an address into a form suitable for use in a function prologue.\n  llvm::Constant *EncodeAddrForUseInPrologue(llvm::Function *F,\n                                             llvm::Constant *Addr);\n\n  /// Decode an address used in a function prologue, encoded by \\c\n  /// EncodeAddrForUseInPrologue.\n  llvm::Value *DecodeAddrUsedInPrologue(llvm::Value *F,\n                                        llvm::Value *EncodedAddr);\n\n  /// EmitFunctionProlog - Emit the target specific LLVM code to load the\n  /// arguments for the given function. This is also responsible for naming the\n  /// LLVM function arguments.\n  void EmitFunctionProlog(const CGFunctionInfo &FI,\n                          llvm::Function *Fn,\n                          const FunctionArgList &Args);\n\n  /// EmitFunctionEpilog - Emit the target specific LLVM code to return the\n  /// given temporary.\n  void EmitFunctionEpilog(const CGFunctionInfo &FI, bool EmitRetDbgLoc,\n                          SourceLocation EndLoc);\n\n  /// Emit a test that checks if the return value \\p RV is nonnull.\n  void EmitReturnValueCheck(llvm::Value *RV);\n\n  /// EmitStartEHSpec - Emit the start of the exception spec.\n  void EmitStartEHSpec(const Decl *D);\n\n  /// EmitEndEHSpec - Emit the end of the exception spec.\n  void EmitEndEHSpec(const Decl *D);\n\n  /// getTerminateLandingPad - Return a landing pad that just calls terminate.\n  llvm::BasicBlock *getTerminateLandingPad();\n\n  /// getTerminateLandingPad - Return a cleanup funclet that just calls\n  /// terminate.\n  llvm::BasicBlock *getTerminateFunclet();\n\n  /// getTerminateHandler - Return a handler (not a landing pad, just\n  /// a catch handler) that just calls terminate.  This is used when\n  /// a terminate scope encloses a try.\n  llvm::BasicBlock *getTerminateHandler();\n\n  llvm::Type *ConvertTypeForMem(QualType T);\n  llvm::Type *ConvertType(QualType T);\n  llvm::Type *ConvertType(const TypeDecl *T) {\n    return ConvertType(getContext().getTypeDeclType(T));\n  }\n\n  /// LoadObjCSelf - Load the value of self. This function is only valid while\n  /// generating code for an Objective-C method.\n  llvm::Value *LoadObjCSelf();\n\n  /// TypeOfSelfObject - Return type of object that this self represents.\n  QualType TypeOfSelfObject();\n\n  /// getEvaluationKind - Return the TypeEvaluationKind of QualType \\c T.\n  static TypeEvaluationKind getEvaluationKind(QualType T);\n\n  static bool hasScalarEvaluationKind(QualType T) {\n    return getEvaluationKind(T) == TEK_Scalar;\n  }\n\n  static bool hasAggregateEvaluationKind(QualType T) {\n    return getEvaluationKind(T) == TEK_Aggregate;\n  }\n\n  /// createBasicBlock - Create an LLVM basic block.\n  llvm::BasicBlock *createBasicBlock(const Twine &name = \"\",\n                                     llvm::Function *parent = nullptr,\n                                     llvm::BasicBlock *before = nullptr) {\n    return llvm::BasicBlock::Create(getLLVMContext(), name, parent, before);\n  }\n\n  /// getBasicBlockForLabel - Return the LLVM basicblock that the specified\n  /// label maps to.\n  JumpDest getJumpDestForLabel(const LabelDecl *S);\n\n  /// SimplifyForwardingBlocks - If the given basic block is only a branch to\n  /// another basic block, simplify it. This assumes that no other code could\n  /// potentially reference the basic block.\n  void SimplifyForwardingBlocks(llvm::BasicBlock *BB);\n\n  /// EmitBlock - Emit the given block \\arg BB and set it as the insert point,\n  /// adding a fall-through branch from the current insert block if\n  /// necessary. It is legal to call this function even if there is no current\n  /// insertion point.\n  ///\n  /// IsFinished - If true, indicates that the caller has finished emitting\n  /// branches to the given block and does not expect to emit code into it. This\n  /// means the block can be ignored if it is unreachable.\n  void EmitBlock(llvm::BasicBlock *BB, bool IsFinished=false);\n\n  /// EmitBlockAfterUses - Emit the given block somewhere hopefully\n  /// near its uses, and leave the insertion point in it.\n  void EmitBlockAfterUses(llvm::BasicBlock *BB);\n\n  /// EmitBranch - Emit a branch to the specified basic block from the current\n  /// insert block, taking care to avoid creation of branches from dummy\n  /// blocks. It is legal to call this function even if there is no current\n  /// insertion point.\n  ///\n  /// This function clears the current insertion point. The caller should follow\n  /// calls to this function with calls to Emit*Block prior to generation new\n  /// code.\n  void EmitBranch(llvm::BasicBlock *Block);\n\n  /// HaveInsertPoint - True if an insertion point is defined. If not, this\n  /// indicates that the current code being emitted is unreachable.\n  bool HaveInsertPoint() const {\n    return Builder.GetInsertBlock() != nullptr;\n  }\n\n  /// EnsureInsertPoint - Ensure that an insertion point is defined so that\n  /// emitted IR has a place to go. Note that by definition, if this function\n  /// creates a block then that block is unreachable; callers may do better to\n  /// detect when no insertion point is defined and simply skip IR generation.\n  void EnsureInsertPoint() {\n    if (!HaveInsertPoint())\n      EmitBlock(createBasicBlock());\n  }\n\n  /// ErrorUnsupported - Print out an error that codegen doesn't support the\n  /// specified stmt yet.\n  void ErrorUnsupported(const Stmt *S, const char *Type);\n\n  //===--------------------------------------------------------------------===//\n  //                                  Helpers\n  //===--------------------------------------------------------------------===//\n\n  LValue MakeAddrLValue(Address Addr, QualType T,\n                        AlignmentSource Source = AlignmentSource::Type) {\n    return LValue::MakeAddr(Addr, T, getContext(), LValueBaseInfo(Source),\n                            CGM.getTBAAAccessInfo(T));\n  }\n\n  LValue MakeAddrLValue(Address Addr, QualType T, LValueBaseInfo BaseInfo,\n                        TBAAAccessInfo TBAAInfo) {\n    return LValue::MakeAddr(Addr, T, getContext(), BaseInfo, TBAAInfo);\n  }\n\n  LValue MakeAddrLValue(llvm::Value *V, QualType T, CharUnits Alignment,\n                        AlignmentSource Source = AlignmentSource::Type) {\n    return LValue::MakeAddr(Address(V, Alignment), T, getContext(),\n                            LValueBaseInfo(Source), CGM.getTBAAAccessInfo(T));\n  }\n\n  LValue MakeAddrLValue(llvm::Value *V, QualType T, CharUnits Alignment,\n                        LValueBaseInfo BaseInfo, TBAAAccessInfo TBAAInfo) {\n    return LValue::MakeAddr(Address(V, Alignment), T, getContext(),\n                            BaseInfo, TBAAInfo);\n  }\n\n  LValue MakeNaturalAlignPointeeAddrLValue(llvm::Value *V, QualType T);\n  LValue MakeNaturalAlignAddrLValue(llvm::Value *V, QualType T);\n\n  Address EmitLoadOfReference(LValue RefLVal,\n                              LValueBaseInfo *PointeeBaseInfo = nullptr,\n                              TBAAAccessInfo *PointeeTBAAInfo = nullptr);\n  LValue EmitLoadOfReferenceLValue(LValue RefLVal);\n  LValue EmitLoadOfReferenceLValue(Address RefAddr, QualType RefTy,\n                                   AlignmentSource Source =\n                                       AlignmentSource::Type) {\n    LValue RefLVal = MakeAddrLValue(RefAddr, RefTy, LValueBaseInfo(Source),\n                                    CGM.getTBAAAccessInfo(RefTy));\n    return EmitLoadOfReferenceLValue(RefLVal);\n  }\n\n  Address EmitLoadOfPointer(Address Ptr, const PointerType *PtrTy,\n                            LValueBaseInfo *BaseInfo = nullptr,\n                            TBAAAccessInfo *TBAAInfo = nullptr);\n  LValue EmitLoadOfPointerLValue(Address Ptr, const PointerType *PtrTy);\n\n  /// CreateTempAlloca - This creates an alloca and inserts it into the entry\n  /// block if \\p ArraySize is nullptr, otherwise inserts it at the current\n  /// insertion point of the builder. The caller is responsible for setting an\n  /// appropriate alignment on\n  /// the alloca.\n  ///\n  /// \\p ArraySize is the number of array elements to be allocated if it\n  ///    is not nullptr.\n  ///\n  /// LangAS::Default is the address space of pointers to local variables and\n  /// temporaries, as exposed in the source language. In certain\n  /// configurations, this is not the same as the alloca address space, and a\n  /// cast is needed to lift the pointer from the alloca AS into\n  /// LangAS::Default. This can happen when the target uses a restricted\n  /// address space for the stack but the source language requires\n  /// LangAS::Default to be a generic address space. The latter condition is\n  /// common for most programming languages; OpenCL is an exception in that\n  /// LangAS::Default is the private address space, which naturally maps\n  /// to the stack.\n  ///\n  /// Because the address of a temporary is often exposed to the program in\n  /// various ways, this function will perform the cast. The original alloca\n  /// instruction is returned through \\p Alloca if it is not nullptr.\n  ///\n  /// The cast is not performaed in CreateTempAllocaWithoutCast. This is\n  /// more efficient if the caller knows that the address will not be exposed.\n  llvm::AllocaInst *CreateTempAlloca(llvm::Type *Ty, const Twine &Name = \"tmp\",\n                                     llvm::Value *ArraySize = nullptr);\n  Address CreateTempAlloca(llvm::Type *Ty, CharUnits align,\n                           const Twine &Name = \"tmp\",\n                           llvm::Value *ArraySize = nullptr,\n                           Address *Alloca = nullptr);\n  Address CreateTempAllocaWithoutCast(llvm::Type *Ty, CharUnits align,\n                                      const Twine &Name = \"tmp\",\n                                      llvm::Value *ArraySize = nullptr);\n\n  /// CreateDefaultAlignedTempAlloca - This creates an alloca with the\n  /// default ABI alignment of the given LLVM type.\n  ///\n  /// IMPORTANT NOTE: This is *not* generally the right alignment for\n  /// any given AST type that happens to have been lowered to the\n  /// given IR type.  This should only ever be used for function-local,\n  /// IR-driven manipulations like saving and restoring a value.  Do\n  /// not hand this address off to arbitrary IRGen routines, and especially\n  /// do not pass it as an argument to a function that might expect a\n  /// properly ABI-aligned value.\n  Address CreateDefaultAlignTempAlloca(llvm::Type *Ty,\n                                       const Twine &Name = \"tmp\");\n\n  /// InitTempAlloca - Provide an initial value for the given alloca which\n  /// will be observable at all locations in the function.\n  ///\n  /// The address should be something that was returned from one of\n  /// the CreateTempAlloca or CreateMemTemp routines, and the\n  /// initializer must be valid in the entry block (i.e. it must\n  /// either be a constant or an argument value).\n  void InitTempAlloca(Address Alloca, llvm::Value *Value);\n\n  /// CreateIRTemp - Create a temporary IR object of the given type, with\n  /// appropriate alignment. This routine should only be used when an temporary\n  /// value needs to be stored into an alloca (for example, to avoid explicit\n  /// PHI construction), but the type is the IR type, not the type appropriate\n  /// for storing in memory.\n  ///\n  /// That is, this is exactly equivalent to CreateMemTemp, but calling\n  /// ConvertType instead of ConvertTypeForMem.\n  Address CreateIRTemp(QualType T, const Twine &Name = \"tmp\");\n\n  /// CreateMemTemp - Create a temporary memory object of the given type, with\n  /// appropriate alignmen and cast it to the default address space. Returns\n  /// the original alloca instruction by \\p Alloca if it is not nullptr.\n  Address CreateMemTemp(QualType T, const Twine &Name = \"tmp\",\n                        Address *Alloca = nullptr);\n  Address CreateMemTemp(QualType T, CharUnits Align, const Twine &Name = \"tmp\",\n                        Address *Alloca = nullptr);\n\n  /// CreateMemTemp - Create a temporary memory object of the given type, with\n  /// appropriate alignmen without casting it to the default address space.\n  Address CreateMemTempWithoutCast(QualType T, const Twine &Name = \"tmp\");\n  Address CreateMemTempWithoutCast(QualType T, CharUnits Align,\n                                   const Twine &Name = \"tmp\");\n\n  /// CreateAggTemp - Create a temporary memory object for the given\n  /// aggregate type.\n  AggValueSlot CreateAggTemp(QualType T, const Twine &Name = \"tmp\",\n                             Address *Alloca = nullptr) {\n    return AggValueSlot::forAddr(CreateMemTemp(T, Name, Alloca),\n                                 T.getQualifiers(),\n                                 AggValueSlot::IsNotDestructed,\n                                 AggValueSlot::DoesNotNeedGCBarriers,\n                                 AggValueSlot::IsNotAliased,\n                                 AggValueSlot::DoesNotOverlap);\n  }\n\n  /// Emit a cast to void* in the appropriate address space.\n  llvm::Value *EmitCastToVoidPtr(llvm::Value *value);\n\n  /// EvaluateExprAsBool - Perform the usual unary conversions on the specified\n  /// expression and compare the result against zero, returning an Int1Ty value.\n  llvm::Value *EvaluateExprAsBool(const Expr *E);\n\n  /// EmitIgnoredExpr - Emit an expression in a context which ignores the result.\n  void EmitIgnoredExpr(const Expr *E);\n\n  /// EmitAnyExpr - Emit code to compute the specified expression which can have\n  /// any type.  The result is returned as an RValue struct.  If this is an\n  /// aggregate expression, the aggloc/agglocvolatile arguments indicate where\n  /// the result should be returned.\n  ///\n  /// \\param ignoreResult True if the resulting value isn't used.\n  RValue EmitAnyExpr(const Expr *E,\n                     AggValueSlot aggSlot = AggValueSlot::ignored(),\n                     bool ignoreResult = false);\n\n  // EmitVAListRef - Emit a \"reference\" to a va_list; this is either the address\n  // or the value of the expression, depending on how va_list is defined.\n  Address EmitVAListRef(const Expr *E);\n\n  /// Emit a \"reference\" to a __builtin_ms_va_list; this is\n  /// always the value of the expression, because a __builtin_ms_va_list is a\n  /// pointer to a char.\n  Address EmitMSVAListRef(const Expr *E);\n\n  /// EmitAnyExprToTemp - Similarly to EmitAnyExpr(), however, the result will\n  /// always be accessible even if no aggregate location is provided.\n  RValue EmitAnyExprToTemp(const Expr *E);\n\n  /// EmitAnyExprToMem - Emits the code necessary to evaluate an\n  /// arbitrary expression into the given memory location.\n  void EmitAnyExprToMem(const Expr *E, Address Location,\n                        Qualifiers Quals, bool IsInitializer);\n\n  void EmitAnyExprToExn(const Expr *E, Address Addr);\n\n  /// EmitExprAsInit - Emits the code necessary to initialize a\n  /// location in memory with the given initializer.\n  void EmitExprAsInit(const Expr *init, const ValueDecl *D, LValue lvalue,\n                      bool capturedByInit);\n\n  /// hasVolatileMember - returns true if aggregate type has a volatile\n  /// member.\n  bool hasVolatileMember(QualType T) {\n    if (const RecordType *RT = T->getAs<RecordType>()) {\n      const RecordDecl *RD = cast<RecordDecl>(RT->getDecl());\n      return RD->hasVolatileMember();\n    }\n    return false;\n  }\n\n  /// Determine whether a return value slot may overlap some other object.\n  AggValueSlot::Overlap_t getOverlapForReturnValue() {\n    // FIXME: Assuming no overlap here breaks guaranteed copy elision for base\n    // class subobjects. These cases may need to be revisited depending on the\n    // resolution of the relevant core issue.\n    return AggValueSlot::DoesNotOverlap;\n  }\n\n  /// Determine whether a field initialization may overlap some other object.\n  AggValueSlot::Overlap_t getOverlapForFieldInit(const FieldDecl *FD);\n\n  /// Determine whether a base class initialization may overlap some other\n  /// object.\n  AggValueSlot::Overlap_t getOverlapForBaseInit(const CXXRecordDecl *RD,\n                                                const CXXRecordDecl *BaseRD,\n                                                bool IsVirtual);\n\n  /// Emit an aggregate assignment.\n  void EmitAggregateAssign(LValue Dest, LValue Src, QualType EltTy) {\n    bool IsVolatile = hasVolatileMember(EltTy);\n    EmitAggregateCopy(Dest, Src, EltTy, AggValueSlot::MayOverlap, IsVolatile);\n  }\n\n  void EmitAggregateCopyCtor(LValue Dest, LValue Src,\n                             AggValueSlot::Overlap_t MayOverlap) {\n    EmitAggregateCopy(Dest, Src, Src.getType(), MayOverlap);\n  }\n\n  /// EmitAggregateCopy - Emit an aggregate copy.\n  ///\n  /// \\param isVolatile \\c true iff either the source or the destination is\n  ///        volatile.\n  /// \\param MayOverlap Whether the tail padding of the destination might be\n  ///        occupied by some other object. More efficient code can often be\n  ///        generated if not.\n  void EmitAggregateCopy(LValue Dest, LValue Src, QualType EltTy,\n                         AggValueSlot::Overlap_t MayOverlap,\n                         bool isVolatile = false);\n\n  /// GetAddrOfLocalVar - Return the address of a local variable.\n  Address GetAddrOfLocalVar(const VarDecl *VD) {\n    auto it = LocalDeclMap.find(VD);\n    assert(it != LocalDeclMap.end() &&\n           \"Invalid argument to GetAddrOfLocalVar(), no decl!\");\n    return it->second;\n  }\n\n  /// Given an opaque value expression, return its LValue mapping if it exists,\n  /// otherwise create one.\n  LValue getOrCreateOpaqueLValueMapping(const OpaqueValueExpr *e);\n\n  /// Given an opaque value expression, return its RValue mapping if it exists,\n  /// otherwise create one.\n  RValue getOrCreateOpaqueRValueMapping(const OpaqueValueExpr *e);\n\n  /// Get the index of the current ArrayInitLoopExpr, if any.\n  llvm::Value *getArrayInitIndex() { return ArrayInitIndex; }\n\n  /// getAccessedFieldNo - Given an encoded value and a result number, return\n  /// the input field number being accessed.\n  static unsigned getAccessedFieldNo(unsigned Idx, const llvm::Constant *Elts);\n\n  llvm::BlockAddress *GetAddrOfLabel(const LabelDecl *L);\n  llvm::BasicBlock *GetIndirectGotoBlock();\n\n  /// Check if \\p E is a C++ \"this\" pointer wrapped in value-preserving casts.\n  static bool IsWrappedCXXThis(const Expr *E);\n\n  /// EmitNullInitialization - Generate code to set a value of the given type to\n  /// null, If the type contains data member pointers, they will be initialized\n  /// to -1 in accordance with the Itanium C++ ABI.\n  void EmitNullInitialization(Address DestPtr, QualType Ty);\n\n  /// Emits a call to an LLVM variable-argument intrinsic, either\n  /// \\c llvm.va_start or \\c llvm.va_end.\n  /// \\param ArgValue A reference to the \\c va_list as emitted by either\n  /// \\c EmitVAListRef or \\c EmitMSVAListRef.\n  /// \\param IsStart If \\c true, emits a call to \\c llvm.va_start; otherwise,\n  /// calls \\c llvm.va_end.\n  llvm::Value *EmitVAStartEnd(llvm::Value *ArgValue, bool IsStart);\n\n  /// Generate code to get an argument from the passed in pointer\n  /// and update it accordingly.\n  /// \\param VE The \\c VAArgExpr for which to generate code.\n  /// \\param VAListAddr Receives a reference to the \\c va_list as emitted by\n  /// either \\c EmitVAListRef or \\c EmitMSVAListRef.\n  /// \\returns A pointer to the argument.\n  // FIXME: We should be able to get rid of this method and use the va_arg\n  // instruction in LLVM instead once it works well enough.\n  Address EmitVAArg(VAArgExpr *VE, Address &VAListAddr);\n\n  /// emitArrayLength - Compute the length of an array, even if it's a\n  /// VLA, and drill down to the base element type.\n  llvm::Value *emitArrayLength(const ArrayType *arrayType,\n                               QualType &baseType,\n                               Address &addr);\n\n  /// EmitVLASize - Capture all the sizes for the VLA expressions in\n  /// the given variably-modified type and store them in the VLASizeMap.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitVariablyModifiedType(QualType Ty);\n\n  struct VlaSizePair {\n    llvm::Value *NumElts;\n    QualType Type;\n\n    VlaSizePair(llvm::Value *NE, QualType T) : NumElts(NE), Type(T) {}\n  };\n\n  /// Return the number of elements for a single dimension\n  /// for the given array type.\n  VlaSizePair getVLAElements1D(const VariableArrayType *vla);\n  VlaSizePair getVLAElements1D(QualType vla);\n\n  /// Returns an LLVM value that corresponds to the size,\n  /// in non-variably-sized elements, of a variable length array type,\n  /// plus that largest non-variably-sized element type.  Assumes that\n  /// the type has already been emitted with EmitVariablyModifiedType.\n  VlaSizePair getVLASize(const VariableArrayType *vla);\n  VlaSizePair getVLASize(QualType vla);\n\n  /// LoadCXXThis - Load the value of 'this'. This function is only valid while\n  /// generating code for an C++ member function.\n  llvm::Value *LoadCXXThis() {\n    assert(CXXThisValue && \"no 'this' value for this function\");\n    return CXXThisValue;\n  }\n  Address LoadCXXThisAddress();\n\n  /// LoadCXXVTT - Load the VTT parameter to base constructors/destructors have\n  /// virtual bases.\n  // FIXME: Every place that calls LoadCXXVTT is something\n  // that needs to be abstracted properly.\n  llvm::Value *LoadCXXVTT() {\n    assert(CXXStructorImplicitParamValue && \"no VTT value for this function\");\n    return CXXStructorImplicitParamValue;\n  }\n\n  /// GetAddressOfBaseOfCompleteClass - Convert the given pointer to a\n  /// complete class to the given direct base.\n  Address\n  GetAddressOfDirectBaseInCompleteClass(Address Value,\n                                        const CXXRecordDecl *Derived,\n                                        const CXXRecordDecl *Base,\n                                        bool BaseIsVirtual);\n\n  static bool ShouldNullCheckClassCastValue(const CastExpr *Cast);\n\n  /// GetAddressOfBaseClass - This function will add the necessary delta to the\n  /// load of 'this' and returns address of the base class.\n  Address GetAddressOfBaseClass(Address Value,\n                                const CXXRecordDecl *Derived,\n                                CastExpr::path_const_iterator PathBegin,\n                                CastExpr::path_const_iterator PathEnd,\n                                bool NullCheckValue, SourceLocation Loc);\n\n  Address GetAddressOfDerivedClass(Address Value,\n                                   const CXXRecordDecl *Derived,\n                                   CastExpr::path_const_iterator PathBegin,\n                                   CastExpr::path_const_iterator PathEnd,\n                                   bool NullCheckValue);\n\n  /// GetVTTParameter - Return the VTT parameter that should be passed to a\n  /// base constructor/destructor with virtual bases.\n  /// FIXME: VTTs are Itanium ABI-specific, so the definition should move\n  /// to ItaniumCXXABI.cpp together with all the references to VTT.\n  llvm::Value *GetVTTParameter(GlobalDecl GD, bool ForVirtualBase,\n                               bool Delegating);\n\n  void EmitDelegateCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                      CXXCtorType CtorType,\n                                      const FunctionArgList &Args,\n                                      SourceLocation Loc);\n  // It's important not to confuse this and the previous function. Delegating\n  // constructors are the C++0x feature. The constructor delegate optimization\n  // is used to reduce duplication in the base and complete consturctors where\n  // they are substantially the same.\n  void EmitDelegatingCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                        const FunctionArgList &Args);\n\n  /// Emit a call to an inheriting constructor (that is, one that invokes a\n  /// constructor inherited from a base class) by inlining its definition. This\n  /// is necessary if the ABI does not support forwarding the arguments to the\n  /// base class constructor (because they're variadic or similar).\n  void EmitInlinedInheritingCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                               CXXCtorType CtorType,\n                                               bool ForVirtualBase,\n                                               bool Delegating,\n                                               CallArgList &Args);\n\n  /// Emit a call to a constructor inherited from a base class, passing the\n  /// current constructor's arguments along unmodified (without even making\n  /// a copy).\n  void EmitInheritedCXXConstructorCall(const CXXConstructorDecl *D,\n                                       bool ForVirtualBase, Address This,\n                                       bool InheritedFromVBase,\n                                       const CXXInheritedCtorInitExpr *E);\n\n  void EmitCXXConstructorCall(const CXXConstructorDecl *D, CXXCtorType Type,\n                              bool ForVirtualBase, bool Delegating,\n                              AggValueSlot ThisAVS, const CXXConstructExpr *E);\n\n  void EmitCXXConstructorCall(const CXXConstructorDecl *D, CXXCtorType Type,\n                              bool ForVirtualBase, bool Delegating,\n                              Address This, CallArgList &Args,\n                              AggValueSlot::Overlap_t Overlap,\n                              SourceLocation Loc, bool NewPointerIsChecked);\n\n  /// Emit assumption load for all bases. Requires to be be called only on\n  /// most-derived class and not under construction of the object.\n  void EmitVTableAssumptionLoads(const CXXRecordDecl *ClassDecl, Address This);\n\n  /// Emit assumption that vptr load == global vtable.\n  void EmitVTableAssumptionLoad(const VPtr &vptr, Address This);\n\n  void EmitSynthesizedCXXCopyCtorCall(const CXXConstructorDecl *D,\n                                      Address This, Address Src,\n                                      const CXXConstructExpr *E);\n\n  void EmitCXXAggrConstructorCall(const CXXConstructorDecl *D,\n                                  const ArrayType *ArrayTy,\n                                  Address ArrayPtr,\n                                  const CXXConstructExpr *E,\n                                  bool NewPointerIsChecked,\n                                  bool ZeroInitialization = false);\n\n  void EmitCXXAggrConstructorCall(const CXXConstructorDecl *D,\n                                  llvm::Value *NumElements,\n                                  Address ArrayPtr,\n                                  const CXXConstructExpr *E,\n                                  bool NewPointerIsChecked,\n                                  bool ZeroInitialization = false);\n\n  static Destroyer destroyCXXObject;\n\n  void EmitCXXDestructorCall(const CXXDestructorDecl *D, CXXDtorType Type,\n                             bool ForVirtualBase, bool Delegating, Address This,\n                             QualType ThisTy);\n\n  void EmitNewArrayInitializer(const CXXNewExpr *E, QualType elementType,\n                               llvm::Type *ElementTy, Address NewPtr,\n                               llvm::Value *NumElements,\n                               llvm::Value *AllocSizeWithoutCookie);\n\n  void EmitCXXTemporary(const CXXTemporary *Temporary, QualType TempType,\n                        Address Ptr);\n\n  llvm::Value *EmitLifetimeStart(uint64_t Size, llvm::Value *Addr);\n  void EmitLifetimeEnd(llvm::Value *Size, llvm::Value *Addr);\n\n  llvm::Value *EmitCXXNewExpr(const CXXNewExpr *E);\n  void EmitCXXDeleteExpr(const CXXDeleteExpr *E);\n\n  void EmitDeleteCall(const FunctionDecl *DeleteFD, llvm::Value *Ptr,\n                      QualType DeleteTy, llvm::Value *NumElements = nullptr,\n                      CharUnits CookieSize = CharUnits());\n\n  RValue EmitBuiltinNewDeleteCall(const FunctionProtoType *Type,\n                                  const CallExpr *TheCallExpr, bool IsDelete);\n\n  llvm::Value *EmitCXXTypeidExpr(const CXXTypeidExpr *E);\n  llvm::Value *EmitDynamicCast(Address V, const CXXDynamicCastExpr *DCE);\n  Address EmitCXXUuidofExpr(const CXXUuidofExpr *E);\n\n  /// Situations in which we might emit a check for the suitability of a\n  /// pointer or glvalue. Needs to be kept in sync with ubsan_handlers.cpp in\n  /// compiler-rt.\n  enum TypeCheckKind {\n    /// Checking the operand of a load. Must be suitably sized and aligned.\n    TCK_Load,\n    /// Checking the destination of a store. Must be suitably sized and aligned.\n    TCK_Store,\n    /// Checking the bound value in a reference binding. Must be suitably sized\n    /// and aligned, but is not required to refer to an object (until the\n    /// reference is used), per core issue 453.\n    TCK_ReferenceBinding,\n    /// Checking the object expression in a non-static data member access. Must\n    /// be an object within its lifetime.\n    TCK_MemberAccess,\n    /// Checking the 'this' pointer for a call to a non-static member function.\n    /// Must be an object within its lifetime.\n    TCK_MemberCall,\n    /// Checking the 'this' pointer for a constructor call.\n    TCK_ConstructorCall,\n    /// Checking the operand of a static_cast to a derived pointer type. Must be\n    /// null or an object within its lifetime.\n    TCK_DowncastPointer,\n    /// Checking the operand of a static_cast to a derived reference type. Must\n    /// be an object within its lifetime.\n    TCK_DowncastReference,\n    /// Checking the operand of a cast to a base object. Must be suitably sized\n    /// and aligned.\n    TCK_Upcast,\n    /// Checking the operand of a cast to a virtual base object. Must be an\n    /// object within its lifetime.\n    TCK_UpcastToVirtualBase,\n    /// Checking the value assigned to a _Nonnull pointer. Must not be null.\n    TCK_NonnullAssign,\n    /// Checking the operand of a dynamic_cast or a typeid expression.  Must be\n    /// null or an object within its lifetime.\n    TCK_DynamicOperation\n  };\n\n  /// Determine whether the pointer type check \\p TCK permits null pointers.\n  static bool isNullPointerAllowed(TypeCheckKind TCK);\n\n  /// Determine whether the pointer type check \\p TCK requires a vptr check.\n  static bool isVptrCheckRequired(TypeCheckKind TCK, QualType Ty);\n\n  /// Whether any type-checking sanitizers are enabled. If \\c false,\n  /// calls to EmitTypeCheck can be skipped.\n  bool sanitizePerformTypeCheck() const;\n\n  /// Emit a check that \\p V is the address of storage of the\n  /// appropriate size and alignment for an object of type \\p Type\n  /// (or if ArraySize is provided, for an array of that bound).\n  void EmitTypeCheck(TypeCheckKind TCK, SourceLocation Loc, llvm::Value *V,\n                     QualType Type, CharUnits Alignment = CharUnits::Zero(),\n                     SanitizerSet SkippedChecks = SanitizerSet(),\n                     llvm::Value *ArraySize = nullptr);\n\n  /// Emit a check that \\p Base points into an array object, which\n  /// we can access at index \\p Index. \\p Accessed should be \\c false if we\n  /// this expression is used as an lvalue, for instance in \"&Arr[Idx]\".\n  void EmitBoundsCheck(const Expr *E, const Expr *Base, llvm::Value *Index,\n                       QualType IndexType, bool Accessed);\n\n  llvm::Value *EmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                       bool isInc, bool isPre);\n  ComplexPairTy EmitComplexPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                         bool isInc, bool isPre);\n\n  /// Converts Location to a DebugLoc, if debug information is enabled.\n  llvm::DebugLoc SourceLocToDebugLoc(SourceLocation Location);\n\n  /// Get the record field index as represented in debug info.\n  unsigned getDebugInfoFIndex(const RecordDecl *Rec, unsigned FieldIndex);\n\n\n  //===--------------------------------------------------------------------===//\n  //                            Declaration Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitDecl - Emit a declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitDecl(const Decl &D);\n\n  /// EmitVarDecl - Emit a local variable declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitVarDecl(const VarDecl &D);\n\n  void EmitScalarInit(const Expr *init, const ValueDecl *D, LValue lvalue,\n                      bool capturedByInit);\n\n  typedef void SpecialInitFn(CodeGenFunction &Init, const VarDecl &D,\n                             llvm::Value *Address);\n\n  /// Determine whether the given initializer is trivial in the sense\n  /// that it requires no code to be generated.\n  bool isTrivialInitializer(const Expr *Init);\n\n  /// EmitAutoVarDecl - Emit an auto variable declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitAutoVarDecl(const VarDecl &D);\n\n  class AutoVarEmission {\n    friend class CodeGenFunction;\n\n    const VarDecl *Variable;\n\n    /// The address of the alloca for languages with explicit address space\n    /// (e.g. OpenCL) or alloca casted to generic pointer for address space\n    /// agnostic languages (e.g. C++). Invalid if the variable was emitted\n    /// as a global constant.\n    Address Addr;\n\n    llvm::Value *NRVOFlag;\n\n    /// True if the variable is a __block variable that is captured by an\n    /// escaping block.\n    bool IsEscapingByRef;\n\n    /// True if the variable is of aggregate type and has a constant\n    /// initializer.\n    bool IsConstantAggregate;\n\n    /// Non-null if we should use lifetime annotations.\n    llvm::Value *SizeForLifetimeMarkers;\n\n    /// Address with original alloca instruction. Invalid if the variable was\n    /// emitted as a global constant.\n    Address AllocaAddr;\n\n    struct Invalid {};\n    AutoVarEmission(Invalid)\n        : Variable(nullptr), Addr(Address::invalid()),\n          AllocaAddr(Address::invalid()) {}\n\n    AutoVarEmission(const VarDecl &variable)\n        : Variable(&variable), Addr(Address::invalid()), NRVOFlag(nullptr),\n          IsEscapingByRef(false), IsConstantAggregate(false),\n          SizeForLifetimeMarkers(nullptr), AllocaAddr(Address::invalid()) {}\n\n    bool wasEmittedAsGlobal() const { return !Addr.isValid(); }\n\n  public:\n    static AutoVarEmission invalid() { return AutoVarEmission(Invalid()); }\n\n    bool useLifetimeMarkers() const {\n      return SizeForLifetimeMarkers != nullptr;\n    }\n    llvm::Value *getSizeForLifetimeMarkers() const {\n      assert(useLifetimeMarkers());\n      return SizeForLifetimeMarkers;\n    }\n\n    /// Returns the raw, allocated address, which is not necessarily\n    /// the address of the object itself. It is casted to default\n    /// address space for address space agnostic languages.\n    Address getAllocatedAddress() const {\n      return Addr;\n    }\n\n    /// Returns the address for the original alloca instruction.\n    Address getOriginalAllocatedAddress() const { return AllocaAddr; }\n\n    /// Returns the address of the object within this declaration.\n    /// Note that this does not chase the forwarding pointer for\n    /// __block decls.\n    Address getObjectAddress(CodeGenFunction &CGF) const {\n      if (!IsEscapingByRef) return Addr;\n\n      return CGF.emitBlockByrefAddress(Addr, Variable, /*forward*/ false);\n    }\n  };\n  AutoVarEmission EmitAutoVarAlloca(const VarDecl &var);\n  void EmitAutoVarInit(const AutoVarEmission &emission);\n  void EmitAutoVarCleanups(const AutoVarEmission &emission);\n  void emitAutoVarTypeCleanup(const AutoVarEmission &emission,\n                              QualType::DestructionKind dtorKind);\n\n  /// Emits the alloca and debug information for the size expressions for each\n  /// dimension of an array. It registers the association of its (1-dimensional)\n  /// QualTypes and size expression's debug node, so that CGDebugInfo can\n  /// reference this node when creating the DISubrange object to describe the\n  /// array types.\n  void EmitAndRegisterVariableArrayDimensions(CGDebugInfo *DI,\n                                              const VarDecl &D,\n                                              bool EmitDebugInfo);\n\n  void EmitStaticVarDecl(const VarDecl &D,\n                         llvm::GlobalValue::LinkageTypes Linkage);\n\n  class ParamValue {\n    llvm::Value *Value;\n    unsigned Alignment;\n    ParamValue(llvm::Value *V, unsigned A) : Value(V), Alignment(A) {}\n  public:\n    static ParamValue forDirect(llvm::Value *value) {\n      return ParamValue(value, 0);\n    }\n    static ParamValue forIndirect(Address addr) {\n      assert(!addr.getAlignment().isZero());\n      return ParamValue(addr.getPointer(), addr.getAlignment().getQuantity());\n    }\n\n    bool isIndirect() const { return Alignment != 0; }\n    llvm::Value *getAnyValue() const { return Value; }\n\n    llvm::Value *getDirectValue() const {\n      assert(!isIndirect());\n      return Value;\n    }\n\n    Address getIndirectAddress() const {\n      assert(isIndirect());\n      return Address(Value, CharUnits::fromQuantity(Alignment));\n    }\n  };\n\n  /// EmitParmDecl - Emit a ParmVarDecl or an ImplicitParamDecl.\n  void EmitParmDecl(const VarDecl &D, ParamValue Arg, unsigned ArgNo);\n\n  /// protectFromPeepholes - Protect a value that we're intending to\n  /// store to the side, but which will probably be used later, from\n  /// aggressive peepholing optimizations that might delete it.\n  ///\n  /// Pass the result to unprotectFromPeepholes to declare that\n  /// protection is no longer required.\n  ///\n  /// There's no particular reason why this shouldn't apply to\n  /// l-values, it's just that no existing peepholes work on pointers.\n  PeepholeProtection protectFromPeepholes(RValue rvalue);\n  void unprotectFromPeepholes(PeepholeProtection protection);\n\n  void emitAlignmentAssumptionCheck(llvm::Value *Ptr, QualType Ty,\n                                    SourceLocation Loc,\n                                    SourceLocation AssumptionLoc,\n                                    llvm::Value *Alignment,\n                                    llvm::Value *OffsetValue,\n                                    llvm::Value *TheCheck,\n                                    llvm::Instruction *Assumption);\n\n  void emitAlignmentAssumption(llvm::Value *PtrValue, QualType Ty,\n                               SourceLocation Loc, SourceLocation AssumptionLoc,\n                               llvm::Value *Alignment,\n                               llvm::Value *OffsetValue = nullptr);\n\n  void emitAlignmentAssumption(llvm::Value *PtrValue, const Expr *E,\n                               SourceLocation AssumptionLoc,\n                               llvm::Value *Alignment,\n                               llvm::Value *OffsetValue = nullptr);\n\n  //===--------------------------------------------------------------------===//\n  //                             Statement Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitStopPoint - Emit a debug stoppoint if we are emitting debug info.\n  void EmitStopPoint(const Stmt *S);\n\n  /// EmitStmt - Emit the code for the statement \\arg S. It is legal to call\n  /// this function even if there is no current insertion point.\n  ///\n  /// This function may clear the current insertion point; callers should use\n  /// EnsureInsertPoint if they wish to subsequently generate code without first\n  /// calling EmitBlock, EmitBranch, or EmitStmt.\n  void EmitStmt(const Stmt *S, ArrayRef<const Attr *> Attrs = None);\n\n  /// EmitSimpleStmt - Try to emit a \"simple\" statement which does not\n  /// necessarily require an insertion point or debug information; typically\n  /// because the statement amounts to a jump or a container of other\n  /// statements.\n  ///\n  /// \\return True if the statement was handled.\n  bool EmitSimpleStmt(const Stmt *S, ArrayRef<const Attr *> Attrs);\n\n  Address EmitCompoundStmt(const CompoundStmt &S, bool GetLast = false,\n                           AggValueSlot AVS = AggValueSlot::ignored());\n  Address EmitCompoundStmtWithoutScope(const CompoundStmt &S,\n                                       bool GetLast = false,\n                                       AggValueSlot AVS =\n                                                AggValueSlot::ignored());\n\n  /// EmitLabel - Emit the block for the given label. It is legal to call this\n  /// function even if there is no current insertion point.\n  void EmitLabel(const LabelDecl *D); // helper for EmitLabelStmt.\n\n  void EmitLabelStmt(const LabelStmt &S);\n  void EmitAttributedStmt(const AttributedStmt &S);\n  void EmitGotoStmt(const GotoStmt &S);\n  void EmitIndirectGotoStmt(const IndirectGotoStmt &S);\n  void EmitIfStmt(const IfStmt &S);\n\n  void EmitWhileStmt(const WhileStmt &S,\n                     ArrayRef<const Attr *> Attrs = None);\n  void EmitDoStmt(const DoStmt &S, ArrayRef<const Attr *> Attrs = None);\n  void EmitForStmt(const ForStmt &S,\n                   ArrayRef<const Attr *> Attrs = None);\n  void EmitReturnStmt(const ReturnStmt &S);\n  void EmitDeclStmt(const DeclStmt &S);\n  void EmitBreakStmt(const BreakStmt &S);\n  void EmitContinueStmt(const ContinueStmt &S);\n  void EmitSwitchStmt(const SwitchStmt &S);\n  void EmitDefaultStmt(const DefaultStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitCaseStmt(const CaseStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitCaseStmtRange(const CaseStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitAsmStmt(const AsmStmt &S);\n\n  void EmitObjCForCollectionStmt(const ObjCForCollectionStmt &S);\n  void EmitObjCAtTryStmt(const ObjCAtTryStmt &S);\n  void EmitObjCAtThrowStmt(const ObjCAtThrowStmt &S);\n  void EmitObjCAtSynchronizedStmt(const ObjCAtSynchronizedStmt &S);\n  void EmitObjCAutoreleasePoolStmt(const ObjCAutoreleasePoolStmt &S);\n\n  void EmitCoroutineBody(const CoroutineBodyStmt &S);\n  void EmitCoreturnStmt(const CoreturnStmt &S);\n  RValue EmitCoawaitExpr(const CoawaitExpr &E,\n                         AggValueSlot aggSlot = AggValueSlot::ignored(),\n                         bool ignoreResult = false);\n  LValue EmitCoawaitLValue(const CoawaitExpr *E);\n  RValue EmitCoyieldExpr(const CoyieldExpr &E,\n                         AggValueSlot aggSlot = AggValueSlot::ignored(),\n                         bool ignoreResult = false);\n  LValue EmitCoyieldLValue(const CoyieldExpr *E);\n  RValue EmitCoroutineIntrinsic(const CallExpr *E, unsigned int IID);\n\n  void EnterCXXTryStmt(const CXXTryStmt &S, bool IsFnTryBlock = false);\n  void ExitCXXTryStmt(const CXXTryStmt &S, bool IsFnTryBlock = false);\n\n  void EmitCXXTryStmt(const CXXTryStmt &S);\n  void EmitSEHTryStmt(const SEHTryStmt &S);\n  void EmitSEHLeaveStmt(const SEHLeaveStmt &S);\n  void EnterSEHTryStmt(const SEHTryStmt &S);\n  void ExitSEHTryStmt(const SEHTryStmt &S);\n\n  void pushSEHCleanup(CleanupKind kind,\n                      llvm::Function *FinallyFunc);\n  void startOutlinedSEHHelper(CodeGenFunction &ParentCGF, bool IsFilter,\n                              const Stmt *OutlinedStmt);\n\n  llvm::Function *GenerateSEHFilterFunction(CodeGenFunction &ParentCGF,\n                                            const SEHExceptStmt &Except);\n\n  llvm::Function *GenerateSEHFinallyFunction(CodeGenFunction &ParentCGF,\n                                             const SEHFinallyStmt &Finally);\n\n  void EmitSEHExceptionCodeSave(CodeGenFunction &ParentCGF,\n                                llvm::Value *ParentFP,\n                                llvm::Value *EntryEBP);\n  llvm::Value *EmitSEHExceptionCode();\n  llvm::Value *EmitSEHExceptionInfo();\n  llvm::Value *EmitSEHAbnormalTermination();\n\n  /// Emit simple code for OpenMP directives in Simd-only mode.\n  void EmitSimpleOMPExecutableDirective(const OMPExecutableDirective &D);\n\n  /// Scan the outlined statement for captures from the parent function. For\n  /// each capture, mark the capture as escaped and emit a call to\n  /// llvm.localrecover. Insert the localrecover result into the LocalDeclMap.\n  void EmitCapturedLocals(CodeGenFunction &ParentCGF, const Stmt *OutlinedStmt,\n                          bool IsFilter);\n\n  /// Recovers the address of a local in a parent function. ParentVar is the\n  /// address of the variable used in the immediate parent function. It can\n  /// either be an alloca or a call to llvm.localrecover if there are nested\n  /// outlined functions. ParentFP is the frame pointer of the outermost parent\n  /// frame.\n  Address recoverAddrOfEscapedLocal(CodeGenFunction &ParentCGF,\n                                    Address ParentVar,\n                                    llvm::Value *ParentFP);\n\n  void EmitCXXForRangeStmt(const CXXForRangeStmt &S,\n                           ArrayRef<const Attr *> Attrs = None);\n\n  /// Controls insertion of cancellation exit blocks in worksharing constructs.\n  class OMPCancelStackRAII {\n    CodeGenFunction &CGF;\n\n  public:\n    OMPCancelStackRAII(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,\n                       bool HasCancel)\n        : CGF(CGF) {\n      CGF.OMPCancelStack.enter(CGF, Kind, HasCancel);\n    }\n    ~OMPCancelStackRAII() { CGF.OMPCancelStack.exit(CGF); }\n  };\n\n  /// Returns calculated size of the specified type.\n  llvm::Value *getTypeSize(QualType Ty);\n  LValue InitCapturedStruct(const CapturedStmt &S);\n  llvm::Function *EmitCapturedStmt(const CapturedStmt &S, CapturedRegionKind K);\n  llvm::Function *GenerateCapturedStmtFunction(const CapturedStmt &S);\n  Address GenerateCapturedStmtArgument(const CapturedStmt &S);\n  llvm::Function *GenerateOpenMPCapturedStmtFunction(const CapturedStmt &S,\n                                                     SourceLocation Loc);\n  void GenerateOpenMPCapturedVars(const CapturedStmt &S,\n                                  SmallVectorImpl<llvm::Value *> &CapturedVars);\n  void emitOMPSimpleStore(LValue LVal, RValue RVal, QualType RValTy,\n                          SourceLocation Loc);\n  /// Perform element by element copying of arrays with type \\a\n  /// OriginalType from \\a SrcAddr to \\a DestAddr using copying procedure\n  /// generated by \\a CopyGen.\n  ///\n  /// \\param DestAddr Address of the destination array.\n  /// \\param SrcAddr Address of the source array.\n  /// \\param OriginalType Type of destination and source arrays.\n  /// \\param CopyGen Copying procedure that copies value of single array element\n  /// to another single array element.\n  void EmitOMPAggregateAssign(\n      Address DestAddr, Address SrcAddr, QualType OriginalType,\n      const llvm::function_ref<void(Address, Address)> CopyGen);\n  /// Emit proper copying of data from one variable to another.\n  ///\n  /// \\param OriginalType Original type of the copied variables.\n  /// \\param DestAddr Destination address.\n  /// \\param SrcAddr Source address.\n  /// \\param DestVD Destination variable used in \\a CopyExpr (for arrays, has\n  /// type of the base array element).\n  /// \\param SrcVD Source variable used in \\a CopyExpr (for arrays, has type of\n  /// the base array element).\n  /// \\param Copy Actual copygin expression for copying data from \\a SrcVD to \\a\n  /// DestVD.\n  void EmitOMPCopy(QualType OriginalType,\n                   Address DestAddr, Address SrcAddr,\n                   const VarDecl *DestVD, const VarDecl *SrcVD,\n                   const Expr *Copy);\n  /// Emit atomic update code for constructs: \\a X = \\a X \\a BO \\a E or\n  /// \\a X = \\a E \\a BO \\a E.\n  ///\n  /// \\param X Value to be updated.\n  /// \\param E Update value.\n  /// \\param BO Binary operation for update operation.\n  /// \\param IsXLHSInRHSPart true if \\a X is LHS in RHS part of the update\n  /// expression, false otherwise.\n  /// \\param AO Atomic ordering of the generated atomic instructions.\n  /// \\param CommonGen Code generator for complex expressions that cannot be\n  /// expressed through atomicrmw instruction.\n  /// \\returns <true, OldAtomicValue> if simple 'atomicrmw' instruction was\n  /// generated, <false, RValue::get(nullptr)> otherwise.\n  std::pair<bool, RValue> EmitOMPAtomicSimpleUpdateExpr(\n      LValue X, RValue E, BinaryOperatorKind BO, bool IsXLHSInRHSPart,\n      llvm::AtomicOrdering AO, SourceLocation Loc,\n      const llvm::function_ref<RValue(RValue)> CommonGen);\n  bool EmitOMPFirstprivateClause(const OMPExecutableDirective &D,\n                                 OMPPrivateScope &PrivateScope);\n  void EmitOMPPrivateClause(const OMPExecutableDirective &D,\n                            OMPPrivateScope &PrivateScope);\n  void EmitOMPUseDevicePtrClause(\n      const OMPUseDevicePtrClause &C, OMPPrivateScope &PrivateScope,\n      const llvm::DenseMap<const ValueDecl *, Address> &CaptureDeviceAddrMap);\n  void EmitOMPUseDeviceAddrClause(\n      const OMPUseDeviceAddrClause &C, OMPPrivateScope &PrivateScope,\n      const llvm::DenseMap<const ValueDecl *, Address> &CaptureDeviceAddrMap);\n  /// Emit code for copyin clause in \\a D directive. The next code is\n  /// generated at the start of outlined functions for directives:\n  /// \\code\n  /// threadprivate_var1 = master_threadprivate_var1;\n  /// operator=(threadprivate_var2, master_threadprivate_var2);\n  /// ...\n  /// __kmpc_barrier(&loc, global_tid);\n  /// \\endcode\n  ///\n  /// \\param D OpenMP directive possibly with 'copyin' clause(s).\n  /// \\returns true if at least one copyin variable is found, false otherwise.\n  bool EmitOMPCopyinClause(const OMPExecutableDirective &D);\n  /// Emit initial code for lastprivate variables. If some variable is\n  /// not also firstprivate, then the default initialization is used. Otherwise\n  /// initialization of this variable is performed by EmitOMPFirstprivateClause\n  /// method.\n  ///\n  /// \\param D Directive that may have 'lastprivate' directives.\n  /// \\param PrivateScope Private scope for capturing lastprivate variables for\n  /// proper codegen in internal captured statement.\n  ///\n  /// \\returns true if there is at least one lastprivate variable, false\n  /// otherwise.\n  bool EmitOMPLastprivateClauseInit(const OMPExecutableDirective &D,\n                                    OMPPrivateScope &PrivateScope);\n  /// Emit final copying of lastprivate values to original variables at\n  /// the end of the worksharing or simd directive.\n  ///\n  /// \\param D Directive that has at least one 'lastprivate' directives.\n  /// \\param IsLastIterCond Boolean condition that must be set to 'i1 true' if\n  /// it is the last iteration of the loop code in associated directive, or to\n  /// 'i1 false' otherwise. If this item is nullptr, no final check is required.\n  void EmitOMPLastprivateClauseFinal(const OMPExecutableDirective &D,\n                                     bool NoFinals,\n                                     llvm::Value *IsLastIterCond = nullptr);\n  /// Emit initial code for linear clauses.\n  void EmitOMPLinearClause(const OMPLoopDirective &D,\n                           CodeGenFunction::OMPPrivateScope &PrivateScope);\n  /// Emit final code for linear clauses.\n  /// \\param CondGen Optional conditional code for final part of codegen for\n  /// linear clause.\n  void EmitOMPLinearClauseFinal(\n      const OMPLoopDirective &D,\n      const llvm::function_ref<llvm::Value *(CodeGenFunction &)> CondGen);\n  /// Emit initial code for reduction variables. Creates reduction copies\n  /// and initializes them with the values according to OpenMP standard.\n  ///\n  /// \\param D Directive (possibly) with the 'reduction' clause.\n  /// \\param PrivateScope Private scope for capturing reduction variables for\n  /// proper codegen in internal captured statement.\n  ///\n  void EmitOMPReductionClauseInit(const OMPExecutableDirective &D,\n                                  OMPPrivateScope &PrivateScope,\n                                  bool ForInscan = false);\n  /// Emit final update of reduction values to original variables at\n  /// the end of the directive.\n  ///\n  /// \\param D Directive that has at least one 'reduction' directives.\n  /// \\param ReductionKind The kind of reduction to perform.\n  void EmitOMPReductionClauseFinal(const OMPExecutableDirective &D,\n                                   const OpenMPDirectiveKind ReductionKind);\n  /// Emit initial code for linear variables. Creates private copies\n  /// and initializes them with the values according to OpenMP standard.\n  ///\n  /// \\param D Directive (possibly) with the 'linear' clause.\n  /// \\return true if at least one linear variable is found that should be\n  /// initialized with the value of the original variable, false otherwise.\n  bool EmitOMPLinearClauseInit(const OMPLoopDirective &D);\n\n  typedef const llvm::function_ref<void(CodeGenFunction & /*CGF*/,\n                                        llvm::Function * /*OutlinedFn*/,\n                                        const OMPTaskDataTy & /*Data*/)>\n      TaskGenTy;\n  void EmitOMPTaskBasedDirective(const OMPExecutableDirective &S,\n                                 const OpenMPDirectiveKind CapturedRegion,\n                                 const RegionCodeGenTy &BodyGen,\n                                 const TaskGenTy &TaskGen, OMPTaskDataTy &Data);\n  struct OMPTargetDataInfo {\n    Address BasePointersArray = Address::invalid();\n    Address PointersArray = Address::invalid();\n    Address SizesArray = Address::invalid();\n    Address MappersArray = Address::invalid();\n    unsigned NumberOfTargetItems = 0;\n    explicit OMPTargetDataInfo() = default;\n    OMPTargetDataInfo(Address BasePointersArray, Address PointersArray,\n                      Address SizesArray, Address MappersArray,\n                      unsigned NumberOfTargetItems)\n        : BasePointersArray(BasePointersArray), PointersArray(PointersArray),\n          SizesArray(SizesArray), MappersArray(MappersArray),\n          NumberOfTargetItems(NumberOfTargetItems) {}\n  };\n  void EmitOMPTargetTaskBasedDirective(const OMPExecutableDirective &S,\n                                       const RegionCodeGenTy &BodyGen,\n                                       OMPTargetDataInfo &InputInfo);\n\n  void EmitOMPParallelDirective(const OMPParallelDirective &S);\n  void EmitOMPSimdDirective(const OMPSimdDirective &S);\n  void EmitOMPTileDirective(const OMPTileDirective &S);\n  void EmitOMPForDirective(const OMPForDirective &S);\n  void EmitOMPForSimdDirective(const OMPForSimdDirective &S);\n  void EmitOMPSectionsDirective(const OMPSectionsDirective &S);\n  void EmitOMPSectionDirective(const OMPSectionDirective &S);\n  void EmitOMPSingleDirective(const OMPSingleDirective &S);\n  void EmitOMPMasterDirective(const OMPMasterDirective &S);\n  void EmitOMPCriticalDirective(const OMPCriticalDirective &S);\n  void EmitOMPParallelForDirective(const OMPParallelForDirective &S);\n  void EmitOMPParallelForSimdDirective(const OMPParallelForSimdDirective &S);\n  void EmitOMPParallelSectionsDirective(const OMPParallelSectionsDirective &S);\n  void EmitOMPParallelMasterDirective(const OMPParallelMasterDirective &S);\n  void EmitOMPTaskDirective(const OMPTaskDirective &S);\n  void EmitOMPTaskyieldDirective(const OMPTaskyieldDirective &S);\n  void EmitOMPBarrierDirective(const OMPBarrierDirective &S);\n  void EmitOMPTaskwaitDirective(const OMPTaskwaitDirective &S);\n  void EmitOMPTaskgroupDirective(const OMPTaskgroupDirective &S);\n  void EmitOMPFlushDirective(const OMPFlushDirective &S);\n  void EmitOMPDepobjDirective(const OMPDepobjDirective &S);\n  void EmitOMPScanDirective(const OMPScanDirective &S);\n  void EmitOMPOrderedDirective(const OMPOrderedDirective &S);\n  void EmitOMPAtomicDirective(const OMPAtomicDirective &S);\n  void EmitOMPTargetDirective(const OMPTargetDirective &S);\n  void EmitOMPTargetDataDirective(const OMPTargetDataDirective &S);\n  void EmitOMPTargetEnterDataDirective(const OMPTargetEnterDataDirective &S);\n  void EmitOMPTargetExitDataDirective(const OMPTargetExitDataDirective &S);\n  void EmitOMPTargetUpdateDirective(const OMPTargetUpdateDirective &S);\n  void EmitOMPTargetParallelDirective(const OMPTargetParallelDirective &S);\n  void\n  EmitOMPTargetParallelForDirective(const OMPTargetParallelForDirective &S);\n  void EmitOMPTeamsDirective(const OMPTeamsDirective &S);\n  void\n  EmitOMPCancellationPointDirective(const OMPCancellationPointDirective &S);\n  void EmitOMPCancelDirective(const OMPCancelDirective &S);\n  void EmitOMPTaskLoopBasedDirective(const OMPLoopDirective &S);\n  void EmitOMPTaskLoopDirective(const OMPTaskLoopDirective &S);\n  void EmitOMPTaskLoopSimdDirective(const OMPTaskLoopSimdDirective &S);\n  void EmitOMPMasterTaskLoopDirective(const OMPMasterTaskLoopDirective &S);\n  void\n  EmitOMPMasterTaskLoopSimdDirective(const OMPMasterTaskLoopSimdDirective &S);\n  void EmitOMPParallelMasterTaskLoopDirective(\n      const OMPParallelMasterTaskLoopDirective &S);\n  void EmitOMPParallelMasterTaskLoopSimdDirective(\n      const OMPParallelMasterTaskLoopSimdDirective &S);\n  void EmitOMPDistributeDirective(const OMPDistributeDirective &S);\n  void EmitOMPDistributeParallelForDirective(\n      const OMPDistributeParallelForDirective &S);\n  void EmitOMPDistributeParallelForSimdDirective(\n      const OMPDistributeParallelForSimdDirective &S);\n  void EmitOMPDistributeSimdDirective(const OMPDistributeSimdDirective &S);\n  void EmitOMPTargetParallelForSimdDirective(\n      const OMPTargetParallelForSimdDirective &S);\n  void EmitOMPTargetSimdDirective(const OMPTargetSimdDirective &S);\n  void EmitOMPTeamsDistributeDirective(const OMPTeamsDistributeDirective &S);\n  void\n  EmitOMPTeamsDistributeSimdDirective(const OMPTeamsDistributeSimdDirective &S);\n  void EmitOMPTeamsDistributeParallelForSimdDirective(\n      const OMPTeamsDistributeParallelForSimdDirective &S);\n  void EmitOMPTeamsDistributeParallelForDirective(\n      const OMPTeamsDistributeParallelForDirective &S);\n  void EmitOMPTargetTeamsDirective(const OMPTargetTeamsDirective &S);\n  void EmitOMPTargetTeamsDistributeDirective(\n      const OMPTargetTeamsDistributeDirective &S);\n  void EmitOMPTargetTeamsDistributeParallelForDirective(\n      const OMPTargetTeamsDistributeParallelForDirective &S);\n  void EmitOMPTargetTeamsDistributeParallelForSimdDirective(\n      const OMPTargetTeamsDistributeParallelForSimdDirective &S);\n  void EmitOMPTargetTeamsDistributeSimdDirective(\n      const OMPTargetTeamsDistributeSimdDirective &S);\n\n  /// Emit device code for the target directive.\n  static void EmitOMPTargetDeviceFunction(CodeGenModule &CGM,\n                                          StringRef ParentName,\n                                          const OMPTargetDirective &S);\n  static void\n  EmitOMPTargetParallelDeviceFunction(CodeGenModule &CGM, StringRef ParentName,\n                                      const OMPTargetParallelDirective &S);\n  /// Emit device code for the target parallel for directive.\n  static void EmitOMPTargetParallelForDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetParallelForDirective &S);\n  /// Emit device code for the target parallel for simd directive.\n  static void EmitOMPTargetParallelForSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetParallelForSimdDirective &S);\n  /// Emit device code for the target teams directive.\n  static void\n  EmitOMPTargetTeamsDeviceFunction(CodeGenModule &CGM, StringRef ParentName,\n                                   const OMPTargetTeamsDirective &S);\n  /// Emit device code for the target teams distribute directive.\n  static void EmitOMPTargetTeamsDistributeDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeDirective &S);\n  /// Emit device code for the target teams distribute simd directive.\n  static void EmitOMPTargetTeamsDistributeSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeSimdDirective &S);\n  /// Emit device code for the target simd directive.\n  static void EmitOMPTargetSimdDeviceFunction(CodeGenModule &CGM,\n                                              StringRef ParentName,\n                                              const OMPTargetSimdDirective &S);\n  /// Emit device code for the target teams distribute parallel for simd\n  /// directive.\n  static void EmitOMPTargetTeamsDistributeParallelForSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeParallelForSimdDirective &S);\n\n  static void EmitOMPTargetTeamsDistributeParallelForDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeParallelForDirective &S);\n\n  /// Emit the Stmt \\p S and return its topmost canonical loop, if any.\n  /// TODO: The \\p Depth paramter is not yet implemented and must be 1. In the\n  /// future it is meant to be the number of loops expected in the loop nests\n  /// (usually specified by the \"collapse\" clause) that are collapsed to a\n  /// single loop by this function.\n  llvm::CanonicalLoopInfo *EmitOMPCollapsedCanonicalLoopNest(const Stmt *S,\n                                                             int Depth);\n\n  /// Emit an OMPCanonicalLoop using the OpenMPIRBuilder.\n  void EmitOMPCanonicalLoop(const OMPCanonicalLoop *S);\n\n  /// Emit inner loop of the worksharing/simd construct.\n  ///\n  /// \\param S Directive, for which the inner loop must be emitted.\n  /// \\param RequiresCleanup true, if directive has some associated private\n  /// variables.\n  /// \\param LoopCond Bollean condition for loop continuation.\n  /// \\param IncExpr Increment expression for loop control variable.\n  /// \\param BodyGen Generator for the inner body of the inner loop.\n  /// \\param PostIncGen Genrator for post-increment code (required for ordered\n  /// loop directvies).\n  void EmitOMPInnerLoop(\n      const OMPExecutableDirective &S, bool RequiresCleanup,\n      const Expr *LoopCond, const Expr *IncExpr,\n      const llvm::function_ref<void(CodeGenFunction &)> BodyGen,\n      const llvm::function_ref<void(CodeGenFunction &)> PostIncGen);\n\n  JumpDest getOMPCancelDestination(OpenMPDirectiveKind Kind);\n  /// Emit initial code for loop counters of loop-based directives.\n  void EmitOMPPrivateLoopCounters(const OMPLoopDirective &S,\n                                  OMPPrivateScope &LoopScope);\n\n  /// Helper for the OpenMP loop directives.\n  void EmitOMPLoopBody(const OMPLoopDirective &D, JumpDest LoopExit);\n\n  /// Emit code for the worksharing loop-based directive.\n  /// \\return true, if this construct has any lastprivate clause, false -\n  /// otherwise.\n  bool EmitOMPWorksharingLoop(const OMPLoopDirective &S, Expr *EUB,\n                              const CodeGenLoopBoundsTy &CodeGenLoopBounds,\n                              const CodeGenDispatchBoundsTy &CGDispatchBounds);\n\n  /// Emit code for the distribute loop-based directive.\n  void EmitOMPDistributeLoop(const OMPLoopDirective &S,\n                             const CodeGenLoopTy &CodeGenLoop, Expr *IncExpr);\n\n  /// Helpers for the OpenMP loop directives.\n  void EmitOMPSimdInit(const OMPLoopDirective &D, bool IsMonotonic = false);\n  void EmitOMPSimdFinal(\n      const OMPLoopDirective &D,\n      const llvm::function_ref<llvm::Value *(CodeGenFunction &)> CondGen);\n\n  /// Emits the lvalue for the expression with possibly captured variable.\n  LValue EmitOMPSharedLValue(const Expr *E);\n\nprivate:\n  /// Helpers for blocks.\n  llvm::Value *EmitBlockLiteral(const CGBlockInfo &Info);\n\n  /// struct with the values to be passed to the OpenMP loop-related functions\n  struct OMPLoopArguments {\n    /// loop lower bound\n    Address LB = Address::invalid();\n    /// loop upper bound\n    Address UB = Address::invalid();\n    /// loop stride\n    Address ST = Address::invalid();\n    /// isLastIteration argument for runtime functions\n    Address IL = Address::invalid();\n    /// Chunk value generated by sema\n    llvm::Value *Chunk = nullptr;\n    /// EnsureUpperBound\n    Expr *EUB = nullptr;\n    /// IncrementExpression\n    Expr *IncExpr = nullptr;\n    /// Loop initialization\n    Expr *Init = nullptr;\n    /// Loop exit condition\n    Expr *Cond = nullptr;\n    /// Update of LB after a whole chunk has been executed\n    Expr *NextLB = nullptr;\n    /// Update of UB after a whole chunk has been executed\n    Expr *NextUB = nullptr;\n    OMPLoopArguments() = default;\n    OMPLoopArguments(Address LB, Address UB, Address ST, Address IL,\n                     llvm::Value *Chunk = nullptr, Expr *EUB = nullptr,\n                     Expr *IncExpr = nullptr, Expr *Init = nullptr,\n                     Expr *Cond = nullptr, Expr *NextLB = nullptr,\n                     Expr *NextUB = nullptr)\n        : LB(LB), UB(UB), ST(ST), IL(IL), Chunk(Chunk), EUB(EUB),\n          IncExpr(IncExpr), Init(Init), Cond(Cond), NextLB(NextLB),\n          NextUB(NextUB) {}\n  };\n  void EmitOMPOuterLoop(bool DynamicOrOrdered, bool IsMonotonic,\n                        const OMPLoopDirective &S, OMPPrivateScope &LoopScope,\n                        const OMPLoopArguments &LoopArgs,\n                        const CodeGenLoopTy &CodeGenLoop,\n                        const CodeGenOrderedTy &CodeGenOrdered);\n  void EmitOMPForOuterLoop(const OpenMPScheduleTy &ScheduleKind,\n                           bool IsMonotonic, const OMPLoopDirective &S,\n                           OMPPrivateScope &LoopScope, bool Ordered,\n                           const OMPLoopArguments &LoopArgs,\n                           const CodeGenDispatchBoundsTy &CGDispatchBounds);\n  void EmitOMPDistributeOuterLoop(OpenMPDistScheduleClauseKind ScheduleKind,\n                                  const OMPLoopDirective &S,\n                                  OMPPrivateScope &LoopScope,\n                                  const OMPLoopArguments &LoopArgs,\n                                  const CodeGenLoopTy &CodeGenLoopContent);\n  /// Emit code for sections directive.\n  void EmitSections(const OMPExecutableDirective &S);\n\npublic:\n\n  //===--------------------------------------------------------------------===//\n  //                         LValue Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  /// Create a check that a scalar RValue is non-null.\n  llvm::Value *EmitNonNullRValueCheck(RValue RV, QualType T);\n\n  /// GetUndefRValue - Get an appropriate 'undef' rvalue for the given type.\n  RValue GetUndefRValue(QualType Ty);\n\n  /// EmitUnsupportedRValue - Emit a dummy r-value using the type of E\n  /// and issue an ErrorUnsupported style diagnostic (using the\n  /// provided Name).\n  RValue EmitUnsupportedRValue(const Expr *E,\n                               const char *Name);\n\n  /// EmitUnsupportedLValue - Emit a dummy l-value using the type of E and issue\n  /// an ErrorUnsupported style diagnostic (using the provided Name).\n  LValue EmitUnsupportedLValue(const Expr *E,\n                               const char *Name);\n\n  /// EmitLValue - Emit code to compute a designator that specifies the location\n  /// of the expression.\n  ///\n  /// This can return one of two things: a simple address or a bitfield\n  /// reference.  In either case, the LLVM Value* in the LValue structure is\n  /// guaranteed to be an LLVM pointer type.\n  ///\n  /// If this returns a bitfield reference, nothing about the pointee type of\n  /// the LLVM value is known: For example, it may not be a pointer to an\n  /// integer.\n  ///\n  /// If this returns a normal address, and if the lvalue's C type is fixed\n  /// size, this method guarantees that the returned pointer type will point to\n  /// an LLVM type of the same size of the lvalue's type.  If the lvalue has a\n  /// variable length type, this is not possible.\n  ///\n  LValue EmitLValue(const Expr *E);\n\n  /// Same as EmitLValue but additionally we generate checking code to\n  /// guard against undefined behavior.  This is only suitable when we know\n  /// that the address will be used to access the object.\n  LValue EmitCheckedLValue(const Expr *E, TypeCheckKind TCK);\n\n  RValue convertTempToRValue(Address addr, QualType type,\n                             SourceLocation Loc);\n\n  void EmitAtomicInit(Expr *E, LValue lvalue);\n\n  bool LValueIsSuitableForInlineAtomic(LValue Src);\n\n  RValue EmitAtomicLoad(LValue LV, SourceLocation SL,\n                        AggValueSlot Slot = AggValueSlot::ignored());\n\n  RValue EmitAtomicLoad(LValue lvalue, SourceLocation loc,\n                        llvm::AtomicOrdering AO, bool IsVolatile = false,\n                        AggValueSlot slot = AggValueSlot::ignored());\n\n  void EmitAtomicStore(RValue rvalue, LValue lvalue, bool isInit);\n\n  void EmitAtomicStore(RValue rvalue, LValue lvalue, llvm::AtomicOrdering AO,\n                       bool IsVolatile, bool isInit);\n\n  std::pair<RValue, llvm::Value *> EmitAtomicCompareExchange(\n      LValue Obj, RValue Expected, RValue Desired, SourceLocation Loc,\n      llvm::AtomicOrdering Success =\n          llvm::AtomicOrdering::SequentiallyConsistent,\n      llvm::AtomicOrdering Failure =\n          llvm::AtomicOrdering::SequentiallyConsistent,\n      bool IsWeak = false, AggValueSlot Slot = AggValueSlot::ignored());\n\n  void EmitAtomicUpdate(LValue LVal, llvm::AtomicOrdering AO,\n                        const llvm::function_ref<RValue(RValue)> &UpdateOp,\n                        bool IsVolatile);\n\n  /// EmitToMemory - Change a scalar value from its value\n  /// representation to its in-memory representation.\n  llvm::Value *EmitToMemory(llvm::Value *Value, QualType Ty);\n\n  /// EmitFromMemory - Change a scalar value from its memory\n  /// representation to its value representation.\n  llvm::Value *EmitFromMemory(llvm::Value *Value, QualType Ty);\n\n  /// Check if the scalar \\p Value is within the valid range for the given\n  /// type \\p Ty.\n  ///\n  /// Returns true if a check is needed (even if the range is unknown).\n  bool EmitScalarRangeCheck(llvm::Value *Value, QualType Ty,\n                            SourceLocation Loc);\n\n  /// EmitLoadOfScalar - Load a scalar value from an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.\n  llvm::Value *EmitLoadOfScalar(Address Addr, bool Volatile, QualType Ty,\n                                SourceLocation Loc,\n                                AlignmentSource Source = AlignmentSource::Type,\n                                bool isNontemporal = false) {\n    return EmitLoadOfScalar(Addr, Volatile, Ty, Loc, LValueBaseInfo(Source),\n                            CGM.getTBAAAccessInfo(Ty), isNontemporal);\n  }\n\n  llvm::Value *EmitLoadOfScalar(Address Addr, bool Volatile, QualType Ty,\n                                SourceLocation Loc, LValueBaseInfo BaseInfo,\n                                TBAAAccessInfo TBAAInfo,\n                                bool isNontemporal = false);\n\n  /// EmitLoadOfScalar - Load a scalar value from an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.  The l-value must be a simple\n  /// l-value.\n  llvm::Value *EmitLoadOfScalar(LValue lvalue, SourceLocation Loc);\n\n  /// EmitStoreOfScalar - Store a scalar value to an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.\n  void EmitStoreOfScalar(llvm::Value *Value, Address Addr,\n                         bool Volatile, QualType Ty,\n                         AlignmentSource Source = AlignmentSource::Type,\n                         bool isInit = false, bool isNontemporal = false) {\n    EmitStoreOfScalar(Value, Addr, Volatile, Ty, LValueBaseInfo(Source),\n                      CGM.getTBAAAccessInfo(Ty), isInit, isNontemporal);\n  }\n\n  void EmitStoreOfScalar(llvm::Value *Value, Address Addr,\n                         bool Volatile, QualType Ty,\n                         LValueBaseInfo BaseInfo, TBAAAccessInfo TBAAInfo,\n                         bool isInit = false, bool isNontemporal = false);\n\n  /// EmitStoreOfScalar - Store a scalar value to an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.  The l-value must be a simple\n  /// l-value.  The isInit flag indicates whether this is an initialization.\n  /// If so, atomic qualifiers are ignored and the store is always non-atomic.\n  void EmitStoreOfScalar(llvm::Value *value, LValue lvalue, bool isInit=false);\n\n  /// EmitLoadOfLValue - Given an expression that represents a value lvalue,\n  /// this method emits the address of the lvalue, then loads the result as an\n  /// rvalue, returning the rvalue.\n  RValue EmitLoadOfLValue(LValue V, SourceLocation Loc);\n  RValue EmitLoadOfExtVectorElementLValue(LValue V);\n  RValue EmitLoadOfBitfieldLValue(LValue LV, SourceLocation Loc);\n  RValue EmitLoadOfGlobalRegLValue(LValue LV);\n\n  /// EmitStoreThroughLValue - Store the specified rvalue into the specified\n  /// lvalue, where both are guaranteed to the have the same type, and that type\n  /// is 'Ty'.\n  void EmitStoreThroughLValue(RValue Src, LValue Dst, bool isInit = false);\n  void EmitStoreThroughExtVectorComponentLValue(RValue Src, LValue Dst);\n  void EmitStoreThroughGlobalRegLValue(RValue Src, LValue Dst);\n\n  /// EmitStoreThroughBitfieldLValue - Store Src into Dst with same constraints\n  /// as EmitStoreThroughLValue.\n  ///\n  /// \\param Result [out] - If non-null, this will be set to a Value* for the\n  /// bit-field contents after the store, appropriate for use as the result of\n  /// an assignment to the bit-field.\n  void EmitStoreThroughBitfieldLValue(RValue Src, LValue Dst,\n                                      llvm::Value **Result=nullptr);\n\n  /// Emit an l-value for an assignment (simple or compound) of complex type.\n  LValue EmitComplexAssignmentLValue(const BinaryOperator *E);\n  LValue EmitComplexCompoundAssignmentLValue(const CompoundAssignOperator *E);\n  LValue EmitScalarCompoundAssignWithComplex(const CompoundAssignOperator *E,\n                                             llvm::Value *&Result);\n\n  // Note: only available for agg return types\n  LValue EmitBinaryOperatorLValue(const BinaryOperator *E);\n  LValue EmitCompoundAssignmentLValue(const CompoundAssignOperator *E);\n  // Note: only available for agg return types\n  LValue EmitCallExprLValue(const CallExpr *E);\n  // Note: only available for agg return types\n  LValue EmitVAArgExprLValue(const VAArgExpr *E);\n  LValue EmitDeclRefLValue(const DeclRefExpr *E);\n  LValue EmitStringLiteralLValue(const StringLiteral *E);\n  LValue EmitObjCEncodeExprLValue(const ObjCEncodeExpr *E);\n  LValue EmitPredefinedLValue(const PredefinedExpr *E);\n  LValue EmitUnaryOpLValue(const UnaryOperator *E);\n  LValue EmitArraySubscriptExpr(const ArraySubscriptExpr *E,\n                                bool Accessed = false);\n  LValue EmitMatrixSubscriptExpr(const MatrixSubscriptExpr *E);\n  LValue EmitOMPArraySectionExpr(const OMPArraySectionExpr *E,\n                                 bool IsLowerBound = true);\n  LValue EmitExtVectorElementExpr(const ExtVectorElementExpr *E);\n  LValue EmitMemberExpr(const MemberExpr *E);\n  LValue EmitObjCIsaExpr(const ObjCIsaExpr *E);\n  LValue EmitCompoundLiteralLValue(const CompoundLiteralExpr *E);\n  LValue EmitInitListLValue(const InitListExpr *E);\n  LValue EmitConditionalOperatorLValue(const AbstractConditionalOperator *E);\n  LValue EmitCastLValue(const CastExpr *E);\n  LValue EmitMaterializeTemporaryExpr(const MaterializeTemporaryExpr *E);\n  LValue EmitOpaqueValueLValue(const OpaqueValueExpr *e);\n\n  Address EmitExtVectorElementLValue(LValue V);\n\n  RValue EmitRValueForField(LValue LV, const FieldDecl *FD, SourceLocation Loc);\n\n  Address EmitArrayToPointerDecay(const Expr *Array,\n                                  LValueBaseInfo *BaseInfo = nullptr,\n                                  TBAAAccessInfo *TBAAInfo = nullptr);\n\n  class ConstantEmission {\n    llvm::PointerIntPair<llvm::Constant*, 1, bool> ValueAndIsReference;\n    ConstantEmission(llvm::Constant *C, bool isReference)\n      : ValueAndIsReference(C, isReference) {}\n  public:\n    ConstantEmission() {}\n    static ConstantEmission forReference(llvm::Constant *C) {\n      return ConstantEmission(C, true);\n    }\n    static ConstantEmission forValue(llvm::Constant *C) {\n      return ConstantEmission(C, false);\n    }\n\n    explicit operator bool() const {\n      return ValueAndIsReference.getOpaqueValue() != nullptr;\n    }\n\n    bool isReference() const { return ValueAndIsReference.getInt(); }\n    LValue getReferenceLValue(CodeGenFunction &CGF, Expr *refExpr) const {\n      assert(isReference());\n      return CGF.MakeNaturalAlignAddrLValue(ValueAndIsReference.getPointer(),\n                                            refExpr->getType());\n    }\n\n    llvm::Constant *getValue() const {\n      assert(!isReference());\n      return ValueAndIsReference.getPointer();\n    }\n  };\n\n  ConstantEmission tryEmitAsConstant(DeclRefExpr *refExpr);\n  ConstantEmission tryEmitAsConstant(const MemberExpr *ME);\n  llvm::Value *emitScalarConstant(const ConstantEmission &Constant, Expr *E);\n\n  RValue EmitPseudoObjectRValue(const PseudoObjectExpr *e,\n                                AggValueSlot slot = AggValueSlot::ignored());\n  LValue EmitPseudoObjectLValue(const PseudoObjectExpr *e);\n\n  llvm::Value *EmitIvarOffset(const ObjCInterfaceDecl *Interface,\n                              const ObjCIvarDecl *Ivar);\n  LValue EmitLValueForField(LValue Base, const FieldDecl* Field);\n  LValue EmitLValueForLambdaField(const FieldDecl *Field);\n\n  /// EmitLValueForFieldInitialization - Like EmitLValueForField, except that\n  /// if the Field is a reference, this will return the address of the reference\n  /// and not the address of the value stored in the reference.\n  LValue EmitLValueForFieldInitialization(LValue Base,\n                                          const FieldDecl* Field);\n\n  LValue EmitLValueForIvar(QualType ObjectTy,\n                           llvm::Value* Base, const ObjCIvarDecl *Ivar,\n                           unsigned CVRQualifiers);\n\n  LValue EmitCXXConstructLValue(const CXXConstructExpr *E);\n  LValue EmitCXXBindTemporaryLValue(const CXXBindTemporaryExpr *E);\n  LValue EmitCXXTypeidLValue(const CXXTypeidExpr *E);\n  LValue EmitCXXUuidofLValue(const CXXUuidofExpr *E);\n\n  LValue EmitObjCMessageExprLValue(const ObjCMessageExpr *E);\n  LValue EmitObjCIvarRefLValue(const ObjCIvarRefExpr *E);\n  LValue EmitStmtExprLValue(const StmtExpr *E);\n  LValue EmitPointerToDataMemberBinaryExpr(const BinaryOperator *E);\n  LValue EmitObjCSelectorLValue(const ObjCSelectorExpr *E);\n  void   EmitDeclRefExprDbgValue(const DeclRefExpr *E, const APValue &Init);\n\n  //===--------------------------------------------------------------------===//\n  //                         Scalar Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitCall - Generate a call of the given function, expecting the given\n  /// result type, and using the given argument list which specifies both the\n  /// LLVM arguments and the types they were derived from.\n  RValue EmitCall(const CGFunctionInfo &CallInfo, const CGCallee &Callee,\n                  ReturnValueSlot ReturnValue, const CallArgList &Args,\n                  llvm::CallBase **callOrInvoke, SourceLocation Loc);\n  RValue EmitCall(const CGFunctionInfo &CallInfo, const CGCallee &Callee,\n                  ReturnValueSlot ReturnValue, const CallArgList &Args,\n                  llvm::CallBase **callOrInvoke = nullptr) {\n    return EmitCall(CallInfo, Callee, ReturnValue, Args, callOrInvoke,\n                    SourceLocation());\n  }\n  RValue EmitCall(QualType FnType, const CGCallee &Callee, const CallExpr *E,\n                  ReturnValueSlot ReturnValue, llvm::Value *Chain = nullptr);\n  RValue EmitCallExpr(const CallExpr *E,\n                      ReturnValueSlot ReturnValue = ReturnValueSlot());\n  RValue EmitSimpleCallExpr(const CallExpr *E, ReturnValueSlot ReturnValue);\n  CGCallee EmitCallee(const Expr *E);\n\n  void checkTargetFeatures(const CallExpr *E, const FunctionDecl *TargetDecl);\n  void checkTargetFeatures(SourceLocation Loc, const FunctionDecl *TargetDecl);\n\n  llvm::CallInst *EmitRuntimeCall(llvm::FunctionCallee callee,\n                                  const Twine &name = \"\");\n  llvm::CallInst *EmitRuntimeCall(llvm::FunctionCallee callee,\n                                  ArrayRef<llvm::Value *> args,\n                                  const Twine &name = \"\");\n  llvm::CallInst *EmitNounwindRuntimeCall(llvm::FunctionCallee callee,\n                                          const Twine &name = \"\");\n  llvm::CallInst *EmitNounwindRuntimeCall(llvm::FunctionCallee callee,\n                                          ArrayRef<llvm::Value *> args,\n                                          const Twine &name = \"\");\n\n  SmallVector<llvm::OperandBundleDef, 1>\n  getBundlesForFunclet(llvm::Value *Callee);\n\n  llvm::CallBase *EmitCallOrInvoke(llvm::FunctionCallee Callee,\n                                   ArrayRef<llvm::Value *> Args,\n                                   const Twine &Name = \"\");\n  llvm::CallBase *EmitRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                          ArrayRef<llvm::Value *> args,\n                                          const Twine &name = \"\");\n  llvm::CallBase *EmitRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                          const Twine &name = \"\");\n  void EmitNoreturnRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                       ArrayRef<llvm::Value *> args);\n\n  CGCallee BuildAppleKextVirtualCall(const CXXMethodDecl *MD,\n                                     NestedNameSpecifier *Qual,\n                                     llvm::Type *Ty);\n\n  CGCallee BuildAppleKextVirtualDestructorCall(const CXXDestructorDecl *DD,\n                                               CXXDtorType Type,\n                                               const CXXRecordDecl *RD);\n\n  // Return the copy constructor name with the prefix \"__copy_constructor_\"\n  // removed.\n  static std::string getNonTrivialCopyConstructorStr(QualType QT,\n                                                     CharUnits Alignment,\n                                                     bool IsVolatile,\n                                                     ASTContext &Ctx);\n\n  // Return the destructor name with the prefix \"__destructor_\" removed.\n  static std::string getNonTrivialDestructorStr(QualType QT,\n                                                CharUnits Alignment,\n                                                bool IsVolatile,\n                                                ASTContext &Ctx);\n\n  // These functions emit calls to the special functions of non-trivial C\n  // structs.\n  void defaultInitNonTrivialCStructVar(LValue Dst);\n  void callCStructDefaultConstructor(LValue Dst);\n  void callCStructDestructor(LValue Dst);\n  void callCStructCopyConstructor(LValue Dst, LValue Src);\n  void callCStructMoveConstructor(LValue Dst, LValue Src);\n  void callCStructCopyAssignmentOperator(LValue Dst, LValue Src);\n  void callCStructMoveAssignmentOperator(LValue Dst, LValue Src);\n\n  RValue\n  EmitCXXMemberOrOperatorCall(const CXXMethodDecl *Method,\n                              const CGCallee &Callee,\n                              ReturnValueSlot ReturnValue, llvm::Value *This,\n                              llvm::Value *ImplicitParam,\n                              QualType ImplicitParamTy, const CallExpr *E,\n                              CallArgList *RtlArgs);\n  RValue EmitCXXDestructorCall(GlobalDecl Dtor, const CGCallee &Callee,\n                               llvm::Value *This, QualType ThisTy,\n                               llvm::Value *ImplicitParam,\n                               QualType ImplicitParamTy, const CallExpr *E);\n  RValue EmitCXXMemberCallExpr(const CXXMemberCallExpr *E,\n                               ReturnValueSlot ReturnValue);\n  RValue EmitCXXMemberOrOperatorMemberCallExpr(const CallExpr *CE,\n                                               const CXXMethodDecl *MD,\n                                               ReturnValueSlot ReturnValue,\n                                               bool HasQualifier,\n                                               NestedNameSpecifier *Qualifier,\n                                               bool IsArrow, const Expr *Base);\n  // Compute the object pointer.\n  Address EmitCXXMemberDataPointerAddress(const Expr *E, Address base,\n                                          llvm::Value *memberPtr,\n                                          const MemberPointerType *memberPtrType,\n                                          LValueBaseInfo *BaseInfo = nullptr,\n                                          TBAAAccessInfo *TBAAInfo = nullptr);\n  RValue EmitCXXMemberPointerCallExpr(const CXXMemberCallExpr *E,\n                                      ReturnValueSlot ReturnValue);\n\n  RValue EmitCXXOperatorMemberCallExpr(const CXXOperatorCallExpr *E,\n                                       const CXXMethodDecl *MD,\n                                       ReturnValueSlot ReturnValue);\n  RValue EmitCXXPseudoDestructorExpr(const CXXPseudoDestructorExpr *E);\n\n  RValue EmitCUDAKernelCallExpr(const CUDAKernelCallExpr *E,\n                                ReturnValueSlot ReturnValue);\n\n  RValue EmitNVPTXDevicePrintfCallExpr(const CallExpr *E,\n                                       ReturnValueSlot ReturnValue);\n  RValue EmitAMDGPUDevicePrintfCallExpr(const CallExpr *E,\n                                        ReturnValueSlot ReturnValue);\n\n  RValue EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,\n                         const CallExpr *E, ReturnValueSlot ReturnValue);\n\n  RValue emitRotate(const CallExpr *E, bool IsRotateRight);\n\n  /// Emit IR for __builtin_os_log_format.\n  RValue emitBuiltinOSLogFormat(const CallExpr &E);\n\n  /// Emit IR for __builtin_is_aligned.\n  RValue EmitBuiltinIsAligned(const CallExpr *E);\n  /// Emit IR for __builtin_align_up/__builtin_align_down.\n  RValue EmitBuiltinAlignTo(const CallExpr *E, bool AlignUp);\n\n  llvm::Function *generateBuiltinOSLogHelperFunction(\n      const analyze_os_log::OSLogBufferLayout &Layout,\n      CharUnits BufferAlignment);\n\n  RValue EmitBlockCallExpr(const CallExpr *E, ReturnValueSlot ReturnValue);\n\n  /// EmitTargetBuiltinExpr - Emit the given builtin call. Returns 0 if the call\n  /// is unhandled by the current target.\n  llvm::Value *EmitTargetBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue);\n\n  llvm::Value *EmitAArch64CompareBuiltinExpr(llvm::Value *Op, llvm::Type *Ty,\n                                             const llvm::CmpInst::Predicate Fp,\n                                             const llvm::CmpInst::Predicate Ip,\n                                             const llvm::Twine &Name = \"\");\n  llvm::Value *EmitARMBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                  ReturnValueSlot ReturnValue,\n                                  llvm::Triple::ArchType Arch);\n  llvm::Value *EmitARMMVEBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue,\n                                     llvm::Triple::ArchType Arch);\n  llvm::Value *EmitARMCDEBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue,\n                                     llvm::Triple::ArchType Arch);\n  llvm::Value *EmitCMSEClearRecord(llvm::Value *V, llvm::IntegerType *ITy,\n                                   QualType RTy);\n  llvm::Value *EmitCMSEClearRecord(llvm::Value *V, llvm::ArrayType *ATy,\n                                   QualType RTy);\n\n  llvm::Value *EmitCommonNeonBuiltinExpr(unsigned BuiltinID,\n                                         unsigned LLVMIntrinsic,\n                                         unsigned AltLLVMIntrinsic,\n                                         const char *NameHint,\n                                         unsigned Modifier,\n                                         const CallExpr *E,\n                                         SmallVectorImpl<llvm::Value *> &Ops,\n                                         Address PtrOp0, Address PtrOp1,\n                                         llvm::Triple::ArchType Arch);\n\n  llvm::Function *LookupNeonLLVMIntrinsic(unsigned IntrinsicID,\n                                          unsigned Modifier, llvm::Type *ArgTy,\n                                          const CallExpr *E);\n  llvm::Value *EmitNeonCall(llvm::Function *F,\n                            SmallVectorImpl<llvm::Value*> &O,\n                            const char *name,\n                            unsigned shift = 0, bool rightshift = false);\n  llvm::Value *EmitNeonSplat(llvm::Value *V, llvm::Constant *Idx,\n                             const llvm::ElementCount &Count);\n  llvm::Value *EmitNeonSplat(llvm::Value *V, llvm::Constant *Idx);\n  llvm::Value *EmitNeonShiftVector(llvm::Value *V, llvm::Type *Ty,\n                                   bool negateForRightShift);\n  llvm::Value *EmitNeonRShiftImm(llvm::Value *Vec, llvm::Value *Amt,\n                                 llvm::Type *Ty, bool usgn, const char *name);\n  llvm::Value *vectorWrapScalar16(llvm::Value *Op);\n  /// SVEBuiltinMemEltTy - Returns the memory element type for this memory\n  /// access builtin.  Only required if it can't be inferred from the base\n  /// pointer operand.\n  llvm::Type *SVEBuiltinMemEltTy(SVETypeFlags TypeFlags);\n\n  SmallVector<llvm::Type *, 2> getSVEOverloadTypes(SVETypeFlags TypeFlags,\n                                                   llvm::Type *ReturnType,\n                                                   ArrayRef<llvm::Value *> Ops);\n  llvm::Type *getEltType(SVETypeFlags TypeFlags);\n  llvm::ScalableVectorType *getSVEType(const SVETypeFlags &TypeFlags);\n  llvm::ScalableVectorType *getSVEPredType(SVETypeFlags TypeFlags);\n  llvm::Value *EmitSVEAllTruePred(SVETypeFlags TypeFlags);\n  llvm::Value *EmitSVEDupX(llvm::Value *Scalar);\n  llvm::Value *EmitSVEDupX(llvm::Value *Scalar, llvm::Type *Ty);\n  llvm::Value *EmitSVEReinterpret(llvm::Value *Val, llvm::Type *Ty);\n  llvm::Value *EmitSVEPMull(SVETypeFlags TypeFlags,\n                            llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                            unsigned BuiltinID);\n  llvm::Value *EmitSVEMovl(SVETypeFlags TypeFlags,\n                           llvm::ArrayRef<llvm::Value *> Ops,\n                           unsigned BuiltinID);\n  llvm::Value *EmitSVEPredicateCast(llvm::Value *Pred,\n                                    llvm::ScalableVectorType *VTy);\n  llvm::Value *EmitSVEGatherLoad(SVETypeFlags TypeFlags,\n                                 llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                                 unsigned IntID);\n  llvm::Value *EmitSVEScatterStore(SVETypeFlags TypeFlags,\n                                   llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                                   unsigned IntID);\n  llvm::Value *EmitSVEMaskedLoad(const CallExpr *, llvm::Type *ReturnTy,\n                                 SmallVectorImpl<llvm::Value *> &Ops,\n                                 unsigned BuiltinID, bool IsZExtReturn);\n  llvm::Value *EmitSVEMaskedStore(const CallExpr *,\n                                  SmallVectorImpl<llvm::Value *> &Ops,\n                                  unsigned BuiltinID);\n  llvm::Value *EmitSVEPrefetchLoad(SVETypeFlags TypeFlags,\n                                   SmallVectorImpl<llvm::Value *> &Ops,\n                                   unsigned BuiltinID);\n  llvm::Value *EmitSVEGatherPrefetch(SVETypeFlags TypeFlags,\n                                     SmallVectorImpl<llvm::Value *> &Ops,\n                                     unsigned IntID);\n  llvm::Value *EmitSVEStructLoad(SVETypeFlags TypeFlags,\n                                 SmallVectorImpl<llvm::Value *> &Ops, unsigned IntID);\n  llvm::Value *EmitSVEStructStore(SVETypeFlags TypeFlags,\n                                  SmallVectorImpl<llvm::Value *> &Ops,\n                                  unsigned IntID);\n  llvm::Value *EmitAArch64SVEBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n\n  llvm::Value *EmitAArch64BuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                      llvm::Triple::ArchType Arch);\n  llvm::Value *EmitBPFBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n\n  llvm::Value *BuildVector(ArrayRef<llvm::Value*> Ops);\n  llvm::Value *EmitX86BuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitPPCBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitAMDGPUBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitSystemZBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitNVPTXBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitWebAssemblyBuiltinExpr(unsigned BuiltinID,\n                                          const CallExpr *E);\n  llvm::Value *EmitHexagonBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitRISCVBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                    ReturnValueSlot ReturnValue);\n  bool ProcessOrderScopeAMDGCN(llvm::Value *Order, llvm::Value *Scope,\n                               llvm::AtomicOrdering &AO,\n                               llvm::SyncScope::ID &SSID);\n\n  enum class MSVCIntrin;\n  llvm::Value *EmitMSVCBuiltinExpr(MSVCIntrin BuiltinID, const CallExpr *E);\n\n  llvm::Value *EmitBuiltinAvailable(const VersionTuple &Version);\n\n  llvm::Value *EmitObjCProtocolExpr(const ObjCProtocolExpr *E);\n  llvm::Value *EmitObjCStringLiteral(const ObjCStringLiteral *E);\n  llvm::Value *EmitObjCBoxedExpr(const ObjCBoxedExpr *E);\n  llvm::Value *EmitObjCArrayLiteral(const ObjCArrayLiteral *E);\n  llvm::Value *EmitObjCDictionaryLiteral(const ObjCDictionaryLiteral *E);\n  llvm::Value *EmitObjCCollectionLiteral(const Expr *E,\n                                const ObjCMethodDecl *MethodWithObjects);\n  llvm::Value *EmitObjCSelectorExpr(const ObjCSelectorExpr *E);\n  RValue EmitObjCMessageExpr(const ObjCMessageExpr *E,\n                             ReturnValueSlot Return = ReturnValueSlot());\n\n  /// Retrieves the default cleanup kind for an ARC cleanup.\n  /// Except under -fobjc-arc-eh, ARC cleanups are normal-only.\n  CleanupKind getARCCleanupKind() {\n    return CGM.getCodeGenOpts().ObjCAutoRefCountExceptions\n             ? NormalAndEHCleanup : NormalCleanup;\n  }\n\n  // ARC primitives.\n  void EmitARCInitWeak(Address addr, llvm::Value *value);\n  void EmitARCDestroyWeak(Address addr);\n  llvm::Value *EmitARCLoadWeak(Address addr);\n  llvm::Value *EmitARCLoadWeakRetained(Address addr);\n  llvm::Value *EmitARCStoreWeak(Address addr, llvm::Value *value, bool ignored);\n  void emitARCCopyAssignWeak(QualType Ty, Address DstAddr, Address SrcAddr);\n  void emitARCMoveAssignWeak(QualType Ty, Address DstAddr, Address SrcAddr);\n  void EmitARCCopyWeak(Address dst, Address src);\n  void EmitARCMoveWeak(Address dst, Address src);\n  llvm::Value *EmitARCRetainAutorelease(QualType type, llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleaseNonBlock(llvm::Value *value);\n  llvm::Value *EmitARCStoreStrong(LValue lvalue, llvm::Value *value,\n                                  bool resultIgnored);\n  llvm::Value *EmitARCStoreStrongCall(Address addr, llvm::Value *value,\n                                      bool resultIgnored);\n  llvm::Value *EmitARCRetain(QualType type, llvm::Value *value);\n  llvm::Value *EmitARCRetainNonBlock(llvm::Value *value);\n  llvm::Value *EmitARCRetainBlock(llvm::Value *value, bool mandatory);\n  void EmitARCDestroyStrong(Address addr, ARCPreciseLifetime_t precise);\n  void EmitARCRelease(llvm::Value *value, ARCPreciseLifetime_t precise);\n  llvm::Value *EmitARCAutorelease(llvm::Value *value);\n  llvm::Value *EmitARCAutoreleaseReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleaseReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleasedReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCUnsafeClaimAutoreleasedReturnValue(llvm::Value *value);\n\n  llvm::Value *EmitObjCAutorelease(llvm::Value *value, llvm::Type *returnType);\n  llvm::Value *EmitObjCRetainNonBlock(llvm::Value *value,\n                                      llvm::Type *returnType);\n  void EmitObjCRelease(llvm::Value *value, ARCPreciseLifetime_t precise);\n\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreAutoreleasing(const BinaryOperator *e);\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreStrong(const BinaryOperator *e, bool ignored);\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreUnsafeUnretained(const BinaryOperator *e, bool ignored);\n\n  llvm::Value *EmitObjCAlloc(llvm::Value *value,\n                             llvm::Type *returnType);\n  llvm::Value *EmitObjCAllocWithZone(llvm::Value *value,\n                                     llvm::Type *returnType);\n  llvm::Value *EmitObjCAllocInit(llvm::Value *value, llvm::Type *resultType);\n\n  llvm::Value *EmitObjCThrowOperand(const Expr *expr);\n  llvm::Value *EmitObjCConsumeObject(QualType T, llvm::Value *Ptr);\n  llvm::Value *EmitObjCExtendObjectLifetime(QualType T, llvm::Value *Ptr);\n\n  llvm::Value *EmitARCExtendBlockObject(const Expr *expr);\n  llvm::Value *EmitARCReclaimReturnedObject(const Expr *e,\n                                            bool allowUnsafeClaim);\n  llvm::Value *EmitARCRetainScalarExpr(const Expr *expr);\n  llvm::Value *EmitARCRetainAutoreleaseScalarExpr(const Expr *expr);\n  llvm::Value *EmitARCUnsafeUnretainedScalarExpr(const Expr *expr);\n\n  void EmitARCIntrinsicUse(ArrayRef<llvm::Value*> values);\n\n  void EmitARCNoopIntrinsicUse(ArrayRef<llvm::Value *> values);\n\n  static Destroyer destroyARCStrongImprecise;\n  static Destroyer destroyARCStrongPrecise;\n  static Destroyer destroyARCWeak;\n  static Destroyer emitARCIntrinsicUse;\n  static Destroyer destroyNonTrivialCStruct;\n\n  void EmitObjCAutoreleasePoolPop(llvm::Value *Ptr);\n  llvm::Value *EmitObjCAutoreleasePoolPush();\n  llvm::Value *EmitObjCMRRAutoreleasePoolPush();\n  void EmitObjCAutoreleasePoolCleanup(llvm::Value *Ptr);\n  void EmitObjCMRRAutoreleasePoolPop(llvm::Value *Ptr);\n\n  /// Emits a reference binding to the passed in expression.\n  RValue EmitReferenceBindingToExpr(const Expr *E);\n\n  //===--------------------------------------------------------------------===//\n  //                           Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  // Expressions are broken into three classes: scalar, complex, aggregate.\n\n  /// EmitScalarExpr - Emit the computation of the specified expression of LLVM\n  /// scalar type, returning the result.\n  llvm::Value *EmitScalarExpr(const Expr *E , bool IgnoreResultAssign = false);\n\n  /// Emit a conversion from the specified type to the specified destination\n  /// type, both of which are LLVM scalar types.\n  llvm::Value *EmitScalarConversion(llvm::Value *Src, QualType SrcTy,\n                                    QualType DstTy, SourceLocation Loc);\n\n  /// Emit a conversion from the specified complex type to the specified\n  /// destination type, where the destination type is an LLVM scalar type.\n  llvm::Value *EmitComplexToScalarConversion(ComplexPairTy Src, QualType SrcTy,\n                                             QualType DstTy,\n                                             SourceLocation Loc);\n\n  /// EmitAggExpr - Emit the computation of the specified expression\n  /// of aggregate type.  The result is computed into the given slot,\n  /// which may be null to indicate that the value is not needed.\n  void EmitAggExpr(const Expr *E, AggValueSlot AS);\n\n  /// EmitAggExprToLValue - Emit the computation of the specified expression of\n  /// aggregate type into a temporary LValue.\n  LValue EmitAggExprToLValue(const Expr *E);\n\n  /// Build all the stores needed to initialize an aggregate at Dest with the\n  /// value Val.\n  void EmitAggregateStore(llvm::Value *Val, Address Dest, bool DestIsVolatile);\n\n  /// EmitExtendGCLifetime - Given a pointer to an Objective-C object,\n  /// make sure it survives garbage collection until this point.\n  void EmitExtendGCLifetime(llvm::Value *object);\n\n  /// EmitComplexExpr - Emit the computation of the specified expression of\n  /// complex type, returning the result.\n  ComplexPairTy EmitComplexExpr(const Expr *E,\n                                bool IgnoreReal = false,\n                                bool IgnoreImag = false);\n\n  /// EmitComplexExprIntoLValue - Emit the given expression of complex\n  /// type and place its result into the specified l-value.\n  void EmitComplexExprIntoLValue(const Expr *E, LValue dest, bool isInit);\n\n  /// EmitStoreOfComplex - Store a complex number into the specified l-value.\n  void EmitStoreOfComplex(ComplexPairTy V, LValue dest, bool isInit);\n\n  /// EmitLoadOfComplex - Load a complex number from the specified l-value.\n  ComplexPairTy EmitLoadOfComplex(LValue src, SourceLocation loc);\n\n  Address emitAddrOfRealComponent(Address complex, QualType complexType);\n  Address emitAddrOfImagComponent(Address complex, QualType complexType);\n\n  /// AddInitializerToStaticVarDecl - Add the initializer for 'D' to the\n  /// global variable that has already been created for it.  If the initializer\n  /// has a different type than GV does, this may free GV and return a different\n  /// one.  Otherwise it just returns GV.\n  llvm::GlobalVariable *\n  AddInitializerToStaticVarDecl(const VarDecl &D,\n                                llvm::GlobalVariable *GV);\n\n  // Emit an @llvm.invariant.start call for the given memory region.\n  void EmitInvariantStart(llvm::Constant *Addr, CharUnits Size);\n\n  /// EmitCXXGlobalVarDeclInit - Create the initializer for a C++\n  /// variable with global storage.\n  void EmitCXXGlobalVarDeclInit(const VarDecl &D, llvm::Constant *DeclPtr,\n                                bool PerformInit);\n\n  llvm::Function *createAtExitStub(const VarDecl &VD, llvm::FunctionCallee Dtor,\n                                   llvm::Constant *Addr);\n\n  /// Call atexit() with a function that passes the given argument to\n  /// the given function.\n  void registerGlobalDtorWithAtExit(const VarDecl &D, llvm::FunctionCallee fn,\n                                    llvm::Constant *addr);\n\n  /// Call atexit() with function dtorStub.\n  void registerGlobalDtorWithAtExit(llvm::Constant *dtorStub);\n\n  /// Call unatexit() with function dtorStub.\n  llvm::Value *unregisterGlobalDtorWithUnAtExit(llvm::Constant *dtorStub);\n\n  /// Emit code in this function to perform a guarded variable\n  /// initialization.  Guarded initializations are used when it's not\n  /// possible to prove that an initialization will be done exactly\n  /// once, e.g. with a static local variable or a static data member\n  /// of a class template.\n  void EmitCXXGuardedInit(const VarDecl &D, llvm::GlobalVariable *DeclPtr,\n                          bool PerformInit);\n\n  enum class GuardKind { VariableGuard, TlsGuard };\n\n  /// Emit a branch to select whether or not to perform guarded initialization.\n  void EmitCXXGuardedInitBranch(llvm::Value *NeedsInit,\n                                llvm::BasicBlock *InitBlock,\n                                llvm::BasicBlock *NoInitBlock,\n                                GuardKind Kind, const VarDecl *D);\n\n  /// GenerateCXXGlobalInitFunc - Generates code for initializing global\n  /// variables.\n  void\n  GenerateCXXGlobalInitFunc(llvm::Function *Fn,\n                            ArrayRef<llvm::Function *> CXXThreadLocals,\n                            ConstantAddress Guard = ConstantAddress::invalid());\n\n  /// GenerateCXXGlobalCleanUpFunc - Generates code for cleaning up global\n  /// variables.\n  void GenerateCXXGlobalCleanUpFunc(\n      llvm::Function *Fn,\n      const std::vector<std::tuple<llvm::FunctionType *, llvm::WeakTrackingVH,\n                                   llvm::Constant *>> &DtorsOrStermFinalizers);\n\n  void GenerateCXXGlobalVarDeclInitFunc(llvm::Function *Fn,\n                                        const VarDecl *D,\n                                        llvm::GlobalVariable *Addr,\n                                        bool PerformInit);\n\n  void EmitCXXConstructExpr(const CXXConstructExpr *E, AggValueSlot Dest);\n\n  void EmitSynthesizedCXXCopyCtor(Address Dest, Address Src, const Expr *Exp);\n\n  void EmitCXXThrowExpr(const CXXThrowExpr *E, bool KeepInsertionPoint = true);\n\n  RValue EmitAtomicExpr(AtomicExpr *E);\n\n  //===--------------------------------------------------------------------===//\n  //                         Annotations Emission\n  //===--------------------------------------------------------------------===//\n\n  /// Emit an annotation call (intrinsic).\n  llvm::Value *EmitAnnotationCall(llvm::Function *AnnotationFn,\n                                  llvm::Value *AnnotatedVal,\n                                  StringRef AnnotationStr,\n                                  SourceLocation Location,\n                                  const AnnotateAttr *Attr);\n\n  /// Emit local annotations for the local variable V, declared by D.\n  void EmitVarAnnotations(const VarDecl *D, llvm::Value *V);\n\n  /// Emit field annotations for the given field & value. Returns the\n  /// annotation result.\n  Address EmitFieldAnnotations(const FieldDecl *D, Address V);\n\n  //===--------------------------------------------------------------------===//\n  //                             Internal Helpers\n  //===--------------------------------------------------------------------===//\n\n  /// ContainsLabel - Return true if the statement contains a label in it.  If\n  /// this statement is not executed normally, it not containing a label means\n  /// that we can just remove the code.\n  static bool ContainsLabel(const Stmt *S, bool IgnoreCaseStmts = false);\n\n  /// containsBreak - Return true if the statement contains a break out of it.\n  /// If the statement (recursively) contains a switch or loop with a break\n  /// inside of it, this is fine.\n  static bool containsBreak(const Stmt *S);\n\n  /// Determine if the given statement might introduce a declaration into the\n  /// current scope, by being a (possibly-labelled) DeclStmt.\n  static bool mightAddDeclToScope(const Stmt *S);\n\n  /// ConstantFoldsToSimpleInteger - If the specified expression does not fold\n  /// to a constant, or if it does but contains a label, return false.  If it\n  /// constant folds return true and set the boolean result in Result.\n  bool ConstantFoldsToSimpleInteger(const Expr *Cond, bool &Result,\n                                    bool AllowLabels = false);\n\n  /// ConstantFoldsToSimpleInteger - If the specified expression does not fold\n  /// to a constant, or if it does but contains a label, return false.  If it\n  /// constant folds return true and set the folded value.\n  bool ConstantFoldsToSimpleInteger(const Expr *Cond, llvm::APSInt &Result,\n                                    bool AllowLabels = false);\n\n  /// isInstrumentedCondition - Determine whether the given condition is an\n  /// instrumentable condition (i.e. no \"&&\" or \"||\").\n  static bool isInstrumentedCondition(const Expr *C);\n\n  /// EmitBranchToCounterBlock - Emit a conditional branch to a new block that\n  /// increments a profile counter based on the semantics of the given logical\n  /// operator opcode.  This is used to instrument branch condition coverage\n  /// for logical operators.\n  void EmitBranchToCounterBlock(const Expr *Cond, BinaryOperator::Opcode LOp,\n                                llvm::BasicBlock *TrueBlock,\n                                llvm::BasicBlock *FalseBlock,\n                                uint64_t TrueCount = 0,\n                                Stmt::Likelihood LH = Stmt::LH_None,\n                                const Expr *CntrIdx = nullptr);\n\n  /// EmitBranchOnBoolExpr - Emit a branch on a boolean condition (e.g. for an\n  /// if statement) to the specified blocks.  Based on the condition, this might\n  /// try to simplify the codegen of the conditional based on the branch.\n  /// TrueCount should be the number of times we expect the condition to\n  /// evaluate to true based on PGO data.\n  void EmitBranchOnBoolExpr(const Expr *Cond, llvm::BasicBlock *TrueBlock,\n                            llvm::BasicBlock *FalseBlock, uint64_t TrueCount,\n                            Stmt::Likelihood LH = Stmt::LH_None);\n\n  /// Given an assignment `*LHS = RHS`, emit a test that checks if \\p RHS is\n  /// nonnull, if \\p LHS is marked _Nonnull.\n  void EmitNullabilityCheck(LValue LHS, llvm::Value *RHS, SourceLocation Loc);\n\n  /// An enumeration which makes it easier to specify whether or not an\n  /// operation is a subtraction.\n  enum { NotSubtraction = false, IsSubtraction = true };\n\n  /// Same as IRBuilder::CreateInBoundsGEP, but additionally emits a check to\n  /// detect undefined behavior when the pointer overflow sanitizer is enabled.\n  /// \\p SignedIndices indicates whether any of the GEP indices are signed.\n  /// \\p IsSubtraction indicates whether the expression used to form the GEP\n  /// is a subtraction.\n  llvm::Value *EmitCheckedInBoundsGEP(llvm::Value *Ptr,\n                                      ArrayRef<llvm::Value *> IdxList,\n                                      bool SignedIndices,\n                                      bool IsSubtraction,\n                                      SourceLocation Loc,\n                                      const Twine &Name = \"\");\n\n  /// Specifies which type of sanitizer check to apply when handling a\n  /// particular builtin.\n  enum BuiltinCheckKind {\n    BCK_CTZPassedZero,\n    BCK_CLZPassedZero,\n  };\n\n  /// Emits an argument for a call to a builtin. If the builtin sanitizer is\n  /// enabled, a runtime check specified by \\p Kind is also emitted.\n  llvm::Value *EmitCheckedArgForBuiltin(const Expr *E, BuiltinCheckKind Kind);\n\n  /// Emit a description of a type in a format suitable for passing to\n  /// a runtime sanitizer handler.\n  llvm::Constant *EmitCheckTypeDescriptor(QualType T);\n\n  /// Convert a value into a format suitable for passing to a runtime\n  /// sanitizer handler.\n  llvm::Value *EmitCheckValue(llvm::Value *V);\n\n  /// Emit a description of a source location in a format suitable for\n  /// passing to a runtime sanitizer handler.\n  llvm::Constant *EmitCheckSourceLocation(SourceLocation Loc);\n\n  /// Create a basic block that will either trap or call a handler function in\n  /// the UBSan runtime with the provided arguments, and create a conditional\n  /// branch to it.\n  void EmitCheck(ArrayRef<std::pair<llvm::Value *, SanitizerMask>> Checked,\n                 SanitizerHandler Check, ArrayRef<llvm::Constant *> StaticArgs,\n                 ArrayRef<llvm::Value *> DynamicArgs);\n\n  /// Emit a slow path cross-DSO CFI check which calls __cfi_slowpath\n  /// if Cond if false.\n  void EmitCfiSlowPathCheck(SanitizerMask Kind, llvm::Value *Cond,\n                            llvm::ConstantInt *TypeId, llvm::Value *Ptr,\n                            ArrayRef<llvm::Constant *> StaticArgs);\n\n  /// Emit a reached-unreachable diagnostic if \\p Loc is valid and runtime\n  /// checking is enabled. Otherwise, just emit an unreachable instruction.\n  void EmitUnreachable(SourceLocation Loc);\n\n  /// Create a basic block that will call the trap intrinsic, and emit a\n  /// conditional branch to it, for the -ftrapv checks.\n  void EmitTrapCheck(llvm::Value *Checked, SanitizerHandler CheckHandlerID);\n\n  /// Emit a call to trap or debugtrap and attach function attribute\n  /// \"trap-func-name\" if specified.\n  llvm::CallInst *EmitTrapCall(llvm::Intrinsic::ID IntrID);\n\n  /// Emit a stub for the cross-DSO CFI check function.\n  void EmitCfiCheckStub();\n\n  /// Emit a cross-DSO CFI failure handling function.\n  void EmitCfiCheckFail();\n\n  /// Create a check for a function parameter that may potentially be\n  /// declared as non-null.\n  void EmitNonNullArgCheck(RValue RV, QualType ArgType, SourceLocation ArgLoc,\n                           AbstractCallee AC, unsigned ParmNum);\n\n  /// EmitCallArg - Emit a single call argument.\n  void EmitCallArg(CallArgList &args, const Expr *E, QualType ArgType);\n\n  /// EmitDelegateCallArg - We are performing a delegate call; that\n  /// is, the current function is delegating to another one.  Produce\n  /// a r-value suitable for passing the given parameter.\n  void EmitDelegateCallArg(CallArgList &args, const VarDecl *param,\n                           SourceLocation loc);\n\n  /// SetFPAccuracy - Set the minimum required accuracy of the given floating\n  /// point operation, expressed as the maximum relative error in ulp.\n  void SetFPAccuracy(llvm::Value *Val, float Accuracy);\n\n  /// SetFPModel - Control floating point behavior via fp-model settings.\n  void SetFPModel();\n\n  /// Set the codegen fast-math flags.\n  void SetFastMathFlags(FPOptions FPFeatures);\n\nprivate:\n  llvm::MDNode *getRangeForLoadFromType(QualType Ty);\n  void EmitReturnOfRValue(RValue RV, QualType Ty);\n\n  void deferPlaceholderReplacement(llvm::Instruction *Old, llvm::Value *New);\n\n  llvm::SmallVector<std::pair<llvm::WeakTrackingVH, llvm::Value *>, 4>\n      DeferredReplacements;\n\n  /// Set the address of a local variable.\n  void setAddrOfLocalVar(const VarDecl *VD, Address Addr) {\n    assert(!LocalDeclMap.count(VD) && \"Decl already exists in LocalDeclMap!\");\n    LocalDeclMap.insert({VD, Addr});\n  }\n\n  /// ExpandTypeFromArgs - Reconstruct a structure of type \\arg Ty\n  /// from function arguments into \\arg Dst. See ABIArgInfo::Expand.\n  ///\n  /// \\param AI - The first function argument of the expansion.\n  void ExpandTypeFromArgs(QualType Ty, LValue Dst,\n                          llvm::Function::arg_iterator &AI);\n\n  /// ExpandTypeToArgs - Expand an CallArg \\arg Arg, with the LLVM type for \\arg\n  /// Ty, into individual arguments on the provided vector \\arg IRCallArgs,\n  /// starting at index \\arg IRCallArgPos. See ABIArgInfo::Expand.\n  void ExpandTypeToArgs(QualType Ty, CallArg Arg, llvm::FunctionType *IRFuncTy,\n                        SmallVectorImpl<llvm::Value *> &IRCallArgs,\n                        unsigned &IRCallArgPos);\n\n  llvm::Value* EmitAsmInput(const TargetInfo::ConstraintInfo &Info,\n                            const Expr *InputExpr, std::string &ConstraintStr);\n\n  llvm::Value* EmitAsmInputLValue(const TargetInfo::ConstraintInfo &Info,\n                                  LValue InputValue, QualType InputType,\n                                  std::string &ConstraintStr,\n                                  SourceLocation Loc);\n\n  /// Attempts to statically evaluate the object size of E. If that\n  /// fails, emits code to figure the size of E out for us. This is\n  /// pass_object_size aware.\n  ///\n  /// If EmittedExpr is non-null, this will use that instead of re-emitting E.\n  llvm::Value *evaluateOrEmitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                               llvm::IntegerType *ResType,\n                                               llvm::Value *EmittedE,\n                                               bool IsDynamic);\n\n  /// Emits the size of E, as required by __builtin_object_size. This\n  /// function is aware of pass_object_size parameters, and will act accordingly\n  /// if E is a parameter with the pass_object_size attribute.\n  llvm::Value *emitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                     llvm::IntegerType *ResType,\n                                     llvm::Value *EmittedE,\n                                     bool IsDynamic);\n\n  void emitZeroOrPatternForAutoVarInit(QualType type, const VarDecl &D,\n                                       Address Loc);\n\npublic:\n  enum class EvaluationOrder {\n    ///! No language constraints on evaluation order.\n    Default,\n    ///! Language semantics require left-to-right evaluation.\n    ForceLeftToRight,\n    ///! Language semantics require right-to-left evaluation.\n    ForceRightToLeft\n  };\n\n  // Wrapper for function prototype sources. Wraps either a FunctionProtoType or\n  // an ObjCMethodDecl.\n  struct PrototypeWrapper {\n    llvm::PointerUnion<const FunctionProtoType *, const ObjCMethodDecl *> P;\n\n    PrototypeWrapper(const FunctionProtoType *FT) : P(FT) {}\n    PrototypeWrapper(const ObjCMethodDecl *MD) : P(MD) {}\n  };\n\n  void EmitCallArgs(CallArgList &Args, PrototypeWrapper Prototype,\n                    llvm::iterator_range<CallExpr::const_arg_iterator> ArgRange,\n                    AbstractCallee AC = AbstractCallee(),\n                    unsigned ParamsToSkip = 0,\n                    EvaluationOrder Order = EvaluationOrder::Default);\n\n  /// EmitPointerWithAlignment - Given an expression with a pointer type,\n  /// emit the value and compute our best estimate of the alignment of the\n  /// pointee.\n  ///\n  /// \\param BaseInfo - If non-null, this will be initialized with\n  /// information about the source of the alignment and the may-alias\n  /// attribute.  Note that this function will conservatively fall back on\n  /// the type when it doesn't recognize the expression and may-alias will\n  /// be set to false.\n  ///\n  /// One reasonable way to use this information is when there's a language\n  /// guarantee that the pointer must be aligned to some stricter value, and\n  /// we're simply trying to ensure that sufficiently obvious uses of under-\n  /// aligned objects don't get miscompiled; for example, a placement new\n  /// into the address of a local variable.  In such a case, it's quite\n  /// reasonable to just ignore the returned alignment when it isn't from an\n  /// explicit source.\n  Address EmitPointerWithAlignment(const Expr *Addr,\n                                   LValueBaseInfo *BaseInfo = nullptr,\n                                   TBAAAccessInfo *TBAAInfo = nullptr);\n\n  /// If \\p E references a parameter with pass_object_size info or a constant\n  /// array size modifier, emit the object size divided by the size of \\p EltTy.\n  /// Otherwise return null.\n  llvm::Value *LoadPassedObjectSize(const Expr *E, QualType EltTy);\n\n  void EmitSanitizerStatReport(llvm::SanitizerStatKind SSK);\n\n  struct MultiVersionResolverOption {\n    llvm::Function *Function;\n    FunctionDecl *FD;\n    struct Conds {\n      StringRef Architecture;\n      llvm::SmallVector<StringRef, 8> Features;\n\n      Conds(StringRef Arch, ArrayRef<StringRef> Feats)\n          : Architecture(Arch), Features(Feats.begin(), Feats.end()) {}\n    } Conditions;\n\n    MultiVersionResolverOption(llvm::Function *F, StringRef Arch,\n                               ArrayRef<StringRef> Feats)\n        : Function(F), Conditions(Arch, Feats) {}\n  };\n\n  // Emits the body of a multiversion function's resolver. Assumes that the\n  // options are already sorted in the proper order, with the 'default' option\n  // last (if it exists).\n  void EmitMultiVersionResolver(llvm::Function *Resolver,\n                                ArrayRef<MultiVersionResolverOption> Options);\n\n  static uint64_t GetX86CpuSupportsMask(ArrayRef<StringRef> FeatureStrs);\n\nprivate:\n  QualType getVarArgType(const Expr *Arg);\n\n  void EmitDeclMetadata();\n\n  BlockByrefHelpers *buildByrefHelpers(llvm::StructType &byrefType,\n                                  const AutoVarEmission &emission);\n\n  void AddObjCARCExceptionMetadata(llvm::Instruction *Inst);\n\n  llvm::Value *GetValueForARMHint(unsigned BuiltinID);\n  llvm::Value *EmitX86CpuIs(const CallExpr *E);\n  llvm::Value *EmitX86CpuIs(StringRef CPUStr);\n  llvm::Value *EmitX86CpuSupports(const CallExpr *E);\n  llvm::Value *EmitX86CpuSupports(ArrayRef<StringRef> FeatureStrs);\n  llvm::Value *EmitX86CpuSupports(uint64_t Mask);\n  llvm::Value *EmitX86CpuInit();\n  llvm::Value *FormResolverCondition(const MultiVersionResolverOption &RO);\n};\n\n/// TargetFeatures - This class is used to check whether the builtin function\n/// has the required tagert specific features. It is able to support the\n/// combination of ','(and), '|'(or), and '()'. By default, the priority of\n/// ',' is higher than that of '|' .\n/// E.g:\n/// A,B|C means the builtin function requires both A and B, or C.\n/// If we want the builtin function requires both A and B, or both A and C,\n/// there are two ways: A,B|A,C or A,(B|C).\n/// The FeaturesList should not contain spaces, and brackets must appear in\n/// pairs.\nclass TargetFeatures {\n  struct FeatureListStatus {\n    bool HasFeatures;\n    StringRef CurFeaturesList;\n  };\n\n  const llvm::StringMap<bool> &CallerFeatureMap;\n\n  FeatureListStatus getAndFeatures(StringRef FeatureList) {\n    int InParentheses = 0;\n    bool HasFeatures = true;\n    size_t SubexpressionStart = 0;\n    for (size_t i = 0, e = FeatureList.size(); i < e; ++i) {\n      char CurrentToken = FeatureList[i];\n      switch (CurrentToken) {\n      default:\n        break;\n      case '(':\n        if (InParentheses == 0)\n          SubexpressionStart = i + 1;\n        ++InParentheses;\n        break;\n      case ')':\n        --InParentheses;\n        assert(InParentheses >= 0 && \"Parentheses are not in pair\");\n        LLVM_FALLTHROUGH;\n      case '|':\n      case ',':\n        if (InParentheses == 0) {\n          if (HasFeatures && i != SubexpressionStart) {\n            StringRef F = FeatureList.slice(SubexpressionStart, i);\n            HasFeatures = CurrentToken == ')' ? hasRequiredFeatures(F)\n                                              : CallerFeatureMap.lookup(F);\n          }\n          SubexpressionStart = i + 1;\n          if (CurrentToken == '|') {\n            return {HasFeatures, FeatureList.substr(SubexpressionStart)};\n          }\n        }\n        break;\n      }\n    }\n    assert(InParentheses == 0 && \"Parentheses are not in pair\");\n    if (HasFeatures && SubexpressionStart != FeatureList.size())\n      HasFeatures =\n          CallerFeatureMap.lookup(FeatureList.substr(SubexpressionStart));\n    return {HasFeatures, StringRef()};\n  }\n\npublic:\n  bool hasRequiredFeatures(StringRef FeatureList) {\n    FeatureListStatus FS = {false, FeatureList};\n    while (!FS.HasFeatures && !FS.CurFeaturesList.empty())\n      FS = getAndFeatures(FS.CurFeaturesList);\n    return FS.HasFeatures;\n  }\n\n  TargetFeatures(const llvm::StringMap<bool> &CallerFeatureMap)\n      : CallerFeatureMap(CallerFeatureMap) {}\n};\n\ninline DominatingLLVMValue::saved_type\nDominatingLLVMValue::save(CodeGenFunction &CGF, llvm::Value *value) {\n  if (!needsSaving(value)) return saved_type(value, false);\n\n  // Otherwise, we need an alloca.\n  auto align = CharUnits::fromQuantity(\n            CGF.CGM.getDataLayout().getPrefTypeAlignment(value->getType()));\n  Address alloca =\n    CGF.CreateTempAlloca(value->getType(), align, \"cond-cleanup.save\");\n  CGF.Builder.CreateStore(value, alloca);\n\n  return saved_type(alloca.getPointer(), true);\n}\n\ninline llvm::Value *DominatingLLVMValue::restore(CodeGenFunction &CGF,\n                                                 saved_type value) {\n  // If the value says it wasn't saved, trust that it's still dominating.\n  if (!value.getInt()) return value.getPointer();\n\n  // Otherwise, it should be an alloca instruction, as set up in save().\n  auto alloca = cast<llvm::AllocaInst>(value.getPointer());\n  return CGF.Builder.CreateAlignedLoad(alloca, alloca->getAlign());\n}\n\n}  // end namespace CodeGen\n\n// Map the LangOption for floating point exception behavior into\n// the corresponding enum in the IR.\nllvm::fp::ExceptionBehavior\nToConstrainedExceptMD(LangOptions::FPExceptionModeKind Kind);\n}  // end namespace clang\n\n#endif\n"}, "43": {"id": 43, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/ConstantEmitter.h", "content": "//===--- ConstantEmitter.h - IR constant emission ---------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// A helper class for emitting expressions and values as llvm::Constants\n// and as initializers for global variables.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CONSTANTEMITTER_H\n#define LLVM_CLANG_LIB_CODEGEN_CONSTANTEMITTER_H\n\n#include \"CodeGenFunction.h\"\n#include \"CodeGenModule.h\"\n\nnamespace clang {\nnamespace CodeGen {\n\nclass ConstantEmitter {\npublic:\n  CodeGenModule &CGM;\n  CodeGenFunction *const CGF;\n\nprivate:\n  bool Abstract = false;\n\n  /// Whether non-abstract components of the emitter have been initialized.\n  bool InitializedNonAbstract = false;\n\n  /// Whether the emitter has been finalized.\n  bool Finalized = false;\n\n  /// Whether the constant-emission failed.\n  bool Failed = false;\n\n  /// Whether we're in a constant context.\n  bool InConstantContext = false;\n\n  /// The AST address space where this (non-abstract) initializer is going.\n  /// Used for generating appropriate placeholders.\n  LangAS DestAddressSpace;\n\n  llvm::SmallVector<std::pair<llvm::Constant *, llvm::GlobalVariable*>, 4>\n    PlaceholderAddresses;\n\npublic:\n  ConstantEmitter(CodeGenModule &CGM, CodeGenFunction *CGF = nullptr)\n    : CGM(CGM), CGF(CGF) {}\n\n  /// Initialize this emission in the context of the given function.\n  /// Use this if the expression might contain contextual references like\n  /// block addresses or PredefinedExprs.\n  ConstantEmitter(CodeGenFunction &CGF)\n    : CGM(CGF.CGM), CGF(&CGF) {}\n\n  ConstantEmitter(const ConstantEmitter &other) = delete;\n  ConstantEmitter &operator=(const ConstantEmitter &other) = delete;\n\n  ~ConstantEmitter();\n\n  /// Is the current emission context abstract?\n  bool isAbstract() const {\n    return Abstract;\n  }\n\n  /// Try to emit the initiaizer of the given declaration as an abstract\n  /// constant.  If this succeeds, the emission must be finalized.\n  llvm::Constant *tryEmitForInitializer(const VarDecl &D);\n  llvm::Constant *tryEmitForInitializer(const Expr *E, LangAS destAddrSpace,\n                                        QualType destType);\n  llvm::Constant *emitForInitializer(const APValue &value, LangAS destAddrSpace,\n                                     QualType destType);\n\n  void finalize(llvm::GlobalVariable *global);\n\n  // All of the \"abstract\" emission methods below permit the emission to\n  // be immediately discarded without finalizing anything.  Therefore, they\n  // must also promise not to do anything that will, in the future, require\n  // finalization:\n  //\n  //   - using the CGF (if present) for anything other than establishing\n  //     semantic context; for example, an expression with ignored\n  //     side-effects must not be emitted as an abstract expression\n  //\n  //   - doing anything that would not be safe to duplicate within an\n  //     initializer or to propagate to another context; for example,\n  //     side effects, or emitting an initialization that requires a\n  //     reference to its current location.\n\n  /// Try to emit the initializer of the given declaration as an abstract\n  /// constant.\n  llvm::Constant *tryEmitAbstractForInitializer(const VarDecl &D);\n\n  /// Emit the result of the given expression as an abstract constant,\n  /// asserting that it succeeded.  This is only safe to do when the\n  /// expression is known to be a constant expression with either a fairly\n  /// simple type or a known simple form.\n  llvm::Constant *emitAbstract(const Expr *E, QualType T);\n  llvm::Constant *emitAbstract(SourceLocation loc, const APValue &value,\n                               QualType T);\n\n  /// Try to emit the result of the given expression as an abstract constant.\n  llvm::Constant *tryEmitAbstract(const Expr *E, QualType T);\n  llvm::Constant *tryEmitAbstractForMemory(const Expr *E, QualType T);\n\n  llvm::Constant *tryEmitAbstract(const APValue &value, QualType T);\n  llvm::Constant *tryEmitAbstractForMemory(const APValue &value, QualType T);\n\n  llvm::Constant *tryEmitConstantExpr(const ConstantExpr *CE);\n\n  llvm::Constant *emitNullForMemory(QualType T) {\n    return emitNullForMemory(CGM, T);\n  }\n  llvm::Constant *emitForMemory(llvm::Constant *C, QualType T) {\n    return emitForMemory(CGM, C, T);\n  }\n\n  static llvm::Constant *emitNullForMemory(CodeGenModule &CGM, QualType T);\n  static llvm::Constant *emitForMemory(CodeGenModule &CGM, llvm::Constant *C,\n                                       QualType T);\n\n  // These are private helper routines of the constant emitter that\n  // can't actually be private because things are split out into helper\n  // functions and classes.\n\n  llvm::Constant *tryEmitPrivateForVarInit(const VarDecl &D);\n\n  llvm::Constant *tryEmitPrivate(const Expr *E, QualType T);\n  llvm::Constant *tryEmitPrivateForMemory(const Expr *E, QualType T);\n\n  llvm::Constant *tryEmitPrivate(const APValue &value, QualType T);\n  llvm::Constant *tryEmitPrivateForMemory(const APValue &value, QualType T);\n\n  /// Get the address of the current location.  This is a constant\n  /// that will resolve, after finalization, to the address of the\n  /// 'signal' value that is registered with the emitter later.\n  llvm::GlobalValue *getCurrentAddrPrivate();\n\n  /// Register a 'signal' value with the emitter to inform it where to\n  /// resolve a placeholder.  The signal value must be unique in the\n  /// initializer; it might, for example, be the address of a global that\n  /// refers to the current-address value in its own initializer.\n  ///\n  /// Uses of the placeholder must be properly anchored before finalizing\n  /// the emitter, e.g. by being installed as the initializer of a global\n  /// variable.  That is, it must be possible to replaceAllUsesWith\n  /// the placeholder with the proper address of the signal.\n  void registerCurrentAddrPrivate(llvm::Constant *signal,\n                                  llvm::GlobalValue *placeholder);\n\nprivate:\n  void initializeNonAbstract(LangAS destAS) {\n    assert(!InitializedNonAbstract);\n    InitializedNonAbstract = true;\n    DestAddressSpace = destAS;\n  }\n  llvm::Constant *markIfFailed(llvm::Constant *init) {\n    if (!init)\n      Failed = true;\n    return init;\n  }\n\n  struct AbstractState {\n    bool OldValue;\n    size_t OldPlaceholdersSize;\n  };\n  AbstractState pushAbstract() {\n    AbstractState saved = { Abstract, PlaceholderAddresses.size() };\n    Abstract = true;\n    return saved;\n  }\n  llvm::Constant *validateAndPopAbstract(llvm::Constant *C, AbstractState save);\n};\n\n}\n}\n\n#endif\n"}, "62": {"id": 62, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_base.h", "content": "//===- llvm/ADT/ilist_base.h - Intrusive List Base --------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_ILIST_BASE_H\n#define LLVM_ADT_ILIST_BASE_H\n\n#include \"llvm/ADT/ilist_node_base.h\"\n#include <cassert>\n\nnamespace llvm {\n\n/// Implementations of list algorithms using ilist_node_base.\ntemplate <bool EnableSentinelTracking> class ilist_base {\npublic:\n  using node_base_type = ilist_node_base<EnableSentinelTracking>;\n\n  static void insertBeforeImpl(node_base_type &Next, node_base_type &N) {\n    node_base_type &Prev = *Next.getPrev();\n    N.setNext(&Next);\n    N.setPrev(&Prev);\n    Prev.setNext(&N);\n    Next.setPrev(&N);\n  }\n\n  static void removeImpl(node_base_type &N) {\n    node_base_type *Prev = N.getPrev();\n    node_base_type *Next = N.getNext();\n    Next->setPrev(Prev);\n    Prev->setNext(Next);\n\n    // Not strictly necessary, but helps catch a class of bugs.\n    N.setPrev(nullptr);\n    N.setNext(nullptr);\n  }\n\n  static void removeRangeImpl(node_base_type &First, node_base_type &Last) {\n    node_base_type *Prev = First.getPrev();\n    node_base_type *Final = Last.getPrev();\n    Last.setPrev(Prev);\n    Prev->setNext(&Last);\n\n    // Not strictly necessary, but helps catch a class of bugs.\n    First.setPrev(nullptr);\n    Final->setNext(nullptr);\n  }\n\n  static void transferBeforeImpl(node_base_type &Next, node_base_type &First,\n                                 node_base_type &Last) {\n    if (&Next == &Last || &First == &Last)\n      return;\n\n    // Position cannot be contained in the range to be transferred.\n    assert(&Next != &First &&\n           // Check for the most common mistake.\n           \"Insertion point can't be one of the transferred nodes\");\n\n    node_base_type &Final = *Last.getPrev();\n\n    // Detach from old list/position.\n    First.getPrev()->setNext(&Last);\n    Last.setPrev(First.getPrev());\n\n    // Splice [First, Final] into its new list/position.\n    node_base_type &Prev = *Next.getPrev();\n    Final.setNext(&Next);\n    First.setPrev(&Prev);\n    Prev.setNext(&First);\n    Next.setPrev(&Final);\n  }\n\n  template <class T> static void insertBefore(T &Next, T &N) {\n    insertBeforeImpl(Next, N);\n  }\n\n  template <class T> static void remove(T &N) { removeImpl(N); }\n  template <class T> static void removeRange(T &First, T &Last) {\n    removeRangeImpl(First, Last);\n  }\n\n  template <class T> static void transferBefore(T &Next, T &First, T &Last) {\n    transferBeforeImpl(Next, First, Last);\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_ILIST_BASE_H\n"}, "63": {"id": 63, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_iterator.h", "content": "//===- llvm/ADT/ilist_iterator.h - Intrusive List Iterator ------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_ILIST_ITERATOR_H\n#define LLVM_ADT_ILIST_ITERATOR_H\n\n#include \"llvm/ADT/ilist_node.h\"\n#include <cassert>\n#include <cstddef>\n#include <iterator>\n#include <type_traits>\n\nnamespace llvm {\n\nnamespace ilist_detail {\n\n/// Find const-correct node types.\ntemplate <class OptionsT, bool IsConst> struct IteratorTraits;\ntemplate <class OptionsT> struct IteratorTraits<OptionsT, false> {\n  using value_type = typename OptionsT::value_type;\n  using pointer = typename OptionsT::pointer;\n  using reference = typename OptionsT::reference;\n  using node_pointer = ilist_node_impl<OptionsT> *;\n  using node_reference = ilist_node_impl<OptionsT> &;\n};\ntemplate <class OptionsT> struct IteratorTraits<OptionsT, true> {\n  using value_type = const typename OptionsT::value_type;\n  using pointer = typename OptionsT::const_pointer;\n  using reference = typename OptionsT::const_reference;\n  using node_pointer = const ilist_node_impl<OptionsT> *;\n  using node_reference = const ilist_node_impl<OptionsT> &;\n};\n\ntemplate <bool IsReverse> struct IteratorHelper;\ntemplate <> struct IteratorHelper<false> : ilist_detail::NodeAccess {\n  using Access = ilist_detail::NodeAccess;\n\n  template <class T> static void increment(T *&I) { I = Access::getNext(*I); }\n  template <class T> static void decrement(T *&I) { I = Access::getPrev(*I); }\n};\ntemplate <> struct IteratorHelper<true> : ilist_detail::NodeAccess {\n  using Access = ilist_detail::NodeAccess;\n\n  template <class T> static void increment(T *&I) { I = Access::getPrev(*I); }\n  template <class T> static void decrement(T *&I) { I = Access::getNext(*I); }\n};\n\n} // end namespace ilist_detail\n\n/// Iterator for intrusive lists  based on ilist_node.\ntemplate <class OptionsT, bool IsReverse, bool IsConst>\nclass ilist_iterator : ilist_detail::SpecificNodeAccess<OptionsT> {\n  friend ilist_iterator<OptionsT, IsReverse, !IsConst>;\n  friend ilist_iterator<OptionsT, !IsReverse, IsConst>;\n  friend ilist_iterator<OptionsT, !IsReverse, !IsConst>;\n\n  using Traits = ilist_detail::IteratorTraits<OptionsT, IsConst>;\n  using Access = ilist_detail::SpecificNodeAccess<OptionsT>;\n\npublic:\n  using value_type = typename Traits::value_type;\n  using pointer = typename Traits::pointer;\n  using reference = typename Traits::reference;\n  using difference_type = ptrdiff_t;\n  using iterator_category = std::bidirectional_iterator_tag;\n  using const_pointer = typename OptionsT::const_pointer;\n  using const_reference = typename OptionsT::const_reference;\n\nprivate:\n  using node_pointer = typename Traits::node_pointer;\n  using node_reference = typename Traits::node_reference;\n\n  node_pointer NodePtr = nullptr;\n\npublic:\n  /// Create from an ilist_node.\n  explicit ilist_iterator(node_reference N) : NodePtr(&N) {}\n\n  explicit ilist_iterator(pointer NP) : NodePtr(Access::getNodePtr(NP)) {}\n  explicit ilist_iterator(reference NR) : NodePtr(Access::getNodePtr(&NR)) {}\n  ilist_iterator() = default;\n\n  // This is templated so that we can allow constructing a const iterator from\n  // a nonconst iterator...\n  template <bool RHSIsConst>\n  ilist_iterator(const ilist_iterator<OptionsT, IsReverse, RHSIsConst> &RHS,\n                 std::enable_if_t<IsConst || !RHSIsConst, void *> = nullptr)\n      : NodePtr(RHS.NodePtr) {}\n\n  // This is templated so that we can allow assigning to a const iterator from\n  // a nonconst iterator...\n  template <bool RHSIsConst>\n  std::enable_if_t<IsConst || !RHSIsConst, ilist_iterator &>\n  operator=(const ilist_iterator<OptionsT, IsReverse, RHSIsConst> &RHS) {\n    NodePtr = RHS.NodePtr;\n    return *this;\n  }\n\n  /// Explicit conversion between forward/reverse iterators.\n  ///\n  /// Translate between forward and reverse iterators without changing range\n  /// boundaries.  The resulting iterator will dereference (and have a handle)\n  /// to the previous node, which is somewhat unexpected; but converting the\n  /// two endpoints in a range will give the same range in reverse.\n  ///\n  /// This matches std::reverse_iterator conversions.\n  explicit ilist_iterator(\n      const ilist_iterator<OptionsT, !IsReverse, IsConst> &RHS)\n      : ilist_iterator(++RHS.getReverse()) {}\n\n  /// Get a reverse iterator to the same node.\n  ///\n  /// Gives a reverse iterator that will dereference (and have a handle) to the\n  /// same node.  Converting the endpoint iterators in a range will give a\n  /// different range; for range operations, use the explicit conversions.\n  ilist_iterator<OptionsT, !IsReverse, IsConst> getReverse() const {\n    if (NodePtr)\n      return ilist_iterator<OptionsT, !IsReverse, IsConst>(*NodePtr);\n    return ilist_iterator<OptionsT, !IsReverse, IsConst>();\n  }\n\n  /// Const-cast.\n  ilist_iterator<OptionsT, IsReverse, false> getNonConst() const {\n    if (NodePtr)\n      return ilist_iterator<OptionsT, IsReverse, false>(\n          const_cast<typename ilist_iterator<OptionsT, IsReverse,\n                                             false>::node_reference>(*NodePtr));\n    return ilist_iterator<OptionsT, IsReverse, false>();\n  }\n\n  // Accessors...\n  reference operator*() const {\n    assert(!NodePtr->isKnownSentinel());\n    return *Access::getValuePtr(NodePtr);\n  }\n  pointer operator->() const { return &operator*(); }\n\n  // Comparison operators\n  friend bool operator==(const ilist_iterator &LHS, const ilist_iterator &RHS) {\n    return LHS.NodePtr == RHS.NodePtr;\n  }\n  friend bool operator!=(const ilist_iterator &LHS, const ilist_iterator &RHS) {\n    return LHS.NodePtr != RHS.NodePtr;\n  }\n\n  // Increment and decrement operators...\n  ilist_iterator &operator--() {\n    NodePtr = IsReverse ? NodePtr->getNext() : NodePtr->getPrev();\n    return *this;\n  }\n  ilist_iterator &operator++() {\n    NodePtr = IsReverse ? NodePtr->getPrev() : NodePtr->getNext();\n    return *this;\n  }\n  ilist_iterator operator--(int) {\n    ilist_iterator tmp = *this;\n    --*this;\n    return tmp;\n  }\n  ilist_iterator operator++(int) {\n    ilist_iterator tmp = *this;\n    ++*this;\n    return tmp;\n  }\n\n  /// Get the underlying ilist_node.\n  node_pointer getNodePtr() const { return static_cast<node_pointer>(NodePtr); }\n\n  /// Check for end.  Only valid if ilist_sentinel_tracking<true>.\n  bool isEnd() const { return NodePtr ? NodePtr->isSentinel() : false; }\n};\n\ntemplate <typename From> struct simplify_type;\n\n/// Allow ilist_iterators to convert into pointers to a node automatically when\n/// used by the dyn_cast, cast, isa mechanisms...\n///\n/// FIXME: remove this, since there is no implicit conversion to NodeTy.\ntemplate <class OptionsT, bool IsConst>\nstruct simplify_type<ilist_iterator<OptionsT, false, IsConst>> {\n  using iterator = ilist_iterator<OptionsT, false, IsConst>;\n  using SimpleType = typename iterator::pointer;\n\n  static SimpleType getSimplifiedValue(const iterator &Node) { return &*Node; }\n};\ntemplate <class OptionsT, bool IsConst>\nstruct simplify_type<const ilist_iterator<OptionsT, false, IsConst>>\n    : simplify_type<ilist_iterator<OptionsT, false, IsConst>> {};\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_ILIST_ITERATOR_H\n"}, "64": {"id": 64, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "content": "//===- llvm/ADT/ilist_node.h - Intrusive Linked List Helper -----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the ilist_node class template, which is a convenient\n// base class for creating classes that can be used with ilists.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_ILIST_NODE_H\n#define LLVM_ADT_ILIST_NODE_H\n\n#include \"llvm/ADT/ilist_node_base.h\"\n#include \"llvm/ADT/ilist_node_options.h\"\n\nnamespace llvm {\n\nnamespace ilist_detail {\n\nstruct NodeAccess;\n\n} // end namespace ilist_detail\n\ntemplate <class OptionsT, bool IsReverse, bool IsConst> class ilist_iterator;\ntemplate <class OptionsT> class ilist_sentinel;\n\n/// Implementation for an ilist node.\n///\n/// Templated on an appropriate \\a ilist_detail::node_options, usually computed\n/// by \\a ilist_detail::compute_node_options.\n///\n/// This is a wrapper around \\a ilist_node_base whose main purpose is to\n/// provide type safety: you can't insert nodes of \\a ilist_node_impl into the\n/// wrong \\a simple_ilist or \\a iplist.\ntemplate <class OptionsT> class ilist_node_impl : OptionsT::node_base_type {\n  using value_type = typename OptionsT::value_type;\n  using node_base_type = typename OptionsT::node_base_type;\n  using list_base_type = typename OptionsT::list_base_type;\n\n  friend typename OptionsT::list_base_type;\n  friend struct ilist_detail::NodeAccess;\n  friend class ilist_sentinel<OptionsT>;\n  friend class ilist_iterator<OptionsT, false, false>;\n  friend class ilist_iterator<OptionsT, false, true>;\n  friend class ilist_iterator<OptionsT, true, false>;\n  friend class ilist_iterator<OptionsT, true, true>;\n\nprotected:\n  using self_iterator = ilist_iterator<OptionsT, false, false>;\n  using const_self_iterator = ilist_iterator<OptionsT, false, true>;\n  using reverse_self_iterator = ilist_iterator<OptionsT, true, false>;\n  using const_reverse_self_iterator = ilist_iterator<OptionsT, true, true>;\n\n  ilist_node_impl() = default;\n\nprivate:\n  ilist_node_impl *getPrev() {\n    return static_cast<ilist_node_impl *>(node_base_type::getPrev());\n  }\n\n  ilist_node_impl *getNext() {\n    return static_cast<ilist_node_impl *>(node_base_type::getNext());\n  }\n\n  const ilist_node_impl *getPrev() const {\n    return static_cast<ilist_node_impl *>(node_base_type::getPrev());\n  }\n\n  const ilist_node_impl *getNext() const {\n    return static_cast<ilist_node_impl *>(node_base_type::getNext());\n  }\n\n  void setPrev(ilist_node_impl *N) { node_base_type::setPrev(N); }\n  void setNext(ilist_node_impl *N) { node_base_type::setNext(N); }\n\npublic:\n  self_iterator getIterator() { return self_iterator(*this); }\n  const_self_iterator getIterator() const { return const_self_iterator(*this); }\n\n  reverse_self_iterator getReverseIterator() {\n    return reverse_self_iterator(*this);\n  }\n\n  const_reverse_self_iterator getReverseIterator() const {\n    return const_reverse_self_iterator(*this);\n  }\n\n  // Under-approximation, but always available for assertions.\n  using node_base_type::isKnownSentinel;\n\n  /// Check whether this is the sentinel node.\n  ///\n  /// This requires sentinel tracking to be explicitly enabled.  Use the\n  /// ilist_sentinel_tracking<true> option to get this API.\n  bool isSentinel() const {\n    static_assert(OptionsT::is_sentinel_tracking_explicit,\n                  \"Use ilist_sentinel_tracking<true> to enable isSentinel()\");\n    return node_base_type::isSentinel();\n  }\n};\n\n/// An intrusive list node.\n///\n/// A base class to enable membership in intrusive lists, including \\a\n/// simple_ilist, \\a iplist, and \\a ilist.  The first template parameter is the\n/// \\a value_type for the list.\n///\n/// An ilist node can be configured with compile-time options to change\n/// behaviour and/or add API.\n///\n/// By default, an \\a ilist_node knows whether it is the list sentinel (an\n/// instance of \\a ilist_sentinel) if and only if\n/// LLVM_ENABLE_ABI_BREAKING_CHECKS.  The function \\a isKnownSentinel() always\n/// returns \\c false tracking is off.  Sentinel tracking steals a bit from the\n/// \"prev\" link, which adds a mask operation when decrementing an iterator, but\n/// enables bug-finding assertions in \\a ilist_iterator.\n///\n/// To turn sentinel tracking on all the time, pass in the\n/// ilist_sentinel_tracking<true> template parameter.  This also enables the \\a\n/// isSentinel() function.  The same option must be passed to the intrusive\n/// list.  (ilist_sentinel_tracking<false> turns sentinel tracking off all the\n/// time.)\n///\n/// A type can inherit from ilist_node multiple times by passing in different\n/// \\a ilist_tag options.  This allows a single instance to be inserted into\n/// multiple lists simultaneously, where each list is given the same tag.\n///\n/// \\example\n/// struct A {};\n/// struct B {};\n/// struct N : ilist_node<N, ilist_tag<A>>, ilist_node<N, ilist_tag<B>> {};\n///\n/// void foo() {\n///   simple_ilist<N, ilist_tag<A>> ListA;\n///   simple_ilist<N, ilist_tag<B>> ListB;\n///   N N1;\n///   ListA.push_back(N1);\n///   ListB.push_back(N1);\n/// }\n/// \\endexample\n///\n/// See \\a is_valid_option for steps on adding a new option.\ntemplate <class T, class... Options>\nclass ilist_node\n    : public ilist_node_impl<\n          typename ilist_detail::compute_node_options<T, Options...>::type> {\n  static_assert(ilist_detail::check_options<Options...>::value,\n                \"Unrecognized node option!\");\n};\n\nnamespace ilist_detail {\n\n/// An access class for ilist_node private API.\n///\n/// This gives access to the private parts of ilist nodes.  Nodes for an ilist\n/// should friend this class if they inherit privately from ilist_node.\n///\n/// Using this class outside of the ilist implementation is unsupported.\nstruct NodeAccess {\nprotected:\n  template <class OptionsT>\n  static ilist_node_impl<OptionsT> *getNodePtr(typename OptionsT::pointer N) {\n    return N;\n  }\n\n  template <class OptionsT>\n  static const ilist_node_impl<OptionsT> *\n  getNodePtr(typename OptionsT::const_pointer N) {\n    return N;\n  }\n\n  template <class OptionsT>\n  static typename OptionsT::pointer getValuePtr(ilist_node_impl<OptionsT> *N) {\n    return static_cast<typename OptionsT::pointer>(N);\n  }\n\n  template <class OptionsT>\n  static typename OptionsT::const_pointer\n  getValuePtr(const ilist_node_impl<OptionsT> *N) {\n    return static_cast<typename OptionsT::const_pointer>(N);\n  }\n\n  template <class OptionsT>\n  static ilist_node_impl<OptionsT> *getPrev(ilist_node_impl<OptionsT> &N) {\n    return N.getPrev();\n  }\n\n  template <class OptionsT>\n  static ilist_node_impl<OptionsT> *getNext(ilist_node_impl<OptionsT> &N) {\n    return N.getNext();\n  }\n\n  template <class OptionsT>\n  static const ilist_node_impl<OptionsT> *\n  getPrev(const ilist_node_impl<OptionsT> &N) {\n    return N.getPrev();\n  }\n\n  template <class OptionsT>\n  static const ilist_node_impl<OptionsT> *\n  getNext(const ilist_node_impl<OptionsT> &N) {\n    return N.getNext();\n  }\n};\n\ntemplate <class OptionsT> struct SpecificNodeAccess : NodeAccess {\nprotected:\n  using pointer = typename OptionsT::pointer;\n  using const_pointer = typename OptionsT::const_pointer;\n  using node_type = ilist_node_impl<OptionsT>;\n\n  static node_type *getNodePtr(pointer N) {\n    return NodeAccess::getNodePtr<OptionsT>(N);\n  }\n\n  static const node_type *getNodePtr(const_pointer N) {\n    return NodeAccess::getNodePtr<OptionsT>(N);\n  }\n\n  static pointer getValuePtr(node_type *N) {\n    return NodeAccess::getValuePtr<OptionsT>(N);\n  }\n\n  static const_pointer getValuePtr(const node_type *N) {\n    return NodeAccess::getValuePtr<OptionsT>(N);\n  }\n};\n\n} // end namespace ilist_detail\n\ntemplate <class OptionsT>\nclass ilist_sentinel : public ilist_node_impl<OptionsT> {\npublic:\n  ilist_sentinel() {\n    this->initializeSentinel();\n    reset();\n  }\n\n  void reset() {\n    this->setPrev(this);\n    this->setNext(this);\n  }\n\n  bool empty() const { return this == this->getPrev(); }\n};\n\n/// An ilist node that can access its parent list.\n///\n/// Requires \\c NodeTy to have \\a getParent() to find the parent node, and the\n/// \\c ParentTy to have \\a getSublistAccess() to get a reference to the list.\ntemplate <typename NodeTy, typename ParentTy, class... Options>\nclass ilist_node_with_parent : public ilist_node<NodeTy, Options...> {\nprotected:\n  ilist_node_with_parent() = default;\n\nprivate:\n  /// Forward to NodeTy::getParent().\n  ///\n  /// Note: do not use the name \"getParent()\".  We want a compile error\n  /// (instead of recursion) when the subclass fails to implement \\a\n  /// getParent().\n  const ParentTy *getNodeParent() const {\n    return static_cast<const NodeTy *>(this)->getParent();\n  }\n\npublic:\n  /// @name Adjacent Node Accessors\n  /// @{\n  /// Get the previous node, or \\c nullptr for the list head.\n  NodeTy *getPrevNode() {\n    // Should be separated to a reused function, but then we couldn't use auto\n    // (and would need the type of the list).\n    const auto &List =\n        getNodeParent()->*(ParentTy::getSublistAccess((NodeTy *)nullptr));\n    return List.getPrevNode(*static_cast<NodeTy *>(this));\n  }\n\n  /// Get the previous node, or \\c nullptr for the list head.\n  const NodeTy *getPrevNode() const {\n    return const_cast<ilist_node_with_parent *>(this)->getPrevNode();\n  }\n\n  /// Get the next node, or \\c nullptr for the list tail.\n  NodeTy *getNextNode() {\n    // Should be separated to a reused function, but then we couldn't use auto\n    // (and would need the type of the list).\n    const auto &List =\n        getNodeParent()->*(ParentTy::getSublistAccess((NodeTy *)nullptr));\n    return List.getNextNode(*static_cast<NodeTy *>(this));\n  }\n\n  /// Get the next node, or \\c nullptr for the list tail.\n  const NodeTy *getNextNode() const {\n    return const_cast<ilist_node_with_parent *>(this)->getNextNode();\n  }\n  /// @}\n};\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_ILIST_NODE_H\n"}, "68": {"id": 68, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Attributes.h", "content": "//===- llvm/Attributes.h - Container for Attributes -------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file\n/// This file contains the simple types necessary to represent the\n/// attributes associated with functions and their calls.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_ATTRIBUTES_H\n#define LLVM_IR_ATTRIBUTES_H\n\n#include \"llvm-c/Types.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Config/llvm-config.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/PointerLikeTypeTraits.h\"\n#include <bitset>\n#include <cassert>\n#include <cstdint>\n#include <map>\n#include <string>\n#include <utility>\n\nnamespace llvm {\n\nclass AttrBuilder;\nclass AttributeImpl;\nclass AttributeListImpl;\nclass AttributeSetNode;\ntemplate<typename T> struct DenseMapInfo;\nclass FoldingSetNodeID;\nclass Function;\nclass LLVMContext;\nclass Type;\n\n//===----------------------------------------------------------------------===//\n/// \\class\n/// Functions, function parameters, and return types can have attributes\n/// to indicate how they should be treated by optimizations and code\n/// generation. This class represents one of those attributes. It's light-weight\n/// and should be passed around by-value.\nclass Attribute {\npublic:\n  /// This enumeration lists the attributes that can be associated with\n  /// parameters, function results, or the function itself.\n  ///\n  /// Note: The `uwtable' attribute is about the ABI or the user mandating an\n  /// entry in the unwind table. The `nounwind' attribute is about an exception\n  /// passing by the function.\n  ///\n  /// In a theoretical system that uses tables for profiling and SjLj for\n  /// exceptions, they would be fully independent. In a normal system that uses\n  /// tables for both, the semantics are:\n  ///\n  /// nil                = Needs an entry because an exception might pass by.\n  /// nounwind           = No need for an entry\n  /// uwtable            = Needs an entry because the ABI says so and because\n  ///                      an exception might pass by.\n  /// uwtable + nounwind = Needs an entry because the ABI says so.\n\n  enum AttrKind {\n    // IR-Level Attributes\n    None,                  ///< No attributes have been set\n    #define GET_ATTR_NAMES\n    #define ATTRIBUTE_ENUM(ENUM_NAME, OTHER) ENUM_NAME,\n    #include \"llvm/IR/Attributes.inc\"\n    EndAttrKinds,          ///< Sentinal value useful for loops\n    EmptyKey,              ///< Use as Empty key for DenseMap of AttrKind\n    TombstoneKey,          ///< Use as Tombstone key for DenseMap of AttrKind\n  };\n\nprivate:\n  AttributeImpl *pImpl = nullptr;\n\n  Attribute(AttributeImpl *A) : pImpl(A) {}\n\npublic:\n  Attribute() = default;\n\n  //===--------------------------------------------------------------------===//\n  // Attribute Construction\n  //===--------------------------------------------------------------------===//\n\n  /// Return a uniquified Attribute object.\n  static Attribute get(LLVMContext &Context, AttrKind Kind, uint64_t Val = 0);\n  static Attribute get(LLVMContext &Context, StringRef Kind,\n                       StringRef Val = StringRef());\n  static Attribute get(LLVMContext &Context, AttrKind Kind, Type *Ty);\n\n  /// Return a uniquified Attribute object that has the specific\n  /// alignment set.\n  static Attribute getWithAlignment(LLVMContext &Context, Align Alignment);\n  static Attribute getWithStackAlignment(LLVMContext &Context, Align Alignment);\n  static Attribute getWithDereferenceableBytes(LLVMContext &Context,\n                                              uint64_t Bytes);\n  static Attribute getWithDereferenceableOrNullBytes(LLVMContext &Context,\n                                                     uint64_t Bytes);\n  static Attribute getWithAllocSizeArgs(LLVMContext &Context,\n                                        unsigned ElemSizeArg,\n                                        const Optional<unsigned> &NumElemsArg);\n  static Attribute getWithByValType(LLVMContext &Context, Type *Ty);\n  static Attribute getWithStructRetType(LLVMContext &Context, Type *Ty);\n  static Attribute getWithByRefType(LLVMContext &Context, Type *Ty);\n  static Attribute getWithPreallocatedType(LLVMContext &Context, Type *Ty);\n\n  /// For a typed attribute, return the equivalent attribute with the type\n  /// changed to \\p ReplacementTy.\n  Attribute getWithNewType(LLVMContext &Context, Type *ReplacementTy) {\n    assert(isTypeAttribute() && \"this requires a typed attribute\");\n    return get(Context, getKindAsEnum(), ReplacementTy);\n  }\n\n  static Attribute::AttrKind getAttrKindFromName(StringRef AttrName);\n\n  static StringRef getNameFromAttrKind(Attribute::AttrKind AttrKind);\n\n  /// Return true if and only if the attribute has an Argument.\n  static bool doesAttrKindHaveArgument(Attribute::AttrKind AttrKind);\n\n  /// Return true if the provided string matches the IR name of an attribute.\n  /// example: \"noalias\" return true but not \"NoAlias\"\n  static bool isExistingAttribute(StringRef Name);\n\n  //===--------------------------------------------------------------------===//\n  // Attribute Accessors\n  //===--------------------------------------------------------------------===//\n\n  /// Return true if the attribute is an Attribute::AttrKind type.\n  bool isEnumAttribute() const;\n\n  /// Return true if the attribute is an integer attribute.\n  bool isIntAttribute() const;\n\n  /// Return true if the attribute is a string (target-dependent)\n  /// attribute.\n  bool isStringAttribute() const;\n\n  /// Return true if the attribute is a type attribute.\n  bool isTypeAttribute() const;\n\n  /// Return true if the attribute is any kind of attribute.\n  bool isValid() const { return pImpl; }\n\n  /// Return true if the attribute is present.\n  bool hasAttribute(AttrKind Val) const;\n\n  /// Return true if the target-dependent attribute is present.\n  bool hasAttribute(StringRef Val) const;\n\n  /// Return the attribute's kind as an enum (Attribute::AttrKind). This\n  /// requires the attribute to be an enum or integer attribute.\n  Attribute::AttrKind getKindAsEnum() const;\n\n  /// Return the attribute's value as an integer. This requires that the\n  /// attribute be an integer attribute.\n  uint64_t getValueAsInt() const;\n\n  /// Return the attribute's kind as a string. This requires the\n  /// attribute to be a string attribute.\n  StringRef getKindAsString() const;\n\n  /// Return the attribute's value as a string. This requires the\n  /// attribute to be a string attribute.\n  StringRef getValueAsString() const;\n\n  /// Return the attribute's value as a Type. This requires the attribute to be\n  /// a type attribute.\n  Type *getValueAsType() const;\n\n  /// Returns the alignment field of an attribute as a byte alignment\n  /// value.\n  MaybeAlign getAlignment() const;\n\n  /// Returns the stack alignment field of an attribute as a byte\n  /// alignment value.\n  MaybeAlign getStackAlignment() const;\n\n  /// Returns the number of dereferenceable bytes from the\n  /// dereferenceable attribute.\n  uint64_t getDereferenceableBytes() const;\n\n  /// Returns the number of dereferenceable_or_null bytes from the\n  /// dereferenceable_or_null attribute.\n  uint64_t getDereferenceableOrNullBytes() const;\n\n  /// Returns the argument numbers for the allocsize attribute (or pair(0, 0)\n  /// if not known).\n  std::pair<unsigned, Optional<unsigned>> getAllocSizeArgs() const;\n\n  /// The Attribute is converted to a string of equivalent mnemonic. This\n  /// is, presumably, for writing out the mnemonics for the assembly writer.\n  std::string getAsString(bool InAttrGrp = false) const;\n\n  /// Equality and non-equality operators.\n  bool operator==(Attribute A) const { return pImpl == A.pImpl; }\n  bool operator!=(Attribute A) const { return pImpl != A.pImpl; }\n\n  /// Less-than operator. Useful for sorting the attributes list.\n  bool operator<(Attribute A) const;\n\n  void Profile(FoldingSetNodeID &ID) const;\n\n  /// Return a raw pointer that uniquely identifies this attribute.\n  void *getRawPointer() const {\n    return pImpl;\n  }\n\n  /// Get an attribute from a raw pointer created by getRawPointer.\n  static Attribute fromRawPointer(void *RawPtr) {\n    return Attribute(reinterpret_cast<AttributeImpl*>(RawPtr));\n  }\n};\n\n// Specialized opaque value conversions.\ninline LLVMAttributeRef wrap(Attribute Attr) {\n  return reinterpret_cast<LLVMAttributeRef>(Attr.getRawPointer());\n}\n\n// Specialized opaque value conversions.\ninline Attribute unwrap(LLVMAttributeRef Attr) {\n  return Attribute::fromRawPointer(Attr);\n}\n\n//===----------------------------------------------------------------------===//\n/// \\class\n/// This class holds the attributes for a particular argument, parameter,\n/// function, or return value. It is an immutable value type that is cheap to\n/// copy. Adding and removing enum attributes is intended to be fast, but adding\n/// and removing string or integer attributes involves a FoldingSet lookup.\nclass AttributeSet {\n  friend AttributeListImpl;\n  template <typename Ty> friend struct DenseMapInfo;\n\n  // TODO: Extract AvailableAttrs from AttributeSetNode and store them here.\n  // This will allow an efficient implementation of addAttribute and\n  // removeAttribute for enum attrs.\n\n  /// Private implementation pointer.\n  AttributeSetNode *SetNode = nullptr;\n\nprivate:\n  explicit AttributeSet(AttributeSetNode *ASN) : SetNode(ASN) {}\n\npublic:\n  /// AttributeSet is a trivially copyable value type.\n  AttributeSet() = default;\n  AttributeSet(const AttributeSet &) = default;\n  ~AttributeSet() = default;\n\n  static AttributeSet get(LLVMContext &C, const AttrBuilder &B);\n  static AttributeSet get(LLVMContext &C, ArrayRef<Attribute> Attrs);\n\n  bool operator==(const AttributeSet &O) const { return SetNode == O.SetNode; }\n  bool operator!=(const AttributeSet &O) const { return !(*this == O); }\n\n  /// Add an argument attribute. Returns a new set because attribute sets are\n  /// immutable.\n  LLVM_NODISCARD AttributeSet addAttribute(LLVMContext &C,\n                                           Attribute::AttrKind Kind) const;\n\n  /// Add a target-dependent attribute. Returns a new set because attribute sets\n  /// are immutable.\n  LLVM_NODISCARD AttributeSet addAttribute(LLVMContext &C, StringRef Kind,\n                                           StringRef Value = StringRef()) const;\n\n  /// Add attributes to the attribute set. Returns a new set because attribute\n  /// sets are immutable.\n  LLVM_NODISCARD AttributeSet addAttributes(LLVMContext &C,\n                                            AttributeSet AS) const;\n\n  /// Remove the specified attribute from this set. Returns a new set because\n  /// attribute sets are immutable.\n  LLVM_NODISCARD AttributeSet removeAttribute(LLVMContext &C,\n                                              Attribute::AttrKind Kind) const;\n\n  /// Remove the specified attribute from this set. Returns a new set because\n  /// attribute sets are immutable.\n  LLVM_NODISCARD AttributeSet removeAttribute(LLVMContext &C,\n                                              StringRef Kind) const;\n\n  /// Remove the specified attributes from this set. Returns a new set because\n  /// attribute sets are immutable.\n  LLVM_NODISCARD AttributeSet\n  removeAttributes(LLVMContext &C, const AttrBuilder &AttrsToRemove) const;\n\n  /// Return the number of attributes in this set.\n  unsigned getNumAttributes() const;\n\n  /// Return true if attributes exists in this set.\n  bool hasAttributes() const { return SetNode != nullptr; }\n\n  /// Return true if the attribute exists in this set.\n  bool hasAttribute(Attribute::AttrKind Kind) const;\n\n  /// Return true if the attribute exists in this set.\n  bool hasAttribute(StringRef Kind) const;\n\n  /// Return the attribute object.\n  Attribute getAttribute(Attribute::AttrKind Kind) const;\n\n  /// Return the target-dependent attribute object.\n  Attribute getAttribute(StringRef Kind) const;\n\n  MaybeAlign getAlignment() const;\n  MaybeAlign getStackAlignment() const;\n  uint64_t getDereferenceableBytes() const;\n  uint64_t getDereferenceableOrNullBytes() const;\n  Type *getByValType() const;\n  Type *getStructRetType() const;\n  Type *getByRefType() const;\n  Type *getPreallocatedType() const;\n  std::pair<unsigned, Optional<unsigned>> getAllocSizeArgs() const;\n  std::string getAsString(bool InAttrGrp = false) const;\n\n  using iterator = const Attribute *;\n\n  iterator begin() const;\n  iterator end() const;\n#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)\n  void dump() const;\n#endif\n};\n\n//===----------------------------------------------------------------------===//\n/// \\class\n/// Provide DenseMapInfo for AttributeSet.\ntemplate <> struct DenseMapInfo<AttributeSet> {\n  static AttributeSet getEmptyKey() {\n    auto Val = static_cast<uintptr_t>(-1);\n    Val <<= PointerLikeTypeTraits<void *>::NumLowBitsAvailable;\n    return AttributeSet(reinterpret_cast<AttributeSetNode *>(Val));\n  }\n\n  static AttributeSet getTombstoneKey() {\n    auto Val = static_cast<uintptr_t>(-2);\n    Val <<= PointerLikeTypeTraits<void *>::NumLowBitsAvailable;\n    return AttributeSet(reinterpret_cast<AttributeSetNode *>(Val));\n  }\n\n  static unsigned getHashValue(AttributeSet AS) {\n    return (unsigned((uintptr_t)AS.SetNode) >> 4) ^\n           (unsigned((uintptr_t)AS.SetNode) >> 9);\n  }\n\n  static bool isEqual(AttributeSet LHS, AttributeSet RHS) { return LHS == RHS; }\n};\n\n//===----------------------------------------------------------------------===//\n/// \\class\n/// This class holds the attributes for a function, its return value, and\n/// its parameters. You access the attributes for each of them via an index into\n/// the AttributeList object. The function attributes are at index\n/// `AttributeList::FunctionIndex', the return value is at index\n/// `AttributeList::ReturnIndex', and the attributes for the parameters start at\n/// index `AttributeList::FirstArgIndex'.\nclass AttributeList {\npublic:\n  enum AttrIndex : unsigned {\n    ReturnIndex = 0U,\n    FunctionIndex = ~0U,\n    FirstArgIndex = 1,\n  };\n\nprivate:\n  friend class AttrBuilder;\n  friend class AttributeListImpl;\n  friend class AttributeSet;\n  friend class AttributeSetNode;\n  template <typename Ty> friend struct DenseMapInfo;\n\n  /// The attributes that we are managing. This can be null to represent\n  /// the empty attributes list.\n  AttributeListImpl *pImpl = nullptr;\n\npublic:\n  /// Create an AttributeList with the specified parameters in it.\n  static AttributeList get(LLVMContext &C,\n                           ArrayRef<std::pair<unsigned, Attribute>> Attrs);\n  static AttributeList get(LLVMContext &C,\n                           ArrayRef<std::pair<unsigned, AttributeSet>> Attrs);\n\n  /// Create an AttributeList from attribute sets for a function, its\n  /// return value, and all of its arguments.\n  static AttributeList get(LLVMContext &C, AttributeSet FnAttrs,\n                           AttributeSet RetAttrs,\n                           ArrayRef<AttributeSet> ArgAttrs);\n\nprivate:\n  explicit AttributeList(AttributeListImpl *LI) : pImpl(LI) {}\n\n  static AttributeList getImpl(LLVMContext &C, ArrayRef<AttributeSet> AttrSets);\n\n  AttributeList setAttributes(LLVMContext &C, unsigned Index,\n                              AttributeSet Attrs) const;\n\npublic:\n  AttributeList() = default;\n\n  //===--------------------------------------------------------------------===//\n  // AttributeList Construction and Mutation\n  //===--------------------------------------------------------------------===//\n\n  /// Return an AttributeList with the specified parameters in it.\n  static AttributeList get(LLVMContext &C, ArrayRef<AttributeList> Attrs);\n  static AttributeList get(LLVMContext &C, unsigned Index,\n                           ArrayRef<Attribute::AttrKind> Kinds);\n  static AttributeList get(LLVMContext &C, unsigned Index,\n                           ArrayRef<Attribute::AttrKind> Kinds,\n                           ArrayRef<uint64_t> Values);\n  static AttributeList get(LLVMContext &C, unsigned Index,\n                           ArrayRef<StringRef> Kind);\n  static AttributeList get(LLVMContext &C, unsigned Index,\n                           const AttrBuilder &B);\n\n  /// Add an attribute to the attribute set at the given index.\n  /// Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addAttribute(LLVMContext &C, unsigned Index,\n                                            Attribute::AttrKind Kind) const;\n\n  /// Add an attribute to the attribute set at the given index.\n  /// Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList\n  addAttribute(LLVMContext &C, unsigned Index, StringRef Kind,\n               StringRef Value = StringRef()) const;\n\n  /// Add an attribute to the attribute set at the given index.\n  /// Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addAttribute(LLVMContext &C, unsigned Index,\n                                            Attribute A) const;\n\n  /// Add attributes to the attribute set at the given index.\n  /// Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addAttributes(LLVMContext &C, unsigned Index,\n                                             const AttrBuilder &B) const;\n\n  /// Add an argument attribute to the list. Returns a new list because\n  /// attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addParamAttribute(\n      LLVMContext &C, unsigned ArgNo, Attribute::AttrKind Kind) const {\n    return addAttribute(C, ArgNo + FirstArgIndex, Kind);\n  }\n\n  /// Add an argument attribute to the list. Returns a new list because\n  /// attribute lists are immutable.\n  LLVM_NODISCARD AttributeList\n  addParamAttribute(LLVMContext &C, unsigned ArgNo, StringRef Kind,\n                    StringRef Value = StringRef()) const {\n    return addAttribute(C, ArgNo + FirstArgIndex, Kind, Value);\n  }\n\n  /// Add an attribute to the attribute list at the given arg indices. Returns a\n  /// new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addParamAttribute(LLVMContext &C,\n                                                 ArrayRef<unsigned> ArgNos,\n                                                 Attribute A) const;\n\n  /// Add an argument attribute to the list. Returns a new list because\n  /// attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addParamAttributes(LLVMContext &C,\n                                                  unsigned ArgNo,\n                                                  const AttrBuilder &B) const {\n    return addAttributes(C, ArgNo + FirstArgIndex, B);\n  }\n\n  /// Remove the specified attribute at the specified index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeAttribute(LLVMContext &C, unsigned Index,\n                                               Attribute::AttrKind Kind) const;\n\n  /// Remove the specified attribute at the specified index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeAttribute(LLVMContext &C, unsigned Index,\n                                               StringRef Kind) const;\n\n  /// Remove the specified attributes at the specified index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeAttributes(\n      LLVMContext &C, unsigned Index, const AttrBuilder &AttrsToRemove) const;\n\n  /// Remove all attributes at the specified index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeAttributes(LLVMContext &C,\n                                                unsigned Index) const;\n\n  /// Remove the specified attribute at the specified arg index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeParamAttribute(\n      LLVMContext &C, unsigned ArgNo, Attribute::AttrKind Kind) const {\n    return removeAttribute(C, ArgNo + FirstArgIndex, Kind);\n  }\n\n  /// Remove the specified attribute at the specified arg index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeParamAttribute(LLVMContext &C,\n                                                    unsigned ArgNo,\n                                                    StringRef Kind) const {\n    return removeAttribute(C, ArgNo + FirstArgIndex, Kind);\n  }\n\n  /// Remove the specified attribute at the specified arg index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeParamAttributes(\n      LLVMContext &C, unsigned ArgNo, const AttrBuilder &AttrsToRemove) const {\n    return removeAttributes(C, ArgNo + FirstArgIndex, AttrsToRemove);\n  }\n\n  /// Remove all attributes at the specified arg index from this\n  /// attribute list. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList removeParamAttributes(LLVMContext &C,\n                                                     unsigned ArgNo) const {\n    return removeAttributes(C, ArgNo + FirstArgIndex);\n  }\n\n  /// Replace the type contained by attribute \\p AttrKind at index \\p ArgNo wih\n  /// \\p ReplacementTy, preserving all other attributes.\n  LLVM_NODISCARD AttributeList replaceAttributeType(LLVMContext &C,\n                                                    unsigned ArgNo,\n                                                    Attribute::AttrKind Kind,\n                                                    Type *ReplacementTy) const {\n    Attribute Attr = getAttribute(ArgNo, Kind);\n    auto Attrs = removeAttribute(C, ArgNo, Kind);\n    return Attrs.addAttribute(C, ArgNo, Attr.getWithNewType(C, ReplacementTy));\n  }\n\n  /// \\brief Add the dereferenceable attribute to the attribute set at the given\n  /// index. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addDereferenceableAttr(LLVMContext &C,\n                                                      unsigned Index,\n                                                      uint64_t Bytes) const;\n\n  /// \\brief Add the dereferenceable attribute to the attribute set at the given\n  /// arg index. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addDereferenceableParamAttr(\n      LLVMContext &C, unsigned ArgNo, uint64_t Bytes) const {\n    return addDereferenceableAttr(C, ArgNo + FirstArgIndex, Bytes);\n  }\n\n  /// Add the dereferenceable_or_null attribute to the attribute set at\n  /// the given index. Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList addDereferenceableOrNullAttr(\n      LLVMContext &C, unsigned Index, uint64_t Bytes) const;\n\n  /// Add the dereferenceable_or_null attribute to the attribute set at\n  /// the given arg index. Returns a new list because attribute lists are\n  /// immutable.\n  LLVM_NODISCARD AttributeList addDereferenceableOrNullParamAttr(\n      LLVMContext &C, unsigned ArgNo, uint64_t Bytes) const {\n    return addDereferenceableOrNullAttr(C, ArgNo + FirstArgIndex, Bytes);\n  }\n\n  /// Add the allocsize attribute to the attribute set at the given index.\n  /// Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList\n  addAllocSizeAttr(LLVMContext &C, unsigned Index, unsigned ElemSizeArg,\n                   const Optional<unsigned> &NumElemsArg);\n\n  /// Add the allocsize attribute to the attribute set at the given arg index.\n  /// Returns a new list because attribute lists are immutable.\n  LLVM_NODISCARD AttributeList\n  addAllocSizeParamAttr(LLVMContext &C, unsigned ArgNo, unsigned ElemSizeArg,\n                        const Optional<unsigned> &NumElemsArg) {\n    return addAllocSizeAttr(C, ArgNo + FirstArgIndex, ElemSizeArg, NumElemsArg);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // AttributeList Accessors\n  //===--------------------------------------------------------------------===//\n\n  /// The attributes for the specified index are returned.\n  AttributeSet getAttributes(unsigned Index) const;\n\n  /// The attributes for the argument or parameter at the given index are\n  /// returned.\n  AttributeSet getParamAttributes(unsigned ArgNo) const;\n\n  /// The attributes for the ret value are returned.\n  AttributeSet getRetAttributes() const;\n\n  /// The function attributes are returned.\n  AttributeSet getFnAttributes() const;\n\n  /// Return true if the attribute exists at the given index.\n  bool hasAttribute(unsigned Index, Attribute::AttrKind Kind) const;\n\n  /// Return true if the attribute exists at the given index.\n  bool hasAttribute(unsigned Index, StringRef Kind) const;\n\n  /// Return true if attribute exists at the given index.\n  bool hasAttributes(unsigned Index) const;\n\n  /// Return true if the attribute exists for the given argument\n  bool hasParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) const {\n    return hasAttribute(ArgNo + FirstArgIndex, Kind);\n  }\n\n  /// Return true if the attribute exists for the given argument\n  bool hasParamAttr(unsigned ArgNo, StringRef Kind) const {\n    return hasAttribute(ArgNo + FirstArgIndex, Kind);\n  }\n\n  /// Return true if attributes exists for the given argument\n  bool hasParamAttrs(unsigned ArgNo) const {\n    return hasAttributes(ArgNo + FirstArgIndex);\n  }\n\n  /// Equivalent to hasAttribute(AttributeList::FunctionIndex, Kind) but\n  /// may be faster.\n  bool hasFnAttribute(Attribute::AttrKind Kind) const;\n\n  /// Equivalent to hasAttribute(AttributeList::FunctionIndex, Kind) but\n  /// may be faster.\n  bool hasFnAttribute(StringRef Kind) const;\n\n  /// Equivalent to hasAttribute(ArgNo + FirstArgIndex, Kind).\n  bool hasParamAttribute(unsigned ArgNo, Attribute::AttrKind Kind) const;\n\n  /// Return true if the specified attribute is set for at least one\n  /// parameter or for the return value. If Index is not nullptr, the index\n  /// of a parameter with the specified attribute is provided.\n  bool hasAttrSomewhere(Attribute::AttrKind Kind,\n                        unsigned *Index = nullptr) const;\n\n  /// Return the attribute object that exists at the given index.\n  Attribute getAttribute(unsigned Index, Attribute::AttrKind Kind) const;\n\n  /// Return the attribute object that exists at the given index.\n  Attribute getAttribute(unsigned Index, StringRef Kind) const;\n\n  /// Return the attribute object that exists at the arg index.\n  Attribute getParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) const {\n    return getAttribute(ArgNo + FirstArgIndex, Kind);\n  }\n\n  /// Return the attribute object that exists at the given index.\n  Attribute getParamAttr(unsigned ArgNo, StringRef Kind) const {\n    return getAttribute(ArgNo + FirstArgIndex, Kind);\n  }\n\n  /// Return the alignment of the return value.\n  MaybeAlign getRetAlignment() const;\n\n  /// Return the alignment for the specified function parameter.\n  MaybeAlign getParamAlignment(unsigned ArgNo) const;\n\n  /// Return the byval type for the specified function parameter.\n  Type *getParamByValType(unsigned ArgNo) const;\n\n  /// Return the sret type for the specified function parameter.\n  Type *getParamStructRetType(unsigned ArgNo) const;\n\n  /// Return the byref type for the specified function parameter.\n  Type *getParamByRefType(unsigned ArgNo) const;\n\n  /// Return the preallocated type for the specified function parameter.\n  Type *getParamPreallocatedType(unsigned ArgNo) const;\n\n  /// Get the stack alignment.\n  MaybeAlign getStackAlignment(unsigned Index) const;\n\n  /// Get the number of dereferenceable bytes (or zero if unknown).\n  uint64_t getDereferenceableBytes(unsigned Index) const;\n\n  /// Get the number of dereferenceable bytes (or zero if unknown) of an\n  /// arg.\n  uint64_t getParamDereferenceableBytes(unsigned ArgNo) const {\n    return getDereferenceableBytes(ArgNo + FirstArgIndex);\n  }\n\n  /// Get the number of dereferenceable_or_null bytes (or zero if\n  /// unknown).\n  uint64_t getDereferenceableOrNullBytes(unsigned Index) const;\n\n  /// Get the number of dereferenceable_or_null bytes (or zero if\n  /// unknown) of an arg.\n  uint64_t getParamDereferenceableOrNullBytes(unsigned ArgNo) const {\n    return getDereferenceableOrNullBytes(ArgNo + FirstArgIndex);\n  }\n\n  /// Get the allocsize argument numbers (or pair(0, 0) if unknown).\n  std::pair<unsigned, Optional<unsigned>>\n  getAllocSizeArgs(unsigned Index) const;\n\n  /// Return the attributes at the index as a string.\n  std::string getAsString(unsigned Index, bool InAttrGrp = false) const;\n\n  //===--------------------------------------------------------------------===//\n  // AttributeList Introspection\n  //===--------------------------------------------------------------------===//\n\n  using iterator = const AttributeSet *;\n\n  iterator begin() const;\n  iterator end() const;\n\n  unsigned getNumAttrSets() const;\n\n  /// Use these to iterate over the valid attribute indices.\n  unsigned index_begin() const { return AttributeList::FunctionIndex; }\n  unsigned index_end() const { return getNumAttrSets() - 1; }\n\n  /// operator==/!= - Provide equality predicates.\n  bool operator==(const AttributeList &RHS) const { return pImpl == RHS.pImpl; }\n  bool operator!=(const AttributeList &RHS) const { return pImpl != RHS.pImpl; }\n\n  /// Return a raw pointer that uniquely identifies this attribute list.\n  void *getRawPointer() const {\n    return pImpl;\n  }\n\n  /// Return true if there are no attributes.\n  bool isEmpty() const { return pImpl == nullptr; }\n\n  void dump() const;\n};\n\n//===----------------------------------------------------------------------===//\n/// \\class\n/// Provide DenseMapInfo for AttributeList.\ntemplate <> struct DenseMapInfo<AttributeList> {\n  static AttributeList getEmptyKey() {\n    auto Val = static_cast<uintptr_t>(-1);\n    Val <<= PointerLikeTypeTraits<void*>::NumLowBitsAvailable;\n    return AttributeList(reinterpret_cast<AttributeListImpl *>(Val));\n  }\n\n  static AttributeList getTombstoneKey() {\n    auto Val = static_cast<uintptr_t>(-2);\n    Val <<= PointerLikeTypeTraits<void*>::NumLowBitsAvailable;\n    return AttributeList(reinterpret_cast<AttributeListImpl *>(Val));\n  }\n\n  static unsigned getHashValue(AttributeList AS) {\n    return (unsigned((uintptr_t)AS.pImpl) >> 4) ^\n           (unsigned((uintptr_t)AS.pImpl) >> 9);\n  }\n\n  static bool isEqual(AttributeList LHS, AttributeList RHS) {\n    return LHS == RHS;\n  }\n};\n\n//===----------------------------------------------------------------------===//\n/// \\class\n/// This class is used in conjunction with the Attribute::get method to\n/// create an Attribute object. The object itself is uniquified. The Builder's\n/// value, however, is not. So this can be used as a quick way to test for\n/// equality, presence of attributes, etc.\nclass AttrBuilder {\n  std::bitset<Attribute::EndAttrKinds> Attrs;\n  std::map<std::string, std::string, std::less<>> TargetDepAttrs;\n  MaybeAlign Alignment;\n  MaybeAlign StackAlignment;\n  uint64_t DerefBytes = 0;\n  uint64_t DerefOrNullBytes = 0;\n  uint64_t AllocSizeArgs = 0;\n  Type *ByValType = nullptr;\n  Type *StructRetType = nullptr;\n  Type *ByRefType = nullptr;\n  Type *PreallocatedType = nullptr;\n\npublic:\n  AttrBuilder() = default;\n\n  AttrBuilder(const Attribute &A) {\n    addAttribute(A);\n  }\n\n  AttrBuilder(AttributeList AS, unsigned Idx);\n  AttrBuilder(AttributeSet AS);\n\n  void clear();\n\n  /// Add an attribute to the builder.\n  AttrBuilder &addAttribute(Attribute::AttrKind Val) {\n    assert((unsigned)Val < Attribute::EndAttrKinds &&\n           \"Attribute out of range!\");\n    assert(!Attribute::doesAttrKindHaveArgument(Val) &&\n           \"Adding integer attribute without adding a value!\");\n    Attrs[Val] = true;\n    return *this;\n  }\n\n  /// Add the Attribute object to the builder.\n  AttrBuilder &addAttribute(Attribute A);\n\n  /// Add the target-dependent attribute to the builder.\n  AttrBuilder &addAttribute(StringRef A, StringRef V = StringRef());\n\n  /// Remove an attribute from the builder.\n  AttrBuilder &removeAttribute(Attribute::AttrKind Val);\n\n  /// Remove the attributes from the builder.\n  AttrBuilder &removeAttributes(AttributeList A, uint64_t WithoutIndex);\n\n  /// Remove the target-dependent attribute to the builder.\n  AttrBuilder &removeAttribute(StringRef A);\n\n  /// Add the attributes from the builder.\n  AttrBuilder &merge(const AttrBuilder &B);\n\n  /// Remove the attributes from the builder.\n  AttrBuilder &remove(const AttrBuilder &B);\n\n  /// Return true if the builder has any attribute that's in the\n  /// specified builder.\n  bool overlaps(const AttrBuilder &B) const;\n\n  /// Return true if the builder has the specified attribute.\n  bool contains(Attribute::AttrKind A) const {\n    assert((unsigned)A < Attribute::EndAttrKinds && \"Attribute out of range!\");\n    return Attrs[A];\n  }\n\n  /// Return true if the builder has the specified target-dependent\n  /// attribute.\n  bool contains(StringRef A) const;\n\n  /// Return true if the builder has IR-level attributes.\n  bool hasAttributes() const;\n\n  /// Return true if the builder has any attribute that's in the\n  /// specified attribute.\n  bool hasAttributes(AttributeList A, uint64_t Index) const;\n\n  /// Return true if the builder has an alignment attribute.\n  bool hasAlignmentAttr() const;\n\n  /// Retrieve the alignment attribute, if it exists.\n  MaybeAlign getAlignment() const { return Alignment; }\n\n  /// Retrieve the stack alignment attribute, if it exists.\n  MaybeAlign getStackAlignment() const { return StackAlignment; }\n\n  /// Retrieve the number of dereferenceable bytes, if the\n  /// dereferenceable attribute exists (zero is returned otherwise).\n  uint64_t getDereferenceableBytes() const { return DerefBytes; }\n\n  /// Retrieve the number of dereferenceable_or_null bytes, if the\n  /// dereferenceable_or_null attribute exists (zero is returned otherwise).\n  uint64_t getDereferenceableOrNullBytes() const { return DerefOrNullBytes; }\n\n  /// Retrieve the byval type.\n  Type *getByValType() const { return ByValType; }\n\n  /// Retrieve the sret type.\n  Type *getStructRetType() const { return StructRetType; }\n\n  /// Retrieve the byref type.\n  Type *getByRefType() const { return ByRefType; }\n\n  /// Retrieve the preallocated type.\n  Type *getPreallocatedType() const { return PreallocatedType; }\n\n  /// Retrieve the allocsize args, if the allocsize attribute exists.  If it\n  /// doesn't exist, pair(0, 0) is returned.\n  std::pair<unsigned, Optional<unsigned>> getAllocSizeArgs() const;\n\n  /// This turns an alignment into the form used internally in Attribute.\n  /// This call has no effect if Align is not set.\n  AttrBuilder &addAlignmentAttr(MaybeAlign Align);\n\n  /// This turns an int alignment (which must be a power of 2) into the\n  /// form used internally in Attribute.\n  /// This call has no effect if Align is 0.\n  /// Deprecated, use the version using a MaybeAlign.\n  inline AttrBuilder &addAlignmentAttr(unsigned Align) {\n    return addAlignmentAttr(MaybeAlign(Align));\n  }\n\n  /// This turns a stack alignment into the form used internally in Attribute.\n  /// This call has no effect if Align is not set.\n  AttrBuilder &addStackAlignmentAttr(MaybeAlign Align);\n\n  /// This turns an int stack alignment (which must be a power of 2) into\n  /// the form used internally in Attribute.\n  /// This call has no effect if Align is 0.\n  /// Deprecated, use the version using a MaybeAlign.\n  inline AttrBuilder &addStackAlignmentAttr(unsigned Align) {\n    return addStackAlignmentAttr(MaybeAlign(Align));\n  }\n\n  /// This turns the number of dereferenceable bytes into the form used\n  /// internally in Attribute.\n  AttrBuilder &addDereferenceableAttr(uint64_t Bytes);\n\n  /// This turns the number of dereferenceable_or_null bytes into the\n  /// form used internally in Attribute.\n  AttrBuilder &addDereferenceableOrNullAttr(uint64_t Bytes);\n\n  /// This turns one (or two) ints into the form used internally in Attribute.\n  AttrBuilder &addAllocSizeAttr(unsigned ElemSizeArg,\n                                const Optional<unsigned> &NumElemsArg);\n\n  /// This turns a byval type into the form used internally in Attribute.\n  AttrBuilder &addByValAttr(Type *Ty);\n\n  /// This turns a sret type into the form used internally in Attribute.\n  AttrBuilder &addStructRetAttr(Type *Ty);\n\n  /// This turns a byref type into the form used internally in Attribute.\n  AttrBuilder &addByRefAttr(Type *Ty);\n\n  /// This turns a preallocated type into the form used internally in Attribute.\n  AttrBuilder &addPreallocatedAttr(Type *Ty);\n\n  /// Add an allocsize attribute, using the representation returned by\n  /// Attribute.getIntValue().\n  AttrBuilder &addAllocSizeAttrFromRawRepr(uint64_t RawAllocSizeRepr);\n\n  /// Return true if the builder contains no target-independent\n  /// attributes.\n  bool empty() const { return Attrs.none(); }\n\n  // Iterators for target-dependent attributes.\n  using td_type = std::pair<std::string, std::string>;\n  using td_iterator = decltype(TargetDepAttrs)::iterator;\n  using td_const_iterator = decltype(TargetDepAttrs)::const_iterator;\n  using td_range = iterator_range<td_iterator>;\n  using td_const_range = iterator_range<td_const_iterator>;\n\n  td_iterator td_begin() { return TargetDepAttrs.begin(); }\n  td_iterator td_end() { return TargetDepAttrs.end(); }\n\n  td_const_iterator td_begin() const { return TargetDepAttrs.begin(); }\n  td_const_iterator td_end() const { return TargetDepAttrs.end(); }\n\n  td_range td_attrs() { return td_range(td_begin(), td_end()); }\n\n  td_const_range td_attrs() const {\n    return td_const_range(td_begin(), td_end());\n  }\n\n  bool td_empty() const { return TargetDepAttrs.empty(); }\n\n  bool operator==(const AttrBuilder &B) const;\n  bool operator!=(const AttrBuilder &B) const { return !(*this == B); }\n};\n\nnamespace AttributeFuncs {\n\n/// Which attributes cannot be applied to a type.\nAttrBuilder typeIncompatible(Type *Ty);\n\n/// \\returns Return true if the two functions have compatible target-independent\n/// attributes for inlining purposes.\nbool areInlineCompatible(const Function &Caller, const Function &Callee);\n\n\n/// Checks  if there are any incompatible function attributes between\n/// \\p A and \\p B.\n///\n/// \\param [in] A - The first function to be compared with.\n/// \\param [in] B - The second function to be compared with.\n/// \\returns true if the functions have compatible attributes.\nbool areOutlineCompatible(const Function &A, const Function &B);\n\n/// Merge caller's and callee's attributes.\nvoid mergeAttributesForInlining(Function &Caller, const Function &Callee);\n\n/// Merges the functions attributes from \\p ToMerge into function \\p Base.\n///\n/// \\param [in,out] Base - The function being merged into.\n/// \\param [in] ToMerge - The function to merge attributes from.\nvoid mergeAttributesForOutlining(Function &Base, const Function &ToMerge);\n\n} // end namespace AttributeFuncs\n\n} // end namespace llvm\n\n#endif // LLVM_IR_ATTRIBUTES_H\n"}, "69": {"id": 69, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/BasicBlock.h", "content": "//===- llvm/BasicBlock.h - Represent a basic block in the VM ----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains the declaration of the BasicBlock class.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_BASICBLOCK_H\n#define LLVM_IR_BASICBLOCK_H\n\n#include \"llvm-c/Types.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/ilist.h\"\n#include \"llvm/ADT/ilist_node.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/SymbolTableListTraits.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/CBindingWrapping.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/Compiler.h\"\n#include <cassert>\n#include <cstddef>\n#include <iterator>\n\nnamespace llvm {\n\nclass AssemblyAnnotationWriter;\nclass CallInst;\nclass Function;\nclass LandingPadInst;\nclass LLVMContext;\nclass Module;\nclass PHINode;\nclass ValueSymbolTable;\n\n/// LLVM Basic Block Representation\n///\n/// This represents a single basic block in LLVM. A basic block is simply a\n/// container of instructions that execute sequentially. Basic blocks are Values\n/// because they are referenced by instructions such as branches and switch\n/// tables. The type of a BasicBlock is \"Type::LabelTy\" because the basic block\n/// represents a label to which a branch can jump.\n///\n/// A well formed basic block is formed of a list of non-terminating\n/// instructions followed by a single terminator instruction. Terminator\n/// instructions may not occur in the middle of basic blocks, and must terminate\n/// the blocks. The BasicBlock class allows malformed basic blocks to occur\n/// because it may be useful in the intermediate stage of constructing or\n/// modifying a program. However, the verifier will ensure that basic blocks are\n/// \"well formed\".\nclass BasicBlock final : public Value, // Basic blocks are data objects also\n                         public ilist_node_with_parent<BasicBlock, Function> {\npublic:\n  using InstListType = SymbolTableList<Instruction>;\n\nprivate:\n  friend class BlockAddress;\n  friend class SymbolTableListTraits<BasicBlock>;\n\n  InstListType InstList;\n  Function *Parent;\n\n  void setParent(Function *parent);\n\n  /// Constructor.\n  ///\n  /// If the function parameter is specified, the basic block is automatically\n  /// inserted at either the end of the function (if InsertBefore is null), or\n  /// before the specified basic block.\n  explicit BasicBlock(LLVMContext &C, const Twine &Name = \"\",\n                      Function *Parent = nullptr,\n                      BasicBlock *InsertBefore = nullptr);\n\npublic:\n  BasicBlock(const BasicBlock &) = delete;\n  BasicBlock &operator=(const BasicBlock &) = delete;\n  ~BasicBlock();\n\n  /// Get the context in which this basic block lives.\n  LLVMContext &getContext() const;\n\n  /// Instruction iterators...\n  using iterator = InstListType::iterator;\n  using const_iterator = InstListType::const_iterator;\n  using reverse_iterator = InstListType::reverse_iterator;\n  using const_reverse_iterator = InstListType::const_reverse_iterator;\n\n  /// Creates a new BasicBlock.\n  ///\n  /// If the Parent parameter is specified, the basic block is automatically\n  /// inserted at either the end of the function (if InsertBefore is 0), or\n  /// before the specified basic block.\n  static BasicBlock *Create(LLVMContext &Context, const Twine &Name = \"\",\n                            Function *Parent = nullptr,\n                            BasicBlock *InsertBefore = nullptr) {\n    return new BasicBlock(Context, Name, Parent, InsertBefore);\n  }\n\n  /// Return the enclosing method, or null if none.\n  const Function *getParent() const { return Parent; }\n        Function *getParent()       { return Parent; }\n\n  /// Return the module owning the function this basic block belongs to, or\n  /// nullptr if the function does not have a module.\n  ///\n  /// Note: this is undefined behavior if the block does not have a parent.\n  const Module *getModule() const;\n  Module *getModule() {\n    return const_cast<Module *>(\n                            static_cast<const BasicBlock *>(this)->getModule());\n  }\n\n  /// Returns the terminator instruction if the block is well formed or null\n  /// if the block is not well formed.\n  const Instruction *getTerminator() const LLVM_READONLY;\n  Instruction *getTerminator() {\n    return const_cast<Instruction *>(\n        static_cast<const BasicBlock *>(this)->getTerminator());\n  }\n\n  /// Returns the call instruction calling \\@llvm.experimental.deoptimize\n  /// prior to the terminating return instruction of this basic block, if such\n  /// a call is present.  Otherwise, returns null.\n  const CallInst *getTerminatingDeoptimizeCall() const;\n  CallInst *getTerminatingDeoptimizeCall() {\n    return const_cast<CallInst *>(\n         static_cast<const BasicBlock *>(this)->getTerminatingDeoptimizeCall());\n  }\n\n  /// Returns the call instruction calling \\@llvm.experimental.deoptimize\n  /// that is present either in current basic block or in block that is a unique\n  /// successor to current block, if such call is present. Otherwise, returns null.\n  const CallInst *getPostdominatingDeoptimizeCall() const;\n  CallInst *getPostdominatingDeoptimizeCall() {\n    return const_cast<CallInst *>(\n         static_cast<const BasicBlock *>(this)->getPostdominatingDeoptimizeCall());\n  }\n\n  /// Returns the call instruction marked 'musttail' prior to the terminating\n  /// return instruction of this basic block, if such a call is present.\n  /// Otherwise, returns null.\n  const CallInst *getTerminatingMustTailCall() const;\n  CallInst *getTerminatingMustTailCall() {\n    return const_cast<CallInst *>(\n           static_cast<const BasicBlock *>(this)->getTerminatingMustTailCall());\n  }\n\n  /// Returns a pointer to the first instruction in this block that is not a\n  /// PHINode instruction.\n  ///\n  /// When adding instructions to the beginning of the basic block, they should\n  /// be added before the returned value, not before the first instruction,\n  /// which might be PHI. Returns 0 is there's no non-PHI instruction.\n  const Instruction* getFirstNonPHI() const;\n  Instruction* getFirstNonPHI() {\n    return const_cast<Instruction *>(\n                       static_cast<const BasicBlock *>(this)->getFirstNonPHI());\n  }\n\n  /// Returns a pointer to the first instruction in this block that is not a\n  /// PHINode or a debug intrinsic, or any pseudo operation if \\c SkipPseudoOp\n  /// is true.\n  const Instruction *getFirstNonPHIOrDbg(bool SkipPseudoOp = false) const;\n  Instruction *getFirstNonPHIOrDbg(bool SkipPseudoOp = false) {\n    return const_cast<Instruction *>(\n        static_cast<const BasicBlock *>(this)->getFirstNonPHIOrDbg(\n            SkipPseudoOp));\n  }\n\n  /// Returns a pointer to the first instruction in this block that is not a\n  /// PHINode, a debug intrinsic, or a lifetime intrinsic, or any pseudo\n  /// operation if \\c SkipPseudoOp is true.\n  const Instruction *\n  getFirstNonPHIOrDbgOrLifetime(bool SkipPseudoOp = false) const;\n  Instruction *getFirstNonPHIOrDbgOrLifetime(bool SkipPseudoOp = false) {\n    return const_cast<Instruction *>(\n        static_cast<const BasicBlock *>(this)->getFirstNonPHIOrDbgOrLifetime(\n            SkipPseudoOp));\n  }\n\n  /// Returns an iterator to the first instruction in this block that is\n  /// suitable for inserting a non-PHI instruction.\n  ///\n  /// In particular, it skips all PHIs and LandingPad instructions.\n  const_iterator getFirstInsertionPt() const;\n  iterator getFirstInsertionPt() {\n    return static_cast<const BasicBlock *>(this)\n                                          ->getFirstInsertionPt().getNonConst();\n  }\n\n  /// Return a const iterator range over the instructions in the block, skipping\n  /// any debug instructions. Skip any pseudo operations as well if \\c\n  /// SkipPseudoOp is true.\n  iterator_range<filter_iterator<BasicBlock::const_iterator,\n                                 std::function<bool(const Instruction &)>>>\n  instructionsWithoutDebug(bool SkipPseudoOp = false) const;\n\n  /// Return an iterator range over the instructions in the block, skipping any\n  /// debug instructions. Skip and any pseudo operations as well if \\c\n  /// SkipPseudoOp is true.\n  iterator_range<\n      filter_iterator<BasicBlock::iterator, std::function<bool(Instruction &)>>>\n  instructionsWithoutDebug(bool SkipPseudoOp = false);\n\n  /// Return the size of the basic block ignoring debug instructions\n  filter_iterator<BasicBlock::const_iterator,\n                  std::function<bool(const Instruction &)>>::difference_type\n  sizeWithoutDebug() const;\n\n  /// Unlink 'this' from the containing function, but do not delete it.\n  void removeFromParent();\n\n  /// Unlink 'this' from the containing function and delete it.\n  ///\n  // \\returns an iterator pointing to the element after the erased one.\n  SymbolTableList<BasicBlock>::iterator eraseFromParent();\n\n  /// Unlink this basic block from its current function and insert it into\n  /// the function that \\p MovePos lives in, right before \\p MovePos.\n  void moveBefore(BasicBlock *MovePos);\n\n  /// Unlink this basic block from its current function and insert it\n  /// right after \\p MovePos in the function \\p MovePos lives in.\n  void moveAfter(BasicBlock *MovePos);\n\n  /// Insert unlinked basic block into a function.\n  ///\n  /// Inserts an unlinked basic block into \\c Parent.  If \\c InsertBefore is\n  /// provided, inserts before that basic block, otherwise inserts at the end.\n  ///\n  /// \\pre \\a getParent() is \\c nullptr.\n  void insertInto(Function *Parent, BasicBlock *InsertBefore = nullptr);\n\n  /// Return the predecessor of this block if it has a single predecessor\n  /// block. Otherwise return a null pointer.\n  const BasicBlock *getSinglePredecessor() const;\n  BasicBlock *getSinglePredecessor() {\n    return const_cast<BasicBlock *>(\n                 static_cast<const BasicBlock *>(this)->getSinglePredecessor());\n  }\n\n  /// Return the predecessor of this block if it has a unique predecessor\n  /// block. Otherwise return a null pointer.\n  ///\n  /// Note that unique predecessor doesn't mean single edge, there can be\n  /// multiple edges from the unique predecessor to this block (for example a\n  /// switch statement with multiple cases having the same destination).\n  const BasicBlock *getUniquePredecessor() const;\n  BasicBlock *getUniquePredecessor() {\n    return const_cast<BasicBlock *>(\n                 static_cast<const BasicBlock *>(this)->getUniquePredecessor());\n  }\n\n  /// Return true if this block has exactly N predecessors.\n  bool hasNPredecessors(unsigned N) const;\n\n  /// Return true if this block has N predecessors or more.\n  bool hasNPredecessorsOrMore(unsigned N) const;\n\n  /// Return the successor of this block if it has a single successor.\n  /// Otherwise return a null pointer.\n  ///\n  /// This method is analogous to getSinglePredecessor above.\n  const BasicBlock *getSingleSuccessor() const;\n  BasicBlock *getSingleSuccessor() {\n    return const_cast<BasicBlock *>(\n                   static_cast<const BasicBlock *>(this)->getSingleSuccessor());\n  }\n\n  /// Return the successor of this block if it has a unique successor.\n  /// Otherwise return a null pointer.\n  ///\n  /// This method is analogous to getUniquePredecessor above.\n  const BasicBlock *getUniqueSuccessor() const;\n  BasicBlock *getUniqueSuccessor() {\n    return const_cast<BasicBlock *>(\n                   static_cast<const BasicBlock *>(this)->getUniqueSuccessor());\n  }\n\n  /// Print the basic block to an output stream with an optional\n  /// AssemblyAnnotationWriter.\n  void print(raw_ostream &OS, AssemblyAnnotationWriter *AAW = nullptr,\n             bool ShouldPreserveUseListOrder = false,\n             bool IsForDebug = false) const;\n\n  //===--------------------------------------------------------------------===//\n  /// Instruction iterator methods\n  ///\n  inline iterator                begin()       { return InstList.begin(); }\n  inline const_iterator          begin() const { return InstList.begin(); }\n  inline iterator                end  ()       { return InstList.end();   }\n  inline const_iterator          end  () const { return InstList.end();   }\n\n  inline reverse_iterator        rbegin()       { return InstList.rbegin(); }\n  inline const_reverse_iterator  rbegin() const { return InstList.rbegin(); }\n  inline reverse_iterator        rend  ()       { return InstList.rend();   }\n  inline const_reverse_iterator  rend  () const { return InstList.rend();   }\n\n  inline size_t                   size() const { return InstList.size();  }\n  inline bool                    empty() const { return InstList.empty(); }\n  inline const Instruction      &front() const { return InstList.front(); }\n  inline       Instruction      &front()       { return InstList.front(); }\n  inline const Instruction       &back() const { return InstList.back();  }\n  inline       Instruction       &back()       { return InstList.back();  }\n\n  /// Iterator to walk just the phi nodes in the basic block.\n  template <typename PHINodeT = PHINode, typename BBIteratorT = iterator>\n  class phi_iterator_impl\n      : public iterator_facade_base<phi_iterator_impl<PHINodeT, BBIteratorT>,\n                                    std::forward_iterator_tag, PHINodeT> {\n    friend BasicBlock;\n\n    PHINodeT *PN;\n\n    phi_iterator_impl(PHINodeT *PN) : PN(PN) {}\n\n  public:\n    // Allow default construction to build variables, but this doesn't build\n    // a useful iterator.\n    phi_iterator_impl() = default;\n\n    // Allow conversion between instantiations where valid.\n    template <typename PHINodeU, typename BBIteratorU,\n              typename = std::enable_if_t<\n                  std::is_convertible<PHINodeU *, PHINodeT *>::value>>\n    phi_iterator_impl(const phi_iterator_impl<PHINodeU, BBIteratorU> &Arg)\n        : PN(Arg.PN) {}\n\n    bool operator==(const phi_iterator_impl &Arg) const { return PN == Arg.PN; }\n\n    PHINodeT &operator*() const { return *PN; }\n\n    using phi_iterator_impl::iterator_facade_base::operator++;\n    phi_iterator_impl &operator++() {\n      assert(PN && \"Cannot increment the end iterator!\");\n      PN = dyn_cast<PHINodeT>(std::next(BBIteratorT(PN)));\n      return *this;\n    }\n  };\n  using phi_iterator = phi_iterator_impl<>;\n  using const_phi_iterator =\n      phi_iterator_impl<const PHINode, BasicBlock::const_iterator>;\n\n  /// Returns a range that iterates over the phis in the basic block.\n  ///\n  /// Note that this cannot be used with basic blocks that have no terminator.\n  iterator_range<const_phi_iterator> phis() const {\n    return const_cast<BasicBlock *>(this)->phis();\n  }\n  iterator_range<phi_iterator> phis();\n\n  /// Return the underlying instruction list container.\n  ///\n  /// Currently you need to access the underlying instruction list container\n  /// directly if you want to modify it.\n  const InstListType &getInstList() const { return InstList; }\n        InstListType &getInstList()       { return InstList; }\n\n  /// Returns a pointer to a member of the instruction list.\n  static InstListType BasicBlock::*getSublistAccess(Instruction*) {\n    return &BasicBlock::InstList;\n  }\n\n  /// Returns a pointer to the symbol table if one exists.\n  ValueSymbolTable *getValueSymbolTable();\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast.\n  static bool classof(const Value *V) {\n    return V->getValueID() == Value::BasicBlockVal;\n  }\n\n  /// Cause all subinstructions to \"let go\" of all the references that said\n  /// subinstructions are maintaining.\n  ///\n  /// This allows one to 'delete' a whole class at a time, even though there may\n  /// be circular references... first all references are dropped, and all use\n  /// counts go to zero.  Then everything is delete'd for real.  Note that no\n  /// operations are valid on an object that has \"dropped all references\",\n  /// except operator delete.\n  void dropAllReferences();\n\n  /// Update PHI nodes in this BasicBlock before removal of predecessor \\p Pred.\n  /// Note that this function does not actually remove the predecessor.\n  ///\n  /// If \\p KeepOneInputPHIs is true then don't remove PHIs that are left with\n  /// zero or one incoming values, and don't simplify PHIs with all incoming\n  /// values the same.\n  void removePredecessor(BasicBlock *Pred, bool KeepOneInputPHIs = false);\n\n  bool canSplitPredecessors() const;\n\n  /// Split the basic block into two basic blocks at the specified instruction.\n  ///\n  /// If \\p Before is true, splitBasicBlockBefore handles the\n  /// block splitting. Otherwise, execution proceeds as described below.\n  ///\n  /// Note that all instructions BEFORE the specified iterator\n  /// stay as part of the original basic block, an unconditional branch is added\n  /// to the original BB, and the rest of the instructions in the BB are moved\n  /// to the new BB, including the old terminator.  The newly formed basic block\n  /// is returned. This function invalidates the specified iterator.\n  ///\n  /// Note that this only works on well formed basic blocks (must have a\n  /// terminator), and \\p 'I' must not be the end of instruction list (which\n  /// would cause a degenerate basic block to be formed, having a terminator\n  /// inside of the basic block).\n  ///\n  /// Also note that this doesn't preserve any passes. To split blocks while\n  /// keeping loop information consistent, use the SplitBlock utility function.\n  BasicBlock *splitBasicBlock(iterator I, const Twine &BBName = \"\",\n                              bool Before = false);\n  BasicBlock *splitBasicBlock(Instruction *I, const Twine &BBName = \"\",\n                              bool Before = false) {\n    return splitBasicBlock(I->getIterator(), BBName, Before);\n  }\n\n  /// Split the basic block into two basic blocks at the specified instruction\n  /// and insert the new basic blocks as the predecessor of the current block.\n  ///\n  /// This function ensures all instructions AFTER and including the specified\n  /// iterator \\p I are part of the original basic block. All Instructions\n  /// BEFORE the iterator \\p I are moved to the new BB and an unconditional\n  /// branch is added to the new BB. The new basic block is returned.\n  ///\n  /// Note that this only works on well formed basic blocks (must have a\n  /// terminator), and \\p 'I' must not be the end of instruction list (which\n  /// would cause a degenerate basic block to be formed, having a terminator\n  /// inside of the basic block).  \\p 'I' cannot be a iterator for a PHINode\n  /// with multiple incoming blocks.\n  ///\n  /// Also note that this doesn't preserve any passes. To split blocks while\n  /// keeping loop information consistent, use the SplitBlockBefore utility\n  /// function.\n  BasicBlock *splitBasicBlockBefore(iterator I, const Twine &BBName = \"\");\n  BasicBlock *splitBasicBlockBefore(Instruction *I, const Twine &BBName = \"\") {\n    return splitBasicBlockBefore(I->getIterator(), BBName);\n  }\n\n  /// Returns true if there are any uses of this basic block other than\n  /// direct branches, switches, etc. to it.\n  bool hasAddressTaken() const {\n    return getBasicBlockBits().BlockAddressRefCount != 0;\n  }\n\n  /// Update all phi nodes in this basic block to refer to basic block \\p New\n  /// instead of basic block \\p Old.\n  void replacePhiUsesWith(BasicBlock *Old, BasicBlock *New);\n\n  /// Update all phi nodes in this basic block's successors to refer to basic\n  /// block \\p New instead of basic block \\p Old.\n  void replaceSuccessorsPhiUsesWith(BasicBlock *Old, BasicBlock *New);\n\n  /// Update all phi nodes in this basic block's successors to refer to basic\n  /// block \\p New instead of to it.\n  void replaceSuccessorsPhiUsesWith(BasicBlock *New);\n\n  /// Return true if this basic block is an exception handling block.\n  bool isEHPad() const { return getFirstNonPHI()->isEHPad(); }\n\n  /// Return true if this basic block is a landing pad.\n  ///\n  /// Being a ``landing pad'' means that the basic block is the destination of\n  /// the 'unwind' edge of an invoke instruction.\n  bool isLandingPad() const;\n\n  /// Return the landingpad instruction associated with the landing pad.\n  const LandingPadInst *getLandingPadInst() const;\n  LandingPadInst *getLandingPadInst() {\n    return const_cast<LandingPadInst *>(\n                    static_cast<const BasicBlock *>(this)->getLandingPadInst());\n  }\n\n  /// Return true if it is legal to hoist instructions into this block.\n  bool isLegalToHoistInto() const;\n\n  Optional<uint64_t> getIrrLoopHeaderWeight() const;\n\n  /// Returns true if the Order field of child Instructions is valid.\n  bool isInstrOrderValid() const {\n    return getBasicBlockBits().InstrOrderValid;\n  }\n\n  /// Mark instruction ordering invalid. Done on every instruction insert.\n  void invalidateOrders() {\n    validateInstrOrdering();\n    BasicBlockBits Bits = getBasicBlockBits();\n    Bits.InstrOrderValid = false;\n    setBasicBlockBits(Bits);\n  }\n\n  /// Renumber instructions and mark the ordering as valid.\n  void renumberInstructions();\n\n  /// Asserts that instruction order numbers are marked invalid, or that they\n  /// are in ascending order. This is constant time if the ordering is invalid,\n  /// and linear in the number of instructions if the ordering is valid. Callers\n  /// should be careful not to call this in ways that make common operations\n  /// O(n^2). For example, it takes O(n) time to assign order numbers to\n  /// instructions, so the order should be validated no more than once after\n  /// each ordering to ensure that transforms have the same algorithmic\n  /// complexity when asserts are enabled as when they are disabled.\n  void validateInstrOrdering() const;\n\nprivate:\n#if defined(_AIX) && (!defined(__GNUC__) || defined(__ibmxl__))\n// Except for GCC; by default, AIX compilers store bit-fields in 4-byte words\n// and give the `pack` pragma push semantics.\n#define BEGIN_TWO_BYTE_PACK() _Pragma(\"pack(2)\")\n#define END_TWO_BYTE_PACK() _Pragma(\"pack(pop)\")\n#else\n#define BEGIN_TWO_BYTE_PACK()\n#define END_TWO_BYTE_PACK()\n#endif\n\n  BEGIN_TWO_BYTE_PACK()\n  /// Bitfield to help interpret the bits in Value::SubclassData.\n  struct BasicBlockBits {\n    unsigned short BlockAddressRefCount : 15;\n    unsigned short InstrOrderValid : 1;\n  };\n  END_TWO_BYTE_PACK()\n\n#undef BEGIN_TWO_BYTE_PACK\n#undef END_TWO_BYTE_PACK\n\n  /// Safely reinterpret the subclass data bits to a more useful form.\n  BasicBlockBits getBasicBlockBits() const {\n    static_assert(sizeof(BasicBlockBits) == sizeof(unsigned short),\n                  \"too many bits for Value::SubclassData\");\n    unsigned short ValueData = getSubclassDataFromValue();\n    BasicBlockBits AsBits;\n    memcpy(&AsBits, &ValueData, sizeof(AsBits));\n    return AsBits;\n  }\n\n  /// Reinterpret our subclass bits and store them back into Value.\n  void setBasicBlockBits(BasicBlockBits AsBits) {\n    unsigned short D;\n    memcpy(&D, &AsBits, sizeof(D));\n    Value::setValueSubclassData(D);\n  }\n\n  /// Increment the internal refcount of the number of BlockAddresses\n  /// referencing this BasicBlock by \\p Amt.\n  ///\n  /// This is almost always 0, sometimes one possibly, but almost never 2, and\n  /// inconceivably 3 or more.\n  void AdjustBlockAddressRefCount(int Amt) {\n    BasicBlockBits Bits = getBasicBlockBits();\n    Bits.BlockAddressRefCount += Amt;\n    setBasicBlockBits(Bits);\n    assert(Bits.BlockAddressRefCount < 255 && \"Refcount wrap-around\");\n  }\n\n  /// Shadow Value::setValueSubclassData with a private forwarding method so\n  /// that any future subclasses cannot accidentally use it.\n  void setValueSubclassData(unsigned short D) {\n    Value::setValueSubclassData(D);\n  }\n};\n\n// Create wrappers for C Binding types (see CBindingWrapping.h).\nDEFINE_SIMPLE_CONVERSION_FUNCTIONS(BasicBlock, LLVMBasicBlockRef)\n\n/// Advance \\p It while it points to a debug instruction and return the result.\n/// This assumes that \\p It is not at the end of a block.\nBasicBlock::iterator skipDebugIntrinsics(BasicBlock::iterator It);\n\n#ifdef NDEBUG\n/// In release builds, this is a no-op. For !NDEBUG builds, the checks are\n/// implemented in the .cpp file to avoid circular header deps.\ninline void BasicBlock::validateInstrOrdering() const {}\n#endif\n\n} // end namespace llvm\n\n#endif // LLVM_IR_BASICBLOCK_H\n"}, "71": {"id": 71, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DataLayout.h", "content": "//===- llvm/DataLayout.h - Data size & alignment info -----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines layout properties related to datatype size/offset/alignment\n// information.  It uses lazy annotations to cache information about how\n// structure types are laid out and used.\n//\n// This structure should be created once, filled in if the defaults are not\n// correct and then passed around by const&.  None of the members functions\n// require modification to the object.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_DATALAYOUT_H\n#define LLVM_IR_DATALAYOUT_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MathExtras.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/TypeSize.h\"\n#include <cassert>\n#include <cstdint>\n#include <string>\n\n// This needs to be outside of the namespace, to avoid conflict with llvm-c\n// decl.\nusing LLVMTargetDataRef = struct LLVMOpaqueTargetData *;\n\nnamespace llvm {\n\nclass GlobalVariable;\nclass LLVMContext;\nclass Module;\nclass StructLayout;\nclass Triple;\nclass Value;\n\n/// Enum used to categorize the alignment types stored by LayoutAlignElem\nenum AlignTypeEnum {\n  INVALID_ALIGN = 0,\n  INTEGER_ALIGN = 'i',\n  VECTOR_ALIGN = 'v',\n  FLOAT_ALIGN = 'f',\n  AGGREGATE_ALIGN = 'a'\n};\n\n// FIXME: Currently the DataLayout string carries a \"preferred alignment\"\n// for types. As the DataLayout is module/global, this should likely be\n// sunk down to an FTTI element that is queried rather than a global\n// preference.\n\n/// Layout alignment element.\n///\n/// Stores the alignment data associated with a given alignment type (integer,\n/// vector, float) and type bit width.\n///\n/// \\note The unusual order of elements in the structure attempts to reduce\n/// padding and make the structure slightly more cache friendly.\nstruct LayoutAlignElem {\n  /// Alignment type from \\c AlignTypeEnum\n  unsigned AlignType : 8;\n  unsigned TypeBitWidth : 24;\n  Align ABIAlign;\n  Align PrefAlign;\n\n  static LayoutAlignElem get(AlignTypeEnum align_type, Align abi_align,\n                             Align pref_align, uint32_t bit_width);\n\n  bool operator==(const LayoutAlignElem &rhs) const;\n};\n\n/// Layout pointer alignment element.\n///\n/// Stores the alignment data associated with a given pointer and address space.\n///\n/// \\note The unusual order of elements in the structure attempts to reduce\n/// padding and make the structure slightly more cache friendly.\nstruct PointerAlignElem {\n  Align ABIAlign;\n  Align PrefAlign;\n  uint32_t TypeByteWidth;\n  uint32_t AddressSpace;\n  uint32_t IndexWidth;\n\n  /// Initializer\n  static PointerAlignElem get(uint32_t AddressSpace, Align ABIAlign,\n                              Align PrefAlign, uint32_t TypeByteWidth,\n                              uint32_t IndexWidth);\n\n  bool operator==(const PointerAlignElem &rhs) const;\n};\n\n/// A parsed version of the target data layout string in and methods for\n/// querying it.\n///\n/// The target data layout string is specified *by the target* - a frontend\n/// generating LLVM IR is required to generate the right target data for the\n/// target being codegen'd to.\nclass DataLayout {\npublic:\n  enum class FunctionPtrAlignType {\n    /// The function pointer alignment is independent of the function alignment.\n    Independent,\n    /// The function pointer alignment is a multiple of the function alignment.\n    MultipleOfFunctionAlign,\n  };\nprivate:\n  /// Defaults to false.\n  bool BigEndian;\n\n  unsigned AllocaAddrSpace;\n  MaybeAlign StackNaturalAlign;\n  unsigned ProgramAddrSpace;\n  unsigned DefaultGlobalsAddrSpace;\n\n  MaybeAlign FunctionPtrAlign;\n  FunctionPtrAlignType TheFunctionPtrAlignType;\n\n  enum ManglingModeT {\n    MM_None,\n    MM_ELF,\n    MM_MachO,\n    MM_WinCOFF,\n    MM_WinCOFFX86,\n    MM_Mips,\n    MM_XCOFF\n  };\n  ManglingModeT ManglingMode;\n\n  SmallVector<unsigned char, 8> LegalIntWidths;\n\n  /// Primitive type alignment data. This is sorted by type and bit\n  /// width during construction.\n  using AlignmentsTy = SmallVector<LayoutAlignElem, 16>;\n  AlignmentsTy Alignments;\n\n  AlignmentsTy::const_iterator\n  findAlignmentLowerBound(AlignTypeEnum AlignType, uint32_t BitWidth) const {\n    return const_cast<DataLayout *>(this)->findAlignmentLowerBound(AlignType,\n                                                                   BitWidth);\n  }\n\n  AlignmentsTy::iterator\n  findAlignmentLowerBound(AlignTypeEnum AlignType, uint32_t BitWidth);\n\n  /// The string representation used to create this DataLayout\n  std::string StringRepresentation;\n\n  using PointersTy = SmallVector<PointerAlignElem, 8>;\n  PointersTy Pointers;\n\n  const PointerAlignElem &getPointerAlignElem(uint32_t AddressSpace) const;\n\n  // The StructType -> StructLayout map.\n  mutable void *LayoutMap = nullptr;\n\n  /// Pointers in these address spaces are non-integral, and don't have a\n  /// well-defined bitwise representation.\n  SmallVector<unsigned, 8> NonIntegralAddressSpaces;\n\n  /// Attempts to set the alignment of the given type. Returns an error\n  /// description on failure.\n  Error setAlignment(AlignTypeEnum align_type, Align abi_align,\n                     Align pref_align, uint32_t bit_width);\n\n  /// Attempts to set the alignment of a pointer in the given address space.\n  /// Returns an error description on failure.\n  Error setPointerAlignment(uint32_t AddrSpace, Align ABIAlign, Align PrefAlign,\n                            uint32_t TypeByteWidth, uint32_t IndexWidth);\n\n  /// Internal helper to get alignment for integer of given bitwidth.\n  Align getIntegerAlignment(uint32_t BitWidth, bool abi_or_pref) const;\n\n  /// Internal helper method that returns requested alignment for type.\n  Align getAlignment(Type *Ty, bool abi_or_pref) const;\n\n  /// Attempts to parse a target data specification string and reports an error\n  /// if the string is malformed.\n  Error parseSpecifier(StringRef Desc);\n\n  // Free all internal data structures.\n  void clear();\n\npublic:\n  /// Constructs a DataLayout from a specification string. See reset().\n  explicit DataLayout(StringRef LayoutDescription) {\n    reset(LayoutDescription);\n  }\n\n  /// Initialize target data from properties stored in the module.\n  explicit DataLayout(const Module *M);\n\n  DataLayout(const DataLayout &DL) { *this = DL; }\n\n  ~DataLayout(); // Not virtual, do not subclass this class\n\n  DataLayout &operator=(const DataLayout &DL) {\n    clear();\n    StringRepresentation = DL.StringRepresentation;\n    BigEndian = DL.isBigEndian();\n    AllocaAddrSpace = DL.AllocaAddrSpace;\n    StackNaturalAlign = DL.StackNaturalAlign;\n    FunctionPtrAlign = DL.FunctionPtrAlign;\n    TheFunctionPtrAlignType = DL.TheFunctionPtrAlignType;\n    ProgramAddrSpace = DL.ProgramAddrSpace;\n    DefaultGlobalsAddrSpace = DL.DefaultGlobalsAddrSpace;\n    ManglingMode = DL.ManglingMode;\n    LegalIntWidths = DL.LegalIntWidths;\n    Alignments = DL.Alignments;\n    Pointers = DL.Pointers;\n    NonIntegralAddressSpaces = DL.NonIntegralAddressSpaces;\n    return *this;\n  }\n\n  bool operator==(const DataLayout &Other) const;\n  bool operator!=(const DataLayout &Other) const { return !(*this == Other); }\n\n  void init(const Module *M);\n\n  /// Parse a data layout string (with fallback to default values).\n  void reset(StringRef LayoutDescription);\n\n  /// Parse a data layout string and return the layout. Return an error\n  /// description on failure.\n  static Expected<DataLayout> parse(StringRef LayoutDescription);\n\n  /// Layout endianness...\n  bool isLittleEndian() const { return !BigEndian; }\n  bool isBigEndian() const { return BigEndian; }\n\n  /// Returns the string representation of the DataLayout.\n  ///\n  /// This representation is in the same format accepted by the string\n  /// constructor above. This should not be used to compare two DataLayout as\n  /// different string can represent the same layout.\n  const std::string &getStringRepresentation() const {\n    return StringRepresentation;\n  }\n\n  /// Test if the DataLayout was constructed from an empty string.\n  bool isDefault() const { return StringRepresentation.empty(); }\n\n  /// Returns true if the specified type is known to be a native integer\n  /// type supported by the CPU.\n  ///\n  /// For example, i64 is not native on most 32-bit CPUs and i37 is not native\n  /// on any known one. This returns false if the integer width is not legal.\n  ///\n  /// The width is specified in bits.\n  bool isLegalInteger(uint64_t Width) const {\n    return llvm::is_contained(LegalIntWidths, Width);\n  }\n\n  bool isIllegalInteger(uint64_t Width) const { return !isLegalInteger(Width); }\n\n  /// Returns true if the given alignment exceeds the natural stack alignment.\n  bool exceedsNaturalStackAlignment(Align Alignment) const {\n    return StackNaturalAlign && (Alignment > *StackNaturalAlign);\n  }\n\n  Align getStackAlignment() const {\n    assert(StackNaturalAlign && \"StackNaturalAlign must be defined\");\n    return *StackNaturalAlign;\n  }\n\n  unsigned getAllocaAddrSpace() const { return AllocaAddrSpace; }\n\n  /// Returns the alignment of function pointers, which may or may not be\n  /// related to the alignment of functions.\n  /// \\see getFunctionPtrAlignType\n  MaybeAlign getFunctionPtrAlign() const { return FunctionPtrAlign; }\n\n  /// Return the type of function pointer alignment.\n  /// \\see getFunctionPtrAlign\n  FunctionPtrAlignType getFunctionPtrAlignType() const {\n    return TheFunctionPtrAlignType;\n  }\n\n  unsigned getProgramAddressSpace() const { return ProgramAddrSpace; }\n  unsigned getDefaultGlobalsAddressSpace() const {\n    return DefaultGlobalsAddrSpace;\n  }\n\n  bool hasMicrosoftFastStdCallMangling() const {\n    return ManglingMode == MM_WinCOFFX86;\n  }\n\n  /// Returns true if symbols with leading question marks should not receive IR\n  /// mangling. True for Windows mangling modes.\n  bool doNotMangleLeadingQuestionMark() const {\n    return ManglingMode == MM_WinCOFF || ManglingMode == MM_WinCOFFX86;\n  }\n\n  bool hasLinkerPrivateGlobalPrefix() const { return ManglingMode == MM_MachO; }\n\n  StringRef getLinkerPrivateGlobalPrefix() const {\n    if (ManglingMode == MM_MachO)\n      return \"l\";\n    return \"\";\n  }\n\n  char getGlobalPrefix() const {\n    switch (ManglingMode) {\n    case MM_None:\n    case MM_ELF:\n    case MM_Mips:\n    case MM_WinCOFF:\n    case MM_XCOFF:\n      return '\\0';\n    case MM_MachO:\n    case MM_WinCOFFX86:\n      return '_';\n    }\n    llvm_unreachable(\"invalid mangling mode\");\n  }\n\n  StringRef getPrivateGlobalPrefix() const {\n    switch (ManglingMode) {\n    case MM_None:\n      return \"\";\n    case MM_ELF:\n    case MM_WinCOFF:\n      return \".L\";\n    case MM_Mips:\n      return \"$\";\n    case MM_MachO:\n    case MM_WinCOFFX86:\n      return \"L\";\n    case MM_XCOFF:\n      return \"L..\";\n    }\n    llvm_unreachable(\"invalid mangling mode\");\n  }\n\n  static const char *getManglingComponent(const Triple &T);\n\n  /// Returns true if the specified type fits in a native integer type\n  /// supported by the CPU.\n  ///\n  /// For example, if the CPU only supports i32 as a native integer type, then\n  /// i27 fits in a legal integer type but i45 does not.\n  bool fitsInLegalInteger(unsigned Width) const {\n    for (unsigned LegalIntWidth : LegalIntWidths)\n      if (Width <= LegalIntWidth)\n        return true;\n    return false;\n  }\n\n  /// Layout pointer alignment\n  Align getPointerABIAlignment(unsigned AS) const;\n\n  /// Return target's alignment for stack-based pointers\n  /// FIXME: The defaults need to be removed once all of\n  /// the backends/clients are updated.\n  Align getPointerPrefAlignment(unsigned AS = 0) const;\n\n  /// Layout pointer size\n  /// FIXME: The defaults need to be removed once all of\n  /// the backends/clients are updated.\n  unsigned getPointerSize(unsigned AS = 0) const;\n\n  /// Returns the maximum pointer size over all address spaces.\n  unsigned getMaxPointerSize() const;\n\n  // Index size used for address calculation.\n  unsigned getIndexSize(unsigned AS) const;\n\n  /// Return the address spaces containing non-integral pointers.  Pointers in\n  /// this address space don't have a well-defined bitwise representation.\n  ArrayRef<unsigned> getNonIntegralAddressSpaces() const {\n    return NonIntegralAddressSpaces;\n  }\n\n  bool isNonIntegralAddressSpace(unsigned AddrSpace) const {\n    ArrayRef<unsigned> NonIntegralSpaces = getNonIntegralAddressSpaces();\n    return is_contained(NonIntegralSpaces, AddrSpace);\n  }\n\n  bool isNonIntegralPointerType(PointerType *PT) const {\n    return isNonIntegralAddressSpace(PT->getAddressSpace());\n  }\n\n  bool isNonIntegralPointerType(Type *Ty) const {\n    auto *PTy = dyn_cast<PointerType>(Ty);\n    return PTy && isNonIntegralPointerType(PTy);\n  }\n\n  /// Layout pointer size, in bits\n  /// FIXME: The defaults need to be removed once all of\n  /// the backends/clients are updated.\n  unsigned getPointerSizeInBits(unsigned AS = 0) const {\n    return getPointerSize(AS) * 8;\n  }\n\n  /// Returns the maximum pointer size over all address spaces.\n  unsigned getMaxPointerSizeInBits() const {\n    return getMaxPointerSize() * 8;\n  }\n\n  /// Size in bits of index used for address calculation in getelementptr.\n  unsigned getIndexSizeInBits(unsigned AS) const {\n    return getIndexSize(AS) * 8;\n  }\n\n  /// Layout pointer size, in bits, based on the type.  If this function is\n  /// called with a pointer type, then the type size of the pointer is returned.\n  /// If this function is called with a vector of pointers, then the type size\n  /// of the pointer is returned.  This should only be called with a pointer or\n  /// vector of pointers.\n  unsigned getPointerTypeSizeInBits(Type *) const;\n\n  /// Layout size of the index used in GEP calculation.\n  /// The function should be called with pointer or vector of pointers type.\n  unsigned getIndexTypeSizeInBits(Type *Ty) const;\n\n  unsigned getPointerTypeSize(Type *Ty) const {\n    return getPointerTypeSizeInBits(Ty) / 8;\n  }\n\n  /// Size examples:\n  ///\n  /// Type        SizeInBits  StoreSizeInBits  AllocSizeInBits[*]\n  /// ----        ----------  ---------------  ---------------\n  ///  i1            1           8                8\n  ///  i8            8           8                8\n  ///  i19          19          24               32\n  ///  i32          32          32               32\n  ///  i100        100         104              128\n  ///  i128        128         128              128\n  ///  Float        32          32               32\n  ///  Double       64          64               64\n  ///  X86_FP80     80          80               96\n  ///\n  /// [*] The alloc size depends on the alignment, and thus on the target.\n  ///     These values are for x86-32 linux.\n\n  /// Returns the number of bits necessary to hold the specified type.\n  ///\n  /// If Ty is a scalable vector type, the scalable property will be set and\n  /// the runtime size will be a positive integer multiple of the base size.\n  ///\n  /// For example, returns 36 for i36 and 80 for x86_fp80. The type passed must\n  /// have a size (Type::isSized() must return true).\n  TypeSize getTypeSizeInBits(Type *Ty) const;\n\n  /// Returns the maximum number of bytes that may be overwritten by\n  /// storing the specified type.\n  ///\n  /// If Ty is a scalable vector type, the scalable property will be set and\n  /// the runtime size will be a positive integer multiple of the base size.\n  ///\n  /// For example, returns 5 for i36 and 10 for x86_fp80.\n  TypeSize getTypeStoreSize(Type *Ty) const {\n    TypeSize BaseSize = getTypeSizeInBits(Ty);\n    return { (BaseSize.getKnownMinSize() + 7) / 8, BaseSize.isScalable() };\n  }\n\n  /// Returns the maximum number of bits that may be overwritten by\n  /// storing the specified type; always a multiple of 8.\n  ///\n  /// If Ty is a scalable vector type, the scalable property will be set and\n  /// the runtime size will be a positive integer multiple of the base size.\n  ///\n  /// For example, returns 40 for i36 and 80 for x86_fp80.\n  TypeSize getTypeStoreSizeInBits(Type *Ty) const {\n    return 8 * getTypeStoreSize(Ty);\n  }\n\n  /// Returns true if no extra padding bits are needed when storing the\n  /// specified type.\n  ///\n  /// For example, returns false for i19 that has a 24-bit store size.\n  bool typeSizeEqualsStoreSize(Type *Ty) const {\n    return getTypeSizeInBits(Ty) == getTypeStoreSizeInBits(Ty);\n  }\n\n  /// Returns the offset in bytes between successive objects of the\n  /// specified type, including alignment padding.\n  ///\n  /// If Ty is a scalable vector type, the scalable property will be set and\n  /// the runtime size will be a positive integer multiple of the base size.\n  ///\n  /// This is the amount that alloca reserves for this type. For example,\n  /// returns 12 or 16 for x86_fp80, depending on alignment.\n  TypeSize getTypeAllocSize(Type *Ty) const {\n    // Round up to the next alignment boundary.\n    return alignTo(getTypeStoreSize(Ty), getABITypeAlignment(Ty));\n  }\n\n  /// Returns the offset in bits between successive objects of the\n  /// specified type, including alignment padding; always a multiple of 8.\n  ///\n  /// If Ty is a scalable vector type, the scalable property will be set and\n  /// the runtime size will be a positive integer multiple of the base size.\n  ///\n  /// This is the amount that alloca reserves for this type. For example,\n  /// returns 96 or 128 for x86_fp80, depending on alignment.\n  TypeSize getTypeAllocSizeInBits(Type *Ty) const {\n    return 8 * getTypeAllocSize(Ty);\n  }\n\n  /// Returns the minimum ABI-required alignment for the specified type.\n  /// FIXME: Deprecate this function once migration to Align is over.\n  unsigned getABITypeAlignment(Type *Ty) const;\n\n  /// Returns the minimum ABI-required alignment for the specified type.\n  Align getABITypeAlign(Type *Ty) const;\n\n  /// Helper function to return `Alignment` if it's set or the result of\n  /// `getABITypeAlignment(Ty)`, in any case the result is a valid alignment.\n  inline Align getValueOrABITypeAlignment(MaybeAlign Alignment,\n                                          Type *Ty) const {\n    return Alignment ? *Alignment : getABITypeAlign(Ty);\n  }\n\n  /// Returns the minimum ABI-required alignment for an integer type of\n  /// the specified bitwidth.\n  Align getABIIntegerTypeAlignment(unsigned BitWidth) const {\n    return getIntegerAlignment(BitWidth, /* abi_or_pref */ true);\n  }\n\n  /// Returns the preferred stack/global alignment for the specified\n  /// type.\n  ///\n  /// This is always at least as good as the ABI alignment.\n  /// FIXME: Deprecate this function once migration to Align is over.\n  unsigned getPrefTypeAlignment(Type *Ty) const;\n\n  /// Returns the preferred stack/global alignment for the specified\n  /// type.\n  ///\n  /// This is always at least as good as the ABI alignment.\n  Align getPrefTypeAlign(Type *Ty) const;\n\n  /// Returns an integer type with size at least as big as that of a\n  /// pointer in the given address space.\n  IntegerType *getIntPtrType(LLVMContext &C, unsigned AddressSpace = 0) const;\n\n  /// Returns an integer (vector of integer) type with size at least as\n  /// big as that of a pointer of the given pointer (vector of pointer) type.\n  Type *getIntPtrType(Type *) const;\n\n  /// Returns the smallest integer type with size at least as big as\n  /// Width bits.\n  Type *getSmallestLegalIntType(LLVMContext &C, unsigned Width = 0) const;\n\n  /// Returns the largest legal integer type, or null if none are set.\n  Type *getLargestLegalIntType(LLVMContext &C) const {\n    unsigned LargestSize = getLargestLegalIntTypeSizeInBits();\n    return (LargestSize == 0) ? nullptr : Type::getIntNTy(C, LargestSize);\n  }\n\n  /// Returns the size of largest legal integer type size, or 0 if none\n  /// are set.\n  unsigned getLargestLegalIntTypeSizeInBits() const;\n\n  /// Returns the type of a GEP index.\n  /// If it was not specified explicitly, it will be the integer type of the\n  /// pointer width - IntPtrType.\n  Type *getIndexType(Type *PtrTy) const;\n\n  /// Returns the offset from the beginning of the type for the specified\n  /// indices.\n  ///\n  /// Note that this takes the element type, not the pointer type.\n  /// This is used to implement getelementptr.\n  int64_t getIndexedOffsetInType(Type *ElemTy, ArrayRef<Value *> Indices) const;\n\n  /// Returns a StructLayout object, indicating the alignment of the\n  /// struct, its size, and the offsets of its fields.\n  ///\n  /// Note that this information is lazily cached.\n  const StructLayout *getStructLayout(StructType *Ty) const;\n\n  /// Returns the preferred alignment of the specified global.\n  ///\n  /// This includes an explicitly requested alignment (if the global has one).\n  Align getPreferredAlign(const GlobalVariable *GV) const;\n\n  /// Returns the preferred alignment of the specified global.\n  ///\n  /// This includes an explicitly requested alignment (if the global has one).\n  LLVM_ATTRIBUTE_DEPRECATED(\n      inline unsigned getPreferredAlignment(const GlobalVariable *GV) const,\n      \"Use getPreferredAlign instead\") {\n    return getPreferredAlign(GV).value();\n  }\n\n  /// Returns the preferred alignment of the specified global, returned\n  /// in log form.\n  ///\n  /// This includes an explicitly requested alignment (if the global has one).\n  LLVM_ATTRIBUTE_DEPRECATED(\n      inline unsigned getPreferredAlignmentLog(const GlobalVariable *GV) const,\n      \"Inline where needed\") {\n    return Log2(getPreferredAlign(GV));\n  }\n};\n\ninline DataLayout *unwrap(LLVMTargetDataRef P) {\n  return reinterpret_cast<DataLayout *>(P);\n}\n\ninline LLVMTargetDataRef wrap(const DataLayout *P) {\n  return reinterpret_cast<LLVMTargetDataRef>(const_cast<DataLayout *>(P));\n}\n\n/// Used to lazily calculate structure layout information for a target machine,\n/// based on the DataLayout structure.\nclass StructLayout {\n  uint64_t StructSize;\n  Align StructAlignment;\n  unsigned IsPadded : 1;\n  unsigned NumElements : 31;\n  uint64_t MemberOffsets[1]; // variable sized array!\n\npublic:\n  uint64_t getSizeInBytes() const { return StructSize; }\n\n  uint64_t getSizeInBits() const { return 8 * StructSize; }\n\n  Align getAlignment() const { return StructAlignment; }\n\n  /// Returns whether the struct has padding or not between its fields.\n  /// NB: Padding in nested element is not taken into account.\n  bool hasPadding() const { return IsPadded; }\n\n  /// Given a valid byte offset into the structure, returns the structure\n  /// index that contains it.\n  unsigned getElementContainingOffset(uint64_t Offset) const;\n\n  uint64_t getElementOffset(unsigned Idx) const {\n    assert(Idx < NumElements && \"Invalid element idx!\");\n    return MemberOffsets[Idx];\n  }\n\n  uint64_t getElementOffsetInBits(unsigned Idx) const {\n    return getElementOffset(Idx) * 8;\n  }\n\nprivate:\n  friend class DataLayout; // Only DataLayout can create this class\n\n  StructLayout(StructType *ST, const DataLayout &DL);\n};\n\n// The implementation of this method is provided inline as it is particularly\n// well suited to constant folding when called on a specific Type subclass.\ninline TypeSize DataLayout::getTypeSizeInBits(Type *Ty) const {\n  assert(Ty->isSized() && \"Cannot getTypeInfo() on a type that is unsized!\");\n  switch (Ty->getTypeID()) {\n  case Type::LabelTyID:\n    return TypeSize::Fixed(getPointerSizeInBits(0));\n  case Type::PointerTyID:\n    return TypeSize::Fixed(getPointerSizeInBits(Ty->getPointerAddressSpace()));\n  case Type::ArrayTyID: {\n    ArrayType *ATy = cast<ArrayType>(Ty);\n    return ATy->getNumElements() *\n           getTypeAllocSizeInBits(ATy->getElementType());\n  }\n  case Type::StructTyID:\n    // Get the layout annotation... which is lazily created on demand.\n    return TypeSize::Fixed(\n                        getStructLayout(cast<StructType>(Ty))->getSizeInBits());\n  case Type::IntegerTyID:\n    return TypeSize::Fixed(Ty->getIntegerBitWidth());\n  case Type::HalfTyID:\n  case Type::BFloatTyID:\n    return TypeSize::Fixed(16);\n  case Type::FloatTyID:\n    return TypeSize::Fixed(32);\n  case Type::DoubleTyID:\n  case Type::X86_MMXTyID:\n    return TypeSize::Fixed(64);\n  case Type::PPC_FP128TyID:\n  case Type::FP128TyID:\n    return TypeSize::Fixed(128);\n  case Type::X86_AMXTyID:\n    return TypeSize::Fixed(8192);\n  // In memory objects this is always aligned to a higher boundary, but\n  // only 80 bits contain information.\n  case Type::X86_FP80TyID:\n    return TypeSize::Fixed(80);\n  case Type::FixedVectorTyID:\n  case Type::ScalableVectorTyID: {\n    VectorType *VTy = cast<VectorType>(Ty);\n    auto EltCnt = VTy->getElementCount();\n    uint64_t MinBits = EltCnt.getKnownMinValue() *\n                       getTypeSizeInBits(VTy->getElementType()).getFixedSize();\n    return TypeSize(MinBits, EltCnt.isScalable());\n  }\n  default:\n    llvm_unreachable(\"DataLayout::getTypeSizeInBits(): Unsupported type\");\n  }\n}\n\n} // end namespace llvm\n\n#endif // LLVM_IR_DATALAYOUT_H\n"}, "73": {"id": 73, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DebugLoc.h", "content": "//===- DebugLoc.h - Debug Location Information ------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines a number of light weight data structures used\n// to describe and track debug location information.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_DEBUGLOC_H\n#define LLVM_IR_DEBUGLOC_H\n\n#include \"llvm/IR/TrackingMDRef.h\"\n#include \"llvm/Support/DataTypes.h\"\n\nnamespace llvm {\n\n  class LLVMContext;\n  class raw_ostream;\n  class DILocation;\n\n  /// A debug info location.\n  ///\n  /// This class is a wrapper around a tracking reference to an \\a DILocation\n  /// pointer.\n  ///\n  /// To avoid extra includes, \\a DebugLoc doubles the \\a DILocation API with a\n  /// one based on relatively opaque \\a MDNode pointers.\n  class DebugLoc {\n    TrackingMDNodeRef Loc;\n\n  public:\n    DebugLoc() = default;\n\n    /// Construct from an \\a DILocation.\n    DebugLoc(const DILocation *L);\n\n    /// Construct from an \\a MDNode.\n    ///\n    /// Note: if \\c N is not an \\a DILocation, a verifier check will fail, and\n    /// accessors will crash.  However, construction from other nodes is\n    /// supported in order to handle forward references when reading textual\n    /// IR.\n    explicit DebugLoc(const MDNode *N);\n\n    /// Get the underlying \\a DILocation.\n    ///\n    /// \\pre !*this or \\c isa<DILocation>(getAsMDNode()).\n    /// @{\n    DILocation *get() const;\n    operator DILocation *() const { return get(); }\n    DILocation *operator->() const { return get(); }\n    DILocation &operator*() const { return *get(); }\n    /// @}\n\n    /// Check for null.\n    ///\n    /// Check for null in a way that is safe with broken debug info.  Unlike\n    /// the conversion to \\c DILocation, this doesn't require that \\c Loc is of\n    /// the right type.  Important for cases like \\a llvm::StripDebugInfo() and\n    /// \\a Instruction::hasMetadata().\n    explicit operator bool() const { return Loc; }\n\n    /// Check whether this has a trivial destructor.\n    bool hasTrivialDestructor() const { return Loc.hasTrivialDestructor(); }\n\n    enum { ReplaceLastInlinedAt = true };\n    /// Rebuild the entire inlined-at chain for this instruction so that the top of\n    /// the chain now is inlined-at the new call site.\n    /// \\param   InlinedAt    The new outermost inlined-at in the chain.\n    static DebugLoc appendInlinedAt(const DebugLoc &DL, DILocation *InlinedAt,\n                                    LLVMContext &Ctx,\n                                    DenseMap<const MDNode *, MDNode *> &Cache);\n\n    unsigned getLine() const;\n    unsigned getCol() const;\n    MDNode *getScope() const;\n    DILocation *getInlinedAt() const;\n\n    /// Get the fully inlined-at scope for a DebugLoc.\n    ///\n    /// Gets the inlined-at scope for a DebugLoc.\n    MDNode *getInlinedAtScope() const;\n\n    /// Find the debug info location for the start of the function.\n    ///\n    /// Walk up the scope chain of given debug loc and find line number info\n    /// for the function.\n    ///\n    /// FIXME: Remove this.  Users should use DILocation/DILocalScope API to\n    /// find the subprogram, and then DILocation::get().\n    DebugLoc getFnDebugLoc() const;\n\n    /// Return \\c this as a bar \\a MDNode.\n    MDNode *getAsMDNode() const { return Loc; }\n\n    /// Check if the DebugLoc corresponds to an implicit code.\n    bool isImplicitCode() const;\n    void setImplicitCode(bool ImplicitCode);\n\n    bool operator==(const DebugLoc &DL) const { return Loc == DL.Loc; }\n    bool operator!=(const DebugLoc &DL) const { return Loc != DL.Loc; }\n\n    void dump() const;\n\n    /// prints source location /path/to/file.exe:line:col @[inlined at]\n    void print(raw_ostream &OS) const;\n  };\n\n} // end namespace llvm\n\n#endif // LLVM_IR_DEBUGLOC_H\n"}, "77": {"id": 77, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/IRBuilder.h", "content": "//===- llvm/IRBuilder.h - Builder for LLVM Instructions ---------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the IRBuilder class, which is used as a convenient way\n// to create LLVM instructions with a consistent and simplified interface.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_IRBUILDER_H\n#define LLVM_IR_IRBUILDER_H\n\n#include \"llvm-c/Types.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/IR/BasicBlock.h\"\n#include \"llvm/IR/Constant.h\"\n#include \"llvm/IR/ConstantFolder.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/DebugInfoMetadata.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/GlobalVariable.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/IntrinsicInst.h\"\n#include \"llvm/IR/LLVMContext.h\"\n#include \"llvm/IR/Module.h\"\n#include \"llvm/IR/Operator.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/IR/ValueHandle.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/CBindingWrapping.h\"\n#include \"llvm/Support/Casting.h\"\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <functional>\n#include <utility>\n\nnamespace llvm {\n\nclass APInt;\nclass MDNode;\nclass Use;\n\n/// This provides the default implementation of the IRBuilder\n/// 'InsertHelper' method that is called whenever an instruction is created by\n/// IRBuilder and needs to be inserted.\n///\n/// By default, this inserts the instruction at the insertion point.\nclass IRBuilderDefaultInserter {\npublic:\n  virtual ~IRBuilderDefaultInserter();\n\n  virtual void InsertHelper(Instruction *I, const Twine &Name,\n                            BasicBlock *BB,\n                            BasicBlock::iterator InsertPt) const {\n    if (BB) BB->getInstList().insert(InsertPt, I);\n    I->setName(Name);\n  }\n};\n\n/// Provides an 'InsertHelper' that calls a user-provided callback after\n/// performing the default insertion.\nclass IRBuilderCallbackInserter : public IRBuilderDefaultInserter {\n  std::function<void(Instruction *)> Callback;\n\npublic:\n  virtual ~IRBuilderCallbackInserter();\n\n  IRBuilderCallbackInserter(std::function<void(Instruction *)> Callback)\n      : Callback(std::move(Callback)) {}\n\n  void InsertHelper(Instruction *I, const Twine &Name,\n                    BasicBlock *BB,\n                    BasicBlock::iterator InsertPt) const override {\n    IRBuilderDefaultInserter::InsertHelper(I, Name, BB, InsertPt);\n    Callback(I);\n  }\n};\n\n/// Common base class shared among various IRBuilders.\nclass IRBuilderBase {\n  /// Pairs of (metadata kind, MDNode *) that should be added to all newly\n  /// created instructions, like !dbg metadata.\n  SmallVector<std::pair<unsigned, MDNode *>, 2> MetadataToCopy;\n\n  /// Add or update the an entry (Kind, MD) to MetadataToCopy, if \\p MD is not\n  /// null. If \\p MD is null, remove the entry with \\p Kind.\n  void AddOrRemoveMetadataToCopy(unsigned Kind, MDNode *MD) {\n    if (!MD) {\n      erase_if(MetadataToCopy, [Kind](const std::pair<unsigned, MDNode *> &KV) {\n        return KV.first == Kind;\n      });\n      return;\n    }\n\n    for (auto &KV : MetadataToCopy)\n      if (KV.first == Kind) {\n        KV.second = MD;\n        return;\n      }\n\n    MetadataToCopy.emplace_back(Kind, MD);\n  }\n\nprotected:\n  BasicBlock *BB;\n  BasicBlock::iterator InsertPt;\n  LLVMContext &Context;\n  const IRBuilderFolder &Folder;\n  const IRBuilderDefaultInserter &Inserter;\n\n  MDNode *DefaultFPMathTag;\n  FastMathFlags FMF;\n\n  bool IsFPConstrained;\n  fp::ExceptionBehavior DefaultConstrainedExcept;\n  RoundingMode DefaultConstrainedRounding;\n\n  ArrayRef<OperandBundleDef> DefaultOperandBundles;\n\npublic:\n  IRBuilderBase(LLVMContext &context, const IRBuilderFolder &Folder,\n                const IRBuilderDefaultInserter &Inserter,\n                MDNode *FPMathTag, ArrayRef<OperandBundleDef> OpBundles)\n      : Context(context), Folder(Folder), Inserter(Inserter),\n        DefaultFPMathTag(FPMathTag), IsFPConstrained(false),\n        DefaultConstrainedExcept(fp::ebStrict),\n        DefaultConstrainedRounding(RoundingMode::Dynamic),\n        DefaultOperandBundles(OpBundles) {\n    ClearInsertionPoint();\n  }\n\n  /// Insert and return the specified instruction.\n  template<typename InstTy>\n  InstTy *Insert(InstTy *I, const Twine &Name = \"\") const {\n    Inserter.InsertHelper(I, Name, BB, InsertPt);\n    AddMetadataToInst(I);\n    return I;\n  }\n\n  /// No-op overload to handle constants.\n  Constant *Insert(Constant *C, const Twine& = \"\") const {\n    return C;\n  }\n\n  Value *Insert(Value *V, const Twine &Name = \"\") const {\n    if (Instruction *I = dyn_cast<Instruction>(V))\n      return Insert(I, Name);\n    assert(isa<Constant>(V));\n    return V;\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Builder configuration methods\n  //===--------------------------------------------------------------------===//\n\n  /// Clear the insertion point: created instructions will not be\n  /// inserted into a block.\n  void ClearInsertionPoint() {\n    BB = nullptr;\n    InsertPt = BasicBlock::iterator();\n  }\n\n  BasicBlock *GetInsertBlock() const { return BB; }\n  BasicBlock::iterator GetInsertPoint() const { return InsertPt; }\n  LLVMContext &getContext() const { return Context; }\n\n  /// This specifies that created instructions should be appended to the\n  /// end of the specified block.\n  void SetInsertPoint(BasicBlock *TheBB) {\n    BB = TheBB;\n    InsertPt = BB->end();\n  }\n\n  /// This specifies that created instructions should be inserted before\n  /// the specified instruction.\n  void SetInsertPoint(Instruction *I) {\n    BB = I->getParent();\n    InsertPt = I->getIterator();\n    assert(InsertPt != BB->end() && \"Can't read debug loc from end()\");\n    SetCurrentDebugLocation(I->getDebugLoc());\n  }\n\n  /// This specifies that created instructions should be inserted at the\n  /// specified point.\n  void SetInsertPoint(BasicBlock *TheBB, BasicBlock::iterator IP) {\n    BB = TheBB;\n    InsertPt = IP;\n    if (IP != TheBB->end())\n      SetCurrentDebugLocation(IP->getDebugLoc());\n  }\n\n  /// Set location information used by debugging information.\n  void SetCurrentDebugLocation(DebugLoc L) {\n    AddOrRemoveMetadataToCopy(LLVMContext::MD_dbg, L.getAsMDNode());\n  }\n\n  /// Collect metadata with IDs \\p MetadataKinds from \\p Src which should be\n  /// added to all created instructions. Entries present in MedataDataToCopy but\n  /// not on \\p Src will be dropped from MetadataToCopy.\n  void CollectMetadataToCopy(Instruction *Src,\n                             ArrayRef<unsigned> MetadataKinds) {\n    for (unsigned K : MetadataKinds)\n      AddOrRemoveMetadataToCopy(K, Src->getMetadata(K));\n  }\n\n  /// Get location information used by debugging information.\n  DebugLoc getCurrentDebugLocation() const {\n    for (auto &KV : MetadataToCopy)\n      if (KV.first == LLVMContext::MD_dbg)\n        return {cast<DILocation>(KV.second)};\n\n    return {};\n  }\n\n  /// If this builder has a current debug location, set it on the\n  /// specified instruction.\n  void SetInstDebugLocation(Instruction *I) const {\n    for (const auto &KV : MetadataToCopy)\n      if (KV.first == LLVMContext::MD_dbg) {\n        I->setDebugLoc(DebugLoc(KV.second));\n        return;\n      }\n  }\n\n  /// Add all entries in MetadataToCopy to \\p I.\n  void AddMetadataToInst(Instruction *I) const {\n    for (auto &KV : MetadataToCopy)\n      I->setMetadata(KV.first, KV.second);\n  }\n\n  /// Get the return type of the current function that we're emitting\n  /// into.\n  Type *getCurrentFunctionReturnType() const;\n\n  /// InsertPoint - A saved insertion point.\n  class InsertPoint {\n    BasicBlock *Block = nullptr;\n    BasicBlock::iterator Point;\n\n  public:\n    /// Creates a new insertion point which doesn't point to anything.\n    InsertPoint() = default;\n\n    /// Creates a new insertion point at the given location.\n    InsertPoint(BasicBlock *InsertBlock, BasicBlock::iterator InsertPoint)\n        : Block(InsertBlock), Point(InsertPoint) {}\n\n    /// Returns true if this insert point is set.\n    bool isSet() const { return (Block != nullptr); }\n\n    BasicBlock *getBlock() const { return Block; }\n    BasicBlock::iterator getPoint() const { return Point; }\n  };\n\n  /// Returns the current insert point.\n  InsertPoint saveIP() const {\n    return InsertPoint(GetInsertBlock(), GetInsertPoint());\n  }\n\n  /// Returns the current insert point, clearing it in the process.\n  InsertPoint saveAndClearIP() {\n    InsertPoint IP(GetInsertBlock(), GetInsertPoint());\n    ClearInsertionPoint();\n    return IP;\n  }\n\n  /// Sets the current insert point to a previously-saved location.\n  void restoreIP(InsertPoint IP) {\n    if (IP.isSet())\n      SetInsertPoint(IP.getBlock(), IP.getPoint());\n    else\n      ClearInsertionPoint();\n  }\n\n  /// Get the floating point math metadata being used.\n  MDNode *getDefaultFPMathTag() const { return DefaultFPMathTag; }\n\n  /// Get the flags to be applied to created floating point ops\n  FastMathFlags getFastMathFlags() const { return FMF; }\n\n  FastMathFlags &getFastMathFlags() { return FMF; }\n\n  /// Clear the fast-math flags.\n  void clearFastMathFlags() { FMF.clear(); }\n\n  /// Set the floating point math metadata to be used.\n  void setDefaultFPMathTag(MDNode *FPMathTag) { DefaultFPMathTag = FPMathTag; }\n\n  /// Set the fast-math flags to be used with generated fp-math operators\n  void setFastMathFlags(FastMathFlags NewFMF) { FMF = NewFMF; }\n\n  /// Enable/Disable use of constrained floating point math. When\n  /// enabled the CreateF<op>() calls instead create constrained\n  /// floating point intrinsic calls. Fast math flags are unaffected\n  /// by this setting.\n  void setIsFPConstrained(bool IsCon) { IsFPConstrained = IsCon; }\n\n  /// Query for the use of constrained floating point math\n  bool getIsFPConstrained() { return IsFPConstrained; }\n\n  /// Set the exception handling to be used with constrained floating point\n  void setDefaultConstrainedExcept(fp::ExceptionBehavior NewExcept) {\n#ifndef NDEBUG\n    Optional<StringRef> ExceptStr = ExceptionBehaviorToStr(NewExcept);\n    assert(ExceptStr.hasValue() && \"Garbage strict exception behavior!\");\n#endif\n    DefaultConstrainedExcept = NewExcept;\n  }\n\n  /// Set the rounding mode handling to be used with constrained floating point\n  void setDefaultConstrainedRounding(RoundingMode NewRounding) {\n#ifndef NDEBUG\n    Optional<StringRef> RoundingStr = RoundingModeToStr(NewRounding);\n    assert(RoundingStr.hasValue() && \"Garbage strict rounding mode!\");\n#endif\n    DefaultConstrainedRounding = NewRounding;\n  }\n\n  /// Get the exception handling used with constrained floating point\n  fp::ExceptionBehavior getDefaultConstrainedExcept() {\n    return DefaultConstrainedExcept;\n  }\n\n  /// Get the rounding mode handling used with constrained floating point\n  RoundingMode getDefaultConstrainedRounding() {\n    return DefaultConstrainedRounding;\n  }\n\n  void setConstrainedFPFunctionAttr() {\n    assert(BB && \"Must have a basic block to set any function attributes!\");\n\n    Function *F = BB->getParent();\n    if (!F->hasFnAttribute(Attribute::StrictFP)) {\n      F->addFnAttr(Attribute::StrictFP);\n    }\n  }\n\n  void setConstrainedFPCallAttr(CallBase *I) {\n    I->addAttribute(AttributeList::FunctionIndex, Attribute::StrictFP);\n  }\n\n  void setDefaultOperandBundles(ArrayRef<OperandBundleDef> OpBundles) {\n    DefaultOperandBundles = OpBundles;\n  }\n\n  //===--------------------------------------------------------------------===//\n  // RAII helpers.\n  //===--------------------------------------------------------------------===//\n\n  // RAII object that stores the current insertion point and restores it\n  // when the object is destroyed. This includes the debug location.\n  class InsertPointGuard {\n    IRBuilderBase &Builder;\n    AssertingVH<BasicBlock> Block;\n    BasicBlock::iterator Point;\n    DebugLoc DbgLoc;\n\n  public:\n    InsertPointGuard(IRBuilderBase &B)\n        : Builder(B), Block(B.GetInsertBlock()), Point(B.GetInsertPoint()),\n          DbgLoc(B.getCurrentDebugLocation()) {}\n\n    InsertPointGuard(const InsertPointGuard &) = delete;\n    InsertPointGuard &operator=(const InsertPointGuard &) = delete;\n\n    ~InsertPointGuard() {\n      Builder.restoreIP(InsertPoint(Block, Point));\n      Builder.SetCurrentDebugLocation(DbgLoc);\n    }\n  };\n\n  // RAII object that stores the current fast math settings and restores\n  // them when the object is destroyed.\n  class FastMathFlagGuard {\n    IRBuilderBase &Builder;\n    FastMathFlags FMF;\n    MDNode *FPMathTag;\n    bool IsFPConstrained;\n    fp::ExceptionBehavior DefaultConstrainedExcept;\n    RoundingMode DefaultConstrainedRounding;\n\n  public:\n    FastMathFlagGuard(IRBuilderBase &B)\n        : Builder(B), FMF(B.FMF), FPMathTag(B.DefaultFPMathTag),\n          IsFPConstrained(B.IsFPConstrained),\n          DefaultConstrainedExcept(B.DefaultConstrainedExcept),\n          DefaultConstrainedRounding(B.DefaultConstrainedRounding) {}\n\n    FastMathFlagGuard(const FastMathFlagGuard &) = delete;\n    FastMathFlagGuard &operator=(const FastMathFlagGuard &) = delete;\n\n    ~FastMathFlagGuard() {\n      Builder.FMF = FMF;\n      Builder.DefaultFPMathTag = FPMathTag;\n      Builder.IsFPConstrained = IsFPConstrained;\n      Builder.DefaultConstrainedExcept = DefaultConstrainedExcept;\n      Builder.DefaultConstrainedRounding = DefaultConstrainedRounding;\n    }\n  };\n\n  // RAII object that stores the current default operand bundles and restores\n  // them when the object is destroyed.\n  class OperandBundlesGuard {\n    IRBuilderBase &Builder;\n    ArrayRef<OperandBundleDef> DefaultOperandBundles;\n\n  public:\n    OperandBundlesGuard(IRBuilderBase &B)\n        : Builder(B), DefaultOperandBundles(B.DefaultOperandBundles) {}\n\n    OperandBundlesGuard(const OperandBundlesGuard &) = delete;\n    OperandBundlesGuard &operator=(const OperandBundlesGuard &) = delete;\n\n    ~OperandBundlesGuard() {\n      Builder.DefaultOperandBundles = DefaultOperandBundles;\n    }\n  };\n\n\n  //===--------------------------------------------------------------------===//\n  // Miscellaneous creation methods.\n  //===--------------------------------------------------------------------===//\n\n  /// Make a new global variable with initializer type i8*\n  ///\n  /// Make a new global variable with an initializer that has array of i8 type\n  /// filled in with the null terminated string value specified.  The new global\n  /// variable will be marked mergable with any others of the same contents.  If\n  /// Name is specified, it is the name of the global variable created.\n  ///\n  /// If no module is given via \\p M, it is take from the insertion point basic\n  /// block.\n  GlobalVariable *CreateGlobalString(StringRef Str, const Twine &Name = \"\",\n                                     unsigned AddressSpace = 0,\n                                     Module *M = nullptr);\n\n  /// Get a constant value representing either true or false.\n  ConstantInt *getInt1(bool V) {\n    return ConstantInt::get(getInt1Ty(), V);\n  }\n\n  /// Get the constant value for i1 true.\n  ConstantInt *getTrue() {\n    return ConstantInt::getTrue(Context);\n  }\n\n  /// Get the constant value for i1 false.\n  ConstantInt *getFalse() {\n    return ConstantInt::getFalse(Context);\n  }\n\n  /// Get a constant 8-bit value.\n  ConstantInt *getInt8(uint8_t C) {\n    return ConstantInt::get(getInt8Ty(), C);\n  }\n\n  /// Get a constant 16-bit value.\n  ConstantInt *getInt16(uint16_t C) {\n    return ConstantInt::get(getInt16Ty(), C);\n  }\n\n  /// Get a constant 32-bit value.\n  ConstantInt *getInt32(uint32_t C) {\n    return ConstantInt::get(getInt32Ty(), C);\n  }\n\n  /// Get a constant 64-bit value.\n  ConstantInt *getInt64(uint64_t C) {\n    return ConstantInt::get(getInt64Ty(), C);\n  }\n\n  /// Get a constant N-bit value, zero extended or truncated from\n  /// a 64-bit value.\n  ConstantInt *getIntN(unsigned N, uint64_t C) {\n    return ConstantInt::get(getIntNTy(N), C);\n  }\n\n  /// Get a constant integer value.\n  ConstantInt *getInt(const APInt &AI) {\n    return ConstantInt::get(Context, AI);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Type creation methods\n  //===--------------------------------------------------------------------===//\n\n  /// Fetch the type representing a single bit\n  IntegerType *getInt1Ty() {\n    return Type::getInt1Ty(Context);\n  }\n\n  /// Fetch the type representing an 8-bit integer.\n  IntegerType *getInt8Ty() {\n    return Type::getInt8Ty(Context);\n  }\n\n  /// Fetch the type representing a 16-bit integer.\n  IntegerType *getInt16Ty() {\n    return Type::getInt16Ty(Context);\n  }\n\n  /// Fetch the type representing a 32-bit integer.\n  IntegerType *getInt32Ty() {\n    return Type::getInt32Ty(Context);\n  }\n\n  /// Fetch the type representing a 64-bit integer.\n  IntegerType *getInt64Ty() {\n    return Type::getInt64Ty(Context);\n  }\n\n  /// Fetch the type representing a 128-bit integer.\n  IntegerType *getInt128Ty() { return Type::getInt128Ty(Context); }\n\n  /// Fetch the type representing an N-bit integer.\n  IntegerType *getIntNTy(unsigned N) {\n    return Type::getIntNTy(Context, N);\n  }\n\n  /// Fetch the type representing a 16-bit floating point value.\n  Type *getHalfTy() {\n    return Type::getHalfTy(Context);\n  }\n\n  /// Fetch the type representing a 16-bit brain floating point value.\n  Type *getBFloatTy() {\n    return Type::getBFloatTy(Context);\n  }\n\n  /// Fetch the type representing a 32-bit floating point value.\n  Type *getFloatTy() {\n    return Type::getFloatTy(Context);\n  }\n\n  /// Fetch the type representing a 64-bit floating point value.\n  Type *getDoubleTy() {\n    return Type::getDoubleTy(Context);\n  }\n\n  /// Fetch the type representing void.\n  Type *getVoidTy() {\n    return Type::getVoidTy(Context);\n  }\n\n  /// Fetch the type representing a pointer to an 8-bit integer value.\n  PointerType *getInt8PtrTy(unsigned AddrSpace = 0) {\n    return Type::getInt8PtrTy(Context, AddrSpace);\n  }\n\n  /// Fetch the type representing a pointer to an integer value.\n  IntegerType *getIntPtrTy(const DataLayout &DL, unsigned AddrSpace = 0) {\n    return DL.getIntPtrType(Context, AddrSpace);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Intrinsic creation methods\n  //===--------------------------------------------------------------------===//\n\n  /// Create and insert a memset to the specified pointer and the\n  /// specified value.\n  ///\n  /// If the pointer isn't an i8*, it will be converted. If a TBAA tag is\n  /// specified, it will be added to the instruction. Likewise with alias.scope\n  /// and noalias tags.\n  CallInst *CreateMemSet(Value *Ptr, Value *Val, uint64_t Size,\n                         MaybeAlign Align, bool isVolatile = false,\n                         MDNode *TBAATag = nullptr, MDNode *ScopeTag = nullptr,\n                         MDNode *NoAliasTag = nullptr) {\n    return CreateMemSet(Ptr, Val, getInt64(Size), Align, isVolatile,\n                        TBAATag, ScopeTag, NoAliasTag);\n  }\n\n  CallInst *CreateMemSet(Value *Ptr, Value *Val, Value *Size, MaybeAlign Align,\n                         bool isVolatile = false, MDNode *TBAATag = nullptr,\n                         MDNode *ScopeTag = nullptr,\n                         MDNode *NoAliasTag = nullptr);\n\n  /// Create and insert an element unordered-atomic memset of the region of\n  /// memory starting at the given pointer to the given value.\n  ///\n  /// If the pointer isn't an i8*, it will be converted. If a TBAA tag is\n  /// specified, it will be added to the instruction. Likewise with alias.scope\n  /// and noalias tags.\n  CallInst *CreateElementUnorderedAtomicMemSet(Value *Ptr, Value *Val,\n                                               uint64_t Size, Align Alignment,\n                                               uint32_t ElementSize,\n                                               MDNode *TBAATag = nullptr,\n                                               MDNode *ScopeTag = nullptr,\n                                               MDNode *NoAliasTag = nullptr) {\n    return CreateElementUnorderedAtomicMemSet(Ptr, Val, getInt64(Size),\n                                              Align(Alignment), ElementSize,\n                                              TBAATag, ScopeTag, NoAliasTag);\n  }\n\n  CallInst *CreateElementUnorderedAtomicMemSet(Value *Ptr, Value *Val,\n                                               Value *Size, Align Alignment,\n                                               uint32_t ElementSize,\n                                               MDNode *TBAATag = nullptr,\n                                               MDNode *ScopeTag = nullptr,\n                                               MDNode *NoAliasTag = nullptr);\n\n  /// Create and insert a memcpy between the specified pointers.\n  ///\n  /// If the pointers aren't i8*, they will be converted.  If a TBAA tag is\n  /// specified, it will be added to the instruction. Likewise with alias.scope\n  /// and noalias tags.\n  CallInst *CreateMemCpy(Value *Dst, MaybeAlign DstAlign, Value *Src,\n                         MaybeAlign SrcAlign, uint64_t Size,\n                         bool isVolatile = false, MDNode *TBAATag = nullptr,\n                         MDNode *TBAAStructTag = nullptr,\n                         MDNode *ScopeTag = nullptr,\n                         MDNode *NoAliasTag = nullptr) {\n    return CreateMemCpy(Dst, DstAlign, Src, SrcAlign, getInt64(Size),\n                        isVolatile, TBAATag, TBAAStructTag, ScopeTag,\n                        NoAliasTag);\n  }\n\n  CallInst *CreateMemTransferInst(\n      Intrinsic::ID IntrID, Value *Dst, MaybeAlign DstAlign, Value *Src,\n      MaybeAlign SrcAlign, Value *Size, bool isVolatile = false,\n      MDNode *TBAATag = nullptr, MDNode *TBAAStructTag = nullptr,\n      MDNode *ScopeTag = nullptr, MDNode *NoAliasTag = nullptr);\n\n  CallInst *CreateMemCpy(Value *Dst, MaybeAlign DstAlign, Value *Src,\n                         MaybeAlign SrcAlign, Value *Size,\n                         bool isVolatile = false, MDNode *TBAATag = nullptr,\n                         MDNode *TBAAStructTag = nullptr,\n                         MDNode *ScopeTag = nullptr,\n                         MDNode *NoAliasTag = nullptr) {\n    return CreateMemTransferInst(Intrinsic::memcpy, Dst, DstAlign, Src,\n                                 SrcAlign, Size, isVolatile, TBAATag,\n                                 TBAAStructTag, ScopeTag, NoAliasTag);\n  }\n\n  CallInst *CreateMemCpyInline(Value *Dst, MaybeAlign DstAlign, Value *Src,\n                               MaybeAlign SrcAlign, Value *Size);\n\n  /// Create and insert an element unordered-atomic memcpy between the\n  /// specified pointers.\n  ///\n  /// DstAlign/SrcAlign are the alignments of the Dst/Src pointers, respectively.\n  ///\n  /// If the pointers aren't i8*, they will be converted.  If a TBAA tag is\n  /// specified, it will be added to the instruction. Likewise with alias.scope\n  /// and noalias tags.\n  CallInst *CreateElementUnorderedAtomicMemCpy(\n      Value *Dst, Align DstAlign, Value *Src, Align SrcAlign, Value *Size,\n      uint32_t ElementSize, MDNode *TBAATag = nullptr,\n      MDNode *TBAAStructTag = nullptr, MDNode *ScopeTag = nullptr,\n      MDNode *NoAliasTag = nullptr);\n\n  CallInst *CreateMemMove(Value *Dst, MaybeAlign DstAlign, Value *Src,\n                          MaybeAlign SrcAlign, uint64_t Size,\n                          bool isVolatile = false, MDNode *TBAATag = nullptr,\n                          MDNode *ScopeTag = nullptr,\n                          MDNode *NoAliasTag = nullptr) {\n    return CreateMemMove(Dst, DstAlign, Src, SrcAlign, getInt64(Size),\n                         isVolatile, TBAATag, ScopeTag, NoAliasTag);\n  }\n\n  CallInst *CreateMemMove(Value *Dst, MaybeAlign DstAlign, Value *Src,\n                          MaybeAlign SrcAlign, Value *Size,\n                          bool isVolatile = false, MDNode *TBAATag = nullptr,\n                          MDNode *ScopeTag = nullptr,\n                          MDNode *NoAliasTag = nullptr);\n\n  /// \\brief Create and insert an element unordered-atomic memmove between the\n  /// specified pointers.\n  ///\n  /// DstAlign/SrcAlign are the alignments of the Dst/Src pointers,\n  /// respectively.\n  ///\n  /// If the pointers aren't i8*, they will be converted.  If a TBAA tag is\n  /// specified, it will be added to the instruction. Likewise with alias.scope\n  /// and noalias tags.\n  CallInst *CreateElementUnorderedAtomicMemMove(\n      Value *Dst, Align DstAlign, Value *Src, Align SrcAlign, Value *Size,\n      uint32_t ElementSize, MDNode *TBAATag = nullptr,\n      MDNode *TBAAStructTag = nullptr, MDNode *ScopeTag = nullptr,\n      MDNode *NoAliasTag = nullptr);\n\n  /// Create a vector fadd reduction intrinsic of the source vector.\n  /// The first parameter is a scalar accumulator value for ordered reductions.\n  CallInst *CreateFAddReduce(Value *Acc, Value *Src);\n\n  /// Create a vector fmul reduction intrinsic of the source vector.\n  /// The first parameter is a scalar accumulator value for ordered reductions.\n  CallInst *CreateFMulReduce(Value *Acc, Value *Src);\n\n  /// Create a vector int add reduction intrinsic of the source vector.\n  CallInst *CreateAddReduce(Value *Src);\n\n  /// Create a vector int mul reduction intrinsic of the source vector.\n  CallInst *CreateMulReduce(Value *Src);\n\n  /// Create a vector int AND reduction intrinsic of the source vector.\n  CallInst *CreateAndReduce(Value *Src);\n\n  /// Create a vector int OR reduction intrinsic of the source vector.\n  CallInst *CreateOrReduce(Value *Src);\n\n  /// Create a vector int XOR reduction intrinsic of the source vector.\n  CallInst *CreateXorReduce(Value *Src);\n\n  /// Create a vector integer max reduction intrinsic of the source\n  /// vector.\n  CallInst *CreateIntMaxReduce(Value *Src, bool IsSigned = false);\n\n  /// Create a vector integer min reduction intrinsic of the source\n  /// vector.\n  CallInst *CreateIntMinReduce(Value *Src, bool IsSigned = false);\n\n  /// Create a vector float max reduction intrinsic of the source\n  /// vector.\n  CallInst *CreateFPMaxReduce(Value *Src);\n\n  /// Create a vector float min reduction intrinsic of the source\n  /// vector.\n  CallInst *CreateFPMinReduce(Value *Src);\n\n  /// Create a lifetime.start intrinsic.\n  ///\n  /// If the pointer isn't i8* it will be converted.\n  CallInst *CreateLifetimeStart(Value *Ptr, ConstantInt *Size = nullptr);\n\n  /// Create a lifetime.end intrinsic.\n  ///\n  /// If the pointer isn't i8* it will be converted.\n  CallInst *CreateLifetimeEnd(Value *Ptr, ConstantInt *Size = nullptr);\n\n  /// Create a call to invariant.start intrinsic.\n  ///\n  /// If the pointer isn't i8* it will be converted.\n  CallInst *CreateInvariantStart(Value *Ptr, ConstantInt *Size = nullptr);\n\n  /// Create a call to Masked Load intrinsic\n  CallInst *CreateMaskedLoad(Value *Ptr, Align Alignment, Value *Mask,\n                             Value *PassThru = nullptr, const Twine &Name = \"\");\n\n  /// Create a call to Masked Store intrinsic\n  CallInst *CreateMaskedStore(Value *Val, Value *Ptr, Align Alignment,\n                              Value *Mask);\n\n  /// Create a call to Masked Gather intrinsic\n  CallInst *CreateMaskedGather(Value *Ptrs, Align Alignment,\n                               Value *Mask = nullptr, Value *PassThru = nullptr,\n                               const Twine &Name = \"\");\n\n  /// Create a call to Masked Scatter intrinsic\n  CallInst *CreateMaskedScatter(Value *Val, Value *Ptrs, Align Alignment,\n                                Value *Mask = nullptr);\n\n  /// Create an assume intrinsic call that allows the optimizer to\n  /// assume that the provided condition will be true.\n  ///\n  /// The optional argument \\p OpBundles specifies operand bundles that are\n  /// added to the call instruction.\n  CallInst *CreateAssumption(Value *Cond,\n                             ArrayRef<OperandBundleDef> OpBundles = llvm::None);\n\n  /// Create a llvm.experimental.noalias.scope.decl intrinsic call.\n  Instruction *CreateNoAliasScopeDeclaration(Value *Scope);\n  Instruction *CreateNoAliasScopeDeclaration(MDNode *ScopeTag) {\n    return CreateNoAliasScopeDeclaration(\n        MetadataAsValue::get(Context, ScopeTag));\n  }\n\n  /// Create a call to the experimental.gc.statepoint intrinsic to\n  /// start a new statepoint sequence.\n  CallInst *CreateGCStatepointCall(uint64_t ID, uint32_t NumPatchBytes,\n                                   Value *ActualCallee,\n                                   ArrayRef<Value *> CallArgs,\n                                   Optional<ArrayRef<Value *>> DeoptArgs,\n                                   ArrayRef<Value *> GCArgs,\n                                   const Twine &Name = \"\");\n\n  /// Create a call to the experimental.gc.statepoint intrinsic to\n  /// start a new statepoint sequence.\n  CallInst *CreateGCStatepointCall(uint64_t ID, uint32_t NumPatchBytes,\n                                   Value *ActualCallee, uint32_t Flags,\n                                   ArrayRef<Value *> CallArgs,\n                                   Optional<ArrayRef<Use>> TransitionArgs,\n                                   Optional<ArrayRef<Use>> DeoptArgs,\n                                   ArrayRef<Value *> GCArgs,\n                                   const Twine &Name = \"\");\n\n  /// Conveninence function for the common case when CallArgs are filled\n  /// in using makeArrayRef(CS.arg_begin(), CS.arg_end()); Use needs to be\n  /// .get()'ed to get the Value pointer.\n  CallInst *CreateGCStatepointCall(uint64_t ID, uint32_t NumPatchBytes,\n                                   Value *ActualCallee, ArrayRef<Use> CallArgs,\n                                   Optional<ArrayRef<Value *>> DeoptArgs,\n                                   ArrayRef<Value *> GCArgs,\n                                   const Twine &Name = \"\");\n\n  /// Create an invoke to the experimental.gc.statepoint intrinsic to\n  /// start a new statepoint sequence.\n  InvokeInst *\n  CreateGCStatepointInvoke(uint64_t ID, uint32_t NumPatchBytes,\n                           Value *ActualInvokee, BasicBlock *NormalDest,\n                           BasicBlock *UnwindDest, ArrayRef<Value *> InvokeArgs,\n                           Optional<ArrayRef<Value *>> DeoptArgs,\n                           ArrayRef<Value *> GCArgs, const Twine &Name = \"\");\n\n  /// Create an invoke to the experimental.gc.statepoint intrinsic to\n  /// start a new statepoint sequence.\n  InvokeInst *CreateGCStatepointInvoke(\n      uint64_t ID, uint32_t NumPatchBytes, Value *ActualInvokee,\n      BasicBlock *NormalDest, BasicBlock *UnwindDest, uint32_t Flags,\n      ArrayRef<Value *> InvokeArgs, Optional<ArrayRef<Use>> TransitionArgs,\n      Optional<ArrayRef<Use>> DeoptArgs, ArrayRef<Value *> GCArgs,\n      const Twine &Name = \"\");\n\n  // Convenience function for the common case when CallArgs are filled in using\n  // makeArrayRef(CS.arg_begin(), CS.arg_end()); Use needs to be .get()'ed to\n  // get the Value *.\n  InvokeInst *\n  CreateGCStatepointInvoke(uint64_t ID, uint32_t NumPatchBytes,\n                           Value *ActualInvokee, BasicBlock *NormalDest,\n                           BasicBlock *UnwindDest, ArrayRef<Use> InvokeArgs,\n                           Optional<ArrayRef<Value *>> DeoptArgs,\n                           ArrayRef<Value *> GCArgs, const Twine &Name = \"\");\n\n  /// Create a call to the experimental.gc.result intrinsic to extract\n  /// the result from a call wrapped in a statepoint.\n  CallInst *CreateGCResult(Instruction *Statepoint,\n                           Type *ResultType,\n                           const Twine &Name = \"\");\n\n  /// Create a call to the experimental.gc.relocate intrinsics to\n  /// project the relocated value of one pointer from the statepoint.\n  CallInst *CreateGCRelocate(Instruction *Statepoint,\n                             int BaseOffset,\n                             int DerivedOffset,\n                             Type *ResultType,\n                             const Twine &Name = \"\");\n\n  /// Create a call to llvm.vscale, multiplied by \\p Scaling. The type of VScale\n  /// will be the same type as that of \\p Scaling.\n  Value *CreateVScale(Constant *Scaling, const Twine &Name = \"\");\n\n  /// Create a call to intrinsic \\p ID with 1 operand which is mangled on its\n  /// type.\n  CallInst *CreateUnaryIntrinsic(Intrinsic::ID ID, Value *V,\n                                 Instruction *FMFSource = nullptr,\n                                 const Twine &Name = \"\");\n\n  /// Create a call to intrinsic \\p ID with 2 operands which is mangled on the\n  /// first type.\n  CallInst *CreateBinaryIntrinsic(Intrinsic::ID ID, Value *LHS, Value *RHS,\n                                  Instruction *FMFSource = nullptr,\n                                  const Twine &Name = \"\");\n\n  /// Create a call to intrinsic \\p ID with \\p args, mangled using \\p Types. If\n  /// \\p FMFSource is provided, copy fast-math-flags from that instruction to\n  /// the intrinsic.\n  CallInst *CreateIntrinsic(Intrinsic::ID ID, ArrayRef<Type *> Types,\n                            ArrayRef<Value *> Args,\n                            Instruction *FMFSource = nullptr,\n                            const Twine &Name = \"\");\n\n  /// Create call to the minnum intrinsic.\n  CallInst *CreateMinNum(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateBinaryIntrinsic(Intrinsic::minnum, LHS, RHS, nullptr, Name);\n  }\n\n  /// Create call to the maxnum intrinsic.\n  CallInst *CreateMaxNum(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateBinaryIntrinsic(Intrinsic::maxnum, LHS, RHS, nullptr, Name);\n  }\n\n  /// Create call to the minimum intrinsic.\n  CallInst *CreateMinimum(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateBinaryIntrinsic(Intrinsic::minimum, LHS, RHS, nullptr, Name);\n  }\n\n  /// Create call to the maximum intrinsic.\n  CallInst *CreateMaximum(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateBinaryIntrinsic(Intrinsic::maximum, LHS, RHS, nullptr, Name);\n  }\n\n  /// Create a call to the experimental.vector.extract intrinsic.\n  CallInst *CreateExtractVector(Type *DstType, Value *SrcVec, Value *Idx,\n                                const Twine &Name = \"\") {\n    return CreateIntrinsic(Intrinsic::experimental_vector_extract,\n                           {DstType, SrcVec->getType()}, {SrcVec, Idx}, nullptr,\n                           Name);\n  }\n\n  /// Create a call to the experimental.vector.insert intrinsic.\n  CallInst *CreateInsertVector(Type *DstType, Value *SrcVec, Value *SubVec,\n                               Value *Idx, const Twine &Name = \"\") {\n    return CreateIntrinsic(Intrinsic::experimental_vector_insert,\n                           {DstType, SubVec->getType()}, {SrcVec, SubVec, Idx},\n                           nullptr, Name);\n  }\n\nprivate:\n  /// Create a call to a masked intrinsic with given Id.\n  CallInst *CreateMaskedIntrinsic(Intrinsic::ID Id, ArrayRef<Value *> Ops,\n                                  ArrayRef<Type *> OverloadedTypes,\n                                  const Twine &Name = \"\");\n\n  Value *getCastedInt8PtrValue(Value *Ptr);\n\n  //===--------------------------------------------------------------------===//\n  // Instruction creation methods: Terminators\n  //===--------------------------------------------------------------------===//\n\nprivate:\n  /// Helper to add branch weight and unpredictable metadata onto an\n  /// instruction.\n  /// \\returns The annotated instruction.\n  template <typename InstTy>\n  InstTy *addBranchMetadata(InstTy *I, MDNode *Weights, MDNode *Unpredictable) {\n    if (Weights)\n      I->setMetadata(LLVMContext::MD_prof, Weights);\n    if (Unpredictable)\n      I->setMetadata(LLVMContext::MD_unpredictable, Unpredictable);\n    return I;\n  }\n\npublic:\n  /// Create a 'ret void' instruction.\n  ReturnInst *CreateRetVoid() {\n    return Insert(ReturnInst::Create(Context));\n  }\n\n  /// Create a 'ret <val>' instruction.\n  ReturnInst *CreateRet(Value *V) {\n    return Insert(ReturnInst::Create(Context, V));\n  }\n\n  /// Create a sequence of N insertvalue instructions,\n  /// with one Value from the retVals array each, that build a aggregate\n  /// return value one value at a time, and a ret instruction to return\n  /// the resulting aggregate value.\n  ///\n  /// This is a convenience function for code that uses aggregate return values\n  /// as a vehicle for having multiple return values.\n  ReturnInst *CreateAggregateRet(Value *const *retVals, unsigned N) {\n    Value *V = UndefValue::get(getCurrentFunctionReturnType());\n    for (unsigned i = 0; i != N; ++i)\n      V = CreateInsertValue(V, retVals[i], i, \"mrv\");\n    return Insert(ReturnInst::Create(Context, V));\n  }\n\n  /// Create an unconditional 'br label X' instruction.\n  BranchInst *CreateBr(BasicBlock *Dest) {\n    return Insert(BranchInst::Create(Dest));\n  }\n\n  /// Create a conditional 'br Cond, TrueDest, FalseDest'\n  /// instruction.\n  BranchInst *CreateCondBr(Value *Cond, BasicBlock *True, BasicBlock *False,\n                           MDNode *BranchWeights = nullptr,\n                           MDNode *Unpredictable = nullptr) {\n    return Insert(addBranchMetadata(BranchInst::Create(True, False, Cond),\n                                    BranchWeights, Unpredictable));\n  }\n\n  /// Create a conditional 'br Cond, TrueDest, FalseDest'\n  /// instruction. Copy branch meta data if available.\n  BranchInst *CreateCondBr(Value *Cond, BasicBlock *True, BasicBlock *False,\n                           Instruction *MDSrc) {\n    BranchInst *Br = BranchInst::Create(True, False, Cond);\n    if (MDSrc) {\n      unsigned WL[4] = {LLVMContext::MD_prof, LLVMContext::MD_unpredictable,\n                        LLVMContext::MD_make_implicit, LLVMContext::MD_dbg};\n      Br->copyMetadata(*MDSrc, makeArrayRef(&WL[0], 4));\n    }\n    return Insert(Br);\n  }\n\n  /// Create a switch instruction with the specified value, default dest,\n  /// and with a hint for the number of cases that will be added (for efficient\n  /// allocation).\n  SwitchInst *CreateSwitch(Value *V, BasicBlock *Dest, unsigned NumCases = 10,\n                           MDNode *BranchWeights = nullptr,\n                           MDNode *Unpredictable = nullptr) {\n    return Insert(addBranchMetadata(SwitchInst::Create(V, Dest, NumCases),\n                                    BranchWeights, Unpredictable));\n  }\n\n  /// Create an indirect branch instruction with the specified address\n  /// operand, with an optional hint for the number of destinations that will be\n  /// added (for efficient allocation).\n  IndirectBrInst *CreateIndirectBr(Value *Addr, unsigned NumDests = 10) {\n    return Insert(IndirectBrInst::Create(Addr, NumDests));\n  }\n\n  /// Create an invoke instruction.\n  InvokeInst *CreateInvoke(FunctionType *Ty, Value *Callee,\n                           BasicBlock *NormalDest, BasicBlock *UnwindDest,\n                           ArrayRef<Value *> Args,\n                           ArrayRef<OperandBundleDef> OpBundles,\n                           const Twine &Name = \"\") {\n    InvokeInst *II =\n        InvokeInst::Create(Ty, Callee, NormalDest, UnwindDest, Args, OpBundles);\n    if (IsFPConstrained)\n      setConstrainedFPCallAttr(II);\n    return Insert(II, Name);\n  }\n  InvokeInst *CreateInvoke(FunctionType *Ty, Value *Callee,\n                           BasicBlock *NormalDest, BasicBlock *UnwindDest,\n                           ArrayRef<Value *> Args = None,\n                           const Twine &Name = \"\") {\n    InvokeInst *II =\n        InvokeInst::Create(Ty, Callee, NormalDest, UnwindDest, Args);\n    if (IsFPConstrained)\n      setConstrainedFPCallAttr(II);\n    return Insert(II, Name);\n  }\n\n  InvokeInst *CreateInvoke(FunctionCallee Callee, BasicBlock *NormalDest,\n                           BasicBlock *UnwindDest, ArrayRef<Value *> Args,\n                           ArrayRef<OperandBundleDef> OpBundles,\n                           const Twine &Name = \"\") {\n    return CreateInvoke(Callee.getFunctionType(), Callee.getCallee(),\n                        NormalDest, UnwindDest, Args, OpBundles, Name);\n  }\n\n  InvokeInst *CreateInvoke(FunctionCallee Callee, BasicBlock *NormalDest,\n                           BasicBlock *UnwindDest,\n                           ArrayRef<Value *> Args = None,\n                           const Twine &Name = \"\") {\n    return CreateInvoke(Callee.getFunctionType(), Callee.getCallee(),\n                        NormalDest, UnwindDest, Args, Name);\n  }\n\n  /// \\brief Create a callbr instruction.\n  CallBrInst *CreateCallBr(FunctionType *Ty, Value *Callee,\n                           BasicBlock *DefaultDest,\n                           ArrayRef<BasicBlock *> IndirectDests,\n                           ArrayRef<Value *> Args = None,\n                           const Twine &Name = \"\") {\n    return Insert(CallBrInst::Create(Ty, Callee, DefaultDest, IndirectDests,\n                                     Args), Name);\n  }\n  CallBrInst *CreateCallBr(FunctionType *Ty, Value *Callee,\n                           BasicBlock *DefaultDest,\n                           ArrayRef<BasicBlock *> IndirectDests,\n                           ArrayRef<Value *> Args,\n                           ArrayRef<OperandBundleDef> OpBundles,\n                           const Twine &Name = \"\") {\n    return Insert(\n        CallBrInst::Create(Ty, Callee, DefaultDest, IndirectDests, Args,\n                           OpBundles), Name);\n  }\n\n  CallBrInst *CreateCallBr(FunctionCallee Callee, BasicBlock *DefaultDest,\n                           ArrayRef<BasicBlock *> IndirectDests,\n                           ArrayRef<Value *> Args = None,\n                           const Twine &Name = \"\") {\n    return CreateCallBr(Callee.getFunctionType(), Callee.getCallee(),\n                        DefaultDest, IndirectDests, Args, Name);\n  }\n  CallBrInst *CreateCallBr(FunctionCallee Callee, BasicBlock *DefaultDest,\n                           ArrayRef<BasicBlock *> IndirectDests,\n                           ArrayRef<Value *> Args,\n                           ArrayRef<OperandBundleDef> OpBundles,\n                           const Twine &Name = \"\") {\n    return CreateCallBr(Callee.getFunctionType(), Callee.getCallee(),\n                        DefaultDest, IndirectDests, Args, Name);\n  }\n\n  ResumeInst *CreateResume(Value *Exn) {\n    return Insert(ResumeInst::Create(Exn));\n  }\n\n  CleanupReturnInst *CreateCleanupRet(CleanupPadInst *CleanupPad,\n                                      BasicBlock *UnwindBB = nullptr) {\n    return Insert(CleanupReturnInst::Create(CleanupPad, UnwindBB));\n  }\n\n  CatchSwitchInst *CreateCatchSwitch(Value *ParentPad, BasicBlock *UnwindBB,\n                                     unsigned NumHandlers,\n                                     const Twine &Name = \"\") {\n    return Insert(CatchSwitchInst::Create(ParentPad, UnwindBB, NumHandlers),\n                  Name);\n  }\n\n  CatchPadInst *CreateCatchPad(Value *ParentPad, ArrayRef<Value *> Args,\n                               const Twine &Name = \"\") {\n    return Insert(CatchPadInst::Create(ParentPad, Args), Name);\n  }\n\n  CleanupPadInst *CreateCleanupPad(Value *ParentPad,\n                                   ArrayRef<Value *> Args = None,\n                                   const Twine &Name = \"\") {\n    return Insert(CleanupPadInst::Create(ParentPad, Args), Name);\n  }\n\n  CatchReturnInst *CreateCatchRet(CatchPadInst *CatchPad, BasicBlock *BB) {\n    return Insert(CatchReturnInst::Create(CatchPad, BB));\n  }\n\n  UnreachableInst *CreateUnreachable() {\n    return Insert(new UnreachableInst(Context));\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Instruction creation methods: Binary Operators\n  //===--------------------------------------------------------------------===//\nprivate:\n  BinaryOperator *CreateInsertNUWNSWBinOp(BinaryOperator::BinaryOps Opc,\n                                          Value *LHS, Value *RHS,\n                                          const Twine &Name,\n                                          bool HasNUW, bool HasNSW) {\n    BinaryOperator *BO = Insert(BinaryOperator::Create(Opc, LHS, RHS), Name);\n    if (HasNUW) BO->setHasNoUnsignedWrap();\n    if (HasNSW) BO->setHasNoSignedWrap();\n    return BO;\n  }\n\n  Instruction *setFPAttrs(Instruction *I, MDNode *FPMD,\n                          FastMathFlags FMF) const {\n    if (!FPMD)\n      FPMD = DefaultFPMathTag;\n    if (FPMD)\n      I->setMetadata(LLVMContext::MD_fpmath, FPMD);\n    I->setFastMathFlags(FMF);\n    return I;\n  }\n\n  Value *foldConstant(Instruction::BinaryOps Opc, Value *L,\n                      Value *R, const Twine &Name) const {\n    auto *LC = dyn_cast<Constant>(L);\n    auto *RC = dyn_cast<Constant>(R);\n    return (LC && RC) ? Insert(Folder.CreateBinOp(Opc, LC, RC), Name) : nullptr;\n  }\n\n  Value *getConstrainedFPRounding(Optional<RoundingMode> Rounding) {\n    RoundingMode UseRounding = DefaultConstrainedRounding;\n\n    if (Rounding.hasValue())\n      UseRounding = Rounding.getValue();\n\n    Optional<StringRef> RoundingStr = RoundingModeToStr(UseRounding);\n    assert(RoundingStr.hasValue() && \"Garbage strict rounding mode!\");\n    auto *RoundingMDS = MDString::get(Context, RoundingStr.getValue());\n\n    return MetadataAsValue::get(Context, RoundingMDS);\n  }\n\n  Value *getConstrainedFPExcept(Optional<fp::ExceptionBehavior> Except) {\n    fp::ExceptionBehavior UseExcept = DefaultConstrainedExcept;\n\n    if (Except.hasValue())\n      UseExcept = Except.getValue();\n\n    Optional<StringRef> ExceptStr = ExceptionBehaviorToStr(UseExcept);\n    assert(ExceptStr.hasValue() && \"Garbage strict exception behavior!\");\n    auto *ExceptMDS = MDString::get(Context, ExceptStr.getValue());\n\n    return MetadataAsValue::get(Context, ExceptMDS);\n  }\n\n  Value *getConstrainedFPPredicate(CmpInst::Predicate Predicate) {\n    assert(CmpInst::isFPPredicate(Predicate) &&\n           Predicate != CmpInst::FCMP_FALSE &&\n           Predicate != CmpInst::FCMP_TRUE &&\n           \"Invalid constrained FP comparison predicate!\");\n\n    StringRef PredicateStr = CmpInst::getPredicateName(Predicate);\n    auto *PredicateMDS = MDString::get(Context, PredicateStr);\n\n    return MetadataAsValue::get(Context, PredicateMDS);\n  }\n\npublic:\n  Value *CreateAdd(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                   bool HasNUW = false, bool HasNSW = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateAdd(LC, RC, HasNUW, HasNSW), Name);\n    return CreateInsertNUWNSWBinOp(Instruction::Add, LHS, RHS, Name,\n                                   HasNUW, HasNSW);\n  }\n\n  Value *CreateNSWAdd(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateAdd(LHS, RHS, Name, false, true);\n  }\n\n  Value *CreateNUWAdd(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateAdd(LHS, RHS, Name, true, false);\n  }\n\n  Value *CreateSub(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                   bool HasNUW = false, bool HasNSW = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateSub(LC, RC, HasNUW, HasNSW), Name);\n    return CreateInsertNUWNSWBinOp(Instruction::Sub, LHS, RHS, Name,\n                                   HasNUW, HasNSW);\n  }\n\n  Value *CreateNSWSub(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateSub(LHS, RHS, Name, false, true);\n  }\n\n  Value *CreateNUWSub(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateSub(LHS, RHS, Name, true, false);\n  }\n\n  Value *CreateMul(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                   bool HasNUW = false, bool HasNSW = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateMul(LC, RC, HasNUW, HasNSW), Name);\n    return CreateInsertNUWNSWBinOp(Instruction::Mul, LHS, RHS, Name,\n                                   HasNUW, HasNSW);\n  }\n\n  Value *CreateNSWMul(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateMul(LHS, RHS, Name, false, true);\n  }\n\n  Value *CreateNUWMul(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateMul(LHS, RHS, Name, true, false);\n  }\n\n  Value *CreateUDiv(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateUDiv(LC, RC, isExact), Name);\n    if (!isExact)\n      return Insert(BinaryOperator::CreateUDiv(LHS, RHS), Name);\n    return Insert(BinaryOperator::CreateExactUDiv(LHS, RHS), Name);\n  }\n\n  Value *CreateExactUDiv(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateUDiv(LHS, RHS, Name, true);\n  }\n\n  Value *CreateSDiv(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateSDiv(LC, RC, isExact), Name);\n    if (!isExact)\n      return Insert(BinaryOperator::CreateSDiv(LHS, RHS), Name);\n    return Insert(BinaryOperator::CreateExactSDiv(LHS, RHS), Name);\n  }\n\n  Value *CreateExactSDiv(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateSDiv(LHS, RHS, Name, true);\n  }\n\n  Value *CreateURem(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    if (Value *V = foldConstant(Instruction::URem, LHS, RHS, Name)) return V;\n    return Insert(BinaryOperator::CreateURem(LHS, RHS), Name);\n  }\n\n  Value *CreateSRem(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    if (Value *V = foldConstant(Instruction::SRem, LHS, RHS, Name)) return V;\n    return Insert(BinaryOperator::CreateSRem(LHS, RHS), Name);\n  }\n\n  Value *CreateShl(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                   bool HasNUW = false, bool HasNSW = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateShl(LC, RC, HasNUW, HasNSW), Name);\n    return CreateInsertNUWNSWBinOp(Instruction::Shl, LHS, RHS, Name,\n                                   HasNUW, HasNSW);\n  }\n\n  Value *CreateShl(Value *LHS, const APInt &RHS, const Twine &Name = \"\",\n                   bool HasNUW = false, bool HasNSW = false) {\n    return CreateShl(LHS, ConstantInt::get(LHS->getType(), RHS), Name,\n                     HasNUW, HasNSW);\n  }\n\n  Value *CreateShl(Value *LHS, uint64_t RHS, const Twine &Name = \"\",\n                   bool HasNUW = false, bool HasNSW = false) {\n    return CreateShl(LHS, ConstantInt::get(LHS->getType(), RHS), Name,\n                     HasNUW, HasNSW);\n  }\n\n  Value *CreateLShr(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateLShr(LC, RC, isExact), Name);\n    if (!isExact)\n      return Insert(BinaryOperator::CreateLShr(LHS, RHS), Name);\n    return Insert(BinaryOperator::CreateExactLShr(LHS, RHS), Name);\n  }\n\n  Value *CreateLShr(Value *LHS, const APInt &RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    return CreateLShr(LHS, ConstantInt::get(LHS->getType(), RHS), Name,isExact);\n  }\n\n  Value *CreateLShr(Value *LHS, uint64_t RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    return CreateLShr(LHS, ConstantInt::get(LHS->getType(), RHS), Name,isExact);\n  }\n\n  Value *CreateAShr(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateAShr(LC, RC, isExact), Name);\n    if (!isExact)\n      return Insert(BinaryOperator::CreateAShr(LHS, RHS), Name);\n    return Insert(BinaryOperator::CreateExactAShr(LHS, RHS), Name);\n  }\n\n  Value *CreateAShr(Value *LHS, const APInt &RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    return CreateAShr(LHS, ConstantInt::get(LHS->getType(), RHS), Name,isExact);\n  }\n\n  Value *CreateAShr(Value *LHS, uint64_t RHS, const Twine &Name = \"\",\n                    bool isExact = false) {\n    return CreateAShr(LHS, ConstantInt::get(LHS->getType(), RHS), Name,isExact);\n  }\n\n  Value *CreateAnd(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    if (auto *RC = dyn_cast<Constant>(RHS)) {\n      if (isa<ConstantInt>(RC) && cast<ConstantInt>(RC)->isMinusOne())\n        return LHS;  // LHS & -1 -> LHS\n      if (auto *LC = dyn_cast<Constant>(LHS))\n        return Insert(Folder.CreateAnd(LC, RC), Name);\n    }\n    return Insert(BinaryOperator::CreateAnd(LHS, RHS), Name);\n  }\n\n  Value *CreateAnd(Value *LHS, const APInt &RHS, const Twine &Name = \"\") {\n    return CreateAnd(LHS, ConstantInt::get(LHS->getType(), RHS), Name);\n  }\n\n  Value *CreateAnd(Value *LHS, uint64_t RHS, const Twine &Name = \"\") {\n    return CreateAnd(LHS, ConstantInt::get(LHS->getType(), RHS), Name);\n  }\n\n  Value *CreateAnd(ArrayRef<Value*> Ops) {\n    assert(!Ops.empty());\n    Value *Accum = Ops[0];\n    for (unsigned i = 1; i < Ops.size(); i++)\n      Accum = CreateAnd(Accum, Ops[i]);\n    return Accum;\n  }\n\n  Value *CreateOr(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    if (auto *RC = dyn_cast<Constant>(RHS)) {\n      if (RC->isNullValue())\n        return LHS;  // LHS | 0 -> LHS\n      if (auto *LC = dyn_cast<Constant>(LHS))\n        return Insert(Folder.CreateOr(LC, RC), Name);\n    }\n    return Insert(BinaryOperator::CreateOr(LHS, RHS), Name);\n  }\n\n  Value *CreateOr(Value *LHS, const APInt &RHS, const Twine &Name = \"\") {\n    return CreateOr(LHS, ConstantInt::get(LHS->getType(), RHS), Name);\n  }\n\n  Value *CreateOr(Value *LHS, uint64_t RHS, const Twine &Name = \"\") {\n    return CreateOr(LHS, ConstantInt::get(LHS->getType(), RHS), Name);\n  }\n\n  Value *CreateOr(ArrayRef<Value*> Ops) {\n    assert(!Ops.empty());\n    Value *Accum = Ops[0];\n    for (unsigned i = 1; i < Ops.size(); i++)\n      Accum = CreateOr(Accum, Ops[i]);\n    return Accum;\n  }\n\n  Value *CreateXor(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    if (Value *V = foldConstant(Instruction::Xor, LHS, RHS, Name)) return V;\n    return Insert(BinaryOperator::CreateXor(LHS, RHS), Name);\n  }\n\n  Value *CreateXor(Value *LHS, const APInt &RHS, const Twine &Name = \"\") {\n    return CreateXor(LHS, ConstantInt::get(LHS->getType(), RHS), Name);\n  }\n\n  Value *CreateXor(Value *LHS, uint64_t RHS, const Twine &Name = \"\") {\n    return CreateXor(LHS, ConstantInt::get(LHS->getType(), RHS), Name);\n  }\n\n  Value *CreateFAdd(Value *L, Value *R, const Twine &Name = \"\",\n                    MDNode *FPMD = nullptr) {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fadd,\n                                      L, R, nullptr, Name, FPMD);\n\n    if (Value *V = foldConstant(Instruction::FAdd, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFAdd(L, R), FPMD, FMF);\n    return Insert(I, Name);\n  }\n\n  /// Copy fast-math-flags from an instruction rather than using the builder's\n  /// default FMF.\n  Value *CreateFAddFMF(Value *L, Value *R, Instruction *FMFSource,\n                       const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fadd,\n                                      L, R, FMFSource, Name);\n\n    if (Value *V = foldConstant(Instruction::FAdd, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFAdd(L, R), nullptr,\n                                FMFSource->getFastMathFlags());\n    return Insert(I, Name);\n  }\n\n  Value *CreateFSub(Value *L, Value *R, const Twine &Name = \"\",\n                    MDNode *FPMD = nullptr) {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fsub,\n                                      L, R, nullptr, Name, FPMD);\n\n    if (Value *V = foldConstant(Instruction::FSub, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFSub(L, R), FPMD, FMF);\n    return Insert(I, Name);\n  }\n\n  /// Copy fast-math-flags from an instruction rather than using the builder's\n  /// default FMF.\n  Value *CreateFSubFMF(Value *L, Value *R, Instruction *FMFSource,\n                       const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fsub,\n                                      L, R, FMFSource, Name);\n\n    if (Value *V = foldConstant(Instruction::FSub, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFSub(L, R), nullptr,\n                                FMFSource->getFastMathFlags());\n    return Insert(I, Name);\n  }\n\n  Value *CreateFMul(Value *L, Value *R, const Twine &Name = \"\",\n                    MDNode *FPMD = nullptr) {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fmul,\n                                      L, R, nullptr, Name, FPMD);\n\n    if (Value *V = foldConstant(Instruction::FMul, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFMul(L, R), FPMD, FMF);\n    return Insert(I, Name);\n  }\n\n  /// Copy fast-math-flags from an instruction rather than using the builder's\n  /// default FMF.\n  Value *CreateFMulFMF(Value *L, Value *R, Instruction *FMFSource,\n                       const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fmul,\n                                      L, R, FMFSource, Name);\n\n    if (Value *V = foldConstant(Instruction::FMul, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFMul(L, R), nullptr,\n                                FMFSource->getFastMathFlags());\n    return Insert(I, Name);\n  }\n\n  Value *CreateFDiv(Value *L, Value *R, const Twine &Name = \"\",\n                    MDNode *FPMD = nullptr) {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fdiv,\n                                      L, R, nullptr, Name, FPMD);\n\n    if (Value *V = foldConstant(Instruction::FDiv, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFDiv(L, R), FPMD, FMF);\n    return Insert(I, Name);\n  }\n\n  /// Copy fast-math-flags from an instruction rather than using the builder's\n  /// default FMF.\n  Value *CreateFDivFMF(Value *L, Value *R, Instruction *FMFSource,\n                       const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_fdiv,\n                                      L, R, FMFSource, Name);\n\n    if (Value *V = foldConstant(Instruction::FDiv, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFDiv(L, R), nullptr,\n                                FMFSource->getFastMathFlags());\n    return Insert(I, Name);\n  }\n\n  Value *CreateFRem(Value *L, Value *R, const Twine &Name = \"\",\n                    MDNode *FPMD = nullptr) {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_frem,\n                                      L, R, nullptr, Name, FPMD);\n\n    if (Value *V = foldConstant(Instruction::FRem, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFRem(L, R), FPMD, FMF);\n    return Insert(I, Name);\n  }\n\n  /// Copy fast-math-flags from an instruction rather than using the builder's\n  /// default FMF.\n  Value *CreateFRemFMF(Value *L, Value *R, Instruction *FMFSource,\n                       const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPBinOp(Intrinsic::experimental_constrained_frem,\n                                      L, R, FMFSource, Name);\n\n    if (Value *V = foldConstant(Instruction::FRem, L, R, Name)) return V;\n    Instruction *I = setFPAttrs(BinaryOperator::CreateFRem(L, R), nullptr,\n                                FMFSource->getFastMathFlags());\n    return Insert(I, Name);\n  }\n\n  Value *CreateBinOp(Instruction::BinaryOps Opc,\n                     Value *LHS, Value *RHS, const Twine &Name = \"\",\n                     MDNode *FPMathTag = nullptr) {\n    if (Value *V = foldConstant(Opc, LHS, RHS, Name)) return V;\n    Instruction *BinOp = BinaryOperator::Create(Opc, LHS, RHS);\n    if (isa<FPMathOperator>(BinOp))\n      setFPAttrs(BinOp, FPMathTag, FMF);\n    return Insert(BinOp, Name);\n  }\n\n  Value *CreateLogicalAnd(Value *Cond1, Value *Cond2, const Twine &Name = \"\") {\n    assert(Cond2->getType()->isIntOrIntVectorTy(1));\n    return CreateSelect(Cond1, Cond2,\n                        ConstantInt::getNullValue(Cond2->getType()), Name);\n  }\n\n  Value *CreateLogicalOr(Value *Cond1, Value *Cond2, const Twine &Name = \"\") {\n    assert(Cond2->getType()->isIntOrIntVectorTy(1));\n    return CreateSelect(Cond1, ConstantInt::getAllOnesValue(Cond2->getType()),\n                        Cond2, Name);\n  }\n\n  CallInst *CreateConstrainedFPBinOp(\n      Intrinsic::ID ID, Value *L, Value *R, Instruction *FMFSource = nullptr,\n      const Twine &Name = \"\", MDNode *FPMathTag = nullptr,\n      Optional<RoundingMode> Rounding = None,\n      Optional<fp::ExceptionBehavior> Except = None);\n\n  Value *CreateNeg(Value *V, const Twine &Name = \"\",\n                   bool HasNUW = false, bool HasNSW = false) {\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateNeg(VC, HasNUW, HasNSW), Name);\n    BinaryOperator *BO = Insert(BinaryOperator::CreateNeg(V), Name);\n    if (HasNUW) BO->setHasNoUnsignedWrap();\n    if (HasNSW) BO->setHasNoSignedWrap();\n    return BO;\n  }\n\n  Value *CreateNSWNeg(Value *V, const Twine &Name = \"\") {\n    return CreateNeg(V, Name, false, true);\n  }\n\n  Value *CreateNUWNeg(Value *V, const Twine &Name = \"\") {\n    return CreateNeg(V, Name, true, false);\n  }\n\n  Value *CreateFNeg(Value *V, const Twine &Name = \"\",\n                    MDNode *FPMathTag = nullptr) {\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateFNeg(VC), Name);\n    return Insert(setFPAttrs(UnaryOperator::CreateFNeg(V), FPMathTag, FMF),\n                  Name);\n  }\n\n  /// Copy fast-math-flags from an instruction rather than using the builder's\n  /// default FMF.\n  Value *CreateFNegFMF(Value *V, Instruction *FMFSource,\n                       const Twine &Name = \"\") {\n   if (auto *VC = dyn_cast<Constant>(V))\n     return Insert(Folder.CreateFNeg(VC), Name);\n   return Insert(setFPAttrs(UnaryOperator::CreateFNeg(V), nullptr,\n                            FMFSource->getFastMathFlags()),\n                 Name);\n  }\n\n  Value *CreateNot(Value *V, const Twine &Name = \"\") {\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateNot(VC), Name);\n    return Insert(BinaryOperator::CreateNot(V), Name);\n  }\n\n  Value *CreateUnOp(Instruction::UnaryOps Opc,\n                    Value *V, const Twine &Name = \"\",\n                    MDNode *FPMathTag = nullptr) {\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateUnOp(Opc, VC), Name);\n    Instruction *UnOp = UnaryOperator::Create(Opc, V);\n    if (isa<FPMathOperator>(UnOp))\n      setFPAttrs(UnOp, FPMathTag, FMF);\n    return Insert(UnOp, Name);\n  }\n\n  /// Create either a UnaryOperator or BinaryOperator depending on \\p Opc.\n  /// Correct number of operands must be passed accordingly.\n  Value *CreateNAryOp(unsigned Opc, ArrayRef<Value *> Ops,\n                      const Twine &Name = \"\", MDNode *FPMathTag = nullptr);\n\n  //===--------------------------------------------------------------------===//\n  // Instruction creation methods: Memory Instructions\n  //===--------------------------------------------------------------------===//\n\n  AllocaInst *CreateAlloca(Type *Ty, unsigned AddrSpace,\n                           Value *ArraySize = nullptr, const Twine &Name = \"\") {\n    const DataLayout &DL = BB->getModule()->getDataLayout();\n    Align AllocaAlign = DL.getPrefTypeAlign(Ty);\n    return Insert(new AllocaInst(Ty, AddrSpace, ArraySize, AllocaAlign), Name);\n  }\n\n  AllocaInst *CreateAlloca(Type *Ty, Value *ArraySize = nullptr,\n                           const Twine &Name = \"\") {\n    const DataLayout &DL = BB->getModule()->getDataLayout();\n    Align AllocaAlign = DL.getPrefTypeAlign(Ty);\n    unsigned AddrSpace = DL.getAllocaAddrSpace();\n    return Insert(new AllocaInst(Ty, AddrSpace, ArraySize, AllocaAlign), Name);\n  }\n\n  /// Provided to resolve 'CreateLoad(Ty, Ptr, \"...\")' correctly, instead of\n  /// converting the string to 'bool' for the isVolatile parameter.\n  LoadInst *CreateLoad(Type *Ty, Value *Ptr, const char *Name) {\n    return CreateAlignedLoad(Ty, Ptr, MaybeAlign(), Name);\n  }\n\n  LoadInst *CreateLoad(Type *Ty, Value *Ptr, const Twine &Name = \"\") {\n    return CreateAlignedLoad(Ty, Ptr, MaybeAlign(), Name);\n  }\n\n  LoadInst *CreateLoad(Type *Ty, Value *Ptr, bool isVolatile,\n                       const Twine &Name = \"\") {\n    return CreateAlignedLoad(Ty, Ptr, MaybeAlign(), isVolatile, Name);\n  }\n\n  // Deprecated [opaque pointer types]\n  LoadInst *CreateLoad(Value *Ptr, const char *Name) {\n    return CreateLoad(Ptr->getType()->getPointerElementType(), Ptr, Name);\n  }\n\n  // Deprecated [opaque pointer types]\n  LoadInst *CreateLoad(Value *Ptr, const Twine &Name = \"\") {\n    return CreateLoad(Ptr->getType()->getPointerElementType(), Ptr, Name);\n  }\n\n  // Deprecated [opaque pointer types]\n  LoadInst *CreateLoad(Value *Ptr, bool isVolatile, const Twine &Name = \"\") {\n    return CreateLoad(Ptr->getType()->getPointerElementType(), Ptr, isVolatile,\n                      Name);\n  }\n\n  StoreInst *CreateStore(Value *Val, Value *Ptr, bool isVolatile = false) {\n    return CreateAlignedStore(Val, Ptr, MaybeAlign(), isVolatile);\n  }\n\n  LoadInst *CreateAlignedLoad(Type *Ty, Value *Ptr, MaybeAlign Align,\n                              const char *Name) {\n    return CreateAlignedLoad(Ty, Ptr, Align, /*isVolatile*/false, Name);\n  }\n\n  LoadInst *CreateAlignedLoad(Type *Ty, Value *Ptr, MaybeAlign Align,\n                              const Twine &Name = \"\") {\n    return CreateAlignedLoad(Ty, Ptr, Align, /*isVolatile*/false, Name);\n  }\n\n  LoadInst *CreateAlignedLoad(Type *Ty, Value *Ptr, MaybeAlign Align,\n                              bool isVolatile, const Twine &Name = \"\") {\n    if (!Align) {\n      const DataLayout &DL = BB->getModule()->getDataLayout();\n      Align = DL.getABITypeAlign(Ty);\n    }\n    return Insert(new LoadInst(Ty, Ptr, Twine(), isVolatile, *Align), Name);\n  }\n\n  // Deprecated [opaque pointer types]\n  LoadInst *CreateAlignedLoad(Value *Ptr, MaybeAlign Align, const char *Name) {\n    return CreateAlignedLoad(Ptr->getType()->getPointerElementType(), Ptr,\n                             Align, Name);\n  }\n  // Deprecated [opaque pointer types]\n  LoadInst *CreateAlignedLoad(Value *Ptr, MaybeAlign Align,\n                              const Twine &Name = \"\") {\n    return CreateAlignedLoad(Ptr->getType()->getPointerElementType(), Ptr,\n                             Align, Name);\n  }\n  // Deprecated [opaque pointer types]\n  LoadInst *CreateAlignedLoad(Value *Ptr, MaybeAlign Align, bool isVolatile,\n                              const Twine &Name = \"\") {\n    return CreateAlignedLoad(Ptr->getType()->getPointerElementType(), Ptr,\n                             Align, isVolatile, Name);\n  }\n\n  StoreInst *CreateAlignedStore(Value *Val, Value *Ptr, MaybeAlign Align,\n                                bool isVolatile = false) {\n    if (!Align) {\n      const DataLayout &DL = BB->getModule()->getDataLayout();\n      Align = DL.getABITypeAlign(Val->getType());\n    }\n    return Insert(new StoreInst(Val, Ptr, isVolatile, *Align));\n  }\n  FenceInst *CreateFence(AtomicOrdering Ordering,\n                         SyncScope::ID SSID = SyncScope::System,\n                         const Twine &Name = \"\") {\n    return Insert(new FenceInst(Context, Ordering, SSID), Name);\n  }\n\n  AtomicCmpXchgInst *\n  CreateAtomicCmpXchg(Value *Ptr, Value *Cmp, Value *New, MaybeAlign Align,\n                      AtomicOrdering SuccessOrdering,\n                      AtomicOrdering FailureOrdering,\n                      SyncScope::ID SSID = SyncScope::System) {\n    if (!Align) {\n      const DataLayout &DL = BB->getModule()->getDataLayout();\n      Align = llvm::Align(DL.getTypeStoreSize(New->getType()));\n    }\n\n    return Insert(new AtomicCmpXchgInst(Ptr, Cmp, New, *Align, SuccessOrdering,\n                                        FailureOrdering, SSID));\n  }\n\n  AtomicRMWInst *CreateAtomicRMW(AtomicRMWInst::BinOp Op, Value *Ptr,\n                                 Value *Val, MaybeAlign Align,\n                                 AtomicOrdering Ordering,\n                                 SyncScope::ID SSID = SyncScope::System) {\n    if (!Align) {\n      const DataLayout &DL = BB->getModule()->getDataLayout();\n      Align = llvm::Align(DL.getTypeStoreSize(Val->getType()));\n    }\n\n    return Insert(new AtomicRMWInst(Op, Ptr, Val, *Align, Ordering, SSID));\n  }\n\n  Value *CreateGEP(Value *Ptr, ArrayRef<Value *> IdxList,\n                   const Twine &Name = \"\") {\n    return CreateGEP(nullptr, Ptr, IdxList, Name);\n  }\n\n  Value *CreateGEP(Type *Ty, Value *Ptr, ArrayRef<Value *> IdxList,\n                   const Twine &Name = \"\") {\n    if (auto *PC = dyn_cast<Constant>(Ptr)) {\n      // Every index must be constant.\n      size_t i, e;\n      for (i = 0, e = IdxList.size(); i != e; ++i)\n        if (!isa<Constant>(IdxList[i]))\n          break;\n      if (i == e)\n        return Insert(Folder.CreateGetElementPtr(Ty, PC, IdxList), Name);\n    }\n    return Insert(GetElementPtrInst::Create(Ty, Ptr, IdxList), Name);\n  }\n\n  Value *CreateInBoundsGEP(Value *Ptr, ArrayRef<Value *> IdxList,\n                           const Twine &Name = \"\") {\n    return CreateInBoundsGEP(nullptr, Ptr, IdxList, Name);\n  }\n\n  Value *CreateInBoundsGEP(Type *Ty, Value *Ptr, ArrayRef<Value *> IdxList,\n                           const Twine &Name = \"\") {\n    if (auto *PC = dyn_cast<Constant>(Ptr)) {\n      // Every index must be constant.\n      size_t i, e;\n      for (i = 0, e = IdxList.size(); i != e; ++i)\n        if (!isa<Constant>(IdxList[i]))\n          break;\n      if (i == e)\n        return Insert(Folder.CreateInBoundsGetElementPtr(Ty, PC, IdxList),\n                      Name);\n    }\n    return Insert(GetElementPtrInst::CreateInBounds(Ty, Ptr, IdxList), Name);\n  }\n\n  Value *CreateGEP(Value *Ptr, Value *Idx, const Twine &Name = \"\") {\n    return CreateGEP(nullptr, Ptr, Idx, Name);\n  }\n\n  Value *CreateGEP(Type *Ty, Value *Ptr, Value *Idx, const Twine &Name = \"\") {\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      if (auto *IC = dyn_cast<Constant>(Idx))\n        return Insert(Folder.CreateGetElementPtr(Ty, PC, IC), Name);\n    return Insert(GetElementPtrInst::Create(Ty, Ptr, Idx), Name);\n  }\n\n  Value *CreateInBoundsGEP(Type *Ty, Value *Ptr, Value *Idx,\n                           const Twine &Name = \"\") {\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      if (auto *IC = dyn_cast<Constant>(Idx))\n        return Insert(Folder.CreateInBoundsGetElementPtr(Ty, PC, IC), Name);\n    return Insert(GetElementPtrInst::CreateInBounds(Ty, Ptr, Idx), Name);\n  }\n\n  Value *CreateConstGEP1_32(Value *Ptr, unsigned Idx0, const Twine &Name = \"\") {\n    return CreateConstGEP1_32(nullptr, Ptr, Idx0, Name);\n  }\n\n  Value *CreateConstGEP1_32(Type *Ty, Value *Ptr, unsigned Idx0,\n                            const Twine &Name = \"\") {\n    Value *Idx = ConstantInt::get(Type::getInt32Ty(Context), Idx0);\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateGetElementPtr(Ty, PC, Idx), Name);\n\n    return Insert(GetElementPtrInst::Create(Ty, Ptr, Idx), Name);\n  }\n\n  Value *CreateConstInBoundsGEP1_32(Type *Ty, Value *Ptr, unsigned Idx0,\n                                    const Twine &Name = \"\") {\n    Value *Idx = ConstantInt::get(Type::getInt32Ty(Context), Idx0);\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateInBoundsGetElementPtr(Ty, PC, Idx), Name);\n\n    return Insert(GetElementPtrInst::CreateInBounds(Ty, Ptr, Idx), Name);\n  }\n\n  Value *CreateConstGEP2_32(Type *Ty, Value *Ptr, unsigned Idx0, unsigned Idx1,\n                            const Twine &Name = \"\") {\n    Value *Idxs[] = {\n      ConstantInt::get(Type::getInt32Ty(Context), Idx0),\n      ConstantInt::get(Type::getInt32Ty(Context), Idx1)\n    };\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateGetElementPtr(Ty, PC, Idxs), Name);\n\n    return Insert(GetElementPtrInst::Create(Ty, Ptr, Idxs), Name);\n  }\n\n  Value *CreateConstInBoundsGEP2_32(Type *Ty, Value *Ptr, unsigned Idx0,\n                                    unsigned Idx1, const Twine &Name = \"\") {\n    Value *Idxs[] = {\n      ConstantInt::get(Type::getInt32Ty(Context), Idx0),\n      ConstantInt::get(Type::getInt32Ty(Context), Idx1)\n    };\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateInBoundsGetElementPtr(Ty, PC, Idxs), Name);\n\n    return Insert(GetElementPtrInst::CreateInBounds(Ty, Ptr, Idxs), Name);\n  }\n\n  Value *CreateConstGEP1_64(Type *Ty, Value *Ptr, uint64_t Idx0,\n                            const Twine &Name = \"\") {\n    Value *Idx = ConstantInt::get(Type::getInt64Ty(Context), Idx0);\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateGetElementPtr(Ty, PC, Idx), Name);\n\n    return Insert(GetElementPtrInst::Create(Ty, Ptr, Idx), Name);\n  }\n\n  Value *CreateConstGEP1_64(Value *Ptr, uint64_t Idx0, const Twine &Name = \"\") {\n    return CreateConstGEP1_64(nullptr, Ptr, Idx0, Name);\n  }\n\n  Value *CreateConstInBoundsGEP1_64(Type *Ty, Value *Ptr, uint64_t Idx0,\n                                    const Twine &Name = \"\") {\n    Value *Idx = ConstantInt::get(Type::getInt64Ty(Context), Idx0);\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateInBoundsGetElementPtr(Ty, PC, Idx), Name);\n\n    return Insert(GetElementPtrInst::CreateInBounds(Ty, Ptr, Idx), Name);\n  }\n\n  Value *CreateConstInBoundsGEP1_64(Value *Ptr, uint64_t Idx0,\n                                    const Twine &Name = \"\") {\n    return CreateConstInBoundsGEP1_64(nullptr, Ptr, Idx0, Name);\n  }\n\n  Value *CreateConstGEP2_64(Type *Ty, Value *Ptr, uint64_t Idx0, uint64_t Idx1,\n                            const Twine &Name = \"\") {\n    Value *Idxs[] = {\n      ConstantInt::get(Type::getInt64Ty(Context), Idx0),\n      ConstantInt::get(Type::getInt64Ty(Context), Idx1)\n    };\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateGetElementPtr(Ty, PC, Idxs), Name);\n\n    return Insert(GetElementPtrInst::Create(Ty, Ptr, Idxs), Name);\n  }\n\n  Value *CreateConstGEP2_64(Value *Ptr, uint64_t Idx0, uint64_t Idx1,\n                            const Twine &Name = \"\") {\n    return CreateConstGEP2_64(nullptr, Ptr, Idx0, Idx1, Name);\n  }\n\n  Value *CreateConstInBoundsGEP2_64(Type *Ty, Value *Ptr, uint64_t Idx0,\n                                    uint64_t Idx1, const Twine &Name = \"\") {\n    Value *Idxs[] = {\n      ConstantInt::get(Type::getInt64Ty(Context), Idx0),\n      ConstantInt::get(Type::getInt64Ty(Context), Idx1)\n    };\n\n    if (auto *PC = dyn_cast<Constant>(Ptr))\n      return Insert(Folder.CreateInBoundsGetElementPtr(Ty, PC, Idxs), Name);\n\n    return Insert(GetElementPtrInst::CreateInBounds(Ty, Ptr, Idxs), Name);\n  }\n\n  Value *CreateConstInBoundsGEP2_64(Value *Ptr, uint64_t Idx0, uint64_t Idx1,\n                                    const Twine &Name = \"\") {\n    return CreateConstInBoundsGEP2_64(nullptr, Ptr, Idx0, Idx1, Name);\n  }\n\n  Value *CreateStructGEP(Type *Ty, Value *Ptr, unsigned Idx,\n                         const Twine &Name = \"\") {\n    return CreateConstInBoundsGEP2_32(Ty, Ptr, 0, Idx, Name);\n  }\n\n  Value *CreateStructGEP(Value *Ptr, unsigned Idx, const Twine &Name = \"\") {\n    return CreateConstInBoundsGEP2_32(nullptr, Ptr, 0, Idx, Name);\n  }\n\n  /// Same as CreateGlobalString, but return a pointer with \"i8*\" type\n  /// instead of a pointer to array of i8.\n  ///\n  /// If no module is given via \\p M, it is take from the insertion point basic\n  /// block.\n  Constant *CreateGlobalStringPtr(StringRef Str, const Twine &Name = \"\",\n                                  unsigned AddressSpace = 0,\n                                  Module *M = nullptr) {\n    GlobalVariable *GV = CreateGlobalString(Str, Name, AddressSpace, M);\n    Constant *Zero = ConstantInt::get(Type::getInt32Ty(Context), 0);\n    Constant *Indices[] = {Zero, Zero};\n    return ConstantExpr::getInBoundsGetElementPtr(GV->getValueType(), GV,\n                                                  Indices);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Instruction creation methods: Cast/Conversion Operators\n  //===--------------------------------------------------------------------===//\n\n  Value *CreateTrunc(Value *V, Type *DestTy, const Twine &Name = \"\") {\n    return CreateCast(Instruction::Trunc, V, DestTy, Name);\n  }\n\n  Value *CreateZExt(Value *V, Type *DestTy, const Twine &Name = \"\") {\n    return CreateCast(Instruction::ZExt, V, DestTy, Name);\n  }\n\n  Value *CreateSExt(Value *V, Type *DestTy, const Twine &Name = \"\") {\n    return CreateCast(Instruction::SExt, V, DestTy, Name);\n  }\n\n  /// Create a ZExt or Trunc from the integer value V to DestTy. Return\n  /// the value untouched if the type of V is already DestTy.\n  Value *CreateZExtOrTrunc(Value *V, Type *DestTy,\n                           const Twine &Name = \"\") {\n    assert(V->getType()->isIntOrIntVectorTy() &&\n           DestTy->isIntOrIntVectorTy() &&\n           \"Can only zero extend/truncate integers!\");\n    Type *VTy = V->getType();\n    if (VTy->getScalarSizeInBits() < DestTy->getScalarSizeInBits())\n      return CreateZExt(V, DestTy, Name);\n    if (VTy->getScalarSizeInBits() > DestTy->getScalarSizeInBits())\n      return CreateTrunc(V, DestTy, Name);\n    return V;\n  }\n\n  /// Create a SExt or Trunc from the integer value V to DestTy. Return\n  /// the value untouched if the type of V is already DestTy.\n  Value *CreateSExtOrTrunc(Value *V, Type *DestTy,\n                           const Twine &Name = \"\") {\n    assert(V->getType()->isIntOrIntVectorTy() &&\n           DestTy->isIntOrIntVectorTy() &&\n           \"Can only sign extend/truncate integers!\");\n    Type *VTy = V->getType();\n    if (VTy->getScalarSizeInBits() < DestTy->getScalarSizeInBits())\n      return CreateSExt(V, DestTy, Name);\n    if (VTy->getScalarSizeInBits() > DestTy->getScalarSizeInBits())\n      return CreateTrunc(V, DestTy, Name);\n    return V;\n  }\n\n  Value *CreateFPToUI(Value *V, Type *DestTy, const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPCast(Intrinsic::experimental_constrained_fptoui,\n                                     V, DestTy, nullptr, Name);\n    return CreateCast(Instruction::FPToUI, V, DestTy, Name);\n  }\n\n  Value *CreateFPToSI(Value *V, Type *DestTy, const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPCast(Intrinsic::experimental_constrained_fptosi,\n                                     V, DestTy, nullptr, Name);\n    return CreateCast(Instruction::FPToSI, V, DestTy, Name);\n  }\n\n  Value *CreateUIToFP(Value *V, Type *DestTy, const Twine &Name = \"\"){\n    if (IsFPConstrained)\n      return CreateConstrainedFPCast(Intrinsic::experimental_constrained_uitofp,\n                                     V, DestTy, nullptr, Name);\n    return CreateCast(Instruction::UIToFP, V, DestTy, Name);\n  }\n\n  Value *CreateSIToFP(Value *V, Type *DestTy, const Twine &Name = \"\"){\n    if (IsFPConstrained)\n      return CreateConstrainedFPCast(Intrinsic::experimental_constrained_sitofp,\n                                     V, DestTy, nullptr, Name);\n    return CreateCast(Instruction::SIToFP, V, DestTy, Name);\n  }\n\n  Value *CreateFPTrunc(Value *V, Type *DestTy,\n                       const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPCast(\n          Intrinsic::experimental_constrained_fptrunc, V, DestTy, nullptr,\n          Name);\n    return CreateCast(Instruction::FPTrunc, V, DestTy, Name);\n  }\n\n  Value *CreateFPExt(Value *V, Type *DestTy, const Twine &Name = \"\") {\n    if (IsFPConstrained)\n      return CreateConstrainedFPCast(Intrinsic::experimental_constrained_fpext,\n                                     V, DestTy, nullptr, Name);\n    return CreateCast(Instruction::FPExt, V, DestTy, Name);\n  }\n\n  Value *CreatePtrToInt(Value *V, Type *DestTy,\n                        const Twine &Name = \"\") {\n    return CreateCast(Instruction::PtrToInt, V, DestTy, Name);\n  }\n\n  Value *CreateIntToPtr(Value *V, Type *DestTy,\n                        const Twine &Name = \"\") {\n    return CreateCast(Instruction::IntToPtr, V, DestTy, Name);\n  }\n\n  Value *CreateBitCast(Value *V, Type *DestTy,\n                       const Twine &Name = \"\") {\n    return CreateCast(Instruction::BitCast, V, DestTy, Name);\n  }\n\n  Value *CreateAddrSpaceCast(Value *V, Type *DestTy,\n                             const Twine &Name = \"\") {\n    return CreateCast(Instruction::AddrSpaceCast, V, DestTy, Name);\n  }\n\n  Value *CreateZExtOrBitCast(Value *V, Type *DestTy,\n                             const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateZExtOrBitCast(VC, DestTy), Name);\n    return Insert(CastInst::CreateZExtOrBitCast(V, DestTy), Name);\n  }\n\n  Value *CreateSExtOrBitCast(Value *V, Type *DestTy,\n                             const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateSExtOrBitCast(VC, DestTy), Name);\n    return Insert(CastInst::CreateSExtOrBitCast(V, DestTy), Name);\n  }\n\n  Value *CreateTruncOrBitCast(Value *V, Type *DestTy,\n                              const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateTruncOrBitCast(VC, DestTy), Name);\n    return Insert(CastInst::CreateTruncOrBitCast(V, DestTy), Name);\n  }\n\n  Value *CreateCast(Instruction::CastOps Op, Value *V, Type *DestTy,\n                    const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateCast(Op, VC, DestTy), Name);\n    return Insert(CastInst::Create(Op, V, DestTy), Name);\n  }\n\n  Value *CreatePointerCast(Value *V, Type *DestTy,\n                           const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreatePointerCast(VC, DestTy), Name);\n    return Insert(CastInst::CreatePointerCast(V, DestTy), Name);\n  }\n\n  Value *CreatePointerBitCastOrAddrSpaceCast(Value *V, Type *DestTy,\n                                             const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n\n    if (auto *VC = dyn_cast<Constant>(V)) {\n      return Insert(Folder.CreatePointerBitCastOrAddrSpaceCast(VC, DestTy),\n                    Name);\n    }\n\n    return Insert(CastInst::CreatePointerBitCastOrAddrSpaceCast(V, DestTy),\n                  Name);\n  }\n\n  Value *CreateIntCast(Value *V, Type *DestTy, bool isSigned,\n                       const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateIntCast(VC, DestTy, isSigned), Name);\n    return Insert(CastInst::CreateIntegerCast(V, DestTy, isSigned), Name);\n  }\n\n  Value *CreateBitOrPointerCast(Value *V, Type *DestTy,\n                                const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (V->getType()->isPtrOrPtrVectorTy() && DestTy->isIntOrIntVectorTy())\n      return CreatePtrToInt(V, DestTy, Name);\n    if (V->getType()->isIntOrIntVectorTy() && DestTy->isPtrOrPtrVectorTy())\n      return CreateIntToPtr(V, DestTy, Name);\n\n    return CreateBitCast(V, DestTy, Name);\n  }\n\n  Value *CreateFPCast(Value *V, Type *DestTy, const Twine &Name = \"\") {\n    if (V->getType() == DestTy)\n      return V;\n    if (auto *VC = dyn_cast<Constant>(V))\n      return Insert(Folder.CreateFPCast(VC, DestTy), Name);\n    return Insert(CastInst::CreateFPCast(V, DestTy), Name);\n  }\n\n  CallInst *CreateConstrainedFPCast(\n      Intrinsic::ID ID, Value *V, Type *DestTy,\n      Instruction *FMFSource = nullptr, const Twine &Name = \"\",\n      MDNode *FPMathTag = nullptr,\n      Optional<RoundingMode> Rounding = None,\n      Optional<fp::ExceptionBehavior> Except = None);\n\n  // Provided to resolve 'CreateIntCast(Ptr, Ptr, \"...\")', giving a\n  // compile time error, instead of converting the string to bool for the\n  // isSigned parameter.\n  Value *CreateIntCast(Value *, Type *, const char *) = delete;\n\n  //===--------------------------------------------------------------------===//\n  // Instruction creation methods: Compare Instructions\n  //===--------------------------------------------------------------------===//\n\n  Value *CreateICmpEQ(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_EQ, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpNE(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_NE, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpUGT(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_UGT, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpUGE(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_UGE, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpULT(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_ULT, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpULE(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_ULE, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpSGT(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_SGT, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpSGE(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_SGE, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpSLT(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_SLT, LHS, RHS, Name);\n  }\n\n  Value *CreateICmpSLE(Value *LHS, Value *RHS, const Twine &Name = \"\") {\n    return CreateICmp(ICmpInst::ICMP_SLE, LHS, RHS, Name);\n  }\n\n  Value *CreateFCmpOEQ(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_OEQ, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpOGT(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_OGT, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpOGE(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_OGE, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpOLT(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_OLT, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpOLE(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_OLE, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpONE(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_ONE, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpORD(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_ORD, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpUNO(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_UNO, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpUEQ(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_UEQ, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpUGT(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_UGT, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpUGE(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_UGE, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpULT(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_ULT, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpULE(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_ULE, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateFCmpUNE(Value *LHS, Value *RHS, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    return CreateFCmp(FCmpInst::FCMP_UNE, LHS, RHS, Name, FPMathTag);\n  }\n\n  Value *CreateICmp(CmpInst::Predicate P, Value *LHS, Value *RHS,\n                    const Twine &Name = \"\") {\n    if (auto *LC = dyn_cast<Constant>(LHS))\n      if (auto *RC = dyn_cast<Constant>(RHS))\n        return Insert(Folder.CreateICmp(P, LC, RC), Name);\n    return Insert(new ICmpInst(P, LHS, RHS), Name);\n  }\n\n  // Create a quiet floating-point comparison (i.e. one that raises an FP\n  // exception only in the case where an input is a signaling NaN).\n  // Note that this differs from CreateFCmpS only if IsFPConstrained is true.\n  Value *CreateFCmp(CmpInst::Predicate P, Value *LHS, Value *RHS,\n                    const Twine &Name = \"\", MDNode *FPMathTag = nullptr) {\n    return CreateFCmpHelper(P, LHS, RHS, Name, FPMathTag, false);\n  }\n\n  Value *CreateCmp(CmpInst::Predicate Pred, Value *LHS, Value *RHS,\n                   const Twine &Name = \"\", MDNode *FPMathTag = nullptr) {\n    return CmpInst::isFPPredicate(Pred)\n               ? CreateFCmp(Pred, LHS, RHS, Name, FPMathTag)\n               : CreateICmp(Pred, LHS, RHS, Name);\n  }\n\n  // Create a signaling floating-point comparison (i.e. one that raises an FP\n  // exception whenever an input is any NaN, signaling or quiet).\n  // Note that this differs from CreateFCmp only if IsFPConstrained is true.\n  Value *CreateFCmpS(CmpInst::Predicate P, Value *LHS, Value *RHS,\n                     const Twine &Name = \"\", MDNode *FPMathTag = nullptr) {\n    return CreateFCmpHelper(P, LHS, RHS, Name, FPMathTag, true);\n  }\n\nprivate:\n  // Helper routine to create either a signaling or a quiet FP comparison.\n  Value *CreateFCmpHelper(CmpInst::Predicate P, Value *LHS, Value *RHS,\n                          const Twine &Name, MDNode *FPMathTag,\n                          bool IsSignaling);\n\npublic:\n  CallInst *CreateConstrainedFPCmp(\n      Intrinsic::ID ID, CmpInst::Predicate P, Value *L, Value *R,\n      const Twine &Name = \"\", Optional<fp::ExceptionBehavior> Except = None);\n\n  //===--------------------------------------------------------------------===//\n  // Instruction creation methods: Other Instructions\n  //===--------------------------------------------------------------------===//\n\n  PHINode *CreatePHI(Type *Ty, unsigned NumReservedValues,\n                     const Twine &Name = \"\") {\n    PHINode *Phi = PHINode::Create(Ty, NumReservedValues);\n    if (isa<FPMathOperator>(Phi))\n      setFPAttrs(Phi, nullptr /* MDNode* */, FMF);\n    return Insert(Phi, Name);\n  }\n\n  CallInst *CreateCall(FunctionType *FTy, Value *Callee,\n                       ArrayRef<Value *> Args = None, const Twine &Name = \"\",\n                       MDNode *FPMathTag = nullptr) {\n    CallInst *CI = CallInst::Create(FTy, Callee, Args, DefaultOperandBundles);\n    if (IsFPConstrained)\n      setConstrainedFPCallAttr(CI);\n    if (isa<FPMathOperator>(CI))\n      setFPAttrs(CI, FPMathTag, FMF);\n    return Insert(CI, Name);\n  }\n\n  CallInst *CreateCall(FunctionType *FTy, Value *Callee, ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> OpBundles,\n                       const Twine &Name = \"\", MDNode *FPMathTag = nullptr) {\n    CallInst *CI = CallInst::Create(FTy, Callee, Args, OpBundles);\n    if (IsFPConstrained)\n      setConstrainedFPCallAttr(CI);\n    if (isa<FPMathOperator>(CI))\n      setFPAttrs(CI, FPMathTag, FMF);\n    return Insert(CI, Name);\n  }\n\n  CallInst *CreateCall(FunctionCallee Callee, ArrayRef<Value *> Args = None,\n                       const Twine &Name = \"\", MDNode *FPMathTag = nullptr) {\n    return CreateCall(Callee.getFunctionType(), Callee.getCallee(), Args, Name,\n                      FPMathTag);\n  }\n\n  CallInst *CreateCall(FunctionCallee Callee, ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> OpBundles,\n                       const Twine &Name = \"\", MDNode *FPMathTag = nullptr) {\n    return CreateCall(Callee.getFunctionType(), Callee.getCallee(), Args,\n                      OpBundles, Name, FPMathTag);\n  }\n\n  CallInst *CreateConstrainedFPCall(\n      Function *Callee, ArrayRef<Value *> Args, const Twine &Name = \"\",\n      Optional<RoundingMode> Rounding = None,\n      Optional<fp::ExceptionBehavior> Except = None);\n\n  Value *CreateSelect(Value *C, Value *True, Value *False,\n                      const Twine &Name = \"\", Instruction *MDFrom = nullptr);\n\n  VAArgInst *CreateVAArg(Value *List, Type *Ty, const Twine &Name = \"\") {\n    return Insert(new VAArgInst(List, Ty), Name);\n  }\n\n  Value *CreateExtractElement(Value *Vec, Value *Idx,\n                              const Twine &Name = \"\") {\n    if (auto *VC = dyn_cast<Constant>(Vec))\n      if (auto *IC = dyn_cast<Constant>(Idx))\n        return Insert(Folder.CreateExtractElement(VC, IC), Name);\n    return Insert(ExtractElementInst::Create(Vec, Idx), Name);\n  }\n\n  Value *CreateExtractElement(Value *Vec, uint64_t Idx,\n                              const Twine &Name = \"\") {\n    return CreateExtractElement(Vec, getInt64(Idx), Name);\n  }\n\n  Value *CreateInsertElement(Value *Vec, Value *NewElt, Value *Idx,\n                             const Twine &Name = \"\") {\n    if (auto *VC = dyn_cast<Constant>(Vec))\n      if (auto *NC = dyn_cast<Constant>(NewElt))\n        if (auto *IC = dyn_cast<Constant>(Idx))\n          return Insert(Folder.CreateInsertElement(VC, NC, IC), Name);\n    return Insert(InsertElementInst::Create(Vec, NewElt, Idx), Name);\n  }\n\n  Value *CreateInsertElement(Value *Vec, Value *NewElt, uint64_t Idx,\n                             const Twine &Name = \"\") {\n    return CreateInsertElement(Vec, NewElt, getInt64(Idx), Name);\n  }\n\n  Value *CreateShuffleVector(Value *V1, Value *V2, Value *Mask,\n                             const Twine &Name = \"\") {\n    SmallVector<int, 16> IntMask;\n    ShuffleVectorInst::getShuffleMask(cast<Constant>(Mask), IntMask);\n    return CreateShuffleVector(V1, V2, IntMask, Name);\n  }\n\n  LLVM_ATTRIBUTE_DEPRECATED(Value *CreateShuffleVector(Value *V1, Value *V2,\n                                                       ArrayRef<uint32_t> Mask,\n                                                       const Twine &Name = \"\"),\n                            \"Pass indices as 'int' instead\") {\n    SmallVector<int, 16> IntMask;\n    IntMask.assign(Mask.begin(), Mask.end());\n    return CreateShuffleVector(V1, V2, IntMask, Name);\n  }\n\n  /// See class ShuffleVectorInst for a description of the mask representation.\n  Value *CreateShuffleVector(Value *V1, Value *V2, ArrayRef<int> Mask,\n                             const Twine &Name = \"\") {\n    if (auto *V1C = dyn_cast<Constant>(V1))\n      if (auto *V2C = dyn_cast<Constant>(V2))\n        return Insert(Folder.CreateShuffleVector(V1C, V2C, Mask), Name);\n    return Insert(new ShuffleVectorInst(V1, V2, Mask), Name);\n  }\n\n  /// Create a unary shuffle. The second vector operand of the IR instruction\n  /// is poison.\n  Value *CreateShuffleVector(Value *V, ArrayRef<int> Mask,\n                             const Twine &Name = \"\") {\n    return CreateShuffleVector(V, PoisonValue::get(V->getType()), Mask, Name);\n  }\n\n  Value *CreateExtractValue(Value *Agg,\n                            ArrayRef<unsigned> Idxs,\n                            const Twine &Name = \"\") {\n    if (auto *AggC = dyn_cast<Constant>(Agg))\n      return Insert(Folder.CreateExtractValue(AggC, Idxs), Name);\n    return Insert(ExtractValueInst::Create(Agg, Idxs), Name);\n  }\n\n  Value *CreateInsertValue(Value *Agg, Value *Val,\n                           ArrayRef<unsigned> Idxs,\n                           const Twine &Name = \"\") {\n    if (auto *AggC = dyn_cast<Constant>(Agg))\n      if (auto *ValC = dyn_cast<Constant>(Val))\n        return Insert(Folder.CreateInsertValue(AggC, ValC, Idxs), Name);\n    return Insert(InsertValueInst::Create(Agg, Val, Idxs), Name);\n  }\n\n  LandingPadInst *CreateLandingPad(Type *Ty, unsigned NumClauses,\n                                   const Twine &Name = \"\") {\n    return Insert(LandingPadInst::Create(Ty, NumClauses), Name);\n  }\n\n  Value *CreateFreeze(Value *V, const Twine &Name = \"\") {\n    return Insert(new FreezeInst(V), Name);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Utility creation methods\n  //===--------------------------------------------------------------------===//\n\n  /// Return an i1 value testing if \\p Arg is null.\n  Value *CreateIsNull(Value *Arg, const Twine &Name = \"\") {\n    return CreateICmpEQ(Arg, Constant::getNullValue(Arg->getType()),\n                        Name);\n  }\n\n  /// Return an i1 value testing if \\p Arg is not null.\n  Value *CreateIsNotNull(Value *Arg, const Twine &Name = \"\") {\n    return CreateICmpNE(Arg, Constant::getNullValue(Arg->getType()),\n                        Name);\n  }\n\n  /// Return the i64 difference between two pointer values, dividing out\n  /// the size of the pointed-to objects.\n  ///\n  /// This is intended to implement C-style pointer subtraction. As such, the\n  /// pointers must be appropriately aligned for their element types and\n  /// pointing into the same object.\n  Value *CreatePtrDiff(Value *LHS, Value *RHS, const Twine &Name = \"\");\n\n  /// Create a launder.invariant.group intrinsic call. If Ptr type is\n  /// different from pointer to i8, it's casted to pointer to i8 in the same\n  /// address space before call and casted back to Ptr type after call.\n  Value *CreateLaunderInvariantGroup(Value *Ptr);\n\n  /// \\brief Create a strip.invariant.group intrinsic call. If Ptr type is\n  /// different from pointer to i8, it's casted to pointer to i8 in the same\n  /// address space before call and casted back to Ptr type after call.\n  Value *CreateStripInvariantGroup(Value *Ptr);\n\n  /// Return a vector value that contains \\arg V broadcasted to \\p\n  /// NumElts elements.\n  Value *CreateVectorSplat(unsigned NumElts, Value *V, const Twine &Name = \"\");\n\n  /// Return a vector value that contains \\arg V broadcasted to \\p\n  /// EC elements.\n  Value *CreateVectorSplat(ElementCount EC, Value *V, const Twine &Name = \"\");\n\n  /// Return a value that has been extracted from a larger integer type.\n  Value *CreateExtractInteger(const DataLayout &DL, Value *From,\n                              IntegerType *ExtractedTy, uint64_t Offset,\n                              const Twine &Name);\n\n  Value *CreatePreserveArrayAccessIndex(Type *ElTy, Value *Base,\n                                        unsigned Dimension, unsigned LastIndex,\n                                        MDNode *DbgInfo);\n\n  Value *CreatePreserveUnionAccessIndex(Value *Base, unsigned FieldIndex,\n                                        MDNode *DbgInfo);\n\n  Value *CreatePreserveStructAccessIndex(Type *ElTy, Value *Base,\n                                         unsigned Index, unsigned FieldIndex,\n                                         MDNode *DbgInfo);\n\nprivate:\n  /// Helper function that creates an assume intrinsic call that\n  /// represents an alignment assumption on the provided pointer \\p PtrValue\n  /// with offset \\p OffsetValue and alignment value \\p AlignValue.\n  CallInst *CreateAlignmentAssumptionHelper(const DataLayout &DL,\n                                            Value *PtrValue, Value *AlignValue,\n                                            Value *OffsetValue);\n\npublic:\n  /// Create an assume intrinsic call that represents an alignment\n  /// assumption on the provided pointer.\n  ///\n  /// An optional offset can be provided, and if it is provided, the offset\n  /// must be subtracted from the provided pointer to get the pointer with the\n  /// specified alignment.\n  CallInst *CreateAlignmentAssumption(const DataLayout &DL, Value *PtrValue,\n                                      unsigned Alignment,\n                                      Value *OffsetValue = nullptr);\n\n  /// Create an assume intrinsic call that represents an alignment\n  /// assumption on the provided pointer.\n  ///\n  /// An optional offset can be provided, and if it is provided, the offset\n  /// must be subtracted from the provided pointer to get the pointer with the\n  /// specified alignment.\n  ///\n  /// This overload handles the condition where the Alignment is dependent\n  /// on an existing value rather than a static value.\n  CallInst *CreateAlignmentAssumption(const DataLayout &DL, Value *PtrValue,\n                                      Value *Alignment,\n                                      Value *OffsetValue = nullptr);\n};\n\n/// This provides a uniform API for creating instructions and inserting\n/// them into a basic block: either at the end of a BasicBlock, or at a specific\n/// iterator location in a block.\n///\n/// Note that the builder does not expose the full generality of LLVM\n/// instructions.  For access to extra instruction properties, use the mutators\n/// (e.g. setVolatile) on the instructions after they have been\n/// created. Convenience state exists to specify fast-math flags and fp-math\n/// tags.\n///\n/// The first template argument specifies a class to use for creating constants.\n/// This defaults to creating minimally folded constants.  The second template\n/// argument allows clients to specify custom insertion hooks that are called on\n/// every newly created insertion.\ntemplate <typename FolderTy = ConstantFolder,\n          typename InserterTy = IRBuilderDefaultInserter>\nclass IRBuilder : public IRBuilderBase {\nprivate:\n  FolderTy Folder;\n  InserterTy Inserter;\n\npublic:\n  IRBuilder(LLVMContext &C, FolderTy Folder, InserterTy Inserter = InserterTy(),\n            MDNode *FPMathTag = nullptr,\n            ArrayRef<OperandBundleDef> OpBundles = None)\n      : IRBuilderBase(C, this->Folder, this->Inserter, FPMathTag, OpBundles),\n        Folder(Folder), Inserter(Inserter) {}\n\n  explicit IRBuilder(LLVMContext &C, MDNode *FPMathTag = nullptr,\n                     ArrayRef<OperandBundleDef> OpBundles = None)\n      : IRBuilderBase(C, this->Folder, this->Inserter, FPMathTag, OpBundles) {}\n\n  explicit IRBuilder(BasicBlock *TheBB, FolderTy Folder,\n                     MDNode *FPMathTag = nullptr,\n                     ArrayRef<OperandBundleDef> OpBundles = None)\n      : IRBuilderBase(TheBB->getContext(), this->Folder, this->Inserter,\n                      FPMathTag, OpBundles), Folder(Folder) {\n    SetInsertPoint(TheBB);\n  }\n\n  explicit IRBuilder(BasicBlock *TheBB, MDNode *FPMathTag = nullptr,\n                     ArrayRef<OperandBundleDef> OpBundles = None)\n      : IRBuilderBase(TheBB->getContext(), this->Folder, this->Inserter,\n                      FPMathTag, OpBundles) {\n    SetInsertPoint(TheBB);\n  }\n\n  explicit IRBuilder(Instruction *IP, MDNode *FPMathTag = nullptr,\n                     ArrayRef<OperandBundleDef> OpBundles = None)\n      : IRBuilderBase(IP->getContext(), this->Folder, this->Inserter,\n                      FPMathTag, OpBundles) {\n    SetInsertPoint(IP);\n  }\n\n  IRBuilder(BasicBlock *TheBB, BasicBlock::iterator IP, FolderTy Folder,\n            MDNode *FPMathTag = nullptr,\n            ArrayRef<OperandBundleDef> OpBundles = None)\n      : IRBuilderBase(TheBB->getContext(), this->Folder, this->Inserter,\n                      FPMathTag, OpBundles), Folder(Folder) {\n    SetInsertPoint(TheBB, IP);\n  }\n\n  IRBuilder(BasicBlock *TheBB, BasicBlock::iterator IP,\n            MDNode *FPMathTag = nullptr,\n            ArrayRef<OperandBundleDef> OpBundles = None)\n      : IRBuilderBase(TheBB->getContext(), this->Folder, this->Inserter,\n                      FPMathTag, OpBundles) {\n    SetInsertPoint(TheBB, IP);\n  }\n\n  /// Avoid copying the full IRBuilder. Prefer using InsertPointGuard\n  /// or FastMathFlagGuard instead.\n  IRBuilder(const IRBuilder &) = delete;\n\n  InserterTy &getInserter() { return Inserter; }\n};\n\n// Create wrappers for C Binding types (see CBindingWrapping.h).\nDEFINE_SIMPLE_CONVERSION_FUNCTIONS(IRBuilder<>, LLVMBuilderRef)\n\n} // end namespace llvm\n\n#endif // LLVM_IR_IRBUILDER_H\n"}, "78": {"id": 78, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InstrTypes.h", "content": "//===- llvm/InstrTypes.h - Important Instruction subclasses -----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines various meta classes of instructions that exist in the VM\n// representation.  Specific concrete subclasses of these may be found in the\n// i*.h files...\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_INSTRTYPES_H\n#define LLVM_IR_INSTRTYPES_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/StringMap.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/LLVMContext.h\"\n#include \"llvm/IR/OperandTraits.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <iterator>\n#include <string>\n#include <vector>\n\nnamespace llvm {\n\nnamespace Intrinsic {\ntypedef unsigned ID;\n}\n\n//===----------------------------------------------------------------------===//\n//                          UnaryInstruction Class\n//===----------------------------------------------------------------------===//\n\nclass UnaryInstruction : public Instruction {\nprotected:\n  UnaryInstruction(Type *Ty, unsigned iType, Value *V,\n                   Instruction *IB = nullptr)\n    : Instruction(Ty, iType, &Op<0>(), 1, IB) {\n    Op<0>() = V;\n  }\n  UnaryInstruction(Type *Ty, unsigned iType, Value *V, BasicBlock *IAE)\n    : Instruction(Ty, iType, &Op<0>(), 1, IAE) {\n    Op<0>() = V;\n  }\n\npublic:\n  // allocate space for exactly one operand\n  void *operator new(size_t s) {\n    return User::operator new(s, 1);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isUnaryOp() ||\n           I->getOpcode() == Instruction::Alloca ||\n           I->getOpcode() == Instruction::Load ||\n           I->getOpcode() == Instruction::VAArg ||\n           I->getOpcode() == Instruction::ExtractValue ||\n           (I->getOpcode() >= CastOpsBegin && I->getOpcode() < CastOpsEnd);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<UnaryInstruction> :\n  public FixedNumOperandTraits<UnaryInstruction, 1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(UnaryInstruction, Value)\n\n//===----------------------------------------------------------------------===//\n//                                UnaryOperator Class\n//===----------------------------------------------------------------------===//\n\nclass UnaryOperator : public UnaryInstruction {\n  void AssertOK();\n\nprotected:\n  UnaryOperator(UnaryOps iType, Value *S, Type *Ty,\n                const Twine &Name, Instruction *InsertBefore);\n  UnaryOperator(UnaryOps iType, Value *S, Type *Ty,\n                const Twine &Name, BasicBlock *InsertAtEnd);\n\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  UnaryOperator *cloneImpl() const;\n\npublic:\n\n  /// Construct a unary instruction, given the opcode and an operand.\n  /// Optionally (if InstBefore is specified) insert the instruction\n  /// into a BasicBlock right before the specified instruction.  The specified\n  /// Instruction is allowed to be a dereferenced end iterator.\n  ///\n  static UnaryOperator *Create(UnaryOps Op, Value *S,\n                               const Twine &Name = Twine(),\n                               Instruction *InsertBefore = nullptr);\n\n  /// Construct a unary instruction, given the opcode and an operand.\n  /// Also automatically insert this instruction to the end of the\n  /// BasicBlock specified.\n  ///\n  static UnaryOperator *Create(UnaryOps Op, Value *S,\n                               const Twine &Name,\n                               BasicBlock *InsertAtEnd);\n\n  /// These methods just forward to Create, and are useful when you\n  /// statically know what type of instruction you're going to create.  These\n  /// helpers just save some typing.\n#define HANDLE_UNARY_INST(N, OPC, CLASS) \\\n  static UnaryOperator *Create##OPC(Value *V, const Twine &Name = \"\") {\\\n    return Create(Instruction::OPC, V, Name);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_UNARY_INST(N, OPC, CLASS) \\\n  static UnaryOperator *Create##OPC(Value *V, const Twine &Name, \\\n                                    BasicBlock *BB) {\\\n    return Create(Instruction::OPC, V, Name, BB);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_UNARY_INST(N, OPC, CLASS) \\\n  static UnaryOperator *Create##OPC(Value *V, const Twine &Name, \\\n                                    Instruction *I) {\\\n    return Create(Instruction::OPC, V, Name, I);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n\n  static UnaryOperator *\n  CreateWithCopiedFlags(UnaryOps Opc, Value *V, Instruction *CopyO,\n                        const Twine &Name = \"\",\n                        Instruction *InsertBefore = nullptr) {\n    UnaryOperator *UO = Create(Opc, V, Name, InsertBefore);\n    UO->copyIRFlags(CopyO);\n    return UO;\n  }\n\n  static UnaryOperator *CreateFNegFMF(Value *Op, Instruction *FMFSource,\n                                      const Twine &Name = \"\",\n                                      Instruction *InsertBefore = nullptr) {\n    return CreateWithCopiedFlags(Instruction::FNeg, Op, FMFSource, Name,\n                                 InsertBefore);\n  }\n\n  UnaryOps getOpcode() const {\n    return static_cast<UnaryOps>(Instruction::getOpcode());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isUnaryOp();\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                           BinaryOperator Class\n//===----------------------------------------------------------------------===//\n\nclass BinaryOperator : public Instruction {\n  void AssertOK();\n\nprotected:\n  BinaryOperator(BinaryOps iType, Value *S1, Value *S2, Type *Ty,\n                 const Twine &Name, Instruction *InsertBefore);\n  BinaryOperator(BinaryOps iType, Value *S1, Value *S2, Type *Ty,\n                 const Twine &Name, BasicBlock *InsertAtEnd);\n\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  BinaryOperator *cloneImpl() const;\n\npublic:\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Construct a binary instruction, given the opcode and the two\n  /// operands.  Optionally (if InstBefore is specified) insert the instruction\n  /// into a BasicBlock right before the specified instruction.  The specified\n  /// Instruction is allowed to be a dereferenced end iterator.\n  ///\n  static BinaryOperator *Create(BinaryOps Op, Value *S1, Value *S2,\n                                const Twine &Name = Twine(),\n                                Instruction *InsertBefore = nullptr);\n\n  /// Construct a binary instruction, given the opcode and the two\n  /// operands.  Also automatically insert this instruction to the end of the\n  /// BasicBlock specified.\n  ///\n  static BinaryOperator *Create(BinaryOps Op, Value *S1, Value *S2,\n                                const Twine &Name, BasicBlock *InsertAtEnd);\n\n  /// These methods just forward to Create, and are useful when you\n  /// statically know what type of instruction you're going to create.  These\n  /// helpers just save some typing.\n#define HANDLE_BINARY_INST(N, OPC, CLASS) \\\n  static BinaryOperator *Create##OPC(Value *V1, Value *V2, \\\n                                     const Twine &Name = \"\") {\\\n    return Create(Instruction::OPC, V1, V2, Name);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_BINARY_INST(N, OPC, CLASS) \\\n  static BinaryOperator *Create##OPC(Value *V1, Value *V2, \\\n                                     const Twine &Name, BasicBlock *BB) {\\\n    return Create(Instruction::OPC, V1, V2, Name, BB);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_BINARY_INST(N, OPC, CLASS) \\\n  static BinaryOperator *Create##OPC(Value *V1, Value *V2, \\\n                                     const Twine &Name, Instruction *I) {\\\n    return Create(Instruction::OPC, V1, V2, Name, I);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n\n  static BinaryOperator *CreateWithCopiedFlags(BinaryOps Opc,\n                                               Value *V1, Value *V2,\n                                               Instruction *CopyO,\n                                               const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->copyIRFlags(CopyO);\n    return BO;\n  }\n\n  static BinaryOperator *CreateFAddFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FAdd, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFSubFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FSub, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFMulFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FMul, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFDivFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FDiv, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFRemFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FRem, V1, V2, FMFSource, Name);\n  }\n\n  static BinaryOperator *CreateNSW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->setHasNoSignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNSW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, BasicBlock *BB) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, BB);\n    BO->setHasNoSignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNSW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, Instruction *I) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, I);\n    BO->setHasNoSignedWrap(true);\n    return BO;\n  }\n\n  static BinaryOperator *CreateNUW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->setHasNoUnsignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNUW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, BasicBlock *BB) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, BB);\n    BO->setHasNoUnsignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNUW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, Instruction *I) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, I);\n    BO->setHasNoUnsignedWrap(true);\n    return BO;\n  }\n\n  static BinaryOperator *CreateExact(BinaryOps Opc, Value *V1, Value *V2,\n                                     const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->setIsExact(true);\n    return BO;\n  }\n  static BinaryOperator *CreateExact(BinaryOps Opc, Value *V1, Value *V2,\n                                     const Twine &Name, BasicBlock *BB) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, BB);\n    BO->setIsExact(true);\n    return BO;\n  }\n  static BinaryOperator *CreateExact(BinaryOps Opc, Value *V1, Value *V2,\n                                     const Twine &Name, Instruction *I) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, I);\n    BO->setIsExact(true);\n    return BO;\n  }\n\n#define DEFINE_HELPERS(OPC, NUWNSWEXACT)                                       \\\n  static BinaryOperator *Create##NUWNSWEXACT##OPC(Value *V1, Value *V2,        \\\n                                                  const Twine &Name = \"\") {    \\\n    return Create##NUWNSWEXACT(Instruction::OPC, V1, V2, Name);                \\\n  }                                                                            \\\n  static BinaryOperator *Create##NUWNSWEXACT##OPC(                             \\\n      Value *V1, Value *V2, const Twine &Name, BasicBlock *BB) {               \\\n    return Create##NUWNSWEXACT(Instruction::OPC, V1, V2, Name, BB);            \\\n  }                                                                            \\\n  static BinaryOperator *Create##NUWNSWEXACT##OPC(                             \\\n      Value *V1, Value *V2, const Twine &Name, Instruction *I) {               \\\n    return Create##NUWNSWEXACT(Instruction::OPC, V1, V2, Name, I);             \\\n  }\n\n  DEFINE_HELPERS(Add, NSW) // CreateNSWAdd\n  DEFINE_HELPERS(Add, NUW) // CreateNUWAdd\n  DEFINE_HELPERS(Sub, NSW) // CreateNSWSub\n  DEFINE_HELPERS(Sub, NUW) // CreateNUWSub\n  DEFINE_HELPERS(Mul, NSW) // CreateNSWMul\n  DEFINE_HELPERS(Mul, NUW) // CreateNUWMul\n  DEFINE_HELPERS(Shl, NSW) // CreateNSWShl\n  DEFINE_HELPERS(Shl, NUW) // CreateNUWShl\n\n  DEFINE_HELPERS(SDiv, Exact)  // CreateExactSDiv\n  DEFINE_HELPERS(UDiv, Exact)  // CreateExactUDiv\n  DEFINE_HELPERS(AShr, Exact)  // CreateExactAShr\n  DEFINE_HELPERS(LShr, Exact)  // CreateExactLShr\n\n#undef DEFINE_HELPERS\n\n  /// Helper functions to construct and inspect unary operations (NEG and NOT)\n  /// via binary operators SUB and XOR:\n  ///\n  /// Create the NEG and NOT instructions out of SUB and XOR instructions.\n  ///\n  static BinaryOperator *CreateNeg(Value *Op, const Twine &Name = \"\",\n                                   Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNeg(Value *Op, const Twine &Name,\n                                   BasicBlock *InsertAtEnd);\n  static BinaryOperator *CreateNSWNeg(Value *Op, const Twine &Name = \"\",\n                                      Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNSWNeg(Value *Op, const Twine &Name,\n                                      BasicBlock *InsertAtEnd);\n  static BinaryOperator *CreateNUWNeg(Value *Op, const Twine &Name = \"\",\n                                      Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNUWNeg(Value *Op, const Twine &Name,\n                                      BasicBlock *InsertAtEnd);\n  static BinaryOperator *CreateNot(Value *Op, const Twine &Name = \"\",\n                                   Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNot(Value *Op, const Twine &Name,\n                                   BasicBlock *InsertAtEnd);\n\n  BinaryOps getOpcode() const {\n    return static_cast<BinaryOps>(Instruction::getOpcode());\n  }\n\n  /// Exchange the two operands to this instruction.\n  /// This instruction is safe to use on any binary instruction and\n  /// does not modify the semantics of the instruction.  If the instruction\n  /// cannot be reversed (ie, it's a Div), then return true.\n  ///\n  bool swapOperands();\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isBinaryOp();\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<BinaryOperator> :\n  public FixedNumOperandTraits<BinaryOperator, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(BinaryOperator, Value)\n\n//===----------------------------------------------------------------------===//\n//                               CastInst Class\n//===----------------------------------------------------------------------===//\n\n/// This is the base class for all instructions that perform data\n/// casts. It is simply provided so that instruction category testing\n/// can be performed with code like:\n///\n/// if (isa<CastInst>(Instr)) { ... }\n/// Base class of casting instructions.\nclass CastInst : public UnaryInstruction {\nprotected:\n  /// Constructor with insert-before-instruction semantics for subclasses\n  CastInst(Type *Ty, unsigned iType, Value *S,\n           const Twine &NameStr = \"\", Instruction *InsertBefore = nullptr)\n    : UnaryInstruction(Ty, iType, S, InsertBefore) {\n    setName(NameStr);\n  }\n  /// Constructor with insert-at-end-of-block semantics for subclasses\n  CastInst(Type *Ty, unsigned iType, Value *S,\n           const Twine &NameStr, BasicBlock *InsertAtEnd)\n    : UnaryInstruction(Ty, iType, S, InsertAtEnd) {\n    setName(NameStr);\n  }\n\npublic:\n  /// Provides a way to construct any of the CastInst subclasses using an\n  /// opcode instead of the subclass's constructor. The opcode must be in the\n  /// CastOps category (Instruction::isCast(opcode) returns true). This\n  /// constructor has insert-before-instruction semantics to automatically\n  /// insert the new CastInst before InsertBefore (if it is non-null).\n  /// Construct any of the CastInst subclasses\n  static CastInst *Create(\n    Instruction::CastOps,    ///< The opcode of the cast instruction\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n  /// Provides a way to construct any of the CastInst subclasses using an\n  /// opcode instead of the subclass's constructor. The opcode must be in the\n  /// CastOps category. This constructor has insert-at-end-of-block semantics\n  /// to automatically insert the new CastInst at the end of InsertAtEnd (if\n  /// its non-null).\n  /// Construct any of the CastInst subclasses\n  static CastInst *Create(\n    Instruction::CastOps,    ///< The opcode for the cast instruction\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a ZExt or BitCast cast instruction\n  static CastInst *CreateZExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a ZExt or BitCast cast instruction\n  static CastInst *CreateZExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a SExt or BitCast cast instruction\n  static CastInst *CreateSExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a SExt or BitCast cast instruction\n  static CastInst *CreateSExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a BitCast AddrSpaceCast, or a PtrToInt cast instruction.\n  static CastInst *CreatePointerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a BitCast, AddrSpaceCast or a PtrToInt cast instruction.\n  static CastInst *CreatePointerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a BitCast or an AddrSpaceCast cast instruction.\n  static CastInst *CreatePointerBitCastOrAddrSpaceCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a BitCast or an AddrSpaceCast cast instruction.\n  static CastInst *CreatePointerBitCastOrAddrSpaceCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a BitCast, a PtrToInt, or an IntToPTr cast instruction.\n  ///\n  /// If the value is a pointer type and the destination an integer type,\n  /// creates a PtrToInt cast. If the value is an integer type and the\n  /// destination a pointer type, creates an IntToPtr cast. Otherwise, creates\n  /// a bitcast.\n  static CastInst *CreateBitOrPointerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a ZExt, BitCast, or Trunc for int -> int casts.\n  static CastInst *CreateIntegerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    bool isSigned,           ///< Whether to regard S as signed or not\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a ZExt, BitCast, or Trunc for int -> int casts.\n  static CastInst *CreateIntegerCast(\n    Value *S,                ///< The integer value to be casted (operand 0)\n    Type *Ty,          ///< The integer type to which operand is casted\n    bool isSigned,           ///< Whether to regard S as signed or not\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create an FPExt, BitCast, or FPTrunc for fp -> fp casts\n  static CastInst *CreateFPCast(\n    Value *S,                ///< The floating point value to be casted\n    Type *Ty,          ///< The floating point type to cast to\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create an FPExt, BitCast, or FPTrunc for fp -> fp casts\n  static CastInst *CreateFPCast(\n    Value *S,                ///< The floating point value to be casted\n    Type *Ty,          ///< The floating point type to cast to\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a Trunc or BitCast cast instruction\n  static CastInst *CreateTruncOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a Trunc or BitCast cast instruction\n  static CastInst *CreateTruncOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Check whether a bitcast between these types is valid\n  static bool isBitCastable(\n    Type *SrcTy, ///< The Type from which the value should be cast.\n    Type *DestTy ///< The Type to which the value should be cast.\n  );\n\n  /// Check whether a bitcast, inttoptr, or ptrtoint cast between these\n  /// types is valid and a no-op.\n  ///\n  /// This ensures that any pointer<->integer cast has enough bits in the\n  /// integer and any other cast is a bitcast.\n  static bool isBitOrNoopPointerCastable(\n      Type *SrcTy,  ///< The Type from which the value should be cast.\n      Type *DestTy, ///< The Type to which the value should be cast.\n      const DataLayout &DL);\n\n  /// Returns the opcode necessary to cast Val into Ty using usual casting\n  /// rules.\n  /// Infer the opcode for cast operand and type\n  static Instruction::CastOps getCastOpcode(\n    const Value *Val, ///< The value to cast\n    bool SrcIsSigned, ///< Whether to treat the source as signed\n    Type *Ty,   ///< The Type to which the value should be casted\n    bool DstIsSigned  ///< Whether to treate the dest. as signed\n  );\n\n  /// There are several places where we need to know if a cast instruction\n  /// only deals with integer source and destination types. To simplify that\n  /// logic, this method is provided.\n  /// @returns true iff the cast has only integral typed operand and dest type.\n  /// Determine if this is an integer-only cast.\n  bool isIntegerCast() const;\n\n  /// A lossless cast is one that does not alter the basic value. It implies\n  /// a no-op cast but is more stringent, preventing things like int->float,\n  /// long->double, or int->ptr.\n  /// @returns true iff the cast is lossless.\n  /// Determine if this is a lossless cast.\n  bool isLosslessCast() const;\n\n  /// A no-op cast is one that can be effected without changing any bits.\n  /// It implies that the source and destination types are the same size. The\n  /// DataLayout argument is to determine the pointer size when examining casts\n  /// involving Integer and Pointer types. They are no-op casts if the integer\n  /// is the same size as the pointer. However, pointer size varies with\n  /// platform.  Note that a precondition of this method is that the cast is\n  /// legal - i.e. the instruction formed with these operands would verify.\n  static bool isNoopCast(\n    Instruction::CastOps Opcode, ///< Opcode of cast\n    Type *SrcTy,         ///< SrcTy of cast\n    Type *DstTy,         ///< DstTy of cast\n    const DataLayout &DL ///< DataLayout to get the Int Ptr type from.\n  );\n\n  /// Determine if this cast is a no-op cast.\n  ///\n  /// \\param DL is the DataLayout to determine pointer size.\n  bool isNoopCast(const DataLayout &DL) const;\n\n  /// Determine how a pair of casts can be eliminated, if they can be at all.\n  /// This is a helper function for both CastInst and ConstantExpr.\n  /// @returns 0 if the CastInst pair can't be eliminated, otherwise\n  /// returns Instruction::CastOps value for a cast that can replace\n  /// the pair, casting SrcTy to DstTy.\n  /// Determine if a cast pair is eliminable\n  static unsigned isEliminableCastPair(\n    Instruction::CastOps firstOpcode,  ///< Opcode of first cast\n    Instruction::CastOps secondOpcode, ///< Opcode of second cast\n    Type *SrcTy, ///< SrcTy of 1st cast\n    Type *MidTy, ///< DstTy of 1st cast & SrcTy of 2nd cast\n    Type *DstTy, ///< DstTy of 2nd cast\n    Type *SrcIntPtrTy, ///< Integer type corresponding to Ptr SrcTy, or null\n    Type *MidIntPtrTy, ///< Integer type corresponding to Ptr MidTy, or null\n    Type *DstIntPtrTy  ///< Integer type corresponding to Ptr DstTy, or null\n  );\n\n  /// Return the opcode of this CastInst\n  Instruction::CastOps getOpcode() const {\n    return Instruction::CastOps(Instruction::getOpcode());\n  }\n\n  /// Return the source type, as a convenience\n  Type* getSrcTy() const { return getOperand(0)->getType(); }\n  /// Return the destination type, as a convenience\n  Type* getDestTy() const { return getType(); }\n\n  /// This method can be used to determine if a cast from SrcTy to DstTy using\n  /// Opcode op is valid or not.\n  /// @returns true iff the proposed cast is valid.\n  /// Determine if a cast is valid without creating one.\n  static bool castIsValid(Instruction::CastOps op, Type *SrcTy, Type *DstTy);\n  static bool castIsValid(Instruction::CastOps op, Value *S, Type *DstTy) {\n    return castIsValid(op, S->getType(), DstTy);\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isCast();\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               CmpInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class is the base class for the comparison instructions.\n/// Abstract base class of comparison instructions.\nclass CmpInst : public Instruction {\npublic:\n  /// This enumeration lists the possible predicates for CmpInst subclasses.\n  /// Values in the range 0-31 are reserved for FCmpInst, while values in the\n  /// range 32-64 are reserved for ICmpInst. This is necessary to ensure the\n  /// predicate values are not overlapping between the classes.\n  ///\n  /// Some passes (e.g. InstCombine) depend on the bit-wise characteristics of\n  /// FCMP_* values. Changing the bit patterns requires a potential change to\n  /// those passes.\n  enum Predicate : unsigned {\n    // Opcode            U L G E    Intuitive operation\n    FCMP_FALSE = 0, ///< 0 0 0 0    Always false (always folded)\n    FCMP_OEQ = 1,   ///< 0 0 0 1    True if ordered and equal\n    FCMP_OGT = 2,   ///< 0 0 1 0    True if ordered and greater than\n    FCMP_OGE = 3,   ///< 0 0 1 1    True if ordered and greater than or equal\n    FCMP_OLT = 4,   ///< 0 1 0 0    True if ordered and less than\n    FCMP_OLE = 5,   ///< 0 1 0 1    True if ordered and less than or equal\n    FCMP_ONE = 6,   ///< 0 1 1 0    True if ordered and operands are unequal\n    FCMP_ORD = 7,   ///< 0 1 1 1    True if ordered (no nans)\n    FCMP_UNO = 8,   ///< 1 0 0 0    True if unordered: isnan(X) | isnan(Y)\n    FCMP_UEQ = 9,   ///< 1 0 0 1    True if unordered or equal\n    FCMP_UGT = 10,  ///< 1 0 1 0    True if unordered or greater than\n    FCMP_UGE = 11,  ///< 1 0 1 1    True if unordered, greater than, or equal\n    FCMP_ULT = 12,  ///< 1 1 0 0    True if unordered or less than\n    FCMP_ULE = 13,  ///< 1 1 0 1    True if unordered, less than, or equal\n    FCMP_UNE = 14,  ///< 1 1 1 0    True if unordered or not equal\n    FCMP_TRUE = 15, ///< 1 1 1 1    Always true (always folded)\n    FIRST_FCMP_PREDICATE = FCMP_FALSE,\n    LAST_FCMP_PREDICATE = FCMP_TRUE,\n    BAD_FCMP_PREDICATE = FCMP_TRUE + 1,\n    ICMP_EQ = 32,  ///< equal\n    ICMP_NE = 33,  ///< not equal\n    ICMP_UGT = 34, ///< unsigned greater than\n    ICMP_UGE = 35, ///< unsigned greater or equal\n    ICMP_ULT = 36, ///< unsigned less than\n    ICMP_ULE = 37, ///< unsigned less or equal\n    ICMP_SGT = 38, ///< signed greater than\n    ICMP_SGE = 39, ///< signed greater or equal\n    ICMP_SLT = 40, ///< signed less than\n    ICMP_SLE = 41, ///< signed less or equal\n    FIRST_ICMP_PREDICATE = ICMP_EQ,\n    LAST_ICMP_PREDICATE = ICMP_SLE,\n    BAD_ICMP_PREDICATE = ICMP_SLE + 1\n  };\n  using PredicateField =\n      Bitfield::Element<Predicate, 0, 6, LAST_ICMP_PREDICATE>;\n\nprotected:\n  CmpInst(Type *ty, Instruction::OtherOps op, Predicate pred,\n          Value *LHS, Value *RHS, const Twine &Name = \"\",\n          Instruction *InsertBefore = nullptr,\n          Instruction *FlagsSource = nullptr);\n\n  CmpInst(Type *ty, Instruction::OtherOps op, Predicate pred,\n          Value *LHS, Value *RHS, const Twine &Name,\n          BasicBlock *InsertAtEnd);\n\npublic:\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  /// Construct a compare instruction, given the opcode, the predicate and\n  /// the two operands.  Optionally (if InstBefore is specified) insert the\n  /// instruction into a BasicBlock right before the specified instruction.\n  /// The specified Instruction is allowed to be a dereferenced end iterator.\n  /// Create a CmpInst\n  static CmpInst *Create(OtherOps Op,\n                         Predicate predicate, Value *S1,\n                         Value *S2, const Twine &Name = \"\",\n                         Instruction *InsertBefore = nullptr);\n\n  /// Construct a compare instruction, given the opcode, the predicate and the\n  /// two operands.  Also automatically insert this instruction to the end of\n  /// the BasicBlock specified.\n  /// Create a CmpInst\n  static CmpInst *Create(OtherOps Op, Predicate predicate, Value *S1,\n                         Value *S2, const Twine &Name, BasicBlock *InsertAtEnd);\n\n  /// Get the opcode casted to the right type\n  OtherOps getOpcode() const {\n    return static_cast<OtherOps>(Instruction::getOpcode());\n  }\n\n  /// Return the predicate for this instruction.\n  Predicate getPredicate() const { return getSubclassData<PredicateField>(); }\n\n  /// Set the predicate for this instruction to the specified value.\n  void setPredicate(Predicate P) { setSubclassData<PredicateField>(P); }\n\n  static bool isFPPredicate(Predicate P) {\n    static_assert(FIRST_FCMP_PREDICATE == 0,\n                  \"FIRST_FCMP_PREDICATE is required to be 0\");\n    return P <= LAST_FCMP_PREDICATE;\n  }\n\n  static bool isIntPredicate(Predicate P) {\n    return P >= FIRST_ICMP_PREDICATE && P <= LAST_ICMP_PREDICATE;\n  }\n\n  static StringRef getPredicateName(Predicate P);\n\n  bool isFPPredicate() const { return isFPPredicate(getPredicate()); }\n  bool isIntPredicate() const { return isIntPredicate(getPredicate()); }\n\n  /// For example, EQ -> NE, UGT -> ULE, SLT -> SGE,\n  ///              OEQ -> UNE, UGT -> OLE, OLT -> UGE, etc.\n  /// @returns the inverse predicate for the instruction's current predicate.\n  /// Return the inverse of the instruction's predicate.\n  Predicate getInversePredicate() const {\n    return getInversePredicate(getPredicate());\n  }\n\n  /// For example, EQ -> NE, UGT -> ULE, SLT -> SGE,\n  ///              OEQ -> UNE, UGT -> OLE, OLT -> UGE, etc.\n  /// @returns the inverse predicate for predicate provided in \\p pred.\n  /// Return the inverse of a given predicate\n  static Predicate getInversePredicate(Predicate pred);\n\n  /// For example, EQ->EQ, SLE->SGE, ULT->UGT,\n  ///              OEQ->OEQ, ULE->UGE, OLT->OGT, etc.\n  /// @returns the predicate that would be the result of exchanging the two\n  /// operands of the CmpInst instruction without changing the result\n  /// produced.\n  /// Return the predicate as if the operands were swapped\n  Predicate getSwappedPredicate() const {\n    return getSwappedPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// Return the predicate as if the operands were swapped.\n  static Predicate getSwappedPredicate(Predicate pred);\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns true if the comparison predicate is strict, false otherwise.\n  static bool isStrictPredicate(Predicate predicate);\n\n  /// @returns true if the comparison predicate is strict, false otherwise.\n  /// Determine if this instruction is using an strict comparison predicate.\n  bool isStrictPredicate() const { return isStrictPredicate(getPredicate()); }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns true if the comparison predicate is non-strict, false otherwise.\n  static bool isNonStrictPredicate(Predicate predicate);\n\n  /// @returns true if the comparison predicate is non-strict, false otherwise.\n  /// Determine if this instruction is using an non-strict comparison predicate.\n  bool isNonStrictPredicate() const {\n    return isNonStrictPredicate(getPredicate());\n  }\n\n  /// For example, SGE -> SGT, SLE -> SLT, ULE -> ULT, UGE -> UGT.\n  /// Returns the strict version of non-strict comparisons.\n  Predicate getStrictPredicate() const {\n    return getStrictPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns the strict version of comparison provided in \\p pred.\n  /// If \\p pred is not a strict comparison predicate, returns \\p pred.\n  /// Returns the strict version of non-strict comparisons.\n  static Predicate getStrictPredicate(Predicate pred);\n\n  /// For example, SGT -> SGE, SLT -> SLE, ULT -> ULE, UGT -> UGE.\n  /// Returns the non-strict version of strict comparisons.\n  Predicate getNonStrictPredicate() const {\n    return getNonStrictPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns the non-strict version of comparison provided in \\p pred.\n  /// If \\p pred is not a strict comparison predicate, returns \\p pred.\n  /// Returns the non-strict version of strict comparisons.\n  static Predicate getNonStrictPredicate(Predicate pred);\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// Return the flipped strictness of predicate\n  static Predicate getFlippedStrictnessPredicate(Predicate pred);\n\n  /// For predicate of kind \"is X or equal to 0\" returns the predicate \"is X\".\n  /// For predicate of kind \"is X\" returns the predicate \"is X or equal to 0\".\n  /// does not support other kind of predicates.\n  /// @returns the predicate that does not contains is equal to zero if\n  /// it had and vice versa.\n  /// Return the flipped strictness of predicate\n  Predicate getFlippedStrictnessPredicate() const {\n    return getFlippedStrictnessPredicate(getPredicate());\n  }\n\n  /// Provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// This is just a convenience that dispatches to the subclasses.\n  /// Swap the operands and adjust predicate accordingly to retain\n  /// the same comparison.\n  void swapOperands();\n\n  /// This is just a convenience that dispatches to the subclasses.\n  /// Determine if this CmpInst is commutative.\n  bool isCommutative() const;\n\n  /// Determine if this is an equals/not equals predicate.\n  /// This is a static version that you can use without an instruction\n  /// available.\n  static bool isEquality(Predicate pred);\n\n  /// Determine if this is an equals/not equals predicate.\n  bool isEquality() const { return isEquality(getPredicate()); }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  static bool isRelational(Predicate P) { return !isEquality(P); }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  bool isRelational() const { return !isEquality(); }\n\n  /// @returns true if the comparison is signed, false otherwise.\n  /// Determine if this instruction is using a signed comparison.\n  bool isSigned() const {\n    return isSigned(getPredicate());\n  }\n\n  /// @returns true if the comparison is unsigned, false otherwise.\n  /// Determine if this instruction is using an unsigned comparison.\n  bool isUnsigned() const {\n    return isUnsigned(getPredicate());\n  }\n\n  /// For example, ULT->SLT, ULE->SLE, UGT->SGT, UGE->SGE, SLT->Failed assert\n  /// @returns the signed version of the unsigned predicate pred.\n  /// return the signed version of a predicate\n  static Predicate getSignedPredicate(Predicate pred);\n\n  /// For example, ULT->SLT, ULE->SLE, UGT->SGT, UGE->SGE, SLT->Failed assert\n  /// @returns the signed version of the predicate for this instruction (which\n  /// has to be an unsigned predicate).\n  /// return the signed version of a predicate\n  Predicate getSignedPredicate() {\n    return getSignedPredicate(getPredicate());\n  }\n\n  /// For example, SLT->ULT, SLE->ULE, SGT->UGT, SGE->UGE, ULT->Failed assert\n  /// @returns the unsigned version of the signed predicate pred.\n  static Predicate getUnsignedPredicate(Predicate pred);\n\n  /// For example, SLT->ULT, SLE->ULE, SGT->UGT, SGE->UGE, ULT->Failed assert\n  /// @returns the unsigned version of the predicate for this instruction (which\n  /// has to be an signed predicate).\n  /// return the unsigned version of a predicate\n  Predicate getUnsignedPredicate() {\n    return getUnsignedPredicate(getPredicate());\n  }\n\n  /// For example, SLT->ULT, ULT->SLT, SLE->ULE, ULE->SLE, EQ->Failed assert\n  /// @returns the unsigned version of the signed predicate pred or\n  ///          the signed version of the signed predicate pred.\n  static Predicate getFlippedSignednessPredicate(Predicate pred);\n\n  /// For example, SLT->ULT, ULT->SLT, SLE->ULE, ULE->SLE, EQ->Failed assert\n  /// @returns the unsigned version of the signed predicate pred or\n  ///          the signed version of the signed predicate pred.\n  Predicate getFlippedSignednessPredicate() {\n    return getFlippedSignednessPredicate(getPredicate());\n  }\n\n  /// This is just a convenience.\n  /// Determine if this is true when both operands are the same.\n  bool isTrueWhenEqual() const {\n    return isTrueWhenEqual(getPredicate());\n  }\n\n  /// This is just a convenience.\n  /// Determine if this is false when both operands are the same.\n  bool isFalseWhenEqual() const {\n    return isFalseWhenEqual(getPredicate());\n  }\n\n  /// @returns true if the predicate is unsigned, false otherwise.\n  /// Determine if the predicate is an unsigned operation.\n  static bool isUnsigned(Predicate predicate);\n\n  /// @returns true if the predicate is signed, false otherwise.\n  /// Determine if the predicate is an signed operation.\n  static bool isSigned(Predicate predicate);\n\n  /// Determine if the predicate is an ordered operation.\n  static bool isOrdered(Predicate predicate);\n\n  /// Determine if the predicate is an unordered operation.\n  static bool isUnordered(Predicate predicate);\n\n  /// Determine if the predicate is true when comparing a value with itself.\n  static bool isTrueWhenEqual(Predicate predicate);\n\n  /// Determine if the predicate is false when comparing a value with itself.\n  static bool isFalseWhenEqual(Predicate predicate);\n\n  /// Determine if Pred1 implies Pred2 is true when two compares have matching\n  /// operands.\n  static bool isImpliedTrueByMatchingCmp(Predicate Pred1, Predicate Pred2);\n\n  /// Determine if Pred1 implies Pred2 is false when two compares have matching\n  /// operands.\n  static bool isImpliedFalseByMatchingCmp(Predicate Pred1, Predicate Pred2);\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ICmp ||\n           I->getOpcode() == Instruction::FCmp;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  /// Create a result type for fcmp/icmp\n  static Type* makeCmpResultType(Type* opnd_type) {\n    if (VectorType* vt = dyn_cast<VectorType>(opnd_type)) {\n      return VectorType::get(Type::getInt1Ty(opnd_type->getContext()),\n                             vt->getElementCount());\n    }\n    return Type::getInt1Ty(opnd_type->getContext());\n  }\n\nprivate:\n  // Shadow Value::setValueSubclassData with a private forwarding method so that\n  // subclasses cannot accidentally use it.\n  void setValueSubclassData(unsigned short D) {\n    Value::setValueSubclassData(D);\n  }\n};\n\n// FIXME: these are redundant if CmpInst < BinaryOperator\ntemplate <>\nstruct OperandTraits<CmpInst> : public FixedNumOperandTraits<CmpInst, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CmpInst, Value)\n\n/// A lightweight accessor for an operand bundle meant to be passed\n/// around by value.\nstruct OperandBundleUse {\n  ArrayRef<Use> Inputs;\n\n  OperandBundleUse() = default;\n  explicit OperandBundleUse(StringMapEntry<uint32_t> *Tag, ArrayRef<Use> Inputs)\n      : Inputs(Inputs), Tag(Tag) {}\n\n  /// Return true if the operand at index \\p Idx in this operand bundle\n  /// has the attribute A.\n  bool operandHasAttr(unsigned Idx, Attribute::AttrKind A) const {\n    if (isDeoptOperandBundle())\n      if (A == Attribute::ReadOnly || A == Attribute::NoCapture)\n        return Inputs[Idx]->getType()->isPointerTy();\n\n    // Conservative answer:  no operands have any attributes.\n    return false;\n  }\n\n  /// Return the tag of this operand bundle as a string.\n  StringRef getTagName() const {\n    return Tag->getKey();\n  }\n\n  /// Return the tag of this operand bundle as an integer.\n  ///\n  /// Operand bundle tags are interned by LLVMContextImpl::getOrInsertBundleTag,\n  /// and this function returns the unique integer getOrInsertBundleTag\n  /// associated the tag of this operand bundle to.\n  uint32_t getTagID() const {\n    return Tag->getValue();\n  }\n\n  /// Return true if this is a \"deopt\" operand bundle.\n  bool isDeoptOperandBundle() const {\n    return getTagID() == LLVMContext::OB_deopt;\n  }\n\n  /// Return true if this is a \"funclet\" operand bundle.\n  bool isFuncletOperandBundle() const {\n    return getTagID() == LLVMContext::OB_funclet;\n  }\n\n  /// Return true if this is a \"cfguardtarget\" operand bundle.\n  bool isCFGuardTargetOperandBundle() const {\n    return getTagID() == LLVMContext::OB_cfguardtarget;\n  }\n\nprivate:\n  /// Pointer to an entry in LLVMContextImpl::getOrInsertBundleTag.\n  StringMapEntry<uint32_t> *Tag;\n};\n\n/// A container for an operand bundle being viewed as a set of values\n/// rather than a set of uses.\n///\n/// Unlike OperandBundleUse, OperandBundleDefT owns the memory it carries, and\n/// so it is possible to create and pass around \"self-contained\" instances of\n/// OperandBundleDef and ConstOperandBundleDef.\ntemplate <typename InputTy> class OperandBundleDefT {\n  std::string Tag;\n  std::vector<InputTy> Inputs;\n\npublic:\n  explicit OperandBundleDefT(std::string Tag, std::vector<InputTy> Inputs)\n      : Tag(std::move(Tag)), Inputs(std::move(Inputs)) {}\n  explicit OperandBundleDefT(std::string Tag, ArrayRef<InputTy> Inputs)\n      : Tag(std::move(Tag)), Inputs(Inputs) {}\n\n  explicit OperandBundleDefT(const OperandBundleUse &OBU) {\n    Tag = std::string(OBU.getTagName());\n    llvm::append_range(Inputs, OBU.Inputs);\n  }\n\n  ArrayRef<InputTy> inputs() const { return Inputs; }\n\n  using input_iterator = typename std::vector<InputTy>::const_iterator;\n\n  size_t input_size() const { return Inputs.size(); }\n  input_iterator input_begin() const { return Inputs.begin(); }\n  input_iterator input_end() const { return Inputs.end(); }\n\n  StringRef getTag() const { return Tag; }\n};\n\nusing OperandBundleDef = OperandBundleDefT<Value *>;\nusing ConstOperandBundleDef = OperandBundleDefT<const Value *>;\n\n//===----------------------------------------------------------------------===//\n//                               CallBase Class\n//===----------------------------------------------------------------------===//\n\n/// Base class for all callable instructions (InvokeInst and CallInst)\n/// Holds everything related to calling a function.\n///\n/// All call-like instructions are required to use a common operand layout:\n/// - Zero or more arguments to the call,\n/// - Zero or more operand bundles with zero or more operand inputs each\n///   bundle,\n/// - Zero or more subclass controlled operands\n/// - The called function.\n///\n/// This allows this base class to easily access the called function and the\n/// start of the arguments without knowing how many other operands a particular\n/// subclass requires. Note that accessing the end of the argument list isn't\n/// as cheap as most other operations on the base class.\nclass CallBase : public Instruction {\nprotected:\n  // The first two bits are reserved by CallInst for fast retrieval,\n  using CallInstReservedField = Bitfield::Element<unsigned, 0, 2>;\n  using CallingConvField =\n      Bitfield::Element<CallingConv::ID, CallInstReservedField::NextBit, 10,\n                        CallingConv::MaxID>;\n  static_assert(\n      Bitfield::areContiguous<CallInstReservedField, CallingConvField>(),\n      \"Bitfields must be contiguous\");\n\n  /// The last operand is the called operand.\n  static constexpr int CalledOperandOpEndIdx = -1;\n\n  AttributeList Attrs; ///< parameter attributes for callable\n  FunctionType *FTy;\n\n  template <class... ArgsTy>\n  CallBase(AttributeList const &A, FunctionType *FT, ArgsTy &&... Args)\n      : Instruction(std::forward<ArgsTy>(Args)...), Attrs(A), FTy(FT) {}\n\n  using Instruction::Instruction;\n\n  bool hasDescriptor() const { return Value::HasDescriptor; }\n\n  unsigned getNumSubclassExtraOperands() const {\n    switch (getOpcode()) {\n    case Instruction::Call:\n      return 0;\n    case Instruction::Invoke:\n      return 2;\n    case Instruction::CallBr:\n      return getNumSubclassExtraOperandsDynamic();\n    }\n    llvm_unreachable(\"Invalid opcode!\");\n  }\n\n  /// Get the number of extra operands for instructions that don't have a fixed\n  /// number of extra operands.\n  unsigned getNumSubclassExtraOperandsDynamic() const;\n\npublic:\n  using Instruction::getContext;\n\n  /// Create a clone of \\p CB with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned call instruction is identical \\p CB in every way except that\n  /// the operand bundles for the new instruction are set to the operand bundles\n  /// in \\p Bundles.\n  static CallBase *Create(CallBase *CB, ArrayRef<OperandBundleDef> Bundles,\n                          Instruction *InsertPt = nullptr);\n\n  /// Create a clone of \\p CB with the operand bundle with the tag matching\n  /// \\p Bundle's tag replaced with Bundle, and insert it before \\p InsertPt.\n  ///\n  /// The returned call instruction is identical \\p CI in every way except that\n  /// the specified operand bundle has been replaced.\n  static CallBase *Create(CallBase *CB,\n                          OperandBundleDef Bundle,\n                          Instruction *InsertPt = nullptr);\n\n  /// Create a clone of \\p CB with operand bundle \\p OB added.\n  static CallBase *addOperandBundle(CallBase *CB, uint32_t ID,\n                                    OperandBundleDef OB,\n                                    Instruction *InsertPt = nullptr);\n\n  /// Create a clone of \\p CB with operand bundle \\p ID removed.\n  static CallBase *removeOperandBundle(CallBase *CB, uint32_t ID,\n                                       Instruction *InsertPt = nullptr);\n\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Call ||\n           I->getOpcode() == Instruction::Invoke ||\n           I->getOpcode() == Instruction::CallBr;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  FunctionType *getFunctionType() const { return FTy; }\n\n  void mutateFunctionType(FunctionType *FTy) {\n    Value::mutateType(FTy->getReturnType());\n    this->FTy = FTy;\n  }\n\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// data_operands_begin/data_operands_end - Return iterators iterating over\n  /// the call / invoke argument list and bundle operands.  For invokes, this is\n  /// the set of instruction operands except the invoke target and the two\n  /// successor blocks; and for calls this is the set of instruction operands\n  /// except the call target.\n  User::op_iterator data_operands_begin() { return op_begin(); }\n  User::const_op_iterator data_operands_begin() const {\n    return const_cast<CallBase *>(this)->data_operands_begin();\n  }\n  User::op_iterator data_operands_end() {\n    // Walk from the end of the operands over the called operand and any\n    // subclass operands.\n    return op_end() - getNumSubclassExtraOperands() - 1;\n  }\n  User::const_op_iterator data_operands_end() const {\n    return const_cast<CallBase *>(this)->data_operands_end();\n  }\n  iterator_range<User::op_iterator> data_ops() {\n    return make_range(data_operands_begin(), data_operands_end());\n  }\n  iterator_range<User::const_op_iterator> data_ops() const {\n    return make_range(data_operands_begin(), data_operands_end());\n  }\n  bool data_operands_empty() const {\n    return data_operands_end() == data_operands_begin();\n  }\n  unsigned data_operands_size() const {\n    return std::distance(data_operands_begin(), data_operands_end());\n  }\n\n  bool isDataOperand(const Use *U) const {\n    assert(this == U->getUser() &&\n           \"Only valid to query with a use of this instruction!\");\n    return data_operands_begin() <= U && U < data_operands_end();\n  }\n  bool isDataOperand(Value::const_user_iterator UI) const {\n    return isDataOperand(&UI.getUse());\n  }\n\n  /// Given a value use iterator, return the data operand corresponding to it.\n  /// Iterator must actually correspond to a data operand.\n  unsigned getDataOperandNo(Value::const_user_iterator UI) const {\n    return getDataOperandNo(&UI.getUse());\n  }\n\n  /// Given a use for a data operand, get the data operand number that\n  /// corresponds to it.\n  unsigned getDataOperandNo(const Use *U) const {\n    assert(isDataOperand(U) && \"Data operand # out of range!\");\n    return U - data_operands_begin();\n  }\n\n  /// Return the iterator pointing to the beginning of the argument list.\n  User::op_iterator arg_begin() { return op_begin(); }\n  User::const_op_iterator arg_begin() const {\n    return const_cast<CallBase *>(this)->arg_begin();\n  }\n\n  /// Return the iterator pointing to the end of the argument list.\n  User::op_iterator arg_end() {\n    // From the end of the data operands, walk backwards past the bundle\n    // operands.\n    return data_operands_end() - getNumTotalBundleOperands();\n  }\n  User::const_op_iterator arg_end() const {\n    return const_cast<CallBase *>(this)->arg_end();\n  }\n\n  /// Iteration adapter for range-for loops.\n  iterator_range<User::op_iterator> args() {\n    return make_range(arg_begin(), arg_end());\n  }\n  iterator_range<User::const_op_iterator> args() const {\n    return make_range(arg_begin(), arg_end());\n  }\n  bool arg_empty() const { return arg_end() == arg_begin(); }\n  unsigned arg_size() const { return arg_end() - arg_begin(); }\n\n  // Legacy API names that duplicate the above and will be removed once users\n  // are migrated.\n  iterator_range<User::op_iterator> arg_operands() {\n    return make_range(arg_begin(), arg_end());\n  }\n  iterator_range<User::const_op_iterator> arg_operands() const {\n    return make_range(arg_begin(), arg_end());\n  }\n  unsigned getNumArgOperands() const { return arg_size(); }\n\n  Value *getArgOperand(unsigned i) const {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    return getOperand(i);\n  }\n\n  void setArgOperand(unsigned i, Value *v) {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    setOperand(i, v);\n  }\n\n  /// Wrappers for getting the \\c Use of a call argument.\n  const Use &getArgOperandUse(unsigned i) const {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    return User::getOperandUse(i);\n  }\n  Use &getArgOperandUse(unsigned i) {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    return User::getOperandUse(i);\n  }\n\n  bool isArgOperand(const Use *U) const {\n    assert(this == U->getUser() &&\n           \"Only valid to query with a use of this instruction!\");\n    return arg_begin() <= U && U < arg_end();\n  }\n  bool isArgOperand(Value::const_user_iterator UI) const {\n    return isArgOperand(&UI.getUse());\n  }\n\n  /// Given a use for a arg operand, get the arg operand number that\n  /// corresponds to it.\n  unsigned getArgOperandNo(const Use *U) const {\n    assert(isArgOperand(U) && \"Arg operand # out of range!\");\n    return U - arg_begin();\n  }\n\n  /// Given a value use iterator, return the arg operand number corresponding to\n  /// it. Iterator must actually correspond to a data operand.\n  unsigned getArgOperandNo(Value::const_user_iterator UI) const {\n    return getArgOperandNo(&UI.getUse());\n  }\n\n  /// Returns true if this CallSite passes the given Value* as an argument to\n  /// the called function.\n  bool hasArgument(const Value *V) const {\n    return llvm::is_contained(args(), V);\n  }\n\n  Value *getCalledOperand() const { return Op<CalledOperandOpEndIdx>(); }\n\n  const Use &getCalledOperandUse() const { return Op<CalledOperandOpEndIdx>(); }\n  Use &getCalledOperandUse() { return Op<CalledOperandOpEndIdx>(); }\n\n  /// Returns the function called, or null if this is an\n  /// indirect function invocation.\n  Function *getCalledFunction() const {\n    return dyn_cast_or_null<Function>(getCalledOperand());\n  }\n\n  /// Return true if the callsite is an indirect call.\n  bool isIndirectCall() const;\n\n  /// Determine whether the passed iterator points to the callee operand's Use.\n  bool isCallee(Value::const_user_iterator UI) const {\n    return isCallee(&UI.getUse());\n  }\n\n  /// Determine whether this Use is the callee operand's Use.\n  bool isCallee(const Use *U) const { return &getCalledOperandUse() == U; }\n\n  /// Helper to get the caller (the parent function).\n  Function *getCaller();\n  const Function *getCaller() const {\n    return const_cast<CallBase *>(this)->getCaller();\n  }\n\n  /// Tests if this call site must be tail call optimized. Only a CallInst can\n  /// be tail call optimized.\n  bool isMustTailCall() const;\n\n  /// Tests if this call site is marked as a tail call.\n  bool isTailCall() const;\n\n  /// Returns the intrinsic ID of the intrinsic called or\n  /// Intrinsic::not_intrinsic if the called function is not an intrinsic, or if\n  /// this is an indirect call.\n  Intrinsic::ID getIntrinsicID() const;\n\n  void setCalledOperand(Value *V) { Op<CalledOperandOpEndIdx>() = V; }\n\n  /// Sets the function called, including updating the function type.\n  void setCalledFunction(Function *Fn) {\n    setCalledFunction(Fn->getFunctionType(), Fn);\n  }\n\n  /// Sets the function called, including updating the function type.\n  void setCalledFunction(FunctionCallee Fn) {\n    setCalledFunction(Fn.getFunctionType(), Fn.getCallee());\n  }\n\n  /// Sets the function called, including updating to the specified function\n  /// type.\n  void setCalledFunction(FunctionType *FTy, Value *Fn) {\n    this->FTy = FTy;\n    assert(FTy == cast<FunctionType>(\n                      cast<PointerType>(Fn->getType())->getElementType()));\n    // This function doesn't mutate the return type, only the function\n    // type. Seems broken, but I'm just gonna stick an assert in for now.\n    assert(getType() == FTy->getReturnType());\n    setCalledOperand(Fn);\n  }\n\n  CallingConv::ID getCallingConv() const {\n    return getSubclassData<CallingConvField>();\n  }\n\n  void setCallingConv(CallingConv::ID CC) {\n    setSubclassData<CallingConvField>(CC);\n  }\n\n  /// Check if this call is an inline asm statement.\n  bool isInlineAsm() const { return isa<InlineAsm>(getCalledOperand()); }\n\n  /// \\name Attribute API\n  ///\n  /// These methods access and modify attributes on this call (including\n  /// looking through to the attributes on the called function when necessary).\n  ///@{\n\n  /// Return the parameter attributes for this call.\n  ///\n  AttributeList getAttributes() const { return Attrs; }\n\n  /// Set the parameter attributes for this call.\n  ///\n  void setAttributes(AttributeList A) { Attrs = A; }\n\n  /// Determine whether this call has the given attribute. If it does not\n  /// then determine if the called function has the attribute, but only if\n  /// the attribute is allowed for the call.\n  bool hasFnAttr(Attribute::AttrKind Kind) const {\n    assert(Kind != Attribute::NoBuiltin &&\n           \"Use CallBase::isNoBuiltin() to check for Attribute::NoBuiltin\");\n    return hasFnAttrImpl(Kind);\n  }\n\n  /// Determine whether this call has the given attribute. If it does not\n  /// then determine if the called function has the attribute, but only if\n  /// the attribute is allowed for the call.\n  bool hasFnAttr(StringRef Kind) const { return hasFnAttrImpl(Kind); }\n\n  /// adds the attribute to the list of attributes.\n  void addAttribute(unsigned i, Attribute::AttrKind Kind) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addAttribute(getContext(), i, Kind);\n    setAttributes(PAL);\n  }\n\n  /// adds the attribute to the list of attributes.\n  void addAttribute(unsigned i, Attribute Attr) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addAttribute(getContext(), i, Attr);\n    setAttributes(PAL);\n  }\n\n  /// Adds the attribute to the indicated argument\n  void addParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addParamAttribute(getContext(), ArgNo, Kind);\n    setAttributes(PAL);\n  }\n\n  /// Adds the attribute to the indicated argument\n  void addParamAttr(unsigned ArgNo, Attribute Attr) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addParamAttribute(getContext(), ArgNo, Attr);\n    setAttributes(PAL);\n  }\n\n  /// removes the attribute from the list of attributes.\n  void removeAttribute(unsigned i, Attribute::AttrKind Kind) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeAttribute(getContext(), i, Kind);\n    setAttributes(PAL);\n  }\n\n  /// removes the attribute from the list of attributes.\n  void removeAttribute(unsigned i, StringRef Kind) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeAttribute(getContext(), i, Kind);\n    setAttributes(PAL);\n  }\n\n  void removeAttributes(unsigned i, const AttrBuilder &Attrs) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeAttributes(getContext(), i, Attrs);\n    setAttributes(PAL);\n  }\n\n  /// Removes the attribute from the given argument\n  void removeParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeParamAttribute(getContext(), ArgNo, Kind);\n    setAttributes(PAL);\n  }\n\n  /// Removes the attribute from the given argument\n  void removeParamAttr(unsigned ArgNo, StringRef Kind) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeParamAttribute(getContext(), ArgNo, Kind);\n    setAttributes(PAL);\n  }\n\n  /// adds the dereferenceable attribute to the list of attributes.\n  void addDereferenceableAttr(unsigned i, uint64_t Bytes) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addDereferenceableAttr(getContext(), i, Bytes);\n    setAttributes(PAL);\n  }\n\n  /// adds the dereferenceable_or_null attribute to the list of\n  /// attributes.\n  void addDereferenceableOrNullAttr(unsigned i, uint64_t Bytes) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addDereferenceableOrNullAttr(getContext(), i, Bytes);\n    setAttributes(PAL);\n  }\n\n  /// Determine whether the return value has the given attribute.\n  bool hasRetAttr(Attribute::AttrKind Kind) const {\n    return hasRetAttrImpl(Kind);\n  }\n  /// Determine whether the return value has the given attribute.\n  bool hasRetAttr(StringRef Kind) const { return hasRetAttrImpl(Kind); }\n\n  /// Determine whether the argument or parameter has the given attribute.\n  bool paramHasAttr(unsigned ArgNo, Attribute::AttrKind Kind) const;\n\n  /// Get the attribute of a given kind at a position.\n  Attribute getAttribute(unsigned i, Attribute::AttrKind Kind) const {\n    return getAttributes().getAttribute(i, Kind);\n  }\n\n  /// Get the attribute of a given kind at a position.\n  Attribute getAttribute(unsigned i, StringRef Kind) const {\n    return getAttributes().getAttribute(i, Kind);\n  }\n\n  /// Get the attribute of a given kind from a given arg\n  Attribute getParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) const {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    return getAttributes().getParamAttr(ArgNo, Kind);\n  }\n\n  /// Get the attribute of a given kind from a given arg\n  Attribute getParamAttr(unsigned ArgNo, StringRef Kind) const {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    return getAttributes().getParamAttr(ArgNo, Kind);\n  }\n\n  /// Return true if the data operand at index \\p i has the attribute \\p\n  /// A.\n  ///\n  /// Data operands include call arguments and values used in operand bundles,\n  /// but does not include the callee operand.  This routine dispatches to the\n  /// underlying AttributeList or the OperandBundleUser as appropriate.\n  ///\n  /// The index \\p i is interpreted as\n  ///\n  ///  \\p i == Attribute::ReturnIndex  -> the return value\n  ///  \\p i in [1, arg_size + 1)  -> argument number (\\p i - 1)\n  ///  \\p i in [arg_size + 1, data_operand_size + 1) -> bundle operand at index\n  ///     (\\p i - 1) in the operand list.\n  bool dataOperandHasImpliedAttr(unsigned i, Attribute::AttrKind Kind) const {\n    // Note that we have to add one because `i` isn't zero-indexed.\n    assert(i < (getNumArgOperands() + getNumTotalBundleOperands() + 1) &&\n           \"Data operand index out of bounds!\");\n\n    // The attribute A can either be directly specified, if the operand in\n    // question is a call argument; or be indirectly implied by the kind of its\n    // containing operand bundle, if the operand is a bundle operand.\n\n    if (i == AttributeList::ReturnIndex)\n      return hasRetAttr(Kind);\n\n    // FIXME: Avoid these i - 1 calculations and update the API to use\n    // zero-based indices.\n    if (i < (getNumArgOperands() + 1))\n      return paramHasAttr(i - 1, Kind);\n\n    assert(hasOperandBundles() && i >= (getBundleOperandsStartIndex() + 1) &&\n           \"Must be either a call argument or an operand bundle!\");\n    return bundleOperandHasAttr(i - 1, Kind);\n  }\n\n  /// Determine whether this data operand is not captured.\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool doesNotCapture(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::NoCapture);\n  }\n\n  /// Determine whether this argument is passed by value.\n  bool isByValArgument(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::ByVal);\n  }\n\n  /// Determine whether this argument is passed in an alloca.\n  bool isInAllocaArgument(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::InAlloca);\n  }\n\n  /// Determine whether this argument is passed by value, in an alloca, or is\n  /// preallocated.\n  bool isPassPointeeByValueArgument(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::ByVal) ||\n           paramHasAttr(ArgNo, Attribute::InAlloca) ||\n           paramHasAttr(ArgNo, Attribute::Preallocated);\n  }\n\n  /// Determine whether passing undef to this argument is undefined behavior.\n  /// If passing undef to this argument is UB, passing poison is UB as well\n  /// because poison is more undefined than undef.\n  bool isPassingUndefUB(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::NoUndef) ||\n           // dereferenceable implies noundef.\n           paramHasAttr(ArgNo, Attribute::Dereferenceable) ||\n           // dereferenceable implies noundef, and null is a well-defined value.\n           paramHasAttr(ArgNo, Attribute::DereferenceableOrNull);\n  }\n\n  /// Determine if there are is an inalloca argument. Only the last argument can\n  /// have the inalloca attribute.\n  bool hasInAllocaArgument() const {\n    return !arg_empty() && paramHasAttr(arg_size() - 1, Attribute::InAlloca);\n  }\n\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool doesNotAccessMemory(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadNone);\n  }\n\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool onlyReadsMemory(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadOnly) ||\n           dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadNone);\n  }\n\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool doesNotReadMemory(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::WriteOnly) ||\n           dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadNone);\n  }\n\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned getRetAlignment() const,\n                            \"Use getRetAlign() instead\") {\n    if (const auto MA = Attrs.getRetAlignment())\n      return MA->value();\n    return 0;\n  }\n\n  /// Extract the alignment of the return value.\n  MaybeAlign getRetAlign() const { return Attrs.getRetAlignment(); }\n\n  /// Extract the alignment for a call or parameter (0=unknown).\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned getParamAlignment(unsigned ArgNo) const,\n                            \"Use getParamAlign() instead\") {\n    if (const auto MA = Attrs.getParamAlignment(ArgNo))\n      return MA->value();\n    return 0;\n  }\n\n  /// Extract the alignment for a call or parameter (0=unknown).\n  MaybeAlign getParamAlign(unsigned ArgNo) const {\n    return Attrs.getParamAlignment(ArgNo);\n  }\n\n  /// Extract the byval type for a call or parameter.\n  Type *getParamByValType(unsigned ArgNo) const {\n    Type *Ty = Attrs.getParamByValType(ArgNo);\n    return Ty ? Ty : getArgOperand(ArgNo)->getType()->getPointerElementType();\n  }\n\n  /// Extract the preallocated type for a call or parameter.\n  Type *getParamPreallocatedType(unsigned ArgNo) const {\n    Type *Ty = Attrs.getParamPreallocatedType(ArgNo);\n    return Ty ? Ty : getArgOperand(ArgNo)->getType()->getPointerElementType();\n  }\n\n  /// Extract the number of dereferenceable bytes for a call or\n  /// parameter (0=unknown).\n  uint64_t getDereferenceableBytes(unsigned i) const {\n    return Attrs.getDereferenceableBytes(i);\n  }\n\n  /// Extract the number of dereferenceable_or_null bytes for a call or\n  /// parameter (0=unknown).\n  uint64_t getDereferenceableOrNullBytes(unsigned i) const {\n    return Attrs.getDereferenceableOrNullBytes(i);\n  }\n\n  /// Return true if the return value is known to be not null.\n  /// This may be because it has the nonnull attribute, or because at least\n  /// one byte is dereferenceable and the pointer is in addrspace(0).\n  bool isReturnNonNull() const;\n\n  /// Determine if the return value is marked with NoAlias attribute.\n  bool returnDoesNotAlias() const {\n    return Attrs.hasAttribute(AttributeList::ReturnIndex, Attribute::NoAlias);\n  }\n\n  /// If one of the arguments has the 'returned' attribute, returns its\n  /// operand value. Otherwise, return nullptr.\n  Value *getReturnedArgOperand() const;\n\n  /// Return true if the call should not be treated as a call to a\n  /// builtin.\n  bool isNoBuiltin() const {\n    return hasFnAttrImpl(Attribute::NoBuiltin) &&\n           !hasFnAttrImpl(Attribute::Builtin);\n  }\n\n  /// Determine if the call requires strict floating point semantics.\n  bool isStrictFP() const { return hasFnAttr(Attribute::StrictFP); }\n\n  /// Return true if the call should not be inlined.\n  bool isNoInline() const { return hasFnAttr(Attribute::NoInline); }\n  void setIsNoInline() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoInline);\n  }\n  /// Determine if the call does not access memory.\n  bool doesNotAccessMemory() const { return hasFnAttr(Attribute::ReadNone); }\n  void setDoesNotAccessMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ReadNone);\n  }\n\n  /// Determine if the call does not access or only reads memory.\n  bool onlyReadsMemory() const {\n    return doesNotAccessMemory() || hasFnAttr(Attribute::ReadOnly);\n  }\n\n  void setOnlyReadsMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ReadOnly);\n  }\n\n  /// Determine if the call does not access or only writes memory.\n  bool doesNotReadMemory() const {\n    return doesNotAccessMemory() || hasFnAttr(Attribute::WriteOnly);\n  }\n  void setDoesNotReadMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::WriteOnly);\n  }\n\n  /// Determine if the call can access memmory only using pointers based\n  /// on its arguments.\n  bool onlyAccessesArgMemory() const {\n    return hasFnAttr(Attribute::ArgMemOnly);\n  }\n  void setOnlyAccessesArgMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ArgMemOnly);\n  }\n\n  /// Determine if the function may only access memory that is\n  /// inaccessible from the IR.\n  bool onlyAccessesInaccessibleMemory() const {\n    return hasFnAttr(Attribute::InaccessibleMemOnly);\n  }\n  void setOnlyAccessesInaccessibleMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::InaccessibleMemOnly);\n  }\n\n  /// Determine if the function may only access memory that is\n  /// either inaccessible from the IR or pointed to by its arguments.\n  bool onlyAccessesInaccessibleMemOrArgMem() const {\n    return hasFnAttr(Attribute::InaccessibleMemOrArgMemOnly);\n  }\n  void setOnlyAccessesInaccessibleMemOrArgMem() {\n    addAttribute(AttributeList::FunctionIndex,\n                 Attribute::InaccessibleMemOrArgMemOnly);\n  }\n  /// Determine if the call cannot return.\n  bool doesNotReturn() const { return hasFnAttr(Attribute::NoReturn); }\n  void setDoesNotReturn() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoReturn);\n  }\n\n  /// Determine if the call should not perform indirect branch tracking.\n  bool doesNoCfCheck() const { return hasFnAttr(Attribute::NoCfCheck); }\n\n  /// Determine if the call cannot unwind.\n  bool doesNotThrow() const { return hasFnAttr(Attribute::NoUnwind); }\n  void setDoesNotThrow() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoUnwind);\n  }\n\n  /// Determine if the invoke cannot be duplicated.\n  bool cannotDuplicate() const { return hasFnAttr(Attribute::NoDuplicate); }\n  void setCannotDuplicate() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoDuplicate);\n  }\n\n  /// Determine if the call cannot be tail merged.\n  bool cannotMerge() const { return hasFnAttr(Attribute::NoMerge); }\n  void setCannotMerge() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoMerge);\n  }\n\n  /// Determine if the invoke is convergent\n  bool isConvergent() const { return hasFnAttr(Attribute::Convergent); }\n  void setConvergent() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::Convergent);\n  }\n  void setNotConvergent() {\n    removeAttribute(AttributeList::FunctionIndex, Attribute::Convergent);\n  }\n\n  /// Determine if the call returns a structure through first\n  /// pointer argument.\n  bool hasStructRetAttr() const {\n    if (getNumArgOperands() == 0)\n      return false;\n\n    // Be friendly and also check the callee.\n    return paramHasAttr(0, Attribute::StructRet);\n  }\n\n  /// Determine if any call argument is an aggregate passed by value.\n  bool hasByValArgument() const {\n    return Attrs.hasAttrSomewhere(Attribute::ByVal);\n  }\n\n  ///@{\n  // End of attribute API.\n\n  /// \\name Operand Bundle API\n  ///\n  /// This group of methods provides the API to access and manipulate operand\n  /// bundles on this call.\n  /// @{\n\n  /// Return the number of operand bundles associated with this User.\n  unsigned getNumOperandBundles() const {\n    return std::distance(bundle_op_info_begin(), bundle_op_info_end());\n  }\n\n  /// Return true if this User has any operand bundles.\n  bool hasOperandBundles() const { return getNumOperandBundles() != 0; }\n\n  /// Return the index of the first bundle operand in the Use array.\n  unsigned getBundleOperandsStartIndex() const {\n    assert(hasOperandBundles() && \"Don't call otherwise!\");\n    return bundle_op_info_begin()->Begin;\n  }\n\n  /// Return the index of the last bundle operand in the Use array.\n  unsigned getBundleOperandsEndIndex() const {\n    assert(hasOperandBundles() && \"Don't call otherwise!\");\n    return bundle_op_info_end()[-1].End;\n  }\n\n  /// Return true if the operand at index \\p Idx is a bundle operand.\n  bool isBundleOperand(unsigned Idx) const {\n    return hasOperandBundles() && Idx >= getBundleOperandsStartIndex() &&\n           Idx < getBundleOperandsEndIndex();\n  }\n\n  /// Returns true if the use is a bundle operand.\n  bool isBundleOperand(const Use *U) const {\n    assert(this == U->getUser() &&\n           \"Only valid to query with a use of this instruction!\");\n    return hasOperandBundles() && isBundleOperand(U - op_begin());\n  }\n  bool isBundleOperand(Value::const_user_iterator UI) const {\n    return isBundleOperand(&UI.getUse());\n  }\n\n  /// Return the total number operands (not operand bundles) used by\n  /// every operand bundle in this OperandBundleUser.\n  unsigned getNumTotalBundleOperands() const {\n    if (!hasOperandBundles())\n      return 0;\n\n    unsigned Begin = getBundleOperandsStartIndex();\n    unsigned End = getBundleOperandsEndIndex();\n\n    assert(Begin <= End && \"Should be!\");\n    return End - Begin;\n  }\n\n  /// Return the operand bundle at a specific index.\n  OperandBundleUse getOperandBundleAt(unsigned Index) const {\n    assert(Index < getNumOperandBundles() && \"Index out of bounds!\");\n    return operandBundleFromBundleOpInfo(*(bundle_op_info_begin() + Index));\n  }\n\n  /// Return the number of operand bundles with the tag Name attached to\n  /// this instruction.\n  unsigned countOperandBundlesOfType(StringRef Name) const {\n    unsigned Count = 0;\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i)\n      if (getOperandBundleAt(i).getTagName() == Name)\n        Count++;\n\n    return Count;\n  }\n\n  /// Return the number of operand bundles with the tag ID attached to\n  /// this instruction.\n  unsigned countOperandBundlesOfType(uint32_t ID) const {\n    unsigned Count = 0;\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i)\n      if (getOperandBundleAt(i).getTagID() == ID)\n        Count++;\n\n    return Count;\n  }\n\n  /// Return an operand bundle by name, if present.\n  ///\n  /// It is an error to call this for operand bundle types that may have\n  /// multiple instances of them on the same instruction.\n  Optional<OperandBundleUse> getOperandBundle(StringRef Name) const {\n    assert(countOperandBundlesOfType(Name) < 2 && \"Precondition violated!\");\n\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i) {\n      OperandBundleUse U = getOperandBundleAt(i);\n      if (U.getTagName() == Name)\n        return U;\n    }\n\n    return None;\n  }\n\n  /// Return an operand bundle by tag ID, if present.\n  ///\n  /// It is an error to call this for operand bundle types that may have\n  /// multiple instances of them on the same instruction.\n  Optional<OperandBundleUse> getOperandBundle(uint32_t ID) const {\n    assert(countOperandBundlesOfType(ID) < 2 && \"Precondition violated!\");\n\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i) {\n      OperandBundleUse U = getOperandBundleAt(i);\n      if (U.getTagID() == ID)\n        return U;\n    }\n\n    return None;\n  }\n\n  /// Return the list of operand bundles attached to this instruction as\n  /// a vector of OperandBundleDefs.\n  ///\n  /// This function copies the OperandBundeUse instances associated with this\n  /// OperandBundleUser to a vector of OperandBundleDefs.  Note:\n  /// OperandBundeUses and OperandBundleDefs are non-trivially *different*\n  /// representations of operand bundles (see documentation above).\n  void getOperandBundlesAsDefs(SmallVectorImpl<OperandBundleDef> &Defs) const;\n\n  /// Return the operand bundle for the operand at index OpIdx.\n  ///\n  /// It is an error to call this with an OpIdx that does not correspond to an\n  /// bundle operand.\n  OperandBundleUse getOperandBundleForOperand(unsigned OpIdx) const {\n    return operandBundleFromBundleOpInfo(getBundleOpInfoForOperand(OpIdx));\n  }\n\n  /// Return true if this operand bundle user has operand bundles that\n  /// may read from the heap.\n  bool hasReadingOperandBundles() const {\n    // Implementation note: this is a conservative implementation of operand\n    // bundle semantics, where *any* operand bundle forces a callsite to be at\n    // least readonly.\n    return hasOperandBundles();\n  }\n\n  /// Return true if this operand bundle user has operand bundles that\n  /// may write to the heap.\n  bool hasClobberingOperandBundles() const {\n    for (auto &BOI : bundle_op_infos()) {\n      if (BOI.Tag->second == LLVMContext::OB_deopt ||\n          BOI.Tag->second == LLVMContext::OB_funclet)\n        continue;\n\n      // This instruction has an operand bundle that is not known to us.\n      // Assume the worst.\n      return true;\n    }\n\n    return false;\n  }\n\n  /// Return true if the bundle operand at index \\p OpIdx has the\n  /// attribute \\p A.\n  bool bundleOperandHasAttr(unsigned OpIdx,  Attribute::AttrKind A) const {\n    auto &BOI = getBundleOpInfoForOperand(OpIdx);\n    auto OBU = operandBundleFromBundleOpInfo(BOI);\n    return OBU.operandHasAttr(OpIdx - BOI.Begin, A);\n  }\n\n  /// Return true if \\p Other has the same sequence of operand bundle\n  /// tags with the same number of operands on each one of them as this\n  /// OperandBundleUser.\n  bool hasIdenticalOperandBundleSchema(const CallBase &Other) const {\n    if (getNumOperandBundles() != Other.getNumOperandBundles())\n      return false;\n\n    return std::equal(bundle_op_info_begin(), bundle_op_info_end(),\n                      Other.bundle_op_info_begin());\n  }\n\n  /// Return true if this operand bundle user contains operand bundles\n  /// with tags other than those specified in \\p IDs.\n  bool hasOperandBundlesOtherThan(ArrayRef<uint32_t> IDs) const {\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i) {\n      uint32_t ID = getOperandBundleAt(i).getTagID();\n      if (!is_contained(IDs, ID))\n        return true;\n    }\n    return false;\n  }\n\n  /// Is the function attribute S disallowed by some operand bundle on\n  /// this operand bundle user?\n  bool isFnAttrDisallowedByOpBundle(StringRef S) const {\n    // Operand bundles only possibly disallow readnone, readonly and argmemonly\n    // attributes.  All String attributes are fine.\n    return false;\n  }\n\n  /// Is the function attribute A disallowed by some operand bundle on\n  /// this operand bundle user?\n  bool isFnAttrDisallowedByOpBundle(Attribute::AttrKind A) const {\n    switch (A) {\n    default:\n      return false;\n\n    case Attribute::InaccessibleMemOrArgMemOnly:\n      return hasReadingOperandBundles();\n\n    case Attribute::InaccessibleMemOnly:\n      return hasReadingOperandBundles();\n\n    case Attribute::ArgMemOnly:\n      return hasReadingOperandBundles();\n\n    case Attribute::ReadNone:\n      return hasReadingOperandBundles();\n\n    case Attribute::ReadOnly:\n      return hasClobberingOperandBundles();\n    }\n\n    llvm_unreachable(\"switch has a default case!\");\n  }\n\n  /// Used to keep track of an operand bundle.  See the main comment on\n  /// OperandBundleUser above.\n  struct BundleOpInfo {\n    /// The operand bundle tag, interned by\n    /// LLVMContextImpl::getOrInsertBundleTag.\n    StringMapEntry<uint32_t> *Tag;\n\n    /// The index in the Use& vector where operands for this operand\n    /// bundle starts.\n    uint32_t Begin;\n\n    /// The index in the Use& vector where operands for this operand\n    /// bundle ends.\n    uint32_t End;\n\n    bool operator==(const BundleOpInfo &Other) const {\n      return Tag == Other.Tag && Begin == Other.Begin && End == Other.End;\n    }\n  };\n\n  /// Simple helper function to map a BundleOpInfo to an\n  /// OperandBundleUse.\n  OperandBundleUse\n  operandBundleFromBundleOpInfo(const BundleOpInfo &BOI) const {\n    auto begin = op_begin();\n    ArrayRef<Use> Inputs(begin + BOI.Begin, begin + BOI.End);\n    return OperandBundleUse(BOI.Tag, Inputs);\n  }\n\n  using bundle_op_iterator = BundleOpInfo *;\n  using const_bundle_op_iterator = const BundleOpInfo *;\n\n  /// Return the start of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  ///\n  /// OperandBundleUser uses the descriptor area co-allocated with the host User\n  /// to store some meta information about which operands are \"normal\" operands,\n  /// and which ones belong to some operand bundle.\n  ///\n  /// The layout of an operand bundle user is\n  ///\n  ///          +-----------uint32_t End-------------------------------------+\n  ///          |                                                            |\n  ///          |  +--------uint32_t Begin--------------------+              |\n  ///          |  |                                          |              |\n  ///          ^  ^                                          v              v\n  ///  |------|------|----|----|----|----|----|---------|----|---------|----|-----\n  ///  | BOI0 | BOI1 | .. | DU | U0 | U1 | .. | BOI0_U0 | .. | BOI1_U0 | .. | Un\n  ///  |------|------|----|----|----|----|----|---------|----|---------|----|-----\n  ///   v  v                                  ^              ^\n  ///   |  |                                  |              |\n  ///   |  +--------uint32_t Begin------------+              |\n  ///   |                                                    |\n  ///   +-----------uint32_t End-----------------------------+\n  ///\n  ///\n  /// BOI0, BOI1 ... are descriptions of operand bundles in this User's use\n  /// list. These descriptions are installed and managed by this class, and\n  /// they're all instances of OperandBundleUser<T>::BundleOpInfo.\n  ///\n  /// DU is an additional descriptor installed by User's 'operator new' to keep\n  /// track of the 'BOI0 ... BOIN' co-allocation.  OperandBundleUser does not\n  /// access or modify DU in any way, it's an implementation detail private to\n  /// User.\n  ///\n  /// The regular Use& vector for the User starts at U0.  The operand bundle\n  /// uses are part of the Use& vector, just like normal uses.  In the diagram\n  /// above, the operand bundle uses start at BOI0_U0.  Each instance of\n  /// BundleOpInfo has information about a contiguous set of uses constituting\n  /// an operand bundle, and the total set of operand bundle uses themselves\n  /// form a contiguous set of uses (i.e. there are no gaps between uses\n  /// corresponding to individual operand bundles).\n  ///\n  /// This class does not know the location of the set of operand bundle uses\n  /// within the use list -- that is decided by the User using this class via\n  /// the BeginIdx argument in populateBundleOperandInfos.\n  ///\n  /// Currently operand bundle users with hung-off operands are not supported.\n  bundle_op_iterator bundle_op_info_begin() {\n    if (!hasDescriptor())\n      return nullptr;\n\n    uint8_t *BytesBegin = getDescriptor().begin();\n    return reinterpret_cast<bundle_op_iterator>(BytesBegin);\n  }\n\n  /// Return the start of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  const_bundle_op_iterator bundle_op_info_begin() const {\n    auto *NonConstThis = const_cast<CallBase *>(this);\n    return NonConstThis->bundle_op_info_begin();\n  }\n\n  /// Return the end of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  bundle_op_iterator bundle_op_info_end() {\n    if (!hasDescriptor())\n      return nullptr;\n\n    uint8_t *BytesEnd = getDescriptor().end();\n    return reinterpret_cast<bundle_op_iterator>(BytesEnd);\n  }\n\n  /// Return the end of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  const_bundle_op_iterator bundle_op_info_end() const {\n    auto *NonConstThis = const_cast<CallBase *>(this);\n    return NonConstThis->bundle_op_info_end();\n  }\n\n  /// Return the range [\\p bundle_op_info_begin, \\p bundle_op_info_end).\n  iterator_range<bundle_op_iterator> bundle_op_infos() {\n    return make_range(bundle_op_info_begin(), bundle_op_info_end());\n  }\n\n  /// Return the range [\\p bundle_op_info_begin, \\p bundle_op_info_end).\n  iterator_range<const_bundle_op_iterator> bundle_op_infos() const {\n    return make_range(bundle_op_info_begin(), bundle_op_info_end());\n  }\n\n  /// Populate the BundleOpInfo instances and the Use& vector from \\p\n  /// Bundles.  Return the op_iterator pointing to the Use& one past the last\n  /// last bundle operand use.\n  ///\n  /// Each \\p OperandBundleDef instance is tracked by a OperandBundleInfo\n  /// instance allocated in this User's descriptor.\n  op_iterator populateBundleOperandInfos(ArrayRef<OperandBundleDef> Bundles,\n                                         const unsigned BeginIndex);\n\npublic:\n  /// Return the BundleOpInfo for the operand at index OpIdx.\n  ///\n  /// It is an error to call this with an OpIdx that does not correspond to an\n  /// bundle operand.\n  BundleOpInfo &getBundleOpInfoForOperand(unsigned OpIdx);\n  const BundleOpInfo &getBundleOpInfoForOperand(unsigned OpIdx) const {\n    return const_cast<CallBase *>(this)->getBundleOpInfoForOperand(OpIdx);\n  }\n\nprotected:\n  /// Return the total number of values used in \\p Bundles.\n  static unsigned CountBundleInputs(ArrayRef<OperandBundleDef> Bundles) {\n    unsigned Total = 0;\n    for (auto &B : Bundles)\n      Total += B.input_size();\n    return Total;\n  }\n\n  /// @}\n  // End of operand bundle API.\n\nprivate:\n  bool hasFnAttrOnCalledFunction(Attribute::AttrKind Kind) const;\n  bool hasFnAttrOnCalledFunction(StringRef Kind) const;\n\n  template <typename AttrKind> bool hasFnAttrImpl(AttrKind Kind) const {\n    if (Attrs.hasFnAttribute(Kind))\n      return true;\n\n    // Operand bundles override attributes on the called function, but don't\n    // override attributes directly present on the call instruction.\n    if (isFnAttrDisallowedByOpBundle(Kind))\n      return false;\n\n    return hasFnAttrOnCalledFunction(Kind);\n  }\n\n  /// Determine whether the return value has the given attribute. Supports\n  /// Attribute::AttrKind and StringRef as \\p AttrKind types.\n  template <typename AttrKind> bool hasRetAttrImpl(AttrKind Kind) const {\n    if (Attrs.hasAttribute(AttributeList::ReturnIndex, Kind))\n      return true;\n\n    // Look at the callee, if available.\n    if (const Function *F = getCalledFunction())\n      return F->getAttributes().hasAttribute(AttributeList::ReturnIndex, Kind);\n    return false;\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CallBase> : public VariadicOperandTraits<CallBase, 1> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CallBase, Value)\n\n//===----------------------------------------------------------------------===//\n//                           FuncletPadInst Class\n//===----------------------------------------------------------------------===//\nclass FuncletPadInst : public Instruction {\nprivate:\n  FuncletPadInst(const FuncletPadInst &CPI);\n\n  explicit FuncletPadInst(Instruction::FuncletPadOps Op, Value *ParentPad,\n                          ArrayRef<Value *> Args, unsigned Values,\n                          const Twine &NameStr, Instruction *InsertBefore);\n  explicit FuncletPadInst(Instruction::FuncletPadOps Op, Value *ParentPad,\n                          ArrayRef<Value *> Args, unsigned Values,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(Value *ParentPad, ArrayRef<Value *> Args, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n  friend class CatchPadInst;\n  friend class CleanupPadInst;\n\n  FuncletPadInst *cloneImpl() const;\n\npublic:\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// getNumArgOperands - Return the number of funcletpad arguments.\n  ///\n  unsigned getNumArgOperands() const { return getNumOperands() - 1; }\n\n  /// Convenience accessors\n\n  /// Return the outer EH-pad this funclet is nested within.\n  ///\n  /// Note: This returns the associated CatchSwitchInst if this FuncletPadInst\n  /// is a CatchPadInst.\n  Value *getParentPad() const { return Op<-1>(); }\n  void setParentPad(Value *ParentPad) {\n    assert(ParentPad);\n    Op<-1>() = ParentPad;\n  }\n\n  /// getArgOperand/setArgOperand - Return/set the i-th funcletpad argument.\n  ///\n  Value *getArgOperand(unsigned i) const { return getOperand(i); }\n  void setArgOperand(unsigned i, Value *v) { setOperand(i, v); }\n\n  /// arg_operands - iteration adapter for range-for loops.\n  op_range arg_operands() { return op_range(op_begin(), op_end() - 1); }\n\n  /// arg_operands - iteration adapter for range-for loops.\n  const_op_range arg_operands() const {\n    return const_op_range(op_begin(), op_end() - 1);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) { return I->isFuncletPad(); }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<FuncletPadInst>\n    : public VariadicOperandTraits<FuncletPadInst, /*MINARITY=*/1> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(FuncletPadInst, Value)\n\n} // end namespace llvm\n\n#endif // LLVM_IR_INSTRTYPES_H\n"}, "79": {"id": 79, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instruction.h", "content": "//===-- llvm/Instruction.h - Instruction class definition -------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains the declaration of the Instruction class, which is the\n// base class for all of the LLVM instructions.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_INSTRUCTION_H\n#define LLVM_IR_INSTRUCTION_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/Bitfields.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/ilist_node.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/SymbolTableListTraits.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstdint>\n#include <utility>\n\nnamespace llvm {\n\nclass BasicBlock;\nclass FastMathFlags;\nclass MDNode;\nclass Module;\nstruct AAMDNodes;\n\ntemplate <> struct ilist_alloc_traits<Instruction> {\n  static inline void deleteNode(Instruction *V);\n};\n\nclass Instruction : public User,\n                    public ilist_node_with_parent<Instruction, BasicBlock> {\n  BasicBlock *Parent;\n  DebugLoc DbgLoc;                         // 'dbg' Metadata cache.\n\n  /// Relative order of this instruction in its parent basic block. Used for\n  /// O(1) local dominance checks between instructions.\n  mutable unsigned Order = 0;\n\nprotected:\n  // The 15 first bits of `Value::SubclassData` are available for subclasses of\n  // `Instruction` to use.\n  using OpaqueField = Bitfield::Element<uint16_t, 0, 15>;\n\n  // Template alias so that all Instruction storing alignment use the same\n  // definiton.\n  // Valid alignments are powers of two from 2^0 to 2^MaxAlignmentExponent =\n  // 2^29. We store them as Log2(Alignment), so we need 5 bits to encode the 30\n  // possible values.\n  template <unsigned Offset>\n  using AlignmentBitfieldElementT =\n      typename Bitfield::Element<unsigned, Offset, 5,\n                                 Value::MaxAlignmentExponent>;\n\n  template <unsigned Offset>\n  using BoolBitfieldElementT = typename Bitfield::Element<bool, Offset, 1>;\n\n  template <unsigned Offset>\n  using AtomicOrderingBitfieldElementT =\n      typename Bitfield::Element<AtomicOrdering, Offset, 3,\n                                 AtomicOrdering::LAST>;\n\nprivate:\n  // The last bit is used to store whether the instruction has metadata attached\n  // or not.\n  using HasMetadataField = Bitfield::Element<bool, 15, 1>;\n\nprotected:\n  ~Instruction(); // Use deleteValue() to delete a generic Instruction.\n\npublic:\n  Instruction(const Instruction &) = delete;\n  Instruction &operator=(const Instruction &) = delete;\n\n  /// Specialize the methods defined in Value, as we know that an instruction\n  /// can only be used by other instructions.\n  Instruction       *user_back()       { return cast<Instruction>(*user_begin());}\n  const Instruction *user_back() const { return cast<Instruction>(*user_begin());}\n\n  inline const BasicBlock *getParent() const { return Parent; }\n  inline       BasicBlock *getParent()       { return Parent; }\n\n  /// Return the module owning the function this instruction belongs to\n  /// or nullptr it the function does not have a module.\n  ///\n  /// Note: this is undefined behavior if the instruction does not have a\n  /// parent, or the parent basic block does not have a parent function.\n  const Module *getModule() const;\n  Module *getModule() {\n    return const_cast<Module *>(\n                           static_cast<const Instruction *>(this)->getModule());\n  }\n\n  /// Return the function this instruction belongs to.\n  ///\n  /// Note: it is undefined behavior to call this on an instruction not\n  /// currently inserted into a function.\n  const Function *getFunction() const;\n  Function *getFunction() {\n    return const_cast<Function *>(\n                         static_cast<const Instruction *>(this)->getFunction());\n  }\n\n  /// This method unlinks 'this' from the containing basic block, but does not\n  /// delete it.\n  void removeFromParent();\n\n  /// This method unlinks 'this' from the containing basic block and deletes it.\n  ///\n  /// \\returns an iterator pointing to the element after the erased one\n  SymbolTableList<Instruction>::iterator eraseFromParent();\n\n  /// Insert an unlinked instruction into a basic block immediately before\n  /// the specified instruction.\n  void insertBefore(Instruction *InsertPos);\n\n  /// Insert an unlinked instruction into a basic block immediately after the\n  /// specified instruction.\n  void insertAfter(Instruction *InsertPos);\n\n  /// Unlink this instruction from its current basic block and insert it into\n  /// the basic block that MovePos lives in, right before MovePos.\n  void moveBefore(Instruction *MovePos);\n\n  /// Unlink this instruction and insert into BB before I.\n  ///\n  /// \\pre I is a valid iterator into BB.\n  void moveBefore(BasicBlock &BB, SymbolTableList<Instruction>::iterator I);\n\n  /// Unlink this instruction from its current basic block and insert it into\n  /// the basic block that MovePos lives in, right after MovePos.\n  void moveAfter(Instruction *MovePos);\n\n  /// Given an instruction Other in the same basic block as this instruction,\n  /// return true if this instruction comes before Other. In this worst case,\n  /// this takes linear time in the number of instructions in the block. The\n  /// results are cached, so in common cases when the block remains unmodified,\n  /// it takes constant time.\n  bool comesBefore(const Instruction *Other) const;\n\n  //===--------------------------------------------------------------------===//\n  // Subclass classification.\n  //===--------------------------------------------------------------------===//\n\n  /// Returns a member of one of the enums like Instruction::Add.\n  unsigned getOpcode() const { return getValueID() - InstructionVal; }\n\n  const char *getOpcodeName() const { return getOpcodeName(getOpcode()); }\n  bool isTerminator() const { return isTerminator(getOpcode()); }\n  bool isUnaryOp() const { return isUnaryOp(getOpcode()); }\n  bool isBinaryOp() const { return isBinaryOp(getOpcode()); }\n  bool isIntDivRem() const { return isIntDivRem(getOpcode()); }\n  bool isShift() const { return isShift(getOpcode()); }\n  bool isCast() const { return isCast(getOpcode()); }\n  bool isFuncletPad() const { return isFuncletPad(getOpcode()); }\n  bool isExceptionalTerminator() const {\n    return isExceptionalTerminator(getOpcode());\n  }\n  bool isIndirectTerminator() const {\n    return isIndirectTerminator(getOpcode());\n  }\n\n  static const char* getOpcodeName(unsigned OpCode);\n\n  static inline bool isTerminator(unsigned OpCode) {\n    return OpCode >= TermOpsBegin && OpCode < TermOpsEnd;\n  }\n\n  static inline bool isUnaryOp(unsigned Opcode) {\n    return Opcode >= UnaryOpsBegin && Opcode < UnaryOpsEnd;\n  }\n  static inline bool isBinaryOp(unsigned Opcode) {\n    return Opcode >= BinaryOpsBegin && Opcode < BinaryOpsEnd;\n  }\n\n  static inline bool isIntDivRem(unsigned Opcode) {\n    return Opcode == UDiv || Opcode == SDiv || Opcode == URem || Opcode == SRem;\n  }\n\n  /// Determine if the Opcode is one of the shift instructions.\n  static inline bool isShift(unsigned Opcode) {\n    return Opcode >= Shl && Opcode <= AShr;\n  }\n\n  /// Return true if this is a logical shift left or a logical shift right.\n  inline bool isLogicalShift() const {\n    return getOpcode() == Shl || getOpcode() == LShr;\n  }\n\n  /// Return true if this is an arithmetic shift right.\n  inline bool isArithmeticShift() const {\n    return getOpcode() == AShr;\n  }\n\n  /// Determine if the Opcode is and/or/xor.\n  static inline bool isBitwiseLogicOp(unsigned Opcode) {\n    return Opcode == And || Opcode == Or || Opcode == Xor;\n  }\n\n  /// Return true if this is and/or/xor.\n  inline bool isBitwiseLogicOp() const {\n    return isBitwiseLogicOp(getOpcode());\n  }\n\n  /// Determine if the OpCode is one of the CastInst instructions.\n  static inline bool isCast(unsigned OpCode) {\n    return OpCode >= CastOpsBegin && OpCode < CastOpsEnd;\n  }\n\n  /// Determine if the OpCode is one of the FuncletPadInst instructions.\n  static inline bool isFuncletPad(unsigned OpCode) {\n    return OpCode >= FuncletPadOpsBegin && OpCode < FuncletPadOpsEnd;\n  }\n\n  /// Returns true if the OpCode is a terminator related to exception handling.\n  static inline bool isExceptionalTerminator(unsigned OpCode) {\n    switch (OpCode) {\n    case Instruction::CatchSwitch:\n    case Instruction::CatchRet:\n    case Instruction::CleanupRet:\n    case Instruction::Invoke:\n    case Instruction::Resume:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  /// Returns true if the OpCode is a terminator with indirect targets.\n  static inline bool isIndirectTerminator(unsigned OpCode) {\n    switch (OpCode) {\n    case Instruction::IndirectBr:\n    case Instruction::CallBr:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Metadata manipulation.\n  //===--------------------------------------------------------------------===//\n\n  /// Return true if this instruction has any metadata attached to it.\n  bool hasMetadata() const { return DbgLoc || Value::hasMetadata(); }\n\n  /// Return true if this instruction has metadata attached to it other than a\n  /// debug location.\n  bool hasMetadataOtherThanDebugLoc() const { return Value::hasMetadata(); }\n\n  /// Return true if this instruction has the given type of metadata attached.\n  bool hasMetadata(unsigned KindID) const {\n    return getMetadata(KindID) != nullptr;\n  }\n\n  /// Return true if this instruction has the given type of metadata attached.\n  bool hasMetadata(StringRef Kind) const {\n    return getMetadata(Kind) != nullptr;\n  }\n\n  /// Get the metadata of given kind attached to this Instruction.\n  /// If the metadata is not found then return null.\n  MDNode *getMetadata(unsigned KindID) const {\n    if (!hasMetadata()) return nullptr;\n    return getMetadataImpl(KindID);\n  }\n\n  /// Get the metadata of given kind attached to this Instruction.\n  /// If the metadata is not found then return null.\n  MDNode *getMetadata(StringRef Kind) const {\n    if (!hasMetadata()) return nullptr;\n    return getMetadataImpl(Kind);\n  }\n\n  /// Get all metadata attached to this Instruction. The first element of each\n  /// pair returned is the KindID, the second element is the metadata value.\n  /// This list is returned sorted by the KindID.\n  void\n  getAllMetadata(SmallVectorImpl<std::pair<unsigned, MDNode *>> &MDs) const {\n    if (hasMetadata())\n      getAllMetadataImpl(MDs);\n  }\n\n  /// This does the same thing as getAllMetadata, except that it filters out the\n  /// debug location.\n  void getAllMetadataOtherThanDebugLoc(\n      SmallVectorImpl<std::pair<unsigned, MDNode *>> &MDs) const {\n    Value::getAllMetadata(MDs);\n  }\n\n  /// Fills the AAMDNodes structure with AA metadata from this instruction.\n  /// When Merge is true, the existing AA metadata is merged with that from this\n  /// instruction providing the most-general result.\n  void getAAMetadata(AAMDNodes &N, bool Merge = false) const;\n\n  /// Set the metadata of the specified kind to the specified node. This updates\n  /// or replaces metadata if already present, or removes it if Node is null.\n  void setMetadata(unsigned KindID, MDNode *Node);\n  void setMetadata(StringRef Kind, MDNode *Node);\n\n  /// Copy metadata from \\p SrcInst to this instruction. \\p WL, if not empty,\n  /// specifies the list of meta data that needs to be copied. If \\p WL is\n  /// empty, all meta data will be copied.\n  void copyMetadata(const Instruction &SrcInst,\n                    ArrayRef<unsigned> WL = ArrayRef<unsigned>());\n\n  /// If the instruction has \"branch_weights\" MD_prof metadata and the MDNode\n  /// has three operands (including name string), swap the order of the\n  /// metadata.\n  void swapProfMetadata();\n\n  /// Drop all unknown metadata except for debug locations.\n  /// @{\n  /// Passes are required to drop metadata they don't understand. This is a\n  /// convenience method for passes to do so.\n  void dropUnknownNonDebugMetadata(ArrayRef<unsigned> KnownIDs);\n  void dropUnknownNonDebugMetadata() {\n    return dropUnknownNonDebugMetadata(None);\n  }\n  void dropUnknownNonDebugMetadata(unsigned ID1) {\n    return dropUnknownNonDebugMetadata(makeArrayRef(ID1));\n  }\n  void dropUnknownNonDebugMetadata(unsigned ID1, unsigned ID2) {\n    unsigned IDs[] = {ID1, ID2};\n    return dropUnknownNonDebugMetadata(IDs);\n  }\n  /// @}\n\n  /// Adds an !annotation metadata node with \\p Annotation to this instruction.\n  /// If this instruction already has !annotation metadata, append \\p Annotation\n  /// to the existing node.\n  void addAnnotationMetadata(StringRef Annotation);\n\n  /// Sets the metadata on this instruction from the AAMDNodes structure.\n  void setAAMetadata(const AAMDNodes &N);\n\n  /// Retrieve the raw weight values of a conditional branch or select.\n  /// Returns true on success with profile weights filled in.\n  /// Returns false if no metadata or invalid metadata was found.\n  bool extractProfMetadata(uint64_t &TrueVal, uint64_t &FalseVal) const;\n\n  /// Retrieve total raw weight values of a branch.\n  /// Returns true on success with profile total weights filled in.\n  /// Returns false if no metadata was found.\n  bool extractProfTotalWeight(uint64_t &TotalVal) const;\n\n  /// Set the debug location information for this instruction.\n  void setDebugLoc(DebugLoc Loc) { DbgLoc = std::move(Loc); }\n\n  /// Return the debug location for this node as a DebugLoc.\n  const DebugLoc &getDebugLoc() const { return DbgLoc; }\n\n  /// Set or clear the nuw flag on this instruction, which must be an operator\n  /// which supports this flag. See LangRef.html for the meaning of this flag.\n  void setHasNoUnsignedWrap(bool b = true);\n\n  /// Set or clear the nsw flag on this instruction, which must be an operator\n  /// which supports this flag. See LangRef.html for the meaning of this flag.\n  void setHasNoSignedWrap(bool b = true);\n\n  /// Set or clear the exact flag on this instruction, which must be an operator\n  /// which supports this flag. See LangRef.html for the meaning of this flag.\n  void setIsExact(bool b = true);\n\n  /// Determine whether the no unsigned wrap flag is set.\n  bool hasNoUnsignedWrap() const;\n\n  /// Determine whether the no signed wrap flag is set.\n  bool hasNoSignedWrap() const;\n\n  /// Drops flags that may cause this instruction to evaluate to poison despite\n  /// having non-poison inputs.\n  void dropPoisonGeneratingFlags();\n\n  /// Determine whether the exact flag is set.\n  bool isExact() const;\n\n  /// Set or clear all fast-math-flags on this instruction, which must be an\n  /// operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setFast(bool B);\n\n  /// Set or clear the reassociation flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasAllowReassoc(bool B);\n\n  /// Set or clear the no-nans flag on this instruction, which must be an\n  /// operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasNoNaNs(bool B);\n\n  /// Set or clear the no-infs flag on this instruction, which must be an\n  /// operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasNoInfs(bool B);\n\n  /// Set or clear the no-signed-zeros flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasNoSignedZeros(bool B);\n\n  /// Set or clear the allow-reciprocal flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasAllowReciprocal(bool B);\n\n  /// Set or clear the allow-contract flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasAllowContract(bool B);\n\n  /// Set or clear the approximate-math-functions flag on this instruction,\n  /// which must be an operator which supports this flag. See LangRef.html for\n  /// the meaning of this flag.\n  void setHasApproxFunc(bool B);\n\n  /// Convenience function for setting multiple fast-math flags on this\n  /// instruction, which must be an operator which supports these flags. See\n  /// LangRef.html for the meaning of these flags.\n  void setFastMathFlags(FastMathFlags FMF);\n\n  /// Convenience function for transferring all fast-math flag values to this\n  /// instruction, which must be an operator which supports these flags. See\n  /// LangRef.html for the meaning of these flags.\n  void copyFastMathFlags(FastMathFlags FMF);\n\n  /// Determine whether all fast-math-flags are set.\n  bool isFast() const;\n\n  /// Determine whether the allow-reassociation flag is set.\n  bool hasAllowReassoc() const;\n\n  /// Determine whether the no-NaNs flag is set.\n  bool hasNoNaNs() const;\n\n  /// Determine whether the no-infs flag is set.\n  bool hasNoInfs() const;\n\n  /// Determine whether the no-signed-zeros flag is set.\n  bool hasNoSignedZeros() const;\n\n  /// Determine whether the allow-reciprocal flag is set.\n  bool hasAllowReciprocal() const;\n\n  /// Determine whether the allow-contract flag is set.\n  bool hasAllowContract() const;\n\n  /// Determine whether the approximate-math-functions flag is set.\n  bool hasApproxFunc() const;\n\n  /// Convenience function for getting all the fast-math flags, which must be an\n  /// operator which supports these flags. See LangRef.html for the meaning of\n  /// these flags.\n  FastMathFlags getFastMathFlags() const;\n\n  /// Copy I's fast-math flags\n  void copyFastMathFlags(const Instruction *I);\n\n  /// Convenience method to copy supported exact, fast-math, and (optionally)\n  /// wrapping flags from V to this instruction.\n  void copyIRFlags(const Value *V, bool IncludeWrapFlags = true);\n\n  /// Logical 'and' of any supported wrapping, exact, and fast-math flags of\n  /// V and this instruction.\n  void andIRFlags(const Value *V);\n\n  /// Merge 2 debug locations and apply it to the Instruction. If the\n  /// instruction is a CallIns, we need to traverse the inline chain to find\n  /// the common scope. This is not efficient for N-way merging as each time\n  /// you merge 2 iterations, you need to rebuild the hashmap to find the\n  /// common scope. However, we still choose this API because:\n  ///  1) Simplicity: it takes 2 locations instead of a list of locations.\n  ///  2) In worst case, it increases the complexity from O(N*I) to\n  ///     O(2*N*I), where N is # of Instructions to merge, and I is the\n  ///     maximum level of inline stack. So it is still linear.\n  ///  3) Merging of call instructions should be extremely rare in real\n  ///     applications, thus the N-way merging should be in code path.\n  /// The DebugLoc attached to this instruction will be overwritten by the\n  /// merged DebugLoc.\n  void applyMergedLocation(const DILocation *LocA, const DILocation *LocB);\n\n  /// Updates the debug location given that the instruction has been hoisted\n  /// from a block to a predecessor of that block.\n  /// Note: it is undefined behavior to call this on an instruction not\n  /// currently inserted into a function.\n  void updateLocationAfterHoist();\n\n  /// Drop the instruction's debug location. This does not guarantee removal\n  /// of the !dbg source location attachment, as it must set a line 0 location\n  /// with scope information attached on call instructions. To guarantee\n  /// removal of the !dbg attachment, use the \\ref setDebugLoc() API.\n  /// Note: it is undefined behavior to call this on an instruction not\n  /// currently inserted into a function.\n  void dropLocation();\n\nprivate:\n  // These are all implemented in Metadata.cpp.\n  MDNode *getMetadataImpl(unsigned KindID) const;\n  MDNode *getMetadataImpl(StringRef Kind) const;\n  void\n  getAllMetadataImpl(SmallVectorImpl<std::pair<unsigned, MDNode *>> &) const;\n\npublic:\n  //===--------------------------------------------------------------------===//\n  // Predicates and helper methods.\n  //===--------------------------------------------------------------------===//\n\n  /// Return true if the instruction is associative:\n  ///\n  ///   Associative operators satisfy:  x op (y op z) === (x op y) op z\n  ///\n  /// In LLVM, the Add, Mul, And, Or, and Xor operators are associative.\n  ///\n  bool isAssociative() const LLVM_READONLY;\n  static bool isAssociative(unsigned Opcode) {\n    return Opcode == And || Opcode == Or || Opcode == Xor ||\n           Opcode == Add || Opcode == Mul;\n  }\n\n  /// Return true if the instruction is commutative:\n  ///\n  ///   Commutative operators satisfy: (x op y) === (y op x)\n  ///\n  /// In LLVM, these are the commutative operators, plus SetEQ and SetNE, when\n  /// applied to any type.\n  ///\n  bool isCommutative() const LLVM_READONLY;\n  static bool isCommutative(unsigned Opcode) {\n    switch (Opcode) {\n    case Add: case FAdd:\n    case Mul: case FMul:\n    case And: case Or: case Xor:\n      return true;\n    default:\n      return false;\n  }\n  }\n\n  /// Return true if the instruction is idempotent:\n  ///\n  ///   Idempotent operators satisfy:  x op x === x\n  ///\n  /// In LLVM, the And and Or operators are idempotent.\n  ///\n  bool isIdempotent() const { return isIdempotent(getOpcode()); }\n  static bool isIdempotent(unsigned Opcode) {\n    return Opcode == And || Opcode == Or;\n  }\n\n  /// Return true if the instruction is nilpotent:\n  ///\n  ///   Nilpotent operators satisfy:  x op x === Id,\n  ///\n  ///   where Id is the identity for the operator, i.e. a constant such that\n  ///     x op Id === x and Id op x === x for all x.\n  ///\n  /// In LLVM, the Xor operator is nilpotent.\n  ///\n  bool isNilpotent() const { return isNilpotent(getOpcode()); }\n  static bool isNilpotent(unsigned Opcode) {\n    return Opcode == Xor;\n  }\n\n  /// Return true if this instruction may modify memory.\n  bool mayWriteToMemory() const;\n\n  /// Return true if this instruction may read memory.\n  bool mayReadFromMemory() const;\n\n  /// Return true if this instruction may read or write memory.\n  bool mayReadOrWriteMemory() const {\n    return mayReadFromMemory() || mayWriteToMemory();\n  }\n\n  /// Return true if this instruction has an AtomicOrdering of unordered or\n  /// higher.\n  bool isAtomic() const;\n\n  /// Return true if this atomic instruction loads from memory.\n  bool hasAtomicLoad() const;\n\n  /// Return true if this atomic instruction stores to memory.\n  bool hasAtomicStore() const;\n\n  /// Return true if this instruction may throw an exception.\n  bool mayThrow() const;\n\n  /// Return true if this instruction behaves like a memory fence: it can load\n  /// or store to memory location without being given a memory location.\n  bool isFenceLike() const {\n    switch (getOpcode()) {\n    default:\n      return false;\n    // This list should be kept in sync with the list in mayWriteToMemory for\n    // all opcodes which don't have a memory location.\n    case Instruction::Fence:\n    case Instruction::CatchPad:\n    case Instruction::CatchRet:\n    case Instruction::Call:\n    case Instruction::Invoke:\n      return true;\n    }\n  }\n\n  /// Return true if the instruction may have side effects.\n  ///\n  /// Note that this does not consider malloc and alloca to have side\n  /// effects because the newly allocated memory is completely invisible to\n  /// instructions which don't use the returned value.  For cases where this\n  /// matters, isSafeToSpeculativelyExecute may be more appropriate.\n  bool mayHaveSideEffects() const { return mayWriteToMemory() || mayThrow(); }\n\n  /// Return true if the instruction can be removed if the result is unused.\n  ///\n  /// When constant folding some instructions cannot be removed even if their\n  /// results are unused. Specifically terminator instructions and calls that\n  /// may have side effects cannot be removed without semantically changing the\n  /// generated program.\n  bool isSafeToRemove() const;\n\n  /// Return true if the instruction will return (unwinding is considered as\n  /// a form of returning control flow here).\n  bool willReturn() const;\n\n  /// Return true if the instruction is a variety of EH-block.\n  bool isEHPad() const {\n    switch (getOpcode()) {\n    case Instruction::CatchSwitch:\n    case Instruction::CatchPad:\n    case Instruction::CleanupPad:\n    case Instruction::LandingPad:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  /// Return true if the instruction is a llvm.lifetime.start or\n  /// llvm.lifetime.end marker.\n  bool isLifetimeStartOrEnd() const;\n\n  /// Return true if the instruction is a DbgInfoIntrinsic or PseudoProbeInst.\n  bool isDebugOrPseudoInst() const;\n\n  /// Return a pointer to the next non-debug instruction in the same basic\n  /// block as 'this', or nullptr if no such instruction exists. Skip any pseudo\n  /// operations if \\c SkipPseudoOp is true.\n  const Instruction *\n  getNextNonDebugInstruction(bool SkipPseudoOp = false) const;\n  Instruction *getNextNonDebugInstruction(bool SkipPseudoOp = false) {\n    return const_cast<Instruction *>(\n        static_cast<const Instruction *>(this)->getNextNonDebugInstruction(\n            SkipPseudoOp));\n  }\n\n  /// Return a pointer to the previous non-debug instruction in the same basic\n  /// block as 'this', or nullptr if no such instruction exists. Skip any pseudo\n  /// operations if \\c SkipPseudoOp is true.\n  const Instruction *\n  getPrevNonDebugInstruction(bool SkipPseudoOp = false) const;\n  Instruction *getPrevNonDebugInstruction(bool SkipPseudoOp = false) {\n    return const_cast<Instruction *>(\n        static_cast<const Instruction *>(this)->getPrevNonDebugInstruction(\n            SkipPseudoOp));\n  }\n\n  /// Create a copy of 'this' instruction that is identical in all ways except\n  /// the following:\n  ///   * The instruction has no parent\n  ///   * The instruction has no name\n  ///\n  Instruction *clone() const;\n\n  /// Return true if the specified instruction is exactly identical to the\n  /// current one. This means that all operands match and any extra information\n  /// (e.g. load is volatile) agree.\n  bool isIdenticalTo(const Instruction *I) const;\n\n  /// This is like isIdenticalTo, except that it ignores the\n  /// SubclassOptionalData flags, which may specify conditions under which the\n  /// instruction's result is undefined.\n  bool isIdenticalToWhenDefined(const Instruction *I) const;\n\n  /// When checking for operation equivalence (using isSameOperationAs) it is\n  /// sometimes useful to ignore certain attributes.\n  enum OperationEquivalenceFlags {\n    /// Check for equivalence ignoring load/store alignment.\n    CompareIgnoringAlignment = 1<<0,\n    /// Check for equivalence treating a type and a vector of that type\n    /// as equivalent.\n    CompareUsingScalarTypes = 1<<1\n  };\n\n  /// This function determines if the specified instruction executes the same\n  /// operation as the current one. This means that the opcodes, type, operand\n  /// types and any other factors affecting the operation must be the same. This\n  /// is similar to isIdenticalTo except the operands themselves don't have to\n  /// be identical.\n  /// @returns true if the specified instruction is the same operation as\n  /// the current one.\n  /// Determine if one instruction is the same operation as another.\n  bool isSameOperationAs(const Instruction *I, unsigned flags = 0) const;\n\n  /// Return true if there are any uses of this instruction in blocks other than\n  /// the specified block. Note that PHI nodes are considered to evaluate their\n  /// operands in the corresponding predecessor block.\n  bool isUsedOutsideOfBlock(const BasicBlock *BB) const;\n\n  /// Return the number of successors that this instruction has. The instruction\n  /// must be a terminator.\n  unsigned getNumSuccessors() const;\n\n  /// Return the specified successor. This instruction must be a terminator.\n  BasicBlock *getSuccessor(unsigned Idx) const;\n\n  /// Update the specified successor to point at the provided block. This\n  /// instruction must be a terminator.\n  void setSuccessor(unsigned Idx, BasicBlock *BB);\n\n  /// Replace specified successor OldBB to point at the provided block.\n  /// This instruction must be a terminator.\n  void replaceSuccessorWith(BasicBlock *OldBB, BasicBlock *NewBB);\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Value *V) {\n    return V->getValueID() >= Value::InstructionVal;\n  }\n\n  //----------------------------------------------------------------------\n  // Exported enumerations.\n  //\n  enum TermOps {       // These terminate basic blocks\n#define  FIRST_TERM_INST(N)             TermOpsBegin = N,\n#define HANDLE_TERM_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_TERM_INST(N)             TermOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum UnaryOps {\n#define  FIRST_UNARY_INST(N)             UnaryOpsBegin = N,\n#define HANDLE_UNARY_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_UNARY_INST(N)             UnaryOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum BinaryOps {\n#define  FIRST_BINARY_INST(N)             BinaryOpsBegin = N,\n#define HANDLE_BINARY_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_BINARY_INST(N)             BinaryOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum MemoryOps {\n#define  FIRST_MEMORY_INST(N)             MemoryOpsBegin = N,\n#define HANDLE_MEMORY_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_MEMORY_INST(N)             MemoryOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum CastOps {\n#define  FIRST_CAST_INST(N)             CastOpsBegin = N,\n#define HANDLE_CAST_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_CAST_INST(N)             CastOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum FuncletPadOps {\n#define  FIRST_FUNCLETPAD_INST(N)             FuncletPadOpsBegin = N,\n#define HANDLE_FUNCLETPAD_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_FUNCLETPAD_INST(N)             FuncletPadOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum OtherOps {\n#define  FIRST_OTHER_INST(N)             OtherOpsBegin = N,\n#define HANDLE_OTHER_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_OTHER_INST(N)             OtherOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\nprivate:\n  friend class SymbolTableListTraits<Instruction>;\n  friend class BasicBlock; // For renumbering.\n\n  // Shadow Value::setValueSubclassData with a private forwarding method so that\n  // subclasses cannot accidentally use it.\n  void setValueSubclassData(unsigned short D) {\n    Value::setValueSubclassData(D);\n  }\n\n  unsigned short getSubclassDataFromValue() const {\n    return Value::getSubclassDataFromValue();\n  }\n\n  void setParent(BasicBlock *P);\n\nprotected:\n  // Instruction subclasses can stick up to 15 bits of stuff into the\n  // SubclassData field of instruction with these members.\n\n  template <typename BitfieldElement>\n  typename BitfieldElement::Type getSubclassData() const {\n    static_assert(\n        std::is_same<BitfieldElement, HasMetadataField>::value ||\n            !Bitfield::isOverlapping<BitfieldElement, HasMetadataField>(),\n        \"Must not overlap with the metadata bit\");\n    return Bitfield::get<BitfieldElement>(getSubclassDataFromValue());\n  }\n\n  template <typename BitfieldElement>\n  void setSubclassData(typename BitfieldElement::Type Value) {\n    static_assert(\n        std::is_same<BitfieldElement, HasMetadataField>::value ||\n            !Bitfield::isOverlapping<BitfieldElement, HasMetadataField>(),\n        \"Must not overlap with the metadata bit\");\n    auto Storage = getSubclassDataFromValue();\n    Bitfield::set<BitfieldElement>(Storage, Value);\n    setValueSubclassData(Storage);\n  }\n\n  Instruction(Type *Ty, unsigned iType, Use *Ops, unsigned NumOps,\n              Instruction *InsertBefore = nullptr);\n  Instruction(Type *Ty, unsigned iType, Use *Ops, unsigned NumOps,\n              BasicBlock *InsertAtEnd);\n\nprivate:\n  /// Create a copy of this instruction.\n  Instruction *cloneImpl() const;\n};\n\ninline void ilist_alloc_traits<Instruction>::deleteNode(Instruction *V) {\n  V->deleteValue();\n}\n\n} // end namespace llvm\n\n#endif // LLVM_IR_INSTRUCTION_H\n"}, "85": {"id": 85, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/SymbolTableListTraits.h", "content": "//===- llvm/SymbolTableListTraits.h - Traits for iplist ---------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines a generic class that is used to implement the automatic\n// symbol table manipulation that occurs when you put (for example) a named\n// instruction into a basic block.\n//\n// The way that this is implemented is by using a special traits class with the\n// intrusive list that makes up the list of instructions in a basic block.  When\n// a new element is added to the list of instructions, the traits class is\n// notified, allowing the symbol table to be updated.\n//\n// This generic class implements the traits class.  It must be generic so that\n// it can work for all uses it, which include lists of instructions, basic\n// blocks, arguments, functions, global variables, etc...\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_SYMBOLTABLELISTTRAITS_H\n#define LLVM_IR_SYMBOLTABLELISTTRAITS_H\n\n#include \"llvm/ADT/ilist.h\"\n#include \"llvm/ADT/simple_ilist.h\"\n#include <cstddef>\n\nnamespace llvm {\n\nclass Argument;\nclass BasicBlock;\nclass Function;\nclass GlobalAlias;\nclass GlobalIFunc;\nclass GlobalVariable;\nclass Instruction;\nclass Module;\nclass ValueSymbolTable;\n\n/// Template metafunction to get the parent type for a symbol table list.\n///\n/// Implementations create a typedef called \\c type so that we only need a\n/// single template parameter for the list and traits.\ntemplate <typename NodeTy> struct SymbolTableListParentType {};\n\n#define DEFINE_SYMBOL_TABLE_PARENT_TYPE(NODE, PARENT)                          \\\n  template <> struct SymbolTableListParentType<NODE> { using type = PARENT; };\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(Instruction, BasicBlock)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(BasicBlock, Function)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(Argument, Function)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(Function, Module)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(GlobalVariable, Module)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(GlobalAlias, Module)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(GlobalIFunc, Module)\n#undef DEFINE_SYMBOL_TABLE_PARENT_TYPE\n\ntemplate <typename NodeTy> class SymbolTableList;\n\n// ValueSubClass   - The type of objects that I hold, e.g. Instruction.\n// ItemParentClass - The type of object that owns the list, e.g. BasicBlock.\n//\ntemplate <typename ValueSubClass>\nclass SymbolTableListTraits : public ilist_alloc_traits<ValueSubClass> {\n  using ListTy = SymbolTableList<ValueSubClass>;\n  using iterator = typename simple_ilist<ValueSubClass>::iterator;\n  using ItemParentClass =\n      typename SymbolTableListParentType<ValueSubClass>::type;\n\npublic:\n  SymbolTableListTraits() = default;\n\nprivate:\n  /// getListOwner - Return the object that owns this list.  If this is a list\n  /// of instructions, it returns the BasicBlock that owns them.\n  ItemParentClass *getListOwner() {\n    size_t Offset = reinterpret_cast<size_t>(\n        &((ItemParentClass *)nullptr->*ItemParentClass::getSublistAccess(\n                                           static_cast<ValueSubClass *>(\n                                               nullptr))));\n    ListTy *Anchor = static_cast<ListTy *>(this);\n    return reinterpret_cast<ItemParentClass*>(reinterpret_cast<char*>(Anchor)-\n                                              Offset);\n  }\n\n  static ListTy &getList(ItemParentClass *Par) {\n    return Par->*(Par->getSublistAccess((ValueSubClass*)nullptr));\n  }\n\n  static ValueSymbolTable *getSymTab(ItemParentClass *Par) {\n    return Par ? toPtr(Par->getValueSymbolTable()) : nullptr;\n  }\n\npublic:\n  void addNodeToList(ValueSubClass *V);\n  void removeNodeFromList(ValueSubClass *V);\n  void transferNodesFromList(SymbolTableListTraits &L2, iterator first,\n                             iterator last);\n  // private:\n  template<typename TPtr>\n  void setSymTabObject(TPtr *, TPtr);\n  static ValueSymbolTable *toPtr(ValueSymbolTable *P) { return P; }\n  static ValueSymbolTable *toPtr(ValueSymbolTable &R) { return &R; }\n};\n\n/// List that automatically updates parent links and symbol tables.\n///\n/// When nodes are inserted into and removed from this list, the associated\n/// symbol table will be automatically updated.  Similarly, parent links get\n/// updated automatically.\ntemplate <class T>\nclass SymbolTableList\n    : public iplist_impl<simple_ilist<T>, SymbolTableListTraits<T>> {};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_SYMBOLTABLELISTTRAITS_H\n"}, "86": {"id": 86, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/TrackingMDRef.h", "content": "//===- llvm/IR/TrackingMDRef.h - Tracking Metadata references ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// References to metadata that track RAUW.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_TRACKINGMDREF_H\n#define LLVM_IR_TRACKINGMDREF_H\n\n#include \"llvm/IR/Metadata.h\"\n#include <algorithm>\n#include <cassert>\n\nnamespace llvm {\n\n/// Tracking metadata reference.\n///\n/// This class behaves like \\a TrackingVH, but for metadata.\nclass TrackingMDRef {\n  Metadata *MD = nullptr;\n\npublic:\n  TrackingMDRef() = default;\n  explicit TrackingMDRef(Metadata *MD) : MD(MD) { track(); }\n\n  TrackingMDRef(TrackingMDRef &&X) : MD(X.MD) { retrack(X); }\n  TrackingMDRef(const TrackingMDRef &X) : MD(X.MD) { track(); }\n\n  TrackingMDRef &operator=(TrackingMDRef &&X) {\n    if (&X == this)\n      return *this;\n\n    untrack();\n    MD = X.MD;\n    retrack(X);\n    return *this;\n  }\n\n  TrackingMDRef &operator=(const TrackingMDRef &X) {\n    if (&X == this)\n      return *this;\n\n    untrack();\n    MD = X.MD;\n    track();\n    return *this;\n  }\n\n  ~TrackingMDRef() { untrack(); }\n\n  Metadata *get() const { return MD; }\n  operator Metadata *() const { return get(); }\n  Metadata *operator->() const { return get(); }\n  Metadata &operator*() const { return *get(); }\n\n  void reset() {\n    untrack();\n    MD = nullptr;\n  }\n  void reset(Metadata *MD) {\n    untrack();\n    this->MD = MD;\n    track();\n  }\n\n  /// Check whether this has a trivial destructor.\n  ///\n  /// If \\c MD isn't replaceable, the destructor will be a no-op.\n  bool hasTrivialDestructor() const {\n    return !MD || !MetadataTracking::isReplaceable(*MD);\n  }\n\n  bool operator==(const TrackingMDRef &X) const { return MD == X.MD; }\n  bool operator!=(const TrackingMDRef &X) const { return MD != X.MD; }\n\nprivate:\n  void track() {\n    if (MD)\n      MetadataTracking::track(MD);\n  }\n\n  void untrack() {\n    if (MD)\n      MetadataTracking::untrack(MD);\n  }\n\n  void retrack(TrackingMDRef &X) {\n    assert(MD == X.MD && \"Expected values to match\");\n    if (X.MD) {\n      MetadataTracking::retrack(X.MD, MD);\n      X.MD = nullptr;\n    }\n  }\n};\n\n/// Typed tracking ref.\n///\n/// Track refererences of a particular type.  It's useful to use this for \\a\n/// MDNode and \\a ValueAsMetadata.\ntemplate <class T> class TypedTrackingMDRef {\n  TrackingMDRef Ref;\n\npublic:\n  TypedTrackingMDRef() = default;\n  explicit TypedTrackingMDRef(T *MD) : Ref(static_cast<Metadata *>(MD)) {}\n\n  TypedTrackingMDRef(TypedTrackingMDRef &&X) : Ref(std::move(X.Ref)) {}\n  TypedTrackingMDRef(const TypedTrackingMDRef &X) : Ref(X.Ref) {}\n\n  TypedTrackingMDRef &operator=(TypedTrackingMDRef &&X) {\n    Ref = std::move(X.Ref);\n    return *this;\n  }\n\n  TypedTrackingMDRef &operator=(const TypedTrackingMDRef &X) {\n    Ref = X.Ref;\n    return *this;\n  }\n\n  T *get() const { return (T *)Ref.get(); }\n  operator T *() const { return get(); }\n  T *operator->() const { return get(); }\n  T &operator*() const { return *get(); }\n\n  bool operator==(const TypedTrackingMDRef &X) const { return Ref == X.Ref; }\n  bool operator!=(const TypedTrackingMDRef &X) const { return Ref != X.Ref; }\n\n  void reset() { Ref.reset(); }\n  void reset(T *MD) { Ref.reset(static_cast<Metadata *>(MD)); }\n\n  /// Check whether this has a trivial destructor.\n  bool hasTrivialDestructor() const { return Ref.hasTrivialDestructor(); }\n};\n\nusing TrackingMDNodeRef = TypedTrackingMDRef<MDNode>;\nusing TrackingValueAsMetadataRef = TypedTrackingMDRef<ValueAsMetadata>;\n\n// Expose the underlying metadata to casting.\ntemplate <> struct simplify_type<TrackingMDRef> {\n  using SimpleType = Metadata *;\n\n  static SimpleType getSimplifiedValue(TrackingMDRef &MD) { return MD.get(); }\n};\n\ntemplate <> struct simplify_type<const TrackingMDRef> {\n  using SimpleType = Metadata *;\n\n  static SimpleType getSimplifiedValue(const TrackingMDRef &MD) {\n    return MD.get();\n  }\n};\n\ntemplate <class T> struct simplify_type<TypedTrackingMDRef<T>> {\n  using SimpleType = T *;\n\n  static SimpleType getSimplifiedValue(TypedTrackingMDRef<T> &MD) {\n    return MD.get();\n  }\n};\n\ntemplate <class T> struct simplify_type<const TypedTrackingMDRef<T>> {\n  using SimpleType = T *;\n\n  static SimpleType getSimplifiedValue(const TypedTrackingMDRef<T> &MD) {\n    return MD.get();\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_TRACKINGMDREF_H\n"}}, "reports": [{"events": [{"location": {"col": 10, "file": 12, "line": 608}, "message": "default constructor 'EvalResult' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/include/clang/AST/Expr.h", "reportHash": "bd8789c5f10351eade7298be1d50634a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 24, "line": 661}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/include/clang/AST/Type.h", "reportHash": "a8ee5bb5b5707b0a993738e3a2c07aa5", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 38, "line": 24}, "message": "destructor '~Address' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/Address.h", "reportHash": "1e6e71514ab70e2bfd43a96c207dbb5d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 38, "line": 24}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/Address.h", "reportHash": "8a30c4bd80a7e85ba46634f8ae75bfa1", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 38, "line": 24}, "message": "move constructor 'Address' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/Address.h", "reportHash": "10b1b7541a3ccb50b3e099cdbaf94e3b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 38, "line": 74}, "message": "destructor '~ConstantAddress' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/Address.h", "reportHash": "2a82ce903319fa1d8f3a00a92ea87ea3", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 38, "line": 74}, "message": "move constructor 'ConstantAddress' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/Address.h", "reportHash": "88e067b72b14545fb2a2e4e6b136303e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 40, "line": 37}, "message": "destructor '~CatchTypeInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCleanup.h", "reportHash": "19eccd5a93d1f8306c2e2106073be960", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 40, "line": 37}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCleanup.h", "reportHash": "48984fe732f490b3b817b6611558122f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 40, "line": 37}, "message": "move constructor 'CatchTypeInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCleanup.h", "reportHash": "2b614606a7aa878015316b321ae13884", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 40, "line": 251}, "message": "default constructor 'ExtInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCleanup.h", "reportHash": "5802e0b53b1120d3e0d7d7e999a46044", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 41, "line": 91}, "message": "default constructor 'BinOpInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "70d5f337a9f7360ac7984d204fe9ff3e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 41, "line": 91}, "message": "move constructor 'BinOpInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "352966a99a5783648d11a2386fabfd7b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 41, "line": 334}, "message": "destructor '~ScalarConversionOpts' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "7d54262d4d7af02733fc0a613855a25f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 41, "line": 334}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "5d659dcb12b74365ff95943dbbc1e405", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 41, "line": 334}, "message": "move constructor 'ScalarConversionOpts' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "1a0de164ffe997758e9ae97534db755e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 29, "file": 41, "line": 1077}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "88001a846b8050aa28bbdc5b6325a59c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 29, "file": 41, "line": 1077}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "c9192ee92cab8e825812a16759f38f00", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 41, "line": 4893}, "message": "destructor '~' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "10b3073f37087f7e0f46284abec02e2a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 41, "line": 4893}, "message": "move constructor '' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "0efa61e1207cbd8bbfcd75fbb490c16e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 42, "line": 1182}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "24c3406009199acedd09ab6c6730d5d6", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 42, "line": 1196}, "message": "destructor '~OpaqueValueMappingData' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "b813f0a2b92e74b03f60e781217de95a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 42, "line": 1196}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "9d17205f1a8bdf27bd94d9811bce26f4", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 42, "line": 1585}, "message": "destructor '~CXXDefaultArgExprScope' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "4092c76ca94475df82e78c1165e33872", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 42, "line": 2708}, "message": "move constructor 'VlaSizePair' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "0c9ef6afd4266dc417972e28182a55ba", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 42, "line": 2974}, "message": "destructor '~AutoVarEmission' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "ad4f6b25af682f535a9fef38d5109b74", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 43, "line": 167}, "message": "move constructor 'AbstractState' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/ConstantEmitter.h", "reportHash": "70a150699e1610269358f0d056817f83", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 46, "file": 62, "line": 18}, "message": "destructor '~ilist_base' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_base.h", "reportHash": "394e3bc7c5167395c91795d606fcc187", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 63, "line": 57}, "message": "destructor '~ilist_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_iterator.h", "reportHash": "8f563dd1dc109dc71cc56be23aa1e745", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 63, "line": 57}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_iterator.h", "reportHash": "24ca98c58c96d1c4e1f60f193fbdf9c7", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 63, "line": 57}, "message": "move constructor 'ilist_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_iterator.h", "reportHash": "726a403042fce4f140a56bc91876845d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 64, "line": 163}, "message": "default constructor 'NodeAccess' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "f91de1ed12b78e5a84e872416a716b9d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 64, "line": 163}, "message": "destructor '~NodeAccess' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "e9d366ae92447e17a0e33a5b38766f00", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 64, "line": 163}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "a7709221537fcc3efe5db4be302dc63a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 64, "line": 163}, "message": "move constructor 'NodeAccess' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "451ca64e674c3fc99febd94e0d2a08d0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 34, "file": 64, "line": 210}, "message": "default constructor 'SpecificNodeAccess' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "809af27a2eb8327a8bac509992c96132", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 34, "file": 64, "line": 210}, "message": "destructor '~SpecificNodeAccess' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "a9529ac9c7bd5a87cd12db6c21db082f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 34, "file": 64, "line": 210}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "fa93fe5bdab20ed3b31580d6eedcbbdc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 34, "file": 64, "line": 210}, "message": "move constructor 'SpecificNodeAccess' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "cf3736221d39c6a64afc5a56c159862c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 64, "line": 236}, "message": "destructor '~ilist_sentinel' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "d2faed66b928ac55d2cabf7560836699", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 68, "line": 365}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Attributes.h", "reportHash": "cd9b1dc42ec64a14dd8911e15e859126", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 69, "line": 315}, "message": "destructor '~phi_iterator_impl' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/BasicBlock.h", "reportHash": "13ae74f18802af3c32a20a14ac268855", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 69, "line": 315}, "message": "move constructor 'phi_iterator_impl' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/BasicBlock.h", "reportHash": "e4a1b2ffcc52fe5705e1add1097485aa", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 71, "line": 71}, "message": "destructor '~LayoutAlignElem' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DataLayout.h", "reportHash": "65ddd75940b8ff41e4125bc343c70d0c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 71, "line": 71}, "message": "move constructor 'LayoutAlignElem' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DataLayout.h", "reportHash": "8054820e8a1f03bd8d9dd77779465ffb", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 71, "line": 90}, "message": "destructor '~PointerAlignElem' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DataLayout.h", "reportHash": "38667fc3bb2ed248257893f993f08f2e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 71, "line": 90}, "message": "move constructor 'PointerAlignElem' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DataLayout.h", "reportHash": "98f3bad45b347cffffc69e06b8578a9d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 73, "line": 33}, "message": "destructor '~DebugLoc' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DebugLoc.h", "reportHash": "9dc981fcb1ad28c99b14d4b5bc0a532a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 77, "line": 251}, "message": "destructor '~InsertPoint' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/IRBuilder.h", "reportHash": "c91c1feabf5b212e1258c23cfe5b4c78", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 78, "line": 57}, "message": "destructor '~UnaryInstruction' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InstrTypes.h", "reportHash": "537b9d93918ad6c893f5e9ee16474a29", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 78, "line": 712}, "message": "destructor '~CmpInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InstrTypes.h", "reportHash": "5565b3a3e1e7074e6ad2cc5d3f6a7278", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 78, "line": 2291}, "message": "destructor '~FuncletPadInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InstrTypes.h", "reportHash": "ca81d8cc7646aceda16b811b2b9729cb", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 79, "line": 41}, "message": "destructor '~ilist_alloc_traits' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instruction.h", "reportHash": "33b7211f01454d8db130ad13fcd3169b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 85, "line": 66}, "message": "destructor '~SymbolTableListTraits' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/SymbolTableListTraits.h", "reportHash": "3638b357344e25058ad8a975b3b717ab", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 85, "line": 114}, "message": "destructor '~SymbolTableList' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/SymbolTableListTraits.h", "reportHash": "951348ce84095044a2a090f28863c773", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 26, "file": 86, "line": 106}, "message": "destructor '~TypedTrackingMDRef' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/TrackingMDRef.h", "reportHash": "bb0d39789b42bebbe8d8f0b10dc2c313", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
