<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"10": {"id": 10, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/IndexedMap.h", "content": "//===- llvm/ADT/IndexedMap.h - An index map implementation ------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file implements an indexed map. The index map template takes two\n// types. The first is the mapped type and the second is a functor\n// that maps its argument to a size_t. On instantiation a \"null\" value\n// can be provided to be used as a \"does not exist\" indicator in the\n// map. A member function grow() is provided that given the value of\n// the maximally indexed key (the argument of the functor) makes sure\n// the map has enough space for it.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_INDEXEDMAP_H\n#define LLVM_ADT_INDEXEDMAP_H\n\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include <cassert>\n\nnamespace llvm {\n\ntemplate <typename T, typename ToIndexT = identity<unsigned>>\n  class IndexedMap {\n    using IndexT = typename ToIndexT::argument_type;\n    // Prefer SmallVector with zero inline storage over std::vector. IndexedMaps\n    // can grow very large and SmallVector grows more efficiently as long as T\n    // is trivially copyable.\n    using StorageT = SmallVector<T, 0>;\n\n    StorageT storage_;\n    T nullVal_;\n    ToIndexT toIndex_;\n\n  public:\n    IndexedMap() : nullVal_(T()) {}\n\n    explicit IndexedMap(const T& val) : nullVal_(val) {}\n\n    typename StorageT::reference operator[](IndexT n) {\n      assert(toIndex_(n) < storage_.size() && \"index out of bounds!\");\n      return storage_[toIndex_(n)];\n    }\n\n    typename StorageT::const_reference operator[](IndexT n) const {\n      assert(toIndex_(n) < storage_.size() && \"index out of bounds!\");\n      return storage_[toIndex_(n)];\n    }\n\n    void reserve(typename StorageT::size_type s) {\n      storage_.reserve(s);\n    }\n\n    void resize(typename StorageT::size_type s) {\n      storage_.resize(s, nullVal_);\n    }\n\n    void clear() {\n      storage_.clear();\n    }\n\n    void grow(IndexT n) {\n      unsigned NewSize = toIndex_(n) + 1;\n      if (NewSize > storage_.size())\n        resize(NewSize);\n    }\n\n    bool inBounds(IndexT n) const {\n      return toIndex_(n) < storage_.size();\n    }\n\n    typename StorageT::size_type size() const {\n      return storage_.size();\n    }\n  };\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_INDEXEDMAP_H\n"}, "25": {"id": 25, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/SparseSet.h", "content": "//===- llvm/ADT/SparseSet.h - Sparse set ------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the SparseSet class derived from the version described in\n// Briggs, Torczon, \"An efficient representation for sparse sets\", ACM Letters\n// on Programming Languages and Systems, Volume 2 Issue 1-4, March-Dec.  1993.\n//\n// A sparse set holds a small number of objects identified by integer keys from\n// a moderately sized universe. The sparse set uses more memory than other\n// containers in order to provide faster operations.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_SPARSESET_H\n#define LLVM_ADT_SPARSESET_H\n\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/Support/AllocatorBase.h\"\n#include <cassert>\n#include <cstdint>\n#include <cstdlib>\n#include <limits>\n#include <utility>\n\nnamespace llvm {\n\n/// SparseSetValTraits - Objects in a SparseSet are identified by keys that can\n/// be uniquely converted to a small integer less than the set's universe. This\n/// class allows the set to hold values that differ from the set's key type as\n/// long as an index can still be derived from the value. SparseSet never\n/// directly compares ValueT, only their indices, so it can map keys to\n/// arbitrary values. SparseSetValTraits computes the index from the value\n/// object. To compute the index from a key, SparseSet uses a separate\n/// KeyFunctorT template argument.\n///\n/// A simple type declaration, SparseSet<Type>, handles these cases:\n/// - unsigned key, identity index, identity value\n/// - unsigned key, identity index, fat value providing getSparseSetIndex()\n///\n/// The type declaration SparseSet<Type, UnaryFunction> handles:\n/// - unsigned key, remapped index, identity value (virtual registers)\n/// - pointer key, pointer-derived index, identity value (node+ID)\n/// - pointer key, pointer-derived index, fat value with getSparseSetIndex()\n///\n/// Only other, unexpected cases require specializing SparseSetValTraits.\n///\n/// For best results, ValueT should not require a destructor.\n///\ntemplate<typename ValueT>\nstruct SparseSetValTraits {\n  static unsigned getValIndex(const ValueT &Val) {\n    return Val.getSparseSetIndex();\n  }\n};\n\n/// SparseSetValFunctor - Helper class for selecting SparseSetValTraits. The\n/// generic implementation handles ValueT classes which either provide\n/// getSparseSetIndex() or specialize SparseSetValTraits<>.\n///\ntemplate<typename KeyT, typename ValueT, typename KeyFunctorT>\nstruct SparseSetValFunctor {\n  unsigned operator()(const ValueT &Val) const {\n    return SparseSetValTraits<ValueT>::getValIndex(Val);\n  }\n};\n\n/// SparseSetValFunctor<KeyT, KeyT> - Helper class for the common case of\n/// identity key/value sets.\ntemplate<typename KeyT, typename KeyFunctorT>\nstruct SparseSetValFunctor<KeyT, KeyT, KeyFunctorT> {\n  unsigned operator()(const KeyT &Key) const {\n    return KeyFunctorT()(Key);\n  }\n};\n\n/// SparseSet - Fast set implementation for objects that can be identified by\n/// small unsigned keys.\n///\n/// SparseSet allocates memory proportional to the size of the key universe, so\n/// it is not recommended for building composite data structures.  It is useful\n/// for algorithms that require a single set with fast operations.\n///\n/// Compared to DenseSet and DenseMap, SparseSet provides constant-time fast\n/// clear() and iteration as fast as a vector.  The find(), insert(), and\n/// erase() operations are all constant time, and typically faster than a hash\n/// table.  The iteration order doesn't depend on numerical key values, it only\n/// depends on the order of insert() and erase() operations.  When no elements\n/// have been erased, the iteration order is the insertion order.\n///\n/// Compared to BitVector, SparseSet<unsigned> uses 8x-40x more memory, but\n/// offers constant-time clear() and size() operations as well as fast\n/// iteration independent on the size of the universe.\n///\n/// SparseSet contains a dense vector holding all the objects and a sparse\n/// array holding indexes into the dense vector.  Most of the memory is used by\n/// the sparse array which is the size of the key universe.  The SparseT\n/// template parameter provides a space/speed tradeoff for sets holding many\n/// elements.\n///\n/// When SparseT is uint32_t, find() only touches 2 cache lines, but the sparse\n/// array uses 4 x Universe bytes.\n///\n/// When SparseT is uint8_t (the default), find() touches up to 2+[N/256] cache\n/// lines, but the sparse array is 4x smaller.  N is the number of elements in\n/// the set.\n///\n/// For sets that may grow to thousands of elements, SparseT should be set to\n/// uint16_t or uint32_t.\n///\n/// @tparam ValueT      The type of objects in the set.\n/// @tparam KeyFunctorT A functor that computes an unsigned index from KeyT.\n/// @tparam SparseT     An unsigned integer type. See above.\n///\ntemplate<typename ValueT,\n         typename KeyFunctorT = identity<unsigned>,\n         typename SparseT = uint8_t>\nclass SparseSet {\n  static_assert(std::numeric_limits<SparseT>::is_integer &&\n                !std::numeric_limits<SparseT>::is_signed,\n                \"SparseT must be an unsigned integer type\");\n\n  using KeyT = typename KeyFunctorT::argument_type;\n  using DenseT = SmallVector<ValueT, 8>;\n  using size_type = unsigned;\n  DenseT Dense;\n  SparseT *Sparse = nullptr;\n  unsigned Universe = 0;\n  KeyFunctorT KeyIndexOf;\n  SparseSetValFunctor<KeyT, ValueT, KeyFunctorT> ValIndexOf;\n\npublic:\n  using value_type = ValueT;\n  using reference = ValueT &;\n  using const_reference = const ValueT &;\n  using pointer = ValueT *;\n  using const_pointer = const ValueT *;\n\n  SparseSet() = default;\n  SparseSet(const SparseSet &) = delete;\n  SparseSet &operator=(const SparseSet &) = delete;\n  ~SparseSet() { free(Sparse); }\n\n  /// setUniverse - Set the universe size which determines the largest key the\n  /// set can hold.  The universe must be sized before any elements can be\n  /// added.\n  ///\n  /// @param U Universe size. All object keys must be less than U.\n  ///\n  void setUniverse(unsigned U) {\n    // It's not hard to resize the universe on a non-empty set, but it doesn't\n    // seem like a likely use case, so we can add that code when we need it.\n    assert(empty() && \"Can only resize universe on an empty map\");\n    // Hysteresis prevents needless reallocations.\n    if (U >= Universe/4 && U <= Universe)\n      return;\n    free(Sparse);\n    // The Sparse array doesn't actually need to be initialized, so malloc\n    // would be enough here, but that will cause tools like valgrind to\n    // complain about branching on uninitialized data.\n    Sparse = static_cast<SparseT*>(safe_calloc(U, sizeof(SparseT)));\n    Universe = U;\n  }\n\n  // Import trivial vector stuff from DenseT.\n  using iterator = typename DenseT::iterator;\n  using const_iterator = typename DenseT::const_iterator;\n\n  const_iterator begin() const { return Dense.begin(); }\n  const_iterator end() const { return Dense.end(); }\n  iterator begin() { return Dense.begin(); }\n  iterator end() { return Dense.end(); }\n\n  /// empty - Returns true if the set is empty.\n  ///\n  /// This is not the same as BitVector::empty().\n  ///\n  bool empty() const { return Dense.empty(); }\n\n  /// size - Returns the number of elements in the set.\n  ///\n  /// This is not the same as BitVector::size() which returns the size of the\n  /// universe.\n  ///\n  size_type size() const { return Dense.size(); }\n\n  /// clear - Clears the set.  This is a very fast constant time operation.\n  ///\n  void clear() {\n    // Sparse does not need to be cleared, see find().\n    Dense.clear();\n  }\n\n  /// findIndex - Find an element by its index.\n  ///\n  /// @param   Idx A valid index to find.\n  /// @returns An iterator to the element identified by key, or end().\n  ///\n  iterator findIndex(unsigned Idx) {\n    assert(Idx < Universe && \"Key out of range\");\n    const unsigned Stride = std::numeric_limits<SparseT>::max() + 1u;\n    for (unsigned i = Sparse[Idx], e = size(); i < e; i += Stride) {\n      const unsigned FoundIdx = ValIndexOf(Dense[i]);\n      assert(FoundIdx < Universe && \"Invalid key in set. Did object mutate?\");\n      if (Idx == FoundIdx)\n        return begin() + i;\n      // Stride is 0 when SparseT >= unsigned.  We don't need to loop.\n      if (!Stride)\n        break;\n    }\n    return end();\n  }\n\n  /// find - Find an element by its key.\n  ///\n  /// @param   Key A valid key to find.\n  /// @returns An iterator to the element identified by key, or end().\n  ///\n  iterator find(const KeyT &Key) {\n    return findIndex(KeyIndexOf(Key));\n  }\n\n  const_iterator find(const KeyT &Key) const {\n    return const_cast<SparseSet*>(this)->findIndex(KeyIndexOf(Key));\n  }\n\n  /// Check if the set contains the given \\c Key.\n  ///\n  /// @param Key A valid key to find.\n  bool contains(const KeyT &Key) const { return find(Key) == end() ? 0 : 1; }\n\n  /// count - Returns 1 if this set contains an element identified by Key,\n  /// 0 otherwise.\n  ///\n  size_type count(const KeyT &Key) const { return contains(Key) ? 1 : 0; }\n\n  /// insert - Attempts to insert a new element.\n  ///\n  /// If Val is successfully inserted, return (I, true), where I is an iterator\n  /// pointing to the newly inserted element.\n  ///\n  /// If the set already contains an element with the same key as Val, return\n  /// (I, false), where I is an iterator pointing to the existing element.\n  ///\n  /// Insertion invalidates all iterators.\n  ///\n  std::pair<iterator, bool> insert(const ValueT &Val) {\n    unsigned Idx = ValIndexOf(Val);\n    iterator I = findIndex(Idx);\n    if (I != end())\n      return std::make_pair(I, false);\n    Sparse[Idx] = size();\n    Dense.push_back(Val);\n    return std::make_pair(end() - 1, true);\n  }\n\n  /// array subscript - If an element already exists with this key, return it.\n  /// Otherwise, automatically construct a new value from Key, insert it,\n  /// and return the newly inserted element.\n  ValueT &operator[](const KeyT &Key) {\n    return *insert(ValueT(Key)).first;\n  }\n\n  ValueT pop_back_val() {\n    // Sparse does not need to be cleared, see find().\n    return Dense.pop_back_val();\n  }\n\n  /// erase - Erases an existing element identified by a valid iterator.\n  ///\n  /// This invalidates all iterators, but erase() returns an iterator pointing\n  /// to the next element.  This makes it possible to erase selected elements\n  /// while iterating over the set:\n  ///\n  ///   for (SparseSet::iterator I = Set.begin(); I != Set.end();)\n  ///     if (test(*I))\n  ///       I = Set.erase(I);\n  ///     else\n  ///       ++I;\n  ///\n  /// Note that end() changes when elements are erased, unlike std::list.\n  ///\n  iterator erase(iterator I) {\n    assert(unsigned(I - begin()) < size() && \"Invalid iterator\");\n    if (I != end() - 1) {\n      *I = Dense.back();\n      unsigned BackIdx = ValIndexOf(Dense.back());\n      assert(BackIdx < Universe && \"Invalid key in set. Did object mutate?\");\n      Sparse[BackIdx] = I - begin();\n    }\n    // This depends on SmallVector::pop_back() not invalidating iterators.\n    // std::vector::pop_back() doesn't give that guarantee.\n    Dense.pop_back();\n    return I;\n  }\n\n  /// erase - Erases an element identified by Key, if it exists.\n  ///\n  /// @param   Key The key identifying the element to erase.\n  /// @returns True when an element was erased, false if no element was found.\n  ///\n  bool erase(const KeyT &Key) {\n    iterator I = find(Key);\n    if (I == end())\n      return false;\n    erase(I);\n    return true;\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_SPARSESET_H\n"}, "38": {"id": 38, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CSEInfo.h", "content": "//===- llvm/CodeGen/GlobalISel/CSEInfo.h ------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// Provides analysis for continuously CSEing during GISel passes.\n//\n//===----------------------------------------------------------------------===//\n#ifndef LLVM_CODEGEN_GLOBALISEL_CSEINFO_H\n#define LLVM_CODEGEN_GLOBALISEL_CSEINFO_H\n\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/CodeGen/CSEConfigBase.h\"\n#include \"llvm/CodeGen/GlobalISel/GISelChangeObserver.h\"\n#include \"llvm/CodeGen/GlobalISel/GISelWorkList.h\"\n#include \"llvm/CodeGen/MachineFunctionPass.h\"\n#include \"llvm/Support/Allocator.h\"\n#include \"llvm/Support/CodeGen.h\"\n\nnamespace llvm {\nclass MachineBasicBlock;\n\n/// A class that wraps MachineInstrs and derives from FoldingSetNode in order to\n/// be uniqued in a CSEMap. The tradeoff here is extra memory allocations for\n/// UniqueMachineInstr vs making MachineInstr bigger.\nclass UniqueMachineInstr : public FoldingSetNode {\n  friend class GISelCSEInfo;\n  const MachineInstr *MI;\n  explicit UniqueMachineInstr(const MachineInstr *MI) : MI(MI) {}\n\npublic:\n  void Profile(FoldingSetNodeID &ID);\n};\n\n// A CSE config for fully optimized builds.\nclass CSEConfigFull : public CSEConfigBase {\npublic:\n  virtual ~CSEConfigFull() = default;\n  virtual bool shouldCSEOpc(unsigned Opc) override;\n};\n\n// Commonly used for O0 config.\nclass CSEConfigConstantOnly : public CSEConfigBase {\npublic:\n  virtual ~CSEConfigConstantOnly() = default;\n  virtual bool shouldCSEOpc(unsigned Opc) override;\n};\n\n// Returns the standard expected CSEConfig for the given optimization level.\n// We have this logic here so targets can make use of it from their derived\n// TargetPassConfig, but can't put this logic into TargetPassConfig directly\n// because the CodeGen library can't depend on GlobalISel.\nstd::unique_ptr<CSEConfigBase>\ngetStandardCSEConfigForOpt(CodeGenOpt::Level Level);\n\n/// The CSE Analysis object.\n/// This installs itself as a delegate to the MachineFunction to track\n/// new instructions as well as deletions. It however will not be able to\n/// track instruction mutations. In such cases, recordNewInstruction should be\n/// called (for eg inside MachineIRBuilder::recordInsertion).\n/// Also because of how just the instruction can be inserted without adding any\n/// operands to the instruction, instructions are uniqued and inserted lazily.\n/// CSEInfo should assert when trying to enter an incomplete instruction into\n/// the CSEMap. There is Opcode level granularity on which instructions can be\n/// CSE'd and for now, only Generic instructions are CSEable.\nclass GISelCSEInfo : public GISelChangeObserver {\n  // Make it accessible only to CSEMIRBuilder.\n  friend class CSEMIRBuilder;\n\n  BumpPtrAllocator UniqueInstrAllocator;\n  FoldingSet<UniqueMachineInstr> CSEMap;\n  MachineRegisterInfo *MRI = nullptr;\n  MachineFunction *MF = nullptr;\n  std::unique_ptr<CSEConfigBase> CSEOpt;\n  /// Keep a cache of UniqueInstrs for each MachineInstr. In GISel,\n  /// often instructions are mutated (while their ID has completely changed).\n  /// Whenever mutation happens, invalidate the UniqueMachineInstr for the\n  /// MachineInstr\n  DenseMap<const MachineInstr *, UniqueMachineInstr *> InstrMapping;\n\n  /// Store instructions that are not fully formed in TemporaryInsts.\n  /// Also because CSE insertion happens lazily, we can remove insts from this\n  /// list and avoid inserting and then removing from the CSEMap.\n  GISelWorkList<8> TemporaryInsts;\n\n  // Only used in asserts.\n  DenseMap<unsigned, unsigned> OpcodeHitTable;\n\n  bool isUniqueMachineInstValid(const UniqueMachineInstr &UMI) const;\n\n  void invalidateUniqueMachineInstr(UniqueMachineInstr *UMI);\n\n  UniqueMachineInstr *getNodeIfExists(FoldingSetNodeID &ID,\n                                      MachineBasicBlock *MBB, void *&InsertPos);\n\n  /// Allocate and construct a new UniqueMachineInstr for MI and return.\n  UniqueMachineInstr *getUniqueInstrForMI(const MachineInstr *MI);\n\n  void insertNode(UniqueMachineInstr *UMI, void *InsertPos = nullptr);\n\n  /// Get the MachineInstr(Unique) if it exists already in the CSEMap and the\n  /// same MachineBasicBlock.\n  MachineInstr *getMachineInstrIfExists(FoldingSetNodeID &ID,\n                                        MachineBasicBlock *MBB,\n                                        void *&InsertPos);\n\n  /// Use this method to allocate a new UniqueMachineInstr for MI and insert it\n  /// into the CSEMap. MI should return true for shouldCSE(MI->getOpcode())\n  void insertInstr(MachineInstr *MI, void *InsertPos = nullptr);\n\npublic:\n  GISelCSEInfo() = default;\n\n  virtual ~GISelCSEInfo();\n\n  void setMF(MachineFunction &MF);\n\n  Error verify();\n\n  /// Records a newly created inst in a list and lazily insert it to the CSEMap.\n  /// Sometimes, this method might be called with a partially constructed\n  /// MachineInstr,\n  //  (right after BuildMI without adding any operands) - and in such cases,\n  //  defer the hashing of the instruction to a later stage.\n  void recordNewInstruction(MachineInstr *MI);\n\n  /// Use this callback to inform CSE about a newly fully created instruction.\n  void handleRecordedInst(MachineInstr *MI);\n\n  /// Use this callback to insert all the recorded instructions. At this point,\n  /// all of these insts need to be fully constructed and should not be missing\n  /// any operands.\n  void handleRecordedInsts();\n\n  /// Remove this inst from the CSE map. If this inst has not been inserted yet,\n  /// it will be removed from the Tempinsts list if it exists.\n  void handleRemoveInst(MachineInstr *MI);\n\n  void releaseMemory();\n\n  void setCSEConfig(std::unique_ptr<CSEConfigBase> Opt) {\n    CSEOpt = std::move(Opt);\n  }\n\n  bool shouldCSE(unsigned Opc) const;\n\n  void analyze(MachineFunction &MF);\n\n  void countOpcodeHit(unsigned Opc);\n\n  void print();\n\n  // Observer API\n  void erasingInstr(MachineInstr &MI) override;\n  void createdInstr(MachineInstr &MI) override;\n  void changingInstr(MachineInstr &MI) override;\n  void changedInstr(MachineInstr &MI) override;\n};\n\nclass TargetRegisterClass;\nclass RegisterBank;\n\n// Simple builder class to easily profile properties about MIs.\nclass GISelInstProfileBuilder {\n  FoldingSetNodeID &ID;\n  const MachineRegisterInfo &MRI;\n\npublic:\n  GISelInstProfileBuilder(FoldingSetNodeID &ID, const MachineRegisterInfo &MRI)\n      : ID(ID), MRI(MRI) {}\n  // Profiling methods.\n  const GISelInstProfileBuilder &addNodeIDOpcode(unsigned Opc) const;\n  const GISelInstProfileBuilder &addNodeIDRegType(const LLT Ty) const;\n  const GISelInstProfileBuilder &addNodeIDRegType(const Register) const;\n\n  const GISelInstProfileBuilder &\n  addNodeIDRegType(const TargetRegisterClass *RC) const;\n  const GISelInstProfileBuilder &addNodeIDRegType(const RegisterBank *RB) const;\n\n  const GISelInstProfileBuilder &addNodeIDRegNum(Register Reg) const;\n\n  const GISelInstProfileBuilder &addNodeIDReg(Register Reg) const;\n\n  const GISelInstProfileBuilder &addNodeIDImmediate(int64_t Imm) const;\n  const GISelInstProfileBuilder &\n  addNodeIDMBB(const MachineBasicBlock *MBB) const;\n\n  const GISelInstProfileBuilder &\n  addNodeIDMachineOperand(const MachineOperand &MO) const;\n\n  const GISelInstProfileBuilder &addNodeIDFlag(unsigned Flag) const;\n  const GISelInstProfileBuilder &addNodeID(const MachineInstr *MI) const;\n};\n\n/// Simple wrapper that does the following.\n/// 1) Lazily evaluate the MachineFunction to compute CSEable instructions.\n/// 2) Allows configuration of which instructions are CSEd through CSEConfig\n/// object. Provides a method called get which takes a CSEConfig object.\nclass GISelCSEAnalysisWrapper {\n  GISelCSEInfo Info;\n  MachineFunction *MF = nullptr;\n  bool AlreadyComputed = false;\n\npublic:\n  /// Takes a CSEConfigBase object that defines what opcodes get CSEd.\n  /// If CSEConfig is already set, and the CSE Analysis has been preserved,\n  /// it will not use the new CSEOpt(use Recompute to force using the new\n  /// CSEOpt).\n  GISelCSEInfo &get(std::unique_ptr<CSEConfigBase> CSEOpt,\n                    bool ReCompute = false);\n  void setMF(MachineFunction &MFunc) { MF = &MFunc; }\n  void setComputed(bool Computed) { AlreadyComputed = Computed; }\n  void releaseMemory() { Info.releaseMemory(); }\n};\n\n/// The actual analysis pass wrapper.\nclass GISelCSEAnalysisWrapperPass : public MachineFunctionPass {\n  GISelCSEAnalysisWrapper Wrapper;\n\npublic:\n  static char ID;\n  GISelCSEAnalysisWrapperPass();\n\n  void getAnalysisUsage(AnalysisUsage &AU) const override;\n\n  const GISelCSEAnalysisWrapper &getCSEWrapper() const { return Wrapper; }\n  GISelCSEAnalysisWrapper &getCSEWrapper() { return Wrapper; }\n\n  bool runOnMachineFunction(MachineFunction &MF) override;\n\n  void releaseMemory() override {\n    Wrapper.releaseMemory();\n    Wrapper.setComputed(false);\n  }\n};\n\n} // namespace llvm\n\n#endif\n"}, "39": {"id": 39, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CallLowering.h", "content": "//===- llvm/CodeGen/GlobalISel/CallLowering.h - Call lowering ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n///\n/// \\file\n/// This file describes how to lower LLVM calls to machine code calls.\n///\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_CALLLOWERING_H\n#define LLVM_CODEGEN_GLOBALISEL_CALLLOWERING_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/CallingConvLower.h\"\n#include \"llvm/CodeGen/MachineOperand.h\"\n#include \"llvm/CodeGen/TargetCallingConv.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include <cstdint>\n#include <functional>\n\nnamespace llvm {\n\nclass CallBase;\nclass DataLayout;\nclass Function;\nclass FunctionLoweringInfo;\nclass MachineIRBuilder;\nstruct MachinePointerInfo;\nclass MachineRegisterInfo;\nclass TargetLowering;\nclass Value;\n\nclass CallLowering {\n  const TargetLowering *TLI;\n\n  virtual void anchor();\npublic:\n  struct BaseArgInfo {\n    Type *Ty;\n    SmallVector<ISD::ArgFlagsTy, 4> Flags;\n    bool IsFixed;\n\n    BaseArgInfo(Type *Ty,\n                ArrayRef<ISD::ArgFlagsTy> Flags = ArrayRef<ISD::ArgFlagsTy>(),\n                bool IsFixed = true)\n        : Ty(Ty), Flags(Flags.begin(), Flags.end()), IsFixed(IsFixed) {}\n\n    BaseArgInfo() : Ty(nullptr), IsFixed(false) {}\n  };\n\n  struct ArgInfo : public BaseArgInfo {\n    SmallVector<Register, 4> Regs;\n    // If the argument had to be split into multiple parts according to the\n    // target calling convention, then this contains the original vregs\n    // if the argument was an incoming arg.\n    SmallVector<Register, 2> OrigRegs;\n\n    ArgInfo(ArrayRef<Register> Regs, Type *Ty,\n            ArrayRef<ISD::ArgFlagsTy> Flags = ArrayRef<ISD::ArgFlagsTy>(),\n            bool IsFixed = true)\n        : BaseArgInfo(Ty, Flags, IsFixed), Regs(Regs.begin(), Regs.end()) {\n      if (!Regs.empty() && Flags.empty())\n        this->Flags.push_back(ISD::ArgFlagsTy());\n      // FIXME: We should have just one way of saying \"no register\".\n      assert(((Ty->isVoidTy() || Ty->isEmptyTy()) ==\n              (Regs.empty() || Regs[0] == 0)) &&\n             \"only void types should have no register\");\n    }\n\n    ArgInfo() : BaseArgInfo() {}\n  };\n\n  struct CallLoweringInfo {\n    /// Calling convention to be used for the call.\n    CallingConv::ID CallConv = CallingConv::C;\n\n    /// Destination of the call. It should be either a register, globaladdress,\n    /// or externalsymbol.\n    MachineOperand Callee = MachineOperand::CreateImm(0);\n\n    /// Descriptor for the return type of the function.\n    ArgInfo OrigRet;\n\n    /// List of descriptors of the arguments passed to the function.\n    SmallVector<ArgInfo, 8> OrigArgs;\n\n    /// Valid if the call has a swifterror inout parameter, and contains the\n    /// vreg that the swifterror should be copied into after the call.\n    Register SwiftErrorVReg;\n\n    MDNode *KnownCallees = nullptr;\n\n    /// True if the call must be tail call optimized.\n    bool IsMustTailCall = false;\n\n    /// True if the call passes all target-independent checks for tail call\n    /// optimization.\n    bool IsTailCall = false;\n\n    /// True if the call was lowered as a tail call. This is consumed by the\n    /// legalizer. This allows the legalizer to lower libcalls as tail calls.\n    bool LoweredTailCall = false;\n\n    /// True if the call is to a vararg function.\n    bool IsVarArg = false;\n\n    /// True if the function's return value can be lowered to registers.\n    bool CanLowerReturn = true;\n\n    /// VReg to hold the hidden sret parameter.\n    Register DemoteRegister;\n\n    /// The stack index for sret demotion.\n    int DemoteStackIndex;\n  };\n\n  /// Argument handling is mostly uniform between the four places that\n  /// make these decisions: function formal arguments, call\n  /// instruction args, call instruction returns and function\n  /// returns. However, once a decision has been made on where an\n  /// argument should go, exactly what happens can vary slightly. This\n  /// class abstracts the differences.\n  struct ValueHandler {\n    ValueHandler(bool IsIncoming, MachineIRBuilder &MIRBuilder,\n                 MachineRegisterInfo &MRI, CCAssignFn *AssignFn)\n        : MIRBuilder(MIRBuilder), MRI(MRI), AssignFn(AssignFn),\n          IsIncomingArgumentHandler(IsIncoming) {}\n\n    virtual ~ValueHandler() = default;\n\n    /// Returns true if the handler is dealing with incoming arguments,\n    /// i.e. those that move values from some physical location to vregs.\n    bool isIncomingArgumentHandler() const {\n      return IsIncomingArgumentHandler;\n    }\n\n    /// Materialize a VReg containing the address of the specified\n    /// stack-based object. This is either based on a FrameIndex or\n    /// direct SP manipulation, depending on the context. \\p MPO\n    /// should be initialized to an appropriate description of the\n    /// address created.\n    virtual Register getStackAddress(uint64_t Size, int64_t Offset,\n                                     MachinePointerInfo &MPO) = 0;\n\n    /// The specified value has been assigned to a physical register,\n    /// handle the appropriate COPY (either to or from) and mark any\n    /// relevant uses/defines as needed.\n    virtual void assignValueToReg(Register ValVReg, Register PhysReg,\n                                  CCValAssign &VA) = 0;\n\n    /// The specified value has been assigned to a stack\n    /// location. Load or store it there, with appropriate extension\n    /// if necessary.\n    virtual void assignValueToAddress(Register ValVReg, Register Addr,\n                                      uint64_t Size, MachinePointerInfo &MPO,\n                                      CCValAssign &VA) = 0;\n\n    /// An overload which takes an ArgInfo if additional information about the\n    /// arg is needed. \\p ValRegIndex is the index in \\p Arg.Regs for the value\n    /// to store.\n    virtual void assignValueToAddress(const ArgInfo &Arg, unsigned ValRegIndex,\n                                      Register Addr, uint64_t Size,\n                                      MachinePointerInfo &MPO,\n                                      CCValAssign &VA) {\n      assignValueToAddress(Arg.Regs[ValRegIndex], Addr, Size, MPO, VA);\n    }\n\n    /// Handle custom values, which may be passed into one or more of \\p VAs.\n    /// \\return The number of \\p VAs that have been assigned after the first\n    ///         one, and which should therefore be skipped from further\n    ///         processing.\n    virtual unsigned assignCustomValue(const ArgInfo &Arg,\n                                       ArrayRef<CCValAssign> VAs) {\n      // This is not a pure virtual method because not all targets need to worry\n      // about custom values.\n      llvm_unreachable(\"Custom values not supported\");\n    }\n\n    /// Extend a register to the location type given in VA, capped at extending\n    /// to at most MaxSize bits. If MaxSizeBits is 0 then no maximum is set.\n    Register extendRegister(Register ValReg, CCValAssign &VA,\n                            unsigned MaxSizeBits = 0);\n\n    virtual bool assignArg(unsigned ValNo, MVT ValVT, MVT LocVT,\n                           CCValAssign::LocInfo LocInfo, const ArgInfo &Info,\n                           ISD::ArgFlagsTy Flags, CCState &State) {\n      return AssignFn(ValNo, ValVT, LocVT, LocInfo, Flags, State);\n    }\n\n    MachineIRBuilder &MIRBuilder;\n    MachineRegisterInfo &MRI;\n    CCAssignFn *AssignFn;\n\n  private:\n    bool IsIncomingArgumentHandler;\n    virtual void anchor();\n  };\n\n  struct IncomingValueHandler : public ValueHandler {\n    IncomingValueHandler(MachineIRBuilder &MIRBuilder, MachineRegisterInfo &MRI,\n                         CCAssignFn *AssignFn)\n        : ValueHandler(true, MIRBuilder, MRI, AssignFn) {}\n\n    /// Insert G_ASSERT_ZEXT/G_ASSERT_SEXT or other hint instruction based on \\p\n    /// VA, returning the new register if a hint was inserted.\n    Register buildExtensionHint(CCValAssign &VA, Register SrcReg, LLT NarrowTy);\n\n    /// Provides a default implementation for argument handling.\n    void assignValueToReg(Register ValVReg, Register PhysReg,\n                          CCValAssign &VA) override;\n  };\n\n  struct OutgoingValueHandler : public ValueHandler {\n    OutgoingValueHandler(MachineIRBuilder &MIRBuilder, MachineRegisterInfo &MRI,\n                         CCAssignFn *AssignFn)\n        : ValueHandler(false, MIRBuilder, MRI, AssignFn) {}\n  };\n\nprotected:\n  /// Getter for generic TargetLowering class.\n  const TargetLowering *getTLI() const {\n    return TLI;\n  }\n\n  /// Getter for target specific TargetLowering class.\n  template <class XXXTargetLowering>\n    const XXXTargetLowering *getTLI() const {\n    return static_cast<const XXXTargetLowering *>(TLI);\n  }\n\n  /// \\returns Flags corresponding to the attributes on the \\p ArgIdx-th\n  /// parameter of \\p Call.\n  ISD::ArgFlagsTy getAttributesForArgIdx(const CallBase &Call,\n                                         unsigned ArgIdx) const;\n\n  /// Adds flags to \\p Flags based off of the attributes in \\p Attrs.\n  /// \\p OpIdx is the index in \\p Attrs to add flags from.\n  void addArgFlagsFromAttributes(ISD::ArgFlagsTy &Flags,\n                                 const AttributeList &Attrs,\n                                 unsigned OpIdx) const;\n\n  template <typename FuncInfoTy>\n  void setArgFlags(ArgInfo &Arg, unsigned OpIdx, const DataLayout &DL,\n                   const FuncInfoTy &FuncInfo) const;\n\n  /// Break \\p OrigArgInfo into one or more pieces the calling convention can\n  /// process, returned in \\p SplitArgs. For example, this should break structs\n  /// down into individual fields.\n  void splitToValueTypes(const ArgInfo &OrigArgInfo,\n                         SmallVectorImpl<ArgInfo> &SplitArgs,\n                         const DataLayout &DL, CallingConv::ID CallConv) const;\n\n  /// Generate instructions for unpacking \\p SrcReg into the \\p DstRegs\n  /// corresponding to the aggregate type \\p PackedTy.\n  ///\n  /// \\param DstRegs should contain one virtual register for each base type in\n  ///        \\p PackedTy, as returned by computeValueLLTs.\n  void unpackRegs(ArrayRef<Register> DstRegs, Register SrcReg, Type *PackedTy,\n                  MachineIRBuilder &MIRBuilder) const;\n\n  /// Invoke Handler::assignArg on each of the given \\p Args and then use\n  /// \\p Handler to move them to the assigned locations.\n  ///\n  /// \\return True if everything has succeeded, false otherwise.\n  bool handleAssignments(MachineIRBuilder &MIRBuilder,\n                         SmallVectorImpl<ArgInfo> &Args, ValueHandler &Handler,\n                         CallingConv::ID CallConv, bool IsVarArg,\n                         Register ThisReturnReg = Register()) const;\n  bool handleAssignments(CCState &CCState,\n                         SmallVectorImpl<CCValAssign> &ArgLocs,\n                         MachineIRBuilder &MIRBuilder,\n                         SmallVectorImpl<ArgInfo> &Args, ValueHandler &Handler,\n                         Register ThisReturnReg = Register()) const;\n\n  /// Analyze passed or returned values from a call, supplied in \\p ArgInfo,\n  /// incorporating info about the passed values into \\p CCState.\n  ///\n  /// Used to check if arguments are suitable for tail call lowering.\n  bool analyzeArgInfo(CCState &CCState, SmallVectorImpl<ArgInfo> &Args,\n                      CCAssignFn &AssignFnFixed,\n                      CCAssignFn &AssignFnVarArg) const;\n\n  /// Check whether parameters to a call that are passed in callee saved\n  /// registers are the same as from the calling function.  This needs to be\n  /// checked for tail call eligibility.\n  bool parametersInCSRMatch(const MachineRegisterInfo &MRI,\n                            const uint32_t *CallerPreservedMask,\n                            const SmallVectorImpl<CCValAssign> &ArgLocs,\n                            const SmallVectorImpl<ArgInfo> &OutVals) const;\n\n  /// \\returns True if the calling convention for a callee and its caller pass\n  /// results in the same way. Typically used for tail call eligibility checks.\n  ///\n  /// \\p Info is the CallLoweringInfo for the call.\n  /// \\p MF is the MachineFunction for the caller.\n  /// \\p InArgs contains the results of the call.\n  /// \\p CalleeAssignFnFixed is the CCAssignFn to be used for the callee for\n  /// fixed arguments.\n  /// \\p CalleeAssignFnVarArg is similar, but for varargs.\n  /// \\p CallerAssignFnFixed is the CCAssignFn to be used for the caller for\n  /// fixed arguments.\n  /// \\p CallerAssignFnVarArg is similar, but for varargs.\n  bool resultsCompatible(CallLoweringInfo &Info, MachineFunction &MF,\n                         SmallVectorImpl<ArgInfo> &InArgs,\n                         CCAssignFn &CalleeAssignFnFixed,\n                         CCAssignFn &CalleeAssignFnVarArg,\n                         CCAssignFn &CallerAssignFnFixed,\n                         CCAssignFn &CallerAssignFnVarArg) const;\n\npublic:\n  CallLowering(const TargetLowering *TLI) : TLI(TLI) {}\n  virtual ~CallLowering() = default;\n\n  /// \\return true if the target is capable of handling swifterror values that\n  /// have been promoted to a specified register. The extended versions of\n  /// lowerReturn and lowerCall should be implemented.\n  virtual bool supportSwiftError() const {\n    return false;\n  }\n\n  /// Load the returned value from the stack into virtual registers in \\p VRegs.\n  /// It uses the frame index \\p FI and the start offset from \\p DemoteReg.\n  /// The loaded data size will be determined from \\p RetTy.\n  void insertSRetLoads(MachineIRBuilder &MIRBuilder, Type *RetTy,\n                       ArrayRef<Register> VRegs, Register DemoteReg,\n                       int FI) const;\n\n  /// Store the return value given by \\p VRegs into stack starting at the offset\n  /// specified in \\p DemoteReg.\n  void insertSRetStores(MachineIRBuilder &MIRBuilder, Type *RetTy,\n                        ArrayRef<Register> VRegs, Register DemoteReg) const;\n\n  /// Insert the hidden sret ArgInfo to the beginning of \\p SplitArgs.\n  /// This function should be called from the target specific\n  /// lowerFormalArguments when \\p F requires the sret demotion.\n  void insertSRetIncomingArgument(const Function &F,\n                                  SmallVectorImpl<ArgInfo> &SplitArgs,\n                                  Register &DemoteReg, MachineRegisterInfo &MRI,\n                                  const DataLayout &DL) const;\n\n  /// For the call-base described by \\p CB, insert the hidden sret ArgInfo to\n  /// the OrigArgs field of \\p Info.\n  void insertSRetOutgoingArgument(MachineIRBuilder &MIRBuilder,\n                                  const CallBase &CB,\n                                  CallLoweringInfo &Info) const;\n\n  /// \\return True if the return type described by \\p Outs can be returned\n  /// without performing sret demotion.\n  bool checkReturn(CCState &CCInfo, SmallVectorImpl<BaseArgInfo> &Outs,\n                   CCAssignFn *Fn) const;\n\n  /// Get the type and the ArgFlags for the split components of \\p RetTy as\n  /// returned by \\c ComputeValueVTs.\n  void getReturnInfo(CallingConv::ID CallConv, Type *RetTy, AttributeList Attrs,\n                     SmallVectorImpl<BaseArgInfo> &Outs,\n                     const DataLayout &DL) const;\n\n  /// Toplevel function to check the return type based on the target calling\n  /// convention. \\return True if the return value of \\p MF can be returned\n  /// without performing sret demotion.\n  bool checkReturnTypeForCallConv(MachineFunction &MF) const;\n\n  /// This hook must be implemented to check whether the return values\n  /// described by \\p Outs can fit into the return registers. If false\n  /// is returned, an sret-demotion is performed.\n  virtual bool canLowerReturn(MachineFunction &MF, CallingConv::ID CallConv,\n                              SmallVectorImpl<BaseArgInfo> &Outs,\n                              bool IsVarArg) const {\n    return true;\n  }\n\n  /// This hook must be implemented to lower outgoing return values, described\n  /// by \\p Val, into the specified virtual registers \\p VRegs.\n  /// This hook is used by GlobalISel.\n  ///\n  /// \\p FLI is required for sret demotion.\n  ///\n  /// \\p SwiftErrorVReg is non-zero if the function has a swifterror parameter\n  /// that needs to be implicitly returned.\n  ///\n  /// \\return True if the lowering succeeds, false otherwise.\n  virtual bool lowerReturn(MachineIRBuilder &MIRBuilder, const Value *Val,\n                           ArrayRef<Register> VRegs, FunctionLoweringInfo &FLI,\n                           Register SwiftErrorVReg) const {\n    if (!supportSwiftError()) {\n      assert(SwiftErrorVReg == 0 && \"attempt to use unsupported swifterror\");\n      return lowerReturn(MIRBuilder, Val, VRegs, FLI);\n    }\n    return false;\n  }\n\n  /// This hook behaves as the extended lowerReturn function, but for targets\n  /// that do not support swifterror value promotion.\n  virtual bool lowerReturn(MachineIRBuilder &MIRBuilder, const Value *Val,\n                           ArrayRef<Register> VRegs,\n                           FunctionLoweringInfo &FLI) const {\n    return false;\n  }\n\n  virtual bool fallBackToDAGISel(const Function &F) const { return false; }\n\n  /// This hook must be implemented to lower the incoming (formal)\n  /// arguments, described by \\p VRegs, for GlobalISel. Each argument\n  /// must end up in the related virtual registers described by \\p VRegs.\n  /// In other words, the first argument should end up in \\c VRegs[0],\n  /// the second in \\c VRegs[1], and so on. For each argument, there will be one\n  /// register for each non-aggregate type, as returned by \\c computeValueLLTs.\n  /// \\p MIRBuilder is set to the proper insertion for the argument\n  /// lowering. \\p FLI is required for sret demotion.\n  ///\n  /// \\return True if the lowering succeeded, false otherwise.\n  virtual bool lowerFormalArguments(MachineIRBuilder &MIRBuilder,\n                                    const Function &F,\n                                    ArrayRef<ArrayRef<Register>> VRegs,\n                                    FunctionLoweringInfo &FLI) const {\n    return false;\n  }\n\n  /// This hook must be implemented to lower the given call instruction,\n  /// including argument and return value marshalling.\n  ///\n  ///\n  /// \\return true if the lowering succeeded, false otherwise.\n  virtual bool lowerCall(MachineIRBuilder &MIRBuilder,\n                         CallLoweringInfo &Info) const {\n    return false;\n  }\n\n  /// Lower the given call instruction, including argument and return value\n  /// marshalling.\n  ///\n  /// \\p CI is the call/invoke instruction.\n  ///\n  /// \\p ResRegs are the registers where the call's return value should be\n  /// stored (or 0 if there is no return value). There will be one register for\n  /// each non-aggregate type, as returned by \\c computeValueLLTs.\n  ///\n  /// \\p ArgRegs is a list of lists of virtual registers containing each\n  /// argument that needs to be passed (argument \\c i should be placed in \\c\n  /// ArgRegs[i]). For each argument, there will be one register for each\n  /// non-aggregate type, as returned by \\c computeValueLLTs.\n  ///\n  /// \\p SwiftErrorVReg is non-zero if the call has a swifterror inout\n  /// parameter, and contains the vreg that the swifterror should be copied into\n  /// after the call.\n  ///\n  /// \\p GetCalleeReg is a callback to materialize a register for the callee if\n  /// the target determines it cannot jump to the destination based purely on \\p\n  /// CI. This might be because \\p CI is indirect, or because of the limited\n  /// range of an immediate jump.\n  ///\n  /// \\return true if the lowering succeeded, false otherwise.\n  bool lowerCall(MachineIRBuilder &MIRBuilder, const CallBase &Call,\n                 ArrayRef<Register> ResRegs,\n                 ArrayRef<ArrayRef<Register>> ArgRegs, Register SwiftErrorVReg,\n                 std::function<unsigned()> GetCalleeReg) const;\n\n  /// For targets which support the \"returned\" parameter attribute, returns\n  /// true if the given type is a valid one to use with \"returned\".\n  virtual bool isTypeIsValidForThisReturn(EVT Ty) const { return false; }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_GLOBALISEL_CALLLOWERING_H\n"}, "40": {"id": 40, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/GISelWorkList.h", "content": "//===- GISelWorkList.h - Worklist for GISel passes ----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_GISELWORKLIST_H\n#define LLVM_CODEGEN_GLOBALISEL_GISELWORKLIST_H\n\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/SmallVector.h\"\n\nnamespace llvm {\n\nclass MachineInstr;\nclass MachineFunction;\n\n// Worklist which mostly works similar to InstCombineWorkList, but on\n// MachineInstrs. The main difference with something like a SetVector is that\n// erasing an element doesn't move all elements over one place - instead just\n// nulls out the element of the vector.\n//\n// FIXME: Does it make sense to factor out common code with the\n// instcombinerWorkList?\ntemplate<unsigned N>\nclass GISelWorkList {\n  SmallVector<MachineInstr *, N> Worklist;\n  DenseMap<MachineInstr *, unsigned> WorklistMap;\n\n#ifndef NDEBUG\n  bool Finalized = true;\n#endif\n\npublic:\n  GISelWorkList() : WorklistMap(N) {}\n\n  bool empty() const { return WorklistMap.empty(); }\n\n  unsigned size() const { return WorklistMap.size(); }\n\n  // Since we don't know ahead of time how many instructions we're going to add\n  // to the worklist, and migrating densemap's elements is quite expensive\n  // everytime we resize, only insert to the smallvector (typically during the\n  // initial phase of populating lists). Before the worklist can be used,\n  // finalize should be called. Also assert with NDEBUG if list is ever used\n  // without finalizing. Note that unlike insert, we won't check for duplicates\n  // - so the ideal place to use this is during the initial prepopulating phase\n  // of most passes.\n  void deferred_insert(MachineInstr *I) {\n    Worklist.push_back(I);\n#ifndef NDEBUG\n    Finalized = false;\n#endif\n  }\n\n  // This should only be called when using deferred_insert.\n  // This asserts that the WorklistMap is empty, and then\n  // inserts all the elements in the Worklist into the map.\n  // It also asserts if there are any duplicate elements found.\n  void finalize() {\n    assert(WorklistMap.empty() && \"Expecting empty worklistmap\");\n    if (Worklist.size() > N)\n      WorklistMap.reserve(Worklist.size());\n    for (unsigned i = 0; i < Worklist.size(); ++i)\n      if (!WorklistMap.try_emplace(Worklist[i], i).second)\n        llvm_unreachable(\"Duplicate elements in the list\");\n#ifndef NDEBUG\n    Finalized = true;\n#endif\n  }\n\n  /// Add the specified instruction to the worklist if it isn't already in it.\n  void insert(MachineInstr *I) {\n    assert(Finalized && \"GISelWorkList used without finalizing\");\n    if (WorklistMap.try_emplace(I, Worklist.size()).second)\n      Worklist.push_back(I);\n  }\n\n  /// Remove I from the worklist if it exists.\n  void remove(const MachineInstr *I) {\n    assert((Finalized || WorklistMap.empty()) && \"Neither finalized nor empty\");\n    auto It = WorklistMap.find(I);\n    if (It == WorklistMap.end())\n      return; // Not in worklist.\n\n    // Don't bother moving everything down, just null out the slot.\n    Worklist[It->second] = nullptr;\n\n    WorklistMap.erase(It);\n  }\n\n  void clear() {\n    Worklist.clear();\n    WorklistMap.clear();\n  }\n\n  MachineInstr *pop_back_val() {\n    assert(Finalized && \"GISelWorkList used without finalizing\");\n    MachineInstr *I;\n    do {\n      I = Worklist.pop_back_val();\n    } while(!I);\n    assert(I && \"Pop back on empty worklist\");\n    WorklistMap.erase(I);\n    return I;\n  }\n};\n\n} // end namespace llvm.\n\n#endif\n"}, "41": {"id": 41, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "content": "//===- llvm/CodeGen/GlobalISel/LegalizerInfo.h ------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// Interface for Targets to specify which operations they can successfully\n/// select and how the others should be expanded most efficiently.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_LEGALIZERINFO_H\n#define LLVM_CODEGEN_GLOBALISEL_LEGALIZERINFO_H\n\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallBitVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/TargetOpcodes.h\"\n#include \"llvm/Support/CommandLine.h\"\n#include \"llvm/Support/LowLevelTypeImpl.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include <cassert>\n#include <cstdint>\n#include <tuple>\n#include <unordered_map>\n#include <utility>\n\nnamespace llvm {\n\nextern cl::opt<bool> DisableGISelLegalityCheck;\n\nclass LegalizerHelper;\nclass MachineInstr;\nclass MachineRegisterInfo;\nclass MCInstrInfo;\nclass GISelChangeObserver;\n\nnamespace LegalizeActions {\nenum LegalizeAction : std::uint8_t {\n  /// The operation is expected to be selectable directly by the target, and\n  /// no transformation is necessary.\n  Legal,\n\n  /// The operation should be synthesized from multiple instructions acting on\n  /// a narrower scalar base-type. For example a 64-bit add might be\n  /// implemented in terms of 32-bit add-with-carry.\n  NarrowScalar,\n\n  /// The operation should be implemented in terms of a wider scalar\n  /// base-type. For example a <2 x s8> add could be implemented as a <2\n  /// x s32> add (ignoring the high bits).\n  WidenScalar,\n\n  /// The (vector) operation should be implemented by splitting it into\n  /// sub-vectors where the operation is legal. For example a <8 x s64> add\n  /// might be implemented as 4 separate <2 x s64> adds.\n  FewerElements,\n\n  /// The (vector) operation should be implemented by widening the input\n  /// vector and ignoring the lanes added by doing so. For example <2 x i8> is\n  /// rarely legal, but you might perform an <8 x i8> and then only look at\n  /// the first two results.\n  MoreElements,\n\n  /// Perform the operation on a different, but equivalently sized type.\n  Bitcast,\n\n  /// The operation itself must be expressed in terms of simpler actions on\n  /// this target. E.g. a SREM replaced by an SDIV and subtraction.\n  Lower,\n\n  /// The operation should be implemented as a call to some kind of runtime\n  /// support library. For example this usually happens on machines that don't\n  /// support floating-point operations natively.\n  Libcall,\n\n  /// The target wants to do something special with this combination of\n  /// operand and type. A callback will be issued when it is needed.\n  Custom,\n\n  /// This operation is completely unsupported on the target. A programming\n  /// error has occurred.\n  Unsupported,\n\n  /// Sentinel value for when no action was found in the specified table.\n  NotFound,\n\n  /// Fall back onto the old rules.\n  /// TODO: Remove this once we've migrated\n  UseLegacyRules,\n};\n} // end namespace LegalizeActions\nraw_ostream &operator<<(raw_ostream &OS, LegalizeActions::LegalizeAction Action);\n\nusing LegalizeActions::LegalizeAction;\n\n/// Legalization is decided based on an instruction's opcode, which type slot\n/// we're considering, and what the existing type is. These aspects are gathered\n/// together for convenience in the InstrAspect class.\nstruct InstrAspect {\n  unsigned Opcode;\n  unsigned Idx = 0;\n  LLT Type;\n\n  InstrAspect(unsigned Opcode, LLT Type) : Opcode(Opcode), Type(Type) {}\n  InstrAspect(unsigned Opcode, unsigned Idx, LLT Type)\n      : Opcode(Opcode), Idx(Idx), Type(Type) {}\n\n  bool operator==(const InstrAspect &RHS) const {\n    return Opcode == RHS.Opcode && Idx == RHS.Idx && Type == RHS.Type;\n  }\n};\n\n/// The LegalityQuery object bundles together all the information that's needed\n/// to decide whether a given operation is legal or not.\n/// For efficiency, it doesn't make a copy of Types so care must be taken not\n/// to free it before using the query.\nstruct LegalityQuery {\n  unsigned Opcode;\n  ArrayRef<LLT> Types;\n\n  struct MemDesc {\n    uint64_t SizeInBits;\n    uint64_t AlignInBits;\n    AtomicOrdering Ordering;\n  };\n\n  /// Operations which require memory can use this to place requirements on the\n  /// memory type for each MMO.\n  ArrayRef<MemDesc> MMODescrs;\n\n  constexpr LegalityQuery(unsigned Opcode, const ArrayRef<LLT> Types,\n                          const ArrayRef<MemDesc> MMODescrs)\n      : Opcode(Opcode), Types(Types), MMODescrs(MMODescrs) {}\n  constexpr LegalityQuery(unsigned Opcode, const ArrayRef<LLT> Types)\n      : LegalityQuery(Opcode, Types, {}) {}\n\n  raw_ostream &print(raw_ostream &OS) const;\n};\n\n/// The result of a query. It either indicates a final answer of Legal or\n/// Unsupported or describes an action that must be taken to make an operation\n/// more legal.\nstruct LegalizeActionStep {\n  /// The action to take or the final answer.\n  LegalizeAction Action;\n  /// If describing an action, the type index to change. Otherwise zero.\n  unsigned TypeIdx;\n  /// If describing an action, the new type for TypeIdx. Otherwise LLT{}.\n  LLT NewType;\n\n  LegalizeActionStep(LegalizeAction Action, unsigned TypeIdx,\n                     const LLT NewType)\n      : Action(Action), TypeIdx(TypeIdx), NewType(NewType) {}\n\n  bool operator==(const LegalizeActionStep &RHS) const {\n    return std::tie(Action, TypeIdx, NewType) ==\n        std::tie(RHS.Action, RHS.TypeIdx, RHS.NewType);\n  }\n};\n\nusing LegalityPredicate = std::function<bool (const LegalityQuery &)>;\nusing LegalizeMutation =\n    std::function<std::pair<unsigned, LLT>(const LegalityQuery &)>;\n\nnamespace LegalityPredicates {\nstruct TypePairAndMemDesc {\n  LLT Type0;\n  LLT Type1;\n  uint64_t MemSize;\n  uint64_t Align;\n\n  bool operator==(const TypePairAndMemDesc &Other) const {\n    return Type0 == Other.Type0 && Type1 == Other.Type1 &&\n           Align == Other.Align &&\n           MemSize == Other.MemSize;\n  }\n\n  /// \\returns true if this memory access is legal with for the access described\n  /// by \\p Other (The alignment is sufficient for the size and result type).\n  bool isCompatible(const TypePairAndMemDesc &Other) const {\n    return Type0 == Other.Type0 && Type1 == Other.Type1 &&\n           Align >= Other.Align &&\n           MemSize == Other.MemSize;\n  }\n};\n\n/// True iff P0 and P1 are true.\ntemplate<typename Predicate>\nPredicate all(Predicate P0, Predicate P1) {\n  return [=](const LegalityQuery &Query) {\n    return P0(Query) && P1(Query);\n  };\n}\n/// True iff all given predicates are true.\ntemplate<typename Predicate, typename... Args>\nPredicate all(Predicate P0, Predicate P1, Args... args) {\n  return all(all(P0, P1), args...);\n}\n\n/// True iff P0 or P1 are true.\ntemplate<typename Predicate>\nPredicate any(Predicate P0, Predicate P1) {\n  return [=](const LegalityQuery &Query) {\n    return P0(Query) || P1(Query);\n  };\n}\n/// True iff any given predicates are true.\ntemplate<typename Predicate, typename... Args>\nPredicate any(Predicate P0, Predicate P1, Args... args) {\n  return any(any(P0, P1), args...);\n}\n\n/// True iff the given type index is the specified type.\nLegalityPredicate typeIs(unsigned TypeIdx, LLT TypesInit);\n/// True iff the given type index is one of the specified types.\nLegalityPredicate typeInSet(unsigned TypeIdx,\n                            std::initializer_list<LLT> TypesInit);\n\n/// True iff the given type index is not the specified type.\ninline LegalityPredicate typeIsNot(unsigned TypeIdx, LLT Type) {\n  return [=](const LegalityQuery &Query) {\n           return Query.Types[TypeIdx] != Type;\n         };\n}\n\n/// True iff the given types for the given pair of type indexes is one of the\n/// specified type pairs.\nLegalityPredicate\ntypePairInSet(unsigned TypeIdx0, unsigned TypeIdx1,\n              std::initializer_list<std::pair<LLT, LLT>> TypesInit);\n/// True iff the given types for the given pair of type indexes is one of the\n/// specified type pairs.\nLegalityPredicate typePairAndMemDescInSet(\n    unsigned TypeIdx0, unsigned TypeIdx1, unsigned MMOIdx,\n    std::initializer_list<TypePairAndMemDesc> TypesAndMemDescInit);\n/// True iff the specified type index is a scalar.\nLegalityPredicate isScalar(unsigned TypeIdx);\n/// True iff the specified type index is a vector.\nLegalityPredicate isVector(unsigned TypeIdx);\n/// True iff the specified type index is a pointer (with any address space).\nLegalityPredicate isPointer(unsigned TypeIdx);\n/// True iff the specified type index is a pointer with the specified address\n/// space.\nLegalityPredicate isPointer(unsigned TypeIdx, unsigned AddrSpace);\n\n/// True if the type index is a vector with element type \\p EltTy\nLegalityPredicate elementTypeIs(unsigned TypeIdx, LLT EltTy);\n\n/// True iff the specified type index is a scalar that's narrower than the given\n/// size.\nLegalityPredicate scalarNarrowerThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar that's wider than the given\n/// size.\nLegalityPredicate scalarWiderThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar or vector with an element type\n/// that's narrower than the given size.\nLegalityPredicate scalarOrEltNarrowerThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar or a vector with an element\n/// type that's wider than the given size.\nLegalityPredicate scalarOrEltWiderThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar whose size is not a power of\n/// 2.\nLegalityPredicate sizeNotPow2(unsigned TypeIdx);\n\n/// True iff the specified type index is a scalar or vector whose element size\n/// is not a power of 2.\nLegalityPredicate scalarOrEltSizeNotPow2(unsigned TypeIdx);\n\n/// True if the total bitwidth of the specified type index is \\p Size bits.\nLegalityPredicate sizeIs(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type indices are both the same bit size.\nLegalityPredicate sameSize(unsigned TypeIdx0, unsigned TypeIdx1);\n\n/// True iff the first type index has a larger total bit size than second type\n/// index.\nLegalityPredicate largerThan(unsigned TypeIdx0, unsigned TypeIdx1);\n\n/// True iff the first type index has a smaller total bit size than second type\n/// index.\nLegalityPredicate smallerThan(unsigned TypeIdx0, unsigned TypeIdx1);\n\n/// True iff the specified MMO index has a size that is not a power of 2\nLegalityPredicate memSizeInBytesNotPow2(unsigned MMOIdx);\n/// True iff the specified type index is a vector whose element count is not a\n/// power of 2.\nLegalityPredicate numElementsNotPow2(unsigned TypeIdx);\n/// True iff the specified MMO index has at an atomic ordering of at Ordering or\n/// stronger.\nLegalityPredicate atomicOrderingAtLeastOrStrongerThan(unsigned MMOIdx,\n                                                      AtomicOrdering Ordering);\n} // end namespace LegalityPredicates\n\nnamespace LegalizeMutations {\n/// Select this specific type for the given type index.\nLegalizeMutation changeTo(unsigned TypeIdx, LLT Ty);\n\n/// Keep the same type as the given type index.\nLegalizeMutation changeTo(unsigned TypeIdx, unsigned FromTypeIdx);\n\n/// Keep the same scalar or element type as the given type index.\nLegalizeMutation changeElementTo(unsigned TypeIdx, unsigned FromTypeIdx);\n\n/// Keep the same scalar or element type as the given type.\nLegalizeMutation changeElementTo(unsigned TypeIdx, LLT Ty);\n\n/// Change the scalar size or element size to have the same scalar size as type\n/// index \\p FromIndex. Unlike changeElementTo, this discards pointer types and\n/// only changes the size.\nLegalizeMutation changeElementSizeTo(unsigned TypeIdx, unsigned FromTypeIdx);\n\n/// Widen the scalar type or vector element type for the given type index to the\n/// next power of 2.\nLegalizeMutation widenScalarOrEltToNextPow2(unsigned TypeIdx, unsigned Min = 0);\n\n/// Add more elements to the type for the given type index to the next power of\n/// 2.\nLegalizeMutation moreElementsToNextPow2(unsigned TypeIdx, unsigned Min = 0);\n/// Break up the vector type for the given type index into the element type.\nLegalizeMutation scalarize(unsigned TypeIdx);\n} // end namespace LegalizeMutations\n\n/// A single rule in a legalizer info ruleset.\n/// The specified action is chosen when the predicate is true. Where appropriate\n/// for the action (e.g. for WidenScalar) the new type is selected using the\n/// given mutator.\nclass LegalizeRule {\n  LegalityPredicate Predicate;\n  LegalizeAction Action;\n  LegalizeMutation Mutation;\n\npublic:\n  LegalizeRule(LegalityPredicate Predicate, LegalizeAction Action,\n               LegalizeMutation Mutation = nullptr)\n      : Predicate(Predicate), Action(Action), Mutation(Mutation) {}\n\n  /// Test whether the LegalityQuery matches.\n  bool match(const LegalityQuery &Query) const {\n    return Predicate(Query);\n  }\n\n  LegalizeAction getAction() const { return Action; }\n\n  /// Determine the change to make.\n  std::pair<unsigned, LLT> determineMutation(const LegalityQuery &Query) const {\n    if (Mutation)\n      return Mutation(Query);\n    return std::make_pair(0, LLT{});\n  }\n};\n\nclass LegalizeRuleSet {\n  /// When non-zero, the opcode we are an alias of\n  unsigned AliasOf;\n  /// If true, there is another opcode that aliases this one\n  bool IsAliasedByAnother;\n  SmallVector<LegalizeRule, 2> Rules;\n\n#ifndef NDEBUG\n  /// If bit I is set, this rule set contains a rule that may handle (predicate\n  /// or perform an action upon (or both)) the type index I. The uncertainty\n  /// comes from free-form rules executing user-provided lambda functions. We\n  /// conservatively assume such rules do the right thing and cover all type\n  /// indices. The bitset is intentionally 1 bit wider than it absolutely needs\n  /// to be to distinguish such cases from the cases where all type indices are\n  /// individually handled.\n  SmallBitVector TypeIdxsCovered{MCOI::OPERAND_LAST_GENERIC -\n                                 MCOI::OPERAND_FIRST_GENERIC + 2};\n  SmallBitVector ImmIdxsCovered{MCOI::OPERAND_LAST_GENERIC_IMM -\n                                MCOI::OPERAND_FIRST_GENERIC_IMM + 2};\n#endif\n\n  unsigned typeIdx(unsigned TypeIdx) {\n    assert(TypeIdx <=\n               (MCOI::OPERAND_LAST_GENERIC - MCOI::OPERAND_FIRST_GENERIC) &&\n           \"Type Index is out of bounds\");\n#ifndef NDEBUG\n    TypeIdxsCovered.set(TypeIdx);\n#endif\n    return TypeIdx;\n  }\n\n  unsigned immIdx(unsigned ImmIdx) {\n    assert(ImmIdx <= (MCOI::OPERAND_LAST_GENERIC_IMM -\n                      MCOI::OPERAND_FIRST_GENERIC_IMM) &&\n           \"Imm Index is out of bounds\");\n#ifndef NDEBUG\n    ImmIdxsCovered.set(ImmIdx);\n#endif\n    return ImmIdx;\n  }\n\n  void markAllIdxsAsCovered() {\n#ifndef NDEBUG\n    TypeIdxsCovered.set();\n    ImmIdxsCovered.set();\n#endif\n  }\n\n  void add(const LegalizeRule &Rule) {\n    assert(AliasOf == 0 &&\n           \"RuleSet is aliased, change the representative opcode instead\");\n    Rules.push_back(Rule);\n  }\n\n  static bool always(const LegalityQuery &) { return true; }\n\n  /// Use the given action when the predicate is true.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionIf(LegalizeAction Action,\n                            LegalityPredicate Predicate) {\n    add({Predicate, Action});\n    return *this;\n  }\n  /// Use the given action when the predicate is true.\n  /// Action should be an action that requires mutation.\n  LegalizeRuleSet &actionIf(LegalizeAction Action, LegalityPredicate Predicate,\n                            LegalizeMutation Mutation) {\n    add({Predicate, Action, Mutation});\n    return *this;\n  }\n  /// Use the given action when type index 0 is any type in the given list.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<LLT> Types) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typeInSet(typeIdx(0), Types));\n  }\n  /// Use the given action when type index 0 is any type in the given list.\n  /// Action should be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<LLT> Types,\n                             LegalizeMutation Mutation) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typeInSet(typeIdx(0), Types), Mutation);\n  }\n  /// Use the given action when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<std::pair<LLT, LLT>> Types) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typePairInSet(typeIdx(0), typeIdx(1), Types));\n  }\n  /// Use the given action when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  /// Action should be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<std::pair<LLT, LLT>> Types,\n                             LegalizeMutation Mutation) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typePairInSet(typeIdx(0), typeIdx(1), Types),\n                    Mutation);\n  }\n  /// Use the given action when type index 0 is any type in the given list and\n  /// imm index 0 is anything. Action should not be an action that requires\n  /// mutation.\n  LegalizeRuleSet &actionForTypeWithAnyImm(LegalizeAction Action,\n                                           std::initializer_list<LLT> Types) {\n    using namespace LegalityPredicates;\n    immIdx(0); // Inform verifier imm idx 0 is handled.\n    return actionIf(Action, typeInSet(typeIdx(0), Types));\n  }\n\n  LegalizeRuleSet &actionForTypeWithAnyImm(\n    LegalizeAction Action, std::initializer_list<std::pair<LLT, LLT>> Types) {\n    using namespace LegalityPredicates;\n    immIdx(0); // Inform verifier imm idx 0 is handled.\n    return actionIf(Action, typePairInSet(typeIdx(0), typeIdx(1), Types));\n  }\n\n  /// Use the given action when type indexes 0 and 1 are both in the given list.\n  /// That is, the type pair is in the cartesian product of the list.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionForCartesianProduct(LegalizeAction Action,\n                                             std::initializer_list<LLT> Types) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, all(typeInSet(typeIdx(0), Types),\n                                typeInSet(typeIdx(1), Types)));\n  }\n  /// Use the given action when type indexes 0 and 1 are both in their\n  /// respective lists.\n  /// That is, the type pair is in the cartesian product of the lists\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &\n  actionForCartesianProduct(LegalizeAction Action,\n                            std::initializer_list<LLT> Types0,\n                            std::initializer_list<LLT> Types1) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, all(typeInSet(typeIdx(0), Types0),\n                                typeInSet(typeIdx(1), Types1)));\n  }\n  /// Use the given action when type indexes 0, 1, and 2 are all in their\n  /// respective lists.\n  /// That is, the type triple is in the cartesian product of the lists\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionForCartesianProduct(\n      LegalizeAction Action, std::initializer_list<LLT> Types0,\n      std::initializer_list<LLT> Types1, std::initializer_list<LLT> Types2) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, all(typeInSet(typeIdx(0), Types0),\n                                all(typeInSet(typeIdx(1), Types1),\n                                    typeInSet(typeIdx(2), Types2))));\n  }\n\npublic:\n  LegalizeRuleSet() : AliasOf(0), IsAliasedByAnother(false), Rules() {}\n\n  bool isAliasedByAnother() { return IsAliasedByAnother; }\n  void setIsAliasedByAnother() { IsAliasedByAnother = true; }\n  void aliasTo(unsigned Opcode) {\n    assert((AliasOf == 0 || AliasOf == Opcode) &&\n           \"Opcode is already aliased to another opcode\");\n    assert(Rules.empty() && \"Aliasing will discard rules\");\n    AliasOf = Opcode;\n  }\n  unsigned getAlias() const { return AliasOf; }\n\n  /// The instruction is legal if predicate is true.\n  LegalizeRuleSet &legalIf(LegalityPredicate Predicate) {\n    // We have no choice but conservatively assume that the free-form\n    // user-provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Legal, Predicate);\n  }\n  /// The instruction is legal when type index 0 is any type in the given list.\n  LegalizeRuleSet &legalFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Legal, Types);\n  }\n  /// The instruction is legal when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  LegalizeRuleSet &legalFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Legal, Types);\n  }\n  /// The instruction is legal when type index 0 is any type in the given list\n  /// and imm index 0 is anything.\n  LegalizeRuleSet &legalForTypeWithAnyImm(std::initializer_list<LLT> Types) {\n    markAllIdxsAsCovered();\n    return actionForTypeWithAnyImm(LegalizeAction::Legal, Types);\n  }\n\n  LegalizeRuleSet &legalForTypeWithAnyImm(\n    std::initializer_list<std::pair<LLT, LLT>> Types) {\n    markAllIdxsAsCovered();\n    return actionForTypeWithAnyImm(LegalizeAction::Legal, Types);\n  }\n\n  /// The instruction is legal when type indexes 0 and 1 along with the memory\n  /// size and minimum alignment is any type and size tuple in the given list.\n  LegalizeRuleSet &legalForTypesWithMemDesc(\n      std::initializer_list<LegalityPredicates::TypePairAndMemDesc>\n          TypesAndMemDesc) {\n    return actionIf(LegalizeAction::Legal,\n                    LegalityPredicates::typePairAndMemDescInSet(\n                        typeIdx(0), typeIdx(1), /*MMOIdx*/ 0, TypesAndMemDesc));\n  }\n  /// The instruction is legal when type indexes 0 and 1 are both in the given\n  /// list. That is, the type pair is in the cartesian product of the list.\n  LegalizeRuleSet &legalForCartesianProduct(std::initializer_list<LLT> Types) {\n    return actionForCartesianProduct(LegalizeAction::Legal, Types);\n  }\n  /// The instruction is legal when type indexes 0 and 1 are both their\n  /// respective lists.\n  LegalizeRuleSet &legalForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1) {\n    return actionForCartesianProduct(LegalizeAction::Legal, Types0, Types1);\n  }\n  /// The instruction is legal when type indexes 0, 1, and 2 are both their\n  /// respective lists.\n  LegalizeRuleSet &legalForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1,\n                                            std::initializer_list<LLT> Types2) {\n    return actionForCartesianProduct(LegalizeAction::Legal, Types0, Types1,\n                                     Types2);\n  }\n\n  LegalizeRuleSet &alwaysLegal() {\n    using namespace LegalizeMutations;\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Legal, always);\n  }\n\n  /// The specified type index is coerced if predicate is true.\n  LegalizeRuleSet &bitcastIf(LegalityPredicate Predicate,\n                             LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that lowering with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Bitcast, Predicate, Mutation);\n  }\n\n  /// The instruction is lowered.\n  LegalizeRuleSet &lower() {\n    using namespace LegalizeMutations;\n    // We have no choice but conservatively assume that predicate-less lowering\n    // properly handles all type indices by design:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Lower, always);\n  }\n  /// The instruction is lowered if predicate is true. Keep type index 0 as the\n  /// same type.\n  LegalizeRuleSet &lowerIf(LegalityPredicate Predicate) {\n    using namespace LegalizeMutations;\n    // We have no choice but conservatively assume that lowering with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Lower, Predicate);\n  }\n  /// The instruction is lowered if predicate is true.\n  LegalizeRuleSet &lowerIf(LegalityPredicate Predicate,\n                           LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that lowering with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Lower, Predicate, Mutation);\n  }\n  /// The instruction is lowered when type index 0 is any type in the given\n  /// list. Keep type index 0 as the same type.\n  LegalizeRuleSet &lowerFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Lower, Types);\n  }\n  /// The instruction is lowered when type index 0 is any type in the given\n  /// list.\n  LegalizeRuleSet &lowerFor(std::initializer_list<LLT> Types,\n                            LegalizeMutation Mutation) {\n    return actionFor(LegalizeAction::Lower, Types, Mutation);\n  }\n  /// The instruction is lowered when type indexes 0 and 1 is any type pair in\n  /// the given list. Keep type index 0 as the same type.\n  LegalizeRuleSet &lowerFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Lower, Types);\n  }\n  /// The instruction is lowered when type indexes 0 and 1 is any type pair in\n  /// the given list.\n  LegalizeRuleSet &lowerFor(std::initializer_list<std::pair<LLT, LLT>> Types,\n                            LegalizeMutation Mutation) {\n    return actionFor(LegalizeAction::Lower, Types, Mutation);\n  }\n  /// The instruction is lowered when type indexes 0 and 1 are both in their\n  /// respective lists.\n  LegalizeRuleSet &lowerForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1) {\n    using namespace LegalityPredicates;\n    return actionForCartesianProduct(LegalizeAction::Lower, Types0, Types1);\n  }\n  /// The instruction is lowered when when type indexes 0, 1, and 2 are all in\n  /// their respective lists.\n  LegalizeRuleSet &lowerForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1,\n                                            std::initializer_list<LLT> Types2) {\n    using namespace LegalityPredicates;\n    return actionForCartesianProduct(LegalizeAction::Lower, Types0, Types1,\n                                     Types2);\n  }\n\n  /// The instruction is emitted as a library call.\n  LegalizeRuleSet &libcall() {\n    using namespace LegalizeMutations;\n    // We have no choice but conservatively assume that predicate-less lowering\n    // properly handles all type indices by design:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Libcall, always);\n  }\n\n  /// Like legalIf, but for the Libcall action.\n  LegalizeRuleSet &libcallIf(LegalityPredicate Predicate) {\n    // We have no choice but conservatively assume that a libcall with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Libcall, Predicate);\n  }\n  LegalizeRuleSet &libcallFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Libcall, Types);\n  }\n  LegalizeRuleSet &\n  libcallFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Libcall, Types);\n  }\n  LegalizeRuleSet &\n  libcallForCartesianProduct(std::initializer_list<LLT> Types) {\n    return actionForCartesianProduct(LegalizeAction::Libcall, Types);\n  }\n  LegalizeRuleSet &\n  libcallForCartesianProduct(std::initializer_list<LLT> Types0,\n                             std::initializer_list<LLT> Types1) {\n    return actionForCartesianProduct(LegalizeAction::Libcall, Types0, Types1);\n  }\n\n  /// Widen the scalar to the one selected by the mutation if the predicate is\n  /// true.\n  LegalizeRuleSet &widenScalarIf(LegalityPredicate Predicate,\n                                 LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::WidenScalar, Predicate, Mutation);\n  }\n  /// Narrow the scalar to the one selected by the mutation if the predicate is\n  /// true.\n  LegalizeRuleSet &narrowScalarIf(LegalityPredicate Predicate,\n                                  LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::NarrowScalar, Predicate, Mutation);\n  }\n  /// Narrow the scalar, specified in mutation, when type indexes 0 and 1 is any\n  /// type pair in the given list.\n  LegalizeRuleSet &\n  narrowScalarFor(std::initializer_list<std::pair<LLT, LLT>> Types,\n                  LegalizeMutation Mutation) {\n    return actionFor(LegalizeAction::NarrowScalar, Types, Mutation);\n  }\n\n  /// Add more elements to reach the type selected by the mutation if the\n  /// predicate is true.\n  LegalizeRuleSet &moreElementsIf(LegalityPredicate Predicate,\n                                  LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::MoreElements, Predicate, Mutation);\n  }\n  /// Remove elements to reach the type selected by the mutation if the\n  /// predicate is true.\n  LegalizeRuleSet &fewerElementsIf(LegalityPredicate Predicate,\n                                   LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::FewerElements, Predicate, Mutation);\n  }\n\n  /// The instruction is unsupported.\n  LegalizeRuleSet &unsupported() {\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Unsupported, always);\n  }\n  LegalizeRuleSet &unsupportedIf(LegalityPredicate Predicate) {\n    return actionIf(LegalizeAction::Unsupported, Predicate);\n  }\n\n  LegalizeRuleSet &unsupportedFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Unsupported, Types);\n  }\n\n  LegalizeRuleSet &unsupportedIfMemSizeNotPow2() {\n    return actionIf(LegalizeAction::Unsupported,\n                    LegalityPredicates::memSizeInBytesNotPow2(0));\n  }\n  LegalizeRuleSet &lowerIfMemSizeNotPow2() {\n    return actionIf(LegalizeAction::Lower,\n                    LegalityPredicates::memSizeInBytesNotPow2(0));\n  }\n\n  LegalizeRuleSet &customIf(LegalityPredicate Predicate) {\n    // We have no choice but conservatively assume that a custom action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Custom, Predicate);\n  }\n  LegalizeRuleSet &customFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Custom, Types);\n  }\n\n  /// The instruction is custom when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  LegalizeRuleSet &customFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Custom, Types);\n  }\n\n  LegalizeRuleSet &customForCartesianProduct(std::initializer_list<LLT> Types) {\n    return actionForCartesianProduct(LegalizeAction::Custom, Types);\n  }\n  LegalizeRuleSet &\n  customForCartesianProduct(std::initializer_list<LLT> Types0,\n                            std::initializer_list<LLT> Types1) {\n    return actionForCartesianProduct(LegalizeAction::Custom, Types0, Types1);\n  }\n\n  /// Unconditionally custom lower.\n  LegalizeRuleSet &custom() {\n    return customIf(always);\n  }\n\n  /// Widen the scalar to the next power of two that is at least MinSize.\n  /// No effect if the type is not a scalar or is a power of two.\n  LegalizeRuleSet &widenScalarToNextPow2(unsigned TypeIdx,\n                                         unsigned MinSize = 0) {\n    using namespace LegalityPredicates;\n    return actionIf(\n        LegalizeAction::WidenScalar, sizeNotPow2(typeIdx(TypeIdx)),\n        LegalizeMutations::widenScalarOrEltToNextPow2(TypeIdx, MinSize));\n  }\n\n  /// Widen the scalar or vector element type to the next power of two that is\n  /// at least MinSize.  No effect if the scalar size is a power of two.\n  LegalizeRuleSet &widenScalarOrEltToNextPow2(unsigned TypeIdx,\n                                              unsigned MinSize = 0) {\n    using namespace LegalityPredicates;\n    return actionIf(\n        LegalizeAction::WidenScalar, scalarOrEltSizeNotPow2(typeIdx(TypeIdx)),\n        LegalizeMutations::widenScalarOrEltToNextPow2(TypeIdx, MinSize));\n  }\n\n  LegalizeRuleSet &narrowScalar(unsigned TypeIdx, LegalizeMutation Mutation) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::NarrowScalar, isScalar(typeIdx(TypeIdx)),\n                    Mutation);\n  }\n\n  LegalizeRuleSet &scalarize(unsigned TypeIdx) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::FewerElements, isVector(typeIdx(TypeIdx)),\n                    LegalizeMutations::scalarize(TypeIdx));\n  }\n\n  LegalizeRuleSet &scalarizeIf(LegalityPredicate Predicate, unsigned TypeIdx) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::FewerElements,\n                    all(Predicate, isVector(typeIdx(TypeIdx))),\n                    LegalizeMutations::scalarize(TypeIdx));\n  }\n\n  /// Ensure the scalar or element is at least as wide as Ty.\n  LegalizeRuleSet &minScalarOrElt(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::WidenScalar,\n                    scalarOrEltNarrowerThan(TypeIdx, Ty.getScalarSizeInBits()),\n                    changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar or element is at least as wide as Ty.\n  LegalizeRuleSet &minScalarOrEltIf(LegalityPredicate Predicate,\n                                    unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::WidenScalar,\n                    all(Predicate, scalarOrEltNarrowerThan(\n                                       TypeIdx, Ty.getScalarSizeInBits())),\n                    changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar is at least as wide as Ty.\n  LegalizeRuleSet &minScalar(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::WidenScalar,\n                    scalarNarrowerThan(TypeIdx, Ty.getSizeInBits()),\n                    changeTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar is at most as wide as Ty.\n  LegalizeRuleSet &maxScalarOrElt(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::NarrowScalar,\n                    scalarOrEltWiderThan(TypeIdx, Ty.getScalarSizeInBits()),\n                    changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar is at most as wide as Ty.\n  LegalizeRuleSet &maxScalar(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::NarrowScalar,\n                    scalarWiderThan(TypeIdx, Ty.getSizeInBits()),\n                    changeTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Conditionally limit the maximum size of the scalar.\n  /// For example, when the maximum size of one type depends on the size of\n  /// another such as extracting N bits from an M bit container.\n  LegalizeRuleSet &maxScalarIf(LegalityPredicate Predicate, unsigned TypeIdx,\n                               const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(\n        LegalizeAction::NarrowScalar,\n        [=](const LegalityQuery &Query) {\n          const LLT QueryTy = Query.Types[TypeIdx];\n          return QueryTy.isScalar() &&\n                 QueryTy.getSizeInBits() > Ty.getSizeInBits() &&\n                 Predicate(Query);\n        },\n        changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Limit the range of scalar sizes to MinTy and MaxTy.\n  LegalizeRuleSet &clampScalar(unsigned TypeIdx, const LLT MinTy,\n                               const LLT MaxTy) {\n    assert(MinTy.isScalar() && MaxTy.isScalar() && \"Expected scalar types\");\n    return minScalar(TypeIdx, MinTy).maxScalar(TypeIdx, MaxTy);\n  }\n\n  /// Limit the range of scalar sizes to MinTy and MaxTy.\n  LegalizeRuleSet &clampScalarOrElt(unsigned TypeIdx, const LLT MinTy,\n                                    const LLT MaxTy) {\n    return minScalarOrElt(TypeIdx, MinTy).maxScalarOrElt(TypeIdx, MaxTy);\n  }\n\n  /// Widen the scalar to match the size of another.\n  LegalizeRuleSet &minScalarSameAs(unsigned TypeIdx, unsigned LargeTypeIdx) {\n    typeIdx(TypeIdx);\n    return widenScalarIf(\n        [=](const LegalityQuery &Query) {\n          return Query.Types[LargeTypeIdx].getScalarSizeInBits() >\n                 Query.Types[TypeIdx].getSizeInBits();\n        },\n        LegalizeMutations::changeElementSizeTo(TypeIdx, LargeTypeIdx));\n  }\n\n  /// Narrow the scalar to match the size of another.\n  LegalizeRuleSet &maxScalarSameAs(unsigned TypeIdx, unsigned NarrowTypeIdx) {\n    typeIdx(TypeIdx);\n    return narrowScalarIf(\n        [=](const LegalityQuery &Query) {\n          return Query.Types[NarrowTypeIdx].getScalarSizeInBits() <\n                 Query.Types[TypeIdx].getSizeInBits();\n        },\n        LegalizeMutations::changeElementSizeTo(TypeIdx, NarrowTypeIdx));\n  }\n\n  /// Change the type \\p TypeIdx to have the same scalar size as type \\p\n  /// SameSizeIdx.\n  LegalizeRuleSet &scalarSameSizeAs(unsigned TypeIdx, unsigned SameSizeIdx) {\n    return minScalarSameAs(TypeIdx, SameSizeIdx)\n          .maxScalarSameAs(TypeIdx, SameSizeIdx);\n  }\n\n  /// Conditionally widen the scalar or elt to match the size of another.\n  LegalizeRuleSet &minScalarEltSameAsIf(LegalityPredicate Predicate,\n                                   unsigned TypeIdx, unsigned LargeTypeIdx) {\n    typeIdx(TypeIdx);\n    return widenScalarIf(\n        [=](const LegalityQuery &Query) {\n          return Query.Types[LargeTypeIdx].getScalarSizeInBits() >\n                     Query.Types[TypeIdx].getScalarSizeInBits() &&\n                 Predicate(Query);\n        },\n        [=](const LegalityQuery &Query) {\n          LLT T = Query.Types[LargeTypeIdx];\n          return std::make_pair(TypeIdx, T);\n        });\n  }\n\n  /// Add more elements to the vector to reach the next power of two.\n  /// No effect if the type is not a vector or the element count is a power of\n  /// two.\n  LegalizeRuleSet &moreElementsToNextPow2(unsigned TypeIdx) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::MoreElements,\n                    numElementsNotPow2(typeIdx(TypeIdx)),\n                    LegalizeMutations::moreElementsToNextPow2(TypeIdx));\n  }\n\n  /// Limit the number of elements in EltTy vectors to at least MinElements.\n  LegalizeRuleSet &clampMinNumElements(unsigned TypeIdx, const LLT EltTy,\n                                       unsigned MinElements) {\n    // Mark the type index as covered:\n    typeIdx(TypeIdx);\n    return actionIf(\n        LegalizeAction::MoreElements,\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          return VecTy.isVector() && VecTy.getElementType() == EltTy &&\n                 VecTy.getNumElements() < MinElements;\n        },\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          return std::make_pair(\n              TypeIdx, LLT::vector(MinElements, VecTy.getElementType()));\n        });\n  }\n  /// Limit the number of elements in EltTy vectors to at most MaxElements.\n  LegalizeRuleSet &clampMaxNumElements(unsigned TypeIdx, const LLT EltTy,\n                                       unsigned MaxElements) {\n    // Mark the type index as covered:\n    typeIdx(TypeIdx);\n    return actionIf(\n        LegalizeAction::FewerElements,\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          return VecTy.isVector() && VecTy.getElementType() == EltTy &&\n                 VecTy.getNumElements() > MaxElements;\n        },\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          LLT NewTy = LLT::scalarOrVector(MaxElements, VecTy.getElementType());\n          return std::make_pair(TypeIdx, NewTy);\n        });\n  }\n  /// Limit the number of elements for the given vectors to at least MinTy's\n  /// number of elements and at most MaxTy's number of elements.\n  ///\n  /// No effect if the type is not a vector or does not have the same element\n  /// type as the constraints.\n  /// The element type of MinTy and MaxTy must match.\n  LegalizeRuleSet &clampNumElements(unsigned TypeIdx, const LLT MinTy,\n                                    const LLT MaxTy) {\n    assert(MinTy.getElementType() == MaxTy.getElementType() &&\n           \"Expected element types to agree\");\n\n    const LLT EltTy = MinTy.getElementType();\n    return clampMinNumElements(TypeIdx, EltTy, MinTy.getNumElements())\n        .clampMaxNumElements(TypeIdx, EltTy, MaxTy.getNumElements());\n  }\n\n  /// Fallback on the previous implementation. This should only be used while\n  /// porting a rule.\n  LegalizeRuleSet &fallback() {\n    add({always, LegalizeAction::UseLegacyRules});\n    return *this;\n  }\n\n  /// Check if there is no type index which is obviously not handled by the\n  /// LegalizeRuleSet in any way at all.\n  /// \\pre Type indices of the opcode form a dense [0, \\p NumTypeIdxs) set.\n  bool verifyTypeIdxsCoverage(unsigned NumTypeIdxs) const;\n  /// Check if there is no imm index which is obviously not handled by the\n  /// LegalizeRuleSet in any way at all.\n  /// \\pre Type indices of the opcode form a dense [0, \\p NumTypeIdxs) set.\n  bool verifyImmIdxsCoverage(unsigned NumImmIdxs) const;\n\n  /// Apply the ruleset to the given LegalityQuery.\n  LegalizeActionStep apply(const LegalityQuery &Query) const;\n};\n\nclass LegalizerInfo {\npublic:\n  LegalizerInfo();\n  virtual ~LegalizerInfo() = default;\n\n  unsigned getOpcodeIdxForOpcode(unsigned Opcode) const;\n  unsigned getActionDefinitionsIdx(unsigned Opcode) const;\n\n  /// Compute any ancillary tables needed to quickly decide how an operation\n  /// should be handled. This must be called after all \"set*Action\"methods but\n  /// before any query is made or incorrect results may be returned.\n  void computeTables();\n\n  /// Perform simple self-diagnostic and assert if there is anything obviously\n  /// wrong with the actions set up.\n  void verify(const MCInstrInfo &MII) const;\n\n  static bool needsLegalizingToDifferentSize(const LegalizeAction Action) {\n    using namespace LegalizeActions;\n    switch (Action) {\n    case NarrowScalar:\n    case WidenScalar:\n    case FewerElements:\n    case MoreElements:\n    case Unsupported:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  using SizeAndAction = std::pair<uint16_t, LegalizeAction>;\n  using SizeAndActionsVec = std::vector<SizeAndAction>;\n  using SizeChangeStrategy =\n      std::function<SizeAndActionsVec(const SizeAndActionsVec &v)>;\n\n  /// More friendly way to set an action for common types that have an LLT\n  /// representation.\n  /// The LegalizeAction must be one for which NeedsLegalizingToDifferentSize\n  /// returns false.\n  void setAction(const InstrAspect &Aspect, LegalizeAction Action) {\n    assert(!needsLegalizingToDifferentSize(Action));\n    TablesInitialized = false;\n    const unsigned OpcodeIdx = Aspect.Opcode - FirstOp;\n    if (SpecifiedActions[OpcodeIdx].size() <= Aspect.Idx)\n      SpecifiedActions[OpcodeIdx].resize(Aspect.Idx + 1);\n    SpecifiedActions[OpcodeIdx][Aspect.Idx][Aspect.Type] = Action;\n  }\n\n  /// The setAction calls record the non-size-changing legalization actions\n  /// to take on specificly-sized types. The SizeChangeStrategy defines what\n  /// to do when the size of the type needs to be changed to reach a legally\n  /// sized type (i.e., one that was defined through a setAction call).\n  /// e.g.\n  /// setAction ({G_ADD, 0, LLT::scalar(32)}, Legal);\n  /// setLegalizeScalarToDifferentSizeStrategy(\n  ///   G_ADD, 0, widenToLargerTypesAndNarrowToLargest);\n  /// will end up defining getAction({G_ADD, 0, T}) to return the following\n  /// actions for different scalar types T:\n  ///  LLT::scalar(1)..LLT::scalar(31): {WidenScalar, 0, LLT::scalar(32)}\n  ///  LLT::scalar(32):                 {Legal, 0, LLT::scalar(32)}\n  ///  LLT::scalar(33)..:               {NarrowScalar, 0, LLT::scalar(32)}\n  ///\n  /// If no SizeChangeAction gets defined, through this function,\n  /// the default is unsupportedForDifferentSizes.\n  void setLegalizeScalarToDifferentSizeStrategy(const unsigned Opcode,\n                                                const unsigned TypeIdx,\n                                                SizeChangeStrategy S) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (ScalarSizeChangeStrategies[OpcodeIdx].size() <= TypeIdx)\n      ScalarSizeChangeStrategies[OpcodeIdx].resize(TypeIdx + 1);\n    ScalarSizeChangeStrategies[OpcodeIdx][TypeIdx] = S;\n  }\n\n  /// See also setLegalizeScalarToDifferentSizeStrategy.\n  /// This function allows to set the SizeChangeStrategy for vector elements.\n  void setLegalizeVectorElementToDifferentSizeStrategy(const unsigned Opcode,\n                                                       const unsigned TypeIdx,\n                                                       SizeChangeStrategy S) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (VectorElementSizeChangeStrategies[OpcodeIdx].size() <= TypeIdx)\n      VectorElementSizeChangeStrategies[OpcodeIdx].resize(TypeIdx + 1);\n    VectorElementSizeChangeStrategies[OpcodeIdx][TypeIdx] = S;\n  }\n\n  /// A SizeChangeStrategy for the common case where legalization for a\n  /// particular operation consists of only supporting a specific set of type\n  /// sizes. E.g.\n  ///   setAction ({G_DIV, 0, LLT::scalar(32)}, Legal);\n  ///   setAction ({G_DIV, 0, LLT::scalar(64)}, Legal);\n  ///   setLegalizeScalarToDifferentSizeStrategy(\n  ///     G_DIV, 0, unsupportedForDifferentSizes);\n  /// will result in getAction({G_DIV, 0, T}) to return Legal for s32 and s64,\n  /// and Unsupported for all other scalar types T.\n  static SizeAndActionsVec\n  unsupportedForDifferentSizes(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return increaseToLargerTypesAndDecreaseToLargest(v, Unsupported,\n                                                     Unsupported);\n  }\n\n  /// A SizeChangeStrategy for the common case where legalization for a\n  /// particular operation consists of widening the type to a large legal type,\n  /// unless there is no such type and then instead it should be narrowed to the\n  /// largest legal type.\n  static SizeAndActionsVec\n  widenToLargerTypesAndNarrowToLargest(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    assert(v.size() > 0 &&\n           \"At least one size that can be legalized towards is needed\"\n           \" for this SizeChangeStrategy\");\n    return increaseToLargerTypesAndDecreaseToLargest(v, WidenScalar,\n                                                     NarrowScalar);\n  }\n\n  static SizeAndActionsVec\n  widenToLargerTypesUnsupportedOtherwise(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return increaseToLargerTypesAndDecreaseToLargest(v, WidenScalar,\n                                                     Unsupported);\n  }\n\n  static SizeAndActionsVec\n  narrowToSmallerAndUnsupportedIfTooSmall(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return decreaseToSmallerTypesAndIncreaseToSmallest(v, NarrowScalar,\n                                                       Unsupported);\n  }\n\n  static SizeAndActionsVec\n  narrowToSmallerAndWidenToSmallest(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    assert(v.size() > 0 &&\n           \"At least one size that can be legalized towards is needed\"\n           \" for this SizeChangeStrategy\");\n    return decreaseToSmallerTypesAndIncreaseToSmallest(v, NarrowScalar,\n                                                       WidenScalar);\n  }\n\n  /// A SizeChangeStrategy for the common case where legalization for a\n  /// particular vector operation consists of having more elements in the\n  /// vector, to a type that is legal. Unless there is no such type and then\n  /// instead it should be legalized towards the widest vector that's still\n  /// legal. E.g.\n  ///   setAction({G_ADD, LLT::vector(8, 8)}, Legal);\n  ///   setAction({G_ADD, LLT::vector(16, 8)}, Legal);\n  ///   setAction({G_ADD, LLT::vector(2, 32)}, Legal);\n  ///   setAction({G_ADD, LLT::vector(4, 32)}, Legal);\n  ///   setLegalizeVectorElementToDifferentSizeStrategy(\n  ///     G_ADD, 0, moreToWiderTypesAndLessToWidest);\n  /// will result in the following getAction results:\n  ///   * getAction({G_ADD, LLT::vector(8,8)}) returns\n  ///       (Legal, vector(8,8)).\n  ///   * getAction({G_ADD, LLT::vector(9,8)}) returns\n  ///       (MoreElements, vector(16,8)).\n  ///   * getAction({G_ADD, LLT::vector(8,32)}) returns\n  ///       (FewerElements, vector(4,32)).\n  static SizeAndActionsVec\n  moreToWiderTypesAndLessToWidest(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return increaseToLargerTypesAndDecreaseToLargest(v, MoreElements,\n                                                     FewerElements);\n  }\n\n  /// Helper function to implement many typical SizeChangeStrategy functions.\n  static SizeAndActionsVec\n  increaseToLargerTypesAndDecreaseToLargest(const SizeAndActionsVec &v,\n                                            LegalizeAction IncreaseAction,\n                                            LegalizeAction DecreaseAction);\n  /// Helper function to implement many typical SizeChangeStrategy functions.\n  static SizeAndActionsVec\n  decreaseToSmallerTypesAndIncreaseToSmallest(const SizeAndActionsVec &v,\n                                              LegalizeAction DecreaseAction,\n                                              LegalizeAction IncreaseAction);\n\n  /// Get the action definitions for the given opcode. Use this to run a\n  /// LegalityQuery through the definitions.\n  const LegalizeRuleSet &getActionDefinitions(unsigned Opcode) const;\n\n  /// Get the action definition builder for the given opcode. Use this to define\n  /// the action definitions.\n  ///\n  /// It is an error to request an opcode that has already been requested by the\n  /// multiple-opcode variant.\n  LegalizeRuleSet &getActionDefinitionsBuilder(unsigned Opcode);\n\n  /// Get the action definition builder for the given set of opcodes. Use this\n  /// to define the action definitions for multiple opcodes at once. The first\n  /// opcode given will be considered the representative opcode and will hold\n  /// the definitions whereas the other opcodes will be configured to refer to\n  /// the representative opcode. This lowers memory requirements and very\n  /// slightly improves performance.\n  ///\n  /// It would be very easy to introduce unexpected side-effects as a result of\n  /// this aliasing if it were permitted to request different but intersecting\n  /// sets of opcodes but that is difficult to keep track of. It is therefore an\n  /// error to request the same opcode twice using this API, to request an\n  /// opcode that already has definitions, or to use the single-opcode API on an\n  /// opcode that has already been requested by this API.\n  LegalizeRuleSet &\n  getActionDefinitionsBuilder(std::initializer_list<unsigned> Opcodes);\n  void aliasActionDefinitions(unsigned OpcodeTo, unsigned OpcodeFrom);\n\n  /// Determine what action should be taken to legalize the described\n  /// instruction. Requires computeTables to have been called.\n  ///\n  /// \\returns a description of the next legalization step to perform.\n  LegalizeActionStep getAction(const LegalityQuery &Query) const;\n\n  /// Determine what action should be taken to legalize the given generic\n  /// instruction.\n  ///\n  /// \\returns a description of the next legalization step to perform.\n  LegalizeActionStep getAction(const MachineInstr &MI,\n                               const MachineRegisterInfo &MRI) const;\n\n  bool isLegal(const LegalityQuery &Query) const {\n    return getAction(Query).Action == LegalizeAction::Legal;\n  }\n\n  bool isLegalOrCustom(const LegalityQuery &Query) const {\n    auto Action = getAction(Query).Action;\n    return Action == LegalizeAction::Legal || Action == LegalizeAction::Custom;\n  }\n\n  bool isLegal(const MachineInstr &MI, const MachineRegisterInfo &MRI) const;\n  bool isLegalOrCustom(const MachineInstr &MI,\n                       const MachineRegisterInfo &MRI) const;\n\n  /// Called for instructions with the Custom LegalizationAction.\n  virtual bool legalizeCustom(LegalizerHelper &Helper,\n                              MachineInstr &MI) const {\n    llvm_unreachable(\"must implement this if custom action is used\");\n  }\n\n  /// \\returns true if MI is either legal or has been legalized and false if not\n  /// legal.\n  /// Return true if MI is either legal or has been legalized and false\n  /// if not legal.\n  virtual bool legalizeIntrinsic(LegalizerHelper &Helper,\n                                 MachineInstr &MI) const {\n    return true;\n  }\n\n  /// Return the opcode (SEXT/ZEXT/ANYEXT) that should be performed while\n  /// widening a constant of type SmallTy which targets can override.\n  /// For eg, the DAG does (SmallTy.isByteSized() ? G_SEXT : G_ZEXT) which\n  /// will be the default.\n  virtual unsigned getExtOpcodeForWideningConstant(LLT SmallTy) const;\n\nprivate:\n  /// Determine what action should be taken to legalize the given generic\n  /// instruction opcode, type-index and type. Requires computeTables to have\n  /// been called.\n  ///\n  /// \\returns a pair consisting of the kind of legalization that should be\n  /// performed and the destination type.\n  std::pair<LegalizeAction, LLT>\n  getAspectAction(const InstrAspect &Aspect) const;\n\n  /// The SizeAndActionsVec is a representation mapping between all natural\n  /// numbers and an Action. The natural number represents the bit size of\n  /// the InstrAspect. For example, for a target with native support for 32-bit\n  /// and 64-bit additions, you'd express that as:\n  /// setScalarAction(G_ADD, 0,\n  ///           {{1, WidenScalar},  // bit sizes [ 1, 31[\n  ///            {32, Legal},       // bit sizes [32, 33[\n  ///            {33, WidenScalar}, // bit sizes [33, 64[\n  ///            {64, Legal},       // bit sizes [64, 65[\n  ///            {65, NarrowScalar} // bit sizes [65, +inf[\n  ///           });\n  /// It may be that only 64-bit pointers are supported on your target:\n  /// setPointerAction(G_PTR_ADD, 0, LLT:pointer(1),\n  ///           {{1, Unsupported},  // bit sizes [ 1, 63[\n  ///            {64, Legal},       // bit sizes [64, 65[\n  ///            {65, Unsupported}, // bit sizes [65, +inf[\n  ///           });\n  void setScalarAction(const unsigned Opcode, const unsigned TypeIndex,\n                       const SizeAndActionsVec &SizeAndActions) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    SmallVector<SizeAndActionsVec, 1> &Actions = ScalarActions[OpcodeIdx];\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n  void setPointerAction(const unsigned Opcode, const unsigned TypeIndex,\n                        const unsigned AddressSpace,\n                        const SizeAndActionsVec &SizeAndActions) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (AddrSpace2PointerActions[OpcodeIdx].find(AddressSpace) ==\n        AddrSpace2PointerActions[OpcodeIdx].end())\n      AddrSpace2PointerActions[OpcodeIdx][AddressSpace] = {{}};\n    SmallVector<SizeAndActionsVec, 1> &Actions =\n        AddrSpace2PointerActions[OpcodeIdx].find(AddressSpace)->second;\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n\n  /// If an operation on a given vector type (say <M x iN>) isn't explicitly\n  /// specified, we proceed in 2 stages. First we legalize the underlying scalar\n  /// (so that there's at least one legal vector with that scalar), then we\n  /// adjust the number of elements in the vector so that it is legal. The\n  /// desired action in the first step is controlled by this function.\n  void setScalarInVectorAction(const unsigned Opcode, const unsigned TypeIndex,\n                               const SizeAndActionsVec &SizeAndActions) {\n    unsigned OpcodeIdx = Opcode - FirstOp;\n    SmallVector<SizeAndActionsVec, 1> &Actions =\n        ScalarInVectorActions[OpcodeIdx];\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n\n  /// See also setScalarInVectorAction.\n  /// This function let's you specify the number of elements in a vector that\n  /// are legal for a legal element size.\n  void setVectorNumElementAction(const unsigned Opcode,\n                                 const unsigned TypeIndex,\n                                 const unsigned ElementSize,\n                                 const SizeAndActionsVec &SizeAndActions) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (NumElements2Actions[OpcodeIdx].find(ElementSize) ==\n        NumElements2Actions[OpcodeIdx].end())\n      NumElements2Actions[OpcodeIdx][ElementSize] = {{}};\n    SmallVector<SizeAndActionsVec, 1> &Actions =\n        NumElements2Actions[OpcodeIdx].find(ElementSize)->second;\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n\n  /// A partial SizeAndActionsVec potentially doesn't cover all bit sizes,\n  /// i.e. it's OK if it doesn't start from size 1.\n  static void checkPartialSizeAndActionsVector(const SizeAndActionsVec& v) {\n    using namespace LegalizeActions;\n#ifndef NDEBUG\n    // The sizes should be in increasing order\n    int prev_size = -1;\n    for(auto SizeAndAction: v) {\n      assert(SizeAndAction.first > prev_size);\n      prev_size = SizeAndAction.first;\n    }\n    // - for every Widen action, there should be a larger bitsize that\n    //   can be legalized towards (e.g. Legal, Lower, Libcall or Custom\n    //   action).\n    // - for every Narrow action, there should be a smaller bitsize that\n    //   can be legalized towards.\n    int SmallestNarrowIdx = -1;\n    int LargestWidenIdx = -1;\n    int SmallestLegalizableToSameSizeIdx = -1;\n    int LargestLegalizableToSameSizeIdx = -1;\n    for(size_t i=0; i<v.size(); ++i) {\n      switch (v[i].second) {\n        case FewerElements:\n        case NarrowScalar:\n          if (SmallestNarrowIdx == -1)\n            SmallestNarrowIdx = i;\n          break;\n        case WidenScalar:\n        case MoreElements:\n          LargestWidenIdx = i;\n          break;\n        case Unsupported:\n          break;\n        default:\n          if (SmallestLegalizableToSameSizeIdx == -1)\n            SmallestLegalizableToSameSizeIdx = i;\n          LargestLegalizableToSameSizeIdx = i;\n      }\n    }\n    if (SmallestNarrowIdx != -1) {\n      assert(SmallestLegalizableToSameSizeIdx != -1);\n      assert(SmallestNarrowIdx > SmallestLegalizableToSameSizeIdx);\n    }\n    if (LargestWidenIdx != -1)\n      assert(LargestWidenIdx < LargestLegalizableToSameSizeIdx);\n#endif\n  }\n\n  /// A full SizeAndActionsVec must cover all bit sizes, i.e. must start with\n  /// from size 1.\n  static void checkFullSizeAndActionsVector(const SizeAndActionsVec& v) {\n#ifndef NDEBUG\n    // Data structure invariant: The first bit size must be size 1.\n    assert(v.size() >= 1);\n    assert(v[0].first == 1);\n    checkPartialSizeAndActionsVector(v);\n#endif\n  }\n\n  /// Sets actions for all bit sizes on a particular generic opcode, type\n  /// index and scalar or pointer type.\n  void setActions(unsigned TypeIndex,\n                  SmallVector<SizeAndActionsVec, 1> &Actions,\n                  const SizeAndActionsVec &SizeAndActions) {\n    checkFullSizeAndActionsVector(SizeAndActions);\n    if (Actions.size() <= TypeIndex)\n      Actions.resize(TypeIndex + 1);\n    Actions[TypeIndex] = SizeAndActions;\n  }\n\n  static SizeAndAction findAction(const SizeAndActionsVec &Vec,\n                                  const uint32_t Size);\n\n  /// Returns the next action needed to get the scalar or pointer type closer\n  /// to being legal\n  /// E.g. findLegalAction({G_REM, 13}) should return\n  /// (WidenScalar, 32). After that, findLegalAction({G_REM, 32}) will\n  /// probably be called, which should return (Lower, 32).\n  /// This is assuming the setScalarAction on G_REM was something like:\n  /// setScalarAction(G_REM, 0,\n  ///           {{1, WidenScalar},  // bit sizes [ 1, 31[\n  ///            {32, Lower},       // bit sizes [32, 33[\n  ///            {33, NarrowScalar} // bit sizes [65, +inf[\n  ///           });\n  std::pair<LegalizeAction, LLT>\n  findScalarLegalAction(const InstrAspect &Aspect) const;\n\n  /// Returns the next action needed towards legalizing the vector type.\n  std::pair<LegalizeAction, LLT>\n  findVectorLegalAction(const InstrAspect &Aspect) const;\n\n  static const int FirstOp = TargetOpcode::PRE_ISEL_GENERIC_OPCODE_START;\n  static const int LastOp = TargetOpcode::PRE_ISEL_GENERIC_OPCODE_END;\n\n  // Data structures used temporarily during construction of legality data:\n  using TypeMap = DenseMap<LLT, LegalizeAction>;\n  SmallVector<TypeMap, 1> SpecifiedActions[LastOp - FirstOp + 1];\n  SmallVector<SizeChangeStrategy, 1>\n      ScalarSizeChangeStrategies[LastOp - FirstOp + 1];\n  SmallVector<SizeChangeStrategy, 1>\n      VectorElementSizeChangeStrategies[LastOp - FirstOp + 1];\n  bool TablesInitialized;\n\n  // Data structures used by getAction:\n  SmallVector<SizeAndActionsVec, 1> ScalarActions[LastOp - FirstOp + 1];\n  SmallVector<SizeAndActionsVec, 1> ScalarInVectorActions[LastOp - FirstOp + 1];\n  std::unordered_map<uint16_t, SmallVector<SizeAndActionsVec, 1>>\n      AddrSpace2PointerActions[LastOp - FirstOp + 1];\n  std::unordered_map<uint16_t, SmallVector<SizeAndActionsVec, 1>>\n      NumElements2Actions[LastOp - FirstOp + 1];\n\n  LegalizeRuleSet RulesForOpcode[LastOp - FirstOp + 1];\n};\n\n#ifndef NDEBUG\n/// Checks that MIR is fully legal, returns an illegal instruction if it's not,\n/// nullptr otherwise\nconst MachineInstr *machineFunctionIsIllegal(const MachineFunction &MF);\n#endif\n\n} // end namespace llvm.\n\n#endif // LLVM_CODEGEN_GLOBALISEL_LEGALIZERINFO_H\n"}, "42": {"id": 42, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "content": "//===-- llvm/CodeGen/GlobalISel/MachineIRBuilder.h - MIBuilder --*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n/// \\file\n/// This file declares the MachineIRBuilder class.\n/// This is a helper class to build MachineInstr.\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_MACHINEIRBUILDER_H\n#define LLVM_CODEGEN_GLOBALISEL_MACHINEIRBUILDER_H\n\n#include \"llvm/CodeGen/GlobalISel/CSEInfo.h\"\n#include \"llvm/CodeGen/LowLevelType.h\"\n#include \"llvm/CodeGen/MachineBasicBlock.h\"\n#include \"llvm/CodeGen/MachineInstrBuilder.h\"\n#include \"llvm/CodeGen/MachineRegisterInfo.h\"\n#include \"llvm/CodeGen/TargetOpcodes.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/Module.h\"\n\nnamespace llvm {\n\n// Forward declarations.\nclass MachineFunction;\nclass MachineInstr;\nclass TargetInstrInfo;\nclass GISelChangeObserver;\n\n/// Class which stores all the state required in a MachineIRBuilder.\n/// Since MachineIRBuilders will only store state in this object, it allows\n/// to transfer BuilderState between different kinds of MachineIRBuilders.\nstruct MachineIRBuilderState {\n  /// MachineFunction under construction.\n  MachineFunction *MF = nullptr;\n  /// Information used to access the description of the opcodes.\n  const TargetInstrInfo *TII = nullptr;\n  /// Information used to verify types are consistent and to create virtual registers.\n  MachineRegisterInfo *MRI = nullptr;\n  /// Debug location to be set to any instruction we create.\n  DebugLoc DL;\n\n  /// \\name Fields describing the insertion point.\n  /// @{\n  MachineBasicBlock *MBB = nullptr;\n  MachineBasicBlock::iterator II;\n  /// @}\n\n  GISelChangeObserver *Observer = nullptr;\n\n  GISelCSEInfo *CSEInfo = nullptr;\n};\n\nclass DstOp {\n  union {\n    LLT LLTTy;\n    Register Reg;\n    const TargetRegisterClass *RC;\n  };\n\npublic:\n  enum class DstType { Ty_LLT, Ty_Reg, Ty_RC };\n  DstOp(unsigned R) : Reg(R), Ty(DstType::Ty_Reg) {}\n  DstOp(Register R) : Reg(R), Ty(DstType::Ty_Reg) {}\n  DstOp(const MachineOperand &Op) : Reg(Op.getReg()), Ty(DstType::Ty_Reg) {}\n  DstOp(const LLT T) : LLTTy(T), Ty(DstType::Ty_LLT) {}\n  DstOp(const TargetRegisterClass *TRC) : RC(TRC), Ty(DstType::Ty_RC) {}\n\n  void addDefToMIB(MachineRegisterInfo &MRI, MachineInstrBuilder &MIB) const {\n    switch (Ty) {\n    case DstType::Ty_Reg:\n      MIB.addDef(Reg);\n      break;\n    case DstType::Ty_LLT:\n      MIB.addDef(MRI.createGenericVirtualRegister(LLTTy));\n      break;\n    case DstType::Ty_RC:\n      MIB.addDef(MRI.createVirtualRegister(RC));\n      break;\n    }\n  }\n\n  LLT getLLTTy(const MachineRegisterInfo &MRI) const {\n    switch (Ty) {\n    case DstType::Ty_RC:\n      return LLT{};\n    case DstType::Ty_LLT:\n      return LLTTy;\n    case DstType::Ty_Reg:\n      return MRI.getType(Reg);\n    }\n    llvm_unreachable(\"Unrecognised DstOp::DstType enum\");\n  }\n\n  Register getReg() const {\n    assert(Ty == DstType::Ty_Reg && \"Not a register\");\n    return Reg;\n  }\n\n  const TargetRegisterClass *getRegClass() const {\n    switch (Ty) {\n    case DstType::Ty_RC:\n      return RC;\n    default:\n      llvm_unreachable(\"Not a RC Operand\");\n    }\n  }\n\n  DstType getDstOpKind() const { return Ty; }\n\nprivate:\n  DstType Ty;\n};\n\nclass SrcOp {\n  union {\n    MachineInstrBuilder SrcMIB;\n    Register Reg;\n    CmpInst::Predicate Pred;\n    int64_t Imm;\n  };\n\npublic:\n  enum class SrcType { Ty_Reg, Ty_MIB, Ty_Predicate, Ty_Imm };\n  SrcOp(Register R) : Reg(R), Ty(SrcType::Ty_Reg) {}\n  SrcOp(const MachineOperand &Op) : Reg(Op.getReg()), Ty(SrcType::Ty_Reg) {}\n  SrcOp(const MachineInstrBuilder &MIB) : SrcMIB(MIB), Ty(SrcType::Ty_MIB) {}\n  SrcOp(const CmpInst::Predicate P) : Pred(P), Ty(SrcType::Ty_Predicate) {}\n  /// Use of registers held in unsigned integer variables (or more rarely signed\n  /// integers) is no longer permitted to avoid ambiguity with upcoming support\n  /// for immediates.\n  SrcOp(unsigned) = delete;\n  SrcOp(int) = delete;\n  SrcOp(uint64_t V) : Imm(V), Ty(SrcType::Ty_Imm) {}\n  SrcOp(int64_t V) : Imm(V), Ty(SrcType::Ty_Imm) {}\n\n  void addSrcToMIB(MachineInstrBuilder &MIB) const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n      MIB.addPredicate(Pred);\n      break;\n    case SrcType::Ty_Reg:\n      MIB.addUse(Reg);\n      break;\n    case SrcType::Ty_MIB:\n      MIB.addUse(SrcMIB->getOperand(0).getReg());\n      break;\n    case SrcType::Ty_Imm:\n      MIB.addImm(Imm);\n      break;\n    }\n  }\n\n  LLT getLLTTy(const MachineRegisterInfo &MRI) const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n    case SrcType::Ty_Imm:\n      llvm_unreachable(\"Not a register operand\");\n    case SrcType::Ty_Reg:\n      return MRI.getType(Reg);\n    case SrcType::Ty_MIB:\n      return MRI.getType(SrcMIB->getOperand(0).getReg());\n    }\n    llvm_unreachable(\"Unrecognised SrcOp::SrcType enum\");\n  }\n\n  Register getReg() const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n    case SrcType::Ty_Imm:\n      llvm_unreachable(\"Not a register operand\");\n    case SrcType::Ty_Reg:\n      return Reg;\n    case SrcType::Ty_MIB:\n      return SrcMIB->getOperand(0).getReg();\n    }\n    llvm_unreachable(\"Unrecognised SrcOp::SrcType enum\");\n  }\n\n  CmpInst::Predicate getPredicate() const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n      return Pred;\n    default:\n      llvm_unreachable(\"Not a register operand\");\n    }\n  }\n\n  int64_t getImm() const {\n    switch (Ty) {\n    case SrcType::Ty_Imm:\n      return Imm;\n    default:\n      llvm_unreachable(\"Not an immediate\");\n    }\n  }\n\n  SrcType getSrcOpKind() const { return Ty; }\n\nprivate:\n  SrcType Ty;\n};\n\nclass FlagsOp {\n  Optional<unsigned> Flags;\n\npublic:\n  explicit FlagsOp(unsigned F) : Flags(F) {}\n  FlagsOp() : Flags(None) {}\n  Optional<unsigned> getFlags() const { return Flags; }\n};\n/// Helper class to build MachineInstr.\n/// It keeps internally the insertion point and debug location for all\n/// the new instructions we want to create.\n/// This information can be modify via the related setters.\nclass MachineIRBuilder {\n\n  MachineIRBuilderState State;\n\nprotected:\n  void validateTruncExt(const LLT Dst, const LLT Src, bool IsExtend);\n\n  void validateUnaryOp(const LLT Res, const LLT Op0);\n  void validateBinaryOp(const LLT Res, const LLT Op0, const LLT Op1);\n  void validateShiftOp(const LLT Res, const LLT Op0, const LLT Op1);\n\n  void validateSelectOp(const LLT ResTy, const LLT TstTy, const LLT Op0Ty,\n                        const LLT Op1Ty);\n\n  void recordInsertion(MachineInstr *InsertedInstr) const {\n    if (State.Observer)\n      State.Observer->createdInstr(*InsertedInstr);\n  }\n\npublic:\n  /// Some constructors for easy use.\n  MachineIRBuilder() = default;\n  MachineIRBuilder(MachineFunction &MF) { setMF(MF); }\n\n  MachineIRBuilder(MachineBasicBlock &MBB, MachineBasicBlock::iterator InsPt) {\n    setMF(*MBB.getParent());\n    setInsertPt(MBB, InsPt);\n  }\n\n  MachineIRBuilder(MachineInstr &MI) :\n    MachineIRBuilder(*MI.getParent(), MI.getIterator()) {\n    setInstr(MI);\n    setDebugLoc(MI.getDebugLoc());\n  }\n\n  MachineIRBuilder(MachineInstr &MI, GISelChangeObserver &Observer) :\n    MachineIRBuilder(MI) {\n    setChangeObserver(Observer);\n  }\n\n  virtual ~MachineIRBuilder() = default;\n\n  MachineIRBuilder(const MachineIRBuilderState &BState) : State(BState) {}\n\n  const TargetInstrInfo &getTII() {\n    assert(State.TII && \"TargetInstrInfo is not set\");\n    return *State.TII;\n  }\n\n  /// Getter for the function we currently build.\n  MachineFunction &getMF() {\n    assert(State.MF && \"MachineFunction is not set\");\n    return *State.MF;\n  }\n\n  const MachineFunction &getMF() const {\n    assert(State.MF && \"MachineFunction is not set\");\n    return *State.MF;\n  }\n\n  const DataLayout &getDataLayout() const {\n    return getMF().getFunction().getParent()->getDataLayout();\n  }\n\n  /// Getter for DebugLoc\n  const DebugLoc &getDL() { return State.DL; }\n\n  /// Getter for MRI\n  MachineRegisterInfo *getMRI() { return State.MRI; }\n  const MachineRegisterInfo *getMRI() const { return State.MRI; }\n\n  /// Getter for the State\n  MachineIRBuilderState &getState() { return State; }\n\n  /// Getter for the basic block we currently build.\n  const MachineBasicBlock &getMBB() const {\n    assert(State.MBB && \"MachineBasicBlock is not set\");\n    return *State.MBB;\n  }\n\n  MachineBasicBlock &getMBB() {\n    return const_cast<MachineBasicBlock &>(\n        const_cast<const MachineIRBuilder *>(this)->getMBB());\n  }\n\n  GISelCSEInfo *getCSEInfo() { return State.CSEInfo; }\n  const GISelCSEInfo *getCSEInfo() const { return State.CSEInfo; }\n\n  /// Current insertion point for new instructions.\n  MachineBasicBlock::iterator getInsertPt() { return State.II; }\n\n  /// Set the insertion point before the specified position.\n  /// \\pre MBB must be in getMF().\n  /// \\pre II must be a valid iterator in MBB.\n  void setInsertPt(MachineBasicBlock &MBB, MachineBasicBlock::iterator II) {\n    assert(MBB.getParent() == &getMF() &&\n           \"Basic block is in a different function\");\n    State.MBB = &MBB;\n    State.II = II;\n  }\n\n  /// @}\n\n  void setCSEInfo(GISelCSEInfo *Info) { State.CSEInfo = Info; }\n\n  /// \\name Setters for the insertion point.\n  /// @{\n  /// Set the MachineFunction where to build instructions.\n  void setMF(MachineFunction &MF);\n\n  /// Set the insertion point to the  end of \\p MBB.\n  /// \\pre \\p MBB must be contained by getMF().\n  void setMBB(MachineBasicBlock &MBB) {\n    State.MBB = &MBB;\n    State.II = MBB.end();\n    assert(&getMF() == MBB.getParent() &&\n           \"Basic block is in a different function\");\n  }\n\n  /// Set the insertion point to before MI.\n  /// \\pre MI must be in getMF().\n  void setInstr(MachineInstr &MI) {\n    assert(MI.getParent() && \"Instruction is not part of a basic block\");\n    setMBB(*MI.getParent());\n    State.II = MI.getIterator();\n  }\n  /// @}\n\n  /// Set the insertion point to before MI, and set the debug loc to MI's loc.\n  /// \\pre MI must be in getMF().\n  void setInstrAndDebugLoc(MachineInstr &MI) {\n    setInstr(MI);\n    setDebugLoc(MI.getDebugLoc());\n  }\n\n  void setChangeObserver(GISelChangeObserver &Observer) {\n    State.Observer = &Observer;\n  }\n\n  void stopObservingChanges() { State.Observer = nullptr; }\n  /// @}\n\n  /// Set the debug location to \\p DL for all the next build instructions.\n  void setDebugLoc(const DebugLoc &DL) { this->State.DL = DL; }\n\n  /// Get the current instruction's debug location.\n  DebugLoc getDebugLoc() { return State.DL; }\n\n  /// Build and insert <empty> = \\p Opcode <empty>.\n  /// The insertion point is the one set by the last call of either\n  /// setBasicBlock or setMI.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildInstr(unsigned Opcode) {\n    return insertInstr(buildInstrNoInsert(Opcode));\n  }\n\n  /// Build but don't insert <empty> = \\p Opcode <empty>.\n  ///\n  /// \\pre setMF, setBasicBlock or setMI  must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildInstrNoInsert(unsigned Opcode);\n\n  /// Insert an existing instruction at the insertion point.\n  MachineInstrBuilder insertInstr(MachineInstrBuilder MIB);\n\n  /// Build and insert a DBG_VALUE instruction expressing the fact that the\n  /// associated \\p Variable lives in \\p Reg (suitably modified by \\p Expr).\n  MachineInstrBuilder buildDirectDbgValue(Register Reg, const MDNode *Variable,\n                                          const MDNode *Expr);\n\n  /// Build and insert a DBG_VALUE instruction expressing the fact that the\n  /// associated \\p Variable lives in memory at \\p Reg (suitably modified by \\p\n  /// Expr).\n  MachineInstrBuilder buildIndirectDbgValue(Register Reg,\n                                            const MDNode *Variable,\n                                            const MDNode *Expr);\n\n  /// Build and insert a DBG_VALUE instruction expressing the fact that the\n  /// associated \\p Variable lives in the stack slot specified by \\p FI\n  /// (suitably modified by \\p Expr).\n  MachineInstrBuilder buildFIDbgValue(int FI, const MDNode *Variable,\n                                      const MDNode *Expr);\n\n  /// Build and insert a DBG_VALUE instructions specifying that \\p Variable is\n  /// given by \\p C (suitably modified by \\p Expr).\n  MachineInstrBuilder buildConstDbgValue(const Constant &C,\n                                         const MDNode *Variable,\n                                         const MDNode *Expr);\n\n  /// Build and insert a DBG_LABEL instructions specifying that \\p Label is\n  /// given. Convert \"llvm.dbg.label Label\" to \"DBG_LABEL Label\".\n  MachineInstrBuilder buildDbgLabel(const MDNode *Label);\n\n  /// Build and insert \\p Res = G_DYN_STACKALLOC \\p Size, \\p Align\n  ///\n  /// G_DYN_STACKALLOC does a dynamic stack allocation and writes the address of\n  /// the allocated memory into \\p Res.\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildDynStackAlloc(const DstOp &Res, const SrcOp &Size,\n                                         Align Alignment);\n\n  /// Build and insert \\p Res = G_FRAME_INDEX \\p Idx\n  ///\n  /// G_FRAME_INDEX materializes the address of an alloca value or other\n  /// stack-based object.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildFrameIndex(const DstOp &Res, int Idx);\n\n  /// Build and insert \\p Res = G_GLOBAL_VALUE \\p GV\n  ///\n  /// G_GLOBAL_VALUE materializes the address of the specified global\n  /// into \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with pointer type\n  ///      in the same address space as \\p GV.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildGlobalValue(const DstOp &Res, const GlobalValue *GV);\n\n  /// Build and insert \\p Res = G_PTR_ADD \\p Op0, \\p Op1\n  ///\n  /// G_PTR_ADD adds \\p Op1 addressible units to the pointer specified by \\p Op0,\n  /// storing the resulting pointer in \\p Res. Addressible units are typically\n  /// bytes but this can vary between targets.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Op0 must be generic virtual registers with pointer\n  ///      type.\n  /// \\pre \\p Op1 must be a generic virtual register with scalar type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildPtrAdd(const DstOp &Res, const SrcOp &Op0,\n                                  const SrcOp &Op1);\n\n  /// Materialize and insert \\p Res = G_PTR_ADD \\p Op0, (G_CONSTANT \\p Value)\n  ///\n  /// G_PTR_ADD adds \\p Value bytes to the pointer specified by \\p Op0,\n  /// storing the resulting pointer in \\p Res. If \\p Value is zero then no\n  /// G_PTR_ADD or G_CONSTANT will be created and \\pre Op0 will be assigned to\n  /// \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Op0 must be a generic virtual register with pointer type.\n  /// \\pre \\p ValueTy must be a scalar type.\n  /// \\pre \\p Res must be 0. This is to detect confusion between\n  ///      materializePtrAdd() and buildPtrAdd().\n  /// \\post \\p Res will either be a new generic virtual register of the same\n  ///       type as \\p Op0 or \\p Op0 itself.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  Optional<MachineInstrBuilder> materializePtrAdd(Register &Res, Register Op0,\n                                                  const LLT ValueTy,\n                                                  uint64_t Value);\n\n  /// Build and insert \\p Res = G_PTRMASK \\p Op0, \\p Op1\n  MachineInstrBuilder buildPtrMask(const DstOp &Res, const SrcOp &Op0,\n                                   const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_PTRMASK, {Res}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res = G_PTRMASK \\p Op0, \\p G_CONSTANT (1 << NumBits) - 1\n  ///\n  /// This clears the low bits of a pointer operand without destroying its\n  /// pointer properties. This has the effect of rounding the address *down* to\n  /// a specified alignment in bits.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Op0 must be generic virtual registers with pointer\n  ///      type.\n  /// \\pre \\p NumBits must be an integer representing the number of low bits to\n  ///      be cleared in \\p Op0.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildMaskLowPtrBits(const DstOp &Res, const SrcOp &Op0,\n                                          uint32_t NumBits);\n\n  /// Build and insert \\p Res, \\p CarryOut = G_UADDO \\p Op0, \\p Op1\n  ///\n  /// G_UADDO sets \\p Res to \\p Op0 + \\p Op1 (truncated to the bit width) and\n  /// sets \\p CarryOut to 1 if the result overflowed in unsigned arithmetic.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers with the\n  /// same scalar type.\n  ////\\pre \\p CarryOut must be generic virtual register with scalar type\n  ///(typically s1)\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildUAddo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_UADDO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_USUBO \\p Op0, \\p Op1\n  MachineInstrBuilder buildUSubo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_USUBO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SADDO \\p Op0, \\p Op1\n  MachineInstrBuilder buildSAddo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_SADDO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SUBO \\p Op0, \\p Op1\n  MachineInstrBuilder buildSSubo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_SSUBO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_UADDE \\p Op0,\n  /// \\p Op1, \\p CarryIn\n  ///\n  /// G_UADDE sets \\p Res to \\p Op0 + \\p Op1 + \\p CarryIn (truncated to the bit\n  /// width) and sets \\p CarryOut to 1 if the result overflowed in unsigned\n  /// arithmetic.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same scalar type.\n  /// \\pre \\p CarryOut and \\p CarryIn must be generic virtual\n  ///      registers with the same scalar type (typically s1)\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildUAdde(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_UADDE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_USUBE \\p Op0, \\p Op1, \\p CarryInp\n  MachineInstrBuilder buildUSube(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_USUBE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SADDE \\p Op0, \\p Op1, \\p CarryInp\n  MachineInstrBuilder buildSAdde(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_SADDE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SSUBE \\p Op0, \\p Op1, \\p CarryInp\n  MachineInstrBuilder buildSSube(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_SSUBE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res = G_ANYEXT \\p Op0\n  ///\n  /// G_ANYEXT produces a register of the specified width, with bits 0 to\n  /// sizeof(\\p Ty) * 8 set to \\p Op. The remaining bits are unspecified\n  /// (i.e. this is neither zero nor sign-extension). For a vector register,\n  /// each element is extended individually.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be smaller than \\p Res\n  ///\n  /// \\return The newly created instruction.\n\n  MachineInstrBuilder buildAnyExt(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_SEXT \\p Op\n  ///\n  /// G_SEXT produces a register of the specified width, with bits 0 to\n  /// sizeof(\\p Ty) * 8 set to \\p Op. The remaining bits are duplicated from the\n  /// high bit of \\p Op (i.e. 2s-complement sign extended).\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be smaller than \\p Res\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildSExt(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_SEXT_INREG \\p Op, ImmOp\n  MachineInstrBuilder buildSExtInReg(const DstOp &Res, const SrcOp &Op, int64_t ImmOp) {\n    return buildInstr(TargetOpcode::G_SEXT_INREG, {Res}, {Op, SrcOp(ImmOp)});\n  }\n\n  /// Build and insert \\p Res = G_FPEXT \\p Op\n  MachineInstrBuilder buildFPExt(const DstOp &Res, const SrcOp &Op,\n                                 Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FPEXT, {Res}, {Op}, Flags);\n  }\n\n\n  /// Build and insert a G_PTRTOINT instruction.\n  MachineInstrBuilder buildPtrToInt(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_PTRTOINT, {Dst}, {Src});\n  }\n\n  /// Build and insert a G_INTTOPTR instruction.\n  MachineInstrBuilder buildIntToPtr(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_INTTOPTR, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Dst = G_BITCAST \\p Src\n  MachineInstrBuilder buildBitcast(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_BITCAST, {Dst}, {Src});\n  }\n\n    /// Build and insert \\p Dst = G_ADDRSPACE_CAST \\p Src\n  MachineInstrBuilder buildAddrSpaceCast(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_ADDRSPACE_CAST, {Dst}, {Src});\n  }\n\n  /// \\return The opcode of the extension the target wants to use for boolean\n  /// values.\n  unsigned getBoolExtOp(bool IsVec, bool IsFP) const;\n\n  // Build and insert \\p Res = G_ANYEXT \\p Op, \\p Res = G_SEXT \\p Op, or \\p Res\n  // = G_ZEXT \\p Op depending on how the target wants to extend boolean values.\n  MachineInstrBuilder buildBoolExt(const DstOp &Res, const SrcOp &Op,\n                                   bool IsFP);\n\n  /// Build and insert \\p Res = G_ZEXT \\p Op\n  ///\n  /// G_ZEXT produces a register of the specified width, with bits 0 to\n  /// sizeof(\\p Ty) * 8 set to \\p Op. The remaining bits are 0. For a vector\n  /// register, each element is extended individually.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be smaller than \\p Res\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildZExt(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_SEXT \\p Op, \\p Res = G_TRUNC \\p Op, or\n  /// \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildSExtOrTrunc(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_ZEXT \\p Op, \\p Res = G_TRUNC \\p Op, or\n  /// \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildZExtOrTrunc(const DstOp &Res, const SrcOp &Op);\n\n  // Build and insert \\p Res = G_ANYEXT \\p Op, \\p Res = G_TRUNC \\p Op, or\n  /// \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildAnyExtOrTrunc(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = \\p ExtOpc, \\p Res = G_TRUNC \\p\n  /// Op, or \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and\n  /// \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildExtOrTrunc(unsigned ExtOpc, const DstOp &Res,\n                                      const SrcOp &Op);\n\n  /// Build and insert an appropriate cast between two registers of equal size.\n  MachineInstrBuilder buildCast(const DstOp &Dst, const SrcOp &Src);\n\n  /// Build and insert G_BR \\p Dest\n  ///\n  /// G_BR is an unconditional branch to \\p Dest.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBr(MachineBasicBlock &Dest);\n\n  /// Build and insert G_BRCOND \\p Tst, \\p Dest\n  ///\n  /// G_BRCOND is a conditional branch to \\p Dest.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Tst must be a generic virtual register with scalar\n  ///      type. At the beginning of legalization, this will be a single\n  ///      bit (s1). Targets with interesting flags registers may change\n  ///      this. For a wider type, whether the branch is taken must only\n  ///      depend on bit 0 (for now).\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildBrCond(const SrcOp &Tst, MachineBasicBlock &Dest);\n\n  /// Build and insert G_BRINDIRECT \\p Tgt\n  ///\n  /// G_BRINDIRECT is an indirect branch to \\p Tgt.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Tgt must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBrIndirect(Register Tgt);\n\n  /// Build and insert G_BRJT \\p TablePtr, \\p JTI, \\p IndexReg\n  ///\n  /// G_BRJT is a jump table branch using a table base pointer \\p TablePtr,\n  /// jump table index \\p JTI and index \\p IndexReg\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p TablePtr must be a generic virtual register with pointer type.\n  /// \\pre \\p JTI must be be a jump table index.\n  /// \\pre \\p IndexReg must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBrJT(Register TablePtr, unsigned JTI,\n                                Register IndexReg);\n\n  /// Build and insert \\p Res = G_CONSTANT \\p Val\n  ///\n  /// G_CONSTANT is an integer constant with the specified size and value. \\p\n  /// Val will be extended or truncated to the size of \\p Reg.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or pointer\n  ///      type.\n  ///\n  /// \\return The newly created instruction.\n  virtual MachineInstrBuilder buildConstant(const DstOp &Res,\n                                            const ConstantInt &Val);\n\n  /// Build and insert \\p Res = G_CONSTANT \\p Val\n  ///\n  /// G_CONSTANT is an integer constant with the specified size and value.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildConstant(const DstOp &Res, int64_t Val);\n  MachineInstrBuilder buildConstant(const DstOp &Res, const APInt &Val);\n\n  /// Build and insert \\p Res = G_FCONSTANT \\p Val\n  ///\n  /// G_FCONSTANT is a floating-point constant with the specified size and\n  /// value.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  virtual MachineInstrBuilder buildFConstant(const DstOp &Res,\n                                             const ConstantFP &Val);\n\n  MachineInstrBuilder buildFConstant(const DstOp &Res, double Val);\n  MachineInstrBuilder buildFConstant(const DstOp &Res, const APFloat &Val);\n\n  /// Build and insert \\p Res = COPY Op\n  ///\n  /// Register-to-register COPY sets \\p Res to \\p Op.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildCopy(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_ASSERT_ZEXT Op, Size\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAssertZExt(const DstOp &Res, const SrcOp &Op,\n                                      unsigned Size);\n\n  /// Build and insert \\p Res = G_ASSERT_SEXT Op, Size\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAssertSExt(const DstOp &Res, const SrcOp &Op,\n                                      unsigned Size);\n\n  /// Build and insert `Res = G_LOAD Addr, MMO`.\n  ///\n  /// Loads the value stored at \\p Addr. Puts the result in \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildLoad(const DstOp &Res, const SrcOp &Addr,\n                                MachineMemOperand &MMO) {\n    return buildLoadInstr(TargetOpcode::G_LOAD, Res, Addr, MMO);\n  }\n\n  /// Build and insert a G_LOAD instruction, while constructing the\n  /// MachineMemOperand.\n  MachineInstrBuilder\n  buildLoad(const DstOp &Res, const SrcOp &Addr, MachinePointerInfo PtrInfo,\n            Align Alignment,\n            MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n            const AAMDNodes &AAInfo = AAMDNodes());\n\n  /// Build and insert `Res = <opcode> Addr, MMO`.\n  ///\n  /// Loads the value stored at \\p Addr. Puts the result in \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildLoadInstr(unsigned Opcode, const DstOp &Res,\n                                     const SrcOp &Addr, MachineMemOperand &MMO);\n\n  /// Helper to create a load from a constant offset given a base address. Load\n  /// the type of \\p Dst from \\p Offset from the given base address and memory\n  /// operand.\n  MachineInstrBuilder buildLoadFromOffset(const DstOp &Dst,\n                                          const SrcOp &BasePtr,\n                                          MachineMemOperand &BaseMMO,\n                                          int64_t Offset);\n\n  /// Build and insert `G_STORE Val, Addr, MMO`.\n  ///\n  /// Stores the value \\p Val to \\p Addr.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Val must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildStore(const SrcOp &Val, const SrcOp &Addr,\n                                 MachineMemOperand &MMO);\n\n  /// Build and insert a G_STORE instruction, while constructing the\n  /// MachineMemOperand.\n  MachineInstrBuilder\n  buildStore(const SrcOp &Val, const SrcOp &Addr, MachinePointerInfo PtrInfo,\n             Align Alignment,\n             MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n             const AAMDNodes &AAInfo = AAMDNodes());\n\n  /// Build and insert `Res0, ... = G_EXTRACT Src, Idx0`.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Src must be generic virtual registers.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildExtract(const DstOp &Res, const SrcOp &Src, uint64_t Index);\n\n  /// Build and insert \\p Res = IMPLICIT_DEF.\n  MachineInstrBuilder buildUndef(const DstOp &Res);\n\n  /// Build and insert instructions to put \\p Ops together at the specified p\n  /// Indices to form a larger register.\n  ///\n  /// If the types of the input registers are uniform and cover the entirity of\n  /// \\p Res then a G_MERGE_VALUES will be produced. Otherwise an IMPLICIT_DEF\n  /// followed by a sequence of G_INSERT instructions.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The final element of the sequence must not extend past the end of the\n  ///      destination register.\n  /// \\pre The bits defined by each Op (derived from index and scalar size) must\n  ///      not overlap.\n  /// \\pre \\p Indices must be in ascending order of bit position.\n  void buildSequence(Register Res, ArrayRef<Register> Ops,\n                     ArrayRef<uint64_t> Indices);\n\n  /// Build and insert \\p Res = G_MERGE_VALUES \\p Op0, ...\n  ///\n  /// G_MERGE_VALUES combines the input elements contiguously into a larger\n  /// register.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the input\n  ///      registers.\n  /// \\pre The type of all \\p Ops registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildMerge(const DstOp &Res, ArrayRef<Register> Ops);\n  MachineInstrBuilder buildMerge(const DstOp &Res,\n                                 std::initializer_list<SrcOp> Ops);\n\n  /// Build and insert \\p Res0, ... = G_UNMERGE_VALUES \\p Op\n  ///\n  /// G_UNMERGE_VALUES splits contiguous bits of the input into multiple\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the input\n  ///      registers.\n  /// \\pre The type of all \\p Res registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildUnmerge(ArrayRef<LLT> Res, const SrcOp &Op);\n  MachineInstrBuilder buildUnmerge(ArrayRef<Register> Res, const SrcOp &Op);\n\n  /// Build and insert an unmerge of \\p Res sized pieces to cover \\p Op\n  MachineInstrBuilder buildUnmerge(LLT Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_BUILD_VECTOR \\p Op0, ...\n  ///\n  /// G_BUILD_VECTOR creates a vector value from multiple scalar registers.\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the\n  ///      input scalar registers.\n  /// \\pre The type of all \\p Ops registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBuildVector(const DstOp &Res,\n                                       ArrayRef<Register> Ops);\n\n  /// Build and insert \\p Res = G_BUILD_VECTOR with \\p Src replicated to fill\n  /// the number of elements\n  MachineInstrBuilder buildSplatVector(const DstOp &Res,\n                                       const SrcOp &Src);\n\n  /// Build and insert \\p Res = G_BUILD_VECTOR_TRUNC \\p Op0, ...\n  ///\n  /// G_BUILD_VECTOR_TRUNC creates a vector value from multiple scalar registers\n  /// which have types larger than the destination vector element type, and\n  /// truncates the values to fit.\n  ///\n  /// If the operands given are already the same size as the vector elt type,\n  /// then this method will instead create a G_BUILD_VECTOR instruction.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The type of all \\p Ops registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBuildVectorTrunc(const DstOp &Res,\n                                            ArrayRef<Register> Ops);\n\n  /// Build and insert a vector splat of a scalar \\p Src using a\n  /// G_INSERT_VECTOR_ELT and G_SHUFFLE_VECTOR idiom.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Src must have the same type as the element type of \\p Dst\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildShuffleSplat(const DstOp &Res, const SrcOp &Src);\n\n  /// Build and insert \\p Res = G_SHUFFLE_VECTOR \\p Src1, \\p Src2, \\p Mask\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildShuffleVector(const DstOp &Res, const SrcOp &Src1,\n                                         const SrcOp &Src2, ArrayRef<int> Mask);\n\n  /// Build and insert \\p Res = G_CONCAT_VECTORS \\p Op0, ...\n  ///\n  /// G_CONCAT_VECTORS creates a vector from the concatenation of 2 or more\n  /// vectors.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the input\n  ///      registers.\n  /// \\pre The type of all source operands must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildConcatVectors(const DstOp &Res,\n                                         ArrayRef<Register> Ops);\n\n  MachineInstrBuilder buildInsert(const DstOp &Res, const SrcOp &Src,\n                                  const SrcOp &Op, unsigned Index);\n\n  /// Build and insert either a G_INTRINSIC (if \\p HasSideEffects is false) or\n  /// G_INTRINSIC_W_SIDE_EFFECTS instruction. Its first operand will be the\n  /// result register definition unless \\p Reg is NoReg (== 0). The second\n  /// operand will be the intrinsic's ID.\n  ///\n  /// Callers are expected to add the required definitions and uses afterwards.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildIntrinsic(Intrinsic::ID ID, ArrayRef<Register> Res,\n                                     bool HasSideEffects);\n  MachineInstrBuilder buildIntrinsic(Intrinsic::ID ID, ArrayRef<DstOp> Res,\n                                     bool HasSideEffects);\n\n  /// Build and insert \\p Res = G_FPTRUNC \\p Op\n  ///\n  /// G_FPTRUNC converts a floating-point value into one with a smaller type.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Res must be smaller than \\p Op\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildFPTrunc(const DstOp &Res, const SrcOp &Op,\n                                   Optional<unsigned> Flags = None);\n\n  /// Build and insert \\p Res = G_TRUNC \\p Op\n  ///\n  /// G_TRUNC extracts the low bits of a type. For a vector type each element is\n  /// truncated independently before being packed into the destination.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Res must be smaller than \\p Op\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildTrunc(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert a \\p Res = G_ICMP \\p Pred, \\p Op0, \\p Op1\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n\n  /// \\pre \\p Res must be a generic virtual register with scalar or\n  ///      vector type. Typically this starts as s1 or <N x s1>.\n  /// \\pre \\p Op0 and Op1 must be generic virtual registers with the\n  ///      same number of elements as \\p Res. If \\p Res is a scalar,\n  ///      \\p Op0 must be either a scalar or pointer.\n  /// \\pre \\p Pred must be an integer predicate.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildICmp(CmpInst::Predicate Pred, const DstOp &Res,\n                                const SrcOp &Op0, const SrcOp &Op1);\n\n  /// Build and insert a \\p Res = G_FCMP \\p Pred\\p Op0, \\p Op1\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n\n  /// \\pre \\p Res must be a generic virtual register with scalar or\n  ///      vector type. Typically this starts as s1 or <N x s1>.\n  /// \\pre \\p Op0 and Op1 must be generic virtual registers with the\n  ///      same number of elements as \\p Res (or scalar, if \\p Res is\n  ///      scalar).\n  /// \\pre \\p Pred must be a floating-point predicate.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildFCmp(CmpInst::Predicate Pred, const DstOp &Res,\n                                const SrcOp &Op0, const SrcOp &Op1,\n                                Optional<unsigned> Flags = None);\n\n  /// Build and insert a \\p Res = G_SELECT \\p Tst, \\p Op0, \\p Op1\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same type.\n  /// \\pre \\p Tst must be a generic virtual register with scalar, pointer or\n  ///      vector type. If vector then it must have the same number of\n  ///      elements as the other parameters.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildSelect(const DstOp &Res, const SrcOp &Tst,\n                                  const SrcOp &Op0, const SrcOp &Op1,\n                                  Optional<unsigned> Flags = None);\n\n  /// Build and insert \\p Res = G_INSERT_VECTOR_ELT \\p Val,\n  /// \\p Elt, \\p Idx\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Val must be a generic virtual register\n  //       with the same vector type.\n  /// \\pre \\p Elt and \\p Idx must be a generic virtual register\n  ///      with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildInsertVectorElement(const DstOp &Res,\n                                               const SrcOp &Val,\n                                               const SrcOp &Elt,\n                                               const SrcOp &Idx);\n\n  /// Build and insert \\p Res = G_EXTRACT_VECTOR_ELT \\p Val, \\p Idx\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar type.\n  /// \\pre \\p Val must be a generic virtual register with vector type.\n  /// \\pre \\p Idx must be a generic virtual register with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildExtractVectorElement(const DstOp &Res,\n                                                const SrcOp &Val,\n                                                const SrcOp &Idx);\n\n  /// Build and insert `OldValRes<def>, SuccessRes<def> =\n  /// G_ATOMIC_CMPXCHG_WITH_SUCCESS Addr, CmpVal, NewVal, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with \\p NewVal if it is currently\n  /// \\p CmpVal otherwise leaves it unchanged. Puts the original value from \\p\n  /// Addr in \\p Res, along with an s1 indicating whether it was replaced.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register of scalar type.\n  /// \\pre \\p SuccessRes must be a generic virtual register of scalar type. It\n  ///      will be assigned 0 on failure and 1 on success.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, \\p CmpVal, and \\p NewVal must be generic virtual\n  ///      registers of the same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder\n  buildAtomicCmpXchgWithSuccess(Register OldValRes, Register SuccessRes,\n                                Register Addr, Register CmpVal, Register NewVal,\n                                MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMIC_CMPXCHG Addr, CmpVal, NewVal,\n  /// MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with \\p NewVal if it is currently\n  /// \\p CmpVal otherwise leaves it unchanged. Puts the original value from \\p\n  /// Addr in \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register of scalar type.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, \\p CmpVal, and \\p NewVal must be generic virtual\n  ///      registers of the same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicCmpXchg(Register OldValRes, Register Addr,\n                                         Register CmpVal, Register NewVal,\n                                         MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_<Opcode> Addr, Val, MMO`.\n  ///\n  /// Atomically read-modify-update the value at \\p Addr with \\p Val. Puts the\n  /// original value from \\p Addr in \\p OldValRes. The modification is\n  /// determined by the opcode.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMW(unsigned Opcode, const DstOp &OldValRes,\n                                     const SrcOp &Addr, const SrcOp &Val,\n                                     MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_XCHG Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with \\p Val. Puts the original\n  /// value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWXchg(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_ADD Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the addition of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWAdd(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_SUB Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the subtraction of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWSub(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_AND Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise and of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWAnd(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_NAND Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise nand of \\p Val\n  /// and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWNand(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_OR Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise or of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWOr(Register OldValRes, Register Addr,\n                                       Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_XOR Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise xor of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWXor(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_MAX Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the signed maximum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWMax(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_MIN Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the signed minimum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWMin(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_UMAX Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the unsigned maximum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWUmax(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_UMIN Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the unsigned minimum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWUmin(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_FADD Addr, Val, MMO`.\n  MachineInstrBuilder buildAtomicRMWFAdd(\n    const DstOp &OldValRes, const SrcOp &Addr, const SrcOp &Val,\n    MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_FSUB Addr, Val, MMO`.\n  MachineInstrBuilder buildAtomicRMWFSub(\n        const DstOp &OldValRes, const SrcOp &Addr, const SrcOp &Val,\n        MachineMemOperand &MMO);\n\n  /// Build and insert `G_FENCE Ordering, Scope`.\n  MachineInstrBuilder buildFence(unsigned Ordering, unsigned Scope);\n\n  /// Build and insert \\p Dst = G_FREEZE \\p Src\n  MachineInstrBuilder buildFreeze(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_FREEZE, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_BLOCK_ADDR \\p BA\n  ///\n  /// G_BLOCK_ADDR computes the address of a basic block.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register of a pointer type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildBlockAddress(Register Res, const BlockAddress *BA);\n\n  /// Build and insert \\p Res = G_ADD \\p Op0, \\p Op1\n  ///\n  /// G_ADD sets \\p Res to the sum of integer parameters \\p Op0 and \\p Op1,\n  /// truncated to their width.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n\n  MachineInstrBuilder buildAdd(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_ADD, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_SUB \\p Op0, \\p Op1\n  ///\n  /// G_SUB sets \\p Res to the sum of integer parameters \\p Op0 and \\p Op1,\n  /// truncated to their width.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n\n  MachineInstrBuilder buildSub(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_SUB, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_MUL \\p Op0, \\p Op1\n  ///\n  /// G_MUL sets \\p Res to the sum of integer parameters \\p Op0 and \\p Op1,\n  /// truncated to their width.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildMul(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_MUL, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildUMulH(const DstOp &Dst, const SrcOp &Src0,\n                                 const SrcOp &Src1,\n                                 Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_UMULH, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildSMulH(const DstOp &Dst, const SrcOp &Src0,\n                                 const SrcOp &Src1,\n                                 Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_SMULH, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMul(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMUL, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMinNum(const DstOp &Dst, const SrcOp &Src0,\n                                   const SrcOp &Src1,\n                                   Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMINNUM, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMaxNum(const DstOp &Dst, const SrcOp &Src0,\n                                   const SrcOp &Src1,\n                                   Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMAXNUM, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMinNumIEEE(const DstOp &Dst, const SrcOp &Src0,\n                                       const SrcOp &Src1,\n                                       Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMINNUM_IEEE, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMaxNumIEEE(const DstOp &Dst, const SrcOp &Src0,\n                                       const SrcOp &Src1,\n                                       Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMAXNUM_IEEE, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildShl(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_SHL, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildLShr(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_LSHR, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildAShr(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_ASHR, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_AND \\p Op0, \\p Op1\n  ///\n  /// G_AND sets \\p Res to the bitwise and of integer parameters \\p Op0 and \\p\n  /// Op1.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n\n  MachineInstrBuilder buildAnd(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_AND, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_OR \\p Op0, \\p Op1\n  ///\n  /// G_OR sets \\p Res to the bitwise or of integer parameters \\p Op0 and \\p\n  /// Op1.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildOr(const DstOp &Dst, const SrcOp &Src0,\n                              const SrcOp &Src1,\n                              Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_OR, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_XOR \\p Op0, \\p Op1\n  MachineInstrBuilder buildXor(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_XOR, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert a bitwise not,\n  /// \\p NegOne = G_CONSTANT -1\n  /// \\p Res = G_OR \\p Op0, NegOne\n  MachineInstrBuilder buildNot(const DstOp &Dst, const SrcOp &Src0) {\n    auto NegOne = buildConstant(Dst.getLLTTy(*getMRI()), -1);\n    return buildInstr(TargetOpcode::G_XOR, {Dst}, {Src0, NegOne});\n  }\n\n  /// Build and insert \\p Res = G_CTPOP \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTPOP(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTPOP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTLZ \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTLZ(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTLZ, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTLZ_ZERO_UNDEF \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTLZ_ZERO_UNDEF(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTLZ_ZERO_UNDEF, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTTZ \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTTZ(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTTZ, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTTZ_ZERO_UNDEF \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTTZ_ZERO_UNDEF(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTTZ_ZERO_UNDEF, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Dst = G_BSWAP \\p Src0\n  MachineInstrBuilder buildBSwap(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_BSWAP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_FADD \\p Op0, \\p Op1\n  MachineInstrBuilder buildFAdd(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FADD, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FSUB \\p Op0, \\p Op1\n  MachineInstrBuilder buildFSub(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FSUB, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FDIV \\p Op0, \\p Op1\n  MachineInstrBuilder buildFDiv(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FDIV, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FMA \\p Op0, \\p Op1, \\p Op2\n  MachineInstrBuilder buildFMA(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1, const SrcOp &Src2,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMA, {Dst}, {Src0, Src1, Src2}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FMAD \\p Op0, \\p Op1, \\p Op2\n  MachineInstrBuilder buildFMAD(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1, const SrcOp &Src2,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMAD, {Dst}, {Src0, Src1, Src2}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FNEG \\p Op0\n  MachineInstrBuilder buildFNeg(const DstOp &Dst, const SrcOp &Src0,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FNEG, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FABS \\p Op0\n  MachineInstrBuilder buildFAbs(const DstOp &Dst, const SrcOp &Src0,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FABS, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FCANONICALIZE \\p Src0\n  MachineInstrBuilder buildFCanonicalize(const DstOp &Dst, const SrcOp &Src0,\n                                         Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FCANONICALIZE, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_INTRINSIC_TRUNC \\p Src0\n  MachineInstrBuilder buildIntrinsicTrunc(const DstOp &Dst, const SrcOp &Src0,\n                                         Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_INTRINSIC_TRUNC, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Res = GFFLOOR \\p Op0, \\p Op1\n  MachineInstrBuilder buildFFloor(const DstOp &Dst, const SrcOp &Src0,\n                                          Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FFLOOR, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FLOG \\p Src\n  MachineInstrBuilder buildFLog(const DstOp &Dst, const SrcOp &Src,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FLOG, {Dst}, {Src}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FLOG2 \\p Src\n  MachineInstrBuilder buildFLog2(const DstOp &Dst, const SrcOp &Src,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FLOG2, {Dst}, {Src}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FEXP2 \\p Src\n  MachineInstrBuilder buildFExp2(const DstOp &Dst, const SrcOp &Src,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FEXP2, {Dst}, {Src}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FPOW \\p Src0, \\p Src1\n  MachineInstrBuilder buildFPow(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FPOW, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FCOPYSIGN \\p Op0, \\p Op1\n  MachineInstrBuilder buildFCopysign(const DstOp &Dst, const SrcOp &Src0,\n                                     const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_FCOPYSIGN, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_UITOFP \\p Src0\n  MachineInstrBuilder buildUITOFP(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_UITOFP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_SITOFP \\p Src0\n  MachineInstrBuilder buildSITOFP(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_SITOFP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_FPTOUI \\p Src0\n  MachineInstrBuilder buildFPTOUI(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_FPTOUI, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_FPTOSI \\p Src0\n  MachineInstrBuilder buildFPTOSI(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_FPTOSI, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_SMIN \\p Op0, \\p Op1\n  MachineInstrBuilder buildSMin(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_SMIN, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_SMAX \\p Op0, \\p Op1\n  MachineInstrBuilder buildSMax(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_SMAX, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_UMIN \\p Op0, \\p Op1\n  MachineInstrBuilder buildUMin(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_UMIN, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_UMAX \\p Op0, \\p Op1\n  MachineInstrBuilder buildUMax(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_UMAX, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Dst = G_ABS \\p Src\n  MachineInstrBuilder buildAbs(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_ABS, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_JUMP_TABLE \\p JTI\n  ///\n  /// G_JUMP_TABLE sets \\p Res to the address of the jump table specified by\n  /// the jump table index \\p JTI.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildJumpTable(const LLT PtrTy, unsigned JTI);\n\n  /// Build and insert \\p Res = G_VECREDUCE_SEQ_FADD \\p ScalarIn, \\p VecIn\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to start the sequential\n  /// reduction operation of \\p VecIn.\n  MachineInstrBuilder buildVecReduceSeqFAdd(const DstOp &Dst,\n                                            const SrcOp &ScalarIn,\n                                            const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SEQ_FADD, {Dst},\n                      {ScalarIn, {VecIn}});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_SEQ_FMUL \\p ScalarIn, \\p VecIn\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to start the sequential\n  /// reduction operation of \\p VecIn.\n  MachineInstrBuilder buildVecReduceSeqFMul(const DstOp &Dst,\n                                            const SrcOp &ScalarIn,\n                                            const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SEQ_FMUL, {Dst},\n                      {ScalarIn, {VecIn}});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FADD \\p Src\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to the reduction operation of\n  /// \\p VecIn.\n  MachineInstrBuilder buildVecReduceFAdd(const DstOp &Dst,\n                                         const SrcOp &ScalarIn,\n                                         const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FADD, {Dst}, {ScalarIn, VecIn});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FMUL \\p Src\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to the reduction operation of\n  /// \\p VecIn.\n  MachineInstrBuilder buildVecReduceFMul(const DstOp &Dst,\n                                         const SrcOp &ScalarIn,\n                                         const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FMUL, {Dst}, {ScalarIn, VecIn});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FMAX \\p Src\n  MachineInstrBuilder buildVecReduceFMax(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FMAX, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FMIN \\p Src\n  MachineInstrBuilder buildVecReduceFMin(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FMIN, {Dst}, {Src});\n  }\n  /// Build and insert \\p Res = G_VECREDUCE_ADD \\p Src\n  MachineInstrBuilder buildVecReduceAdd(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_ADD, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_MUL \\p Src\n  MachineInstrBuilder buildVecReduceMul(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_MUL, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_AND \\p Src\n  MachineInstrBuilder buildVecReduceAnd(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_AND, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_OR \\p Src\n  MachineInstrBuilder buildVecReduceOr(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_OR, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_XOR \\p Src\n  MachineInstrBuilder buildVecReduceXor(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_XOR, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_SMAX \\p Src\n  MachineInstrBuilder buildVecReduceSMax(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SMAX, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_SMIN \\p Src\n  MachineInstrBuilder buildVecReduceSMin(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SMIN, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_UMAX \\p Src\n  MachineInstrBuilder buildVecReduceUMax(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_UMAX, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_UMIN \\p Src\n  MachineInstrBuilder buildVecReduceUMin(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_UMIN, {Dst}, {Src});\n  }\n  virtual MachineInstrBuilder buildInstr(unsigned Opc, ArrayRef<DstOp> DstOps,\n                                         ArrayRef<SrcOp> SrcOps,\n                                         Optional<unsigned> Flags = None);\n};\n\n} // End namespace llvm.\n#endif // LLVM_CODEGEN_GLOBALISEL_MACHINEIRBUILDER_H\n"}, "43": {"id": 43, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/LiveRegUnits.h", "content": "//===- llvm/CodeGen/LiveRegUnits.h - Register Unit Set ----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file\n/// A set of register units. It is intended for register liveness tracking.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_LIVEREGUNITS_H\n#define LLVM_CODEGEN_LIVEREGUNITS_H\n\n#include \"llvm/ADT/BitVector.h\"\n#include \"llvm/CodeGen/MachineInstrBundle.h\"\n#include \"llvm/CodeGen/TargetRegisterInfo.h\"\n#include \"llvm/MC/LaneBitmask.h\"\n#include \"llvm/MC/MCRegisterInfo.h\"\n#include <cstdint>\n\nnamespace llvm {\n\nclass MachineInstr;\nclass MachineBasicBlock;\n\n/// A set of register units used to track register liveness.\nclass LiveRegUnits {\n  const TargetRegisterInfo *TRI = nullptr;\n  BitVector Units;\n\npublic:\n  /// Constructs a new empty LiveRegUnits set.\n  LiveRegUnits() = default;\n\n  /// Constructs and initialize an empty LiveRegUnits set.\n  LiveRegUnits(const TargetRegisterInfo &TRI) {\n    init(TRI);\n  }\n\n  /// For a machine instruction \\p MI, adds all register units used in\n  /// \\p UsedRegUnits and defined or clobbered in \\p ModifiedRegUnits. This is\n  /// useful when walking over a range of instructions to track registers\n  /// used or defined seperately.\n  static void accumulateUsedDefed(const MachineInstr &MI,\n                                  LiveRegUnits &ModifiedRegUnits,\n                                  LiveRegUnits &UsedRegUnits,\n                                  const TargetRegisterInfo *TRI) {\n    for (ConstMIBundleOperands O(MI); O.isValid(); ++O) {\n      if (O->isRegMask())\n        ModifiedRegUnits.addRegsInMask(O->getRegMask());\n      if (!O->isReg())\n        continue;\n      Register Reg = O->getReg();\n      if (!Reg.isPhysical())\n        continue;\n      if (O->isDef()) {\n        // Some architectures (e.g. AArch64 XZR/WZR) have registers that are\n        // constant and may be used as destinations to indicate the generated\n        // value is discarded. No need to track such case as a def.\n        if (!TRI->isConstantPhysReg(Reg))\n          ModifiedRegUnits.addReg(Reg);\n      } else {\n        assert(O->isUse() && \"Reg operand not a def and not a use\");\n        UsedRegUnits.addReg(Reg);\n      }\n    }\n  }\n\n  /// Initialize and clear the set.\n  void init(const TargetRegisterInfo &TRI) {\n    this->TRI = &TRI;\n    Units.reset();\n    Units.resize(TRI.getNumRegUnits());\n  }\n\n  /// Clears the set.\n  void clear() { Units.reset(); }\n\n  /// Returns true if the set is empty.\n  bool empty() const { return Units.none(); }\n\n  /// Adds register units covered by physical register \\p Reg.\n  void addReg(MCPhysReg Reg) {\n    for (MCRegUnitIterator Unit(Reg, TRI); Unit.isValid(); ++Unit)\n      Units.set(*Unit);\n  }\n\n  /// Adds register units covered by physical register \\p Reg that are\n  /// part of the lanemask \\p Mask.\n  void addRegMasked(MCPhysReg Reg, LaneBitmask Mask) {\n    for (MCRegUnitMaskIterator Unit(Reg, TRI); Unit.isValid(); ++Unit) {\n      LaneBitmask UnitMask = (*Unit).second;\n      if (UnitMask.none() || (UnitMask & Mask).any())\n        Units.set((*Unit).first);\n    }\n  }\n\n  /// Removes all register units covered by physical register \\p Reg.\n  void removeReg(MCPhysReg Reg) {\n    for (MCRegUnitIterator Unit(Reg, TRI); Unit.isValid(); ++Unit)\n      Units.reset(*Unit);\n  }\n\n  /// Removes register units not preserved by the regmask \\p RegMask.\n  /// The regmask has the same format as the one in the RegMask machine operand.\n  void removeRegsNotPreserved(const uint32_t *RegMask);\n\n  /// Adds register units not preserved by the regmask \\p RegMask.\n  /// The regmask has the same format as the one in the RegMask machine operand.\n  void addRegsInMask(const uint32_t *RegMask);\n\n  /// Returns true if no part of physical register \\p Reg is live.\n  bool available(MCPhysReg Reg) const {\n    for (MCRegUnitIterator Unit(Reg, TRI); Unit.isValid(); ++Unit) {\n      if (Units.test(*Unit))\n        return false;\n    }\n    return true;\n  }\n\n  /// Updates liveness when stepping backwards over the instruction \\p MI.\n  /// This removes all register units defined or clobbered in \\p MI and then\n  /// adds the units used (as in use operands) in \\p MI.\n  void stepBackward(const MachineInstr &MI);\n\n  /// Adds all register units used, defined or clobbered in \\p MI.\n  /// This is useful when walking over a range of instruction to find registers\n  /// unused over the whole range.\n  void accumulate(const MachineInstr &MI);\n\n  /// Adds registers living out of block \\p MBB.\n  /// Live out registers are the union of the live-in registers of the successor\n  /// blocks and pristine registers. Live out registers of the end block are the\n  /// callee saved registers.\n  void addLiveOuts(const MachineBasicBlock &MBB);\n\n  /// Adds registers living into block \\p MBB.\n  void addLiveIns(const MachineBasicBlock &MBB);\n\n  /// Adds all register units marked in the bitvector \\p RegUnits.\n  void addUnits(const BitVector &RegUnits) {\n    Units |= RegUnits;\n  }\n  /// Removes all register units marked in the bitvector \\p RegUnits.\n  void removeUnits(const BitVector &RegUnits) {\n    Units.reset(RegUnits);\n  }\n  /// Return the internal bitvector representation of the set.\n  const BitVector &getBitVector() const {\n    return Units;\n  }\n\nprivate:\n  /// Adds pristine registers. Pristine registers are callee saved registers\n  /// that are unused in the function.\n  void addPristines(const MachineFunction &MF);\n};\n\n/// Returns an iterator range over all physical register and mask operands for\n/// \\p MI and bundled instructions. This also skips any debug operands.\ninline iterator_range<filter_iterator<\n    ConstMIBundleOperands, std::function<bool(const MachineOperand &)>>>\nphys_regs_and_masks(const MachineInstr &MI) {\n  std::function<bool(const MachineOperand &)> Pred =\n      [](const MachineOperand &MOP) {\n        return MOP.isRegMask() || (MOP.isReg() && !MOP.isDebug() &&\n                                   Register::isPhysicalRegister(MOP.getReg()));\n      };\n  return make_filter_range(const_mi_bundle_ops(MI), Pred);\n}\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_LIVEREGUNITS_H\n"}, "44": {"id": 44, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MIRFormatter.h", "content": "//===-- llvm/CodeGen/MIRFormatter.h -----------------------------*- C++ -*-===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is distributed under the University of Illinois Open Source\n// License. See LICENSE.TXT for details.\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains the declaration of the MIRFormatter class.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_MIRFORMATTER_H\n#define LLVM_CODEGEN_MIRFORMATTER_H\n\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/CodeGen/PseudoSourceValue.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include <cstdint>\n\nnamespace llvm {\n\nclass MachineFunction;\nclass MachineInstr;\nstruct PerFunctionMIParsingState;\nstruct SlotMapping;\n\n/// MIRFormater - Interface to format MIR operand based on target\nclass MIRFormatter {\npublic:\n  typedef function_ref<bool(StringRef::iterator Loc, const Twine &)>\n      ErrorCallbackType;\n\n  MIRFormatter() {}\n  virtual ~MIRFormatter() = default;\n\n  /// Implement target specific printing for machine operand immediate value, so\n  /// that we can have more meaningful mnemonic than a 64-bit integer. Passing\n  /// None to OpIdx means the index is unknown.\n  virtual void printImm(raw_ostream &OS, const MachineInstr &MI,\n                        Optional<unsigned> OpIdx, int64_t Imm) const {\n    OS << Imm;\n  }\n\n  /// Implement target specific parsing of immediate mnemonics. The mnemonic is\n  /// dot seperated strings.\n  virtual bool parseImmMnemonic(const unsigned OpCode, const unsigned OpIdx,\n                                StringRef Src, int64_t &Imm,\n                                ErrorCallbackType ErrorCallback) const {\n    llvm_unreachable(\"target did not implement parsing MIR immediate mnemonic\");\n  }\n\n  /// Implement target specific printing of target custom pseudo source value.\n  /// Default implementation is not necessarily the correct MIR serialization\n  /// format.\n  virtual void\n  printCustomPseudoSourceValue(raw_ostream &OS, ModuleSlotTracker &MST,\n                               const PseudoSourceValue &PSV) const {\n    PSV.printCustom(OS);\n  }\n\n  /// Implement target specific parsing of target custom pseudo source value.\n  virtual bool parseCustomPseudoSourceValue(\n      StringRef Src, MachineFunction &MF, PerFunctionMIParsingState &PFS,\n      const PseudoSourceValue *&PSV, ErrorCallbackType ErrorCallback) const {\n    llvm_unreachable(\n        \"target did not implement parsing MIR custom pseudo source value\");\n  }\n\n  /// Helper functions to print IR value as MIR serialization format which will\n  /// be useful for target specific printer, e.g. for printing IR value in\n  /// custom pseudo source value.\n  static void printIRValue(raw_ostream &OS, const Value &V,\n                           ModuleSlotTracker &MST);\n\n  /// Helper functions to parse IR value from MIR serialization format which\n  /// will be useful for target specific parser, e.g. for parsing IR value for\n  /// custom pseudo source value.\n  static bool parseIRValue(StringRef Src, MachineFunction &MF,\n                           PerFunctionMIParsingState &PFS, const Value *&V,\n                           ErrorCallbackType ErrorCallback);\n};\n\n} // end namespace llvm\n\n#endif\n"}, "48": {"id": 48, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "content": "//===-- llvm/CodeGen/MachineModuleInfo.h ------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// Collect meta information for a module.  This information should be in a\n// neutral form that can be used by different debugging and exception handling\n// schemes.\n//\n// The organization of information is primarily clustered around the source\n// compile units.  The main exception is source line correspondence where\n// inlining may interleave code from various compile units.\n//\n// The following information can be retrieved from the MachineModuleInfo.\n//\n//  -- Source directories - Directories are uniqued based on their canonical\n//     string and assigned a sequential numeric ID (base 1.)\n//  -- Source files - Files are also uniqued based on their name and directory\n//     ID.  A file ID is sequential number (base 1.)\n//  -- Source line correspondence - A vector of file ID, line#, column# triples.\n//     A DEBUG_LOCATION instruction is generated  by the DAG Legalizer\n//     corresponding to each entry in the source line list.  This allows a debug\n//     emitter to generate labels referenced by debug information tables.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_MACHINEMODULEINFO_H\n#define LLVM_CODEGEN_MACHINEMODULEINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/PointerIntPair.h\"\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/MC/MCContext.h\"\n#include \"llvm/MC/MCSymbol.h\"\n#include \"llvm/Pass.h\"\n#include <memory>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass BasicBlock;\nclass CallInst;\nclass Function;\nclass LLVMTargetMachine;\nclass MMIAddrLabelMap;\nclass MachineFunction;\nclass Module;\n\n//===----------------------------------------------------------------------===//\n/// This class can be derived from and used by targets to hold private\n/// target-specific information for each Module.  Objects of type are\n/// accessed/created with MachineModuleInfo::getObjFileInfo and destroyed when\n/// the MachineModuleInfo is destroyed.\n///\nclass MachineModuleInfoImpl {\npublic:\n  using StubValueTy = PointerIntPair<MCSymbol *, 1, bool>;\n  using SymbolListTy = std::vector<std::pair<MCSymbol *, StubValueTy>>;\n\n  virtual ~MachineModuleInfoImpl();\n\nprotected:\n  /// Return the entries from a DenseMap in a deterministic sorted orer.\n  /// Clears the map.\n  static SymbolListTy getSortedStubs(DenseMap<MCSymbol*, StubValueTy>&);\n};\n\n//===----------------------------------------------------------------------===//\n/// This class contains meta information specific to a module.  Queries can be\n/// made by different debugging and exception handling schemes and reformated\n/// for specific use.\n///\nclass MachineModuleInfo {\n  friend class MachineModuleInfoWrapperPass;\n  friend class MachineModuleAnalysis;\n\n  const LLVMTargetMachine &TM;\n\n  /// This is the MCContext used for the entire code generator.\n  MCContext Context;\n  // This is an external context, that if assigned, will be used instead of the\n  // internal context.\n  MCContext *ExternalContext = nullptr;\n\n  /// This is the LLVM Module being worked on.\n  const Module *TheModule;\n\n  /// This is the object-file-format-specific implementation of\n  /// MachineModuleInfoImpl, which lets targets accumulate whatever info they\n  /// want.\n  MachineModuleInfoImpl *ObjFileMMI;\n\n  /// \\name Exception Handling\n  /// \\{\n\n  /// Vector of all personality functions ever seen. Used to emit common EH\n  /// frames.\n  std::vector<const Function *> Personalities;\n\n  /// The current call site index being processed, if any. 0 if none.\n  unsigned CurCallSite;\n\n  /// \\}\n\n  /// This map keeps track of which symbol is being used for the specified\n  /// basic block's address of label.\n  MMIAddrLabelMap *AddrLabelSymbols;\n\n  // TODO: Ideally, what we'd like is to have a switch that allows emitting\n  // synchronous (precise at call-sites only) CFA into .eh_frame. However,\n  // even under this switch, we'd like .debug_frame to be precise when using\n  // -g. At this moment, there's no way to specify that some CFI directives\n  // go into .eh_frame only, while others go into .debug_frame only.\n\n  /// True if debugging information is available in this module.\n  bool DbgInfoAvailable;\n\n  /// True if this module is being built for windows/msvc, and uses floating\n  /// point.  This is used to emit an undefined reference to _fltused.\n  bool UsesMSVCFloatingPoint;\n\n  /// True if the module calls the __morestack function indirectly, as is\n  /// required under the large code model on x86. This is used to emit\n  /// a definition of a symbol, __morestack_addr, containing the address. See\n  /// comments in lib/Target/X86/X86FrameLowering.cpp for more details.\n  bool UsesMorestackAddr;\n\n  /// True if the module contains split-stack functions. This is used to\n  /// emit .note.GNU-split-stack section as required by the linker for\n  /// special handling split-stack function calling no-split-stack function.\n  bool HasSplitStack;\n\n  /// True if the module contains no-split-stack functions. This is used to\n  /// emit .note.GNU-no-split-stack section when it also contains split-stack\n  /// functions.\n  bool HasNosplitStack;\n\n  /// Maps IR Functions to their corresponding MachineFunctions.\n  DenseMap<const Function*, std::unique_ptr<MachineFunction>> MachineFunctions;\n  /// Next unique number available for a MachineFunction.\n  unsigned NextFnNum = 0;\n  const Function *LastRequest = nullptr; ///< Used for shortcut/cache.\n  MachineFunction *LastResult = nullptr; ///< Used for shortcut/cache.\n\n  MachineModuleInfo &operator=(MachineModuleInfo &&MMII) = delete;\n\npublic:\n  explicit MachineModuleInfo(const LLVMTargetMachine *TM = nullptr);\n\n  explicit MachineModuleInfo(const LLVMTargetMachine *TM,\n                             MCContext *ExtContext);\n\n  MachineModuleInfo(MachineModuleInfo &&MMII);\n\n  ~MachineModuleInfo();\n\n  void initialize();\n  void finalize();\n\n  const LLVMTargetMachine &getTarget() const { return TM; }\n\n  const MCContext &getContext() const {\n    return ExternalContext ? *ExternalContext : Context;\n  }\n  MCContext &getContext() {\n    return ExternalContext ? *ExternalContext : Context;\n  }\n\n  const Module *getModule() const { return TheModule; }\n\n  /// Returns the MachineFunction constructed for the IR function \\p F.\n  /// Creates a new MachineFunction if none exists yet.\n  MachineFunction &getOrCreateMachineFunction(Function &F);\n\n  /// \\brief Returns the MachineFunction associated to IR function \\p F if there\n  /// is one, otherwise nullptr.\n  MachineFunction *getMachineFunction(const Function &F) const;\n\n  /// Delete the MachineFunction \\p MF and reset the link in the IR Function to\n  /// Machine Function map.\n  void deleteMachineFunctionFor(Function &F);\n\n  /// Keep track of various per-function pieces of information for backends\n  /// that would like to do so.\n  template<typename Ty>\n  Ty &getObjFileInfo() {\n    if (ObjFileMMI == nullptr)\n      ObjFileMMI = new Ty(*this);\n    return *static_cast<Ty*>(ObjFileMMI);\n  }\n\n  template<typename Ty>\n  const Ty &getObjFileInfo() const {\n    return const_cast<MachineModuleInfo*>(this)->getObjFileInfo<Ty>();\n  }\n\n  /// Returns true if valid debug info is present.\n  bool hasDebugInfo() const { return DbgInfoAvailable; }\n  void setDebugInfoAvailability(bool avail) { DbgInfoAvailable = avail; }\n\n  bool usesMSVCFloatingPoint() const { return UsesMSVCFloatingPoint; }\n\n  void setUsesMSVCFloatingPoint(bool b) { UsesMSVCFloatingPoint = b; }\n\n  bool usesMorestackAddr() const {\n    return UsesMorestackAddr;\n  }\n\n  void setUsesMorestackAddr(bool b) {\n    UsesMorestackAddr = b;\n  }\n\n  bool hasSplitStack() const {\n    return HasSplitStack;\n  }\n\n  void setHasSplitStack(bool b) {\n    HasSplitStack = b;\n  }\n\n  bool hasNosplitStack() const {\n    return HasNosplitStack;\n  }\n\n  void setHasNosplitStack(bool b) {\n    HasNosplitStack = b;\n  }\n\n  /// Return the symbol to be used for the specified basic block when its\n  /// address is taken.  This cannot be its normal LBB label because the block\n  /// may be accessed outside its containing function.\n  MCSymbol *getAddrLabelSymbol(const BasicBlock *BB) {\n    return getAddrLabelSymbolToEmit(BB).front();\n  }\n\n  /// Return the symbol to be used for the specified basic block when its\n  /// address is taken.  If other blocks were RAUW'd to this one, we may have\n  /// to emit them as well, return the whole set.\n  ArrayRef<MCSymbol *> getAddrLabelSymbolToEmit(const BasicBlock *BB);\n\n  /// \\name Exception Handling\n  /// \\{\n\n  /// Set the call site currently being processed.\n  void setCurrentCallSite(unsigned Site) { CurCallSite = Site; }\n\n  /// Get the call site currently being processed, if any.  return zero if\n  /// none.\n  unsigned getCurrentCallSite() { return CurCallSite; }\n\n  /// Provide the personality function for the exception information.\n  void addPersonality(const Function *Personality);\n\n  /// Return array of personality functions ever seen.\n  const std::vector<const Function *>& getPersonalities() const {\n    return Personalities;\n  }\n  /// \\}\n\n  // MMI owes MCContext. It should never be invalidated.\n  bool invalidate(Module &, const PreservedAnalyses &,\n                  ModuleAnalysisManager::Invalidator &) {\n    return false;\n  }\n}; // End class MachineModuleInfo\n\nclass MachineModuleInfoWrapperPass : public ImmutablePass {\n  MachineModuleInfo MMI;\n\npublic:\n  static char ID; // Pass identification, replacement for typeid\n  explicit MachineModuleInfoWrapperPass(const LLVMTargetMachine *TM = nullptr);\n\n  explicit MachineModuleInfoWrapperPass(const LLVMTargetMachine *TM,\n                                        MCContext *ExtContext);\n\n  // Initialization and Finalization\n  bool doInitialization(Module &) override;\n  bool doFinalization(Module &) override;\n\n  MachineModuleInfo &getMMI() { return MMI; }\n  const MachineModuleInfo &getMMI() const { return MMI; }\n};\n\n/// An analysis that produces \\c MachineInfo for a module.\nclass MachineModuleAnalysis : public AnalysisInfoMixin<MachineModuleAnalysis> {\n  friend AnalysisInfoMixin<MachineModuleAnalysis>;\n  static AnalysisKey Key;\n\n  const LLVMTargetMachine *TM;\n\npublic:\n  /// Provide the result type for this analysis pass.\n  using Result = MachineModuleInfo;\n\n  MachineModuleAnalysis(const LLVMTargetMachine *TM) : TM(TM) {}\n\n  /// Run the analysis pass and produce machine module information.\n  MachineModuleInfo run(Module &M, ModuleAnalysisManager &);\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_MACHINEMODULEINFO_H\n"}, "50": {"id": 50, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineOutliner.h", "content": "//===---- MachineOutliner.h - Outliner data structures ------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n///\n/// \\file\n/// Contains all data structures shared between the outliner implemented in\n/// MachineOutliner.cpp and target implementations of the outliner.\n///\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_MACHINEOUTLINER_H\n#define LLVM_CODEGEN_MACHINEOUTLINER_H\n\n#include \"llvm/CodeGen/LivePhysRegs.h\"\n#include \"llvm/CodeGen/LiveRegUnits.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineRegisterInfo.h\"\n#include \"llvm/CodeGen/TargetRegisterInfo.h\"\n\nnamespace llvm {\nnamespace outliner {\n\n/// Represents how an instruction should be mapped by the outliner.\n/// \\p Legal instructions are those which are safe to outline.\n/// \\p LegalTerminator instructions are safe to outline, but only as the\n/// last instruction in a sequence.\n/// \\p Illegal instructions are those which cannot be outlined.\n/// \\p Invisible instructions are instructions which can be outlined, but\n/// shouldn't actually impact the outlining result.\nenum InstrType { Legal, LegalTerminator, Illegal, Invisible };\n\n/// An individual sequence of instructions to be replaced with a call to\n/// an outlined function.\nstruct Candidate {\nprivate:\n  /// The start index of this \\p Candidate in the instruction list.\n  unsigned StartIdx = 0;\n\n  /// The number of instructions in this \\p Candidate.\n  unsigned Len = 0;\n\n  // The first instruction in this \\p Candidate.\n  MachineBasicBlock::iterator FirstInst;\n\n  // The last instruction in this \\p Candidate.\n  MachineBasicBlock::iterator LastInst;\n\n  // The basic block that contains this Candidate.\n  MachineBasicBlock *MBB = nullptr;\n\n  /// Cost of calling an outlined function from this point as defined by the\n  /// target.\n  unsigned CallOverhead = 0;\n\npublic:\n  /// The index of this \\p Candidate's \\p OutlinedFunction in the list of\n  /// \\p OutlinedFunctions.\n  unsigned FunctionIdx = 0;\n\n  /// Identifier denoting the instructions to emit to call an outlined function\n  /// from this point. Defined by the target.\n  unsigned CallConstructionID = 0;\n\n  /// Contains physical register liveness information for the MBB containing\n  /// this \\p Candidate.\n  ///\n  /// This is optionally used by the target to calculate more fine-grained\n  /// cost model information.\n  LiveRegUnits LRU;\n\n  /// Contains the accumulated register liveness information for the\n  /// instructions in this \\p Candidate.\n  ///\n  /// This is optionally used by the target to determine which registers have\n  /// been used across the sequence.\n  LiveRegUnits UsedInSequence;\n\n  /// Target-specific flags for this Candidate's MBB.\n  unsigned Flags = 0x0;\n\n  /// True if initLRU has been called on this Candidate.\n  bool LRUWasSet = false;\n\n  /// Return the number of instructions in this Candidate.\n  unsigned getLength() const { return Len; }\n\n  /// Return the start index of this candidate.\n  unsigned getStartIdx() const { return StartIdx; }\n\n  /// Return the end index of this candidate.\n  unsigned getEndIdx() const { return StartIdx + Len - 1; }\n\n  /// Set the CallConstructionID and CallOverhead of this candidate to CID and\n  /// CO respectively.\n  void setCallInfo(unsigned CID, unsigned CO) {\n    CallConstructionID = CID;\n    CallOverhead = CO;\n  }\n\n  /// Returns the call overhead of this candidate if it is in the list.\n  unsigned getCallOverhead() const { return CallOverhead; }\n\n  MachineBasicBlock::iterator &front() { return FirstInst; }\n  MachineBasicBlock::iterator &back() { return LastInst; }\n  MachineFunction *getMF() const { return MBB->getParent(); }\n  MachineBasicBlock *getMBB() const { return MBB; }\n\n  /// The number of instructions that would be saved by outlining every\n  /// candidate of this type.\n  ///\n  /// This is a fixed value which is not updated during the candidate pruning\n  /// process. It is only used for deciding which candidate to keep if two\n  /// candidates overlap. The true benefit is stored in the OutlinedFunction\n  /// for some given candidate.\n  unsigned Benefit = 0;\n\n  Candidate(unsigned StartIdx, unsigned Len,\n            MachineBasicBlock::iterator &FirstInst,\n            MachineBasicBlock::iterator &LastInst, MachineBasicBlock *MBB,\n            unsigned FunctionIdx, unsigned Flags)\n      : StartIdx(StartIdx), Len(Len), FirstInst(FirstInst), LastInst(LastInst),\n        MBB(MBB), FunctionIdx(FunctionIdx), Flags(Flags) {}\n  Candidate() {}\n\n  /// Used to ensure that \\p Candidates are outlined in an order that\n  /// preserves the start and end indices of other \\p Candidates.\n  bool operator<(const Candidate &RHS) const {\n    return getStartIdx() > RHS.getStartIdx();\n  }\n\n  /// Compute the registers that are live across this Candidate.\n  /// Used by targets that need this information for cost model calculation.\n  /// If a target does not need this information, then this should not be\n  /// called.\n  void initLRU(const TargetRegisterInfo &TRI) {\n    assert(MBB->getParent()->getRegInfo().tracksLiveness() &&\n           \"Candidate's Machine Function must track liveness\");\n    // Only initialize once.\n    if (LRUWasSet)\n      return;\n    LRUWasSet = true;\n    LRU.init(TRI);\n    LRU.addLiveOuts(*MBB);\n\n    // Compute liveness from the end of the block up to the beginning of the\n    // outlining candidate.\n    std::for_each(MBB->rbegin(), (MachineBasicBlock::reverse_iterator)front(),\n                  [this](MachineInstr &MI) { LRU.stepBackward(MI); });\n\n    // Walk over the sequence itself and figure out which registers were used\n    // in the sequence.\n    UsedInSequence.init(TRI);\n    std::for_each(front(), std::next(back()),\n                  [this](MachineInstr &MI) { UsedInSequence.accumulate(MI); });\n  }\n};\n\n/// The information necessary to create an outlined function for some\n/// class of candidate.\nstruct OutlinedFunction {\n\npublic:\n  std::vector<Candidate> Candidates;\n\n  /// The actual outlined function created.\n  /// This is initialized after we go through and create the actual function.\n  MachineFunction *MF = nullptr;\n\n  /// Represents the size of a sequence in bytes. (Some instructions vary\n  /// widely in size, so just counting the instructions isn't very useful.)\n  unsigned SequenceSize = 0;\n\n  /// Target-defined overhead of constructing a frame for this function.\n  unsigned FrameOverhead = 0;\n\n  /// Target-defined identifier for constructing a frame for this function.\n  unsigned FrameConstructionID = 0;\n\n  /// Return the number of candidates for this \\p OutlinedFunction.\n  unsigned getOccurrenceCount() const { return Candidates.size(); }\n\n  /// Return the number of bytes it would take to outline this\n  /// function.\n  unsigned getOutliningCost() const {\n    unsigned CallOverhead = 0;\n    for (const Candidate &C : Candidates)\n      CallOverhead += C.getCallOverhead();\n    return CallOverhead + SequenceSize + FrameOverhead;\n  }\n\n  /// Return the size in bytes of the unoutlined sequences.\n  unsigned getNotOutlinedCost() const {\n    return getOccurrenceCount() * SequenceSize;\n  }\n\n  /// Return the number of instructions that would be saved by outlining\n  /// this function.\n  unsigned getBenefit() const {\n    unsigned NotOutlinedCost = getNotOutlinedCost();\n    unsigned OutlinedCost = getOutliningCost();\n    return (NotOutlinedCost < OutlinedCost) ? 0\n                                            : NotOutlinedCost - OutlinedCost;\n  }\n\n  /// Return the number of instructions in this sequence.\n  unsigned getNumInstrs() const { return Candidates[0].getLength(); }\n\n  OutlinedFunction(std::vector<Candidate> &Candidates, unsigned SequenceSize,\n                   unsigned FrameOverhead, unsigned FrameConstructionID)\n      : Candidates(Candidates), SequenceSize(SequenceSize),\n        FrameOverhead(FrameOverhead), FrameConstructionID(FrameConstructionID) {\n    const unsigned B = getBenefit();\n    for (Candidate &C : Candidates)\n      C.Benefit = B;\n  }\n\n  OutlinedFunction() {}\n};\n} // namespace outliner\n} // namespace llvm\n\n#endif\n"}, "52": {"id": 52, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineRegisterInfo.h", "content": "//===- llvm/CodeGen/MachineRegisterInfo.h -----------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the MachineRegisterInfo class.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_MACHINEREGISTERINFO_H\n#define LLVM_CODEGEN_MACHINEREGISTERINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/BitVector.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/IndexedMap.h\"\n#include \"llvm/ADT/PointerUnion.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringSet.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/CodeGen/GlobalISel/RegisterBank.h\"\n#include \"llvm/CodeGen/LowLevelType.h\"\n#include \"llvm/CodeGen/MachineBasicBlock.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineInstrBundle.h\"\n#include \"llvm/CodeGen/MachineOperand.h\"\n#include \"llvm/CodeGen/TargetRegisterInfo.h\"\n#include \"llvm/CodeGen/TargetSubtargetInfo.h\"\n#include \"llvm/MC/LaneBitmask.h\"\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <iterator>\n#include <memory>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass PSetIterator;\n\n/// Convenient type to represent either a register class or a register bank.\nusing RegClassOrRegBank =\n    PointerUnion<const TargetRegisterClass *, const RegisterBank *>;\n\n/// MachineRegisterInfo - Keep track of information for virtual and physical\n/// registers, including vreg register classes, use/def chains for registers,\n/// etc.\nclass MachineRegisterInfo {\npublic:\n  class Delegate {\n    virtual void anchor();\n\n  public:\n    virtual ~Delegate() = default;\n\n    virtual void MRI_NoteNewVirtualRegister(Register Reg) = 0;\n  };\n\nprivate:\n  MachineFunction *MF;\n  Delegate *TheDelegate = nullptr;\n\n  /// True if subregister liveness is tracked.\n  const bool TracksSubRegLiveness;\n\n  /// VRegInfo - Information we keep for each virtual register.\n  ///\n  /// Each element in this list contains the register class of the vreg and the\n  /// start of the use/def list for the register.\n  IndexedMap<std::pair<RegClassOrRegBank, MachineOperand *>,\n             VirtReg2IndexFunctor>\n      VRegInfo;\n\n  /// Map for recovering vreg name from vreg number.\n  /// This map is used by the MIR Printer.\n  IndexedMap<std::string, VirtReg2IndexFunctor> VReg2Name;\n\n  /// StringSet that is used to unique vreg names.\n  StringSet<> VRegNames;\n\n  /// The flag is true upon \\p UpdatedCSRs initialization\n  /// and false otherwise.\n  bool IsUpdatedCSRsInitialized;\n\n  /// Contains the updated callee saved register list.\n  /// As opposed to the static list defined in register info,\n  /// all registers that were disabled are removed from the list.\n  SmallVector<MCPhysReg, 16> UpdatedCSRs;\n\n  /// RegAllocHints - This vector records register allocation hints for\n  /// virtual registers. For each virtual register, it keeps a pair of hint\n  /// type and hints vector making up the allocation hints. Only the first\n  /// hint may be target specific, and in that case this is reflected by the\n  /// first member of the pair being non-zero. If the hinted register is\n  /// virtual, it means the allocator should prefer the physical register\n  /// allocated to it if any.\n  IndexedMap<std::pair<Register, SmallVector<Register, 4>>,\n             VirtReg2IndexFunctor> RegAllocHints;\n\n  /// PhysRegUseDefLists - This is an array of the head of the use/def list for\n  /// physical registers.\n  std::unique_ptr<MachineOperand *[]> PhysRegUseDefLists;\n\n  /// getRegUseDefListHead - Return the head pointer for the register use/def\n  /// list for the specified virtual or physical register.\n  MachineOperand *&getRegUseDefListHead(Register RegNo) {\n    if (RegNo.isVirtual())\n      return VRegInfo[RegNo.id()].second;\n    return PhysRegUseDefLists[RegNo.id()];\n  }\n\n  MachineOperand *getRegUseDefListHead(Register RegNo) const {\n    if (RegNo.isVirtual())\n      return VRegInfo[RegNo.id()].second;\n    return PhysRegUseDefLists[RegNo.id()];\n  }\n\n  /// Get the next element in the use-def chain.\n  static MachineOperand *getNextOperandForReg(const MachineOperand *MO) {\n    assert(MO && MO->isReg() && \"This is not a register operand!\");\n    return MO->Contents.Reg.Next;\n  }\n\n  /// UsedPhysRegMask - Additional used physregs including aliases.\n  /// This bit vector represents all the registers clobbered by function calls.\n  BitVector UsedPhysRegMask;\n\n  /// ReservedRegs - This is a bit vector of reserved registers.  The target\n  /// may change its mind about which registers should be reserved.  This\n  /// vector is the frozen set of reserved registers when register allocation\n  /// started.\n  BitVector ReservedRegs;\n\n  using VRegToTypeMap = IndexedMap<LLT, VirtReg2IndexFunctor>;\n  /// Map generic virtual registers to their low-level type.\n  VRegToTypeMap VRegToType;\n\n  /// Keep track of the physical registers that are live in to the function.\n  /// Live in values are typically arguments in registers.  LiveIn values are\n  /// allowed to have virtual registers associated with them, stored in the\n  /// second element.\n  std::vector<std::pair<MCRegister, Register>> LiveIns;\n\npublic:\n  explicit MachineRegisterInfo(MachineFunction *MF);\n  MachineRegisterInfo(const MachineRegisterInfo &) = delete;\n  MachineRegisterInfo &operator=(const MachineRegisterInfo &) = delete;\n\n  const TargetRegisterInfo *getTargetRegisterInfo() const {\n    return MF->getSubtarget().getRegisterInfo();\n  }\n\n  void resetDelegate(Delegate *delegate) {\n    // Ensure another delegate does not take over unless the current\n    // delegate first unattaches itself. If we ever need to multicast\n    // notifications, we will need to change to using a list.\n    assert(TheDelegate == delegate &&\n           \"Only the current delegate can perform reset!\");\n    TheDelegate = nullptr;\n  }\n\n  void setDelegate(Delegate *delegate) {\n    assert(delegate && !TheDelegate &&\n           \"Attempted to set delegate to null, or to change it without \"\n           \"first resetting it!\");\n\n    TheDelegate = delegate;\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Function State\n  //===--------------------------------------------------------------------===//\n\n  // isSSA - Returns true when the machine function is in SSA form. Early\n  // passes require the machine function to be in SSA form where every virtual\n  // register has a single defining instruction.\n  //\n  // The TwoAddressInstructionPass and PHIElimination passes take the machine\n  // function out of SSA form when they introduce multiple defs per virtual\n  // register.\n  bool isSSA() const {\n    return MF->getProperties().hasProperty(\n        MachineFunctionProperties::Property::IsSSA);\n  }\n\n  // leaveSSA - Indicates that the machine function is no longer in SSA form.\n  void leaveSSA() {\n    MF->getProperties().reset(MachineFunctionProperties::Property::IsSSA);\n  }\n\n  /// tracksLiveness - Returns true when tracking register liveness accurately.\n  /// (see MachineFUnctionProperties::Property description for details)\n  bool tracksLiveness() const {\n    return MF->getProperties().hasProperty(\n        MachineFunctionProperties::Property::TracksLiveness);\n  }\n\n  /// invalidateLiveness - Indicates that register liveness is no longer being\n  /// tracked accurately.\n  ///\n  /// This should be called by late passes that invalidate the liveness\n  /// information.\n  void invalidateLiveness() {\n    MF->getProperties().reset(\n        MachineFunctionProperties::Property::TracksLiveness);\n  }\n\n  /// Returns true if liveness for register class @p RC should be tracked at\n  /// the subregister level.\n  bool shouldTrackSubRegLiveness(const TargetRegisterClass &RC) const {\n    return subRegLivenessEnabled() && RC.HasDisjunctSubRegs;\n  }\n  bool shouldTrackSubRegLiveness(Register VReg) const {\n    assert(VReg.isVirtual() && \"Must pass a VReg\");\n    return shouldTrackSubRegLiveness(*getRegClass(VReg));\n  }\n  bool subRegLivenessEnabled() const {\n    return TracksSubRegLiveness;\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Register Info\n  //===--------------------------------------------------------------------===//\n\n  /// Returns true if the updated CSR list was initialized and false otherwise.\n  bool isUpdatedCSRsInitialized() const { return IsUpdatedCSRsInitialized; }\n\n  /// Disables the register from the list of CSRs.\n  /// I.e. the register will not appear as part of the CSR mask.\n  /// \\see UpdatedCalleeSavedRegs.\n  void disableCalleeSavedRegister(MCRegister Reg);\n\n  /// Returns list of callee saved registers.\n  /// The function returns the updated CSR list (after taking into account\n  /// registers that are disabled from the CSR list).\n  const MCPhysReg *getCalleeSavedRegs() const;\n\n  /// Sets the updated Callee Saved Registers list.\n  /// Notice that it will override ant previously disabled/saved CSRs.\n  void setCalleeSavedRegs(ArrayRef<MCPhysReg> CSRs);\n\n  // Strictly for use by MachineInstr.cpp.\n  void addRegOperandToUseList(MachineOperand *MO);\n\n  // Strictly for use by MachineInstr.cpp.\n  void removeRegOperandFromUseList(MachineOperand *MO);\n\n  // Strictly for use by MachineInstr.cpp.\n  void moveOperands(MachineOperand *Dst, MachineOperand *Src, unsigned NumOps);\n\n  /// Verify the sanity of the use list for Reg.\n  void verifyUseList(Register Reg) const;\n\n  /// Verify the use list of all registers.\n  void verifyUseLists() const;\n\n  /// reg_begin/reg_end - Provide iteration support to walk over all definitions\n  /// and uses of a register within the MachineFunction that corresponds to this\n  /// MachineRegisterInfo object.\n  template<bool Uses, bool Defs, bool SkipDebug,\n           bool ByOperand, bool ByInstr, bool ByBundle>\n  class defusechain_iterator;\n  template<bool Uses, bool Defs, bool SkipDebug,\n           bool ByOperand, bool ByInstr, bool ByBundle>\n  class defusechain_instr_iterator;\n\n  // Make it a friend so it can access getNextOperandForReg().\n  template<bool, bool, bool, bool, bool, bool>\n    friend class defusechain_iterator;\n  template<bool, bool, bool, bool, bool, bool>\n    friend class defusechain_instr_iterator;\n\n  /// reg_iterator/reg_begin/reg_end - Walk all defs and uses of the specified\n  /// register.\n  using reg_iterator =\n      defusechain_iterator<true, true, false, true, false, false>;\n  reg_iterator reg_begin(Register RegNo) const {\n    return reg_iterator(getRegUseDefListHead(RegNo));\n  }\n  static reg_iterator reg_end() { return reg_iterator(nullptr); }\n\n  inline iterator_range<reg_iterator> reg_operands(Register Reg) const {\n    return make_range(reg_begin(Reg), reg_end());\n  }\n\n  /// reg_instr_iterator/reg_instr_begin/reg_instr_end - Walk all defs and uses\n  /// of the specified register, stepping by MachineInstr.\n  using reg_instr_iterator =\n      defusechain_instr_iterator<true, true, false, false, true, false>;\n  reg_instr_iterator reg_instr_begin(Register RegNo) const {\n    return reg_instr_iterator(getRegUseDefListHead(RegNo));\n  }\n  static reg_instr_iterator reg_instr_end() {\n    return reg_instr_iterator(nullptr);\n  }\n\n  inline iterator_range<reg_instr_iterator>\n  reg_instructions(Register Reg) const {\n    return make_range(reg_instr_begin(Reg), reg_instr_end());\n  }\n\n  /// reg_bundle_iterator/reg_bundle_begin/reg_bundle_end - Walk all defs and uses\n  /// of the specified register, stepping by bundle.\n  using reg_bundle_iterator =\n      defusechain_instr_iterator<true, true, false, false, false, true>;\n  reg_bundle_iterator reg_bundle_begin(Register RegNo) const {\n    return reg_bundle_iterator(getRegUseDefListHead(RegNo));\n  }\n  static reg_bundle_iterator reg_bundle_end() {\n    return reg_bundle_iterator(nullptr);\n  }\n\n  inline iterator_range<reg_bundle_iterator> reg_bundles(Register Reg) const {\n    return make_range(reg_bundle_begin(Reg), reg_bundle_end());\n  }\n\n  /// reg_empty - Return true if there are no instructions using or defining the\n  /// specified register (it may be live-in).\n  bool reg_empty(Register RegNo) const { return reg_begin(RegNo) == reg_end(); }\n\n  /// reg_nodbg_iterator/reg_nodbg_begin/reg_nodbg_end - Walk all defs and uses\n  /// of the specified register, skipping those marked as Debug.\n  using reg_nodbg_iterator =\n      defusechain_iterator<true, true, true, true, false, false>;\n  reg_nodbg_iterator reg_nodbg_begin(Register RegNo) const {\n    return reg_nodbg_iterator(getRegUseDefListHead(RegNo));\n  }\n  static reg_nodbg_iterator reg_nodbg_end() {\n    return reg_nodbg_iterator(nullptr);\n  }\n\n  inline iterator_range<reg_nodbg_iterator>\n  reg_nodbg_operands(Register Reg) const {\n    return make_range(reg_nodbg_begin(Reg), reg_nodbg_end());\n  }\n\n  /// reg_instr_nodbg_iterator/reg_instr_nodbg_begin/reg_instr_nodbg_end - Walk\n  /// all defs and uses of the specified register, stepping by MachineInstr,\n  /// skipping those marked as Debug.\n  using reg_instr_nodbg_iterator =\n      defusechain_instr_iterator<true, true, true, false, true, false>;\n  reg_instr_nodbg_iterator reg_instr_nodbg_begin(Register RegNo) const {\n    return reg_instr_nodbg_iterator(getRegUseDefListHead(RegNo));\n  }\n  static reg_instr_nodbg_iterator reg_instr_nodbg_end() {\n    return reg_instr_nodbg_iterator(nullptr);\n  }\n\n  inline iterator_range<reg_instr_nodbg_iterator>\n  reg_nodbg_instructions(Register Reg) const {\n    return make_range(reg_instr_nodbg_begin(Reg), reg_instr_nodbg_end());\n  }\n\n  /// reg_bundle_nodbg_iterator/reg_bundle_nodbg_begin/reg_bundle_nodbg_end - Walk\n  /// all defs and uses of the specified register, stepping by bundle,\n  /// skipping those marked as Debug.\n  using reg_bundle_nodbg_iterator =\n      defusechain_instr_iterator<true, true, true, false, false, true>;\n  reg_bundle_nodbg_iterator reg_bundle_nodbg_begin(Register RegNo) const {\n    return reg_bundle_nodbg_iterator(getRegUseDefListHead(RegNo));\n  }\n  static reg_bundle_nodbg_iterator reg_bundle_nodbg_end() {\n    return reg_bundle_nodbg_iterator(nullptr);\n  }\n\n  inline iterator_range<reg_bundle_nodbg_iterator>\n  reg_nodbg_bundles(Register Reg) const {\n    return make_range(reg_bundle_nodbg_begin(Reg), reg_bundle_nodbg_end());\n  }\n\n  /// reg_nodbg_empty - Return true if the only instructions using or defining\n  /// Reg are Debug instructions.\n  bool reg_nodbg_empty(Register RegNo) const {\n    return reg_nodbg_begin(RegNo) == reg_nodbg_end();\n  }\n\n  /// def_iterator/def_begin/def_end - Walk all defs of the specified register.\n  using def_iterator =\n      defusechain_iterator<false, true, false, true, false, false>;\n  def_iterator def_begin(Register RegNo) const {\n    return def_iterator(getRegUseDefListHead(RegNo));\n  }\n  static def_iterator def_end() { return def_iterator(nullptr); }\n\n  inline iterator_range<def_iterator> def_operands(Register Reg) const {\n    return make_range(def_begin(Reg), def_end());\n  }\n\n  /// def_instr_iterator/def_instr_begin/def_instr_end - Walk all defs of the\n  /// specified register, stepping by MachineInst.\n  using def_instr_iterator =\n      defusechain_instr_iterator<false, true, false, false, true, false>;\n  def_instr_iterator def_instr_begin(Register RegNo) const {\n    return def_instr_iterator(getRegUseDefListHead(RegNo));\n  }\n  static def_instr_iterator def_instr_end() {\n    return def_instr_iterator(nullptr);\n  }\n\n  inline iterator_range<def_instr_iterator>\n  def_instructions(Register Reg) const {\n    return make_range(def_instr_begin(Reg), def_instr_end());\n  }\n\n  /// def_bundle_iterator/def_bundle_begin/def_bundle_end - Walk all defs of the\n  /// specified register, stepping by bundle.\n  using def_bundle_iterator =\n      defusechain_instr_iterator<false, true, false, false, false, true>;\n  def_bundle_iterator def_bundle_begin(Register RegNo) const {\n    return def_bundle_iterator(getRegUseDefListHead(RegNo));\n  }\n  static def_bundle_iterator def_bundle_end() {\n    return def_bundle_iterator(nullptr);\n  }\n\n  inline iterator_range<def_bundle_iterator> def_bundles(Register Reg) const {\n    return make_range(def_bundle_begin(Reg), def_bundle_end());\n  }\n\n  /// def_empty - Return true if there are no instructions defining the\n  /// specified register (it may be live-in).\n  bool def_empty(Register RegNo) const { return def_begin(RegNo) == def_end(); }\n\n  StringRef getVRegName(Register Reg) const {\n    return VReg2Name.inBounds(Reg) ? StringRef(VReg2Name[Reg]) : \"\";\n  }\n\n  void insertVRegByName(StringRef Name, Register Reg) {\n    assert((Name.empty() || VRegNames.find(Name) == VRegNames.end()) &&\n           \"Named VRegs Must be Unique.\");\n    if (!Name.empty()) {\n      VRegNames.insert(Name);\n      VReg2Name.grow(Reg);\n      VReg2Name[Reg] = Name.str();\n    }\n  }\n\n  /// Return true if there is exactly one operand defining the specified\n  /// register.\n  bool hasOneDef(Register RegNo) const {\n    return hasSingleElement(def_operands(RegNo));\n  }\n\n  /// Returns the defining operand if there is exactly one operand defining the\n  /// specified register, otherwise nullptr.\n  MachineOperand *getOneDef(Register Reg) const {\n    def_iterator DI = def_begin(Reg);\n    if (DI == def_end()) // No defs.\n      return nullptr;\n\n    def_iterator OneDef = DI;\n    if (++DI == def_end())\n      return &*OneDef;\n    return nullptr; // Multiple defs.\n  }\n\n  /// use_iterator/use_begin/use_end - Walk all uses of the specified register.\n  using use_iterator =\n      defusechain_iterator<true, false, false, true, false, false>;\n  use_iterator use_begin(Register RegNo) const {\n    return use_iterator(getRegUseDefListHead(RegNo));\n  }\n  static use_iterator use_end() { return use_iterator(nullptr); }\n\n  inline iterator_range<use_iterator> use_operands(Register Reg) const {\n    return make_range(use_begin(Reg), use_end());\n  }\n\n  /// use_instr_iterator/use_instr_begin/use_instr_end - Walk all uses of the\n  /// specified register, stepping by MachineInstr.\n  using use_instr_iterator =\n      defusechain_instr_iterator<true, false, false, false, true, false>;\n  use_instr_iterator use_instr_begin(Register RegNo) const {\n    return use_instr_iterator(getRegUseDefListHead(RegNo));\n  }\n  static use_instr_iterator use_instr_end() {\n    return use_instr_iterator(nullptr);\n  }\n\n  inline iterator_range<use_instr_iterator>\n  use_instructions(Register Reg) const {\n    return make_range(use_instr_begin(Reg), use_instr_end());\n  }\n\n  /// use_bundle_iterator/use_bundle_begin/use_bundle_end - Walk all uses of the\n  /// specified register, stepping by bundle.\n  using use_bundle_iterator =\n      defusechain_instr_iterator<true, false, false, false, false, true>;\n  use_bundle_iterator use_bundle_begin(Register RegNo) const {\n    return use_bundle_iterator(getRegUseDefListHead(RegNo));\n  }\n  static use_bundle_iterator use_bundle_end() {\n    return use_bundle_iterator(nullptr);\n  }\n\n  inline iterator_range<use_bundle_iterator> use_bundles(Register Reg) const {\n    return make_range(use_bundle_begin(Reg), use_bundle_end());\n  }\n\n  /// use_empty - Return true if there are no instructions using the specified\n  /// register.\n  bool use_empty(Register RegNo) const { return use_begin(RegNo) == use_end(); }\n\n  /// hasOneUse - Return true if there is exactly one instruction using the\n  /// specified register.\n  bool hasOneUse(Register RegNo) const {\n    return hasSingleElement(use_operands(RegNo));\n  }\n\n  /// use_nodbg_iterator/use_nodbg_begin/use_nodbg_end - Walk all uses of the\n  /// specified register, skipping those marked as Debug.\n  using use_nodbg_iterator =\n      defusechain_iterator<true, false, true, true, false, false>;\n  use_nodbg_iterator use_nodbg_begin(Register RegNo) const {\n    return use_nodbg_iterator(getRegUseDefListHead(RegNo));\n  }\n  static use_nodbg_iterator use_nodbg_end() {\n    return use_nodbg_iterator(nullptr);\n  }\n\n  inline iterator_range<use_nodbg_iterator>\n  use_nodbg_operands(Register Reg) const {\n    return make_range(use_nodbg_begin(Reg), use_nodbg_end());\n  }\n\n  /// use_instr_nodbg_iterator/use_instr_nodbg_begin/use_instr_nodbg_end - Walk\n  /// all uses of the specified register, stepping by MachineInstr, skipping\n  /// those marked as Debug.\n  using use_instr_nodbg_iterator =\n      defusechain_instr_iterator<true, false, true, false, true, false>;\n  use_instr_nodbg_iterator use_instr_nodbg_begin(Register RegNo) const {\n    return use_instr_nodbg_iterator(getRegUseDefListHead(RegNo));\n  }\n  static use_instr_nodbg_iterator use_instr_nodbg_end() {\n    return use_instr_nodbg_iterator(nullptr);\n  }\n\n  inline iterator_range<use_instr_nodbg_iterator>\n  use_nodbg_instructions(Register Reg) const {\n    return make_range(use_instr_nodbg_begin(Reg), use_instr_nodbg_end());\n  }\n\n  /// use_bundle_nodbg_iterator/use_bundle_nodbg_begin/use_bundle_nodbg_end - Walk\n  /// all uses of the specified register, stepping by bundle, skipping\n  /// those marked as Debug.\n  using use_bundle_nodbg_iterator =\n      defusechain_instr_iterator<true, false, true, false, false, true>;\n  use_bundle_nodbg_iterator use_bundle_nodbg_begin(Register RegNo) const {\n    return use_bundle_nodbg_iterator(getRegUseDefListHead(RegNo));\n  }\n  static use_bundle_nodbg_iterator use_bundle_nodbg_end() {\n    return use_bundle_nodbg_iterator(nullptr);\n  }\n\n  inline iterator_range<use_bundle_nodbg_iterator>\n  use_nodbg_bundles(Register Reg) const {\n    return make_range(use_bundle_nodbg_begin(Reg), use_bundle_nodbg_end());\n  }\n\n  /// use_nodbg_empty - Return true if there are no non-Debug instructions\n  /// using the specified register.\n  bool use_nodbg_empty(Register RegNo) const {\n    return use_nodbg_begin(RegNo) == use_nodbg_end();\n  }\n\n  /// hasOneNonDBGUse - Return true if there is exactly one non-Debug\n  /// use of the specified register.\n  bool hasOneNonDBGUse(Register RegNo) const;\n\n  /// hasOneNonDBGUse - Return true if there is exactly one non-Debug\n  /// instruction using the specified register. Said instruction may have\n  /// multiple uses.\n  bool hasOneNonDBGUser(Register RegNo) const;\n\n  /// replaceRegWith - Replace all instances of FromReg with ToReg in the\n  /// machine function.  This is like llvm-level X->replaceAllUsesWith(Y),\n  /// except that it also changes any definitions of the register as well.\n  ///\n  /// Note that it is usually necessary to first constrain ToReg's register\n  /// class and register bank to match the FromReg constraints using one of the\n  /// methods:\n  ///\n  ///   constrainRegClass(ToReg, getRegClass(FromReg))\n  ///   constrainRegAttrs(ToReg, FromReg)\n  ///   RegisterBankInfo::constrainGenericRegister(ToReg,\n  ///       *MRI.getRegClass(FromReg), MRI)\n  ///\n  /// These functions will return a falsy result if the virtual registers have\n  /// incompatible constraints.\n  ///\n  /// Note that if ToReg is a physical register the function will replace and\n  /// apply sub registers to ToReg in order to obtain a final/proper physical\n  /// register.\n  void replaceRegWith(Register FromReg, Register ToReg);\n\n  /// getVRegDef - Return the machine instr that defines the specified virtual\n  /// register or null if none is found.  This assumes that the code is in SSA\n  /// form, so there should only be one definition.\n  MachineInstr *getVRegDef(Register Reg) const;\n\n  /// getUniqueVRegDef - Return the unique machine instr that defines the\n  /// specified virtual register or null if none is found.  If there are\n  /// multiple definitions or no definition, return null.\n  MachineInstr *getUniqueVRegDef(Register Reg) const;\n\n  /// clearKillFlags - Iterate over all the uses of the given register and\n  /// clear the kill flag from the MachineOperand. This function is used by\n  /// optimization passes which extend register lifetimes and need only\n  /// preserve conservative kill flag information.\n  void clearKillFlags(Register Reg) const;\n\n  void dumpUses(Register RegNo) const;\n\n  /// Returns true if PhysReg is unallocatable and constant throughout the\n  /// function. Writing to a constant register has no effect.\n  bool isConstantPhysReg(MCRegister PhysReg) const;\n\n  /// Get an iterator over the pressure sets affected by the given physical or\n  /// virtual register. If RegUnit is physical, it must be a register unit (from\n  /// MCRegUnitIterator).\n  PSetIterator getPressureSets(Register RegUnit) const;\n\n  //===--------------------------------------------------------------------===//\n  // Virtual Register Info\n  //===--------------------------------------------------------------------===//\n\n  /// Return the register class of the specified virtual register.\n  /// This shouldn't be used directly unless \\p Reg has a register class.\n  /// \\see getRegClassOrNull when this might happen.\n  const TargetRegisterClass *getRegClass(Register Reg) const {\n    assert(VRegInfo[Reg.id()].first.is<const TargetRegisterClass *>() &&\n           \"Register class not set, wrong accessor\");\n    return VRegInfo[Reg.id()].first.get<const TargetRegisterClass *>();\n  }\n\n  /// Return the register class of \\p Reg, or null if Reg has not been assigned\n  /// a register class yet.\n  ///\n  /// \\note A null register class can only happen when these two\n  /// conditions are met:\n  /// 1. Generic virtual registers are created.\n  /// 2. The machine function has not completely been through the\n  ///    instruction selection process.\n  /// None of this condition is possible without GlobalISel for now.\n  /// In other words, if GlobalISel is not used or if the query happens after\n  /// the select pass, using getRegClass is safe.\n  const TargetRegisterClass *getRegClassOrNull(Register Reg) const {\n    const RegClassOrRegBank &Val = VRegInfo[Reg].first;\n    return Val.dyn_cast<const TargetRegisterClass *>();\n  }\n\n  /// Return the register bank of \\p Reg, or null if Reg has not been assigned\n  /// a register bank or has been assigned a register class.\n  /// \\note It is possible to get the register bank from the register class via\n  /// RegisterBankInfo::getRegBankFromRegClass.\n  const RegisterBank *getRegBankOrNull(Register Reg) const {\n    const RegClassOrRegBank &Val = VRegInfo[Reg].first;\n    return Val.dyn_cast<const RegisterBank *>();\n  }\n\n  /// Return the register bank or register class of \\p Reg.\n  /// \\note Before the register bank gets assigned (i.e., before the\n  /// RegBankSelect pass) \\p Reg may not have either.\n  const RegClassOrRegBank &getRegClassOrRegBank(Register Reg) const {\n    return VRegInfo[Reg].first;\n  }\n\n  /// setRegClass - Set the register class of the specified virtual register.\n  void setRegClass(Register Reg, const TargetRegisterClass *RC);\n\n  /// Set the register bank to \\p RegBank for \\p Reg.\n  void setRegBank(Register Reg, const RegisterBank &RegBank);\n\n  void setRegClassOrRegBank(Register Reg,\n                            const RegClassOrRegBank &RCOrRB){\n    VRegInfo[Reg].first = RCOrRB;\n  }\n\n  /// constrainRegClass - Constrain the register class of the specified virtual\n  /// register to be a common subclass of RC and the current register class,\n  /// but only if the new class has at least MinNumRegs registers.  Return the\n  /// new register class, or NULL if no such class exists.\n  /// This should only be used when the constraint is known to be trivial, like\n  /// GR32 -> GR32_NOSP. Beware of increasing register pressure.\n  ///\n  /// \\note Assumes that the register has a register class assigned.\n  /// Use RegisterBankInfo::constrainGenericRegister in GlobalISel's\n  /// InstructionSelect pass and constrainRegAttrs in every other pass,\n  /// including non-select passes of GlobalISel, instead.\n  const TargetRegisterClass *constrainRegClass(Register Reg,\n                                               const TargetRegisterClass *RC,\n                                               unsigned MinNumRegs = 0);\n\n  /// Constrain the register class or the register bank of the virtual register\n  /// \\p Reg (and low-level type) to be a common subclass or a common bank of\n  /// both registers provided respectively (and a common low-level type). Do\n  /// nothing if any of the attributes (classes, banks, or low-level types) of\n  /// the registers are deemed incompatible, or if the resulting register will\n  /// have a class smaller than before and of size less than \\p MinNumRegs.\n  /// Return true if such register attributes exist, false otherwise.\n  ///\n  /// \\note Use this method instead of constrainRegClass and\n  /// RegisterBankInfo::constrainGenericRegister everywhere but SelectionDAG\n  /// ISel / FastISel and GlobalISel's InstructionSelect pass respectively.\n  bool constrainRegAttrs(Register Reg, Register ConstrainingReg,\n                         unsigned MinNumRegs = 0);\n\n  /// recomputeRegClass - Try to find a legal super-class of Reg's register\n  /// class that still satisfies the constraints from the instructions using\n  /// Reg.  Returns true if Reg was upgraded.\n  ///\n  /// This method can be used after constraints have been removed from a\n  /// virtual register, for example after removing instructions or splitting\n  /// the live range.\n  bool recomputeRegClass(Register Reg);\n\n  /// createVirtualRegister - Create and return a new virtual register in the\n  /// function with the specified register class.\n  Register createVirtualRegister(const TargetRegisterClass *RegClass,\n                                 StringRef Name = \"\");\n\n  /// Create and return a new virtual register in the function with the same\n  /// attributes as the given register.\n  Register cloneVirtualRegister(Register VReg, StringRef Name = \"\");\n\n  /// Get the low-level type of \\p Reg or LLT{} if Reg is not a generic\n  /// (target independent) virtual register.\n  LLT getType(Register Reg) const {\n    if (Register::isVirtualRegister(Reg) && VRegToType.inBounds(Reg))\n      return VRegToType[Reg];\n    return LLT{};\n  }\n\n  /// Set the low-level type of \\p VReg to \\p Ty.\n  void setType(Register VReg, LLT Ty);\n\n  /// Create and return a new generic virtual register with low-level\n  /// type \\p Ty.\n  Register createGenericVirtualRegister(LLT Ty, StringRef Name = \"\");\n\n  /// Remove all types associated to virtual registers (after instruction\n  /// selection and constraining of all generic virtual registers).\n  void clearVirtRegTypes();\n\n  /// Creates a new virtual register that has no register class, register bank\n  /// or size assigned yet. This is only allowed to be used\n  /// temporarily while constructing machine instructions. Most operations are\n  /// undefined on an incomplete register until one of setRegClass(),\n  /// setRegBank() or setSize() has been called on it.\n  Register createIncompleteVirtualRegister(StringRef Name = \"\");\n\n  /// getNumVirtRegs - Return the number of virtual registers created.\n  unsigned getNumVirtRegs() const { return VRegInfo.size(); }\n\n  /// clearVirtRegs - Remove all virtual registers (after physreg assignment).\n  void clearVirtRegs();\n\n  /// setRegAllocationHint - Specify a register allocation hint for the\n  /// specified virtual register. This is typically used by target, and in case\n  /// of an earlier hint it will be overwritten.\n  void setRegAllocationHint(Register VReg, unsigned Type, Register PrefReg) {\n    assert(VReg.isVirtual());\n    RegAllocHints[VReg].first  = Type;\n    RegAllocHints[VReg].second.clear();\n    RegAllocHints[VReg].second.push_back(PrefReg);\n  }\n\n  /// addRegAllocationHint - Add a register allocation hint to the hints\n  /// vector for VReg.\n  void addRegAllocationHint(Register VReg, Register PrefReg) {\n    assert(Register::isVirtualRegister(VReg));\n    RegAllocHints[VReg].second.push_back(PrefReg);\n  }\n\n  /// Specify the preferred (target independent) register allocation hint for\n  /// the specified virtual register.\n  void setSimpleHint(Register VReg, Register PrefReg) {\n    setRegAllocationHint(VReg, /*Type=*/0, PrefReg);\n  }\n\n  void clearSimpleHint(Register VReg) {\n    assert (!RegAllocHints[VReg].first &&\n            \"Expected to clear a non-target hint!\");\n    RegAllocHints[VReg].second.clear();\n  }\n\n  /// getRegAllocationHint - Return the register allocation hint for the\n  /// specified virtual register. If there are many hints, this returns the\n  /// one with the greatest weight.\n  std::pair<Register, Register>\n  getRegAllocationHint(Register VReg) const {\n    assert(VReg.isVirtual());\n    Register BestHint = (RegAllocHints[VReg.id()].second.size() ?\n                         RegAllocHints[VReg.id()].second[0] : Register());\n    return std::pair<Register, Register>(RegAllocHints[VReg.id()].first,\n                                         BestHint);\n  }\n\n  /// getSimpleHint - same as getRegAllocationHint except it will only return\n  /// a target independent hint.\n  Register getSimpleHint(Register VReg) const {\n    assert(VReg.isVirtual());\n    std::pair<Register, Register> Hint = getRegAllocationHint(VReg);\n    return Hint.first ? Register() : Hint.second;\n  }\n\n  /// getRegAllocationHints - Return a reference to the vector of all\n  /// register allocation hints for VReg.\n  const std::pair<Register, SmallVector<Register, 4>>\n  &getRegAllocationHints(Register VReg) const {\n    assert(VReg.isVirtual());\n    return RegAllocHints[VReg];\n  }\n\n  /// markUsesInDebugValueAsUndef - Mark every DBG_VALUE referencing the\n  /// specified register as undefined which causes the DBG_VALUE to be\n  /// deleted during LiveDebugVariables analysis.\n  void markUsesInDebugValueAsUndef(Register Reg) const;\n\n  /// updateDbgUsersToReg - Update a collection of DBG_VALUE instructions\n  /// to refer to the designated register.\n  void updateDbgUsersToReg(Register Reg,\n                           ArrayRef<MachineInstr*> Users) const {\n    for (MachineInstr *MI : Users) {\n      assert(MI->isDebugInstr());\n      assert(MI->getOperand(0).isReg());\n      MI->getOperand(0).setReg(Reg);\n    }\n  }\n\n  /// Return true if the specified register is modified in this function.\n  /// This checks that no defining machine operands exist for the register or\n  /// any of its aliases. Definitions found on functions marked noreturn are\n  /// ignored, to consider them pass 'true' for optional parameter\n  /// SkipNoReturnDef. The register is also considered modified when it is set\n  /// in the UsedPhysRegMask.\n  bool isPhysRegModified(MCRegister PhysReg, bool SkipNoReturnDef = false) const;\n\n  /// Return true if the specified register is modified or read in this\n  /// function. This checks that no machine operands exist for the register or\n  /// any of its aliases. The register is also considered used when it is set\n  /// in the UsedPhysRegMask.\n  bool isPhysRegUsed(MCRegister PhysReg) const;\n\n  /// addPhysRegsUsedFromRegMask - Mark any registers not in RegMask as used.\n  /// This corresponds to the bit mask attached to register mask operands.\n  void addPhysRegsUsedFromRegMask(const uint32_t *RegMask) {\n    UsedPhysRegMask.setBitsNotInMask(RegMask);\n  }\n\n  const BitVector &getUsedPhysRegsMask() const { return UsedPhysRegMask; }\n\n  //===--------------------------------------------------------------------===//\n  // Reserved Register Info\n  //===--------------------------------------------------------------------===//\n  //\n  // The set of reserved registers must be invariant during register\n  // allocation.  For example, the target cannot suddenly decide it needs a\n  // frame pointer when the register allocator has already used the frame\n  // pointer register for something else.\n  //\n  // These methods can be used by target hooks like hasFP() to avoid changing\n  // the reserved register set during register allocation.\n\n  /// freezeReservedRegs - Called by the register allocator to freeze the set\n  /// of reserved registers before allocation begins.\n  void freezeReservedRegs(const MachineFunction&);\n\n  /// reservedRegsFrozen - Returns true after freezeReservedRegs() was called\n  /// to ensure the set of reserved registers stays constant.\n  bool reservedRegsFrozen() const {\n    return !ReservedRegs.empty();\n  }\n\n  /// canReserveReg - Returns true if PhysReg can be used as a reserved\n  /// register.  Any register can be reserved before freezeReservedRegs() is\n  /// called.\n  bool canReserveReg(MCRegister PhysReg) const {\n    return !reservedRegsFrozen() || ReservedRegs.test(PhysReg);\n  }\n\n  /// getReservedRegs - Returns a reference to the frozen set of reserved\n  /// registers. This method should always be preferred to calling\n  /// TRI::getReservedRegs() when possible.\n  const BitVector &getReservedRegs() const {\n    assert(reservedRegsFrozen() &&\n           \"Reserved registers haven't been frozen yet. \"\n           \"Use TRI::getReservedRegs().\");\n    return ReservedRegs;\n  }\n\n  /// isReserved - Returns true when PhysReg is a reserved register.\n  ///\n  /// Reserved registers may belong to an allocatable register class, but the\n  /// target has explicitly requested that they are not used.\n  bool isReserved(MCRegister PhysReg) const {\n    return getReservedRegs().test(PhysReg.id());\n  }\n\n  /// Returns true when the given register unit is considered reserved.\n  ///\n  /// Register units are considered reserved when for at least one of their\n  /// root registers, the root register and all super registers are reserved.\n  /// This currently iterates the register hierarchy and may be slower than\n  /// expected.\n  bool isReservedRegUnit(unsigned Unit) const;\n\n  /// isAllocatable - Returns true when PhysReg belongs to an allocatable\n  /// register class and it hasn't been reserved.\n  ///\n  /// Allocatable registers may show up in the allocation order of some virtual\n  /// register, so a register allocator needs to track its liveness and\n  /// availability.\n  bool isAllocatable(MCRegister PhysReg) const {\n    return getTargetRegisterInfo()->isInAllocatableClass(PhysReg) &&\n      !isReserved(PhysReg);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // LiveIn Management\n  //===--------------------------------------------------------------------===//\n\n  /// addLiveIn - Add the specified register as a live-in.  Note that it\n  /// is an error to add the same register to the same set more than once.\n  void addLiveIn(MCRegister Reg, Register vreg = Register()) {\n    LiveIns.push_back(std::make_pair(Reg, vreg));\n  }\n\n  // Iteration support for the live-ins set.  It's kept in sorted order\n  // by register number.\n  using livein_iterator =\n      std::vector<std::pair<MCRegister,Register>>::const_iterator;\n  livein_iterator livein_begin() const { return LiveIns.begin(); }\n  livein_iterator livein_end()   const { return LiveIns.end(); }\n  bool            livein_empty() const { return LiveIns.empty(); }\n\n  ArrayRef<std::pair<MCRegister, Register>> liveins() const {\n    return LiveIns;\n  }\n\n  bool isLiveIn(Register Reg) const;\n\n  /// getLiveInPhysReg - If VReg is a live-in virtual register, return the\n  /// corresponding live-in physical register.\n  MCRegister getLiveInPhysReg(Register VReg) const;\n\n  /// getLiveInVirtReg - If PReg is a live-in physical register, return the\n  /// corresponding live-in physical register.\n  Register getLiveInVirtReg(MCRegister PReg) const;\n\n  /// EmitLiveInCopies - Emit copies to initialize livein virtual registers\n  /// into the given entry block.\n  void EmitLiveInCopies(MachineBasicBlock *EntryMBB,\n                        const TargetRegisterInfo &TRI,\n                        const TargetInstrInfo &TII);\n\n  /// Returns a mask covering all bits that can appear in lane masks of\n  /// subregisters of the virtual register @p Reg.\n  LaneBitmask getMaxLaneMaskForVReg(Register Reg) const;\n\n  /// defusechain_iterator - This class provides iterator support for machine\n  /// operands in the function that use or define a specific register.  If\n  /// ReturnUses is true it returns uses of registers, if ReturnDefs is true it\n  /// returns defs.  If neither are true then you are silly and it always\n  /// returns end().  If SkipDebug is true it skips uses marked Debug\n  /// when incrementing.\n  template <bool ReturnUses, bool ReturnDefs, bool SkipDebug, bool ByOperand,\n            bool ByInstr, bool ByBundle>\n  class defusechain_iterator : public std::iterator<std::forward_iterator_tag,\n                                                    MachineOperand, ptrdiff_t> {\n    friend class MachineRegisterInfo;\n\n    MachineOperand *Op = nullptr;\n\n    explicit defusechain_iterator(MachineOperand *op) : Op(op) {\n      // If the first node isn't one we're interested in, advance to one that\n      // we are interested in.\n      if (op) {\n        if ((!ReturnUses && op->isUse()) ||\n            (!ReturnDefs && op->isDef()) ||\n            (SkipDebug && op->isDebug()))\n          advance();\n      }\n    }\n\n    void advance() {\n      assert(Op && \"Cannot increment end iterator!\");\n      Op = getNextOperandForReg(Op);\n\n      // All defs come before the uses, so stop def_iterator early.\n      if (!ReturnUses) {\n        if (Op) {\n          if (Op->isUse())\n            Op = nullptr;\n          else\n            assert(!Op->isDebug() && \"Can't have debug defs\");\n        }\n      } else {\n        // If this is an operand we don't care about, skip it.\n        while (Op && ((!ReturnDefs && Op->isDef()) ||\n                      (SkipDebug && Op->isDebug())))\n          Op = getNextOperandForReg(Op);\n      }\n    }\n\n  public:\n    using reference = std::iterator<std::forward_iterator_tag, MachineOperand,\n                                    ptrdiff_t>::reference;\n    using pointer = std::iterator<std::forward_iterator_tag, MachineOperand,\n                                  ptrdiff_t>::pointer;\n\n    defusechain_iterator() = default;\n\n    bool operator==(const defusechain_iterator &x) const {\n      return Op == x.Op;\n    }\n    bool operator!=(const defusechain_iterator &x) const {\n      return !operator==(x);\n    }\n\n    /// atEnd - return true if this iterator is equal to reg_end() on the value.\n    bool atEnd() const { return Op == nullptr; }\n\n    // Iterator traversal: forward iteration only\n    defusechain_iterator &operator++() {          // Preincrement\n      assert(Op && \"Cannot increment end iterator!\");\n      if (ByOperand)\n        advance();\n      else if (ByInstr) {\n        MachineInstr *P = Op->getParent();\n        do {\n          advance();\n        } while (Op && Op->getParent() == P);\n      } else if (ByBundle) {\n        MachineBasicBlock::instr_iterator P =\n            getBundleStart(Op->getParent()->getIterator());\n        do {\n          advance();\n        } while (Op && getBundleStart(Op->getParent()->getIterator()) == P);\n      }\n\n      return *this;\n    }\n    defusechain_iterator operator++(int) {        // Postincrement\n      defusechain_iterator tmp = *this; ++*this; return tmp;\n    }\n\n    /// getOperandNo - Return the operand # of this MachineOperand in its\n    /// MachineInstr.\n    unsigned getOperandNo() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      return Op - &Op->getParent()->getOperand(0);\n    }\n\n    // Retrieve a reference to the current operand.\n    MachineOperand &operator*() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      return *Op;\n    }\n\n    MachineOperand *operator->() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      return Op;\n    }\n  };\n\n  /// defusechain_iterator - This class provides iterator support for machine\n  /// operands in the function that use or define a specific register.  If\n  /// ReturnUses is true it returns uses of registers, if ReturnDefs is true it\n  /// returns defs.  If neither are true then you are silly and it always\n  /// returns end().  If SkipDebug is true it skips uses marked Debug\n  /// when incrementing.\n  template<bool ReturnUses, bool ReturnDefs, bool SkipDebug,\n           bool ByOperand, bool ByInstr, bool ByBundle>\n  class defusechain_instr_iterator\n    : public std::iterator<std::forward_iterator_tag, MachineInstr, ptrdiff_t> {\n    friend class MachineRegisterInfo;\n\n    MachineOperand *Op = nullptr;\n\n    explicit defusechain_instr_iterator(MachineOperand *op) : Op(op) {\n      // If the first node isn't one we're interested in, advance to one that\n      // we are interested in.\n      if (op) {\n        if ((!ReturnUses && op->isUse()) ||\n            (!ReturnDefs && op->isDef()) ||\n            (SkipDebug && op->isDebug()))\n          advance();\n      }\n    }\n\n    void advance() {\n      assert(Op && \"Cannot increment end iterator!\");\n      Op = getNextOperandForReg(Op);\n\n      // All defs come before the uses, so stop def_iterator early.\n      if (!ReturnUses) {\n        if (Op) {\n          if (Op->isUse())\n            Op = nullptr;\n          else\n            assert(!Op->isDebug() && \"Can't have debug defs\");\n        }\n      } else {\n        // If this is an operand we don't care about, skip it.\n        while (Op && ((!ReturnDefs && Op->isDef()) ||\n                      (SkipDebug && Op->isDebug())))\n          Op = getNextOperandForReg(Op);\n      }\n    }\n\n  public:\n    using reference = std::iterator<std::forward_iterator_tag,\n                                    MachineInstr, ptrdiff_t>::reference;\n    using pointer = std::iterator<std::forward_iterator_tag,\n                                  MachineInstr, ptrdiff_t>::pointer;\n\n    defusechain_instr_iterator() = default;\n\n    bool operator==(const defusechain_instr_iterator &x) const {\n      return Op == x.Op;\n    }\n    bool operator!=(const defusechain_instr_iterator &x) const {\n      return !operator==(x);\n    }\n\n    /// atEnd - return true if this iterator is equal to reg_end() on the value.\n    bool atEnd() const { return Op == nullptr; }\n\n    // Iterator traversal: forward iteration only\n    defusechain_instr_iterator &operator++() {          // Preincrement\n      assert(Op && \"Cannot increment end iterator!\");\n      if (ByOperand)\n        advance();\n      else if (ByInstr) {\n        MachineInstr *P = Op->getParent();\n        do {\n          advance();\n        } while (Op && Op->getParent() == P);\n      } else if (ByBundle) {\n        MachineBasicBlock::instr_iterator P =\n            getBundleStart(Op->getParent()->getIterator());\n        do {\n          advance();\n        } while (Op && getBundleStart(Op->getParent()->getIterator()) == P);\n      }\n\n      return *this;\n    }\n    defusechain_instr_iterator operator++(int) {        // Postincrement\n      defusechain_instr_iterator tmp = *this; ++*this; return tmp;\n    }\n\n    // Retrieve a reference to the current operand.\n    MachineInstr &operator*() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      if (ByBundle)\n        return *getBundleStart(Op->getParent()->getIterator());\n      return *Op->getParent();\n    }\n\n    MachineInstr *operator->() const { return &operator*(); }\n  };\n};\n\n/// Iterate over the pressure sets affected by the given physical or virtual\n/// register. If Reg is physical, it must be a register unit (from\n/// MCRegUnitIterator).\nclass PSetIterator {\n  const int *PSet = nullptr;\n  unsigned Weight = 0;\n\npublic:\n  PSetIterator() = default;\n\n  PSetIterator(Register RegUnit, const MachineRegisterInfo *MRI) {\n    const TargetRegisterInfo *TRI = MRI->getTargetRegisterInfo();\n    if (RegUnit.isVirtual()) {\n      const TargetRegisterClass *RC = MRI->getRegClass(RegUnit);\n      PSet = TRI->getRegClassPressureSets(RC);\n      Weight = TRI->getRegClassWeight(RC).RegWeight;\n    } else {\n      PSet = TRI->getRegUnitPressureSets(RegUnit);\n      Weight = TRI->getRegUnitWeight(RegUnit);\n    }\n    if (*PSet == -1)\n      PSet = nullptr;\n  }\n\n  bool isValid() const { return PSet; }\n\n  unsigned getWeight() const { return Weight; }\n\n  unsigned operator*() const { return *PSet; }\n\n  void operator++() {\n    assert(isValid() && \"Invalid PSetIterator.\");\n    ++PSet;\n    if (*PSet == -1)\n      PSet = nullptr;\n  }\n};\n\ninline PSetIterator\nMachineRegisterInfo::getPressureSets(Register RegUnit) const {\n  return PSetIterator(RegUnit, this);\n}\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_MACHINEREGISTERINFO_H\n"}, "54": {"id": 54, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/RegisterClassInfo.h", "content": "//===- RegisterClassInfo.h - Dynamic Register Class Info --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file implements the RegisterClassInfo class which provides dynamic\n// information about target register classes. Callee saved and reserved\n// registers depends on calling conventions and other dynamic information, so\n// some things cannot be determined statically.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_REGISTERCLASSINFO_H\n#define LLVM_CODEGEN_REGISTERCLASSINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/BitVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/TargetRegisterInfo.h\"\n#include \"llvm/MC/MCRegisterInfo.h\"\n#include <cassert>\n#include <cstdint>\n#include <memory>\n\nnamespace llvm {\n\nclass RegisterClassInfo {\n  struct RCInfo {\n    unsigned Tag = 0;\n    unsigned NumRegs = 0;\n    bool ProperSubClass = false;\n    uint8_t MinCost = 0;\n    uint16_t LastCostChange = 0;\n    std::unique_ptr<MCPhysReg[]> Order;\n\n    RCInfo() = default;\n\n    operator ArrayRef<MCPhysReg>() const {\n      return makeArrayRef(Order.get(), NumRegs);\n    }\n  };\n\n  // Brief cached information for each register class.\n  std::unique_ptr<RCInfo[]> RegClass;\n\n  // Tag changes whenever cached information needs to be recomputed. An RCInfo\n  // entry is valid when its tag matches.\n  unsigned Tag = 0;\n\n  const MachineFunction *MF = nullptr;\n  const TargetRegisterInfo *TRI = nullptr;\n\n  // Callee saved registers of last MF. Assumed to be valid until the next\n  // runOnFunction() call.\n  // Used only to determine if an update was made to CalleeSavedAliases.\n  const MCPhysReg *CalleeSavedRegs = nullptr;\n\n  // Map register alias to the callee saved Register.\n  SmallVector<MCPhysReg, 4> CalleeSavedAliases;\n\n  // Reserved registers in the current MF.\n  BitVector Reserved;\n\n  std::unique_ptr<unsigned[]> PSetLimits;\n\n  // The register cost values.\n  ArrayRef<uint8_t> RegCosts;\n\n  // Compute all information about RC.\n  void compute(const TargetRegisterClass *RC) const;\n\n  // Return an up-to-date RCInfo for RC.\n  const RCInfo &get(const TargetRegisterClass *RC) const {\n    const RCInfo &RCI = RegClass[RC->getID()];\n    if (Tag != RCI.Tag)\n      compute(RC);\n    return RCI;\n  }\n\npublic:\n  RegisterClassInfo();\n\n  /// runOnFunction - Prepare to answer questions about MF. This must be called\n  /// before any other methods are used.\n  void runOnMachineFunction(const MachineFunction &MF);\n\n  /// getNumAllocatableRegs - Returns the number of actually allocatable\n  /// registers in RC in the current function.\n  unsigned getNumAllocatableRegs(const TargetRegisterClass *RC) const {\n    return get(RC).NumRegs;\n  }\n\n  /// getOrder - Returns the preferred allocation order for RC. The order\n  /// contains no reserved registers, and registers that alias callee saved\n  /// registers come last.\n  ArrayRef<MCPhysReg> getOrder(const TargetRegisterClass *RC) const {\n    return get(RC);\n  }\n\n  /// isProperSubClass - Returns true if RC has a legal super-class with more\n  /// allocatable registers.\n  ///\n  /// Register classes like GR32_NOSP are not proper sub-classes because %esp\n  /// is not allocatable.  Similarly, tGPR is not a proper sub-class in Thumb\n  /// mode because the GPR super-class is not legal.\n  bool isProperSubClass(const TargetRegisterClass *RC) const {\n    return get(RC).ProperSubClass;\n  }\n\n  /// getLastCalleeSavedAlias - Returns the last callee saved register that\n  /// overlaps PhysReg, or NoRegister if Reg doesn't overlap a\n  /// CalleeSavedAliases.\n  MCRegister getLastCalleeSavedAlias(MCRegister PhysReg) const {\n    if (PhysReg.id() < CalleeSavedAliases.size())\n      return CalleeSavedAliases[PhysReg];\n    return MCRegister::NoRegister;\n  }\n\n  /// Get the minimum register cost in RC's allocation order.\n  /// This is the smallest value in RegCosts[Reg] for all\n  /// the registers in getOrder(RC).\n  uint8_t getMinCost(const TargetRegisterClass *RC) const {\n    return get(RC).MinCost;\n  }\n\n  /// Get the position of the last cost change in getOrder(RC).\n  ///\n  /// All registers in getOrder(RC).slice(getLastCostChange(RC)) will have the\n  /// same cost according to RegCosts[Reg].\n  unsigned getLastCostChange(const TargetRegisterClass *RC) const {\n    return get(RC).LastCostChange;\n  }\n\n  /// Get the register unit limit for the given pressure set index.\n  ///\n  /// RegisterClassInfo adjusts this limit for reserved registers.\n  unsigned getRegPressureSetLimit(unsigned Idx) const {\n    if (!PSetLimits[Idx])\n      PSetLimits[Idx] = computePSetLimit(Idx);\n    return PSetLimits[Idx];\n  }\n\nprotected:\n  unsigned computePSetLimit(unsigned Idx) const;\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_REGISTERCLASSINFO_H\n"}, "55": {"id": 55, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "content": "//===- llvm/CodeGen/SelectionDAGNodes.h - SelectionDAG Nodes ----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file declares the SDNode class and derived classes, which are used to\n// represent the nodes and operations present in a SelectionDAG.  These nodes\n// and operations are machine code level operations, with some similarities to\n// the GCC RTL representation.\n//\n// Clients should include the SelectionDAG.h file instead of this file directly.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_SELECTIONDAGNODES_H\n#define LLVM_CODEGEN_SELECTIONDAGNODES_H\n\n#include \"llvm/ADT/APFloat.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/BitVector.h\"\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/ADT/GraphTraits.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/ilist_node.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/CodeGen/ISDOpcodes.h\"\n#include \"llvm/CodeGen/MachineMemOperand.h\"\n#include \"llvm/CodeGen/Register.h\"\n#include \"llvm/CodeGen/ValueTypes.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/Metadata.h\"\n#include \"llvm/IR/Operator.h\"\n#include \"llvm/Support/AlignOf.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include \"llvm/Support/TypeSize.h\"\n#include <algorithm>\n#include <cassert>\n#include <climits>\n#include <cstddef>\n#include <cstdint>\n#include <cstring>\n#include <iterator>\n#include <string>\n#include <tuple>\n\nnamespace llvm {\n\nclass APInt;\nclass Constant;\ntemplate <typename T> struct DenseMapInfo;\nclass GlobalValue;\nclass MachineBasicBlock;\nclass MachineConstantPoolValue;\nclass MCSymbol;\nclass raw_ostream;\nclass SDNode;\nclass SelectionDAG;\nclass Type;\nclass Value;\n\nvoid checkForCycles(const SDNode *N, const SelectionDAG *DAG = nullptr,\n                    bool force = false);\n\n/// This represents a list of ValueType's that has been intern'd by\n/// a SelectionDAG.  Instances of this simple value class are returned by\n/// SelectionDAG::getVTList(...).\n///\nstruct SDVTList {\n  const EVT *VTs;\n  unsigned int NumVTs;\n};\n\nnamespace ISD {\n\n  /// Node predicates\n\n/// If N is a BUILD_VECTOR or SPLAT_VECTOR node whose elements are all the\n/// same constant or undefined, return true and return the constant value in\n/// \\p SplatValue.\nbool isConstantSplatVector(const SDNode *N, APInt &SplatValue);\n\n/// Return true if the specified node is a BUILD_VECTOR or SPLAT_VECTOR where\n/// all of the elements are ~0 or undef. If \\p BuildVectorOnly is set to\n/// true, it only checks BUILD_VECTOR.\nbool isConstantSplatVectorAllOnes(const SDNode *N,\n                                  bool BuildVectorOnly = false);\n\n/// Return true if the specified node is a BUILD_VECTOR or SPLAT_VECTOR where\n/// all of the elements are 0 or undef. If \\p BuildVectorOnly is set to true, it\n/// only checks BUILD_VECTOR.\nbool isConstantSplatVectorAllZeros(const SDNode *N,\n                                   bool BuildVectorOnly = false);\n\n/// Return true if the specified node is a BUILD_VECTOR where all of the\n/// elements are ~0 or undef.\nbool isBuildVectorAllOnes(const SDNode *N);\n\n/// Return true if the specified node is a BUILD_VECTOR where all of the\n/// elements are 0 or undef.\nbool isBuildVectorAllZeros(const SDNode *N);\n\n/// Return true if the specified node is a BUILD_VECTOR node of all\n/// ConstantSDNode or undef.\nbool isBuildVectorOfConstantSDNodes(const SDNode *N);\n\n/// Return true if the specified node is a BUILD_VECTOR node of all\n/// ConstantFPSDNode or undef.\nbool isBuildVectorOfConstantFPSDNodes(const SDNode *N);\n\n/// Return true if the node has at least one operand and all operands of the\n/// specified node are ISD::UNDEF.\nbool allOperandsUndef(const SDNode *N);\n\n} // end namespace ISD\n\n//===----------------------------------------------------------------------===//\n/// Unlike LLVM values, Selection DAG nodes may return multiple\n/// values as the result of a computation.  Many nodes return multiple values,\n/// from loads (which define a token and a return value) to ADDC (which returns\n/// a result and a carry value), to calls (which may return an arbitrary number\n/// of values).\n///\n/// As such, each use of a SelectionDAG computation must indicate the node that\n/// computes it as well as which return value to use from that node.  This pair\n/// of information is represented with the SDValue value type.\n///\nclass SDValue {\n  friend struct DenseMapInfo<SDValue>;\n\n  SDNode *Node = nullptr; // The node defining the value we are using.\n  unsigned ResNo = 0;     // Which return value of the node we are using.\n\npublic:\n  SDValue() = default;\n  SDValue(SDNode *node, unsigned resno);\n\n  /// get the index which selects a specific result in the SDNode\n  unsigned getResNo() const { return ResNo; }\n\n  /// get the SDNode which holds the desired result\n  SDNode *getNode() const { return Node; }\n\n  /// set the SDNode\n  void setNode(SDNode *N) { Node = N; }\n\n  inline SDNode *operator->() const { return Node; }\n\n  bool operator==(const SDValue &O) const {\n    return Node == O.Node && ResNo == O.ResNo;\n  }\n  bool operator!=(const SDValue &O) const {\n    return !operator==(O);\n  }\n  bool operator<(const SDValue &O) const {\n    return std::tie(Node, ResNo) < std::tie(O.Node, O.ResNo);\n  }\n  explicit operator bool() const {\n    return Node != nullptr;\n  }\n\n  SDValue getValue(unsigned R) const {\n    return SDValue(Node, R);\n  }\n\n  /// Return true if this node is an operand of N.\n  bool isOperandOf(const SDNode *N) const;\n\n  /// Return the ValueType of the referenced return value.\n  inline EVT getValueType() const;\n\n  /// Return the simple ValueType of the referenced return value.\n  MVT getSimpleValueType() const {\n    return getValueType().getSimpleVT();\n  }\n\n  /// Returns the size of the value in bits.\n  ///\n  /// If the value type is a scalable vector type, the scalable property will\n  /// be set and the runtime size will be a positive integer multiple of the\n  /// base size.\n  TypeSize getValueSizeInBits() const {\n    return getValueType().getSizeInBits();\n  }\n\n  uint64_t getScalarValueSizeInBits() const {\n    return getValueType().getScalarType().getFixedSizeInBits();\n  }\n\n  // Forwarding methods - These forward to the corresponding methods in SDNode.\n  inline unsigned getOpcode() const;\n  inline unsigned getNumOperands() const;\n  inline const SDValue &getOperand(unsigned i) const;\n  inline uint64_t getConstantOperandVal(unsigned i) const;\n  inline const APInt &getConstantOperandAPInt(unsigned i) const;\n  inline bool isTargetMemoryOpcode() const;\n  inline bool isTargetOpcode() const;\n  inline bool isMachineOpcode() const;\n  inline bool isUndef() const;\n  inline unsigned getMachineOpcode() const;\n  inline const DebugLoc &getDebugLoc() const;\n  inline void dump() const;\n  inline void dump(const SelectionDAG *G) const;\n  inline void dumpr() const;\n  inline void dumpr(const SelectionDAG *G) const;\n\n  /// Return true if this operand (which must be a chain) reaches the\n  /// specified operand without crossing any side-effecting instructions.\n  /// In practice, this looks through token factors and non-volatile loads.\n  /// In order to remain efficient, this only\n  /// looks a couple of nodes in, it does not do an exhaustive search.\n  bool reachesChainWithoutSideEffects(SDValue Dest,\n                                      unsigned Depth = 2) const;\n\n  /// Return true if there are no nodes using value ResNo of Node.\n  inline bool use_empty() const;\n\n  /// Return true if there is exactly one node using value ResNo of Node.\n  inline bool hasOneUse() const;\n};\n\ntemplate<> struct DenseMapInfo<SDValue> {\n  static inline SDValue getEmptyKey() {\n    SDValue V;\n    V.ResNo = -1U;\n    return V;\n  }\n\n  static inline SDValue getTombstoneKey() {\n    SDValue V;\n    V.ResNo = -2U;\n    return V;\n  }\n\n  static unsigned getHashValue(const SDValue &Val) {\n    return ((unsigned)((uintptr_t)Val.getNode() >> 4) ^\n            (unsigned)((uintptr_t)Val.getNode() >> 9)) + Val.getResNo();\n  }\n\n  static bool isEqual(const SDValue &LHS, const SDValue &RHS) {\n    return LHS == RHS;\n  }\n};\n\n/// Allow casting operators to work directly on\n/// SDValues as if they were SDNode*'s.\ntemplate<> struct simplify_type<SDValue> {\n  using SimpleType = SDNode *;\n\n  static SimpleType getSimplifiedValue(SDValue &Val) {\n    return Val.getNode();\n  }\n};\ntemplate<> struct simplify_type<const SDValue> {\n  using SimpleType = /*const*/ SDNode *;\n\n  static SimpleType getSimplifiedValue(const SDValue &Val) {\n    return Val.getNode();\n  }\n};\n\n/// Represents a use of a SDNode. This class holds an SDValue,\n/// which records the SDNode being used and the result number, a\n/// pointer to the SDNode using the value, and Next and Prev pointers,\n/// which link together all the uses of an SDNode.\n///\nclass SDUse {\n  /// Val - The value being used.\n  SDValue Val;\n  /// User - The user of this value.\n  SDNode *User = nullptr;\n  /// Prev, Next - Pointers to the uses list of the SDNode referred by\n  /// this operand.\n  SDUse **Prev = nullptr;\n  SDUse *Next = nullptr;\n\npublic:\n  SDUse() = default;\n  SDUse(const SDUse &U) = delete;\n  SDUse &operator=(const SDUse &) = delete;\n\n  /// Normally SDUse will just implicitly convert to an SDValue that it holds.\n  operator const SDValue&() const { return Val; }\n\n  /// If implicit conversion to SDValue doesn't work, the get() method returns\n  /// the SDValue.\n  const SDValue &get() const { return Val; }\n\n  /// This returns the SDNode that contains this Use.\n  SDNode *getUser() { return User; }\n\n  /// Get the next SDUse in the use list.\n  SDUse *getNext() const { return Next; }\n\n  /// Convenience function for get().getNode().\n  SDNode *getNode() const { return Val.getNode(); }\n  /// Convenience function for get().getResNo().\n  unsigned getResNo() const { return Val.getResNo(); }\n  /// Convenience function for get().getValueType().\n  EVT getValueType() const { return Val.getValueType(); }\n\n  /// Convenience function for get().operator==\n  bool operator==(const SDValue &V) const {\n    return Val == V;\n  }\n\n  /// Convenience function for get().operator!=\n  bool operator!=(const SDValue &V) const {\n    return Val != V;\n  }\n\n  /// Convenience function for get().operator<\n  bool operator<(const SDValue &V) const {\n    return Val < V;\n  }\n\nprivate:\n  friend class SelectionDAG;\n  friend class SDNode;\n  // TODO: unfriend HandleSDNode once we fix its operand handling.\n  friend class HandleSDNode;\n\n  void setUser(SDNode *p) { User = p; }\n\n  /// Remove this use from its existing use list, assign it the\n  /// given value, and add it to the new value's node's use list.\n  inline void set(const SDValue &V);\n  /// Like set, but only supports initializing a newly-allocated\n  /// SDUse with a non-null value.\n  inline void setInitial(const SDValue &V);\n  /// Like set, but only sets the Node portion of the value,\n  /// leaving the ResNo portion unmodified.\n  inline void setNode(SDNode *N);\n\n  void addToList(SDUse **List) {\n    Next = *List;\n    if (Next) Next->Prev = &Next;\n    Prev = List;\n    *List = this;\n  }\n\n  void removeFromList() {\n    *Prev = Next;\n    if (Next) Next->Prev = Prev;\n  }\n};\n\n/// simplify_type specializations - Allow casting operators to work directly on\n/// SDValues as if they were SDNode*'s.\ntemplate<> struct simplify_type<SDUse> {\n  using SimpleType = SDNode *;\n\n  static SimpleType getSimplifiedValue(SDUse &Val) {\n    return Val.getNode();\n  }\n};\n\n/// These are IR-level optimization flags that may be propagated to SDNodes.\n/// TODO: This data structure should be shared by the IR optimizer and the\n/// the backend.\nstruct SDNodeFlags {\nprivate:\n  bool NoUnsignedWrap : 1;\n  bool NoSignedWrap : 1;\n  bool Exact : 1;\n  bool NoNaNs : 1;\n  bool NoInfs : 1;\n  bool NoSignedZeros : 1;\n  bool AllowReciprocal : 1;\n  bool AllowContract : 1;\n  bool ApproximateFuncs : 1;\n  bool AllowReassociation : 1;\n\n  // We assume instructions do not raise floating-point exceptions by default,\n  // and only those marked explicitly may do so.  We could choose to represent\n  // this via a positive \"FPExcept\" flags like on the MI level, but having a\n  // negative \"NoFPExcept\" flag here (that defaults to true) makes the flag\n  // intersection logic more straightforward.\n  bool NoFPExcept : 1;\n\npublic:\n  /// Default constructor turns off all optimization flags.\n  SDNodeFlags()\n      : NoUnsignedWrap(false), NoSignedWrap(false), Exact(false), NoNaNs(false),\n        NoInfs(false), NoSignedZeros(false), AllowReciprocal(false),\n        AllowContract(false), ApproximateFuncs(false),\n        AllowReassociation(false), NoFPExcept(false) {}\n\n  /// Propagate the fast-math-flags from an IR FPMathOperator.\n  void copyFMF(const FPMathOperator &FPMO) {\n    setNoNaNs(FPMO.hasNoNaNs());\n    setNoInfs(FPMO.hasNoInfs());\n    setNoSignedZeros(FPMO.hasNoSignedZeros());\n    setAllowReciprocal(FPMO.hasAllowReciprocal());\n    setAllowContract(FPMO.hasAllowContract());\n    setApproximateFuncs(FPMO.hasApproxFunc());\n    setAllowReassociation(FPMO.hasAllowReassoc());\n  }\n\n  // These are mutators for each flag.\n  void setNoUnsignedWrap(bool b) { NoUnsignedWrap = b; }\n  void setNoSignedWrap(bool b) { NoSignedWrap = b; }\n  void setExact(bool b) { Exact = b; }\n  void setNoNaNs(bool b) { NoNaNs = b; }\n  void setNoInfs(bool b) { NoInfs = b; }\n  void setNoSignedZeros(bool b) { NoSignedZeros = b; }\n  void setAllowReciprocal(bool b) { AllowReciprocal = b; }\n  void setAllowContract(bool b) { AllowContract = b; }\n  void setApproximateFuncs(bool b) { ApproximateFuncs = b; }\n  void setAllowReassociation(bool b) { AllowReassociation = b; }\n  void setNoFPExcept(bool b) { NoFPExcept = b; }\n\n  // These are accessors for each flag.\n  bool hasNoUnsignedWrap() const { return NoUnsignedWrap; }\n  bool hasNoSignedWrap() const { return NoSignedWrap; }\n  bool hasExact() const { return Exact; }\n  bool hasNoNaNs() const { return NoNaNs; }\n  bool hasNoInfs() const { return NoInfs; }\n  bool hasNoSignedZeros() const { return NoSignedZeros; }\n  bool hasAllowReciprocal() const { return AllowReciprocal; }\n  bool hasAllowContract() const { return AllowContract; }\n  bool hasApproximateFuncs() const { return ApproximateFuncs; }\n  bool hasAllowReassociation() const { return AllowReassociation; }\n  bool hasNoFPExcept() const { return NoFPExcept; }\n\n  /// Clear any flags in this flag set that aren't also set in Flags. All\n  /// flags will be cleared if Flags are undefined.\n  void intersectWith(const SDNodeFlags Flags) {\n    NoUnsignedWrap &= Flags.NoUnsignedWrap;\n    NoSignedWrap &= Flags.NoSignedWrap;\n    Exact &= Flags.Exact;\n    NoNaNs &= Flags.NoNaNs;\n    NoInfs &= Flags.NoInfs;\n    NoSignedZeros &= Flags.NoSignedZeros;\n    AllowReciprocal &= Flags.AllowReciprocal;\n    AllowContract &= Flags.AllowContract;\n    ApproximateFuncs &= Flags.ApproximateFuncs;\n    AllowReassociation &= Flags.AllowReassociation;\n    NoFPExcept &= Flags.NoFPExcept;\n  }\n};\n\n/// Represents one node in the SelectionDAG.\n///\nclass SDNode : public FoldingSetNode, public ilist_node<SDNode> {\nprivate:\n  /// The operation that this node performs.\n  int16_t NodeType;\n\nprotected:\n  // We define a set of mini-helper classes to help us interpret the bits in our\n  // SubclassData.  These are designed to fit within a uint16_t so they pack\n  // with NodeType.\n\n#if defined(_AIX) && (!defined(__GNUC__) || defined(__ibmxl__))\n// Except for GCC; by default, AIX compilers store bit-fields in 4-byte words\n// and give the `pack` pragma push semantics.\n#define BEGIN_TWO_BYTE_PACK() _Pragma(\"pack(2)\")\n#define END_TWO_BYTE_PACK() _Pragma(\"pack(pop)\")\n#else\n#define BEGIN_TWO_BYTE_PACK()\n#define END_TWO_BYTE_PACK()\n#endif\n\nBEGIN_TWO_BYTE_PACK()\n  class SDNodeBitfields {\n    friend class SDNode;\n    friend class MemIntrinsicSDNode;\n    friend class MemSDNode;\n    friend class SelectionDAG;\n\n    uint16_t HasDebugValue : 1;\n    uint16_t IsMemIntrinsic : 1;\n    uint16_t IsDivergent : 1;\n  };\n  enum { NumSDNodeBits = 3 };\n\n  class ConstantSDNodeBitfields {\n    friend class ConstantSDNode;\n\n    uint16_t : NumSDNodeBits;\n\n    uint16_t IsOpaque : 1;\n  };\n\n  class MemSDNodeBitfields {\n    friend class MemSDNode;\n    friend class MemIntrinsicSDNode;\n    friend class AtomicSDNode;\n\n    uint16_t : NumSDNodeBits;\n\n    uint16_t IsVolatile : 1;\n    uint16_t IsNonTemporal : 1;\n    uint16_t IsDereferenceable : 1;\n    uint16_t IsInvariant : 1;\n  };\n  enum { NumMemSDNodeBits = NumSDNodeBits + 4 };\n\n  class LSBaseSDNodeBitfields {\n    friend class LSBaseSDNode;\n    friend class MaskedLoadStoreSDNode;\n    friend class MaskedGatherScatterSDNode;\n\n    uint16_t : NumMemSDNodeBits;\n\n    // This storage is shared between disparate class hierarchies to hold an\n    // enumeration specific to the class hierarchy in use.\n    //   LSBaseSDNode => enum ISD::MemIndexedMode\n    //   MaskedLoadStoreBaseSDNode => enum ISD::MemIndexedMode\n    //   MaskedGatherScatterSDNode => enum ISD::MemIndexType\n    uint16_t AddressingMode : 3;\n  };\n  enum { NumLSBaseSDNodeBits = NumMemSDNodeBits + 3 };\n\n  class LoadSDNodeBitfields {\n    friend class LoadSDNode;\n    friend class MaskedLoadSDNode;\n    friend class MaskedGatherSDNode;\n\n    uint16_t : NumLSBaseSDNodeBits;\n\n    uint16_t ExtTy : 2; // enum ISD::LoadExtType\n    uint16_t IsExpanding : 1;\n  };\n\n  class StoreSDNodeBitfields {\n    friend class StoreSDNode;\n    friend class MaskedStoreSDNode;\n    friend class MaskedScatterSDNode;\n\n    uint16_t : NumLSBaseSDNodeBits;\n\n    uint16_t IsTruncating : 1;\n    uint16_t IsCompressing : 1;\n  };\n\n  union {\n    char RawSDNodeBits[sizeof(uint16_t)];\n    SDNodeBitfields SDNodeBits;\n    ConstantSDNodeBitfields ConstantSDNodeBits;\n    MemSDNodeBitfields MemSDNodeBits;\n    LSBaseSDNodeBitfields LSBaseSDNodeBits;\n    LoadSDNodeBitfields LoadSDNodeBits;\n    StoreSDNodeBitfields StoreSDNodeBits;\n  };\nEND_TWO_BYTE_PACK()\n#undef BEGIN_TWO_BYTE_PACK\n#undef END_TWO_BYTE_PACK\n\n  // RawSDNodeBits must cover the entirety of the union.  This means that all of\n  // the union's members must have size <= RawSDNodeBits.  We write the RHS as\n  // \"2\" instead of sizeof(RawSDNodeBits) because MSVC can't handle the latter.\n  static_assert(sizeof(SDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(ConstantSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(MemSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(LSBaseSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(LoadSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(StoreSDNodeBitfields) <= 2, \"field too wide\");\n\nprivate:\n  friend class SelectionDAG;\n  // TODO: unfriend HandleSDNode once we fix its operand handling.\n  friend class HandleSDNode;\n\n  /// Unique id per SDNode in the DAG.\n  int NodeId = -1;\n\n  /// The values that are used by this operation.\n  SDUse *OperandList = nullptr;\n\n  /// The types of the values this node defines.  SDNode's may\n  /// define multiple values simultaneously.\n  const EVT *ValueList;\n\n  /// List of uses for this SDNode.\n  SDUse *UseList = nullptr;\n\n  /// The number of entries in the Operand/Value list.\n  unsigned short NumOperands = 0;\n  unsigned short NumValues;\n\n  // The ordering of the SDNodes. It roughly corresponds to the ordering of the\n  // original LLVM instructions.\n  // This is used for turning off scheduling, because we'll forgo\n  // the normal scheduling algorithms and output the instructions according to\n  // this ordering.\n  unsigned IROrder;\n\n  /// Source line information.\n  DebugLoc debugLoc;\n\n  /// Return a pointer to the specified value type.\n  static const EVT *getValueTypeList(EVT VT);\n\n  SDNodeFlags Flags;\n\npublic:\n  /// Unique and persistent id per SDNode in the DAG.\n  /// Used for debug printing.\n  uint16_t PersistentId;\n\n  //===--------------------------------------------------------------------===//\n  //  Accessors\n  //\n\n  /// Return the SelectionDAG opcode value for this node. For\n  /// pre-isel nodes (those for which isMachineOpcode returns false), these\n  /// are the opcode values in the ISD and <target>ISD namespaces. For\n  /// post-isel opcodes, see getMachineOpcode.\n  unsigned getOpcode()  const { return (unsigned short)NodeType; }\n\n  /// Test if this node has a target-specific opcode (in the\n  /// \\<target\\>ISD namespace).\n  bool isTargetOpcode() const { return NodeType >= ISD::BUILTIN_OP_END; }\n\n  /// Test if this node has a target-specific opcode that may raise\n  /// FP exceptions (in the \\<target\\>ISD namespace and greater than\n  /// FIRST_TARGET_STRICTFP_OPCODE).  Note that all target memory\n  /// opcode are currently automatically considered to possibly raise\n  /// FP exceptions as well.\n  bool isTargetStrictFPOpcode() const {\n    return NodeType >= ISD::FIRST_TARGET_STRICTFP_OPCODE;\n  }\n\n  /// Test if this node has a target-specific\n  /// memory-referencing opcode (in the \\<target\\>ISD namespace and\n  /// greater than FIRST_TARGET_MEMORY_OPCODE).\n  bool isTargetMemoryOpcode() const {\n    return NodeType >= ISD::FIRST_TARGET_MEMORY_OPCODE;\n  }\n\n  /// Return true if the type of the node type undefined.\n  bool isUndef() const { return NodeType == ISD::UNDEF; }\n\n  /// Test if this node is a memory intrinsic (with valid pointer information).\n  /// INTRINSIC_W_CHAIN and INTRINSIC_VOID nodes are sometimes created for\n  /// non-memory intrinsics (with chains) that are not really instances of\n  /// MemSDNode. For such nodes, we need some extra state to determine the\n  /// proper classof relationship.\n  bool isMemIntrinsic() const {\n    return (NodeType == ISD::INTRINSIC_W_CHAIN ||\n            NodeType == ISD::INTRINSIC_VOID) &&\n           SDNodeBits.IsMemIntrinsic;\n  }\n\n  /// Test if this node is a strict floating point pseudo-op.\n  bool isStrictFPOpcode() {\n    switch (NodeType) {\n      default:\n        return false;\n      case ISD::STRICT_FP16_TO_FP:\n      case ISD::STRICT_FP_TO_FP16:\n#define DAG_INSTRUCTION(NAME, NARG, ROUND_MODE, INTRINSIC, DAGN)               \\\n      case ISD::STRICT_##DAGN:\n#include \"llvm/IR/ConstrainedOps.def\"\n        return true;\n    }\n  }\n\n  /// Test if this node has a post-isel opcode, directly\n  /// corresponding to a MachineInstr opcode.\n  bool isMachineOpcode() const { return NodeType < 0; }\n\n  /// This may only be called if isMachineOpcode returns\n  /// true. It returns the MachineInstr opcode value that the node's opcode\n  /// corresponds to.\n  unsigned getMachineOpcode() const {\n    assert(isMachineOpcode() && \"Not a MachineInstr opcode!\");\n    return ~NodeType;\n  }\n\n  bool getHasDebugValue() const { return SDNodeBits.HasDebugValue; }\n  void setHasDebugValue(bool b) { SDNodeBits.HasDebugValue = b; }\n\n  bool isDivergent() const { return SDNodeBits.IsDivergent; }\n\n  /// Return true if there are no uses of this node.\n  bool use_empty() const { return UseList == nullptr; }\n\n  /// Return true if there is exactly one use of this node.\n  bool hasOneUse() const { return hasSingleElement(uses()); }\n\n  /// Return the number of uses of this node. This method takes\n  /// time proportional to the number of uses.\n  size_t use_size() const { return std::distance(use_begin(), use_end()); }\n\n  /// Return the unique node id.\n  int getNodeId() const { return NodeId; }\n\n  /// Set unique node id.\n  void setNodeId(int Id) { NodeId = Id; }\n\n  /// Return the node ordering.\n  unsigned getIROrder() const { return IROrder; }\n\n  /// Set the node ordering.\n  void setIROrder(unsigned Order) { IROrder = Order; }\n\n  /// Return the source location info.\n  const DebugLoc &getDebugLoc() const { return debugLoc; }\n\n  /// Set source location info.  Try to avoid this, putting\n  /// it in the constructor is preferable.\n  void setDebugLoc(DebugLoc dl) { debugLoc = std::move(dl); }\n\n  /// This class provides iterator support for SDUse\n  /// operands that use a specific SDNode.\n  class use_iterator\n    : public std::iterator<std::forward_iterator_tag, SDUse, ptrdiff_t> {\n    friend class SDNode;\n\n    SDUse *Op = nullptr;\n\n    explicit use_iterator(SDUse *op) : Op(op) {}\n\n  public:\n    using reference = std::iterator<std::forward_iterator_tag,\n                                    SDUse, ptrdiff_t>::reference;\n    using pointer = std::iterator<std::forward_iterator_tag,\n                                  SDUse, ptrdiff_t>::pointer;\n\n    use_iterator() = default;\n    use_iterator(const use_iterator &I) : Op(I.Op) {}\n\n    bool operator==(const use_iterator &x) const {\n      return Op == x.Op;\n    }\n    bool operator!=(const use_iterator &x) const {\n      return !operator==(x);\n    }\n\n    /// Return true if this iterator is at the end of uses list.\n    bool atEnd() const { return Op == nullptr; }\n\n    // Iterator traversal: forward iteration only.\n    use_iterator &operator++() {          // Preincrement\n      assert(Op && \"Cannot increment end iterator!\");\n      Op = Op->getNext();\n      return *this;\n    }\n\n    use_iterator operator++(int) {        // Postincrement\n      use_iterator tmp = *this; ++*this; return tmp;\n    }\n\n    /// Retrieve a pointer to the current user node.\n    SDNode *operator*() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      return Op->getUser();\n    }\n\n    SDNode *operator->() const { return operator*(); }\n\n    SDUse &getUse() const { return *Op; }\n\n    /// Retrieve the operand # of this use in its user.\n    unsigned getOperandNo() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      return (unsigned)(Op - Op->getUser()->OperandList);\n    }\n  };\n\n  /// Provide iteration support to walk over all uses of an SDNode.\n  use_iterator use_begin() const {\n    return use_iterator(UseList);\n  }\n\n  static use_iterator use_end() { return use_iterator(nullptr); }\n\n  inline iterator_range<use_iterator> uses() {\n    return make_range(use_begin(), use_end());\n  }\n  inline iterator_range<use_iterator> uses() const {\n    return make_range(use_begin(), use_end());\n  }\n\n  /// Return true if there are exactly NUSES uses of the indicated value.\n  /// This method ignores uses of other values defined by this operation.\n  bool hasNUsesOfValue(unsigned NUses, unsigned Value) const;\n\n  /// Return true if there are any use of the indicated value.\n  /// This method ignores uses of other values defined by this operation.\n  bool hasAnyUseOfValue(unsigned Value) const;\n\n  /// Return true if this node is the only use of N.\n  bool isOnlyUserOf(const SDNode *N) const;\n\n  /// Return true if this node is an operand of N.\n  bool isOperandOf(const SDNode *N) const;\n\n  /// Return true if this node is a predecessor of N.\n  /// NOTE: Implemented on top of hasPredecessor and every bit as\n  /// expensive. Use carefully.\n  bool isPredecessorOf(const SDNode *N) const {\n    return N->hasPredecessor(this);\n  }\n\n  /// Return true if N is a predecessor of this node.\n  /// N is either an operand of this node, or can be reached by recursively\n  /// traversing up the operands.\n  /// NOTE: This is an expensive method. Use it carefully.\n  bool hasPredecessor(const SDNode *N) const;\n\n  /// Returns true if N is a predecessor of any node in Worklist. This\n  /// helper keeps Visited and Worklist sets externally to allow unions\n  /// searches to be performed in parallel, caching of results across\n  /// queries and incremental addition to Worklist. Stops early if N is\n  /// found but will resume. Remember to clear Visited and Worklists\n  /// if DAG changes. MaxSteps gives a maximum number of nodes to visit before\n  /// giving up. The TopologicalPrune flag signals that positive NodeIds are\n  /// topologically ordered (Operands have strictly smaller node id) and search\n  /// can be pruned leveraging this.\n  static bool hasPredecessorHelper(const SDNode *N,\n                                   SmallPtrSetImpl<const SDNode *> &Visited,\n                                   SmallVectorImpl<const SDNode *> &Worklist,\n                                   unsigned int MaxSteps = 0,\n                                   bool TopologicalPrune = false) {\n    SmallVector<const SDNode *, 8> DeferredNodes;\n    if (Visited.count(N))\n      return true;\n\n    // Node Id's are assigned in three places: As a topological\n    // ordering (> 0), during legalization (results in values set to\n    // 0), new nodes (set to -1). If N has a topolgical id then we\n    // know that all nodes with ids smaller than it cannot be\n    // successors and we need not check them. Filter out all node\n    // that can't be matches. We add them to the worklist before exit\n    // in case of multiple calls. Note that during selection the topological id\n    // may be violated if a node's predecessor is selected before it. We mark\n    // this at selection negating the id of unselected successors and\n    // restricting topological pruning to positive ids.\n\n    int NId = N->getNodeId();\n    // If we Invalidated the Id, reconstruct original NId.\n    if (NId < -1)\n      NId = -(NId + 1);\n\n    bool Found = false;\n    while (!Worklist.empty()) {\n      const SDNode *M = Worklist.pop_back_val();\n      int MId = M->getNodeId();\n      if (TopologicalPrune && M->getOpcode() != ISD::TokenFactor && (NId > 0) &&\n          (MId > 0) && (MId < NId)) {\n        DeferredNodes.push_back(M);\n        continue;\n      }\n      for (const SDValue &OpV : M->op_values()) {\n        SDNode *Op = OpV.getNode();\n        if (Visited.insert(Op).second)\n          Worklist.push_back(Op);\n        if (Op == N)\n          Found = true;\n      }\n      if (Found)\n        break;\n      if (MaxSteps != 0 && Visited.size() >= MaxSteps)\n        break;\n    }\n    // Push deferred nodes back on worklist.\n    Worklist.append(DeferredNodes.begin(), DeferredNodes.end());\n    // If we bailed early, conservatively return found.\n    if (MaxSteps != 0 && Visited.size() >= MaxSteps)\n      return true;\n    return Found;\n  }\n\n  /// Return true if all the users of N are contained in Nodes.\n  /// NOTE: Requires at least one match, but doesn't require them all.\n  static bool areOnlyUsersOf(ArrayRef<const SDNode *> Nodes, const SDNode *N);\n\n  /// Return the number of values used by this operation.\n  unsigned getNumOperands() const { return NumOperands; }\n\n  /// Return the maximum number of operands that a SDNode can hold.\n  static constexpr size_t getMaxNumOperands() {\n    return std::numeric_limits<decltype(SDNode::NumOperands)>::max();\n  }\n\n  /// Helper method returns the integer value of a ConstantSDNode operand.\n  inline uint64_t getConstantOperandVal(unsigned Num) const;\n\n  /// Helper method returns the APInt of a ConstantSDNode operand.\n  inline const APInt &getConstantOperandAPInt(unsigned Num) const;\n\n  const SDValue &getOperand(unsigned Num) const {\n    assert(Num < NumOperands && \"Invalid child # of SDNode!\");\n    return OperandList[Num];\n  }\n\n  using op_iterator = SDUse *;\n\n  op_iterator op_begin() const { return OperandList; }\n  op_iterator op_end() const { return OperandList+NumOperands; }\n  ArrayRef<SDUse> ops() const { return makeArrayRef(op_begin(), op_end()); }\n\n  /// Iterator for directly iterating over the operand SDValue's.\n  struct value_op_iterator\n      : iterator_adaptor_base<value_op_iterator, op_iterator,\n                              std::random_access_iterator_tag, SDValue,\n                              ptrdiff_t, value_op_iterator *,\n                              value_op_iterator *> {\n    explicit value_op_iterator(SDUse *U = nullptr)\n      : iterator_adaptor_base(U) {}\n\n    const SDValue &operator*() const { return I->get(); }\n  };\n\n  iterator_range<value_op_iterator> op_values() const {\n    return make_range(value_op_iterator(op_begin()),\n                      value_op_iterator(op_end()));\n  }\n\n  SDVTList getVTList() const {\n    SDVTList X = { ValueList, NumValues };\n    return X;\n  }\n\n  /// If this node has a glue operand, return the node\n  /// to which the glue operand points. Otherwise return NULL.\n  SDNode *getGluedNode() const {\n    if (getNumOperands() != 0 &&\n        getOperand(getNumOperands()-1).getValueType() == MVT::Glue)\n      return getOperand(getNumOperands()-1).getNode();\n    return nullptr;\n  }\n\n  /// If this node has a glue value with a user, return\n  /// the user (there is at most one). Otherwise return NULL.\n  SDNode *getGluedUser() const {\n    for (use_iterator UI = use_begin(), UE = use_end(); UI != UE; ++UI)\n      if (UI.getUse().get().getValueType() == MVT::Glue)\n        return *UI;\n    return nullptr;\n  }\n\n  SDNodeFlags getFlags() const { return Flags; }\n  void setFlags(SDNodeFlags NewFlags) { Flags = NewFlags; }\n\n  /// Clear any flags in this node that aren't also set in Flags.\n  /// If Flags is not in a defined state then this has no effect.\n  void intersectFlagsWith(const SDNodeFlags Flags);\n\n  /// Return the number of values defined/returned by this operator.\n  unsigned getNumValues() const { return NumValues; }\n\n  /// Return the type of a specified result.\n  EVT getValueType(unsigned ResNo) const {\n    assert(ResNo < NumValues && \"Illegal result number!\");\n    return ValueList[ResNo];\n  }\n\n  /// Return the type of a specified result as a simple type.\n  MVT getSimpleValueType(unsigned ResNo) const {\n    return getValueType(ResNo).getSimpleVT();\n  }\n\n  /// Returns MVT::getSizeInBits(getValueType(ResNo)).\n  ///\n  /// If the value type is a scalable vector type, the scalable property will\n  /// be set and the runtime size will be a positive integer multiple of the\n  /// base size.\n  TypeSize getValueSizeInBits(unsigned ResNo) const {\n    return getValueType(ResNo).getSizeInBits();\n  }\n\n  using value_iterator = const EVT *;\n\n  value_iterator value_begin() const { return ValueList; }\n  value_iterator value_end() const { return ValueList+NumValues; }\n  iterator_range<value_iterator> values() const {\n    return llvm::make_range(value_begin(), value_end());\n  }\n\n  /// Return the opcode of this operation for printing.\n  std::string getOperationName(const SelectionDAG *G = nullptr) const;\n  static const char* getIndexedModeName(ISD::MemIndexedMode AM);\n  void print_types(raw_ostream &OS, const SelectionDAG *G) const;\n  void print_details(raw_ostream &OS, const SelectionDAG *G) const;\n  void print(raw_ostream &OS, const SelectionDAG *G = nullptr) const;\n  void printr(raw_ostream &OS, const SelectionDAG *G = nullptr) const;\n\n  /// Print a SelectionDAG node and all children down to\n  /// the leaves.  The given SelectionDAG allows target-specific nodes\n  /// to be printed in human-readable form.  Unlike printr, this will\n  /// print the whole DAG, including children that appear multiple\n  /// times.\n  ///\n  void printrFull(raw_ostream &O, const SelectionDAG *G = nullptr) const;\n\n  /// Print a SelectionDAG node and children up to\n  /// depth \"depth.\"  The given SelectionDAG allows target-specific\n  /// nodes to be printed in human-readable form.  Unlike printr, this\n  /// will print children that appear multiple times wherever they are\n  /// used.\n  ///\n  void printrWithDepth(raw_ostream &O, const SelectionDAG *G = nullptr,\n                       unsigned depth = 100) const;\n\n  /// Dump this node, for debugging.\n  void dump() const;\n\n  /// Dump (recursively) this node and its use-def subgraph.\n  void dumpr() const;\n\n  /// Dump this node, for debugging.\n  /// The given SelectionDAG allows target-specific nodes to be printed\n  /// in human-readable form.\n  void dump(const SelectionDAG *G) const;\n\n  /// Dump (recursively) this node and its use-def subgraph.\n  /// The given SelectionDAG allows target-specific nodes to be printed\n  /// in human-readable form.\n  void dumpr(const SelectionDAG *G) const;\n\n  /// printrFull to dbgs().  The given SelectionDAG allows\n  /// target-specific nodes to be printed in human-readable form.\n  /// Unlike dumpr, this will print the whole DAG, including children\n  /// that appear multiple times.\n  void dumprFull(const SelectionDAG *G = nullptr) const;\n\n  /// printrWithDepth to dbgs().  The given\n  /// SelectionDAG allows target-specific nodes to be printed in\n  /// human-readable form.  Unlike dumpr, this will print children\n  /// that appear multiple times wherever they are used.\n  ///\n  void dumprWithDepth(const SelectionDAG *G = nullptr,\n                      unsigned depth = 100) const;\n\n  /// Gather unique data for the node.\n  void Profile(FoldingSetNodeID &ID) const;\n\n  /// This method should only be used by the SDUse class.\n  void addUse(SDUse &U) { U.addToList(&UseList); }\n\nprotected:\n  static SDVTList getSDVTList(EVT VT) {\n    SDVTList Ret = { getValueTypeList(VT), 1 };\n    return Ret;\n  }\n\n  /// Create an SDNode.\n  ///\n  /// SDNodes are created without any operands, and never own the operand\n  /// storage. To add operands, see SelectionDAG::createOperands.\n  SDNode(unsigned Opc, unsigned Order, DebugLoc dl, SDVTList VTs)\n      : NodeType(Opc), ValueList(VTs.VTs), NumValues(VTs.NumVTs),\n        IROrder(Order), debugLoc(std::move(dl)) {\n    memset(&RawSDNodeBits, 0, sizeof(RawSDNodeBits));\n    assert(debugLoc.hasTrivialDestructor() && \"Expected trivial destructor\");\n    assert(NumValues == VTs.NumVTs &&\n           \"NumValues wasn't wide enough for its operands!\");\n  }\n\n  /// Release the operands and set this node to have zero operands.\n  void DropOperands();\n};\n\n/// Wrapper class for IR location info (IR ordering and DebugLoc) to be passed\n/// into SDNode creation functions.\n/// When an SDNode is created from the DAGBuilder, the DebugLoc is extracted\n/// from the original Instruction, and IROrder is the ordinal position of\n/// the instruction.\n/// When an SDNode is created after the DAG is being built, both DebugLoc and\n/// the IROrder are propagated from the original SDNode.\n/// So SDLoc class provides two constructors besides the default one, one to\n/// be used by the DAGBuilder, the other to be used by others.\nclass SDLoc {\nprivate:\n  DebugLoc DL;\n  int IROrder = 0;\n\npublic:\n  SDLoc() = default;\n  SDLoc(const SDNode *N) : DL(N->getDebugLoc()), IROrder(N->getIROrder()) {}\n  SDLoc(const SDValue V) : SDLoc(V.getNode()) {}\n  SDLoc(const Instruction *I, int Order) : IROrder(Order) {\n    assert(Order >= 0 && \"bad IROrder\");\n    if (I)\n      DL = I->getDebugLoc();\n  }\n\n  unsigned getIROrder() const { return IROrder; }\n  const DebugLoc &getDebugLoc() const { return DL; }\n};\n\n// Define inline functions from the SDValue class.\n\ninline SDValue::SDValue(SDNode *node, unsigned resno)\n    : Node(node), ResNo(resno) {\n  // Explicitly check for !ResNo to avoid use-after-free, because there are\n  // callers that use SDValue(N, 0) with a deleted N to indicate successful\n  // combines.\n  assert((!Node || !ResNo || ResNo < Node->getNumValues()) &&\n         \"Invalid result number for the given node!\");\n  assert(ResNo < -2U && \"Cannot use result numbers reserved for DenseMaps.\");\n}\n\ninline unsigned SDValue::getOpcode() const {\n  return Node->getOpcode();\n}\n\ninline EVT SDValue::getValueType() const {\n  return Node->getValueType(ResNo);\n}\n\ninline unsigned SDValue::getNumOperands() const {\n  return Node->getNumOperands();\n}\n\ninline const SDValue &SDValue::getOperand(unsigned i) const {\n  return Node->getOperand(i);\n}\n\ninline uint64_t SDValue::getConstantOperandVal(unsigned i) const {\n  return Node->getConstantOperandVal(i);\n}\n\ninline const APInt &SDValue::getConstantOperandAPInt(unsigned i) const {\n  return Node->getConstantOperandAPInt(i);\n}\n\ninline bool SDValue::isTargetOpcode() const {\n  return Node->isTargetOpcode();\n}\n\ninline bool SDValue::isTargetMemoryOpcode() const {\n  return Node->isTargetMemoryOpcode();\n}\n\ninline bool SDValue::isMachineOpcode() const {\n  return Node->isMachineOpcode();\n}\n\ninline unsigned SDValue::getMachineOpcode() const {\n  return Node->getMachineOpcode();\n}\n\ninline bool SDValue::isUndef() const {\n  return Node->isUndef();\n}\n\ninline bool SDValue::use_empty() const {\n  return !Node->hasAnyUseOfValue(ResNo);\n}\n\ninline bool SDValue::hasOneUse() const {\n  return Node->hasNUsesOfValue(1, ResNo);\n}\n\ninline const DebugLoc &SDValue::getDebugLoc() const {\n  return Node->getDebugLoc();\n}\n\ninline void SDValue::dump() const {\n  return Node->dump();\n}\n\ninline void SDValue::dump(const SelectionDAG *G) const {\n  return Node->dump(G);\n}\n\ninline void SDValue::dumpr() const {\n  return Node->dumpr();\n}\n\ninline void SDValue::dumpr(const SelectionDAG *G) const {\n  return Node->dumpr(G);\n}\n\n// Define inline functions from the SDUse class.\n\ninline void SDUse::set(const SDValue &V) {\n  if (Val.getNode()) removeFromList();\n  Val = V;\n  if (V.getNode()) V.getNode()->addUse(*this);\n}\n\ninline void SDUse::setInitial(const SDValue &V) {\n  Val = V;\n  V.getNode()->addUse(*this);\n}\n\ninline void SDUse::setNode(SDNode *N) {\n  if (Val.getNode()) removeFromList();\n  Val.setNode(N);\n  if (N) N->addUse(*this);\n}\n\n/// This class is used to form a handle around another node that\n/// is persistent and is updated across invocations of replaceAllUsesWith on its\n/// operand.  This node should be directly created by end-users and not added to\n/// the AllNodes list.\nclass HandleSDNode : public SDNode {\n  SDUse Op;\n\npublic:\n  explicit HandleSDNode(SDValue X)\n    : SDNode(ISD::HANDLENODE, 0, DebugLoc(), getSDVTList(MVT::Other)) {\n    // HandleSDNodes are never inserted into the DAG, so they won't be\n    // auto-numbered. Use ID 65535 as a sentinel.\n    PersistentId = 0xffff;\n\n    // Manually set up the operand list. This node type is special in that it's\n    // always stack allocated and SelectionDAG does not manage its operands.\n    // TODO: This should either (a) not be in the SDNode hierarchy, or (b) not\n    // be so special.\n    Op.setUser(this);\n    Op.setInitial(X);\n    NumOperands = 1;\n    OperandList = &Op;\n  }\n  ~HandleSDNode();\n\n  const SDValue &getValue() const { return Op; }\n};\n\nclass AddrSpaceCastSDNode : public SDNode {\nprivate:\n  unsigned SrcAddrSpace;\n  unsigned DestAddrSpace;\n\npublic:\n  AddrSpaceCastSDNode(unsigned Order, const DebugLoc &dl, EVT VT,\n                      unsigned SrcAS, unsigned DestAS);\n\n  unsigned getSrcAddressSpace() const { return SrcAddrSpace; }\n  unsigned getDestAddressSpace() const { return DestAddrSpace; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ADDRSPACECAST;\n  }\n};\n\n/// This is an abstract virtual class for memory operations.\nclass MemSDNode : public SDNode {\nprivate:\n  // VT of in-memory value.\n  EVT MemoryVT;\n\nprotected:\n  /// Memory reference information.\n  MachineMemOperand *MMO;\n\npublic:\n  MemSDNode(unsigned Opc, unsigned Order, const DebugLoc &dl, SDVTList VTs,\n            EVT memvt, MachineMemOperand *MMO);\n\n  bool readMem() const { return MMO->isLoad(); }\n  bool writeMem() const { return MMO->isStore(); }\n\n  /// Returns alignment and volatility of the memory access\n  Align getOriginalAlign() const { return MMO->getBaseAlign(); }\n  Align getAlign() const { return MMO->getAlign(); }\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned getOriginalAlignment() const,\n                            \"Use getOriginalAlign() instead\") {\n    return MMO->getBaseAlign().value();\n  }\n  // FIXME: Remove once transition to getAlign is over.\n  unsigned getAlignment() const { return MMO->getAlign().value(); }\n\n  /// Return the SubclassData value, without HasDebugValue. This contains an\n  /// encoding of the volatile flag, as well as bits used by subclasses. This\n  /// function should only be used to compute a FoldingSetNodeID value.\n  /// The HasDebugValue bit is masked out because CSE map needs to match\n  /// nodes with debug info with nodes without debug info. Same is about\n  /// isDivergent bit.\n  unsigned getRawSubclassData() const {\n    uint16_t Data;\n    union {\n      char RawSDNodeBits[sizeof(uint16_t)];\n      SDNodeBitfields SDNodeBits;\n    };\n    memcpy(&RawSDNodeBits, &this->RawSDNodeBits, sizeof(this->RawSDNodeBits));\n    SDNodeBits.HasDebugValue = 0;\n    SDNodeBits.IsDivergent = false;\n    memcpy(&Data, &RawSDNodeBits, sizeof(RawSDNodeBits));\n    return Data;\n  }\n\n  bool isVolatile() const { return MemSDNodeBits.IsVolatile; }\n  bool isNonTemporal() const { return MemSDNodeBits.IsNonTemporal; }\n  bool isDereferenceable() const { return MemSDNodeBits.IsDereferenceable; }\n  bool isInvariant() const { return MemSDNodeBits.IsInvariant; }\n\n  // Returns the offset from the location of the access.\n  int64_t getSrcValueOffset() const { return MMO->getOffset(); }\n\n  /// Returns the AA info that describes the dereference.\n  AAMDNodes getAAInfo() const { return MMO->getAAInfo(); }\n\n  /// Returns the Ranges that describes the dereference.\n  const MDNode *getRanges() const { return MMO->getRanges(); }\n\n  /// Returns the synchronization scope ID for this memory operation.\n  SyncScope::ID getSyncScopeID() const { return MMO->getSyncScopeID(); }\n\n  /// Return the atomic ordering requirements for this memory operation. For\n  /// cmpxchg atomic operations, return the atomic ordering requirements when\n  /// store occurs.\n  AtomicOrdering getOrdering() const { return MMO->getOrdering(); }\n\n  /// Return true if the memory operation ordering is Unordered or higher.\n  bool isAtomic() const { return MMO->isAtomic(); }\n\n  /// Returns true if the memory operation doesn't imply any ordering\n  /// constraints on surrounding memory operations beyond the normal memory\n  /// aliasing rules.\n  bool isUnordered() const { return MMO->isUnordered(); }\n\n  /// Returns true if the memory operation is neither atomic or volatile.\n  bool isSimple() const { return !isAtomic() && !isVolatile(); }\n\n  /// Return the type of the in-memory value.\n  EVT getMemoryVT() const { return MemoryVT; }\n\n  /// Return a MachineMemOperand object describing the memory\n  /// reference performed by operation.\n  MachineMemOperand *getMemOperand() const { return MMO; }\n\n  const MachinePointerInfo &getPointerInfo() const {\n    return MMO->getPointerInfo();\n  }\n\n  /// Return the address space for the associated pointer\n  unsigned getAddressSpace() const {\n    return getPointerInfo().getAddrSpace();\n  }\n\n  /// Update this MemSDNode's MachineMemOperand information\n  /// to reflect the alignment of NewMMO, if it has a greater alignment.\n  /// This must only be used when the new alignment applies to all users of\n  /// this MachineMemOperand.\n  void refineAlignment(const MachineMemOperand *NewMMO) {\n    MMO->refineAlignment(NewMMO);\n  }\n\n  const SDValue &getChain() const { return getOperand(0); }\n\n  const SDValue &getBasePtr() const {\n    switch (getOpcode()) {\n    case ISD::STORE:\n    case ISD::MSTORE:\n      return getOperand(2);\n    case ISD::MGATHER:\n    case ISD::MSCATTER:\n      return getOperand(3);\n    default:\n      return getOperand(1);\n    }\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    // For some targets, we lower some target intrinsics to a MemIntrinsicNode\n    // with either an intrinsic or a target opcode.\n    return N->getOpcode() == ISD::LOAD                ||\n           N->getOpcode() == ISD::STORE               ||\n           N->getOpcode() == ISD::PREFETCH            ||\n           N->getOpcode() == ISD::ATOMIC_CMP_SWAP     ||\n           N->getOpcode() == ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS ||\n           N->getOpcode() == ISD::ATOMIC_SWAP         ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_ADD     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_SUB     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_AND     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_CLR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_OR      ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_XOR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_NAND    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MIN     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MAX     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMIN    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMAX    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FADD    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FSUB    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD         ||\n           N->getOpcode() == ISD::ATOMIC_STORE        ||\n           N->getOpcode() == ISD::MLOAD               ||\n           N->getOpcode() == ISD::MSTORE              ||\n           N->getOpcode() == ISD::MGATHER             ||\n           N->getOpcode() == ISD::MSCATTER            ||\n           N->isMemIntrinsic()                        ||\n           N->isTargetMemoryOpcode();\n  }\n};\n\n/// This is an SDNode representing atomic operations.\nclass AtomicSDNode : public MemSDNode {\npublic:\n  AtomicSDNode(unsigned Opc, unsigned Order, const DebugLoc &dl, SDVTList VTL,\n               EVT MemVT, MachineMemOperand *MMO)\n    : MemSDNode(Opc, Order, dl, VTL, MemVT, MMO) {\n    assert(((Opc != ISD::ATOMIC_LOAD && Opc != ISD::ATOMIC_STORE) ||\n            MMO->isAtomic()) && \"then why are we using an AtomicSDNode?\");\n  }\n\n  const SDValue &getBasePtr() const { return getOperand(1); }\n  const SDValue &getVal() const { return getOperand(2); }\n\n  /// Returns true if this SDNode represents cmpxchg atomic operation, false\n  /// otherwise.\n  bool isCompareAndSwap() const {\n    unsigned Op = getOpcode();\n    return Op == ISD::ATOMIC_CMP_SWAP ||\n           Op == ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS;\n  }\n\n  /// For cmpxchg atomic operations, return the atomic ordering requirements\n  /// when store does not occur.\n  AtomicOrdering getFailureOrdering() const {\n    assert(isCompareAndSwap() && \"Must be cmpxchg operation\");\n    return MMO->getFailureOrdering();\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ATOMIC_CMP_SWAP     ||\n           N->getOpcode() == ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS ||\n           N->getOpcode() == ISD::ATOMIC_SWAP         ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_ADD     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_SUB     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_AND     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_CLR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_OR      ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_XOR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_NAND    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MIN     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MAX     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMIN    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMAX    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FADD    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FSUB    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD         ||\n           N->getOpcode() == ISD::ATOMIC_STORE;\n  }\n};\n\n/// This SDNode is used for target intrinsics that touch\n/// memory and need an associated MachineMemOperand. Its opcode may be\n/// INTRINSIC_VOID, INTRINSIC_W_CHAIN, PREFETCH, or a target-specific opcode\n/// with a value not less than FIRST_TARGET_MEMORY_OPCODE.\nclass MemIntrinsicSDNode : public MemSDNode {\npublic:\n  MemIntrinsicSDNode(unsigned Opc, unsigned Order, const DebugLoc &dl,\n                     SDVTList VTs, EVT MemoryVT, MachineMemOperand *MMO)\n      : MemSDNode(Opc, Order, dl, VTs, MemoryVT, MMO) {\n    SDNodeBits.IsMemIntrinsic = true;\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    // We lower some target intrinsics to their target opcode\n    // early a node with a target opcode can be of this class\n    return N->isMemIntrinsic()             ||\n           N->getOpcode() == ISD::PREFETCH ||\n           N->isTargetMemoryOpcode();\n  }\n};\n\n/// This SDNode is used to implement the code generator\n/// support for the llvm IR shufflevector instruction.  It combines elements\n/// from two input vectors into a new input vector, with the selection and\n/// ordering of elements determined by an array of integers, referred to as\n/// the shuffle mask.  For input vectors of width N, mask indices of 0..N-1\n/// refer to elements from the LHS input, and indices from N to 2N-1 the RHS.\n/// An index of -1 is treated as undef, such that the code generator may put\n/// any value in the corresponding element of the result.\nclass ShuffleVectorSDNode : public SDNode {\n  // The memory for Mask is owned by the SelectionDAG's OperandAllocator, and\n  // is freed when the SelectionDAG object is destroyed.\n  const int *Mask;\n\nprotected:\n  friend class SelectionDAG;\n\n  ShuffleVectorSDNode(EVT VT, unsigned Order, const DebugLoc &dl, const int *M)\n      : SDNode(ISD::VECTOR_SHUFFLE, Order, dl, getSDVTList(VT)), Mask(M) {}\n\npublic:\n  ArrayRef<int> getMask() const {\n    EVT VT = getValueType(0);\n    return makeArrayRef(Mask, VT.getVectorNumElements());\n  }\n\n  int getMaskElt(unsigned Idx) const {\n    assert(Idx < getValueType(0).getVectorNumElements() && \"Idx out of range!\");\n    return Mask[Idx];\n  }\n\n  bool isSplat() const { return isSplatMask(Mask, getValueType(0)); }\n\n  int getSplatIndex() const {\n    assert(isSplat() && \"Cannot get splat index for non-splat!\");\n    EVT VT = getValueType(0);\n    for (unsigned i = 0, e = VT.getVectorNumElements(); i != e; ++i)\n      if (Mask[i] >= 0)\n        return Mask[i];\n\n    // We can choose any index value here and be correct because all elements\n    // are undefined. Return 0 for better potential for callers to simplify.\n    return 0;\n  }\n\n  static bool isSplatMask(const int *Mask, EVT VT);\n\n  /// Change values in a shuffle permute mask assuming\n  /// the two vector operands have swapped position.\n  static void commuteMask(MutableArrayRef<int> Mask) {\n    unsigned NumElems = Mask.size();\n    for (unsigned i = 0; i != NumElems; ++i) {\n      int idx = Mask[i];\n      if (idx < 0)\n        continue;\n      else if (idx < (int)NumElems)\n        Mask[i] = idx + NumElems;\n      else\n        Mask[i] = idx - NumElems;\n    }\n  }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::VECTOR_SHUFFLE;\n  }\n};\n\nclass ConstantSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const ConstantInt *Value;\n\n  ConstantSDNode(bool isTarget, bool isOpaque, const ConstantInt *val, EVT VT)\n      : SDNode(isTarget ? ISD::TargetConstant : ISD::Constant, 0, DebugLoc(),\n               getSDVTList(VT)),\n        Value(val) {\n    ConstantSDNodeBits.IsOpaque = isOpaque;\n  }\n\npublic:\n  const ConstantInt *getConstantIntValue() const { return Value; }\n  const APInt &getAPIntValue() const { return Value->getValue(); }\n  uint64_t getZExtValue() const { return Value->getZExtValue(); }\n  int64_t getSExtValue() const { return Value->getSExtValue(); }\n  uint64_t getLimitedValue(uint64_t Limit = UINT64_MAX) {\n    return Value->getLimitedValue(Limit);\n  }\n  MaybeAlign getMaybeAlignValue() const { return Value->getMaybeAlignValue(); }\n  Align getAlignValue() const { return Value->getAlignValue(); }\n\n  bool isOne() const { return Value->isOne(); }\n  bool isNullValue() const { return Value->isZero(); }\n  bool isAllOnesValue() const { return Value->isMinusOne(); }\n\n  bool isOpaque() const { return ConstantSDNodeBits.IsOpaque; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::Constant ||\n           N->getOpcode() == ISD::TargetConstant;\n  }\n};\n\nuint64_t SDNode::getConstantOperandVal(unsigned Num) const {\n  return cast<ConstantSDNode>(getOperand(Num))->getZExtValue();\n}\n\nconst APInt &SDNode::getConstantOperandAPInt(unsigned Num) const {\n  return cast<ConstantSDNode>(getOperand(Num))->getAPIntValue();\n}\n\nclass ConstantFPSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const ConstantFP *Value;\n\n  ConstantFPSDNode(bool isTarget, const ConstantFP *val, EVT VT)\n      : SDNode(isTarget ? ISD::TargetConstantFP : ISD::ConstantFP, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Value(val) {}\n\npublic:\n  const APFloat& getValueAPF() const { return Value->getValueAPF(); }\n  const ConstantFP *getConstantFPValue() const { return Value; }\n\n  /// Return true if the value is positive or negative zero.\n  bool isZero() const { return Value->isZero(); }\n\n  /// Return true if the value is a NaN.\n  bool isNaN() const { return Value->isNaN(); }\n\n  /// Return true if the value is an infinity\n  bool isInfinity() const { return Value->isInfinity(); }\n\n  /// Return true if the value is negative.\n  bool isNegative() const { return Value->isNegative(); }\n\n  /// We don't rely on operator== working on double values, as\n  /// it returns true for things that are clearly not equal, like -0.0 and 0.0.\n  /// As such, this method can be used to do an exact bit-for-bit comparison of\n  /// two floating point values.\n\n  /// We leave the version with the double argument here because it's just so\n  /// convenient to write \"2.0\" and the like.  Without this function we'd\n  /// have to duplicate its logic everywhere it's called.\n  bool isExactlyValue(double V) const {\n    return Value->getValueAPF().isExactlyValue(V);\n  }\n  bool isExactlyValue(const APFloat& V) const;\n\n  static bool isValueValidForType(EVT VT, const APFloat& Val);\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ConstantFP ||\n           N->getOpcode() == ISD::TargetConstantFP;\n  }\n};\n\n/// Returns true if \\p V is a constant integer zero.\nbool isNullConstant(SDValue V);\n\n/// Returns true if \\p V is an FP constant with a value of positive zero.\nbool isNullFPConstant(SDValue V);\n\n/// Returns true if \\p V is an integer constant with all bits set.\nbool isAllOnesConstant(SDValue V);\n\n/// Returns true if \\p V is a constant integer one.\nbool isOneConstant(SDValue V);\n\n/// Return the non-bitcasted source operand of \\p V if it exists.\n/// If \\p V is not a bitcasted value, it is returned as-is.\nSDValue peekThroughBitcasts(SDValue V);\n\n/// Return the non-bitcasted and one-use source operand of \\p V if it exists.\n/// If \\p V is not a bitcasted one-use value, it is returned as-is.\nSDValue peekThroughOneUseBitcasts(SDValue V);\n\n/// Return the non-extracted vector source operand of \\p V if it exists.\n/// If \\p V is not an extracted subvector, it is returned as-is.\nSDValue peekThroughExtractSubvectors(SDValue V);\n\n/// Returns true if \\p V is a bitwise not operation. Assumes that an all ones\n/// constant is canonicalized to be operand 1.\nbool isBitwiseNot(SDValue V, bool AllowUndefs = false);\n\n/// Returns the SDNode if it is a constant splat BuildVector or constant int.\nConstantSDNode *isConstOrConstSplat(SDValue N, bool AllowUndefs = false,\n                                    bool AllowTruncation = false);\n\n/// Returns the SDNode if it is a demanded constant splat BuildVector or\n/// constant int.\nConstantSDNode *isConstOrConstSplat(SDValue N, const APInt &DemandedElts,\n                                    bool AllowUndefs = false,\n                                    bool AllowTruncation = false);\n\n/// Returns the SDNode if it is a constant splat BuildVector or constant float.\nConstantFPSDNode *isConstOrConstSplatFP(SDValue N, bool AllowUndefs = false);\n\n/// Returns the SDNode if it is a demanded constant splat BuildVector or\n/// constant float.\nConstantFPSDNode *isConstOrConstSplatFP(SDValue N, const APInt &DemandedElts,\n                                        bool AllowUndefs = false);\n\n/// Return true if the value is a constant 0 integer or a splatted vector of\n/// a constant 0 integer (with no undefs by default).\n/// Build vector implicit truncation is not an issue for null values.\nbool isNullOrNullSplat(SDValue V, bool AllowUndefs = false);\n\n/// Return true if the value is a constant 1 integer or a splatted vector of a\n/// constant 1 integer (with no undefs).\n/// Does not permit build vector implicit truncation.\nbool isOneOrOneSplat(SDValue V, bool AllowUndefs = false);\n\n/// Return true if the value is a constant -1 integer or a splatted vector of a\n/// constant -1 integer (with no undefs).\n/// Does not permit build vector implicit truncation.\nbool isAllOnesOrAllOnesSplat(SDValue V, bool AllowUndefs = false);\n\nclass GlobalAddressSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const GlobalValue *TheGlobal;\n  int64_t Offset;\n  unsigned TargetFlags;\n\n  GlobalAddressSDNode(unsigned Opc, unsigned Order, const DebugLoc &DL,\n                      const GlobalValue *GA, EVT VT, int64_t o,\n                      unsigned TF);\n\npublic:\n  const GlobalValue *getGlobal() const { return TheGlobal; }\n  int64_t getOffset() const { return Offset; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n  // Return the address space this GlobalAddress belongs to.\n  unsigned getAddressSpace() const;\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::GlobalAddress ||\n           N->getOpcode() == ISD::TargetGlobalAddress ||\n           N->getOpcode() == ISD::GlobalTLSAddress ||\n           N->getOpcode() == ISD::TargetGlobalTLSAddress;\n  }\n};\n\nclass FrameIndexSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  int FI;\n\n  FrameIndexSDNode(int fi, EVT VT, bool isTarg)\n    : SDNode(isTarg ? ISD::TargetFrameIndex : ISD::FrameIndex,\n      0, DebugLoc(), getSDVTList(VT)), FI(fi) {\n  }\n\npublic:\n  int getIndex() const { return FI; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::FrameIndex ||\n           N->getOpcode() == ISD::TargetFrameIndex;\n  }\n};\n\n/// This SDNode is used for LIFETIME_START/LIFETIME_END values, which indicate\n/// the offet and size that are started/ended in the underlying FrameIndex.\nclass LifetimeSDNode : public SDNode {\n  friend class SelectionDAG;\n  int64_t Size;\n  int64_t Offset; // -1 if offset is unknown.\n\n  LifetimeSDNode(unsigned Opcode, unsigned Order, const DebugLoc &dl,\n                 SDVTList VTs, int64_t Size, int64_t Offset)\n      : SDNode(Opcode, Order, dl, VTs), Size(Size), Offset(Offset) {}\npublic:\n  int64_t getFrameIndex() const {\n    return cast<FrameIndexSDNode>(getOperand(1))->getIndex();\n  }\n\n  bool hasOffset() const { return Offset >= 0; }\n  int64_t getOffset() const {\n    assert(hasOffset() && \"offset is unknown\");\n    return Offset;\n  }\n  int64_t getSize() const {\n    assert(hasOffset() && \"offset is unknown\");\n    return Size;\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::LIFETIME_START ||\n           N->getOpcode() == ISD::LIFETIME_END;\n  }\n};\n\n/// This SDNode is used for PSEUDO_PROBE values, which are the function guid and\n/// the index of the basic block being probed. A pseudo probe serves as a place\n/// holder and will be removed at the end of compilation. It does not have any\n/// operand because we do not want the instruction selection to deal with any.\nclass PseudoProbeSDNode : public SDNode {\n  friend class SelectionDAG;\n  uint64_t Guid;\n  uint64_t Index;\n  uint32_t Attributes;\n\n  PseudoProbeSDNode(unsigned Opcode, unsigned Order, const DebugLoc &Dl,\n                    SDVTList VTs, uint64_t Guid, uint64_t Index, uint32_t Attr)\n      : SDNode(Opcode, Order, Dl, VTs), Guid(Guid), Index(Index),\n        Attributes(Attr) {}\n\npublic:\n  uint64_t getGuid() const { return Guid; }\n  uint64_t getIndex() const { return Index; }\n  uint32_t getAttributes() const { return Attributes; }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::PSEUDO_PROBE;\n  }\n};\n\nclass JumpTableSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  int JTI;\n  unsigned TargetFlags;\n\n  JumpTableSDNode(int jti, EVT VT, bool isTarg, unsigned TF)\n    : SDNode(isTarg ? ISD::TargetJumpTable : ISD::JumpTable,\n      0, DebugLoc(), getSDVTList(VT)), JTI(jti), TargetFlags(TF) {\n  }\n\npublic:\n  int getIndex() const { return JTI; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::JumpTable ||\n           N->getOpcode() == ISD::TargetJumpTable;\n  }\n};\n\nclass ConstantPoolSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  union {\n    const Constant *ConstVal;\n    MachineConstantPoolValue *MachineCPVal;\n  } Val;\n  int Offset;  // It's a MachineConstantPoolValue if top bit is set.\n  Align Alignment; // Minimum alignment requirement of CP.\n  unsigned TargetFlags;\n\n  ConstantPoolSDNode(bool isTarget, const Constant *c, EVT VT, int o,\n                     Align Alignment, unsigned TF)\n      : SDNode(isTarget ? ISD::TargetConstantPool : ISD::ConstantPool, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Offset(o), Alignment(Alignment), TargetFlags(TF) {\n    assert(Offset >= 0 && \"Offset is too large\");\n    Val.ConstVal = c;\n  }\n\n  ConstantPoolSDNode(bool isTarget, MachineConstantPoolValue *v, EVT VT, int o,\n                     Align Alignment, unsigned TF)\n      : SDNode(isTarget ? ISD::TargetConstantPool : ISD::ConstantPool, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Offset(o), Alignment(Alignment), TargetFlags(TF) {\n    assert(Offset >= 0 && \"Offset is too large\");\n    Val.MachineCPVal = v;\n    Offset |= 1 << (sizeof(unsigned)*CHAR_BIT-1);\n  }\n\npublic:\n  bool isMachineConstantPoolEntry() const {\n    return Offset < 0;\n  }\n\n  const Constant *getConstVal() const {\n    assert(!isMachineConstantPoolEntry() && \"Wrong constantpool type\");\n    return Val.ConstVal;\n  }\n\n  MachineConstantPoolValue *getMachineCPVal() const {\n    assert(isMachineConstantPoolEntry() && \"Wrong constantpool type\");\n    return Val.MachineCPVal;\n  }\n\n  int getOffset() const {\n    return Offset & ~(1 << (sizeof(unsigned)*CHAR_BIT-1));\n  }\n\n  // Return the alignment of this constant pool object, which is either 0 (for\n  // default alignment) or the desired value.\n  Align getAlign() const { return Alignment; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  Type *getType() const;\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ConstantPool ||\n           N->getOpcode() == ISD::TargetConstantPool;\n  }\n};\n\n/// Completely target-dependent object reference.\nclass TargetIndexSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  unsigned TargetFlags;\n  int Index;\n  int64_t Offset;\n\npublic:\n  TargetIndexSDNode(int Idx, EVT VT, int64_t Ofs, unsigned TF)\n      : SDNode(ISD::TargetIndex, 0, DebugLoc(), getSDVTList(VT)),\n        TargetFlags(TF), Index(Idx), Offset(Ofs) {}\n\n  unsigned getTargetFlags() const { return TargetFlags; }\n  int getIndex() const { return Index; }\n  int64_t getOffset() const { return Offset; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::TargetIndex;\n  }\n};\n\nclass BasicBlockSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  MachineBasicBlock *MBB;\n\n  /// Debug info is meaningful and potentially useful here, but we create\n  /// blocks out of order when they're jumped to, which makes it a bit\n  /// harder.  Let's see if we need it first.\n  explicit BasicBlockSDNode(MachineBasicBlock *mbb)\n    : SDNode(ISD::BasicBlock, 0, DebugLoc(), getSDVTList(MVT::Other)), MBB(mbb)\n  {}\n\npublic:\n  MachineBasicBlock *getBasicBlock() const { return MBB; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::BasicBlock;\n  }\n};\n\n/// A \"pseudo-class\" with methods for operating on BUILD_VECTORs.\nclass BuildVectorSDNode : public SDNode {\npublic:\n  // These are constructed as SDNodes and then cast to BuildVectorSDNodes.\n  explicit BuildVectorSDNode() = delete;\n\n  /// Check if this is a constant splat, and if so, find the\n  /// smallest element size that splats the vector.  If MinSplatBits is\n  /// nonzero, the element size must be at least that large.  Note that the\n  /// splat element may be the entire vector (i.e., a one element vector).\n  /// Returns the splat element value in SplatValue.  Any undefined bits in\n  /// that value are zero, and the corresponding bits in the SplatUndef mask\n  /// are set.  The SplatBitSize value is set to the splat element size in\n  /// bits.  HasAnyUndefs is set to true if any bits in the vector are\n  /// undefined.  isBigEndian describes the endianness of the target.\n  bool isConstantSplat(APInt &SplatValue, APInt &SplatUndef,\n                       unsigned &SplatBitSize, bool &HasAnyUndefs,\n                       unsigned MinSplatBits = 0,\n                       bool isBigEndian = false) const;\n\n  /// Returns the demanded splatted value or a null value if this is not a\n  /// splat.\n  ///\n  /// The DemandedElts mask indicates the elements that must be in the splat.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  SDValue getSplatValue(const APInt &DemandedElts,\n                        BitVector *UndefElements = nullptr) const;\n\n  /// Returns the splatted value or a null value if this is not a splat.\n  ///\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  SDValue getSplatValue(BitVector *UndefElements = nullptr) const;\n\n  /// Find the shortest repeating sequence of values in the build vector.\n  ///\n  /// e.g. { u, X, u, X, u, u, X, u } -> { X }\n  ///      { X, Y, u, Y, u, u, X, u } -> { X, Y }\n  ///\n  /// Currently this must be a power-of-2 build vector.\n  /// The DemandedElts mask indicates the elements that must be present,\n  /// undemanded elements in Sequence may be null (SDValue()). If passed a\n  /// non-null UndefElements bitvector, it will resize it to match the original\n  /// vector width and set the bits where elements are undef. If result is\n  /// false, Sequence will be empty.\n  bool getRepeatedSequence(const APInt &DemandedElts,\n                           SmallVectorImpl<SDValue> &Sequence,\n                           BitVector *UndefElements = nullptr) const;\n\n  /// Find the shortest repeating sequence of values in the build vector.\n  ///\n  /// e.g. { u, X, u, X, u, u, X, u } -> { X }\n  ///      { X, Y, u, Y, u, u, X, u } -> { X, Y }\n  ///\n  /// Currently this must be a power-of-2 build vector.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the original vector width and set the bits where elements are undef.\n  /// If result is false, Sequence will be empty.\n  bool getRepeatedSequence(SmallVectorImpl<SDValue> &Sequence,\n                           BitVector *UndefElements = nullptr) const;\n\n  /// Returns the demanded splatted constant or null if this is not a constant\n  /// splat.\n  ///\n  /// The DemandedElts mask indicates the elements that must be in the splat.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantSDNode *\n  getConstantSplatNode(const APInt &DemandedElts,\n                       BitVector *UndefElements = nullptr) const;\n\n  /// Returns the splatted constant or null if this is not a constant\n  /// splat.\n  ///\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantSDNode *\n  getConstantSplatNode(BitVector *UndefElements = nullptr) const;\n\n  /// Returns the demanded splatted constant FP or null if this is not a\n  /// constant FP splat.\n  ///\n  /// The DemandedElts mask indicates the elements that must be in the splat.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantFPSDNode *\n  getConstantFPSplatNode(const APInt &DemandedElts,\n                         BitVector *UndefElements = nullptr) const;\n\n  /// Returns the splatted constant FP or null if this is not a constant\n  /// FP splat.\n  ///\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantFPSDNode *\n  getConstantFPSplatNode(BitVector *UndefElements = nullptr) const;\n\n  /// If this is a constant FP splat and the splatted constant FP is an\n  /// exact power or 2, return the log base 2 integer value.  Otherwise,\n  /// return -1.\n  ///\n  /// The BitWidth specifies the necessary bit precision.\n  int32_t getConstantFPSplatPow2ToLog2Int(BitVector *UndefElements,\n                                          uint32_t BitWidth) const;\n\n  bool isConstant() const;\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::BUILD_VECTOR;\n  }\n};\n\n/// An SDNode that holds an arbitrary LLVM IR Value. This is\n/// used when the SelectionDAG needs to make a simple reference to something\n/// in the LLVM IR representation.\n///\nclass SrcValueSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const Value *V;\n\n  /// Create a SrcValue for a general value.\n  explicit SrcValueSDNode(const Value *v)\n    : SDNode(ISD::SRCVALUE, 0, DebugLoc(), getSDVTList(MVT::Other)), V(v) {}\n\npublic:\n  /// Return the contained Value.\n  const Value *getValue() const { return V; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::SRCVALUE;\n  }\n};\n\nclass MDNodeSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const MDNode *MD;\n\n  explicit MDNodeSDNode(const MDNode *md)\n  : SDNode(ISD::MDNODE_SDNODE, 0, DebugLoc(), getSDVTList(MVT::Other)), MD(md)\n  {}\n\npublic:\n  const MDNode *getMD() const { return MD; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MDNODE_SDNODE;\n  }\n};\n\nclass RegisterSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  Register Reg;\n\n  RegisterSDNode(Register reg, EVT VT)\n    : SDNode(ISD::Register, 0, DebugLoc(), getSDVTList(VT)), Reg(reg) {}\n\npublic:\n  Register getReg() const { return Reg; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::Register;\n  }\n};\n\nclass RegisterMaskSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  // The memory for RegMask is not owned by the node.\n  const uint32_t *RegMask;\n\n  RegisterMaskSDNode(const uint32_t *mask)\n    : SDNode(ISD::RegisterMask, 0, DebugLoc(), getSDVTList(MVT::Untyped)),\n      RegMask(mask) {}\n\npublic:\n  const uint32_t *getRegMask() const { return RegMask; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::RegisterMask;\n  }\n};\n\nclass BlockAddressSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const BlockAddress *BA;\n  int64_t Offset;\n  unsigned TargetFlags;\n\n  BlockAddressSDNode(unsigned NodeTy, EVT VT, const BlockAddress *ba,\n                     int64_t o, unsigned Flags)\n    : SDNode(NodeTy, 0, DebugLoc(), getSDVTList(VT)),\n             BA(ba), Offset(o), TargetFlags(Flags) {}\n\npublic:\n  const BlockAddress *getBlockAddress() const { return BA; }\n  int64_t getOffset() const { return Offset; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::BlockAddress ||\n           N->getOpcode() == ISD::TargetBlockAddress;\n  }\n};\n\nclass LabelSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  MCSymbol *Label;\n\n  LabelSDNode(unsigned Opcode, unsigned Order, const DebugLoc &dl, MCSymbol *L)\n      : SDNode(Opcode, Order, dl, getSDVTList(MVT::Other)), Label(L) {\n    assert(LabelSDNode::classof(this) && \"not a label opcode\");\n  }\n\npublic:\n  MCSymbol *getLabel() const { return Label; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::EH_LABEL ||\n           N->getOpcode() == ISD::ANNOTATION_LABEL;\n  }\n};\n\nclass ExternalSymbolSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const char *Symbol;\n  unsigned TargetFlags;\n\n  ExternalSymbolSDNode(bool isTarget, const char *Sym, unsigned TF, EVT VT)\n      : SDNode(isTarget ? ISD::TargetExternalSymbol : ISD::ExternalSymbol, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Symbol(Sym), TargetFlags(TF) {}\n\npublic:\n  const char *getSymbol() const { return Symbol; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ExternalSymbol ||\n           N->getOpcode() == ISD::TargetExternalSymbol;\n  }\n};\n\nclass MCSymbolSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  MCSymbol *Symbol;\n\n  MCSymbolSDNode(MCSymbol *Symbol, EVT VT)\n      : SDNode(ISD::MCSymbol, 0, DebugLoc(), getSDVTList(VT)), Symbol(Symbol) {}\n\npublic:\n  MCSymbol *getMCSymbol() const { return Symbol; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MCSymbol;\n  }\n};\n\nclass CondCodeSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  ISD::CondCode Condition;\n\n  explicit CondCodeSDNode(ISD::CondCode Cond)\n    : SDNode(ISD::CONDCODE, 0, DebugLoc(), getSDVTList(MVT::Other)),\n      Condition(Cond) {}\n\npublic:\n  ISD::CondCode get() const { return Condition; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::CONDCODE;\n  }\n};\n\n/// This class is used to represent EVT's, which are used\n/// to parameterize some operations.\nclass VTSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  EVT ValueType;\n\n  explicit VTSDNode(EVT VT)\n    : SDNode(ISD::VALUETYPE, 0, DebugLoc(), getSDVTList(MVT::Other)),\n      ValueType(VT) {}\n\npublic:\n  EVT getVT() const { return ValueType; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::VALUETYPE;\n  }\n};\n\n/// Base class for LoadSDNode and StoreSDNode\nclass LSBaseSDNode : public MemSDNode {\npublic:\n  LSBaseSDNode(ISD::NodeType NodeTy, unsigned Order, const DebugLoc &dl,\n               SDVTList VTs, ISD::MemIndexedMode AM, EVT MemVT,\n               MachineMemOperand *MMO)\n      : MemSDNode(NodeTy, Order, dl, VTs, MemVT, MMO) {\n    LSBaseSDNodeBits.AddressingMode = AM;\n    assert(getAddressingMode() == AM && \"Value truncated\");\n  }\n\n  const SDValue &getOffset() const {\n    return getOperand(getOpcode() == ISD::LOAD ? 2 : 3);\n  }\n\n  /// Return the addressing mode for this load or store:\n  /// unindexed, pre-inc, pre-dec, post-inc, or post-dec.\n  ISD::MemIndexedMode getAddressingMode() const {\n    return static_cast<ISD::MemIndexedMode>(LSBaseSDNodeBits.AddressingMode);\n  }\n\n  /// Return true if this is a pre/post inc/dec load/store.\n  bool isIndexed() const { return getAddressingMode() != ISD::UNINDEXED; }\n\n  /// Return true if this is NOT a pre/post inc/dec load/store.\n  bool isUnindexed() const { return getAddressingMode() == ISD::UNINDEXED; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::LOAD ||\n           N->getOpcode() == ISD::STORE;\n  }\n};\n\n/// This class is used to represent ISD::LOAD nodes.\nclass LoadSDNode : public LSBaseSDNode {\n  friend class SelectionDAG;\n\n  LoadSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n             ISD::MemIndexedMode AM, ISD::LoadExtType ETy, EVT MemVT,\n             MachineMemOperand *MMO)\n      : LSBaseSDNode(ISD::LOAD, Order, dl, VTs, AM, MemVT, MMO) {\n    LoadSDNodeBits.ExtTy = ETy;\n    assert(readMem() && \"Load MachineMemOperand is not a load!\");\n    assert(!writeMem() && \"Load MachineMemOperand is a store!\");\n  }\n\npublic:\n  /// Return whether this is a plain node,\n  /// or one of the varieties of value-extending loads.\n  ISD::LoadExtType getExtensionType() const {\n    return static_cast<ISD::LoadExtType>(LoadSDNodeBits.ExtTy);\n  }\n\n  const SDValue &getBasePtr() const { return getOperand(1); }\n  const SDValue &getOffset() const { return getOperand(2); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::LOAD;\n  }\n};\n\n/// This class is used to represent ISD::STORE nodes.\nclass StoreSDNode : public LSBaseSDNode {\n  friend class SelectionDAG;\n\n  StoreSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n              ISD::MemIndexedMode AM, bool isTrunc, EVT MemVT,\n              MachineMemOperand *MMO)\n      : LSBaseSDNode(ISD::STORE, Order, dl, VTs, AM, MemVT, MMO) {\n    StoreSDNodeBits.IsTruncating = isTrunc;\n    assert(!readMem() && \"Store MachineMemOperand is a load!\");\n    assert(writeMem() && \"Store MachineMemOperand is not a store!\");\n  }\n\npublic:\n  /// Return true if the op does a truncation before store.\n  /// For integers this is the same as doing a TRUNCATE and storing the result.\n  /// For floats, it is the same as doing an FP_ROUND and storing the result.\n  bool isTruncatingStore() const { return StoreSDNodeBits.IsTruncating; }\n  void setTruncatingStore(bool Truncating) {\n    StoreSDNodeBits.IsTruncating = Truncating;\n  }\n\n  const SDValue &getValue() const { return getOperand(1); }\n  const SDValue &getBasePtr() const { return getOperand(2); }\n  const SDValue &getOffset() const { return getOperand(3); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::STORE;\n  }\n};\n\n/// This base class is used to represent MLOAD and MSTORE nodes\nclass MaskedLoadStoreSDNode : public MemSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedLoadStoreSDNode(ISD::NodeType NodeTy, unsigned Order,\n                        const DebugLoc &dl, SDVTList VTs,\n                        ISD::MemIndexedMode AM, EVT MemVT,\n                        MachineMemOperand *MMO)\n      : MemSDNode(NodeTy, Order, dl, VTs, MemVT, MMO) {\n    LSBaseSDNodeBits.AddressingMode = AM;\n    assert(getAddressingMode() == AM && \"Value truncated\");\n  }\n\n  // MaskedLoadSDNode (Chain, ptr, offset, mask, passthru)\n  // MaskedStoreSDNode (Chain, data, ptr, offset, mask)\n  // Mask is a vector of i1 elements\n  const SDValue &getOffset() const {\n    return getOperand(getOpcode() == ISD::MLOAD ? 2 : 3);\n  }\n  const SDValue &getMask() const {\n    return getOperand(getOpcode() == ISD::MLOAD ? 3 : 4);\n  }\n\n  /// Return the addressing mode for this load or store:\n  /// unindexed, pre-inc, pre-dec, post-inc, or post-dec.\n  ISD::MemIndexedMode getAddressingMode() const {\n    return static_cast<ISD::MemIndexedMode>(LSBaseSDNodeBits.AddressingMode);\n  }\n\n  /// Return true if this is a pre/post inc/dec load/store.\n  bool isIndexed() const { return getAddressingMode() != ISD::UNINDEXED; }\n\n  /// Return true if this is NOT a pre/post inc/dec load/store.\n  bool isUnindexed() const { return getAddressingMode() == ISD::UNINDEXED; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MLOAD ||\n           N->getOpcode() == ISD::MSTORE;\n  }\n};\n\n/// This class is used to represent an MLOAD node\nclass MaskedLoadSDNode : public MaskedLoadStoreSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedLoadSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                   ISD::MemIndexedMode AM, ISD::LoadExtType ETy,\n                   bool IsExpanding, EVT MemVT, MachineMemOperand *MMO)\n      : MaskedLoadStoreSDNode(ISD::MLOAD, Order, dl, VTs, AM, MemVT, MMO) {\n    LoadSDNodeBits.ExtTy = ETy;\n    LoadSDNodeBits.IsExpanding = IsExpanding;\n  }\n\n  ISD::LoadExtType getExtensionType() const {\n    return static_cast<ISD::LoadExtType>(LoadSDNodeBits.ExtTy);\n  }\n\n  const SDValue &getBasePtr() const { return getOperand(1); }\n  const SDValue &getOffset() const { return getOperand(2); }\n  const SDValue &getMask() const { return getOperand(3); }\n  const SDValue &getPassThru() const { return getOperand(4); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MLOAD;\n  }\n\n  bool isExpandingLoad() const { return LoadSDNodeBits.IsExpanding; }\n};\n\n/// This class is used to represent an MSTORE node\nclass MaskedStoreSDNode : public MaskedLoadStoreSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedStoreSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                    ISD::MemIndexedMode AM, bool isTrunc, bool isCompressing,\n                    EVT MemVT, MachineMemOperand *MMO)\n      : MaskedLoadStoreSDNode(ISD::MSTORE, Order, dl, VTs, AM, MemVT, MMO) {\n    StoreSDNodeBits.IsTruncating = isTrunc;\n    StoreSDNodeBits.IsCompressing = isCompressing;\n  }\n\n  /// Return true if the op does a truncation before store.\n  /// For integers this is the same as doing a TRUNCATE and storing the result.\n  /// For floats, it is the same as doing an FP_ROUND and storing the result.\n  bool isTruncatingStore() const { return StoreSDNodeBits.IsTruncating; }\n\n  /// Returns true if the op does a compression to the vector before storing.\n  /// The node contiguously stores the active elements (integers or floats)\n  /// in src (those with their respective bit set in writemask k) to unaligned\n  /// memory at base_addr.\n  bool isCompressingStore() const { return StoreSDNodeBits.IsCompressing; }\n\n  const SDValue &getValue() const { return getOperand(1); }\n  const SDValue &getBasePtr() const { return getOperand(2); }\n  const SDValue &getOffset() const { return getOperand(3); }\n  const SDValue &getMask() const { return getOperand(4); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MSTORE;\n  }\n};\n\n/// This is a base class used to represent\n/// MGATHER and MSCATTER nodes\n///\nclass MaskedGatherScatterSDNode : public MemSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedGatherScatterSDNode(ISD::NodeType NodeTy, unsigned Order,\n                            const DebugLoc &dl, SDVTList VTs, EVT MemVT,\n                            MachineMemOperand *MMO, ISD::MemIndexType IndexType)\n      : MemSDNode(NodeTy, Order, dl, VTs, MemVT, MMO) {\n    LSBaseSDNodeBits.AddressingMode = IndexType;\n    assert(getIndexType() == IndexType && \"Value truncated\");\n  }\n\n  /// How is Index applied to BasePtr when computing addresses.\n  ISD::MemIndexType getIndexType() const {\n    return static_cast<ISD::MemIndexType>(LSBaseSDNodeBits.AddressingMode);\n  }\n  void setIndexType(ISD::MemIndexType IndexType) {\n    LSBaseSDNodeBits.AddressingMode = IndexType;\n  }\n  bool isIndexScaled() const {\n    return (getIndexType() == ISD::SIGNED_SCALED) ||\n           (getIndexType() == ISD::UNSIGNED_SCALED);\n  }\n  bool isIndexSigned() const {\n    return (getIndexType() == ISD::SIGNED_SCALED) ||\n           (getIndexType() == ISD::SIGNED_UNSCALED);\n  }\n\n  // In the both nodes address is Op1, mask is Op2:\n  // MaskedGatherSDNode  (Chain, passthru, mask, base, index, scale)\n  // MaskedScatterSDNode (Chain, value, mask, base, index, scale)\n  // Mask is a vector of i1 elements\n  const SDValue &getBasePtr() const { return getOperand(3); }\n  const SDValue &getIndex()   const { return getOperand(4); }\n  const SDValue &getMask()    const { return getOperand(2); }\n  const SDValue &getScale()   const { return getOperand(5); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MGATHER ||\n           N->getOpcode() == ISD::MSCATTER;\n  }\n};\n\n/// This class is used to represent an MGATHER node\n///\nclass MaskedGatherSDNode : public MaskedGatherScatterSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedGatherSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                     EVT MemVT, MachineMemOperand *MMO,\n                     ISD::MemIndexType IndexType, ISD::LoadExtType ETy)\n      : MaskedGatherScatterSDNode(ISD::MGATHER, Order, dl, VTs, MemVT, MMO,\n                                  IndexType) {\n    LoadSDNodeBits.ExtTy = ETy;\n  }\n\n  const SDValue &getPassThru() const { return getOperand(1); }\n\n  ISD::LoadExtType getExtensionType() const {\n    return ISD::LoadExtType(LoadSDNodeBits.ExtTy);\n  }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MGATHER;\n  }\n};\n\n/// This class is used to represent an MSCATTER node\n///\nclass MaskedScatterSDNode : public MaskedGatherScatterSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedScatterSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                      EVT MemVT, MachineMemOperand *MMO,\n                      ISD::MemIndexType IndexType, bool IsTrunc)\n      : MaskedGatherScatterSDNode(ISD::MSCATTER, Order, dl, VTs, MemVT, MMO,\n                                  IndexType) {\n    StoreSDNodeBits.IsTruncating = IsTrunc;\n  }\n\n  /// Return true if the op does a truncation before store.\n  /// For integers this is the same as doing a TRUNCATE and storing the result.\n  /// For floats, it is the same as doing an FP_ROUND and storing the result.\n  bool isTruncatingStore() const { return StoreSDNodeBits.IsTruncating; }\n\n  const SDValue &getValue() const { return getOperand(1); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MSCATTER;\n  }\n};\n\n/// An SDNode that represents everything that will be needed\n/// to construct a MachineInstr. These nodes are created during the\n/// instruction selection proper phase.\n///\n/// Note that the only supported way to set the `memoperands` is by calling the\n/// `SelectionDAG::setNodeMemRefs` function as the memory management happens\n/// inside the DAG rather than in the node.\nclass MachineSDNode : public SDNode {\nprivate:\n  friend class SelectionDAG;\n\n  MachineSDNode(unsigned Opc, unsigned Order, const DebugLoc &DL, SDVTList VTs)\n      : SDNode(Opc, Order, DL, VTs) {}\n\n  // We use a pointer union between a single `MachineMemOperand` pointer and\n  // a pointer to an array of `MachineMemOperand` pointers. This is null when\n  // the number of these is zero, the single pointer variant used when the\n  // number is one, and the array is used for larger numbers.\n  //\n  // The array is allocated via the `SelectionDAG`'s allocator and so will\n  // always live until the DAG is cleaned up and doesn't require ownership here.\n  //\n  // We can't use something simpler like `TinyPtrVector` here because `SDNode`\n  // subclasses aren't managed in a conforming C++ manner. See the comments on\n  // `SelectionDAG::MorphNodeTo` which details what all goes on, but the\n  // constraint here is that these don't manage memory with their constructor or\n  // destructor and can be initialized to a good state even if they start off\n  // uninitialized.\n  PointerUnion<MachineMemOperand *, MachineMemOperand **> MemRefs = {};\n\n  // Note that this could be folded into the above `MemRefs` member if doing so\n  // is advantageous at some point. We don't need to store this in most cases.\n  // However, at the moment this doesn't appear to make the allocation any\n  // smaller and makes the code somewhat simpler to read.\n  int NumMemRefs = 0;\n\npublic:\n  using mmo_iterator = ArrayRef<MachineMemOperand *>::const_iterator;\n\n  ArrayRef<MachineMemOperand *> memoperands() const {\n    // Special case the common cases.\n    if (NumMemRefs == 0)\n      return {};\n    if (NumMemRefs == 1)\n      return makeArrayRef(MemRefs.getAddrOfPtr1(), 1);\n\n    // Otherwise we have an actual array.\n    return makeArrayRef(MemRefs.get<MachineMemOperand **>(), NumMemRefs);\n  }\n  mmo_iterator memoperands_begin() const { return memoperands().begin(); }\n  mmo_iterator memoperands_end() const { return memoperands().end(); }\n  bool memoperands_empty() const { return memoperands().empty(); }\n\n  /// Clear out the memory reference descriptor list.\n  void clearMemRefs() {\n    MemRefs = nullptr;\n    NumMemRefs = 0;\n  }\n\n  static bool classof(const SDNode *N) {\n    return N->isMachineOpcode();\n  }\n};\n\n/// An SDNode that records if a register contains a value that is guaranteed to\n/// be aligned accordingly.\nclass AssertAlignSDNode : public SDNode {\n  Align Alignment;\n\npublic:\n  AssertAlignSDNode(unsigned Order, const DebugLoc &DL, EVT VT, Align A)\n      : SDNode(ISD::AssertAlign, Order, DL, getSDVTList(VT)), Alignment(A) {}\n\n  Align getAlign() const { return Alignment; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::AssertAlign;\n  }\n};\n\nclass SDNodeIterator : public std::iterator<std::forward_iterator_tag,\n                                            SDNode, ptrdiff_t> {\n  const SDNode *Node;\n  unsigned Operand;\n\n  SDNodeIterator(const SDNode *N, unsigned Op) : Node(N), Operand(Op) {}\n\npublic:\n  bool operator==(const SDNodeIterator& x) const {\n    return Operand == x.Operand;\n  }\n  bool operator!=(const SDNodeIterator& x) const { return !operator==(x); }\n\n  pointer operator*() const {\n    return Node->getOperand(Operand).getNode();\n  }\n  pointer operator->() const { return operator*(); }\n\n  SDNodeIterator& operator++() {                // Preincrement\n    ++Operand;\n    return *this;\n  }\n  SDNodeIterator operator++(int) { // Postincrement\n    SDNodeIterator tmp = *this; ++*this; return tmp;\n  }\n  size_t operator-(SDNodeIterator Other) const {\n    assert(Node == Other.Node &&\n           \"Cannot compare iterators of two different nodes!\");\n    return Operand - Other.Operand;\n  }\n\n  static SDNodeIterator begin(const SDNode *N) { return SDNodeIterator(N, 0); }\n  static SDNodeIterator end  (const SDNode *N) {\n    return SDNodeIterator(N, N->getNumOperands());\n  }\n\n  unsigned getOperand() const { return Operand; }\n  const SDNode *getNode() const { return Node; }\n};\n\ntemplate <> struct GraphTraits<SDNode*> {\n  using NodeRef = SDNode *;\n  using ChildIteratorType = SDNodeIterator;\n\n  static NodeRef getEntryNode(SDNode *N) { return N; }\n\n  static ChildIteratorType child_begin(NodeRef N) {\n    return SDNodeIterator::begin(N);\n  }\n\n  static ChildIteratorType child_end(NodeRef N) {\n    return SDNodeIterator::end(N);\n  }\n};\n\n/// A representation of the largest SDNode, for use in sizeof().\n///\n/// This needs to be a union because the largest node differs on 32 bit systems\n/// with 4 and 8 byte pointer alignment, respectively.\nusing LargestSDNode = AlignedCharArrayUnion<AtomicSDNode, TargetIndexSDNode,\n                                            BlockAddressSDNode,\n                                            GlobalAddressSDNode,\n                                            PseudoProbeSDNode>;\n\n/// The SDNode class with the greatest alignment requirement.\nusing MostAlignedSDNode = GlobalAddressSDNode;\n\nnamespace ISD {\n\n  /// Returns true if the specified node is a non-extending and unindexed load.\n  inline bool isNormalLoad(const SDNode *N) {\n    const LoadSDNode *Ld = dyn_cast<LoadSDNode>(N);\n    return Ld && Ld->getExtensionType() == ISD::NON_EXTLOAD &&\n      Ld->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Returns true if the specified node is a non-extending load.\n  inline bool isNON_EXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::NON_EXTLOAD;\n  }\n\n  /// Returns true if the specified node is a EXTLOAD.\n  inline bool isEXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::EXTLOAD;\n  }\n\n  /// Returns true if the specified node is a SEXTLOAD.\n  inline bool isSEXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::SEXTLOAD;\n  }\n\n  /// Returns true if the specified node is a ZEXTLOAD.\n  inline bool isZEXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::ZEXTLOAD;\n  }\n\n  /// Returns true if the specified node is an unindexed load.\n  inline bool isUNINDEXEDLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Returns true if the specified node is a non-truncating\n  /// and unindexed store.\n  inline bool isNormalStore(const SDNode *N) {\n    const StoreSDNode *St = dyn_cast<StoreSDNode>(N);\n    return St && !St->isTruncatingStore() &&\n      St->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Returns true if the specified node is a non-truncating store.\n  inline bool isNON_TRUNCStore(const SDNode *N) {\n    return isa<StoreSDNode>(N) && !cast<StoreSDNode>(N)->isTruncatingStore();\n  }\n\n  /// Returns true if the specified node is a truncating store.\n  inline bool isTRUNCStore(const SDNode *N) {\n    return isa<StoreSDNode>(N) && cast<StoreSDNode>(N)->isTruncatingStore();\n  }\n\n  /// Returns true if the specified node is an unindexed store.\n  inline bool isUNINDEXEDStore(const SDNode *N) {\n    return isa<StoreSDNode>(N) &&\n      cast<StoreSDNode>(N)->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Attempt to match a unary predicate against a scalar/splat constant or\n  /// every element of a constant BUILD_VECTOR.\n  /// If AllowUndef is true, then UNDEF elements will pass nullptr to Match.\n  bool matchUnaryPredicate(SDValue Op,\n                           std::function<bool(ConstantSDNode *)> Match,\n                           bool AllowUndefs = false);\n\n  /// Attempt to match a binary predicate against a pair of scalar/splat\n  /// constants or every element of a pair of constant BUILD_VECTORs.\n  /// If AllowUndef is true, then UNDEF elements will pass nullptr to Match.\n  /// If AllowTypeMismatch is true then RetType + ArgTypes don't need to match.\n  bool matchBinaryPredicate(\n      SDValue LHS, SDValue RHS,\n      std::function<bool(ConstantSDNode *, ConstantSDNode *)> Match,\n      bool AllowUndefs = false, bool AllowTypeMismatch = false);\n\n  /// Returns true if the specified value is the overflow result from one\n  /// of the overflow intrinsic nodes.\n  inline bool isOverflowIntrOpRes(SDValue Op) {\n    unsigned Opc = Op.getOpcode();\n    return (Op.getResNo() == 1 &&\n            (Opc == ISD::SADDO || Opc == ISD::UADDO || Opc == ISD::SSUBO ||\n             Opc == ISD::USUBO || Opc == ISD::SMULO || Opc == ISD::UMULO));\n  }\n\n} // end namespace ISD\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_SELECTIONDAGNODES_H\n"}, "56": {"id": 56, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetCallingConv.h", "content": "//===-- llvm/CodeGen/TargetCallingConv.h - Calling Convention ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines types for working with calling-convention information.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_TARGETCALLINGCONV_H\n#define LLVM_CODEGEN_TARGETCALLINGCONV_H\n\n#include \"llvm/CodeGen/ValueTypes.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include \"llvm/Support/MathExtras.h\"\n#include <cassert>\n#include <climits>\n#include <cstdint>\n\nnamespace llvm {\nnamespace ISD {\n\n  struct ArgFlagsTy {\n  private:\n    unsigned IsZExt : 1;     ///< Zero extended\n    unsigned IsSExt : 1;     ///< Sign extended\n    unsigned IsInReg : 1;    ///< Passed in register\n    unsigned IsSRet : 1;     ///< Hidden struct-ret ptr\n    unsigned IsByVal : 1;    ///< Struct passed by value\n    unsigned IsByRef : 1;    ///< Passed in memory\n    unsigned IsNest : 1;     ///< Nested fn static chain\n    unsigned IsReturned : 1; ///< Always returned\n    unsigned IsSplit : 1;\n    unsigned IsInAlloca : 1;   ///< Passed with inalloca\n    unsigned IsPreallocated : 1; ///< ByVal without the copy\n    unsigned IsSplitEnd : 1;   ///< Last part of a split\n    unsigned IsSwiftSelf : 1;  ///< Swift self parameter\n    unsigned IsSwiftError : 1; ///< Swift error parameter\n    unsigned IsCFGuardTarget : 1; ///< Control Flow Guard target\n    unsigned IsHva : 1;        ///< HVA field for\n    unsigned IsHvaStart : 1;   ///< HVA structure start\n    unsigned IsSecArgPass : 1; ///< Second argument\n    unsigned ByValOrByRefAlign : 4; ///< Log 2 of byval/byref alignment\n    unsigned OrigAlign : 5;    ///< Log 2 of original alignment\n    unsigned IsInConsecutiveRegsLast : 1;\n    unsigned IsInConsecutiveRegs : 1;\n    unsigned IsCopyElisionCandidate : 1; ///< Argument copy elision candidate\n    unsigned IsPointer : 1;\n\n    unsigned ByValOrByRefSize; ///< Byval or byref struct size\n\n    unsigned PointerAddrSpace; ///< Address space of pointer argument\n\n    /// Set the alignment used by byref or byval parameters.\n    void setAlignImpl(Align A) {\n      ByValOrByRefAlign = encode(A);\n      assert(getNonZeroByValAlign() == A && \"bitfield overflow\");\n    }\n\n  public:\n    ArgFlagsTy()\n      : IsZExt(0), IsSExt(0), IsInReg(0), IsSRet(0), IsByVal(0), IsByRef(0),\n          IsNest(0), IsReturned(0), IsSplit(0), IsInAlloca(0), IsPreallocated(0),\n          IsSplitEnd(0), IsSwiftSelf(0), IsSwiftError(0), IsCFGuardTarget(0),\n          IsHva(0), IsHvaStart(0), IsSecArgPass(0), ByValOrByRefAlign(0),\n          OrigAlign(0), IsInConsecutiveRegsLast(0), IsInConsecutiveRegs(0),\n          IsCopyElisionCandidate(0), IsPointer(0), ByValOrByRefSize(0),\n          PointerAddrSpace(0) {\n      static_assert(sizeof(*this) == 3 * sizeof(unsigned), \"flags are too big\");\n    }\n\n    bool isZExt() const { return IsZExt; }\n    void setZExt() { IsZExt = 1; }\n\n    bool isSExt() const { return IsSExt; }\n    void setSExt() { IsSExt = 1; }\n\n    bool isInReg() const { return IsInReg; }\n    void setInReg() { IsInReg = 1; }\n\n    bool isSRet() const { return IsSRet; }\n    void setSRet() { IsSRet = 1; }\n\n    bool isByVal() const { return IsByVal; }\n    void setByVal() { IsByVal = 1; }\n\n    bool isByRef() const { return IsByRef; }\n    void setByRef() { IsByRef = 1; }\n\n    bool isInAlloca() const { return IsInAlloca; }\n    void setInAlloca() { IsInAlloca = 1; }\n\n    bool isPreallocated() const { return IsPreallocated; }\n    void setPreallocated() { IsPreallocated = 1; }\n\n    bool isSwiftSelf() const { return IsSwiftSelf; }\n    void setSwiftSelf() { IsSwiftSelf = 1; }\n\n    bool isSwiftError() const { return IsSwiftError; }\n    void setSwiftError() { IsSwiftError = 1; }\n\n    bool isCFGuardTarget() const { return IsCFGuardTarget; }\n    void setCFGuardTarget() { IsCFGuardTarget = 1; }\n\n    bool isHva() const { return IsHva; }\n    void setHva() { IsHva = 1; }\n\n    bool isHvaStart() const { return IsHvaStart; }\n    void setHvaStart() { IsHvaStart = 1; }\n\n    bool isSecArgPass() const { return IsSecArgPass; }\n    void setSecArgPass() { IsSecArgPass = 1; }\n\n    bool isNest() const { return IsNest; }\n    void setNest() { IsNest = 1; }\n\n    bool isReturned() const { return IsReturned; }\n    void setReturned(bool V = true) { IsReturned = V; }\n\n    bool isInConsecutiveRegs()  const { return IsInConsecutiveRegs; }\n    void setInConsecutiveRegs(bool Flag = true) { IsInConsecutiveRegs = Flag; }\n\n    bool isInConsecutiveRegsLast() const { return IsInConsecutiveRegsLast; }\n    void setInConsecutiveRegsLast(bool Flag = true) {\n      IsInConsecutiveRegsLast = Flag;\n    }\n\n    bool isSplit()   const { return IsSplit; }\n    void setSplit()  { IsSplit = 1; }\n\n    bool isSplitEnd()   const { return IsSplitEnd; }\n    void setSplitEnd()  { IsSplitEnd = 1; }\n\n    bool isCopyElisionCandidate()  const { return IsCopyElisionCandidate; }\n    void setCopyElisionCandidate() { IsCopyElisionCandidate = 1; }\n\n    bool isPointer()  const { return IsPointer; }\n    void setPointer() { IsPointer = 1; }\n\n    LLVM_ATTRIBUTE_DEPRECATED(unsigned getByValAlign() const,\n                              \"Use getNonZeroByValAlign() instead\") {\n      MaybeAlign A = decodeMaybeAlign(ByValOrByRefAlign);\n      return A ? A->value() : 0;\n    }\n    Align getNonZeroByValAlign() const {\n      MaybeAlign A = decodeMaybeAlign(ByValOrByRefAlign);\n      assert(A && \"ByValAlign must be defined\");\n      return *A;\n    }\n    void setByValAlign(Align A) {\n      assert(isByVal() && !isByRef());\n      setAlignImpl(A);\n    }\n\n    void setByRefAlign(Align A) {\n      assert(!isByVal() && isByRef());\n      setAlignImpl(A);\n    }\n\n    LLVM_ATTRIBUTE_DEPRECATED(unsigned getOrigAlign() const,\n                              \"Use getNonZeroOrigAlign() instead\") {\n      MaybeAlign A = decodeMaybeAlign(OrigAlign);\n      return A ? A->value() : 0;\n    }\n    Align getNonZeroOrigAlign() const {\n      return decodeMaybeAlign(OrigAlign).valueOrOne();\n    }\n    void setOrigAlign(Align A) {\n      OrigAlign = encode(A);\n      assert(getNonZeroOrigAlign() == A && \"bitfield overflow\");\n    }\n\n    unsigned getByValSize() const {\n      assert(isByVal() && !isByRef());\n      return ByValOrByRefSize;\n    }\n    void setByValSize(unsigned S) {\n      assert(isByVal() && !isByRef());\n      ByValOrByRefSize = S;\n    }\n\n    unsigned getByRefSize() const {\n      assert(!isByVal() && isByRef());\n      return ByValOrByRefSize;\n    }\n    void setByRefSize(unsigned S) {\n      assert(!isByVal() && isByRef());\n      ByValOrByRefSize = S;\n    }\n\n    unsigned getPointerAddrSpace() const { return PointerAddrSpace; }\n    void setPointerAddrSpace(unsigned AS) { PointerAddrSpace = AS; }\n};\n\n  /// InputArg - This struct carries flags and type information about a\n  /// single incoming (formal) argument or incoming (from the perspective\n  /// of the caller) return value virtual register.\n  ///\n  struct InputArg {\n    ArgFlagsTy Flags;\n    MVT VT = MVT::Other;\n    EVT ArgVT;\n    bool Used = false;\n\n    /// Index original Function's argument.\n    unsigned OrigArgIndex;\n    /// Sentinel value for implicit machine-level input arguments.\n    static const unsigned NoArgIndex = UINT_MAX;\n\n    /// Offset in bytes of current input value relative to the beginning of\n    /// original argument. E.g. if argument was splitted into four 32 bit\n    /// registers, we got 4 InputArgs with PartOffsets 0, 4, 8 and 12.\n    unsigned PartOffset;\n\n    InputArg() = default;\n    InputArg(ArgFlagsTy flags, EVT vt, EVT argvt, bool used,\n             unsigned origIdx, unsigned partOffs)\n      : Flags(flags), Used(used), OrigArgIndex(origIdx), PartOffset(partOffs) {\n      VT = vt.getSimpleVT();\n      ArgVT = argvt;\n    }\n\n    bool isOrigArg() const {\n      return OrigArgIndex != NoArgIndex;\n    }\n\n    unsigned getOrigArgIndex() const {\n      assert(OrigArgIndex != NoArgIndex && \"Implicit machine-level argument\");\n      return OrigArgIndex;\n    }\n  };\n\n  /// OutputArg - This struct carries flags and a value for a\n  /// single outgoing (actual) argument or outgoing (from the perspective\n  /// of the caller) return value virtual register.\n  ///\n  struct OutputArg {\n    ArgFlagsTy Flags;\n    MVT VT;\n    EVT ArgVT;\n\n    /// IsFixed - Is this a \"fixed\" value, ie not passed through a vararg \"...\".\n    bool IsFixed = false;\n\n    /// Index original Function's argument.\n    unsigned OrigArgIndex;\n\n    /// Offset in bytes of current output value relative to the beginning of\n    /// original argument. E.g. if argument was splitted into four 32 bit\n    /// registers, we got 4 OutputArgs with PartOffsets 0, 4, 8 and 12.\n    unsigned PartOffset;\n\n    OutputArg() = default;\n    OutputArg(ArgFlagsTy flags, EVT vt, EVT argvt, bool isfixed,\n              unsigned origIdx, unsigned partOffs)\n      : Flags(flags), IsFixed(isfixed), OrigArgIndex(origIdx),\n        PartOffset(partOffs) {\n      VT = vt.getSimpleVT();\n      ArgVT = argvt;\n    }\n  };\n\n} // end namespace ISD\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_TARGETCALLINGCONV_H\n"}, "57": {"id": 57, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetInstrInfo.h", "content": "//===- llvm/CodeGen/TargetInstrInfo.h - Instruction Info --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file describes the target machine instruction set to the code generator.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_TARGETINSTRINFO_H\n#define LLVM_CODEGEN_TARGETINSTRINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DenseMapInfo.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/CodeGen/MIRFormatter.h\"\n#include \"llvm/CodeGen/MachineBasicBlock.h\"\n#include \"llvm/CodeGen/MachineCombinerPattern.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineInstr.h\"\n#include \"llvm/CodeGen/MachineInstrBuilder.h\"\n#include \"llvm/CodeGen/MachineOperand.h\"\n#include \"llvm/CodeGen/MachineOutliner.h\"\n#include \"llvm/CodeGen/RegisterClassInfo.h\"\n#include \"llvm/CodeGen/VirtRegMap.h\"\n#include \"llvm/MC/MCInstrInfo.h\"\n#include \"llvm/Support/BranchProbability.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass AAResults;\nclass DFAPacketizer;\nclass InstrItineraryData;\nclass LiveIntervals;\nclass LiveVariables;\nclass MachineLoop;\nclass MachineMemOperand;\nclass MachineRegisterInfo;\nclass MCAsmInfo;\nclass MCInst;\nstruct MCSchedModel;\nclass Module;\nclass ScheduleDAG;\nclass ScheduleDAGMI;\nclass ScheduleHazardRecognizer;\nclass SDNode;\nclass SelectionDAG;\nclass RegScavenger;\nclass TargetRegisterClass;\nclass TargetRegisterInfo;\nclass TargetSchedModel;\nclass TargetSubtargetInfo;\n\ntemplate <class T> class SmallVectorImpl;\n\nusing ParamLoadedValue = std::pair<MachineOperand, DIExpression*>;\n\nstruct DestSourcePair {\n  const MachineOperand *Destination;\n  const MachineOperand *Source;\n\n  DestSourcePair(const MachineOperand &Dest, const MachineOperand &Src)\n      : Destination(&Dest), Source(&Src) {}\n};\n\n/// Used to describe a register and immediate addition.\nstruct RegImmPair {\n  Register Reg;\n  int64_t Imm;\n\n  RegImmPair(Register Reg, int64_t Imm) : Reg(Reg), Imm(Imm) {}\n};\n\n/// Used to describe addressing mode similar to ExtAddrMode in CodeGenPrepare.\n/// It holds the register values, the scale value and the displacement.\nstruct ExtAddrMode {\n  Register BaseReg;\n  Register ScaledReg;\n  int64_t Scale;\n  int64_t Displacement;\n};\n\n//---------------------------------------------------------------------------\n///\n/// TargetInstrInfo - Interface to description of machine instruction set\n///\nclass TargetInstrInfo : public MCInstrInfo {\npublic:\n  TargetInstrInfo(unsigned CFSetupOpcode = ~0u, unsigned CFDestroyOpcode = ~0u,\n                  unsigned CatchRetOpcode = ~0u, unsigned ReturnOpcode = ~0u)\n      : CallFrameSetupOpcode(CFSetupOpcode),\n        CallFrameDestroyOpcode(CFDestroyOpcode), CatchRetOpcode(CatchRetOpcode),\n        ReturnOpcode(ReturnOpcode) {}\n  TargetInstrInfo(const TargetInstrInfo &) = delete;\n  TargetInstrInfo &operator=(const TargetInstrInfo &) = delete;\n  virtual ~TargetInstrInfo();\n\n  static bool isGenericOpcode(unsigned Opc) {\n    return Opc <= TargetOpcode::GENERIC_OP_END;\n  }\n\n  /// Given a machine instruction descriptor, returns the register\n  /// class constraint for OpNum, or NULL.\n  virtual\n  const TargetRegisterClass *getRegClass(const MCInstrDesc &MCID, unsigned OpNum,\n                                         const TargetRegisterInfo *TRI,\n                                         const MachineFunction &MF) const;\n\n  /// Return true if the instruction is trivially rematerializable, meaning it\n  /// has no side effects and requires no operands that aren't always available.\n  /// This means the only allowed uses are constants and unallocatable physical\n  /// registers so that the instructions result is independent of the place\n  /// in the function.\n  bool isTriviallyReMaterializable(const MachineInstr &MI,\n                                   AAResults *AA = nullptr) const {\n    return MI.getOpcode() == TargetOpcode::IMPLICIT_DEF ||\n           (MI.getDesc().isRematerializable() &&\n            (isReallyTriviallyReMaterializable(MI, AA) ||\n             isReallyTriviallyReMaterializableGeneric(MI, AA)));\n  }\n\nprotected:\n  /// For instructions with opcodes for which the M_REMATERIALIZABLE flag is\n  /// set, this hook lets the target specify whether the instruction is actually\n  /// trivially rematerializable, taking into consideration its operands. This\n  /// predicate must return false if the instruction has any side effects other\n  /// than producing a value, or if it requres any address registers that are\n  /// not always available.\n  /// Requirements must be check as stated in isTriviallyReMaterializable() .\n  virtual bool isReallyTriviallyReMaterializable(const MachineInstr &MI,\n                                                 AAResults *AA) const {\n    return false;\n  }\n\n  /// This method commutes the operands of the given machine instruction MI.\n  /// The operands to be commuted are specified by their indices OpIdx1 and\n  /// OpIdx2.\n  ///\n  /// If a target has any instructions that are commutable but require\n  /// converting to different instructions or making non-trivial changes\n  /// to commute them, this method can be overloaded to do that.\n  /// The default implementation simply swaps the commutable operands.\n  ///\n  /// If NewMI is false, MI is modified in place and returned; otherwise, a\n  /// new machine instruction is created and returned.\n  ///\n  /// Do not call this method for a non-commutable instruction.\n  /// Even though the instruction is commutable, the method may still\n  /// fail to commute the operands, null pointer is returned in such cases.\n  virtual MachineInstr *commuteInstructionImpl(MachineInstr &MI, bool NewMI,\n                                               unsigned OpIdx1,\n                                               unsigned OpIdx2) const;\n\n  /// Assigns the (CommutableOpIdx1, CommutableOpIdx2) pair of commutable\n  /// operand indices to (ResultIdx1, ResultIdx2).\n  /// One or both input values of the pair: (ResultIdx1, ResultIdx2) may be\n  /// predefined to some indices or be undefined (designated by the special\n  /// value 'CommuteAnyOperandIndex').\n  /// The predefined result indices cannot be re-defined.\n  /// The function returns true iff after the result pair redefinition\n  /// the fixed result pair is equal to or equivalent to the source pair of\n  /// indices: (CommutableOpIdx1, CommutableOpIdx2). It is assumed here that\n  /// the pairs (x,y) and (y,x) are equivalent.\n  static bool fixCommutedOpIndices(unsigned &ResultIdx1, unsigned &ResultIdx2,\n                                   unsigned CommutableOpIdx1,\n                                   unsigned CommutableOpIdx2);\n\nprivate:\n  /// For instructions with opcodes for which the M_REMATERIALIZABLE flag is\n  /// set and the target hook isReallyTriviallyReMaterializable returns false,\n  /// this function does target-independent tests to determine if the\n  /// instruction is really trivially rematerializable.\n  bool isReallyTriviallyReMaterializableGeneric(const MachineInstr &MI,\n                                                AAResults *AA) const;\n\npublic:\n  /// These methods return the opcode of the frame setup/destroy instructions\n  /// if they exist (-1 otherwise).  Some targets use pseudo instructions in\n  /// order to abstract away the difference between operating with a frame\n  /// pointer and operating without, through the use of these two instructions.\n  ///\n  unsigned getCallFrameSetupOpcode() const { return CallFrameSetupOpcode; }\n  unsigned getCallFrameDestroyOpcode() const { return CallFrameDestroyOpcode; }\n\n  /// Returns true if the argument is a frame pseudo instruction.\n  bool isFrameInstr(const MachineInstr &I) const {\n    return I.getOpcode() == getCallFrameSetupOpcode() ||\n           I.getOpcode() == getCallFrameDestroyOpcode();\n  }\n\n  /// Returns true if the argument is a frame setup pseudo instruction.\n  bool isFrameSetup(const MachineInstr &I) const {\n    return I.getOpcode() == getCallFrameSetupOpcode();\n  }\n\n  /// Returns size of the frame associated with the given frame instruction.\n  /// For frame setup instruction this is frame that is set up space set up\n  /// after the instruction. For frame destroy instruction this is the frame\n  /// freed by the caller.\n  /// Note, in some cases a call frame (or a part of it) may be prepared prior\n  /// to the frame setup instruction. It occurs in the calls that involve\n  /// inalloca arguments. This function reports only the size of the frame part\n  /// that is set up between the frame setup and destroy pseudo instructions.\n  int64_t getFrameSize(const MachineInstr &I) const {\n    assert(isFrameInstr(I) && \"Not a frame instruction\");\n    assert(I.getOperand(0).getImm() >= 0);\n    return I.getOperand(0).getImm();\n  }\n\n  /// Returns the total frame size, which is made up of the space set up inside\n  /// the pair of frame start-stop instructions and the space that is set up\n  /// prior to the pair.\n  int64_t getFrameTotalSize(const MachineInstr &I) const {\n    if (isFrameSetup(I)) {\n      assert(I.getOperand(1).getImm() >= 0 &&\n             \"Frame size must not be negative\");\n      return getFrameSize(I) + I.getOperand(1).getImm();\n    }\n    return getFrameSize(I);\n  }\n\n  unsigned getCatchReturnOpcode() const { return CatchRetOpcode; }\n  unsigned getReturnOpcode() const { return ReturnOpcode; }\n\n  /// Returns the actual stack pointer adjustment made by an instruction\n  /// as part of a call sequence. By default, only call frame setup/destroy\n  /// instructions adjust the stack, but targets may want to override this\n  /// to enable more fine-grained adjustment, or adjust by a different value.\n  virtual int getSPAdjust(const MachineInstr &MI) const;\n\n  /// Return true if the instruction is a \"coalescable\" extension instruction.\n  /// That is, it's like a copy where it's legal for the source to overlap the\n  /// destination. e.g. X86::MOVSX64rr32. If this returns true, then it's\n  /// expected the pre-extension value is available as a subreg of the result\n  /// register. This also returns the sub-register index in SubIdx.\n  virtual bool isCoalescableExtInstr(const MachineInstr &MI, Register &SrcReg,\n                                     Register &DstReg, unsigned &SubIdx) const {\n    return false;\n  }\n\n  /// If the specified machine instruction is a direct\n  /// load from a stack slot, return the virtual or physical register number of\n  /// the destination along with the FrameIndex of the loaded stack slot.  If\n  /// not, return 0.  This predicate must return 0 if the instruction has\n  /// any side effects other than loading from the stack slot.\n  virtual unsigned isLoadFromStackSlot(const MachineInstr &MI,\n                                       int &FrameIndex) const {\n    return 0;\n  }\n\n  /// Optional extension of isLoadFromStackSlot that returns the number of\n  /// bytes loaded from the stack. This must be implemented if a backend\n  /// supports partial stack slot spills/loads to further disambiguate\n  /// what the load does.\n  virtual unsigned isLoadFromStackSlot(const MachineInstr &MI,\n                                       int &FrameIndex,\n                                       unsigned &MemBytes) const {\n    MemBytes = 0;\n    return isLoadFromStackSlot(MI, FrameIndex);\n  }\n\n  /// Check for post-frame ptr elimination stack locations as well.\n  /// This uses a heuristic so it isn't reliable for correctness.\n  virtual unsigned isLoadFromStackSlotPostFE(const MachineInstr &MI,\n                                             int &FrameIndex) const {\n    return 0;\n  }\n\n  /// If the specified machine instruction has a load from a stack slot,\n  /// return true along with the FrameIndices of the loaded stack slot and the\n  /// machine mem operands containing the reference.\n  /// If not, return false.  Unlike isLoadFromStackSlot, this returns true for\n  /// any instructions that loads from the stack.  This is just a hint, as some\n  /// cases may be missed.\n  virtual bool hasLoadFromStackSlot(\n      const MachineInstr &MI,\n      SmallVectorImpl<const MachineMemOperand *> &Accesses) const;\n\n  /// If the specified machine instruction is a direct\n  /// store to a stack slot, return the virtual or physical register number of\n  /// the source reg along with the FrameIndex of the loaded stack slot.  If\n  /// not, return 0.  This predicate must return 0 if the instruction has\n  /// any side effects other than storing to the stack slot.\n  virtual unsigned isStoreToStackSlot(const MachineInstr &MI,\n                                      int &FrameIndex) const {\n    return 0;\n  }\n\n  /// Optional extension of isStoreToStackSlot that returns the number of\n  /// bytes stored to the stack. This must be implemented if a backend\n  /// supports partial stack slot spills/loads to further disambiguate\n  /// what the store does.\n  virtual unsigned isStoreToStackSlot(const MachineInstr &MI,\n                                      int &FrameIndex,\n                                      unsigned &MemBytes) const {\n    MemBytes = 0;\n    return isStoreToStackSlot(MI, FrameIndex);\n  }\n\n  /// Check for post-frame ptr elimination stack locations as well.\n  /// This uses a heuristic, so it isn't reliable for correctness.\n  virtual unsigned isStoreToStackSlotPostFE(const MachineInstr &MI,\n                                            int &FrameIndex) const {\n    return 0;\n  }\n\n  /// If the specified machine instruction has a store to a stack slot,\n  /// return true along with the FrameIndices of the loaded stack slot and the\n  /// machine mem operands containing the reference.\n  /// If not, return false.  Unlike isStoreToStackSlot,\n  /// this returns true for any instructions that stores to the\n  /// stack.  This is just a hint, as some cases may be missed.\n  virtual bool hasStoreToStackSlot(\n      const MachineInstr &MI,\n      SmallVectorImpl<const MachineMemOperand *> &Accesses) const;\n\n  /// Return true if the specified machine instruction\n  /// is a copy of one stack slot to another and has no other effect.\n  /// Provide the identity of the two frame indices.\n  virtual bool isStackSlotCopy(const MachineInstr &MI, int &DestFrameIndex,\n                               int &SrcFrameIndex) const {\n    return false;\n  }\n\n  /// Compute the size in bytes and offset within a stack slot of a spilled\n  /// register or subregister.\n  ///\n  /// \\param [out] Size in bytes of the spilled value.\n  /// \\param [out] Offset in bytes within the stack slot.\n  /// \\returns true if both Size and Offset are successfully computed.\n  ///\n  /// Not all subregisters have computable spill slots. For example,\n  /// subregisters registers may not be byte-sized, and a pair of discontiguous\n  /// subregisters has no single offset.\n  ///\n  /// Targets with nontrivial bigendian implementations may need to override\n  /// this, particularly to support spilled vector registers.\n  virtual bool getStackSlotRange(const TargetRegisterClass *RC, unsigned SubIdx,\n                                 unsigned &Size, unsigned &Offset,\n                                 const MachineFunction &MF) const;\n\n  /// Return true if the given instruction is terminator that is unspillable,\n  /// according to isUnspillableTerminatorImpl.\n  bool isUnspillableTerminator(const MachineInstr *MI) const {\n    return MI->isTerminator() && isUnspillableTerminatorImpl(MI);\n  }\n\n  /// Returns the size in bytes of the specified MachineInstr, or ~0U\n  /// when this function is not implemented by a target.\n  virtual unsigned getInstSizeInBytes(const MachineInstr &MI) const {\n    return ~0U;\n  }\n\n  /// Return true if the instruction is as cheap as a move instruction.\n  ///\n  /// Targets for different archs need to override this, and different\n  /// micro-architectures can also be finely tuned inside.\n  virtual bool isAsCheapAsAMove(const MachineInstr &MI) const {\n    return MI.isAsCheapAsAMove();\n  }\n\n  /// Return true if the instruction should be sunk by MachineSink.\n  ///\n  /// MachineSink determines on its own whether the instruction is safe to sink;\n  /// this gives the target a hook to override the default behavior with regards\n  /// to which instructions should be sunk.\n  virtual bool shouldSink(const MachineInstr &MI) const { return true; }\n\n  /// Re-issue the specified 'original' instruction at the\n  /// specific location targeting a new destination register.\n  /// The register in Orig->getOperand(0).getReg() will be substituted by\n  /// DestReg:SubIdx. Any existing subreg index is preserved or composed with\n  /// SubIdx.\n  virtual void reMaterialize(MachineBasicBlock &MBB,\n                             MachineBasicBlock::iterator MI, Register DestReg,\n                             unsigned SubIdx, const MachineInstr &Orig,\n                             const TargetRegisterInfo &TRI) const;\n\n  /// Clones instruction or the whole instruction bundle \\p Orig and\n  /// insert into \\p MBB before \\p InsertBefore. The target may update operands\n  /// that are required to be unique.\n  ///\n  /// \\p Orig must not return true for MachineInstr::isNotDuplicable().\n  virtual MachineInstr &duplicate(MachineBasicBlock &MBB,\n                                  MachineBasicBlock::iterator InsertBefore,\n                                  const MachineInstr &Orig) const;\n\n  /// This method must be implemented by targets that\n  /// set the M_CONVERTIBLE_TO_3_ADDR flag.  When this flag is set, the target\n  /// may be able to convert a two-address instruction into one or more true\n  /// three-address instructions on demand.  This allows the X86 target (for\n  /// example) to convert ADD and SHL instructions into LEA instructions if they\n  /// would require register copies due to two-addressness.\n  ///\n  /// This method returns a null pointer if the transformation cannot be\n  /// performed, otherwise it returns the last new instruction.\n  ///\n  virtual MachineInstr *convertToThreeAddress(MachineFunction::iterator &MFI,\n                                              MachineInstr &MI,\n                                              LiveVariables *LV) const {\n    return nullptr;\n  }\n\n  // This constant can be used as an input value of operand index passed to\n  // the method findCommutedOpIndices() to tell the method that the\n  // corresponding operand index is not pre-defined and that the method\n  // can pick any commutable operand.\n  static const unsigned CommuteAnyOperandIndex = ~0U;\n\n  /// This method commutes the operands of the given machine instruction MI.\n  ///\n  /// The operands to be commuted are specified by their indices OpIdx1 and\n  /// OpIdx2. OpIdx1 and OpIdx2 arguments may be set to a special value\n  /// 'CommuteAnyOperandIndex', which means that the method is free to choose\n  /// any arbitrarily chosen commutable operand. If both arguments are set to\n  /// 'CommuteAnyOperandIndex' then the method looks for 2 different commutable\n  /// operands; then commutes them if such operands could be found.\n  ///\n  /// If NewMI is false, MI is modified in place and returned; otherwise, a\n  /// new machine instruction is created and returned.\n  ///\n  /// Do not call this method for a non-commutable instruction or\n  /// for non-commuable operands.\n  /// Even though the instruction is commutable, the method may still\n  /// fail to commute the operands, null pointer is returned in such cases.\n  MachineInstr *\n  commuteInstruction(MachineInstr &MI, bool NewMI = false,\n                     unsigned OpIdx1 = CommuteAnyOperandIndex,\n                     unsigned OpIdx2 = CommuteAnyOperandIndex) const;\n\n  /// Returns true iff the routine could find two commutable operands in the\n  /// given machine instruction.\n  /// The 'SrcOpIdx1' and 'SrcOpIdx2' are INPUT and OUTPUT arguments.\n  /// If any of the INPUT values is set to the special value\n  /// 'CommuteAnyOperandIndex' then the method arbitrarily picks a commutable\n  /// operand, then returns its index in the corresponding argument.\n  /// If both of INPUT values are set to 'CommuteAnyOperandIndex' then method\n  /// looks for 2 commutable operands.\n  /// If INPUT values refer to some operands of MI, then the method simply\n  /// returns true if the corresponding operands are commutable and returns\n  /// false otherwise.\n  ///\n  /// For example, calling this method this way:\n  ///     unsigned Op1 = 1, Op2 = CommuteAnyOperandIndex;\n  ///     findCommutedOpIndices(MI, Op1, Op2);\n  /// can be interpreted as a query asking to find an operand that would be\n  /// commutable with the operand#1.\n  virtual bool findCommutedOpIndices(const MachineInstr &MI,\n                                     unsigned &SrcOpIdx1,\n                                     unsigned &SrcOpIdx2) const;\n\n  /// A pair composed of a register and a sub-register index.\n  /// Used to give some type checking when modeling Reg:SubReg.\n  struct RegSubRegPair {\n    Register Reg;\n    unsigned SubReg;\n\n    RegSubRegPair(Register Reg = Register(), unsigned SubReg = 0)\n        : Reg(Reg), SubReg(SubReg) {}\n\n    bool operator==(const RegSubRegPair& P) const {\n      return Reg == P.Reg && SubReg == P.SubReg;\n    }\n    bool operator!=(const RegSubRegPair& P) const {\n      return !(*this == P);\n    }\n  };\n\n  /// A pair composed of a pair of a register and a sub-register index,\n  /// and another sub-register index.\n  /// Used to give some type checking when modeling Reg:SubReg1, SubReg2.\n  struct RegSubRegPairAndIdx : RegSubRegPair {\n    unsigned SubIdx;\n\n    RegSubRegPairAndIdx(Register Reg = Register(), unsigned SubReg = 0,\n                        unsigned SubIdx = 0)\n        : RegSubRegPair(Reg, SubReg), SubIdx(SubIdx) {}\n  };\n\n  /// Build the equivalent inputs of a REG_SEQUENCE for the given \\p MI\n  /// and \\p DefIdx.\n  /// \\p [out] InputRegs of the equivalent REG_SEQUENCE. Each element of\n  /// the list is modeled as <Reg:SubReg, SubIdx>. Operands with the undef\n  /// flag are not added to this list.\n  /// E.g., REG_SEQUENCE %1:sub1, sub0, %2, sub1 would produce\n  /// two elements:\n  /// - %1:sub1, sub0\n  /// - %2<:0>, sub1\n  ///\n  /// \\returns true if it is possible to build such an input sequence\n  /// with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isRegSequence() or MI.isRegSequenceLike().\n  ///\n  /// \\note The generic implementation does not provide any support for\n  /// MI.isRegSequenceLike(). In other words, one has to override\n  /// getRegSequenceLikeInputs for target specific instructions.\n  bool\n  getRegSequenceInputs(const MachineInstr &MI, unsigned DefIdx,\n                       SmallVectorImpl<RegSubRegPairAndIdx> &InputRegs) const;\n\n  /// Build the equivalent inputs of a EXTRACT_SUBREG for the given \\p MI\n  /// and \\p DefIdx.\n  /// \\p [out] InputReg of the equivalent EXTRACT_SUBREG.\n  /// E.g., EXTRACT_SUBREG %1:sub1, sub0, sub1 would produce:\n  /// - %1:sub1, sub0\n  ///\n  /// \\returns true if it is possible to build such an input sequence\n  /// with the pair \\p MI, \\p DefIdx and the operand has no undef flag set.\n  /// False otherwise.\n  ///\n  /// \\pre MI.isExtractSubreg() or MI.isExtractSubregLike().\n  ///\n  /// \\note The generic implementation does not provide any support for\n  /// MI.isExtractSubregLike(). In other words, one has to override\n  /// getExtractSubregLikeInputs for target specific instructions.\n  bool getExtractSubregInputs(const MachineInstr &MI, unsigned DefIdx,\n                              RegSubRegPairAndIdx &InputReg) const;\n\n  /// Build the equivalent inputs of a INSERT_SUBREG for the given \\p MI\n  /// and \\p DefIdx.\n  /// \\p [out] BaseReg and \\p [out] InsertedReg contain\n  /// the equivalent inputs of INSERT_SUBREG.\n  /// E.g., INSERT_SUBREG %0:sub0, %1:sub1, sub3 would produce:\n  /// - BaseReg: %0:sub0\n  /// - InsertedReg: %1:sub1, sub3\n  ///\n  /// \\returns true if it is possible to build such an input sequence\n  /// with the pair \\p MI, \\p DefIdx and the operand has no undef flag set.\n  /// False otherwise.\n  ///\n  /// \\pre MI.isInsertSubreg() or MI.isInsertSubregLike().\n  ///\n  /// \\note The generic implementation does not provide any support for\n  /// MI.isInsertSubregLike(). In other words, one has to override\n  /// getInsertSubregLikeInputs for target specific instructions.\n  bool getInsertSubregInputs(const MachineInstr &MI, unsigned DefIdx,\n                             RegSubRegPair &BaseReg,\n                             RegSubRegPairAndIdx &InsertedReg) const;\n\n  /// Return true if two machine instructions would produce identical values.\n  /// By default, this is only true when the two instructions\n  /// are deemed identical except for defs. If this function is called when the\n  /// IR is still in SSA form, the caller can pass the MachineRegisterInfo for\n  /// aggressive checks.\n  virtual bool produceSameValue(const MachineInstr &MI0,\n                                const MachineInstr &MI1,\n                                const MachineRegisterInfo *MRI = nullptr) const;\n\n  /// \\returns true if a branch from an instruction with opcode \\p BranchOpc\n  ///  bytes is capable of jumping to a position \\p BrOffset bytes away.\n  virtual bool isBranchOffsetInRange(unsigned BranchOpc,\n                                     int64_t BrOffset) const {\n    llvm_unreachable(\"target did not implement\");\n  }\n\n  /// \\returns The block that branch instruction \\p MI jumps to.\n  virtual MachineBasicBlock *getBranchDestBlock(const MachineInstr &MI) const {\n    llvm_unreachable(\"target did not implement\");\n  }\n\n  /// Insert an unconditional indirect branch at the end of \\p MBB to \\p\n  /// NewDestBB.  \\p BrOffset indicates the offset of \\p NewDestBB relative to\n  /// the offset of the position to insert the new branch.\n  ///\n  /// \\returns The number of bytes added to the block.\n  virtual unsigned insertIndirectBranch(MachineBasicBlock &MBB,\n                                        MachineBasicBlock &NewDestBB,\n                                        const DebugLoc &DL,\n                                        int64_t BrOffset = 0,\n                                        RegScavenger *RS = nullptr) const {\n    llvm_unreachable(\"target did not implement\");\n  }\n\n  /// Analyze the branching code at the end of MBB, returning\n  /// true if it cannot be understood (e.g. it's a switch dispatch or isn't\n  /// implemented for a target).  Upon success, this returns false and returns\n  /// with the following information in various cases:\n  ///\n  /// 1. If this block ends with no branches (it just falls through to its succ)\n  ///    just return false, leaving TBB/FBB null.\n  /// 2. If this block ends with only an unconditional branch, it sets TBB to be\n  ///    the destination block.\n  /// 3. If this block ends with a conditional branch and it falls through to a\n  ///    successor block, it sets TBB to be the branch destination block and a\n  ///    list of operands that evaluate the condition. These operands can be\n  ///    passed to other TargetInstrInfo methods to create new branches.\n  /// 4. If this block ends with a conditional branch followed by an\n  ///    unconditional branch, it returns the 'true' destination in TBB, the\n  ///    'false' destination in FBB, and a list of operands that evaluate the\n  ///    condition.  These operands can be passed to other TargetInstrInfo\n  ///    methods to create new branches.\n  ///\n  /// Note that removeBranch and insertBranch must be implemented to support\n  /// cases where this method returns success.\n  ///\n  /// If AllowModify is true, then this routine is allowed to modify the basic\n  /// block (e.g. delete instructions after the unconditional branch).\n  ///\n  /// The CFG information in MBB.Predecessors and MBB.Successors must be valid\n  /// before calling this function.\n  virtual bool analyzeBranch(MachineBasicBlock &MBB, MachineBasicBlock *&TBB,\n                             MachineBasicBlock *&FBB,\n                             SmallVectorImpl<MachineOperand> &Cond,\n                             bool AllowModify = false) const {\n    return true;\n  }\n\n  /// Represents a predicate at the MachineFunction level.  The control flow a\n  /// MachineBranchPredicate represents is:\n  ///\n  ///  Reg = LHS `Predicate` RHS         == ConditionDef\n  ///  if Reg then goto TrueDest else goto FalseDest\n  ///\n  struct MachineBranchPredicate {\n    enum ComparePredicate {\n      PRED_EQ,     // True if two values are equal\n      PRED_NE,     // True if two values are not equal\n      PRED_INVALID // Sentinel value\n    };\n\n    ComparePredicate Predicate = PRED_INVALID;\n    MachineOperand LHS = MachineOperand::CreateImm(0);\n    MachineOperand RHS = MachineOperand::CreateImm(0);\n    MachineBasicBlock *TrueDest = nullptr;\n    MachineBasicBlock *FalseDest = nullptr;\n    MachineInstr *ConditionDef = nullptr;\n\n    /// SingleUseCondition is true if ConditionDef is dead except for the\n    /// branch(es) at the end of the basic block.\n    ///\n    bool SingleUseCondition = false;\n\n    explicit MachineBranchPredicate() = default;\n  };\n\n  /// Analyze the branching code at the end of MBB and parse it into the\n  /// MachineBranchPredicate structure if possible.  Returns false on success\n  /// and true on failure.\n  ///\n  /// If AllowModify is true, then this routine is allowed to modify the basic\n  /// block (e.g. delete instructions after the unconditional branch).\n  ///\n  virtual bool analyzeBranchPredicate(MachineBasicBlock &MBB,\n                                      MachineBranchPredicate &MBP,\n                                      bool AllowModify = false) const {\n    return true;\n  }\n\n  /// Remove the branching code at the end of the specific MBB.\n  /// This is only invoked in cases where analyzeBranch returns success. It\n  /// returns the number of instructions that were removed.\n  /// If \\p BytesRemoved is non-null, report the change in code size from the\n  /// removed instructions.\n  virtual unsigned removeBranch(MachineBasicBlock &MBB,\n                                int *BytesRemoved = nullptr) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::removeBranch!\");\n  }\n\n  /// Insert branch code into the end of the specified MachineBasicBlock. The\n  /// operands to this method are the same as those returned by analyzeBranch.\n  /// This is only invoked in cases where analyzeBranch returns success. It\n  /// returns the number of instructions inserted. If \\p BytesAdded is non-null,\n  /// report the change in code size from the added instructions.\n  ///\n  /// It is also invoked by tail merging to add unconditional branches in\n  /// cases where analyzeBranch doesn't apply because there was no original\n  /// branch to analyze.  At least this much must be implemented, else tail\n  /// merging needs to be disabled.\n  ///\n  /// The CFG information in MBB.Predecessors and MBB.Successors must be valid\n  /// before calling this function.\n  virtual unsigned insertBranch(MachineBasicBlock &MBB, MachineBasicBlock *TBB,\n                                MachineBasicBlock *FBB,\n                                ArrayRef<MachineOperand> Cond,\n                                const DebugLoc &DL,\n                                int *BytesAdded = nullptr) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::insertBranch!\");\n  }\n\n  unsigned insertUnconditionalBranch(MachineBasicBlock &MBB,\n                                     MachineBasicBlock *DestBB,\n                                     const DebugLoc &DL,\n                                     int *BytesAdded = nullptr) const {\n    return insertBranch(MBB, DestBB, nullptr, ArrayRef<MachineOperand>(), DL,\n                        BytesAdded);\n  }\n\n  /// Object returned by analyzeLoopForPipelining. Allows software pipelining\n  /// implementations to query attributes of the loop being pipelined and to\n  /// apply target-specific updates to the loop once pipelining is complete.\n  class PipelinerLoopInfo {\n  public:\n    virtual ~PipelinerLoopInfo();\n    /// Return true if the given instruction should not be pipelined and should\n    /// be ignored. An example could be a loop comparison, or induction variable\n    /// update with no users being pipelined.\n    virtual bool shouldIgnoreForPipelining(const MachineInstr *MI) const = 0;\n\n    /// Create a condition to determine if the trip count of the loop is greater\n    /// than TC.\n    ///\n    /// If the trip count is statically known to be greater than TC, return\n    /// true. If the trip count is statically known to be not greater than TC,\n    /// return false. Otherwise return nullopt and fill out Cond with the test\n    /// condition.\n    virtual Optional<bool>\n    createTripCountGreaterCondition(int TC, MachineBasicBlock &MBB,\n                                    SmallVectorImpl<MachineOperand> &Cond) = 0;\n\n    /// Modify the loop such that the trip count is\n    /// OriginalTC + TripCountAdjust.\n    virtual void adjustTripCount(int TripCountAdjust) = 0;\n\n    /// Called when the loop's preheader has been modified to NewPreheader.\n    virtual void setPreheader(MachineBasicBlock *NewPreheader) = 0;\n\n    /// Called when the loop is being removed. Any instructions in the preheader\n    /// should be removed.\n    ///\n    /// Once this function is called, no other functions on this object are\n    /// valid; the loop has been removed.\n    virtual void disposed() = 0;\n  };\n\n  /// Analyze loop L, which must be a single-basic-block loop, and if the\n  /// conditions can be understood enough produce a PipelinerLoopInfo object.\n  virtual std::unique_ptr<PipelinerLoopInfo>\n  analyzeLoopForPipelining(MachineBasicBlock *LoopBB) const {\n    return nullptr;\n  }\n\n  /// Analyze the loop code, return true if it cannot be understood. Upon\n  /// success, this function returns false and returns information about the\n  /// induction variable and compare instruction used at the end.\n  virtual bool analyzeLoop(MachineLoop &L, MachineInstr *&IndVarInst,\n                           MachineInstr *&CmpInst) const {\n    return true;\n  }\n\n  /// Generate code to reduce the loop iteration by one and check if the loop\n  /// is finished.  Return the value/register of the new loop count.  We need\n  /// this function when peeling off one or more iterations of a loop. This\n  /// function assumes the nth iteration is peeled first.\n  virtual unsigned reduceLoopCount(MachineBasicBlock &MBB,\n                                   MachineBasicBlock &PreHeader,\n                                   MachineInstr *IndVar, MachineInstr &Cmp,\n                                   SmallVectorImpl<MachineOperand> &Cond,\n                                   SmallVectorImpl<MachineInstr *> &PrevInsts,\n                                   unsigned Iter, unsigned MaxIter) const {\n    llvm_unreachable(\"Target didn't implement ReduceLoopCount\");\n  }\n\n  /// Delete the instruction OldInst and everything after it, replacing it with\n  /// an unconditional branch to NewDest. This is used by the tail merging pass.\n  virtual void ReplaceTailWithBranchTo(MachineBasicBlock::iterator Tail,\n                                       MachineBasicBlock *NewDest) const;\n\n  /// Return true if it's legal to split the given basic\n  /// block at the specified instruction (i.e. instruction would be the start\n  /// of a new basic block).\n  virtual bool isLegalToSplitMBBAt(MachineBasicBlock &MBB,\n                                   MachineBasicBlock::iterator MBBI) const {\n    return true;\n  }\n\n  /// Return true if it's profitable to predicate\n  /// instructions with accumulated instruction latency of \"NumCycles\"\n  /// of the specified basic block, where the probability of the instructions\n  /// being executed is given by Probability, and Confidence is a measure\n  /// of our confidence that it will be properly predicted.\n  virtual bool isProfitableToIfCvt(MachineBasicBlock &MBB, unsigned NumCycles,\n                                   unsigned ExtraPredCycles,\n                                   BranchProbability Probability) const {\n    return false;\n  }\n\n  /// Second variant of isProfitableToIfCvt. This one\n  /// checks for the case where two basic blocks from true and false path\n  /// of a if-then-else (diamond) are predicated on mutually exclusive\n  /// predicates, where the probability of the true path being taken is given\n  /// by Probability, and Confidence is a measure of our confidence that it\n  /// will be properly predicted.\n  virtual bool isProfitableToIfCvt(MachineBasicBlock &TMBB, unsigned NumTCycles,\n                                   unsigned ExtraTCycles,\n                                   MachineBasicBlock &FMBB, unsigned NumFCycles,\n                                   unsigned ExtraFCycles,\n                                   BranchProbability Probability) const {\n    return false;\n  }\n\n  /// Return true if it's profitable for if-converter to duplicate instructions\n  /// of specified accumulated instruction latencies in the specified MBB to\n  /// enable if-conversion.\n  /// The probability of the instructions being executed is given by\n  /// Probability, and Confidence is a measure of our confidence that it\n  /// will be properly predicted.\n  virtual bool isProfitableToDupForIfCvt(MachineBasicBlock &MBB,\n                                         unsigned NumCycles,\n                                         BranchProbability Probability) const {\n    return false;\n  }\n\n  /// Return the increase in code size needed to predicate a contiguous run of\n  /// NumInsts instructions.\n  virtual unsigned extraSizeToPredicateInstructions(const MachineFunction &MF,\n                                                    unsigned NumInsts) const {\n    return 0;\n  }\n\n  /// Return an estimate for the code size reduction (in bytes) which will be\n  /// caused by removing the given branch instruction during if-conversion.\n  virtual unsigned predictBranchSizeForIfCvt(MachineInstr &MI) const {\n    return getInstSizeInBytes(MI);\n  }\n\n  /// Return true if it's profitable to unpredicate\n  /// one side of a 'diamond', i.e. two sides of if-else predicated on mutually\n  /// exclusive predicates.\n  /// e.g.\n  ///   subeq  r0, r1, #1\n  ///   addne  r0, r1, #1\n  /// =>\n  ///   sub    r0, r1, #1\n  ///   addne  r0, r1, #1\n  ///\n  /// This may be profitable is conditional instructions are always executed.\n  virtual bool isProfitableToUnpredicate(MachineBasicBlock &TMBB,\n                                         MachineBasicBlock &FMBB) const {\n    return false;\n  }\n\n  /// Return true if it is possible to insert a select\n  /// instruction that chooses between TrueReg and FalseReg based on the\n  /// condition code in Cond.\n  ///\n  /// When successful, also return the latency in cycles from TrueReg,\n  /// FalseReg, and Cond to the destination register. In most cases, a select\n  /// instruction will be 1 cycle, so CondCycles = TrueCycles = FalseCycles = 1\n  ///\n  /// Some x86 implementations have 2-cycle cmov instructions.\n  ///\n  /// @param MBB         Block where select instruction would be inserted.\n  /// @param Cond        Condition returned by analyzeBranch.\n  /// @param DstReg      Virtual dest register that the result should write to.\n  /// @param TrueReg     Virtual register to select when Cond is true.\n  /// @param FalseReg    Virtual register to select when Cond is false.\n  /// @param CondCycles  Latency from Cond+Branch to select output.\n  /// @param TrueCycles  Latency from TrueReg to select output.\n  /// @param FalseCycles Latency from FalseReg to select output.\n  virtual bool canInsertSelect(const MachineBasicBlock &MBB,\n                               ArrayRef<MachineOperand> Cond, Register DstReg,\n                               Register TrueReg, Register FalseReg,\n                               int &CondCycles, int &TrueCycles,\n                               int &FalseCycles) const {\n    return false;\n  }\n\n  /// Insert a select instruction into MBB before I that will copy TrueReg to\n  /// DstReg when Cond is true, and FalseReg to DstReg when Cond is false.\n  ///\n  /// This function can only be called after canInsertSelect() returned true.\n  /// The condition in Cond comes from analyzeBranch, and it can be assumed\n  /// that the same flags or registers required by Cond are available at the\n  /// insertion point.\n  ///\n  /// @param MBB      Block where select instruction should be inserted.\n  /// @param I        Insertion point.\n  /// @param DL       Source location for debugging.\n  /// @param DstReg   Virtual register to be defined by select instruction.\n  /// @param Cond     Condition as computed by analyzeBranch.\n  /// @param TrueReg  Virtual register to copy when Cond is true.\n  /// @param FalseReg Virtual register to copy when Cons is false.\n  virtual void insertSelect(MachineBasicBlock &MBB,\n                            MachineBasicBlock::iterator I, const DebugLoc &DL,\n                            Register DstReg, ArrayRef<MachineOperand> Cond,\n                            Register TrueReg, Register FalseReg) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::insertSelect!\");\n  }\n\n  /// Analyze the given select instruction, returning true if\n  /// it cannot be understood. It is assumed that MI->isSelect() is true.\n  ///\n  /// When successful, return the controlling condition and the operands that\n  /// determine the true and false result values.\n  ///\n  ///   Result = SELECT Cond, TrueOp, FalseOp\n  ///\n  /// Some targets can optimize select instructions, for example by predicating\n  /// the instruction defining one of the operands. Such targets should set\n  /// Optimizable.\n  ///\n  /// @param         MI Select instruction to analyze.\n  /// @param Cond    Condition controlling the select.\n  /// @param TrueOp  Operand number of the value selected when Cond is true.\n  /// @param FalseOp Operand number of the value selected when Cond is false.\n  /// @param Optimizable Returned as true if MI is optimizable.\n  /// @returns False on success.\n  virtual bool analyzeSelect(const MachineInstr &MI,\n                             SmallVectorImpl<MachineOperand> &Cond,\n                             unsigned &TrueOp, unsigned &FalseOp,\n                             bool &Optimizable) const {\n    assert(MI.getDesc().isSelect() && \"MI must be a select instruction\");\n    return true;\n  }\n\n  /// Given a select instruction that was understood by\n  /// analyzeSelect and returned Optimizable = true, attempt to optimize MI by\n  /// merging it with one of its operands. Returns NULL on failure.\n  ///\n  /// When successful, returns the new select instruction. The client is\n  /// responsible for deleting MI.\n  ///\n  /// If both sides of the select can be optimized, PreferFalse is used to pick\n  /// a side.\n  ///\n  /// @param MI          Optimizable select instruction.\n  /// @param NewMIs     Set that record all MIs in the basic block up to \\p\n  /// MI. Has to be updated with any newly created MI or deleted ones.\n  /// @param PreferFalse Try to optimize FalseOp instead of TrueOp.\n  /// @returns Optimized instruction or NULL.\n  virtual MachineInstr *optimizeSelect(MachineInstr &MI,\n                                       SmallPtrSetImpl<MachineInstr *> &NewMIs,\n                                       bool PreferFalse = false) const {\n    // This function must be implemented if Optimizable is ever set.\n    llvm_unreachable(\"Target must implement TargetInstrInfo::optimizeSelect!\");\n  }\n\n  /// Emit instructions to copy a pair of physical registers.\n  ///\n  /// This function should support copies within any legal register class as\n  /// well as any cross-class copies created during instruction selection.\n  ///\n  /// The source and destination registers may overlap, which may require a\n  /// careful implementation when multiple copy instructions are required for\n  /// large registers. See for example the ARM target.\n  virtual void copyPhysReg(MachineBasicBlock &MBB,\n                           MachineBasicBlock::iterator MI, const DebugLoc &DL,\n                           MCRegister DestReg, MCRegister SrcReg,\n                           bool KillSrc) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::copyPhysReg!\");\n  }\n\n  /// Allow targets to tell MachineVerifier whether a specific register\n  /// MachineOperand can be used as part of PC-relative addressing.\n  /// PC-relative addressing modes in many CISC architectures contain\n  /// (non-PC) registers as offsets or scaling values, which inherently\n  /// tags the corresponding MachineOperand with OPERAND_PCREL.\n  ///\n  /// @param MO The MachineOperand in question. MO.isReg() should always\n  /// be true.\n  /// @return Whether this operand is allowed to be used PC-relatively.\n  virtual bool isPCRelRegisterOperandLegal(const MachineOperand &MO) const {\n    return false;\n  }\n\nprotected:\n  /// Target-dependent implementation for IsCopyInstr.\n  /// If the specific machine instruction is a instruction that moves/copies\n  /// value from one register to another register return destination and source\n  /// registers as machine operands.\n  virtual Optional<DestSourcePair>\n  isCopyInstrImpl(const MachineInstr &MI) const {\n    return None;\n  }\n\n  /// Return true if the given terminator MI is not expected to spill. This\n  /// sets the live interval as not spillable and adjusts phi node lowering to\n  /// not introduce copies after the terminator. Use with care, these are\n  /// currently used for hardware loop intrinsics in very controlled situations,\n  /// created prior to registry allocation in loops that only have single phi\n  /// users for the terminators value. They may run out of registers if not used\n  /// carefully.\n  virtual bool isUnspillableTerminatorImpl(const MachineInstr *MI) const {\n    return false;\n  }\n\npublic:\n  /// If the specific machine instruction is a instruction that moves/copies\n  /// value from one register to another register return destination and source\n  /// registers as machine operands.\n  /// For COPY-instruction the method naturally returns destination and source\n  /// registers as machine operands, for all other instructions the method calls\n  /// target-dependent implementation.\n  Optional<DestSourcePair> isCopyInstr(const MachineInstr &MI) const {\n    if (MI.isCopy()) {\n      return DestSourcePair{MI.getOperand(0), MI.getOperand(1)};\n    }\n    return isCopyInstrImpl(MI);\n  }\n\n  /// If the specific machine instruction is an instruction that adds an\n  /// immediate value and a physical register, and stores the result in\n  /// the given physical register \\c Reg, return a pair of the source\n  /// register and the offset which has been added.\n  virtual Optional<RegImmPair> isAddImmediate(const MachineInstr &MI,\n                                              Register Reg) const {\n    return None;\n  }\n\n  /// Returns true if MI is an instruction that defines Reg to have a constant\n  /// value and the value is recorded in ImmVal. The ImmVal is a result that\n  /// should be interpreted as modulo size of Reg.\n  virtual bool getConstValDefinedInReg(const MachineInstr &MI,\n                                       const Register Reg,\n                                       int64_t &ImmVal) const {\n    return false;\n  }\n\n  /// Store the specified register of the given register class to the specified\n  /// stack frame index. The store instruction is to be added to the given\n  /// machine basic block before the specified machine instruction. If isKill\n  /// is true, the register operand is the last use and must be marked kill.\n  virtual void storeRegToStackSlot(MachineBasicBlock &MBB,\n                                   MachineBasicBlock::iterator MI,\n                                   Register SrcReg, bool isKill, int FrameIndex,\n                                   const TargetRegisterClass *RC,\n                                   const TargetRegisterInfo *TRI) const {\n    llvm_unreachable(\"Target didn't implement \"\n                     \"TargetInstrInfo::storeRegToStackSlot!\");\n  }\n\n  /// Load the specified register of the given register class from the specified\n  /// stack frame index. The load instruction is to be added to the given\n  /// machine basic block before the specified machine instruction.\n  virtual void loadRegFromStackSlot(MachineBasicBlock &MBB,\n                                    MachineBasicBlock::iterator MI,\n                                    Register DestReg, int FrameIndex,\n                                    const TargetRegisterClass *RC,\n                                    const TargetRegisterInfo *TRI) const {\n    llvm_unreachable(\"Target didn't implement \"\n                     \"TargetInstrInfo::loadRegFromStackSlot!\");\n  }\n\n  /// This function is called for all pseudo instructions\n  /// that remain after register allocation. Many pseudo instructions are\n  /// created to help register allocation. This is the place to convert them\n  /// into real instructions. The target can edit MI in place, or it can insert\n  /// new instructions and erase MI. The function should return true if\n  /// anything was changed.\n  virtual bool expandPostRAPseudo(MachineInstr &MI) const { return false; }\n\n  /// Check whether the target can fold a load that feeds a subreg operand\n  /// (or a subreg operand that feeds a store).\n  /// For example, X86 may want to return true if it can fold\n  /// movl (%esp), %eax\n  /// subb, %al, ...\n  /// Into:\n  /// subb (%esp), ...\n  ///\n  /// Ideally, we'd like the target implementation of foldMemoryOperand() to\n  /// reject subregs - but since this behavior used to be enforced in the\n  /// target-independent code, moving this responsibility to the targets\n  /// has the potential of causing nasty silent breakage in out-of-tree targets.\n  virtual bool isSubregFoldable() const { return false; }\n\n  /// Attempt to fold a load or store of the specified stack\n  /// slot into the specified machine instruction for the specified operand(s).\n  /// If this is possible, a new instruction is returned with the specified\n  /// operand folded, otherwise NULL is returned.\n  /// The new instruction is inserted before MI, and the client is responsible\n  /// for removing the old instruction.\n  /// If VRM is passed, the assigned physregs can be inspected by target to\n  /// decide on using an opcode (note that those assignments can still change).\n  MachineInstr *foldMemoryOperand(MachineInstr &MI, ArrayRef<unsigned> Ops,\n                                  int FI,\n                                  LiveIntervals *LIS = nullptr,\n                                  VirtRegMap *VRM = nullptr) const;\n\n  /// Same as the previous version except it allows folding of any load and\n  /// store from / to any address, not just from a specific stack slot.\n  MachineInstr *foldMemoryOperand(MachineInstr &MI, ArrayRef<unsigned> Ops,\n                                  MachineInstr &LoadMI,\n                                  LiveIntervals *LIS = nullptr) const;\n\n  /// Return true when there is potentially a faster code sequence\n  /// for an instruction chain ending in \\p Root. All potential patterns are\n  /// returned in the \\p Pattern vector. Pattern should be sorted in priority\n  /// order since the pattern evaluator stops checking as soon as it finds a\n  /// faster sequence.\n  /// \\param Root - Instruction that could be combined with one of its operands\n  /// \\param Patterns - Vector of possible combination patterns\n  virtual bool\n  getMachineCombinerPatterns(MachineInstr &Root,\n                             SmallVectorImpl<MachineCombinerPattern> &Patterns,\n                             bool DoRegPressureReduce) const;\n\n  /// Return true if target supports reassociation of instructions in machine\n  /// combiner pass to reduce register pressure for a given BB.\n  virtual bool\n  shouldReduceRegisterPressure(MachineBasicBlock *MBB,\n                               RegisterClassInfo *RegClassInfo) const {\n    return false;\n  }\n\n  /// Fix up the placeholder we may add in genAlternativeCodeSequence().\n  virtual void\n  finalizeInsInstrs(MachineInstr &Root, MachineCombinerPattern &P,\n                    SmallVectorImpl<MachineInstr *> &InsInstrs) const {}\n\n  /// Return true when a code sequence can improve throughput. It\n  /// should be called only for instructions in loops.\n  /// \\param Pattern - combiner pattern\n  virtual bool isThroughputPattern(MachineCombinerPattern Pattern) const;\n\n  /// Return true if the input \\P Inst is part of a chain of dependent ops\n  /// that are suitable for reassociation, otherwise return false.\n  /// If the instruction's operands must be commuted to have a previous\n  /// instruction of the same type define the first source operand, \\P Commuted\n  /// will be set to true.\n  bool isReassociationCandidate(const MachineInstr &Inst, bool &Commuted) const;\n\n  /// Return true when \\P Inst is both associative and commutative.\n  virtual bool isAssociativeAndCommutative(const MachineInstr &Inst) const {\n    return false;\n  }\n\n  /// Return true when \\P Inst has reassociable operands in the same \\P MBB.\n  virtual bool hasReassociableOperands(const MachineInstr &Inst,\n                                       const MachineBasicBlock *MBB) const;\n\n  /// Return true when \\P Inst has reassociable sibling.\n  bool hasReassociableSibling(const MachineInstr &Inst, bool &Commuted) const;\n\n  /// When getMachineCombinerPatterns() finds patterns, this function generates\n  /// the instructions that could replace the original code sequence. The client\n  /// has to decide whether the actual replacement is beneficial or not.\n  /// \\param Root - Instruction that could be combined with one of its operands\n  /// \\param Pattern - Combination pattern for Root\n  /// \\param InsInstrs - Vector of new instructions that implement P\n  /// \\param DelInstrs - Old instructions, including Root, that could be\n  /// replaced by InsInstr\n  /// \\param InstIdxForVirtReg - map of virtual register to instruction in\n  /// InsInstr that defines it\n  virtual void genAlternativeCodeSequence(\n      MachineInstr &Root, MachineCombinerPattern Pattern,\n      SmallVectorImpl<MachineInstr *> &InsInstrs,\n      SmallVectorImpl<MachineInstr *> &DelInstrs,\n      DenseMap<unsigned, unsigned> &InstIdxForVirtReg) const;\n\n  /// Attempt to reassociate \\P Root and \\P Prev according to \\P Pattern to\n  /// reduce critical path length.\n  void reassociateOps(MachineInstr &Root, MachineInstr &Prev,\n                      MachineCombinerPattern Pattern,\n                      SmallVectorImpl<MachineInstr *> &InsInstrs,\n                      SmallVectorImpl<MachineInstr *> &DelInstrs,\n                      DenseMap<unsigned, unsigned> &InstrIdxForVirtReg) const;\n\n  /// The limit on resource length extension we accept in MachineCombiner Pass.\n  virtual int getExtendResourceLenLimit() const { return 0; }\n\n  /// This is an architecture-specific helper function of reassociateOps.\n  /// Set special operand attributes for new instructions after reassociation.\n  virtual void setSpecialOperandAttr(MachineInstr &OldMI1, MachineInstr &OldMI2,\n                                     MachineInstr &NewMI1,\n                                     MachineInstr &NewMI2) const {}\n\n  virtual void setSpecialOperandAttr(MachineInstr &MI, uint16_t Flags) const {}\n\n  /// Return true when a target supports MachineCombiner.\n  virtual bool useMachineCombiner() const { return false; }\n\n  /// Return true if the given SDNode can be copied during scheduling\n  /// even if it has glue.\n  virtual bool canCopyGluedNodeDuringSchedule(SDNode *N) const { return false; }\n\nprotected:\n  /// Target-dependent implementation for foldMemoryOperand.\n  /// Target-independent code in foldMemoryOperand will\n  /// take care of adding a MachineMemOperand to the newly created instruction.\n  /// The instruction and any auxiliary instructions necessary will be inserted\n  /// at InsertPt.\n  virtual MachineInstr *\n  foldMemoryOperandImpl(MachineFunction &MF, MachineInstr &MI,\n                        ArrayRef<unsigned> Ops,\n                        MachineBasicBlock::iterator InsertPt, int FrameIndex,\n                        LiveIntervals *LIS = nullptr,\n                        VirtRegMap *VRM = nullptr) const {\n    return nullptr;\n  }\n\n  /// Target-dependent implementation for foldMemoryOperand.\n  /// Target-independent code in foldMemoryOperand will\n  /// take care of adding a MachineMemOperand to the newly created instruction.\n  /// The instruction and any auxiliary instructions necessary will be inserted\n  /// at InsertPt.\n  virtual MachineInstr *foldMemoryOperandImpl(\n      MachineFunction &MF, MachineInstr &MI, ArrayRef<unsigned> Ops,\n      MachineBasicBlock::iterator InsertPt, MachineInstr &LoadMI,\n      LiveIntervals *LIS = nullptr) const {\n    return nullptr;\n  }\n\n  /// Target-dependent implementation of getRegSequenceInputs.\n  ///\n  /// \\returns true if it is possible to build the equivalent\n  /// REG_SEQUENCE inputs with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isRegSequenceLike().\n  ///\n  /// \\see TargetInstrInfo::getRegSequenceInputs.\n  virtual bool getRegSequenceLikeInputs(\n      const MachineInstr &MI, unsigned DefIdx,\n      SmallVectorImpl<RegSubRegPairAndIdx> &InputRegs) const {\n    return false;\n  }\n\n  /// Target-dependent implementation of getExtractSubregInputs.\n  ///\n  /// \\returns true if it is possible to build the equivalent\n  /// EXTRACT_SUBREG inputs with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isExtractSubregLike().\n  ///\n  /// \\see TargetInstrInfo::getExtractSubregInputs.\n  virtual bool getExtractSubregLikeInputs(const MachineInstr &MI,\n                                          unsigned DefIdx,\n                                          RegSubRegPairAndIdx &InputReg) const {\n    return false;\n  }\n\n  /// Target-dependent implementation of getInsertSubregInputs.\n  ///\n  /// \\returns true if it is possible to build the equivalent\n  /// INSERT_SUBREG inputs with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isInsertSubregLike().\n  ///\n  /// \\see TargetInstrInfo::getInsertSubregInputs.\n  virtual bool\n  getInsertSubregLikeInputs(const MachineInstr &MI, unsigned DefIdx,\n                            RegSubRegPair &BaseReg,\n                            RegSubRegPairAndIdx &InsertedReg) const {\n    return false;\n  }\n\npublic:\n  /// getAddressSpaceForPseudoSourceKind - Given the kind of memory\n  /// (e.g. stack) the target returns the corresponding address space.\n  virtual unsigned\n  getAddressSpaceForPseudoSourceKind(unsigned Kind) const {\n    return 0;\n  }\n\n  /// unfoldMemoryOperand - Separate a single instruction which folded a load or\n  /// a store or a load and a store into two or more instruction. If this is\n  /// possible, returns true as well as the new instructions by reference.\n  virtual bool\n  unfoldMemoryOperand(MachineFunction &MF, MachineInstr &MI, unsigned Reg,\n                      bool UnfoldLoad, bool UnfoldStore,\n                      SmallVectorImpl<MachineInstr *> &NewMIs) const {\n    return false;\n  }\n\n  virtual bool unfoldMemoryOperand(SelectionDAG &DAG, SDNode *N,\n                                   SmallVectorImpl<SDNode *> &NewNodes) const {\n    return false;\n  }\n\n  /// Returns the opcode of the would be new\n  /// instruction after load / store are unfolded from an instruction of the\n  /// specified opcode. It returns zero if the specified unfolding is not\n  /// possible. If LoadRegIndex is non-null, it is filled in with the operand\n  /// index of the operand which will hold the register holding the loaded\n  /// value.\n  virtual unsigned\n  getOpcodeAfterMemoryUnfold(unsigned Opc, bool UnfoldLoad, bool UnfoldStore,\n                             unsigned *LoadRegIndex = nullptr) const {\n    return 0;\n  }\n\n  /// This is used by the pre-regalloc scheduler to determine if two loads are\n  /// loading from the same base address. It should only return true if the base\n  /// pointers are the same and the only differences between the two addresses\n  /// are the offset. It also returns the offsets by reference.\n  virtual bool areLoadsFromSameBasePtr(SDNode *Load1, SDNode *Load2,\n                                       int64_t &Offset1,\n                                       int64_t &Offset2) const {\n    return false;\n  }\n\n  /// This is a used by the pre-regalloc scheduler to determine (in conjunction\n  /// with areLoadsFromSameBasePtr) if two loads should be scheduled together.\n  /// On some targets if two loads are loading from\n  /// addresses in the same cache line, it's better if they are scheduled\n  /// together. This function takes two integers that represent the load offsets\n  /// from the common base address. It returns true if it decides it's desirable\n  /// to schedule the two loads together. \"NumLoads\" is the number of loads that\n  /// have already been scheduled after Load1.\n  virtual bool shouldScheduleLoadsNear(SDNode *Load1, SDNode *Load2,\n                                       int64_t Offset1, int64_t Offset2,\n                                       unsigned NumLoads) const {\n    return false;\n  }\n\n  /// Get the base operand and byte offset of an instruction that reads/writes\n  /// memory. This is a convenience function for callers that are only prepared\n  /// to handle a single base operand.\n  bool getMemOperandWithOffset(const MachineInstr &MI,\n                               const MachineOperand *&BaseOp, int64_t &Offset,\n                               bool &OffsetIsScalable,\n                               const TargetRegisterInfo *TRI) const;\n\n  /// Get zero or more base operands and the byte offset of an instruction that\n  /// reads/writes memory. Note that there may be zero base operands if the\n  /// instruction accesses a constant address.\n  /// It returns false if MI does not read/write memory.\n  /// It returns false if base operands and offset could not be determined.\n  /// It is not guaranteed to always recognize base operands and offsets in all\n  /// cases.\n  virtual bool getMemOperandsWithOffsetWidth(\n      const MachineInstr &MI, SmallVectorImpl<const MachineOperand *> &BaseOps,\n      int64_t &Offset, bool &OffsetIsScalable, unsigned &Width,\n      const TargetRegisterInfo *TRI) const {\n    return false;\n  }\n\n  /// Return true if the instruction contains a base register and offset. If\n  /// true, the function also sets the operand position in the instruction\n  /// for the base register and offset.\n  virtual bool getBaseAndOffsetPosition(const MachineInstr &MI,\n                                        unsigned &BasePos,\n                                        unsigned &OffsetPos) const {\n    return false;\n  }\n\n  /// Target dependent implementation to get the values constituting the address\n  /// MachineInstr that is accessing memory. These values are returned as a\n  /// struct ExtAddrMode which contains all relevant information to make up the\n  /// address.\n  virtual Optional<ExtAddrMode>\n  getAddrModeFromMemoryOp(const MachineInstr &MemI,\n                          const TargetRegisterInfo *TRI) const {\n    return None;\n  }\n\n  /// Returns true if MI's Def is NullValueReg, and the MI\n  /// does not change the Zero value. i.e. cases such as rax = shr rax, X where\n  /// NullValueReg = rax. Note that if the NullValueReg is non-zero, this\n  /// function can return true even if becomes zero. Specifically cases such as\n  /// NullValueReg = shl NullValueReg, 63.\n  virtual bool preservesZeroValueInReg(const MachineInstr *MI,\n                                       const Register NullValueReg,\n                                       const TargetRegisterInfo *TRI) const {\n    return false;\n  }\n\n  /// If the instruction is an increment of a constant value, return the amount.\n  virtual bool getIncrementValue(const MachineInstr &MI, int &Value) const {\n    return false;\n  }\n\n  /// Returns true if the two given memory operations should be scheduled\n  /// adjacent. Note that you have to add:\n  ///   DAG->addMutation(createLoadClusterDAGMutation(DAG->TII, DAG->TRI));\n  /// or\n  ///   DAG->addMutation(createStoreClusterDAGMutation(DAG->TII, DAG->TRI));\n  /// to TargetPassConfig::createMachineScheduler() to have an effect.\n  ///\n  /// \\p BaseOps1 and \\p BaseOps2 are memory operands of two memory operations.\n  /// \\p NumLoads is the number of loads that will be in the cluster if this\n  /// hook returns true.\n  /// \\p NumBytes is the number of bytes that will be loaded from all the\n  /// clustered loads if this hook returns true.\n  virtual bool shouldClusterMemOps(ArrayRef<const MachineOperand *> BaseOps1,\n                                   ArrayRef<const MachineOperand *> BaseOps2,\n                                   unsigned NumLoads, unsigned NumBytes) const {\n    llvm_unreachable(\"target did not implement shouldClusterMemOps()\");\n  }\n\n  /// Reverses the branch condition of the specified condition list,\n  /// returning false on success and true if it cannot be reversed.\n  virtual bool\n  reverseBranchCondition(SmallVectorImpl<MachineOperand> &Cond) const {\n    return true;\n  }\n\n  /// Insert a noop into the instruction stream at the specified point.\n  virtual void insertNoop(MachineBasicBlock &MBB,\n                          MachineBasicBlock::iterator MI) const;\n\n  /// Insert noops into the instruction stream at the specified point.\n  virtual void insertNoops(MachineBasicBlock &MBB,\n                           MachineBasicBlock::iterator MI,\n                           unsigned Quantity) const;\n\n  /// Return the noop instruction to use for a noop.\n  virtual void getNoop(MCInst &NopInst) const;\n\n  /// Return true for post-incremented instructions.\n  virtual bool isPostIncrement(const MachineInstr &MI) const { return false; }\n\n  /// Returns true if the instruction is already predicated.\n  virtual bool isPredicated(const MachineInstr &MI) const { return false; }\n\n  // Returns a MIRPrinter comment for this machine operand.\n  virtual std::string\n  createMIROperandComment(const MachineInstr &MI, const MachineOperand &Op,\n                          unsigned OpIdx, const TargetRegisterInfo *TRI) const;\n\n  /// Returns true if the instruction is a\n  /// terminator instruction that has not been predicated.\n  bool isUnpredicatedTerminator(const MachineInstr &MI) const;\n\n  /// Returns true if MI is an unconditional tail call.\n  virtual bool isUnconditionalTailCall(const MachineInstr &MI) const {\n    return false;\n  }\n\n  /// Returns true if the tail call can be made conditional on BranchCond.\n  virtual bool canMakeTailCallConditional(SmallVectorImpl<MachineOperand> &Cond,\n                                          const MachineInstr &TailCall) const {\n    return false;\n  }\n\n  /// Replace the conditional branch in MBB with a conditional tail call.\n  virtual void replaceBranchWithTailCall(MachineBasicBlock &MBB,\n                                         SmallVectorImpl<MachineOperand> &Cond,\n                                         const MachineInstr &TailCall) const {\n    llvm_unreachable(\"Target didn't implement replaceBranchWithTailCall!\");\n  }\n\n  /// Convert the instruction into a predicated instruction.\n  /// It returns true if the operation was successful.\n  virtual bool PredicateInstruction(MachineInstr &MI,\n                                    ArrayRef<MachineOperand> Pred) const;\n\n  /// Returns true if the first specified predicate\n  /// subsumes the second, e.g. GE subsumes GT.\n  virtual bool SubsumesPredicate(ArrayRef<MachineOperand> Pred1,\n                                 ArrayRef<MachineOperand> Pred2) const {\n    return false;\n  }\n\n  /// If the specified instruction defines any predicate\n  /// or condition code register(s) used for predication, returns true as well\n  /// as the definition predicate(s) by reference.\n  /// SkipDead should be set to false at any point that dead\n  /// predicate instructions should be considered as being defined.\n  /// A dead predicate instruction is one that is guaranteed to be removed\n  /// after a call to PredicateInstruction.\n  virtual bool ClobbersPredicate(MachineInstr &MI,\n                                 std::vector<MachineOperand> &Pred,\n                                 bool SkipDead) const {\n    return false;\n  }\n\n  /// Return true if the specified instruction can be predicated.\n  /// By default, this returns true for every instruction with a\n  /// PredicateOperand.\n  virtual bool isPredicable(const MachineInstr &MI) const {\n    return MI.getDesc().isPredicable();\n  }\n\n  /// Return true if it's safe to move a machine\n  /// instruction that defines the specified register class.\n  virtual bool isSafeToMoveRegClassDefs(const TargetRegisterClass *RC) const {\n    return true;\n  }\n\n  /// Test if the given instruction should be considered a scheduling boundary.\n  /// This primarily includes labels and terminators.\n  virtual bool isSchedulingBoundary(const MachineInstr &MI,\n                                    const MachineBasicBlock *MBB,\n                                    const MachineFunction &MF) const;\n\n  /// Measure the specified inline asm to determine an approximation of its\n  /// length.\n  virtual unsigned getInlineAsmLength(\n    const char *Str, const MCAsmInfo &MAI,\n    const TargetSubtargetInfo *STI = nullptr) const;\n\n  /// Allocate and return a hazard recognizer to use for this target when\n  /// scheduling the machine instructions before register allocation.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetHazardRecognizer(const TargetSubtargetInfo *STI,\n                               const ScheduleDAG *DAG) const;\n\n  /// Allocate and return a hazard recognizer to use for this target when\n  /// scheduling the machine instructions before register allocation.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetMIHazardRecognizer(const InstrItineraryData *,\n                                 const ScheduleDAGMI *DAG) const;\n\n  /// Allocate and return a hazard recognizer to use for this target when\n  /// scheduling the machine instructions after register allocation.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetPostRAHazardRecognizer(const InstrItineraryData *,\n                                     const ScheduleDAG *DAG) const;\n\n  /// Allocate and return a hazard recognizer to use for by non-scheduling\n  /// passes.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetPostRAHazardRecognizer(const MachineFunction &MF) const {\n    return nullptr;\n  }\n\n  /// Provide a global flag for disabling the PreRA hazard recognizer that\n  /// targets may choose to honor.\n  bool usePreRAHazardRecognizer() const;\n\n  /// For a comparison instruction, return the source registers\n  /// in SrcReg and SrcReg2 if having two register operands, and the value it\n  /// compares against in CmpValue. Return true if the comparison instruction\n  /// can be analyzed.\n  virtual bool analyzeCompare(const MachineInstr &MI, Register &SrcReg,\n                              Register &SrcReg2, int &Mask, int &Value) const {\n    return false;\n  }\n\n  /// See if the comparison instruction can be converted\n  /// into something more efficient. E.g., on ARM most instructions can set the\n  /// flags register, obviating the need for a separate CMP.\n  virtual bool optimizeCompareInstr(MachineInstr &CmpInstr, Register SrcReg,\n                                    Register SrcReg2, int Mask, int Value,\n                                    const MachineRegisterInfo *MRI) const {\n    return false;\n  }\n  virtual bool optimizeCondBranch(MachineInstr &MI) const { return false; }\n\n  /// Try to remove the load by folding it to a register operand at the use.\n  /// We fold the load instructions if and only if the\n  /// def and use are in the same BB. We only look at one load and see\n  /// whether it can be folded into MI. FoldAsLoadDefReg is the virtual register\n  /// defined by the load we are trying to fold. DefMI returns the machine\n  /// instruction that defines FoldAsLoadDefReg, and the function returns\n  /// the machine instruction generated due to folding.\n  virtual MachineInstr *optimizeLoadInstr(MachineInstr &MI,\n                                          const MachineRegisterInfo *MRI,\n                                          Register &FoldAsLoadDefReg,\n                                          MachineInstr *&DefMI) const {\n    return nullptr;\n  }\n\n  /// 'Reg' is known to be defined by a move immediate instruction,\n  /// try to fold the immediate into the use instruction.\n  /// If MRI->hasOneNonDBGUse(Reg) is true, and this function returns true,\n  /// then the caller may assume that DefMI has been erased from its parent\n  /// block. The caller may assume that it will not be erased by this\n  /// function otherwise.\n  virtual bool FoldImmediate(MachineInstr &UseMI, MachineInstr &DefMI,\n                             Register Reg, MachineRegisterInfo *MRI) const {\n    return false;\n  }\n\n  /// Return the number of u-operations the given machine\n  /// instruction will be decoded to on the target cpu. The itinerary's\n  /// IssueWidth is the number of microops that can be dispatched each\n  /// cycle. An instruction with zero microops takes no dispatch resources.\n  virtual unsigned getNumMicroOps(const InstrItineraryData *ItinData,\n                                  const MachineInstr &MI) const;\n\n  /// Return true for pseudo instructions that don't consume any\n  /// machine resources in their current form. These are common cases that the\n  /// scheduler should consider free, rather than conservatively handling them\n  /// as instructions with no itinerary.\n  bool isZeroCost(unsigned Opcode) const {\n    return Opcode <= TargetOpcode::COPY;\n  }\n\n  virtual int getOperandLatency(const InstrItineraryData *ItinData,\n                                SDNode *DefNode, unsigned DefIdx,\n                                SDNode *UseNode, unsigned UseIdx) const;\n\n  /// Compute and return the use operand latency of a given pair of def and use.\n  /// In most cases, the static scheduling itinerary was enough to determine the\n  /// operand latency. But it may not be possible for instructions with variable\n  /// number of defs / uses.\n  ///\n  /// This is a raw interface to the itinerary that may be directly overridden\n  /// by a target. Use computeOperandLatency to get the best estimate of\n  /// latency.\n  virtual int getOperandLatency(const InstrItineraryData *ItinData,\n                                const MachineInstr &DefMI, unsigned DefIdx,\n                                const MachineInstr &UseMI,\n                                unsigned UseIdx) const;\n\n  /// Compute the instruction latency of a given instruction.\n  /// If the instruction has higher cost when predicated, it's returned via\n  /// PredCost.\n  virtual unsigned getInstrLatency(const InstrItineraryData *ItinData,\n                                   const MachineInstr &MI,\n                                   unsigned *PredCost = nullptr) const;\n\n  virtual unsigned getPredicationCost(const MachineInstr &MI) const;\n\n  virtual int getInstrLatency(const InstrItineraryData *ItinData,\n                              SDNode *Node) const;\n\n  /// Return the default expected latency for a def based on its opcode.\n  unsigned defaultDefLatency(const MCSchedModel &SchedModel,\n                             const MachineInstr &DefMI) const;\n\n  int computeDefOperandLatency(const InstrItineraryData *ItinData,\n                               const MachineInstr &DefMI) const;\n\n  /// Return true if this opcode has high latency to its result.\n  virtual bool isHighLatencyDef(int opc) const { return false; }\n\n  /// Compute operand latency between a def of 'Reg'\n  /// and a use in the current loop. Return true if the target considered\n  /// it 'high'. This is used by optimization passes such as machine LICM to\n  /// determine whether it makes sense to hoist an instruction out even in a\n  /// high register pressure situation.\n  virtual bool hasHighOperandLatency(const TargetSchedModel &SchedModel,\n                                     const MachineRegisterInfo *MRI,\n                                     const MachineInstr &DefMI, unsigned DefIdx,\n                                     const MachineInstr &UseMI,\n                                     unsigned UseIdx) const {\n    return false;\n  }\n\n  /// Compute operand latency of a def of 'Reg'. Return true\n  /// if the target considered it 'low'.\n  virtual bool hasLowDefLatency(const TargetSchedModel &SchedModel,\n                                const MachineInstr &DefMI,\n                                unsigned DefIdx) const;\n\n  /// Perform target-specific instruction verification.\n  virtual bool verifyInstruction(const MachineInstr &MI,\n                                 StringRef &ErrInfo) const {\n    return true;\n  }\n\n  /// Return the current execution domain and bit mask of\n  /// possible domains for instruction.\n  ///\n  /// Some micro-architectures have multiple execution domains, and multiple\n  /// opcodes that perform the same operation in different domains.  For\n  /// example, the x86 architecture provides the por, orps, and orpd\n  /// instructions that all do the same thing.  There is a latency penalty if a\n  /// register is written in one domain and read in another.\n  ///\n  /// This function returns a pair (domain, mask) containing the execution\n  /// domain of MI, and a bit mask of possible domains.  The setExecutionDomain\n  /// function can be used to change the opcode to one of the domains in the\n  /// bit mask.  Instructions whose execution domain can't be changed should\n  /// return a 0 mask.\n  ///\n  /// The execution domain numbers don't have any special meaning except domain\n  /// 0 is used for instructions that are not associated with any interesting\n  /// execution domain.\n  ///\n  virtual std::pair<uint16_t, uint16_t>\n  getExecutionDomain(const MachineInstr &MI) const {\n    return std::make_pair(0, 0);\n  }\n\n  /// Change the opcode of MI to execute in Domain.\n  ///\n  /// The bit (1 << Domain) must be set in the mask returned from\n  /// getExecutionDomain(MI).\n  virtual void setExecutionDomain(MachineInstr &MI, unsigned Domain) const {}\n\n  /// Returns the preferred minimum clearance\n  /// before an instruction with an unwanted partial register update.\n  ///\n  /// Some instructions only write part of a register, and implicitly need to\n  /// read the other parts of the register.  This may cause unwanted stalls\n  /// preventing otherwise unrelated instructions from executing in parallel in\n  /// an out-of-order CPU.\n  ///\n  /// For example, the x86 instruction cvtsi2ss writes its result to bits\n  /// [31:0] of the destination xmm register. Bits [127:32] are unaffected, so\n  /// the instruction needs to wait for the old value of the register to become\n  /// available:\n  ///\n  ///   addps %xmm1, %xmm0\n  ///   movaps %xmm0, (%rax)\n  ///   cvtsi2ss %rbx, %xmm0\n  ///\n  /// In the code above, the cvtsi2ss instruction needs to wait for the addps\n  /// instruction before it can issue, even though the high bits of %xmm0\n  /// probably aren't needed.\n  ///\n  /// This hook returns the preferred clearance before MI, measured in\n  /// instructions.  Other defs of MI's operand OpNum are avoided in the last N\n  /// instructions before MI.  It should only return a positive value for\n  /// unwanted dependencies.  If the old bits of the defined register have\n  /// useful values, or if MI is determined to otherwise read the dependency,\n  /// the hook should return 0.\n  ///\n  /// The unwanted dependency may be handled by:\n  ///\n  /// 1. Allocating the same register for an MI def and use.  That makes the\n  ///    unwanted dependency identical to a required dependency.\n  ///\n  /// 2. Allocating a register for the def that has no defs in the previous N\n  ///    instructions.\n  ///\n  /// 3. Calling breakPartialRegDependency() with the same arguments.  This\n  ///    allows the target to insert a dependency breaking instruction.\n  ///\n  virtual unsigned\n  getPartialRegUpdateClearance(const MachineInstr &MI, unsigned OpNum,\n                               const TargetRegisterInfo *TRI) const {\n    // The default implementation returns 0 for no partial register dependency.\n    return 0;\n  }\n\n  /// Return the minimum clearance before an instruction that reads an\n  /// unused register.\n  ///\n  /// For example, AVX instructions may copy part of a register operand into\n  /// the unused high bits of the destination register.\n  ///\n  /// vcvtsi2sdq %rax, undef %xmm0, %xmm14\n  ///\n  /// In the code above, vcvtsi2sdq copies %xmm0[127:64] into %xmm14 creating a\n  /// false dependence on any previous write to %xmm0.\n  ///\n  /// This hook works similarly to getPartialRegUpdateClearance, except that it\n  /// does not take an operand index. Instead sets \\p OpNum to the index of the\n  /// unused register.\n  virtual unsigned getUndefRegClearance(const MachineInstr &MI, unsigned OpNum,\n                                        const TargetRegisterInfo *TRI) const {\n    // The default implementation returns 0 for no undef register dependency.\n    return 0;\n  }\n\n  /// Insert a dependency-breaking instruction\n  /// before MI to eliminate an unwanted dependency on OpNum.\n  ///\n  /// If it wasn't possible to avoid a def in the last N instructions before MI\n  /// (see getPartialRegUpdateClearance), this hook will be called to break the\n  /// unwanted dependency.\n  ///\n  /// On x86, an xorps instruction can be used as a dependency breaker:\n  ///\n  ///   addps %xmm1, %xmm0\n  ///   movaps %xmm0, (%rax)\n  ///   xorps %xmm0, %xmm0\n  ///   cvtsi2ss %rbx, %xmm0\n  ///\n  /// An <imp-kill> operand should be added to MI if an instruction was\n  /// inserted.  This ties the instructions together in the post-ra scheduler.\n  ///\n  virtual void breakPartialRegDependency(MachineInstr &MI, unsigned OpNum,\n                                         const TargetRegisterInfo *TRI) const {}\n\n  /// Create machine specific model for scheduling.\n  virtual DFAPacketizer *\n  CreateTargetScheduleState(const TargetSubtargetInfo &) const {\n    return nullptr;\n  }\n\n  /// Sometimes, it is possible for the target\n  /// to tell, even without aliasing information, that two MIs access different\n  /// memory addresses. This function returns true if two MIs access different\n  /// memory addresses and false otherwise.\n  ///\n  /// Assumes any physical registers used to compute addresses have the same\n  /// value for both instructions. (This is the most useful assumption for\n  /// post-RA scheduling.)\n  ///\n  /// See also MachineInstr::mayAlias, which is implemented on top of this\n  /// function.\n  virtual bool\n  areMemAccessesTriviallyDisjoint(const MachineInstr &MIa,\n                                  const MachineInstr &MIb) const {\n    assert(MIa.mayLoadOrStore() &&\n           \"MIa must load from or modify a memory location\");\n    assert(MIb.mayLoadOrStore() &&\n           \"MIb must load from or modify a memory location\");\n    return false;\n  }\n\n  /// Return the value to use for the MachineCSE's LookAheadLimit,\n  /// which is a heuristic used for CSE'ing phys reg defs.\n  virtual unsigned getMachineCSELookAheadLimit() const {\n    // The default lookahead is small to prevent unprofitable quadratic\n    // behavior.\n    return 5;\n  }\n\n  /// Return the maximal number of alias checks on memory operands. For\n  /// instructions with more than one memory operands, the alias check on a\n  /// single MachineInstr pair has quadratic overhead and results in\n  /// unacceptable performance in the worst case. The limit here is to clamp\n  /// that maximal checks performed. Usually, that's the product of memory\n  /// operand numbers from that pair of MachineInstr to be checked. For\n  /// instance, with two MachineInstrs with 4 and 5 memory operands\n  /// correspondingly, a total of 20 checks are required. With this limit set to\n  /// 16, their alias check is skipped. We choose to limit the product instead\n  /// of the individual instruction as targets may have special MachineInstrs\n  /// with a considerably high number of memory operands, such as `ldm` in ARM.\n  /// Setting this limit per MachineInstr would result in either too high\n  /// overhead or too rigid restriction.\n  virtual unsigned getMemOperandAACheckLimit() const { return 16; }\n\n  /// Return an array that contains the ids of the target indices (used for the\n  /// TargetIndex machine operand) and their names.\n  ///\n  /// MIR Serialization is able to serialize only the target indices that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<int, const char *>>\n  getSerializableTargetIndices() const {\n    return None;\n  }\n\n  /// Decompose the machine operand's target flags into two values - the direct\n  /// target flag value and any of bit flags that are applied.\n  virtual std::pair<unsigned, unsigned>\n  decomposeMachineOperandsTargetFlags(unsigned /*TF*/) const {\n    return std::make_pair(0u, 0u);\n  }\n\n  /// Return an array that contains the direct target flag values and their\n  /// names.\n  ///\n  /// MIR Serialization is able to serialize only the target flags that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<unsigned, const char *>>\n  getSerializableDirectMachineOperandTargetFlags() const {\n    return None;\n  }\n\n  /// Return an array that contains the bitmask target flag values and their\n  /// names.\n  ///\n  /// MIR Serialization is able to serialize only the target flags that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<unsigned, const char *>>\n  getSerializableBitmaskMachineOperandTargetFlags() const {\n    return None;\n  }\n\n  /// Return an array that contains the MMO target flag values and their\n  /// names.\n  ///\n  /// MIR Serialization is able to serialize only the MMO target flags that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<MachineMemOperand::Flags, const char *>>\n  getSerializableMachineMemOperandTargetFlags() const {\n    return None;\n  }\n\n  /// Determines whether \\p Inst is a tail call instruction. Override this\n  /// method on targets that do not properly set MCID::Return and MCID::Call on\n  /// tail call instructions.\"\n  virtual bool isTailCall(const MachineInstr &Inst) const {\n    return Inst.isReturn() && Inst.isCall();\n  }\n\n  /// True if the instruction is bound to the top of its basic block and no\n  /// other instructions shall be inserted before it. This can be implemented\n  /// to prevent register allocator to insert spills before such instructions.\n  virtual bool isBasicBlockPrologue(const MachineInstr &MI) const {\n    return false;\n  }\n\n  /// During PHI eleimination lets target to make necessary checks and\n  /// insert the copy to the PHI destination register in a target specific\n  /// manner.\n  virtual MachineInstr *createPHIDestinationCopy(\n      MachineBasicBlock &MBB, MachineBasicBlock::iterator InsPt,\n      const DebugLoc &DL, Register Src, Register Dst) const {\n    return BuildMI(MBB, InsPt, DL, get(TargetOpcode::COPY), Dst)\n        .addReg(Src);\n  }\n\n  /// During PHI eleimination lets target to make necessary checks and\n  /// insert the copy to the PHI destination register in a target specific\n  /// manner.\n  virtual MachineInstr *createPHISourceCopy(MachineBasicBlock &MBB,\n                                            MachineBasicBlock::iterator InsPt,\n                                            const DebugLoc &DL, Register Src,\n                                            unsigned SrcSubReg,\n                                            Register Dst) const {\n    return BuildMI(MBB, InsPt, DL, get(TargetOpcode::COPY), Dst)\n        .addReg(Src, 0, SrcSubReg);\n  }\n\n  /// Returns a \\p outliner::OutlinedFunction struct containing target-specific\n  /// information for a set of outlining candidates.\n  virtual outliner::OutlinedFunction getOutliningCandidateInfo(\n      std::vector<outliner::Candidate> &RepeatedSequenceLocs) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::getOutliningCandidateInfo!\");\n  }\n\n  /// Returns how or if \\p MI should be outlined.\n  virtual outliner::InstrType\n  getOutliningType(MachineBasicBlock::iterator &MIT, unsigned Flags) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::getOutliningType!\");\n  }\n\n  /// Optional target hook that returns true if \\p MBB is safe to outline from,\n  /// and returns any target-specific information in \\p Flags.\n  virtual bool isMBBSafeToOutlineFrom(MachineBasicBlock &MBB,\n                                      unsigned &Flags) const {\n    return true;\n  }\n\n  /// Insert a custom frame for outlined functions.\n  virtual void buildOutlinedFrame(MachineBasicBlock &MBB, MachineFunction &MF,\n                                  const outliner::OutlinedFunction &OF) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::buildOutlinedFrame!\");\n  }\n\n  /// Insert a call to an outlined function into the program.\n  /// Returns an iterator to the spot where we inserted the call. This must be\n  /// implemented by the target.\n  virtual MachineBasicBlock::iterator\n  insertOutlinedCall(Module &M, MachineBasicBlock &MBB,\n                     MachineBasicBlock::iterator &It, MachineFunction &MF,\n                     const outliner::Candidate &C) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::insertOutlinedCall!\");\n  }\n\n  /// Return true if the function can safely be outlined from.\n  /// A function \\p MF is considered safe for outlining if an outlined function\n  /// produced from instructions in F will produce a program which produces the\n  /// same output for any set of given inputs.\n  virtual bool isFunctionSafeToOutlineFrom(MachineFunction &MF,\n                                           bool OutlineFromLinkOnceODRs) const {\n    llvm_unreachable(\"Target didn't implement \"\n                     \"TargetInstrInfo::isFunctionSafeToOutlineFrom!\");\n  }\n\n  /// Return true if the function should be outlined from by default.\n  virtual bool shouldOutlineFromFunctionByDefault(MachineFunction &MF) const {\n    return false;\n  }\n\n  /// Produce the expression describing the \\p MI loading a value into\n  /// the physical register \\p Reg. This hook should only be used with\n  /// \\p MIs belonging to VReg-less functions.\n  virtual Optional<ParamLoadedValue> describeLoadedValue(const MachineInstr &MI,\n                                                         Register Reg) const;\n\n  /// Given the generic extension instruction \\p ExtMI, returns true if this\n  /// extension is a likely candidate for being folded into an another\n  /// instruction.\n  virtual bool isExtendLikelyToBeFolded(MachineInstr &ExtMI,\n                                        MachineRegisterInfo &MRI) const {\n    return false;\n  }\n\n  /// Return MIR formatter to format/parse MIR operands.  Target can override\n  /// this virtual function and return target specific MIR formatter.\n  virtual const MIRFormatter *getMIRFormatter() const {\n    if (!Formatter.get())\n      Formatter = std::make_unique<MIRFormatter>();\n    return Formatter.get();\n  }\n\n  /// Returns the target-specific default value for tail duplication.\n  /// This value will be used if the tail-dup-placement-threshold argument is\n  /// not provided.\n  virtual unsigned getTailDuplicateSize(CodeGenOpt::Level OptLevel) const {\n    return OptLevel >= CodeGenOpt::Aggressive ? 4 : 2;\n  }\n\nprivate:\n  mutable std::unique_ptr<MIRFormatter> Formatter;\n  unsigned CallFrameSetupOpcode, CallFrameDestroyOpcode;\n  unsigned CatchRetOpcode;\n  unsigned ReturnOpcode;\n};\n\n/// Provide DenseMapInfo for TargetInstrInfo::RegSubRegPair.\ntemplate <> struct DenseMapInfo<TargetInstrInfo::RegSubRegPair> {\n  using RegInfo = DenseMapInfo<unsigned>;\n\n  static inline TargetInstrInfo::RegSubRegPair getEmptyKey() {\n    return TargetInstrInfo::RegSubRegPair(RegInfo::getEmptyKey(),\n                                          RegInfo::getEmptyKey());\n  }\n\n  static inline TargetInstrInfo::RegSubRegPair getTombstoneKey() {\n    return TargetInstrInfo::RegSubRegPair(RegInfo::getTombstoneKey(),\n                                          RegInfo::getTombstoneKey());\n  }\n\n  /// Reuse getHashValue implementation from\n  /// std::pair<unsigned, unsigned>.\n  static unsigned getHashValue(const TargetInstrInfo::RegSubRegPair &Val) {\n    std::pair<unsigned, unsigned> PairVal = std::make_pair(Val.Reg, Val.SubReg);\n    return DenseMapInfo<std::pair<unsigned, unsigned>>::getHashValue(PairVal);\n  }\n\n  static bool isEqual(const TargetInstrInfo::RegSubRegPair &LHS,\n                      const TargetInstrInfo::RegSubRegPair &RHS) {\n    return RegInfo::isEqual(LHS.Reg, RHS.Reg) &&\n           RegInfo::isEqual(LHS.SubReg, RHS.SubReg);\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_TARGETINSTRINFO_H\n"}, "58": {"id": 58, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "content": "//===- llvm/CodeGen/TargetLowering.h - Target Lowering Info -----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n///\n/// \\file\n/// This file describes how to lower LLVM code to machine code.  This has two\n/// main components:\n///\n///  1. Which ValueTypes are natively supported by the target.\n///  2. Which operations are supported for supported ValueTypes.\n///  3. Cost thresholds for alternative implementations of certain operations.\n///\n/// In addition it has a few other components, like information about FP\n/// immediates.\n///\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_TARGETLOWERING_H\n#define LLVM_CODEGEN_TARGETLOWERING_H\n\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/CodeGen/DAGCombine.h\"\n#include \"llvm/CodeGen/ISDOpcodes.h\"\n#include \"llvm/CodeGen/RuntimeLibcalls.h\"\n#include \"llvm/CodeGen/SelectionDAG.h\"\n#include \"llvm/CodeGen/SelectionDAGNodes.h\"\n#include \"llvm/CodeGen/TargetCallingConv.h\"\n#include \"llvm/CodeGen/ValueTypes.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include \"llvm/IR/InlineAsm.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include <algorithm>\n#include <cassert>\n#include <climits>\n#include <cstdint>\n#include <iterator>\n#include <map>\n#include <string>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass BranchProbability;\nclass CCState;\nclass CCValAssign;\nclass Constant;\nclass FastISel;\nclass FunctionLoweringInfo;\nclass GlobalValue;\nclass GISelKnownBits;\nclass IntrinsicInst;\nstruct KnownBits;\nclass LegacyDivergenceAnalysis;\nclass LLVMContext;\nclass MachineBasicBlock;\nclass MachineFunction;\nclass MachineInstr;\nclass MachineJumpTableInfo;\nclass MachineLoop;\nclass MachineRegisterInfo;\nclass MCContext;\nclass MCExpr;\nclass Module;\nclass ProfileSummaryInfo;\nclass TargetLibraryInfo;\nclass TargetMachine;\nclass TargetRegisterClass;\nclass TargetRegisterInfo;\nclass TargetTransformInfo;\nclass Value;\n\nnamespace Sched {\n\n  enum Preference {\n    None,             // No preference\n    Source,           // Follow source order.\n    RegPressure,      // Scheduling for lowest register pressure.\n    Hybrid,           // Scheduling for both latency and register pressure.\n    ILP,              // Scheduling for ILP in low register pressure mode.\n    VLIW              // Scheduling for VLIW targets.\n  };\n\n} // end namespace Sched\n\n// MemOp models a memory operation, either memset or memcpy/memmove.\nstruct MemOp {\nprivate:\n  // Shared\n  uint64_t Size;\n  bool DstAlignCanChange; // true if destination alignment can satisfy any\n                          // constraint.\n  Align DstAlign;         // Specified alignment of the memory operation.\n\n  bool AllowOverlap;\n  // memset only\n  bool IsMemset;   // If setthis memory operation is a memset.\n  bool ZeroMemset; // If set clears out memory with zeros.\n  // memcpy only\n  bool MemcpyStrSrc; // Indicates whether the memcpy source is an in-register\n                     // constant so it does not need to be loaded.\n  Align SrcAlign;    // Inferred alignment of the source or default value if the\n                     // memory operation does not need to load the value.\npublic:\n  static MemOp Copy(uint64_t Size, bool DstAlignCanChange, Align DstAlign,\n                    Align SrcAlign, bool IsVolatile,\n                    bool MemcpyStrSrc = false) {\n    MemOp Op;\n    Op.Size = Size;\n    Op.DstAlignCanChange = DstAlignCanChange;\n    Op.DstAlign = DstAlign;\n    Op.AllowOverlap = !IsVolatile;\n    Op.IsMemset = false;\n    Op.ZeroMemset = false;\n    Op.MemcpyStrSrc = MemcpyStrSrc;\n    Op.SrcAlign = SrcAlign;\n    return Op;\n  }\n\n  static MemOp Set(uint64_t Size, bool DstAlignCanChange, Align DstAlign,\n                   bool IsZeroMemset, bool IsVolatile) {\n    MemOp Op;\n    Op.Size = Size;\n    Op.DstAlignCanChange = DstAlignCanChange;\n    Op.DstAlign = DstAlign;\n    Op.AllowOverlap = !IsVolatile;\n    Op.IsMemset = true;\n    Op.ZeroMemset = IsZeroMemset;\n    Op.MemcpyStrSrc = false;\n    return Op;\n  }\n\n  uint64_t size() const { return Size; }\n  Align getDstAlign() const {\n    assert(!DstAlignCanChange);\n    return DstAlign;\n  }\n  bool isFixedDstAlign() const { return !DstAlignCanChange; }\n  bool allowOverlap() const { return AllowOverlap; }\n  bool isMemset() const { return IsMemset; }\n  bool isMemcpy() const { return !IsMemset; }\n  bool isMemcpyWithFixedDstAlign() const {\n    return isMemcpy() && !DstAlignCanChange;\n  }\n  bool isZeroMemset() const { return isMemset() && ZeroMemset; }\n  bool isMemcpyStrSrc() const {\n    assert(isMemcpy() && \"Must be a memcpy\");\n    return MemcpyStrSrc;\n  }\n  Align getSrcAlign() const {\n    assert(isMemcpy() && \"Must be a memcpy\");\n    return SrcAlign;\n  }\n  bool isSrcAligned(Align AlignCheck) const {\n    return isMemset() || llvm::isAligned(AlignCheck, SrcAlign.value());\n  }\n  bool isDstAligned(Align AlignCheck) const {\n    return DstAlignCanChange || llvm::isAligned(AlignCheck, DstAlign.value());\n  }\n  bool isAligned(Align AlignCheck) const {\n    return isSrcAligned(AlignCheck) && isDstAligned(AlignCheck);\n  }\n};\n\n/// This base class for TargetLowering contains the SelectionDAG-independent\n/// parts that can be used from the rest of CodeGen.\nclass TargetLoweringBase {\npublic:\n  /// This enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum LegalizeAction : uint8_t {\n    Legal,      // The target natively supports this operation.\n    Promote,    // This operation should be executed in a larger type.\n    Expand,     // Try to expand this to other ops, otherwise use a libcall.\n    LibCall,    // Don't try to expand this to other ops, always use a libcall.\n    Custom      // Use the LowerOperation hook to implement custom lowering.\n  };\n\n  /// This enum indicates whether a types are legal for a target, and if not,\n  /// what action should be used to make them valid.\n  enum LegalizeTypeAction : uint8_t {\n    TypeLegal,           // The target natively supports this type.\n    TypePromoteInteger,  // Replace this integer with a larger one.\n    TypeExpandInteger,   // Split this integer into two of half the size.\n    TypeSoftenFloat,     // Convert this float to a same size integer type.\n    TypeExpandFloat,     // Split this float into two of half the size.\n    TypeScalarizeVector, // Replace this one-element vector with its element.\n    TypeSplitVector,     // Split this vector into two of half the size.\n    TypeWidenVector,     // This vector should be widened into a larger vector.\n    TypePromoteFloat,    // Replace this float with a larger one.\n    TypeSoftPromoteHalf, // Soften half to i16 and use float to do arithmetic.\n    TypeScalarizeScalableVector, // This action is explicitly left unimplemented.\n                                 // While it is theoretically possible to\n                                 // legalize operations on scalable types with a\n                                 // loop that handles the vscale * #lanes of the\n                                 // vector, this is non-trivial at SelectionDAG\n                                 // level and these types are better to be\n                                 // widened or promoted.\n  };\n\n  /// LegalizeKind holds the legalization kind that needs to happen to EVT\n  /// in order to type-legalize it.\n  using LegalizeKind = std::pair<LegalizeTypeAction, EVT>;\n\n  /// Enum that describes how the target represents true/false values.\n  enum BooleanContent {\n    UndefinedBooleanContent,    // Only bit 0 counts, the rest can hold garbage.\n    ZeroOrOneBooleanContent,        // All bits zero except for bit 0.\n    ZeroOrNegativeOneBooleanContent // All bits equal to bit 0.\n  };\n\n  /// Enum that describes what type of support for selects the target has.\n  enum SelectSupportKind {\n    ScalarValSelect,      // The target supports scalar selects (ex: cmov).\n    ScalarCondVectorVal,  // The target supports selects with a scalar condition\n                          // and vector values (ex: cmov).\n    VectorMaskSelect      // The target supports vector selects with a vector\n                          // mask (ex: x86 blends).\n  };\n\n  /// Enum that specifies what an atomic load/AtomicRMWInst is expanded\n  /// to, if at all. Exists because different targets have different levels of\n  /// support for these atomic instructions, and also have different options\n  /// w.r.t. what they should expand to.\n  enum class AtomicExpansionKind {\n    None,    // Don't expand the instruction.\n    LLSC,    // Expand the instruction into loadlinked/storeconditional; used\n             // by ARM/AArch64.\n    LLOnly,  // Expand the (load) instruction into just a load-linked, which has\n             // greater atomic guarantees than a normal load.\n    CmpXChg, // Expand the instruction into cmpxchg; used by at least X86.\n    MaskedIntrinsic, // Use a target-specific intrinsic for the LL/SC loop.\n  };\n\n  /// Enum that specifies when a multiplication should be expanded.\n  enum class MulExpansionKind {\n    Always,            // Always expand the instruction.\n    OnlyLegalOrCustom, // Only expand when the resulting instructions are legal\n                       // or custom.\n  };\n\n  /// Enum that specifies when a float negation is beneficial.\n  enum class NegatibleCost {\n    Cheaper = 0,    // Negated expression is cheaper.\n    Neutral = 1,    // Negated expression has the same cost.\n    Expensive = 2   // Negated expression is more expensive.\n  };\n\n  class ArgListEntry {\n  public:\n    Value *Val = nullptr;\n    SDValue Node = SDValue();\n    Type *Ty = nullptr;\n    bool IsSExt : 1;\n    bool IsZExt : 1;\n    bool IsInReg : 1;\n    bool IsSRet : 1;\n    bool IsNest : 1;\n    bool IsByVal : 1;\n    bool IsByRef : 1;\n    bool IsInAlloca : 1;\n    bool IsPreallocated : 1;\n    bool IsReturned : 1;\n    bool IsSwiftSelf : 1;\n    bool IsSwiftError : 1;\n    bool IsCFGuardTarget : 1;\n    MaybeAlign Alignment = None;\n    Type *ByValType = nullptr;\n    Type *PreallocatedType = nullptr;\n\n    ArgListEntry()\n        : IsSExt(false), IsZExt(false), IsInReg(false), IsSRet(false),\n          IsNest(false), IsByVal(false), IsByRef(false), IsInAlloca(false),\n          IsPreallocated(false), IsReturned(false), IsSwiftSelf(false),\n          IsSwiftError(false), IsCFGuardTarget(false) {}\n\n    void setAttributes(const CallBase *Call, unsigned ArgIdx);\n  };\n  using ArgListTy = std::vector<ArgListEntry>;\n\n  virtual void markLibCallAttributes(MachineFunction *MF, unsigned CC,\n                                     ArgListTy &Args) const {};\n\n  static ISD::NodeType getExtendForContent(BooleanContent Content) {\n    switch (Content) {\n    case UndefinedBooleanContent:\n      // Extend by adding rubbish bits.\n      return ISD::ANY_EXTEND;\n    case ZeroOrOneBooleanContent:\n      // Extend by adding zero bits.\n      return ISD::ZERO_EXTEND;\n    case ZeroOrNegativeOneBooleanContent:\n      // Extend by copying the sign bit.\n      return ISD::SIGN_EXTEND;\n    }\n    llvm_unreachable(\"Invalid content kind\");\n  }\n\n  explicit TargetLoweringBase(const TargetMachine &TM);\n  TargetLoweringBase(const TargetLoweringBase &) = delete;\n  TargetLoweringBase &operator=(const TargetLoweringBase &) = delete;\n  virtual ~TargetLoweringBase() = default;\n\n  /// Return true if the target support strict float operation\n  bool isStrictFPEnabled() const {\n    return IsStrictFPEnabled;\n  }\n\nprotected:\n  /// Initialize all of the actions to default values.\n  void initActions();\n\npublic:\n  const TargetMachine &getTargetMachine() const { return TM; }\n\n  virtual bool useSoftFloat() const { return false; }\n\n  /// Return the pointer type for the given address space, defaults to\n  /// the pointer type from the data layout.\n  /// FIXME: The default needs to be removed once all the code is updated.\n  virtual MVT getPointerTy(const DataLayout &DL, uint32_t AS = 0) const {\n    return MVT::getIntegerVT(DL.getPointerSizeInBits(AS));\n  }\n\n  /// Return the in-memory pointer type for the given address space, defaults to\n  /// the pointer type from the data layout.  FIXME: The default needs to be\n  /// removed once all the code is updated.\n  MVT getPointerMemTy(const DataLayout &DL, uint32_t AS = 0) const {\n    return MVT::getIntegerVT(DL.getPointerSizeInBits(AS));\n  }\n\n  /// Return the type for frame index, which is determined by\n  /// the alloca address space specified through the data layout.\n  MVT getFrameIndexTy(const DataLayout &DL) const {\n    return getPointerTy(DL, DL.getAllocaAddrSpace());\n  }\n\n  /// Return the type for code pointers, which is determined by the program\n  /// address space specified through the data layout.\n  MVT getProgramPointerTy(const DataLayout &DL) const {\n    return getPointerTy(DL, DL.getProgramAddressSpace());\n  }\n\n  /// Return the type for operands of fence.\n  /// TODO: Let fence operands be of i32 type and remove this.\n  virtual MVT getFenceOperandTy(const DataLayout &DL) const {\n    return getPointerTy(DL);\n  }\n\n  /// EVT is not used in-tree, but is used by out-of-tree target.\n  /// A documentation for this function would be nice...\n  virtual MVT getScalarShiftAmountTy(const DataLayout &, EVT) const;\n\n  EVT getShiftAmountTy(EVT LHSTy, const DataLayout &DL,\n                       bool LegalTypes = true) const;\n\n  /// Return the preferred type to use for a shift opcode, given the shifted\n  /// amount type is \\p ShiftValueTy.\n  LLVM_READONLY\n  virtual LLT getPreferredShiftAmountTy(LLT ShiftValueTy) const {\n    return ShiftValueTy;\n  }\n\n  /// Returns the type to be used for the index operand of:\n  /// ISD::INSERT_VECTOR_ELT, ISD::EXTRACT_VECTOR_ELT,\n  /// ISD::INSERT_SUBVECTOR, and ISD::EXTRACT_SUBVECTOR\n  virtual MVT getVectorIdxTy(const DataLayout &DL) const {\n    return getPointerTy(DL);\n  }\n\n  /// This callback is used to inspect load/store instructions and add\n  /// target-specific MachineMemOperand flags to them.  The default\n  /// implementation does nothing.\n  virtual MachineMemOperand::Flags getTargetMMOFlags(const Instruction &I) const {\n    return MachineMemOperand::MONone;\n  }\n\n  MachineMemOperand::Flags getLoadMemOperandFlags(const LoadInst &LI,\n                                                  const DataLayout &DL) const;\n  MachineMemOperand::Flags getStoreMemOperandFlags(const StoreInst &SI,\n                                                   const DataLayout &DL) const;\n  MachineMemOperand::Flags getAtomicMemOperandFlags(const Instruction &AI,\n                                                    const DataLayout &DL) const;\n\n  virtual bool isSelectSupported(SelectSupportKind /*kind*/) const {\n    return true;\n  }\n\n  /// Return true if it is profitable to convert a select of FP constants into\n  /// a constant pool load whose address depends on the select condition. The\n  /// parameter may be used to differentiate a select with FP compare from\n  /// integer compare.\n  virtual bool reduceSelectOfFPConstantLoads(EVT CmpOpVT) const {\n    return true;\n  }\n\n  /// Return true if multiple condition registers are available.\n  bool hasMultipleConditionRegisters() const {\n    return HasMultipleConditionRegisters;\n  }\n\n  /// Return true if the target has BitExtract instructions.\n  bool hasExtractBitsInsn() const { return HasExtractBitsInsn; }\n\n  /// Return the preferred vector type legalization action.\n  virtual TargetLoweringBase::LegalizeTypeAction\n  getPreferredVectorAction(MVT VT) const {\n    // The default action for one element vectors is to scalarize\n    if (VT.getVectorElementCount().isScalar())\n      return TypeScalarizeVector;\n    // The default action for an odd-width vector is to widen.\n    if (!VT.isPow2VectorType())\n      return TypeWidenVector;\n    // The default action for other vectors is to promote\n    return TypePromoteInteger;\n  }\n\n  // Return true if the half type should be passed around as i16, but promoted\n  // to float around arithmetic. The default behavior is to pass around as\n  // float and convert around loads/stores/bitcasts and other places where\n  // the size matters.\n  virtual bool softPromoteHalfType() const { return false; }\n\n  // There are two general methods for expanding a BUILD_VECTOR node:\n  //  1. Use SCALAR_TO_VECTOR on the defined scalar values and then shuffle\n  //     them together.\n  //  2. Build the vector on the stack and then load it.\n  // If this function returns true, then method (1) will be used, subject to\n  // the constraint that all of the necessary shuffles are legal (as determined\n  // by isShuffleMaskLegal). If this function returns false, then method (2) is\n  // always used. The vector type, and the number of defined values, are\n  // provided.\n  virtual bool\n  shouldExpandBuildVectorWithShuffles(EVT /* VT */,\n                                      unsigned DefinedValues) const {\n    return DefinedValues < 3;\n  }\n\n  /// Return true if integer divide is usually cheaper than a sequence of\n  /// several shifts, adds, and multiplies for this target.\n  /// The definition of \"cheaper\" may depend on whether we're optimizing\n  /// for speed or for size.\n  virtual bool isIntDivCheap(EVT VT, AttributeList Attr) const { return false; }\n\n  /// Return true if the target can handle a standalone remainder operation.\n  virtual bool hasStandaloneRem(EVT VT) const {\n    return true;\n  }\n\n  /// Return true if SQRT(X) shouldn't be replaced with X*RSQRT(X).\n  virtual bool isFsqrtCheap(SDValue X, SelectionDAG &DAG) const {\n    // Default behavior is to replace SQRT(X) with X*RSQRT(X).\n    return false;\n  }\n\n  /// Reciprocal estimate status values used by the functions below.\n  enum ReciprocalEstimate : int {\n    Unspecified = -1,\n    Disabled = 0,\n    Enabled = 1\n  };\n\n  /// Return a ReciprocalEstimate enum value for a square root of the given type\n  /// based on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getRecipEstimateSqrtEnabled(EVT VT, MachineFunction &MF) const;\n\n  /// Return a ReciprocalEstimate enum value for a division of the given type\n  /// based on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getRecipEstimateDivEnabled(EVT VT, MachineFunction &MF) const;\n\n  /// Return the refinement step count for a square root of the given type based\n  /// on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getSqrtRefinementSteps(EVT VT, MachineFunction &MF) const;\n\n  /// Return the refinement step count for a division of the given type based\n  /// on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getDivRefinementSteps(EVT VT, MachineFunction &MF) const;\n\n  /// Returns true if target has indicated at least one type should be bypassed.\n  bool isSlowDivBypassed() const { return !BypassSlowDivWidths.empty(); }\n\n  /// Returns map of slow types for division or remainder with corresponding\n  /// fast types\n  const DenseMap<unsigned int, unsigned int> &getBypassSlowDivWidths() const {\n    return BypassSlowDivWidths;\n  }\n\n  /// Return true if Flow Control is an expensive operation that should be\n  /// avoided.\n  bool isJumpExpensive() const { return JumpIsExpensive; }\n\n  /// Return true if selects are only cheaper than branches if the branch is\n  /// unlikely to be predicted right.\n  bool isPredictableSelectExpensive() const {\n    return PredictableSelectIsExpensive;\n  }\n\n  virtual bool fallBackToDAGISel(const Instruction &Inst) const {\n    return false;\n  }\n\n  /// If a branch or a select condition is skewed in one direction by more than\n  /// this factor, it is very likely to be predicted correctly.\n  virtual BranchProbability getPredictableBranchThreshold() const;\n\n  /// Return true if the following transform is beneficial:\n  /// fold (conv (load x)) -> (load (conv*)x)\n  /// On architectures that don't natively support some vector loads\n  /// efficiently, casting the load to a smaller vector of larger types and\n  /// loading is more efficient, however, this can be undone by optimizations in\n  /// dag combiner.\n  virtual bool isLoadBitCastBeneficial(EVT LoadVT, EVT BitcastVT,\n                                       const SelectionDAG &DAG,\n                                       const MachineMemOperand &MMO) const {\n    // Don't do if we could do an indexed load on the original type, but not on\n    // the new one.\n    if (!LoadVT.isSimple() || !BitcastVT.isSimple())\n      return true;\n\n    MVT LoadMVT = LoadVT.getSimpleVT();\n\n    // Don't bother doing this if it's just going to be promoted again later, as\n    // doing so might interfere with other combines.\n    if (getOperationAction(ISD::LOAD, LoadMVT) == Promote &&\n        getTypeToPromoteTo(ISD::LOAD, LoadMVT) == BitcastVT.getSimpleVT())\n      return false;\n\n    bool Fast = false;\n    return allowsMemoryAccess(*DAG.getContext(), DAG.getDataLayout(), BitcastVT,\n                              MMO, &Fast) && Fast;\n  }\n\n  /// Return true if the following transform is beneficial:\n  /// (store (y (conv x)), y*)) -> (store x, (x*))\n  virtual bool isStoreBitCastBeneficial(EVT StoreVT, EVT BitcastVT,\n                                        const SelectionDAG &DAG,\n                                        const MachineMemOperand &MMO) const {\n    // Default to the same logic as loads.\n    return isLoadBitCastBeneficial(StoreVT, BitcastVT, DAG, MMO);\n  }\n\n  /// Return true if it is expected to be cheaper to do a store of a non-zero\n  /// vector constant with the given size and type for the address space than to\n  /// store the individual scalar element constants.\n  virtual bool storeOfVectorConstantIsCheap(EVT MemVT,\n                                            unsigned NumElem,\n                                            unsigned AddrSpace) const {\n    return false;\n  }\n\n  /// Allow store merging for the specified type after legalization in addition\n  /// to before legalization. This may transform stores that do not exist\n  /// earlier (for example, stores created from intrinsics).\n  virtual bool mergeStoresAfterLegalization(EVT MemVT) const {\n    return true;\n  }\n\n  /// Returns if it's reasonable to merge stores to MemVT size.\n  virtual bool canMergeStoresTo(unsigned AS, EVT MemVT,\n                                const SelectionDAG &DAG) const {\n    return true;\n  }\n\n  /// Return true if it is cheap to speculate a call to intrinsic cttz.\n  virtual bool isCheapToSpeculateCttz() const {\n    return false;\n  }\n\n  /// Return true if it is cheap to speculate a call to intrinsic ctlz.\n  virtual bool isCheapToSpeculateCtlz() const {\n    return false;\n  }\n\n  /// Return true if ctlz instruction is fast.\n  virtual bool isCtlzFast() const {\n    return false;\n  }\n\n  /// Return the maximum number of \"x & (x - 1)\" operations that can be done\n  /// instead of deferring to a custom CTPOP.\n  virtual unsigned getCustomCtpopCost(EVT VT, ISD::CondCode Cond) const {\n    return 1;\n  }\n\n  /// Return true if instruction generated for equality comparison is folded\n  /// with instruction generated for signed comparison.\n  virtual bool isEqualityCmpFoldedWithSignedCmp() const { return true; }\n\n  /// Return true if it is safe to transform an integer-domain bitwise operation\n  /// into the equivalent floating-point operation. This should be set to true\n  /// if the target has IEEE-754-compliant fabs/fneg operations for the input\n  /// type.\n  virtual bool hasBitPreservingFPLogic(EVT VT) const {\n    return false;\n  }\n\n  /// Return true if it is cheaper to split the store of a merged int val\n  /// from a pair of smaller values into multiple stores.\n  virtual bool isMultiStoresCheaperThanBitsMerge(EVT LTy, EVT HTy) const {\n    return false;\n  }\n\n  /// Return if the target supports combining a\n  /// chain like:\n  /// \\code\n  ///   %andResult = and %val1, #mask\n  ///   %icmpResult = icmp %andResult, 0\n  /// \\endcode\n  /// into a single machine instruction of a form like:\n  /// \\code\n  ///   cc = test %register, #mask\n  /// \\endcode\n  virtual bool isMaskAndCmp0FoldingBeneficial(const Instruction &AndI) const {\n    return false;\n  }\n\n  /// Use bitwise logic to make pairs of compares more efficient. For example:\n  /// and (seteq A, B), (seteq C, D) --> seteq (or (xor A, B), (xor C, D)), 0\n  /// This should be true when it takes more than one instruction to lower\n  /// setcc (cmp+set on x86 scalar), when bitwise ops are faster than logic on\n  /// condition bits (crand on PowerPC), and/or when reducing cmp+br is a win.\n  virtual bool convertSetCCLogicToBitwiseLogic(EVT VT) const {\n    return false;\n  }\n\n  /// Return the preferred operand type if the target has a quick way to compare\n  /// integer values of the given size. Assume that any legal integer type can\n  /// be compared efficiently. Targets may override this to allow illegal wide\n  /// types to return a vector type if there is support to compare that type.\n  virtual MVT hasFastEqualityCompare(unsigned NumBits) const {\n    MVT VT = MVT::getIntegerVT(NumBits);\n    return isTypeLegal(VT) ? VT : MVT::INVALID_SIMPLE_VALUE_TYPE;\n  }\n\n  /// Return true if the target should transform:\n  /// (X & Y) == Y ---> (~X & Y) == 0\n  /// (X & Y) != Y ---> (~X & Y) != 0\n  ///\n  /// This may be profitable if the target has a bitwise and-not operation that\n  /// sets comparison flags. A target may want to limit the transformation based\n  /// on the type of Y or if Y is a constant.\n  ///\n  /// Note that the transform will not occur if Y is known to be a power-of-2\n  /// because a mask and compare of a single bit can be handled by inverting the\n  /// predicate, for example:\n  /// (X & 8) == 8 ---> (X & 8) != 0\n  virtual bool hasAndNotCompare(SDValue Y) const {\n    return false;\n  }\n\n  /// Return true if the target has a bitwise and-not operation:\n  /// X = ~A & B\n  /// This can be used to simplify select or other instructions.\n  virtual bool hasAndNot(SDValue X) const {\n    // If the target has the more complex version of this operation, assume that\n    // it has this operation too.\n    return hasAndNotCompare(X);\n  }\n\n  /// Return true if the target has a bit-test instruction:\n  ///   (X & (1 << Y)) ==/!= 0\n  /// This knowledge can be used to prevent breaking the pattern,\n  /// or creating it if it could be recognized.\n  virtual bool hasBitTest(SDValue X, SDValue Y) const { return false; }\n\n  /// There are two ways to clear extreme bits (either low or high):\n  /// Mask:    x &  (-1 << y)  (the instcombine canonical form)\n  /// Shifts:  x >> y << y\n  /// Return true if the variant with 2 variable shifts is preferred.\n  /// Return false if there is no preference.\n  virtual bool shouldFoldMaskToVariableShiftPair(SDValue X) const {\n    // By default, let's assume that no one prefers shifts.\n    return false;\n  }\n\n  /// Return true if it is profitable to fold a pair of shifts into a mask.\n  /// This is usually true on most targets. But some targets, like Thumb1,\n  /// have immediate shift instructions, but no immediate \"and\" instruction;\n  /// this makes the fold unprofitable.\n  virtual bool shouldFoldConstantShiftPairToMask(const SDNode *N,\n                                                 CombineLevel Level) const {\n    return true;\n  }\n\n  /// Should we tranform the IR-optimal check for whether given truncation\n  /// down into KeptBits would be truncating or not:\n  ///   (add %x, (1 << (KeptBits-1))) srccond (1 << KeptBits)\n  /// Into it's more traditional form:\n  ///   ((%x << C) a>> C) dstcond %x\n  /// Return true if we should transform.\n  /// Return false if there is no preference.\n  virtual bool shouldTransformSignedTruncationCheck(EVT XVT,\n                                                    unsigned KeptBits) const {\n    // By default, let's assume that no one prefers shifts.\n    return false;\n  }\n\n  /// Given the pattern\n  ///   (X & (C l>>/<< Y)) ==/!= 0\n  /// return true if it should be transformed into:\n  ///   ((X <</l>> Y) & C) ==/!= 0\n  /// WARNING: if 'X' is a constant, the fold may deadlock!\n  /// FIXME: we could avoid passing XC, but we can't use isConstOrConstSplat()\n  ///        here because it can end up being not linked in.\n  virtual bool shouldProduceAndByConstByHoistingConstFromShiftsLHSOfAnd(\n      SDValue X, ConstantSDNode *XC, ConstantSDNode *CC, SDValue Y,\n      unsigned OldShiftOpcode, unsigned NewShiftOpcode,\n      SelectionDAG &DAG) const {\n    if (hasBitTest(X, Y)) {\n      // One interesting pattern that we'd want to form is 'bit test':\n      //   ((1 << Y) & C) ==/!= 0\n      // But we also need to be careful not to try to reverse that fold.\n\n      // Is this '1 << Y' ?\n      if (OldShiftOpcode == ISD::SHL && CC->isOne())\n        return false; // Keep the 'bit test' pattern.\n\n      // Will it be '1 << Y' after the transform ?\n      if (XC && NewShiftOpcode == ISD::SHL && XC->isOne())\n        return true; // Do form the 'bit test' pattern.\n    }\n\n    // If 'X' is a constant, and we transform, then we will immediately\n    // try to undo the fold, thus causing endless combine loop.\n    // So by default, let's assume everyone prefers the fold\n    // iff 'X' is not a constant.\n    return !XC;\n  }\n\n  /// These two forms are equivalent:\n  ///   sub %y, (xor %x, -1)\n  ///   add (add %x, 1), %y\n  /// The variant with two add's is IR-canonical.\n  /// Some targets may prefer one to the other.\n  virtual bool preferIncOfAddToSubOfNot(EVT VT) const {\n    // By default, let's assume that everyone prefers the form with two add's.\n    return true;\n  }\n\n  /// Return true if the target wants to use the optimization that\n  /// turns ext(promotableInst1(...(promotableInstN(load)))) into\n  /// promotedInst1(...(promotedInstN(ext(load)))).\n  bool enableExtLdPromotion() const { return EnableExtLdPromotion; }\n\n  /// Return true if the target can combine store(extractelement VectorTy,\n  /// Idx).\n  /// \\p Cost[out] gives the cost of that transformation when this is true.\n  virtual bool canCombineStoreAndExtract(Type *VectorTy, Value *Idx,\n                                         unsigned &Cost) const {\n    return false;\n  }\n\n  /// Return true if inserting a scalar into a variable element of an undef\n  /// vector is more efficiently handled by splatting the scalar instead.\n  virtual bool shouldSplatInsEltVarIndex(EVT) const {\n    return false;\n  }\n\n  /// Return true if target always beneficiates from combining into FMA for a\n  /// given value type. This must typically return false on targets where FMA\n  /// takes more cycles to execute than FADD.\n  virtual bool enableAggressiveFMAFusion(EVT VT) const {\n    return false;\n  }\n\n  /// Return the ValueType of the result of SETCC operations.\n  virtual EVT getSetCCResultType(const DataLayout &DL, LLVMContext &Context,\n                                 EVT VT) const;\n\n  /// Return the ValueType for comparison libcalls. Comparions libcalls include\n  /// floating point comparion calls, and Ordered/Unordered check calls on\n  /// floating point numbers.\n  virtual\n  MVT::SimpleValueType getCmpLibcallReturnType() const;\n\n  /// For targets without i1 registers, this gives the nature of the high-bits\n  /// of boolean values held in types wider than i1.\n  ///\n  /// \"Boolean values\" are special true/false values produced by nodes like\n  /// SETCC and consumed (as the condition) by nodes like SELECT and BRCOND.\n  /// Not to be confused with general values promoted from i1.  Some cpus\n  /// distinguish between vectors of boolean and scalars; the isVec parameter\n  /// selects between the two kinds.  For example on X86 a scalar boolean should\n  /// be zero extended from i1, while the elements of a vector of booleans\n  /// should be sign extended from i1.\n  ///\n  /// Some cpus also treat floating point types the same way as they treat\n  /// vectors instead of the way they treat scalars.\n  BooleanContent getBooleanContents(bool isVec, bool isFloat) const {\n    if (isVec)\n      return BooleanVectorContents;\n    return isFloat ? BooleanFloatContents : BooleanContents;\n  }\n\n  BooleanContent getBooleanContents(EVT Type) const {\n    return getBooleanContents(Type.isVector(), Type.isFloatingPoint());\n  }\n\n  /// Return target scheduling preference.\n  Sched::Preference getSchedulingPreference() const {\n    return SchedPreferenceInfo;\n  }\n\n  /// Some scheduler, e.g. hybrid, can switch to different scheduling heuristics\n  /// for different nodes. This function returns the preference (or none) for\n  /// the given node.\n  virtual Sched::Preference getSchedulingPreference(SDNode *) const {\n    return Sched::None;\n  }\n\n  /// Return the register class that should be used for the specified value\n  /// type.\n  virtual const TargetRegisterClass *getRegClassFor(MVT VT, bool isDivergent = false) const {\n    (void)isDivergent;\n    const TargetRegisterClass *RC = RegClassForVT[VT.SimpleTy];\n    assert(RC && \"This value type is not natively supported!\");\n    return RC;\n  }\n\n  /// Allows target to decide about the register class of the\n  /// specific value that is live outside the defining block.\n  /// Returns true if the value needs uniform register class.\n  virtual bool requiresUniformRegister(MachineFunction &MF,\n                                       const Value *) const {\n    return false;\n  }\n\n  /// Return the 'representative' register class for the specified value\n  /// type.\n  ///\n  /// The 'representative' register class is the largest legal super-reg\n  /// register class for the register class of the value type.  For example, on\n  /// i386 the rep register class for i8, i16, and i32 are GR32; while the rep\n  /// register class is GR64 on x86_64.\n  virtual const TargetRegisterClass *getRepRegClassFor(MVT VT) const {\n    const TargetRegisterClass *RC = RepRegClassForVT[VT.SimpleTy];\n    return RC;\n  }\n\n  /// Return the cost of the 'representative' register class for the specified\n  /// value type.\n  virtual uint8_t getRepRegClassCostFor(MVT VT) const {\n    return RepRegClassCostForVT[VT.SimpleTy];\n  }\n\n  /// Return true if SHIFT instructions should be expanded to SHIFT_PARTS\n  /// instructions, and false if a library call is preferred (e.g for code-size\n  /// reasons).\n  virtual bool shouldExpandShift(SelectionDAG &DAG, SDNode *N) const {\n    return true;\n  }\n\n  /// Return true if the target has native support for the specified value type.\n  /// This means that it has a register that directly holds it without\n  /// promotions or expansions.\n  bool isTypeLegal(EVT VT) const {\n    assert(!VT.isSimple() ||\n           (unsigned)VT.getSimpleVT().SimpleTy < array_lengthof(RegClassForVT));\n    return VT.isSimple() && RegClassForVT[VT.getSimpleVT().SimpleTy] != nullptr;\n  }\n\n  class ValueTypeActionImpl {\n    /// ValueTypeActions - For each value type, keep a LegalizeTypeAction enum\n    /// that indicates how instruction selection should deal with the type.\n    LegalizeTypeAction ValueTypeActions[MVT::LAST_VALUETYPE];\n\n  public:\n    ValueTypeActionImpl() {\n      std::fill(std::begin(ValueTypeActions), std::end(ValueTypeActions),\n                TypeLegal);\n    }\n\n    LegalizeTypeAction getTypeAction(MVT VT) const {\n      return ValueTypeActions[VT.SimpleTy];\n    }\n\n    void setTypeAction(MVT VT, LegalizeTypeAction Action) {\n      ValueTypeActions[VT.SimpleTy] = Action;\n    }\n  };\n\n  const ValueTypeActionImpl &getValueTypeActions() const {\n    return ValueTypeActions;\n  }\n\n  /// Return how we should legalize values of this type, either it is already\n  /// legal (return 'Legal') or we need to promote it to a larger type (return\n  /// 'Promote'), or we need to expand it into multiple registers of smaller\n  /// integer type (return 'Expand').  'Custom' is not an option.\n  LegalizeTypeAction getTypeAction(LLVMContext &Context, EVT VT) const {\n    return getTypeConversion(Context, VT).first;\n  }\n  LegalizeTypeAction getTypeAction(MVT VT) const {\n    return ValueTypeActions.getTypeAction(VT);\n  }\n\n  /// For types supported by the target, this is an identity function.  For\n  /// types that must be promoted to larger types, this returns the larger type\n  /// to promote to.  For integer types that are larger than the largest integer\n  /// register, this contains one step in the expansion to get to the smaller\n  /// register. For illegal floating point types, this returns the integer type\n  /// to transform to.\n  EVT getTypeToTransformTo(LLVMContext &Context, EVT VT) const {\n    return getTypeConversion(Context, VT).second;\n  }\n\n  /// For types supported by the target, this is an identity function.  For\n  /// types that must be expanded (i.e. integer types that are larger than the\n  /// largest integer register or illegal floating point types), this returns\n  /// the largest legal type it will be expanded to.\n  EVT getTypeToExpandTo(LLVMContext &Context, EVT VT) const {\n    assert(!VT.isVector());\n    while (true) {\n      switch (getTypeAction(Context, VT)) {\n      case TypeLegal:\n        return VT;\n      case TypeExpandInteger:\n        VT = getTypeToTransformTo(Context, VT);\n        break;\n      default:\n        llvm_unreachable(\"Type is not legal nor is it to be expanded!\");\n      }\n    }\n  }\n\n  /// Vector types are broken down into some number of legal first class types.\n  /// For example, EVT::v8f32 maps to 2 EVT::v4f32 with Altivec or SSE1, or 8\n  /// promoted EVT::f64 values with the X86 FP stack.  Similarly, EVT::v2i64\n  /// turns into 4 EVT::i32 values with both PPC and X86.\n  ///\n  /// This method returns the number of registers needed, and the VT for each\n  /// register.  It also returns the VT and quantity of the intermediate values\n  /// before they are promoted/expanded.\n  unsigned getVectorTypeBreakdown(LLVMContext &Context, EVT VT,\n                                  EVT &IntermediateVT,\n                                  unsigned &NumIntermediates,\n                                  MVT &RegisterVT) const;\n\n  /// Certain targets such as MIPS require that some types such as vectors are\n  /// always broken down into scalars in some contexts. This occurs even if the\n  /// vector type is legal.\n  virtual unsigned getVectorTypeBreakdownForCallingConv(\n      LLVMContext &Context, CallingConv::ID CC, EVT VT, EVT &IntermediateVT,\n      unsigned &NumIntermediates, MVT &RegisterVT) const {\n    return getVectorTypeBreakdown(Context, VT, IntermediateVT, NumIntermediates,\n                                  RegisterVT);\n  }\n\n  struct IntrinsicInfo {\n    unsigned     opc = 0;          // target opcode\n    EVT          memVT;            // memory VT\n\n    // value representing memory location\n    PointerUnion<const Value *, const PseudoSourceValue *> ptrVal;\n\n    int          offset = 0;       // offset off of ptrVal\n    uint64_t     size = 0;         // the size of the memory location\n                                   // (taken from memVT if zero)\n    MaybeAlign align = Align(1);   // alignment\n\n    MachineMemOperand::Flags flags = MachineMemOperand::MONone;\n    IntrinsicInfo() = default;\n  };\n\n  /// Given an intrinsic, checks if on the target the intrinsic will need to map\n  /// to a MemIntrinsicNode (touches memory). If this is the case, it returns\n  /// true and store the intrinsic information into the IntrinsicInfo that was\n  /// passed to the function.\n  virtual bool getTgtMemIntrinsic(IntrinsicInfo &, const CallInst &,\n                                  MachineFunction &,\n                                  unsigned /*Intrinsic*/) const {\n    return false;\n  }\n\n  /// Returns true if the target can instruction select the specified FP\n  /// immediate natively. If false, the legalizer will materialize the FP\n  /// immediate as a load from a constant pool.\n  virtual bool isFPImmLegal(const APFloat & /*Imm*/, EVT /*VT*/,\n                            bool ForCodeSize = false) const {\n    return false;\n  }\n\n  /// Targets can use this to indicate that they only support *some*\n  /// VECTOR_SHUFFLE operations, those with specific masks.  By default, if a\n  /// target supports the VECTOR_SHUFFLE node, all mask values are assumed to be\n  /// legal.\n  virtual bool isShuffleMaskLegal(ArrayRef<int> /*Mask*/, EVT /*VT*/) const {\n    return true;\n  }\n\n  /// Returns true if the operation can trap for the value type.\n  ///\n  /// VT must be a legal type. By default, we optimistically assume most\n  /// operations don't trap except for integer divide and remainder.\n  virtual bool canOpTrap(unsigned Op, EVT VT) const;\n\n  /// Similar to isShuffleMaskLegal. Targets can use this to indicate if there\n  /// is a suitable VECTOR_SHUFFLE that can be used to replace a VAND with a\n  /// constant pool entry.\n  virtual bool isVectorClearMaskLegal(ArrayRef<int> /*Mask*/,\n                                      EVT /*VT*/) const {\n    return false;\n  }\n\n  /// Return how this operation should be treated: either it is legal, needs to\n  /// be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getOperationAction(unsigned Op, EVT VT) const {\n    if (VT.isExtended()) return Expand;\n    // If a target-specific SDNode requires legalization, require the target\n    // to provide custom legalization for it.\n    if (Op >= array_lengthof(OpActions[0])) return Custom;\n    return OpActions[(unsigned)VT.getSimpleVT().SimpleTy][Op];\n  }\n\n  /// Custom method defined by each target to indicate if an operation which\n  /// may require a scale is supported natively by the target.\n  /// If not, the operation is illegal.\n  virtual bool isSupportedFixedPointOperation(unsigned Op, EVT VT,\n                                              unsigned Scale) const {\n    return false;\n  }\n\n  /// Some fixed point operations may be natively supported by the target but\n  /// only for specific scales. This method allows for checking\n  /// if the width is supported by the target for a given operation that may\n  /// depend on scale.\n  LegalizeAction getFixedPointOperationAction(unsigned Op, EVT VT,\n                                              unsigned Scale) const {\n    auto Action = getOperationAction(Op, VT);\n    if (Action != Legal)\n      return Action;\n\n    // This operation is supported in this type but may only work on specific\n    // scales.\n    bool Supported;\n    switch (Op) {\n    default:\n      llvm_unreachable(\"Unexpected fixed point operation.\");\n    case ISD::SMULFIX:\n    case ISD::SMULFIXSAT:\n    case ISD::UMULFIX:\n    case ISD::UMULFIXSAT:\n    case ISD::SDIVFIX:\n    case ISD::SDIVFIXSAT:\n    case ISD::UDIVFIX:\n    case ISD::UDIVFIXSAT:\n      Supported = isSupportedFixedPointOperation(Op, VT, Scale);\n      break;\n    }\n\n    return Supported ? Action : Expand;\n  }\n\n  // If Op is a strict floating-point operation, return the result\n  // of getOperationAction for the equivalent non-strict operation.\n  LegalizeAction getStrictFPOperationAction(unsigned Op, EVT VT) const {\n    unsigned EqOpc;\n    switch (Op) {\n      default: llvm_unreachable(\"Unexpected FP pseudo-opcode\");\n#define DAG_INSTRUCTION(NAME, NARG, ROUND_MODE, INTRINSIC, DAGN)               \\\n      case ISD::STRICT_##DAGN: EqOpc = ISD::DAGN; break;\n#define CMP_INSTRUCTION(NAME, NARG, ROUND_MODE, INTRINSIC, DAGN)               \\\n      case ISD::STRICT_##DAGN: EqOpc = ISD::SETCC; break;\n#include \"llvm/IR/ConstrainedOps.def\"\n    }\n\n    return getOperationAction(EqOpc, VT);\n  }\n\n  /// Return true if the specified operation is legal on this target or can be\n  /// made legal with custom lowering. This is used to help guide high-level\n  /// lowering decisions. LegalOnly is an optional convenience for code paths\n  /// traversed pre and post legalisation.\n  bool isOperationLegalOrCustom(unsigned Op, EVT VT,\n                                bool LegalOnly = false) const {\n    if (LegalOnly)\n      return isOperationLegal(Op, VT);\n\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n      (getOperationAction(Op, VT) == Legal ||\n       getOperationAction(Op, VT) == Custom);\n  }\n\n  /// Return true if the specified operation is legal on this target or can be\n  /// made legal using promotion. This is used to help guide high-level lowering\n  /// decisions. LegalOnly is an optional convenience for code paths traversed\n  /// pre and post legalisation.\n  bool isOperationLegalOrPromote(unsigned Op, EVT VT,\n                                 bool LegalOnly = false) const {\n    if (LegalOnly)\n      return isOperationLegal(Op, VT);\n\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n      (getOperationAction(Op, VT) == Legal ||\n       getOperationAction(Op, VT) == Promote);\n  }\n\n  /// Return true if the specified operation is legal on this target or can be\n  /// made legal with custom lowering or using promotion. This is used to help\n  /// guide high-level lowering decisions. LegalOnly is an optional convenience\n  /// for code paths traversed pre and post legalisation.\n  bool isOperationLegalOrCustomOrPromote(unsigned Op, EVT VT,\n                                         bool LegalOnly = false) const {\n    if (LegalOnly)\n      return isOperationLegal(Op, VT);\n\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n      (getOperationAction(Op, VT) == Legal ||\n       getOperationAction(Op, VT) == Custom ||\n       getOperationAction(Op, VT) == Promote);\n  }\n\n  /// Return true if the operation uses custom lowering, regardless of whether\n  /// the type is legal or not.\n  bool isOperationCustom(unsigned Op, EVT VT) const {\n    return getOperationAction(Op, VT) == Custom;\n  }\n\n  /// Return true if lowering to a jump table is allowed.\n  virtual bool areJTsAllowed(const Function *Fn) const {\n    if (Fn->getFnAttribute(\"no-jump-tables\").getValueAsString() == \"true\")\n      return false;\n\n    return isOperationLegalOrCustom(ISD::BR_JT, MVT::Other) ||\n           isOperationLegalOrCustom(ISD::BRIND, MVT::Other);\n  }\n\n  /// Check whether the range [Low,High] fits in a machine word.\n  bool rangeFitsInWord(const APInt &Low, const APInt &High,\n                       const DataLayout &DL) const {\n    // FIXME: Using the pointer type doesn't seem ideal.\n    uint64_t BW = DL.getIndexSizeInBits(0u);\n    uint64_t Range = (High - Low).getLimitedValue(UINT64_MAX - 1) + 1;\n    return Range <= BW;\n  }\n\n  /// Return true if lowering to a jump table is suitable for a set of case\n  /// clusters which may contain \\p NumCases cases, \\p Range range of values.\n  virtual bool isSuitableForJumpTable(const SwitchInst *SI, uint64_t NumCases,\n                                      uint64_t Range, ProfileSummaryInfo *PSI,\n                                      BlockFrequencyInfo *BFI) const;\n\n  /// Return true if lowering to a bit test is suitable for a set of case\n  /// clusters which contains \\p NumDests unique destinations, \\p Low and\n  /// \\p High as its lowest and highest case values, and expects \\p NumCmps\n  /// case value comparisons. Check if the number of destinations, comparison\n  /// metric, and range are all suitable.\n  bool isSuitableForBitTests(unsigned NumDests, unsigned NumCmps,\n                             const APInt &Low, const APInt &High,\n                             const DataLayout &DL) const {\n    // FIXME: I don't think NumCmps is the correct metric: a single case and a\n    // range of cases both require only one branch to lower. Just looking at the\n    // number of clusters and destinations should be enough to decide whether to\n    // build bit tests.\n\n    // To lower a range with bit tests, the range must fit the bitwidth of a\n    // machine word.\n    if (!rangeFitsInWord(Low, High, DL))\n      return false;\n\n    // Decide whether it's profitable to lower this range with bit tests. Each\n    // destination requires a bit test and branch, and there is an overall range\n    // check branch. For a small number of clusters, separate comparisons might\n    // be cheaper, and for many destinations, splitting the range might be\n    // better.\n    return (NumDests == 1 && NumCmps >= 3) || (NumDests == 2 && NumCmps >= 5) ||\n           (NumDests == 3 && NumCmps >= 6);\n  }\n\n  /// Return true if the specified operation is illegal on this target or\n  /// unlikely to be made legal with custom lowering. This is used to help guide\n  /// high-level lowering decisions.\n  bool isOperationExpand(unsigned Op, EVT VT) const {\n    return (!isTypeLegal(VT) || getOperationAction(Op, VT) == Expand);\n  }\n\n  /// Return true if the specified operation is legal on this target.\n  bool isOperationLegal(unsigned Op, EVT VT) const {\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n           getOperationAction(Op, VT) == Legal;\n  }\n\n  /// Return how this load with extension should be treated: either it is legal,\n  /// needs to be promoted to a larger size, needs to be expanded to some other\n  /// code sequence, or the target has a custom expander for it.\n  LegalizeAction getLoadExtAction(unsigned ExtType, EVT ValVT,\n                                  EVT MemVT) const {\n    if (ValVT.isExtended() || MemVT.isExtended()) return Expand;\n    unsigned ValI = (unsigned) ValVT.getSimpleVT().SimpleTy;\n    unsigned MemI = (unsigned) MemVT.getSimpleVT().SimpleTy;\n    assert(ExtType < ISD::LAST_LOADEXT_TYPE && ValI < MVT::LAST_VALUETYPE &&\n           MemI < MVT::LAST_VALUETYPE && \"Table isn't big enough!\");\n    unsigned Shift = 4 * ExtType;\n    return (LegalizeAction)((LoadExtActions[ValI][MemI] >> Shift) & 0xf);\n  }\n\n  /// Return true if the specified load with extension is legal on this target.\n  bool isLoadExtLegal(unsigned ExtType, EVT ValVT, EVT MemVT) const {\n    return getLoadExtAction(ExtType, ValVT, MemVT) == Legal;\n  }\n\n  /// Return true if the specified load with extension is legal or custom\n  /// on this target.\n  bool isLoadExtLegalOrCustom(unsigned ExtType, EVT ValVT, EVT MemVT) const {\n    return getLoadExtAction(ExtType, ValVT, MemVT) == Legal ||\n           getLoadExtAction(ExtType, ValVT, MemVT) == Custom;\n  }\n\n  /// Return how this store with truncation should be treated: either it is\n  /// legal, needs to be promoted to a larger size, needs to be expanded to some\n  /// other code sequence, or the target has a custom expander for it.\n  LegalizeAction getTruncStoreAction(EVT ValVT, EVT MemVT) const {\n    if (ValVT.isExtended() || MemVT.isExtended()) return Expand;\n    unsigned ValI = (unsigned) ValVT.getSimpleVT().SimpleTy;\n    unsigned MemI = (unsigned) MemVT.getSimpleVT().SimpleTy;\n    assert(ValI < MVT::LAST_VALUETYPE && MemI < MVT::LAST_VALUETYPE &&\n           \"Table isn't big enough!\");\n    return TruncStoreActions[ValI][MemI];\n  }\n\n  /// Return true if the specified store with truncation is legal on this\n  /// target.\n  bool isTruncStoreLegal(EVT ValVT, EVT MemVT) const {\n    return isTypeLegal(ValVT) && getTruncStoreAction(ValVT, MemVT) == Legal;\n  }\n\n  /// Return true if the specified store with truncation has solution on this\n  /// target.\n  bool isTruncStoreLegalOrCustom(EVT ValVT, EVT MemVT) const {\n    return isTypeLegal(ValVT) &&\n      (getTruncStoreAction(ValVT, MemVT) == Legal ||\n       getTruncStoreAction(ValVT, MemVT) == Custom);\n  }\n\n  /// Return how the indexed load should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedLoadAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_Load);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedLoadLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n      (getIndexedLoadAction(IdxMode, VT.getSimpleVT()) == Legal ||\n       getIndexedLoadAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Return how the indexed store should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedStoreAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_Store);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedStoreLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n      (getIndexedStoreAction(IdxMode, VT.getSimpleVT()) == Legal ||\n       getIndexedStoreAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Return how the indexed load should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedMaskedLoadAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_MaskedLoad);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedMaskedLoadLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n           (getIndexedMaskedLoadAction(IdxMode, VT.getSimpleVT()) == Legal ||\n            getIndexedMaskedLoadAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Return how the indexed store should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedMaskedStoreAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_MaskedStore);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedMaskedStoreLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n           (getIndexedMaskedStoreAction(IdxMode, VT.getSimpleVT()) == Legal ||\n            getIndexedMaskedStoreAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Returns true if the index type for a masked gather/scatter requires\n  /// extending\n  virtual bool shouldExtendGSIndex(EVT VT, EVT &EltTy) const { return false; }\n\n  // Returns true if VT is a legal index type for masked gathers/scatters\n  // on this target\n  virtual bool shouldRemoveExtendFromGSIndex(EVT VT) const { return false; }\n\n  /// Return how the condition code should be treated: either it is legal, needs\n  /// to be expanded to some other code sequence, or the target has a custom\n  /// expander for it.\n  LegalizeAction\n  getCondCodeAction(ISD::CondCode CC, MVT VT) const {\n    assert((unsigned)CC < array_lengthof(CondCodeActions) &&\n           ((unsigned)VT.SimpleTy >> 3) < array_lengthof(CondCodeActions[0]) &&\n           \"Table isn't big enough!\");\n    // See setCondCodeAction for how this is encoded.\n    uint32_t Shift = 4 * (VT.SimpleTy & 0x7);\n    uint32_t Value = CondCodeActions[CC][VT.SimpleTy >> 3];\n    LegalizeAction Action = (LegalizeAction) ((Value >> Shift) & 0xF);\n    assert(Action != Promote && \"Can't promote condition code!\");\n    return Action;\n  }\n\n  /// Return true if the specified condition code is legal on this target.\n  bool isCondCodeLegal(ISD::CondCode CC, MVT VT) const {\n    return getCondCodeAction(CC, VT) == Legal;\n  }\n\n  /// Return true if the specified condition code is legal or custom on this\n  /// target.\n  bool isCondCodeLegalOrCustom(ISD::CondCode CC, MVT VT) const {\n    return getCondCodeAction(CC, VT) == Legal ||\n           getCondCodeAction(CC, VT) == Custom;\n  }\n\n  /// If the action for this operation is to promote, this method returns the\n  /// ValueType to promote to.\n  MVT getTypeToPromoteTo(unsigned Op, MVT VT) const {\n    assert(getOperationAction(Op, VT) == Promote &&\n           \"This operation isn't promoted!\");\n\n    // See if this has an explicit type specified.\n    std::map<std::pair<unsigned, MVT::SimpleValueType>,\n             MVT::SimpleValueType>::const_iterator PTTI =\n      PromoteToType.find(std::make_pair(Op, VT.SimpleTy));\n    if (PTTI != PromoteToType.end()) return PTTI->second;\n\n    assert((VT.isInteger() || VT.isFloatingPoint()) &&\n           \"Cannot autopromote this type, add it with AddPromotedToType.\");\n\n    MVT NVT = VT;\n    do {\n      NVT = (MVT::SimpleValueType)(NVT.SimpleTy+1);\n      assert(NVT.isInteger() == VT.isInteger() && NVT != MVT::isVoid &&\n             \"Didn't find type to promote to!\");\n    } while (!isTypeLegal(NVT) ||\n              getOperationAction(Op, NVT) == Promote);\n    return NVT;\n  }\n\n  /// Return the EVT corresponding to this LLVM type.  This is fixed by the LLVM\n  /// operations except for the pointer size.  If AllowUnknown is true, this\n  /// will return MVT::Other for types with no EVT counterpart (e.g. structs),\n  /// otherwise it will assert.\n  EVT getValueType(const DataLayout &DL, Type *Ty,\n                   bool AllowUnknown = false) const {\n    // Lower scalar pointers to native pointer types.\n    if (auto *PTy = dyn_cast<PointerType>(Ty))\n      return getPointerTy(DL, PTy->getAddressSpace());\n\n    if (auto *VTy = dyn_cast<VectorType>(Ty)) {\n      Type *EltTy = VTy->getElementType();\n      // Lower vectors of pointers to native pointer types.\n      if (auto *PTy = dyn_cast<PointerType>(EltTy)) {\n        EVT PointerTy(getPointerTy(DL, PTy->getAddressSpace()));\n        EltTy = PointerTy.getTypeForEVT(Ty->getContext());\n      }\n      return EVT::getVectorVT(Ty->getContext(), EVT::getEVT(EltTy, false),\n                              VTy->getElementCount());\n    }\n\n    return EVT::getEVT(Ty, AllowUnknown);\n  }\n\n  EVT getMemValueType(const DataLayout &DL, Type *Ty,\n                      bool AllowUnknown = false) const {\n    // Lower scalar pointers to native pointer types.\n    if (PointerType *PTy = dyn_cast<PointerType>(Ty))\n      return getPointerMemTy(DL, PTy->getAddressSpace());\n    else if (VectorType *VTy = dyn_cast<VectorType>(Ty)) {\n      Type *Elm = VTy->getElementType();\n      if (PointerType *PT = dyn_cast<PointerType>(Elm)) {\n        EVT PointerTy(getPointerMemTy(DL, PT->getAddressSpace()));\n        Elm = PointerTy.getTypeForEVT(Ty->getContext());\n      }\n      return EVT::getVectorVT(Ty->getContext(), EVT::getEVT(Elm, false),\n                              VTy->getElementCount());\n    }\n\n    return getValueType(DL, Ty, AllowUnknown);\n  }\n\n\n  /// Return the MVT corresponding to this LLVM type. See getValueType.\n  MVT getSimpleValueType(const DataLayout &DL, Type *Ty,\n                         bool AllowUnknown = false) const {\n    return getValueType(DL, Ty, AllowUnknown).getSimpleVT();\n  }\n\n  /// Return the desired alignment for ByVal or InAlloca aggregate function\n  /// arguments in the caller parameter area.  This is the actual alignment, not\n  /// its logarithm.\n  virtual unsigned getByValTypeAlignment(Type *Ty, const DataLayout &DL) const;\n\n  /// Return the type of registers that this ValueType will eventually require.\n  MVT getRegisterType(MVT VT) const {\n    assert((unsigned)VT.SimpleTy < array_lengthof(RegisterTypeForVT));\n    return RegisterTypeForVT[VT.SimpleTy];\n  }\n\n  /// Return the type of registers that this ValueType will eventually require.\n  MVT getRegisterType(LLVMContext &Context, EVT VT) const {\n    if (VT.isSimple()) {\n      assert((unsigned)VT.getSimpleVT().SimpleTy <\n                array_lengthof(RegisterTypeForVT));\n      return RegisterTypeForVT[VT.getSimpleVT().SimpleTy];\n    }\n    if (VT.isVector()) {\n      EVT VT1;\n      MVT RegisterVT;\n      unsigned NumIntermediates;\n      (void)getVectorTypeBreakdown(Context, VT, VT1,\n                                   NumIntermediates, RegisterVT);\n      return RegisterVT;\n    }\n    if (VT.isInteger()) {\n      return getRegisterType(Context, getTypeToTransformTo(Context, VT));\n    }\n    llvm_unreachable(\"Unsupported extended type!\");\n  }\n\n  /// Return the number of registers that this ValueType will eventually\n  /// require.\n  ///\n  /// This is one for any types promoted to live in larger registers, but may be\n  /// more than one for types (like i64) that are split into pieces.  For types\n  /// like i140, which are first promoted then expanded, it is the number of\n  /// registers needed to hold all the bits of the original type.  For an i140\n  /// on a 32 bit machine this means 5 registers.\n  unsigned getNumRegisters(LLVMContext &Context, EVT VT) const {\n    if (VT.isSimple()) {\n      assert((unsigned)VT.getSimpleVT().SimpleTy <\n                array_lengthof(NumRegistersForVT));\n      return NumRegistersForVT[VT.getSimpleVT().SimpleTy];\n    }\n    if (VT.isVector()) {\n      EVT VT1;\n      MVT VT2;\n      unsigned NumIntermediates;\n      return getVectorTypeBreakdown(Context, VT, VT1, NumIntermediates, VT2);\n    }\n    if (VT.isInteger()) {\n      unsigned BitWidth = VT.getSizeInBits();\n      unsigned RegWidth = getRegisterType(Context, VT).getSizeInBits();\n      return (BitWidth + RegWidth - 1) / RegWidth;\n    }\n    llvm_unreachable(\"Unsupported extended type!\");\n  }\n\n  /// Certain combinations of ABIs, Targets and features require that types\n  /// are legal for some operations and not for other operations.\n  /// For MIPS all vector types must be passed through the integer register set.\n  virtual MVT getRegisterTypeForCallingConv(LLVMContext &Context,\n                                            CallingConv::ID CC, EVT VT) const {\n    return getRegisterType(Context, VT);\n  }\n\n  /// Certain targets require unusual breakdowns of certain types. For MIPS,\n  /// this occurs when a vector type is used, as vector are passed through the\n  /// integer register set.\n  virtual unsigned getNumRegistersForCallingConv(LLVMContext &Context,\n                                                 CallingConv::ID CC,\n                                                 EVT VT) const {\n    return getNumRegisters(Context, VT);\n  }\n\n  /// Certain targets have context senstive alignment requirements, where one\n  /// type has the alignment requirement of another type.\n  virtual Align getABIAlignmentForCallingConv(Type *ArgTy,\n                                              DataLayout DL) const {\n    return DL.getABITypeAlign(ArgTy);\n  }\n\n  /// If true, then instruction selection should seek to shrink the FP constant\n  /// of the specified type to a smaller type in order to save space and / or\n  /// reduce runtime.\n  virtual bool ShouldShrinkFPConstant(EVT) const { return true; }\n\n  /// Return true if it is profitable to reduce a load to a smaller type.\n  /// Example: (i16 (trunc (i32 (load x))) -> i16 load x\n  virtual bool shouldReduceLoadWidth(SDNode *Load, ISD::LoadExtType ExtTy,\n                                     EVT NewVT) const {\n    // By default, assume that it is cheaper to extract a subvector from a wide\n    // vector load rather than creating multiple narrow vector loads.\n    if (NewVT.isVector() && !Load->hasOneUse())\n      return false;\n\n    return true;\n  }\n\n  /// When splitting a value of the specified type into parts, does the Lo\n  /// or Hi part come first?  This usually follows the endianness, except\n  /// for ppcf128, where the Hi part always comes first.\n  bool hasBigEndianPartOrdering(EVT VT, const DataLayout &DL) const {\n    return DL.isBigEndian() || VT == MVT::ppcf128;\n  }\n\n  /// If true, the target has custom DAG combine transformations that it can\n  /// perform for the specified node.\n  bool hasTargetDAGCombine(ISD::NodeType NT) const {\n    assert(unsigned(NT >> 3) < array_lengthof(TargetDAGCombineArray));\n    return TargetDAGCombineArray[NT >> 3] & (1 << (NT&7));\n  }\n\n  unsigned getGatherAllAliasesMaxDepth() const {\n    return GatherAllAliasesMaxDepth;\n  }\n\n  /// Returns the size of the platform's va_list object.\n  virtual unsigned getVaListSizeInBits(const DataLayout &DL) const {\n    return getPointerTy(DL).getSizeInBits();\n  }\n\n  /// Get maximum # of store operations permitted for llvm.memset\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to replace a call to llvm.memset. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxStoresPerMemset(bool OptSize) const {\n    return OptSize ? MaxStoresPerMemsetOptSize : MaxStoresPerMemset;\n  }\n\n  /// Get maximum # of store operations permitted for llvm.memcpy\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to replace a call to llvm.memcpy. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxStoresPerMemcpy(bool OptSize) const {\n    return OptSize ? MaxStoresPerMemcpyOptSize : MaxStoresPerMemcpy;\n  }\n\n  /// \\brief Get maximum # of store operations to be glued together\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to glue together during lowering of llvm.memcpy. The value is set by\n  //  the target at the performance threshold for such a replacement.\n  virtual unsigned getMaxGluedStoresPerMemcpy() const {\n    return MaxGluedStoresPerMemcpy;\n  }\n\n  /// Get maximum # of load operations permitted for memcmp\n  ///\n  /// This function returns the maximum number of load operations permitted\n  /// to replace a call to memcmp. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxExpandSizeMemcmp(bool OptSize) const {\n    return OptSize ? MaxLoadsPerMemcmpOptSize : MaxLoadsPerMemcmp;\n  }\n\n  /// Get maximum # of store operations permitted for llvm.memmove\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to replace a call to llvm.memmove. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxStoresPerMemmove(bool OptSize) const {\n    return OptSize ? MaxStoresPerMemmoveOptSize : MaxStoresPerMemmove;\n  }\n\n  /// Determine if the target supports unaligned memory accesses.\n  ///\n  /// This function returns true if the target allows unaligned memory accesses\n  /// of the specified type in the given address space. If true, it also returns\n  /// whether the unaligned memory access is \"fast\" in the last argument by\n  /// reference. This is used, for example, in situations where an array\n  /// copy/move/set is converted to a sequence of store operations. Its use\n  /// helps to ensure that such replacements don't generate code that causes an\n  /// alignment error (trap) on the target machine.\n  virtual bool allowsMisalignedMemoryAccesses(\n      EVT, unsigned AddrSpace = 0, Align Alignment = Align(1),\n      MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n      bool * /*Fast*/ = nullptr) const {\n    return false;\n  }\n\n  /// LLT handling variant.\n  virtual bool allowsMisalignedMemoryAccesses(\n      LLT, unsigned AddrSpace = 0, Align Alignment = Align(1),\n      MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n      bool * /*Fast*/ = nullptr) const {\n    return false;\n  }\n\n  /// This function returns true if the memory access is aligned or if the\n  /// target allows this specific unaligned memory access. If the access is\n  /// allowed, the optional final parameter returns if the access is also fast\n  /// (as defined by the target).\n  bool allowsMemoryAccessForAlignment(\n      LLVMContext &Context, const DataLayout &DL, EVT VT,\n      unsigned AddrSpace = 0, Align Alignment = Align(1),\n      MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n      bool *Fast = nullptr) const;\n\n  /// Return true if the memory access of this type is aligned or if the target\n  /// allows this specific unaligned access for the given MachineMemOperand.\n  /// If the access is allowed, the optional final parameter returns if the\n  /// access is also fast (as defined by the target).\n  bool allowsMemoryAccessForAlignment(LLVMContext &Context,\n                                      const DataLayout &DL, EVT VT,\n                                      const MachineMemOperand &MMO,\n                                      bool *Fast = nullptr) const;\n\n  /// Return true if the target supports a memory access of this type for the\n  /// given address space and alignment. If the access is allowed, the optional\n  /// final parameter returns if the access is also fast (as defined by the\n  /// target).\n  virtual bool\n  allowsMemoryAccess(LLVMContext &Context, const DataLayout &DL, EVT VT,\n                     unsigned AddrSpace = 0, Align Alignment = Align(1),\n                     MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n                     bool *Fast = nullptr) const;\n\n  /// Return true if the target supports a memory access of this type for the\n  /// given MachineMemOperand. If the access is allowed, the optional\n  /// final parameter returns if the access is also fast (as defined by the\n  /// target).\n  bool allowsMemoryAccess(LLVMContext &Context, const DataLayout &DL, EVT VT,\n                          const MachineMemOperand &MMO,\n                          bool *Fast = nullptr) const;\n\n  /// LLT handling variant.\n  bool allowsMemoryAccess(LLVMContext &Context, const DataLayout &DL, LLT Ty,\n                          const MachineMemOperand &MMO,\n                          bool *Fast = nullptr) const;\n\n  /// Returns the target specific optimal type for load and store operations as\n  /// a result of memset, memcpy, and memmove lowering.\n  /// It returns EVT::Other if the type should be determined using generic\n  /// target-independent logic.\n  virtual EVT\n  getOptimalMemOpType(const MemOp &Op,\n                      const AttributeList & /*FuncAttributes*/) const {\n    return MVT::Other;\n  }\n\n  /// LLT returning variant.\n  virtual LLT\n  getOptimalMemOpLLT(const MemOp &Op,\n                     const AttributeList & /*FuncAttributes*/) const {\n    return LLT();\n  }\n\n  /// Returns true if it's safe to use load / store of the specified type to\n  /// expand memcpy / memset inline.\n  ///\n  /// This is mostly true for all types except for some special cases. For\n  /// example, on X86 targets without SSE2 f64 load / store are done with fldl /\n  /// fstpl which also does type conversion. Note the specified type doesn't\n  /// have to be legal as the hook is used before type legalization.\n  virtual bool isSafeMemOpType(MVT /*VT*/) const { return true; }\n\n  /// Return lower limit for number of blocks in a jump table.\n  virtual unsigned getMinimumJumpTableEntries() const;\n\n  /// Return lower limit of the density in a jump table.\n  unsigned getMinimumJumpTableDensity(bool OptForSize) const;\n\n  /// Return upper limit for number of entries in a jump table.\n  /// Zero if no limit.\n  unsigned getMaximumJumpTableSize() const;\n\n  virtual bool isJumpTableRelative() const;\n\n  /// If a physical register, this specifies the register that\n  /// llvm.savestack/llvm.restorestack should save and restore.\n  Register getStackPointerRegisterToSaveRestore() const {\n    return StackPointerRegisterToSaveRestore;\n  }\n\n  /// If a physical register, this returns the register that receives the\n  /// exception address on entry to an EH pad.\n  virtual Register\n  getExceptionPointerRegister(const Constant *PersonalityFn) const {\n    return Register();\n  }\n\n  /// If a physical register, this returns the register that receives the\n  /// exception typeid on entry to a landing pad.\n  virtual Register\n  getExceptionSelectorRegister(const Constant *PersonalityFn) const {\n    return Register();\n  }\n\n  virtual bool needsFixedCatchObjects() const {\n    report_fatal_error(\"Funclet EH is not implemented for this target\");\n  }\n\n  /// Return the minimum stack alignment of an argument.\n  Align getMinStackArgumentAlignment() const {\n    return MinStackArgumentAlignment;\n  }\n\n  /// Return the minimum function alignment.\n  Align getMinFunctionAlignment() const { return MinFunctionAlignment; }\n\n  /// Return the preferred function alignment.\n  Align getPrefFunctionAlignment() const { return PrefFunctionAlignment; }\n\n  /// Return the preferred loop alignment.\n  virtual Align getPrefLoopAlignment(MachineLoop *ML = nullptr) const {\n    return PrefLoopAlignment;\n  }\n\n  /// Should loops be aligned even when the function is marked OptSize (but not\n  /// MinSize).\n  virtual bool alignLoopsWithOptSize() const {\n    return false;\n  }\n\n  /// If the target has a standard location for the stack protector guard,\n  /// returns the address of that location. Otherwise, returns nullptr.\n  /// DEPRECATED: please override useLoadStackGuardNode and customize\n  ///             LOAD_STACK_GUARD, or customize \\@llvm.stackguard().\n  virtual Value *getIRStackGuard(IRBuilder<> &IRB) const;\n\n  /// Inserts necessary declarations for SSP (stack protection) purpose.\n  /// Should be used only when getIRStackGuard returns nullptr.\n  virtual void insertSSPDeclarations(Module &M) const;\n\n  /// Return the variable that's previously inserted by insertSSPDeclarations,\n  /// if any, otherwise return nullptr. Should be used only when\n  /// getIRStackGuard returns nullptr.\n  virtual Value *getSDagStackGuard(const Module &M) const;\n\n  /// If this function returns true, stack protection checks should XOR the\n  /// frame pointer (or whichever pointer is used to address locals) into the\n  /// stack guard value before checking it. getIRStackGuard must return nullptr\n  /// if this returns true.\n  virtual bool useStackGuardXorFP() const { return false; }\n\n  /// If the target has a standard stack protection check function that\n  /// performs validation and error handling, returns the function. Otherwise,\n  /// returns nullptr. Must be previously inserted by insertSSPDeclarations.\n  /// Should be used only when getIRStackGuard returns nullptr.\n  virtual Function *getSSPStackGuardCheck(const Module &M) const;\n\nprotected:\n  Value *getDefaultSafeStackPointerLocation(IRBuilder<> &IRB,\n                                            bool UseTLS) const;\n\npublic:\n  /// Returns the target-specific address of the unsafe stack pointer.\n  virtual Value *getSafeStackPointerLocation(IRBuilder<> &IRB) const;\n\n  /// Returns the name of the symbol used to emit stack probes or the empty\n  /// string if not applicable.\n  virtual bool hasStackProbeSymbol(MachineFunction &MF) const { return false; }\n\n  virtual bool hasInlineStackProbe(MachineFunction &MF) const { return false; }\n\n  virtual StringRef getStackProbeSymbolName(MachineFunction &MF) const {\n    return \"\";\n  }\n\n  /// Returns true if a cast from SrcAS to DestAS is \"cheap\", such that e.g. we\n  /// are happy to sink it into basic blocks. A cast may be free, but not\n  /// necessarily a no-op. e.g. a free truncate from a 64-bit to 32-bit pointer.\n  virtual bool isFreeAddrSpaceCast(unsigned SrcAS, unsigned DestAS) const;\n\n  /// Return true if the pointer arguments to CI should be aligned by aligning\n  /// the object whose address is being passed. If so then MinSize is set to the\n  /// minimum size the object must be to be aligned and PrefAlign is set to the\n  /// preferred alignment.\n  virtual bool shouldAlignPointerArgs(CallInst * /*CI*/, unsigned & /*MinSize*/,\n                                      unsigned & /*PrefAlign*/) const {\n    return false;\n  }\n\n  //===--------------------------------------------------------------------===//\n  /// \\name Helpers for TargetTransformInfo implementations\n  /// @{\n\n  /// Get the ISD node that corresponds to the Instruction class opcode.\n  int InstructionOpcodeToISD(unsigned Opcode) const;\n\n  /// Estimate the cost of type-legalization and the legalized type.\n  std::pair<int, MVT> getTypeLegalizationCost(const DataLayout &DL,\n                                              Type *Ty) const;\n\n  /// @}\n\n  //===--------------------------------------------------------------------===//\n  /// \\name Helpers for atomic expansion.\n  /// @{\n\n  /// Returns the maximum atomic operation size (in bits) supported by\n  /// the backend. Atomic operations greater than this size (as well\n  /// as ones that are not naturally aligned), will be expanded by\n  /// AtomicExpandPass into an __atomic_* library call.\n  unsigned getMaxAtomicSizeInBitsSupported() const {\n    return MaxAtomicSizeInBitsSupported;\n  }\n\n  /// Returns the size of the smallest cmpxchg or ll/sc instruction\n  /// the backend supports.  Any smaller operations are widened in\n  /// AtomicExpandPass.\n  ///\n  /// Note that *unlike* operations above the maximum size, atomic ops\n  /// are still natively supported below the minimum; they just\n  /// require a more complex expansion.\n  unsigned getMinCmpXchgSizeInBits() const { return MinCmpXchgSizeInBits; }\n\n  /// Whether the target supports unaligned atomic operations.\n  bool supportsUnalignedAtomics() const { return SupportsUnalignedAtomics; }\n\n  /// Whether AtomicExpandPass should automatically insert fences and reduce\n  /// ordering for this atomic. This should be true for most architectures with\n  /// weak memory ordering. Defaults to false.\n  virtual bool shouldInsertFencesForAtomic(const Instruction *I) const {\n    return false;\n  }\n\n  /// Perform a load-linked operation on Addr, returning a \"Value *\" with the\n  /// corresponding pointee type. This may entail some non-trivial operations to\n  /// truncate or reconstruct types that will be illegal in the backend. See\n  /// ARMISelLowering for an example implementation.\n  virtual Value *emitLoadLinked(IRBuilder<> &Builder, Value *Addr,\n                                AtomicOrdering Ord) const {\n    llvm_unreachable(\"Load linked unimplemented on this target\");\n  }\n\n  /// Perform a store-conditional operation to Addr. Return the status of the\n  /// store. This should be 0 if the store succeeded, non-zero otherwise.\n  virtual Value *emitStoreConditional(IRBuilder<> &Builder, Value *Val,\n                                      Value *Addr, AtomicOrdering Ord) const {\n    llvm_unreachable(\"Store conditional unimplemented on this target\");\n  }\n\n  /// Perform a masked atomicrmw using a target-specific intrinsic. This\n  /// represents the core LL/SC loop which will be lowered at a late stage by\n  /// the backend.\n  virtual Value *emitMaskedAtomicRMWIntrinsic(IRBuilder<> &Builder,\n                                              AtomicRMWInst *AI,\n                                              Value *AlignedAddr, Value *Incr,\n                                              Value *Mask, Value *ShiftAmt,\n                                              AtomicOrdering Ord) const {\n    llvm_unreachable(\"Masked atomicrmw expansion unimplemented on this target\");\n  }\n\n  /// Perform a masked cmpxchg using a target-specific intrinsic. This\n  /// represents the core LL/SC loop which will be lowered at a late stage by\n  /// the backend.\n  virtual Value *emitMaskedAtomicCmpXchgIntrinsic(\n      IRBuilder<> &Builder, AtomicCmpXchgInst *CI, Value *AlignedAddr,\n      Value *CmpVal, Value *NewVal, Value *Mask, AtomicOrdering Ord) const {\n    llvm_unreachable(\"Masked cmpxchg expansion unimplemented on this target\");\n  }\n\n  /// Inserts in the IR a target-specific intrinsic specifying a fence.\n  /// It is called by AtomicExpandPass before expanding an\n  ///   AtomicRMW/AtomicCmpXchg/AtomicStore/AtomicLoad\n  ///   if shouldInsertFencesForAtomic returns true.\n  ///\n  /// Inst is the original atomic instruction, prior to other expansions that\n  /// may be performed.\n  ///\n  /// This function should either return a nullptr, or a pointer to an IR-level\n  ///   Instruction*. Even complex fence sequences can be represented by a\n  ///   single Instruction* through an intrinsic to be lowered later.\n  /// Backends should override this method to produce target-specific intrinsic\n  ///   for their fences.\n  /// FIXME: Please note that the default implementation here in terms of\n  ///   IR-level fences exists for historical/compatibility reasons and is\n  ///   *unsound* ! Fences cannot, in general, be used to restore sequential\n  ///   consistency. For example, consider the following example:\n  /// atomic<int> x = y = 0;\n  /// int r1, r2, r3, r4;\n  /// Thread 0:\n  ///   x.store(1);\n  /// Thread 1:\n  ///   y.store(1);\n  /// Thread 2:\n  ///   r1 = x.load();\n  ///   r2 = y.load();\n  /// Thread 3:\n  ///   r3 = y.load();\n  ///   r4 = x.load();\n  ///  r1 = r3 = 1 and r2 = r4 = 0 is impossible as long as the accesses are all\n  ///  seq_cst. But if they are lowered to monotonic accesses, no amount of\n  ///  IR-level fences can prevent it.\n  /// @{\n  virtual Instruction *emitLeadingFence(IRBuilder<> &Builder, Instruction *Inst,\n                                        AtomicOrdering Ord) const {\n    if (isReleaseOrStronger(Ord) && Inst->hasAtomicStore())\n      return Builder.CreateFence(Ord);\n    else\n      return nullptr;\n  }\n\n  virtual Instruction *emitTrailingFence(IRBuilder<> &Builder,\n                                         Instruction *Inst,\n                                         AtomicOrdering Ord) const {\n    if (isAcquireOrStronger(Ord))\n      return Builder.CreateFence(Ord);\n    else\n      return nullptr;\n  }\n  /// @}\n\n  // Emits code that executes when the comparison result in the ll/sc\n  // expansion of a cmpxchg instruction is such that the store-conditional will\n  // not execute.  This makes it possible to balance out the load-linked with\n  // a dedicated instruction, if desired.\n  // E.g., on ARM, if ldrex isn't followed by strex, the exclusive monitor would\n  // be unnecessarily held, except if clrex, inserted by this hook, is executed.\n  virtual void emitAtomicCmpXchgNoStoreLLBalance(IRBuilder<> &Builder) const {}\n\n  /// Returns true if the given (atomic) store should be expanded by the\n  /// IR-level AtomicExpand pass into an \"atomic xchg\" which ignores its input.\n  virtual bool shouldExpandAtomicStoreInIR(StoreInst *SI) const {\n    return false;\n  }\n\n  /// Returns true if arguments should be sign-extended in lib calls.\n  virtual bool shouldSignExtendTypeInLibCall(EVT Type, bool IsSigned) const {\n    return IsSigned;\n  }\n\n  /// Returns true if arguments should be extended in lib calls.\n  virtual bool shouldExtendTypeInLibCall(EVT Type) const {\n    return true;\n  }\n\n  /// Returns how the given (atomic) load should be expanded by the\n  /// IR-level AtomicExpand pass.\n  virtual AtomicExpansionKind shouldExpandAtomicLoadInIR(LoadInst *LI) const {\n    return AtomicExpansionKind::None;\n  }\n\n  /// Returns how the given atomic cmpxchg should be expanded by the IR-level\n  /// AtomicExpand pass.\n  virtual AtomicExpansionKind\n  shouldExpandAtomicCmpXchgInIR(AtomicCmpXchgInst *AI) const {\n    return AtomicExpansionKind::None;\n  }\n\n  /// Returns how the IR-level AtomicExpand pass should expand the given\n  /// AtomicRMW, if at all. Default is to never expand.\n  virtual AtomicExpansionKind shouldExpandAtomicRMWInIR(AtomicRMWInst *RMW) const {\n    return RMW->isFloatingPointOperation() ?\n      AtomicExpansionKind::CmpXChg : AtomicExpansionKind::None;\n  }\n\n  /// On some platforms, an AtomicRMW that never actually modifies the value\n  /// (such as fetch_add of 0) can be turned into a fence followed by an\n  /// atomic load. This may sound useless, but it makes it possible for the\n  /// processor to keep the cacheline shared, dramatically improving\n  /// performance. And such idempotent RMWs are useful for implementing some\n  /// kinds of locks, see for example (justification + benchmarks):\n  /// http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf\n  /// This method tries doing that transformation, returning the atomic load if\n  /// it succeeds, and nullptr otherwise.\n  /// If shouldExpandAtomicLoadInIR returns true on that load, it will undergo\n  /// another round of expansion.\n  virtual LoadInst *\n  lowerIdempotentRMWIntoFencedLoad(AtomicRMWInst *RMWI) const {\n    return nullptr;\n  }\n\n  /// Returns how the platform's atomic operations are extended (ZERO_EXTEND,\n  /// SIGN_EXTEND, or ANY_EXTEND).\n  virtual ISD::NodeType getExtendForAtomicOps() const {\n    return ISD::ZERO_EXTEND;\n  }\n\n  /// Returns how the platform's atomic compare and swap expects its comparison\n  /// value to be extended (ZERO_EXTEND, SIGN_EXTEND, or ANY_EXTEND). This is\n  /// separate from getExtendForAtomicOps, which is concerned with the\n  /// sign-extension of the instruction's output, whereas here we are concerned\n  /// with the sign-extension of the input. For targets with compare-and-swap\n  /// instructions (or sub-word comparisons in their LL/SC loop expansions),\n  /// the input can be ANY_EXTEND, but the output will still have a specific\n  /// extension.\n  virtual ISD::NodeType getExtendForAtomicCmpSwapArg() const {\n    return ISD::ANY_EXTEND;\n  }\n\n  /// @}\n\n  /// Returns true if we should normalize\n  /// select(N0&N1, X, Y) => select(N0, select(N1, X, Y), Y) and\n  /// select(N0|N1, X, Y) => select(N0, select(N1, X, Y, Y)) if it is likely\n  /// that it saves us from materializing N0 and N1 in an integer register.\n  /// Targets that are able to perform and/or on flags should return false here.\n  virtual bool shouldNormalizeToSelectSequence(LLVMContext &Context,\n                                               EVT VT) const {\n    // If a target has multiple condition registers, then it likely has logical\n    // operations on those registers.\n    if (hasMultipleConditionRegisters())\n      return false;\n    // Only do the transform if the value won't be split into multiple\n    // registers.\n    LegalizeTypeAction Action = getTypeAction(Context, VT);\n    return Action != TypeExpandInteger && Action != TypeExpandFloat &&\n      Action != TypeSplitVector;\n  }\n\n  virtual bool isProfitableToCombineMinNumMaxNum(EVT VT) const { return true; }\n\n  /// Return true if a select of constants (select Cond, C1, C2) should be\n  /// transformed into simple math ops with the condition value. For example:\n  /// select Cond, C1, C1-1 --> add (zext Cond), C1-1\n  virtual bool convertSelectOfConstantsToMath(EVT VT) const {\n    return false;\n  }\n\n  /// Return true if it is profitable to transform an integer\n  /// multiplication-by-constant into simpler operations like shifts and adds.\n  /// This may be true if the target does not directly support the\n  /// multiplication operation for the specified type or the sequence of simpler\n  /// ops is faster than the multiply.\n  virtual bool decomposeMulByConstant(LLVMContext &Context,\n                                      EVT VT, SDValue C) const {\n    return false;\n  }\n\n  /// Return true if it is more correct/profitable to use strict FP_TO_INT\n  /// conversion operations - canonicalizing the FP source value instead of\n  /// converting all cases and then selecting based on value.\n  /// This may be true if the target throws exceptions for out of bounds\n  /// conversions or has fast FP CMOV.\n  virtual bool shouldUseStrictFP_TO_INT(EVT FpVT, EVT IntVT,\n                                        bool IsSigned) const {\n    return false;\n  }\n\n  //===--------------------------------------------------------------------===//\n  // TargetLowering Configuration Methods - These methods should be invoked by\n  // the derived class constructor to configure this object for the target.\n  //\nprotected:\n  /// Specify how the target extends the result of integer and floating point\n  /// boolean values from i1 to a wider type.  See getBooleanContents.\n  void setBooleanContents(BooleanContent Ty) {\n    BooleanContents = Ty;\n    BooleanFloatContents = Ty;\n  }\n\n  /// Specify how the target extends the result of integer and floating point\n  /// boolean values from i1 to a wider type.  See getBooleanContents.\n  void setBooleanContents(BooleanContent IntTy, BooleanContent FloatTy) {\n    BooleanContents = IntTy;\n    BooleanFloatContents = FloatTy;\n  }\n\n  /// Specify how the target extends the result of a vector boolean value from a\n  /// vector of i1 to a wider type.  See getBooleanContents.\n  void setBooleanVectorContents(BooleanContent Ty) {\n    BooleanVectorContents = Ty;\n  }\n\n  /// Specify the target scheduling preference.\n  void setSchedulingPreference(Sched::Preference Pref) {\n    SchedPreferenceInfo = Pref;\n  }\n\n  /// Indicate the minimum number of blocks to generate jump tables.\n  void setMinimumJumpTableEntries(unsigned Val);\n\n  /// Indicate the maximum number of entries in jump tables.\n  /// Set to zero to generate unlimited jump tables.\n  void setMaximumJumpTableSize(unsigned);\n\n  /// If set to a physical register, this specifies the register that\n  /// llvm.savestack/llvm.restorestack should save and restore.\n  void setStackPointerRegisterToSaveRestore(Register R) {\n    StackPointerRegisterToSaveRestore = R;\n  }\n\n  /// Tells the code generator that the target has multiple (allocatable)\n  /// condition registers that can be used to store the results of comparisons\n  /// for use by selects and conditional branches. With multiple condition\n  /// registers, the code generator will not aggressively sink comparisons into\n  /// the blocks of their users.\n  void setHasMultipleConditionRegisters(bool hasManyRegs = true) {\n    HasMultipleConditionRegisters = hasManyRegs;\n  }\n\n  /// Tells the code generator that the target has BitExtract instructions.\n  /// The code generator will aggressively sink \"shift\"s into the blocks of\n  /// their users if the users will generate \"and\" instructions which can be\n  /// combined with \"shift\" to BitExtract instructions.\n  void setHasExtractBitsInsn(bool hasExtractInsn = true) {\n    HasExtractBitsInsn = hasExtractInsn;\n  }\n\n  /// Tells the code generator not to expand logic operations on comparison\n  /// predicates into separate sequences that increase the amount of flow\n  /// control.\n  void setJumpIsExpensive(bool isExpensive = true);\n\n  /// Tells the code generator which bitwidths to bypass.\n  void addBypassSlowDiv(unsigned int SlowBitWidth, unsigned int FastBitWidth) {\n    BypassSlowDivWidths[SlowBitWidth] = FastBitWidth;\n  }\n\n  /// Add the specified register class as an available regclass for the\n  /// specified value type. This indicates the selector can handle values of\n  /// that class natively.\n  void addRegisterClass(MVT VT, const TargetRegisterClass *RC) {\n    assert((unsigned)VT.SimpleTy < array_lengthof(RegClassForVT));\n    RegClassForVT[VT.SimpleTy] = RC;\n  }\n\n  /// Return the largest legal super-reg register class of the register class\n  /// for the specified type and its associated \"cost\".\n  virtual std::pair<const TargetRegisterClass *, uint8_t>\n  findRepresentativeClass(const TargetRegisterInfo *TRI, MVT VT) const;\n\n  /// Once all of the register classes are added, this allows us to compute\n  /// derived properties we expose.\n  void computeRegisterProperties(const TargetRegisterInfo *TRI);\n\n  /// Indicate that the specified operation does not work with the specified\n  /// type and indicate what to do about it. Note that VT may refer to either\n  /// the type of a result or that of an operand of Op.\n  void setOperationAction(unsigned Op, MVT VT,\n                          LegalizeAction Action) {\n    assert(Op < array_lengthof(OpActions[0]) && \"Table isn't big enough!\");\n    OpActions[(unsigned)VT.SimpleTy][Op] = Action;\n  }\n\n  /// Indicate that the specified load with extension does not work with the\n  /// specified type and indicate what to do about it.\n  void setLoadExtAction(unsigned ExtType, MVT ValVT, MVT MemVT,\n                        LegalizeAction Action) {\n    assert(ExtType < ISD::LAST_LOADEXT_TYPE && ValVT.isValid() &&\n           MemVT.isValid() && \"Table isn't big enough!\");\n    assert((unsigned)Action < 0x10 && \"too many bits for bitfield array\");\n    unsigned Shift = 4 * ExtType;\n    LoadExtActions[ValVT.SimpleTy][MemVT.SimpleTy] &= ~((uint16_t)0xF << Shift);\n    LoadExtActions[ValVT.SimpleTy][MemVT.SimpleTy] |= (uint16_t)Action << Shift;\n  }\n\n  /// Indicate that the specified truncating store does not work with the\n  /// specified type and indicate what to do about it.\n  void setTruncStoreAction(MVT ValVT, MVT MemVT,\n                           LegalizeAction Action) {\n    assert(ValVT.isValid() && MemVT.isValid() && \"Table isn't big enough!\");\n    TruncStoreActions[(unsigned)ValVT.SimpleTy][MemVT.SimpleTy] = Action;\n  }\n\n  /// Indicate that the specified indexed load does or does not work with the\n  /// specified type and indicate what to do abort it.\n  ///\n  /// NOTE: All indexed mode loads are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedLoadAction(unsigned IdxMode, MVT VT, LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_Load, Action);\n  }\n\n  /// Indicate that the specified indexed store does or does not work with the\n  /// specified type and indicate what to do about it.\n  ///\n  /// NOTE: All indexed mode stores are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedStoreAction(unsigned IdxMode, MVT VT, LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_Store, Action);\n  }\n\n  /// Indicate that the specified indexed masked load does or does not work with\n  /// the specified type and indicate what to do about it.\n  ///\n  /// NOTE: All indexed mode masked loads are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedMaskedLoadAction(unsigned IdxMode, MVT VT,\n                                  LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_MaskedLoad, Action);\n  }\n\n  /// Indicate that the specified indexed masked store does or does not work\n  /// with the specified type and indicate what to do about it.\n  ///\n  /// NOTE: All indexed mode masked stores are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedMaskedStoreAction(unsigned IdxMode, MVT VT,\n                                   LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_MaskedStore, Action);\n  }\n\n  /// Indicate that the specified condition code is or isn't supported on the\n  /// target and indicate what to do about it.\n  void setCondCodeAction(ISD::CondCode CC, MVT VT,\n                         LegalizeAction Action) {\n    assert(VT.isValid() && (unsigned)CC < array_lengthof(CondCodeActions) &&\n           \"Table isn't big enough!\");\n    assert((unsigned)Action < 0x10 && \"too many bits for bitfield array\");\n    /// The lower 3 bits of the SimpleTy index into Nth 4bit set from the 32-bit\n    /// value and the upper 29 bits index into the second dimension of the array\n    /// to select what 32-bit value to use.\n    uint32_t Shift = 4 * (VT.SimpleTy & 0x7);\n    CondCodeActions[CC][VT.SimpleTy >> 3] &= ~((uint32_t)0xF << Shift);\n    CondCodeActions[CC][VT.SimpleTy >> 3] |= (uint32_t)Action << Shift;\n  }\n\n  /// If Opc/OrigVT is specified as being promoted, the promotion code defaults\n  /// to trying a larger integer/fp until it can find one that works. If that\n  /// default is insufficient, this method can be used by the target to override\n  /// the default.\n  void AddPromotedToType(unsigned Opc, MVT OrigVT, MVT DestVT) {\n    PromoteToType[std::make_pair(Opc, OrigVT.SimpleTy)] = DestVT.SimpleTy;\n  }\n\n  /// Convenience method to set an operation to Promote and specify the type\n  /// in a single call.\n  void setOperationPromotedToType(unsigned Opc, MVT OrigVT, MVT DestVT) {\n    setOperationAction(Opc, OrigVT, Promote);\n    AddPromotedToType(Opc, OrigVT, DestVT);\n  }\n\n  /// Targets should invoke this method for each target independent node that\n  /// they want to provide a custom DAG combiner for by implementing the\n  /// PerformDAGCombine virtual method.\n  void setTargetDAGCombine(ISD::NodeType NT) {\n    assert(unsigned(NT >> 3) < array_lengthof(TargetDAGCombineArray));\n    TargetDAGCombineArray[NT >> 3] |= 1 << (NT&7);\n  }\n\n  /// Set the target's minimum function alignment.\n  void setMinFunctionAlignment(Align Alignment) {\n    MinFunctionAlignment = Alignment;\n  }\n\n  /// Set the target's preferred function alignment.  This should be set if\n  /// there is a performance benefit to higher-than-minimum alignment\n  void setPrefFunctionAlignment(Align Alignment) {\n    PrefFunctionAlignment = Alignment;\n  }\n\n  /// Set the target's preferred loop alignment. Default alignment is one, it\n  /// means the target does not care about loop alignment. The target may also\n  /// override getPrefLoopAlignment to provide per-loop values.\n  void setPrefLoopAlignment(Align Alignment) { PrefLoopAlignment = Alignment; }\n\n  /// Set the minimum stack alignment of an argument.\n  void setMinStackArgumentAlignment(Align Alignment) {\n    MinStackArgumentAlignment = Alignment;\n  }\n\n  /// Set the maximum atomic operation size supported by the\n  /// backend. Atomic operations greater than this size (as well as\n  /// ones that are not naturally aligned), will be expanded by\n  /// AtomicExpandPass into an __atomic_* library call.\n  void setMaxAtomicSizeInBitsSupported(unsigned SizeInBits) {\n    MaxAtomicSizeInBitsSupported = SizeInBits;\n  }\n\n  /// Sets the minimum cmpxchg or ll/sc size supported by the backend.\n  void setMinCmpXchgSizeInBits(unsigned SizeInBits) {\n    MinCmpXchgSizeInBits = SizeInBits;\n  }\n\n  /// Sets whether unaligned atomic operations are supported.\n  void setSupportsUnalignedAtomics(bool UnalignedSupported) {\n    SupportsUnalignedAtomics = UnalignedSupported;\n  }\n\npublic:\n  //===--------------------------------------------------------------------===//\n  // Addressing mode description hooks (used by LSR etc).\n  //\n\n  /// CodeGenPrepare sinks address calculations into the same BB as Load/Store\n  /// instructions reading the address. This allows as much computation as\n  /// possible to be done in the address mode for that operand. This hook lets\n  /// targets also pass back when this should be done on intrinsics which\n  /// load/store.\n  virtual bool getAddrModeArguments(IntrinsicInst * /*I*/,\n                                    SmallVectorImpl<Value*> &/*Ops*/,\n                                    Type *&/*AccessTy*/) const {\n    return false;\n  }\n\n  /// This represents an addressing mode of:\n  ///    BaseGV + BaseOffs + BaseReg + Scale*ScaleReg\n  /// If BaseGV is null,  there is no BaseGV.\n  /// If BaseOffs is zero, there is no base offset.\n  /// If HasBaseReg is false, there is no base register.\n  /// If Scale is zero, there is no ScaleReg.  Scale of 1 indicates a reg with\n  /// no scale.\n  struct AddrMode {\n    GlobalValue *BaseGV = nullptr;\n    int64_t      BaseOffs = 0;\n    bool         HasBaseReg = false;\n    int64_t      Scale = 0;\n    AddrMode() = default;\n  };\n\n  /// Return true if the addressing mode represented by AM is legal for this\n  /// target, for a load/store of the specified type.\n  ///\n  /// The type may be VoidTy, in which case only return true if the addressing\n  /// mode is legal for a load/store of any legal type.  TODO: Handle\n  /// pre/postinc as well.\n  ///\n  /// If the address space cannot be determined, it will be -1.\n  ///\n  /// TODO: Remove default argument\n  virtual bool isLegalAddressingMode(const DataLayout &DL, const AddrMode &AM,\n                                     Type *Ty, unsigned AddrSpace,\n                                     Instruction *I = nullptr) const;\n\n  /// Return the cost of the scaling factor used in the addressing mode\n  /// represented by AM for this target, for a load/store of the specified type.\n  ///\n  /// If the AM is supported, the return value must be >= 0.\n  /// If the AM is not supported, it returns a negative value.\n  /// TODO: Handle pre/postinc as well.\n  /// TODO: Remove default argument\n  virtual int getScalingFactorCost(const DataLayout &DL, const AddrMode &AM,\n                                   Type *Ty, unsigned AS = 0) const {\n    // Default: assume that any scaling factor used in a legal AM is free.\n    if (isLegalAddressingMode(DL, AM, Ty, AS))\n      return 0;\n    return -1;\n  }\n\n  /// Return true if the specified immediate is legal icmp immediate, that is\n  /// the target has icmp instructions which can compare a register against the\n  /// immediate without having to materialize the immediate into a register.\n  virtual bool isLegalICmpImmediate(int64_t) const {\n    return true;\n  }\n\n  /// Return true if the specified immediate is legal add immediate, that is the\n  /// target has add instructions which can add a register with the immediate\n  /// without having to materialize the immediate into a register.\n  virtual bool isLegalAddImmediate(int64_t) const {\n    return true;\n  }\n\n  /// Return true if the specified immediate is legal for the value input of a\n  /// store instruction.\n  virtual bool isLegalStoreImmediate(int64_t Value) const {\n    // Default implementation assumes that at least 0 works since it is likely\n    // that a zero register exists or a zero immediate is allowed.\n    return Value == 0;\n  }\n\n  /// Return true if it's significantly cheaper to shift a vector by a uniform\n  /// scalar than by an amount which will vary across each lane. On x86 before\n  /// AVX2 for example, there is a \"psllw\" instruction for the former case, but\n  /// no simple instruction for a general \"a << b\" operation on vectors.\n  /// This should also apply to lowering for vector funnel shifts (rotates).\n  virtual bool isVectorShiftByScalarCheap(Type *Ty) const {\n    return false;\n  }\n\n  /// Given a shuffle vector SVI representing a vector splat, return a new\n  /// scalar type of size equal to SVI's scalar type if the new type is more\n  /// profitable. Returns nullptr otherwise. For example under MVE float splats\n  /// are converted to integer to prevent the need to move from SPR to GPR\n  /// registers.\n  virtual Type* shouldConvertSplatType(ShuffleVectorInst* SVI) const {\n    return nullptr;\n  }\n\n  /// Given a set in interconnected phis of type 'From' that are loaded/stored\n  /// or bitcast to type 'To', return true if the set should be converted to\n  /// 'To'.\n  virtual bool shouldConvertPhiType(Type *From, Type *To) const {\n    return (From->isIntegerTy() || From->isFloatingPointTy()) &&\n           (To->isIntegerTy() || To->isFloatingPointTy());\n  }\n\n  /// Returns true if the opcode is a commutative binary operation.\n  virtual bool isCommutativeBinOp(unsigned Opcode) const {\n    // FIXME: This should get its info from the td file.\n    switch (Opcode) {\n    case ISD::ADD:\n    case ISD::SMIN:\n    case ISD::SMAX:\n    case ISD::UMIN:\n    case ISD::UMAX:\n    case ISD::MUL:\n    case ISD::MULHU:\n    case ISD::MULHS:\n    case ISD::SMUL_LOHI:\n    case ISD::UMUL_LOHI:\n    case ISD::FADD:\n    case ISD::FMUL:\n    case ISD::AND:\n    case ISD::OR:\n    case ISD::XOR:\n    case ISD::SADDO:\n    case ISD::UADDO:\n    case ISD::ADDC:\n    case ISD::ADDE:\n    case ISD::SADDSAT:\n    case ISD::UADDSAT:\n    case ISD::FMINNUM:\n    case ISD::FMAXNUM:\n    case ISD::FMINNUM_IEEE:\n    case ISD::FMAXNUM_IEEE:\n    case ISD::FMINIMUM:\n    case ISD::FMAXIMUM:\n      return true;\n    default: return false;\n    }\n  }\n\n  /// Return true if the node is a math/logic binary operator.\n  virtual bool isBinOp(unsigned Opcode) const {\n    // A commutative binop must be a binop.\n    if (isCommutativeBinOp(Opcode))\n      return true;\n    // These are non-commutative binops.\n    switch (Opcode) {\n    case ISD::SUB:\n    case ISD::SHL:\n    case ISD::SRL:\n    case ISD::SRA:\n    case ISD::SDIV:\n    case ISD::UDIV:\n    case ISD::SREM:\n    case ISD::UREM:\n    case ISD::FSUB:\n    case ISD::FDIV:\n    case ISD::FREM:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  /// Return true if it's free to truncate a value of type FromTy to type\n  /// ToTy. e.g. On x86 it's free to truncate a i32 value in register EAX to i16\n  /// by referencing its sub-register AX.\n  /// Targets must return false when FromTy <= ToTy.\n  virtual bool isTruncateFree(Type *FromTy, Type *ToTy) const {\n    return false;\n  }\n\n  /// Return true if a truncation from FromTy to ToTy is permitted when deciding\n  /// whether a call is in tail position. Typically this means that both results\n  /// would be assigned to the same register or stack slot, but it could mean\n  /// the target performs adequate checks of its own before proceeding with the\n  /// tail call.  Targets must return false when FromTy <= ToTy.\n  virtual bool allowTruncateForTailCall(Type *FromTy, Type *ToTy) const {\n    return false;\n  }\n\n  virtual bool isTruncateFree(EVT FromVT, EVT ToVT) const {\n    return false;\n  }\n\n  virtual bool isProfitableToHoist(Instruction *I) const { return true; }\n\n  /// Return true if the extension represented by \\p I is free.\n  /// Unlikely the is[Z|FP]ExtFree family which is based on types,\n  /// this method can use the context provided by \\p I to decide\n  /// whether or not \\p I is free.\n  /// This method extends the behavior of the is[Z|FP]ExtFree family.\n  /// In other words, if is[Z|FP]Free returns true, then this method\n  /// returns true as well. The converse is not true.\n  /// The target can perform the adequate checks by overriding isExtFreeImpl.\n  /// \\pre \\p I must be a sign, zero, or fp extension.\n  bool isExtFree(const Instruction *I) const {\n    switch (I->getOpcode()) {\n    case Instruction::FPExt:\n      if (isFPExtFree(EVT::getEVT(I->getType()),\n                      EVT::getEVT(I->getOperand(0)->getType())))\n        return true;\n      break;\n    case Instruction::ZExt:\n      if (isZExtFree(I->getOperand(0)->getType(), I->getType()))\n        return true;\n      break;\n    case Instruction::SExt:\n      break;\n    default:\n      llvm_unreachable(\"Instruction is not an extension\");\n    }\n    return isExtFreeImpl(I);\n  }\n\n  /// Return true if \\p Load and \\p Ext can form an ExtLoad.\n  /// For example, in AArch64\n  ///   %L = load i8, i8* %ptr\n  ///   %E = zext i8 %L to i32\n  /// can be lowered into one load instruction\n  ///   ldrb w0, [x0]\n  bool isExtLoad(const LoadInst *Load, const Instruction *Ext,\n                 const DataLayout &DL) const {\n    EVT VT = getValueType(DL, Ext->getType());\n    EVT LoadVT = getValueType(DL, Load->getType());\n\n    // If the load has other users and the truncate is not free, the ext\n    // probably isn't free.\n    if (!Load->hasOneUse() && (isTypeLegal(LoadVT) || !isTypeLegal(VT)) &&\n        !isTruncateFree(Ext->getType(), Load->getType()))\n      return false;\n\n    // Check whether the target supports casts folded into loads.\n    unsigned LType;\n    if (isa<ZExtInst>(Ext))\n      LType = ISD::ZEXTLOAD;\n    else {\n      assert(isa<SExtInst>(Ext) && \"Unexpected ext type!\");\n      LType = ISD::SEXTLOAD;\n    }\n\n    return isLoadExtLegal(LType, VT, LoadVT);\n  }\n\n  /// Return true if any actual instruction that defines a value of type FromTy\n  /// implicitly zero-extends the value to ToTy in the result register.\n  ///\n  /// The function should return true when it is likely that the truncate can\n  /// be freely folded with an instruction defining a value of FromTy. If\n  /// the defining instruction is unknown (because you're looking at a\n  /// function argument, PHI, etc.) then the target may require an\n  /// explicit truncate, which is not necessarily free, but this function\n  /// does not deal with those cases.\n  /// Targets must return false when FromTy >= ToTy.\n  virtual bool isZExtFree(Type *FromTy, Type *ToTy) const {\n    return false;\n  }\n\n  virtual bool isZExtFree(EVT FromTy, EVT ToTy) const {\n    return false;\n  }\n\n  /// Return true if sign-extension from FromTy to ToTy is cheaper than\n  /// zero-extension.\n  virtual bool isSExtCheaperThanZExt(EVT FromTy, EVT ToTy) const {\n    return false;\n  }\n\n  /// Return true if sinking I's operands to the same basic block as I is\n  /// profitable, e.g. because the operands can be folded into a target\n  /// instruction during instruction selection. After calling the function\n  /// \\p Ops contains the Uses to sink ordered by dominance (dominating users\n  /// come first).\n  virtual bool shouldSinkOperands(Instruction *I,\n                                  SmallVectorImpl<Use *> &Ops) const {\n    return false;\n  }\n\n  /// Return true if the target supplies and combines to a paired load\n  /// two loaded values of type LoadedType next to each other in memory.\n  /// RequiredAlignment gives the minimal alignment constraints that must be met\n  /// to be able to select this paired load.\n  ///\n  /// This information is *not* used to generate actual paired loads, but it is\n  /// used to generate a sequence of loads that is easier to combine into a\n  /// paired load.\n  /// For instance, something like this:\n  /// a = load i64* addr\n  /// b = trunc i64 a to i32\n  /// c = lshr i64 a, 32\n  /// d = trunc i64 c to i32\n  /// will be optimized into:\n  /// b = load i32* addr1\n  /// d = load i32* addr2\n  /// Where addr1 = addr2 +/- sizeof(i32).\n  ///\n  /// In other words, unless the target performs a post-isel load combining,\n  /// this information should not be provided because it will generate more\n  /// loads.\n  virtual bool hasPairedLoad(EVT /*LoadedType*/,\n                             Align & /*RequiredAlignment*/) const {\n    return false;\n  }\n\n  /// Return true if the target has a vector blend instruction.\n  virtual bool hasVectorBlend() const { return false; }\n\n  /// Get the maximum supported factor for interleaved memory accesses.\n  /// Default to be the minimum interleave factor: 2.\n  virtual unsigned getMaxSupportedInterleaveFactor() const { return 2; }\n\n  /// Lower an interleaved load to target specific intrinsics. Return\n  /// true on success.\n  ///\n  /// \\p LI is the vector load instruction.\n  /// \\p Shuffles is the shufflevector list to DE-interleave the loaded vector.\n  /// \\p Indices is the corresponding indices for each shufflevector.\n  /// \\p Factor is the interleave factor.\n  virtual bool lowerInterleavedLoad(LoadInst *LI,\n                                    ArrayRef<ShuffleVectorInst *> Shuffles,\n                                    ArrayRef<unsigned> Indices,\n                                    unsigned Factor) const {\n    return false;\n  }\n\n  /// Lower an interleaved store to target specific intrinsics. Return\n  /// true on success.\n  ///\n  /// \\p SI is the vector store instruction.\n  /// \\p SVI is the shufflevector to RE-interleave the stored vector.\n  /// \\p Factor is the interleave factor.\n  virtual bool lowerInterleavedStore(StoreInst *SI, ShuffleVectorInst *SVI,\n                                     unsigned Factor) const {\n    return false;\n  }\n\n  /// Return true if zero-extending the specific node Val to type VT2 is free\n  /// (either because it's implicitly zero-extended such as ARM ldrb / ldrh or\n  /// because it's folded such as X86 zero-extending loads).\n  virtual bool isZExtFree(SDValue Val, EVT VT2) const {\n    return isZExtFree(Val.getValueType(), VT2);\n  }\n\n  /// Return true if an fpext operation is free (for instance, because\n  /// single-precision floating-point numbers are implicitly extended to\n  /// double-precision).\n  virtual bool isFPExtFree(EVT DestVT, EVT SrcVT) const {\n    assert(SrcVT.isFloatingPoint() && DestVT.isFloatingPoint() &&\n           \"invalid fpext types\");\n    return false;\n  }\n\n  /// Return true if an fpext operation input to an \\p Opcode operation is free\n  /// (for instance, because half-precision floating-point numbers are\n  /// implicitly extended to float-precision) for an FMA instruction.\n  virtual bool isFPExtFoldable(const SelectionDAG &DAG, unsigned Opcode,\n                               EVT DestVT, EVT SrcVT) const {\n    assert(DestVT.isFloatingPoint() && SrcVT.isFloatingPoint() &&\n           \"invalid fpext types\");\n    return isFPExtFree(DestVT, SrcVT);\n  }\n\n  /// Return true if folding a vector load into ExtVal (a sign, zero, or any\n  /// extend node) is profitable.\n  virtual bool isVectorLoadExtDesirable(SDValue ExtVal) const { return false; }\n\n  /// Return true if an fneg operation is free to the point where it is never\n  /// worthwhile to replace it with a bitwise operation.\n  virtual bool isFNegFree(EVT VT) const {\n    assert(VT.isFloatingPoint());\n    return false;\n  }\n\n  /// Return true if an fabs operation is free to the point where it is never\n  /// worthwhile to replace it with a bitwise operation.\n  virtual bool isFAbsFree(EVT VT) const {\n    assert(VT.isFloatingPoint());\n    return false;\n  }\n\n  /// Return true if an FMA operation is faster than a pair of fmul and fadd\n  /// instructions. fmuladd intrinsics will be expanded to FMAs when this method\n  /// returns true, otherwise fmuladd is expanded to fmul + fadd.\n  ///\n  /// NOTE: This may be called before legalization on types for which FMAs are\n  /// not legal, but should return true if those types will eventually legalize\n  /// to types that support FMAs. After legalization, it will only be called on\n  /// types that support FMAs (via Legal or Custom actions)\n  virtual bool isFMAFasterThanFMulAndFAdd(const MachineFunction &MF,\n                                          EVT) const {\n    return false;\n  }\n\n  /// IR version\n  virtual bool isFMAFasterThanFMulAndFAdd(const Function &F, Type *) const {\n    return false;\n  }\n\n  /// Returns true if be combined with to form an ISD::FMAD. \\p N may be an\n  /// ISD::FADD, ISD::FSUB, or an ISD::FMUL which will be distributed into an\n  /// fadd/fsub.\n  virtual bool isFMADLegal(const SelectionDAG &DAG, const SDNode *N) const {\n    assert((N->getOpcode() == ISD::FADD || N->getOpcode() == ISD::FSUB ||\n            N->getOpcode() == ISD::FMUL) &&\n           \"unexpected node in FMAD forming combine\");\n    return isOperationLegal(ISD::FMAD, N->getValueType(0));\n  }\n\n  // Return true when the decision to generate FMA's (or FMS, FMLA etc) rather\n  // than FMUL and ADD is delegated to the machine combiner.\n  virtual bool generateFMAsInMachineCombiner(EVT VT,\n                                             CodeGenOpt::Level OptLevel) const {\n    return false;\n  }\n\n  /// Return true if it's profitable to narrow operations of type VT1 to\n  /// VT2. e.g. on x86, it's profitable to narrow from i32 to i8 but not from\n  /// i32 to i16.\n  virtual bool isNarrowingProfitable(EVT /*VT1*/, EVT /*VT2*/) const {\n    return false;\n  }\n\n  /// Return true if it is beneficial to convert a load of a constant to\n  /// just the constant itself.\n  /// On some targets it might be more efficient to use a combination of\n  /// arithmetic instructions to materialize the constant instead of loading it\n  /// from a constant pool.\n  virtual bool shouldConvertConstantLoadToIntImm(const APInt &Imm,\n                                                 Type *Ty) const {\n    return false;\n  }\n\n  /// Return true if EXTRACT_SUBVECTOR is cheap for extracting this result type\n  /// from this source type with this index. This is needed because\n  /// EXTRACT_SUBVECTOR usually has custom lowering that depends on the index of\n  /// the first element, and only the target knows which lowering is cheap.\n  virtual bool isExtractSubvectorCheap(EVT ResVT, EVT SrcVT,\n                                       unsigned Index) const {\n    return false;\n  }\n\n  /// Try to convert an extract element of a vector binary operation into an\n  /// extract element followed by a scalar operation.\n  virtual bool shouldScalarizeBinop(SDValue VecOp) const {\n    return false;\n  }\n\n  /// Return true if extraction of a scalar element from the given vector type\n  /// at the given index is cheap. For example, if scalar operations occur on\n  /// the same register file as vector operations, then an extract element may\n  /// be a sub-register rename rather than an actual instruction.\n  virtual bool isExtractVecEltCheap(EVT VT, unsigned Index) const {\n    return false;\n  }\n\n  /// Try to convert math with an overflow comparison into the corresponding DAG\n  /// node operation. Targets may want to override this independently of whether\n  /// the operation is legal/custom for the given type because it may obscure\n  /// matching of other patterns.\n  virtual bool shouldFormOverflowOp(unsigned Opcode, EVT VT,\n                                    bool MathUsed) const {\n    // TODO: The default logic is inherited from code in CodeGenPrepare.\n    // The opcode should not make a difference by default?\n    if (Opcode != ISD::UADDO)\n      return false;\n\n    // Allow the transform as long as we have an integer type that is not\n    // obviously illegal and unsupported and if the math result is used\n    // besides the overflow check. On some targets (e.g. SPARC), it is\n    // not profitable to form on overflow op if the math result has no\n    // concrete users.\n    if (VT.isVector())\n      return false;\n    return MathUsed && (VT.isSimple() || !isOperationExpand(Opcode, VT));\n  }\n\n  // Return true if it is profitable to use a scalar input to a BUILD_VECTOR\n  // even if the vector itself has multiple uses.\n  virtual bool aggressivelyPreferBuildVectorSources(EVT VecVT) const {\n    return false;\n  }\n\n  // Return true if CodeGenPrepare should consider splitting large offset of a\n  // GEP to make the GEP fit into the addressing mode and can be sunk into the\n  // same blocks of its users.\n  virtual bool shouldConsiderGEPOffsetSplit() const { return false; }\n\n  /// Return true if creating a shift of the type by the given\n  /// amount is not profitable.\n  virtual bool shouldAvoidTransformToShift(EVT VT, unsigned Amount) const {\n    return false;\n  }\n\n  /// Does this target require the clearing of high-order bits in a register\n  /// passed to the fp16 to fp conversion library function.\n  virtual bool shouldKeepZExtForFP16Conv() const { return false; }\n\n  //===--------------------------------------------------------------------===//\n  // Runtime Library hooks\n  //\n\n  /// Rename the default libcall routine name for the specified libcall.\n  void setLibcallName(RTLIB::Libcall Call, const char *Name) {\n    LibcallRoutineNames[Call] = Name;\n  }\n\n  /// Get the libcall routine name for the specified libcall.\n  const char *getLibcallName(RTLIB::Libcall Call) const {\n    return LibcallRoutineNames[Call];\n  }\n\n  /// Override the default CondCode to be used to test the result of the\n  /// comparison libcall against zero.\n  void setCmpLibcallCC(RTLIB::Libcall Call, ISD::CondCode CC) {\n    CmpLibcallCCs[Call] = CC;\n  }\n\n  /// Get the CondCode that's to be used to test the result of the comparison\n  /// libcall against zero.\n  ISD::CondCode getCmpLibcallCC(RTLIB::Libcall Call) const {\n    return CmpLibcallCCs[Call];\n  }\n\n  /// Set the CallingConv that should be used for the specified libcall.\n  void setLibcallCallingConv(RTLIB::Libcall Call, CallingConv::ID CC) {\n    LibcallCallingConvs[Call] = CC;\n  }\n\n  /// Get the CallingConv that should be used for the specified libcall.\n  CallingConv::ID getLibcallCallingConv(RTLIB::Libcall Call) const {\n    return LibcallCallingConvs[Call];\n  }\n\n  /// Execute target specific actions to finalize target lowering.\n  /// This is used to set extra flags in MachineFrameInformation and freezing\n  /// the set of reserved registers.\n  /// The default implementation just freezes the set of reserved registers.\n  virtual void finalizeLowering(MachineFunction &MF) const;\n\n  //===----------------------------------------------------------------------===//\n  //  GlobalISel Hooks\n  //===----------------------------------------------------------------------===//\n  /// Check whether or not \\p MI needs to be moved close to its uses.\n  virtual bool shouldLocalize(const MachineInstr &MI, const TargetTransformInfo *TTI) const;\n\n\nprivate:\n  const TargetMachine &TM;\n\n  /// Tells the code generator that the target has multiple (allocatable)\n  /// condition registers that can be used to store the results of comparisons\n  /// for use by selects and conditional branches. With multiple condition\n  /// registers, the code generator will not aggressively sink comparisons into\n  /// the blocks of their users.\n  bool HasMultipleConditionRegisters;\n\n  /// Tells the code generator that the target has BitExtract instructions.\n  /// The code generator will aggressively sink \"shift\"s into the blocks of\n  /// their users if the users will generate \"and\" instructions which can be\n  /// combined with \"shift\" to BitExtract instructions.\n  bool HasExtractBitsInsn;\n\n  /// Tells the code generator to bypass slow divide or remainder\n  /// instructions. For example, BypassSlowDivWidths[32,8] tells the code\n  /// generator to bypass 32-bit integer div/rem with an 8-bit unsigned integer\n  /// div/rem when the operands are positive and less than 256.\n  DenseMap <unsigned int, unsigned int> BypassSlowDivWidths;\n\n  /// Tells the code generator that it shouldn't generate extra flow control\n  /// instructions and should attempt to combine flow control instructions via\n  /// predication.\n  bool JumpIsExpensive;\n\n  /// Information about the contents of the high-bits in boolean values held in\n  /// a type wider than i1. See getBooleanContents.\n  BooleanContent BooleanContents;\n\n  /// Information about the contents of the high-bits in boolean values held in\n  /// a type wider than i1. See getBooleanContents.\n  BooleanContent BooleanFloatContents;\n\n  /// Information about the contents of the high-bits in boolean vector values\n  /// when the element type is wider than i1. See getBooleanContents.\n  BooleanContent BooleanVectorContents;\n\n  /// The target scheduling preference: shortest possible total cycles or lowest\n  /// register usage.\n  Sched::Preference SchedPreferenceInfo;\n\n  /// The minimum alignment that any argument on the stack needs to have.\n  Align MinStackArgumentAlignment;\n\n  /// The minimum function alignment (used when optimizing for size, and to\n  /// prevent explicitly provided alignment from leading to incorrect code).\n  Align MinFunctionAlignment;\n\n  /// The preferred function alignment (used when alignment unspecified and\n  /// optimizing for speed).\n  Align PrefFunctionAlignment;\n\n  /// The preferred loop alignment (in log2 bot in bytes).\n  Align PrefLoopAlignment;\n\n  /// Size in bits of the maximum atomics size the backend supports.\n  /// Accesses larger than this will be expanded by AtomicExpandPass.\n  unsigned MaxAtomicSizeInBitsSupported;\n\n  /// Size in bits of the minimum cmpxchg or ll/sc operation the\n  /// backend supports.\n  unsigned MinCmpXchgSizeInBits;\n\n  /// This indicates if the target supports unaligned atomic operations.\n  bool SupportsUnalignedAtomics;\n\n  /// If set to a physical register, this specifies the register that\n  /// llvm.savestack/llvm.restorestack should save and restore.\n  Register StackPointerRegisterToSaveRestore;\n\n  /// This indicates the default register class to use for each ValueType the\n  /// target supports natively.\n  const TargetRegisterClass *RegClassForVT[MVT::LAST_VALUETYPE];\n  uint16_t NumRegistersForVT[MVT::LAST_VALUETYPE];\n  MVT RegisterTypeForVT[MVT::LAST_VALUETYPE];\n\n  /// This indicates the \"representative\" register class to use for each\n  /// ValueType the target supports natively. This information is used by the\n  /// scheduler to track register pressure. By default, the representative\n  /// register class is the largest legal super-reg register class of the\n  /// register class of the specified type. e.g. On x86, i8, i16, and i32's\n  /// representative class would be GR32.\n  const TargetRegisterClass *RepRegClassForVT[MVT::LAST_VALUETYPE];\n\n  /// This indicates the \"cost\" of the \"representative\" register class for each\n  /// ValueType. The cost is used by the scheduler to approximate register\n  /// pressure.\n  uint8_t RepRegClassCostForVT[MVT::LAST_VALUETYPE];\n\n  /// For any value types we are promoting or expanding, this contains the value\n  /// type that we are changing to.  For Expanded types, this contains one step\n  /// of the expand (e.g. i64 -> i32), even if there are multiple steps required\n  /// (e.g. i64 -> i16).  For types natively supported by the system, this holds\n  /// the same type (e.g. i32 -> i32).\n  MVT TransformToType[MVT::LAST_VALUETYPE];\n\n  /// For each operation and each value type, keep a LegalizeAction that\n  /// indicates how instruction selection should deal with the operation.  Most\n  /// operations are Legal (aka, supported natively by the target), but\n  /// operations that are not should be described.  Note that operations on\n  /// non-legal value types are not described here.\n  LegalizeAction OpActions[MVT::LAST_VALUETYPE][ISD::BUILTIN_OP_END];\n\n  /// For each load extension type and each value type, keep a LegalizeAction\n  /// that indicates how instruction selection should deal with a load of a\n  /// specific value type and extension type. Uses 4-bits to store the action\n  /// for each of the 4 load ext types.\n  uint16_t LoadExtActions[MVT::LAST_VALUETYPE][MVT::LAST_VALUETYPE];\n\n  /// For each value type pair keep a LegalizeAction that indicates whether a\n  /// truncating store of a specific value type and truncating type is legal.\n  LegalizeAction TruncStoreActions[MVT::LAST_VALUETYPE][MVT::LAST_VALUETYPE];\n\n  /// For each indexed mode and each value type, keep a quad of LegalizeAction\n  /// that indicates how instruction selection should deal with the load /\n  /// store / maskedload / maskedstore.\n  ///\n  /// The first dimension is the value_type for the reference. The second\n  /// dimension represents the various modes for load store.\n  uint16_t IndexedModeActions[MVT::LAST_VALUETYPE][ISD::LAST_INDEXED_MODE];\n\n  /// For each condition code (ISD::CondCode) keep a LegalizeAction that\n  /// indicates how instruction selection should deal with the condition code.\n  ///\n  /// Because each CC action takes up 4 bits, we need to have the array size be\n  /// large enough to fit all of the value types. This can be done by rounding\n  /// up the MVT::LAST_VALUETYPE value to the next multiple of 8.\n  uint32_t CondCodeActions[ISD::SETCC_INVALID][(MVT::LAST_VALUETYPE + 7) / 8];\n\n  ValueTypeActionImpl ValueTypeActions;\n\nprivate:\n  LegalizeKind getTypeConversion(LLVMContext &Context, EVT VT) const;\n\n  /// Targets can specify ISD nodes that they would like PerformDAGCombine\n  /// callbacks for by calling setTargetDAGCombine(), which sets a bit in this\n  /// array.\n  unsigned char\n  TargetDAGCombineArray[(ISD::BUILTIN_OP_END+CHAR_BIT-1)/CHAR_BIT];\n\n  /// For operations that must be promoted to a specific type, this holds the\n  /// destination type.  This map should be sparse, so don't hold it as an\n  /// array.\n  ///\n  /// Targets add entries to this map with AddPromotedToType(..), clients access\n  /// this with getTypeToPromoteTo(..).\n  std::map<std::pair<unsigned, MVT::SimpleValueType>, MVT::SimpleValueType>\n    PromoteToType;\n\n  /// Stores the name each libcall.\n  const char *LibcallRoutineNames[RTLIB::UNKNOWN_LIBCALL + 1];\n\n  /// The ISD::CondCode that should be used to test the result of each of the\n  /// comparison libcall against zero.\n  ISD::CondCode CmpLibcallCCs[RTLIB::UNKNOWN_LIBCALL];\n\n  /// Stores the CallingConv that should be used for each libcall.\n  CallingConv::ID LibcallCallingConvs[RTLIB::UNKNOWN_LIBCALL];\n\n  /// Set default libcall names and calling conventions.\n  void InitLibcalls(const Triple &TT);\n\n  /// The bits of IndexedModeActions used to store the legalisation actions\n  /// We store the data as   | ML | MS |  L |  S | each taking 4 bits.\n  enum IndexedModeActionsBits {\n    IMAB_Store = 0,\n    IMAB_Load = 4,\n    IMAB_MaskedStore = 8,\n    IMAB_MaskedLoad = 12\n  };\n\n  void setIndexedModeAction(unsigned IdxMode, MVT VT, unsigned Shift,\n                            LegalizeAction Action) {\n    assert(VT.isValid() && IdxMode < ISD::LAST_INDEXED_MODE &&\n           (unsigned)Action < 0xf && \"Table isn't big enough!\");\n    unsigned Ty = (unsigned)VT.SimpleTy;\n    IndexedModeActions[Ty][IdxMode] &= ~(0xf << Shift);\n    IndexedModeActions[Ty][IdxMode] |= ((uint16_t)Action) << Shift;\n  }\n\n  LegalizeAction getIndexedModeAction(unsigned IdxMode, MVT VT,\n                                      unsigned Shift) const {\n    assert(IdxMode < ISD::LAST_INDEXED_MODE && VT.isValid() &&\n           \"Table isn't big enough!\");\n    unsigned Ty = (unsigned)VT.SimpleTy;\n    return (LegalizeAction)((IndexedModeActions[Ty][IdxMode] >> Shift) & 0xf);\n  }\n\nprotected:\n  /// Return true if the extension represented by \\p I is free.\n  /// \\pre \\p I is a sign, zero, or fp extension and\n  ///      is[Z|FP]ExtFree of the related types is not true.\n  virtual bool isExtFreeImpl(const Instruction *I) const { return false; }\n\n  /// Depth that GatherAllAliases should should continue looking for chain\n  /// dependencies when trying to find a more preferable chain. As an\n  /// approximation, this should be more than the number of consecutive stores\n  /// expected to be merged.\n  unsigned GatherAllAliasesMaxDepth;\n\n  /// \\brief Specify maximum number of store instructions per memset call.\n  ///\n  /// When lowering \\@llvm.memset this field specifies the maximum number of\n  /// store operations that may be substituted for the call to memset. Targets\n  /// must set this value based on the cost threshold for that target. Targets\n  /// should assume that the memset will be done using as many of the largest\n  /// store operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, storing 9 bytes on a 32-bit machine\n  /// with 16-bit alignment would result in four 2-byte stores and one 1-byte\n  /// store.  This only applies to setting a constant array of a constant size.\n  unsigned MaxStoresPerMemset;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxStoresPerMemsetOptSize;\n\n  /// \\brief Specify maximum number of store instructions per memcpy call.\n  ///\n  /// When lowering \\@llvm.memcpy this field specifies the maximum number of\n  /// store operations that may be substituted for a call to memcpy. Targets\n  /// must set this value based on the cost threshold for that target. Targets\n  /// should assume that the memcpy will be done using as many of the largest\n  /// store operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, storing 7 bytes on a 32-bit machine\n  /// with 32-bit alignment would result in one 4-byte store, a one 2-byte store\n  /// and one 1-byte store. This only applies to copying a constant array of\n  /// constant size.\n  unsigned MaxStoresPerMemcpy;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxStoresPerMemcpyOptSize;\n  /// \\brief Specify max number of store instructions to glue in inlined memcpy.\n  ///\n  /// When memcpy is inlined based on MaxStoresPerMemcpy, specify maximum number\n  /// of store instructions to keep together. This helps in pairing and\n  //  vectorization later on.\n  unsigned MaxGluedStoresPerMemcpy = 0;\n\n  /// \\brief Specify maximum number of load instructions per memcmp call.\n  ///\n  /// When lowering \\@llvm.memcmp this field specifies the maximum number of\n  /// pairs of load operations that may be substituted for a call to memcmp.\n  /// Targets must set this value based on the cost threshold for that target.\n  /// Targets should assume that the memcmp will be done using as many of the\n  /// largest load operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, loading 7 bytes on a 32-bit machine\n  /// with 32-bit alignment would result in one 4-byte load, a one 2-byte load\n  /// and one 1-byte load. This only applies to copying a constant array of\n  /// constant size.\n  unsigned MaxLoadsPerMemcmp;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxLoadsPerMemcmpOptSize;\n\n  /// \\brief Specify maximum number of store instructions per memmove call.\n  ///\n  /// When lowering \\@llvm.memmove this field specifies the maximum number of\n  /// store instructions that may be substituted for a call to memmove. Targets\n  /// must set this value based on the cost threshold for that target. Targets\n  /// should assume that the memmove will be done using as many of the largest\n  /// store operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, moving 9 bytes on a 32-bit machine\n  /// with 8-bit alignment would result in nine 1-byte stores.  This only\n  /// applies to copying a constant array of constant size.\n  unsigned MaxStoresPerMemmove;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxStoresPerMemmoveOptSize;\n\n  /// Tells the code generator that select is more expensive than a branch if\n  /// the branch is usually predicted right.\n  bool PredictableSelectIsExpensive;\n\n  /// \\see enableExtLdPromotion.\n  bool EnableExtLdPromotion;\n\n  /// Return true if the value types that can be represented by the specified\n  /// register class are all legal.\n  bool isLegalRC(const TargetRegisterInfo &TRI,\n                 const TargetRegisterClass &RC) const;\n\n  /// Replace/modify any TargetFrameIndex operands with a targte-dependent\n  /// sequence of memory operands that is recognized by PrologEpilogInserter.\n  MachineBasicBlock *emitPatchPoint(MachineInstr &MI,\n                                    MachineBasicBlock *MBB) const;\n\n  bool IsStrictFPEnabled;\n};\n\n/// This class defines information used to lower LLVM code to legal SelectionDAG\n/// operators that the target instruction selector can accept natively.\n///\n/// This class also defines callbacks that targets must implement to lower\n/// target-specific constructs to SelectionDAG operators.\nclass TargetLowering : public TargetLoweringBase {\npublic:\n  struct DAGCombinerInfo;\n  struct MakeLibCallOptions;\n\n  TargetLowering(const TargetLowering &) = delete;\n  TargetLowering &operator=(const TargetLowering &) = delete;\n\n  explicit TargetLowering(const TargetMachine &TM);\n\n  bool isPositionIndependent() const;\n\n  virtual bool isSDNodeSourceOfDivergence(const SDNode *N,\n                                          FunctionLoweringInfo *FLI,\n                                          LegacyDivergenceAnalysis *DA) const {\n    return false;\n  }\n\n  virtual bool isSDNodeAlwaysUniform(const SDNode * N) const {\n    return false;\n  }\n\n  /// Returns true by value, base pointer and offset pointer and addressing mode\n  /// by reference if the node's address can be legally represented as\n  /// pre-indexed load / store address.\n  virtual bool getPreIndexedAddressParts(SDNode * /*N*/, SDValue &/*Base*/,\n                                         SDValue &/*Offset*/,\n                                         ISD::MemIndexedMode &/*AM*/,\n                                         SelectionDAG &/*DAG*/) const {\n    return false;\n  }\n\n  /// Returns true by value, base pointer and offset pointer and addressing mode\n  /// by reference if this node can be combined with a load / store to form a\n  /// post-indexed load / store.\n  virtual bool getPostIndexedAddressParts(SDNode * /*N*/, SDNode * /*Op*/,\n                                          SDValue &/*Base*/,\n                                          SDValue &/*Offset*/,\n                                          ISD::MemIndexedMode &/*AM*/,\n                                          SelectionDAG &/*DAG*/) const {\n    return false;\n  }\n\n  /// Returns true if the specified base+offset is a legal indexed addressing\n  /// mode for this target. \\p MI is the load or store instruction that is being\n  /// considered for transformation.\n  virtual bool isIndexingLegal(MachineInstr &MI, Register Base, Register Offset,\n                               bool IsPre, MachineRegisterInfo &MRI) const {\n    return false;\n  }\n\n  /// Return the entry encoding for a jump table in the current function.  The\n  /// returned value is a member of the MachineJumpTableInfo::JTEntryKind enum.\n  virtual unsigned getJumpTableEncoding() const;\n\n  virtual const MCExpr *\n  LowerCustomJumpTableEntry(const MachineJumpTableInfo * /*MJTI*/,\n                            const MachineBasicBlock * /*MBB*/, unsigned /*uid*/,\n                            MCContext &/*Ctx*/) const {\n    llvm_unreachable(\"Need to implement this hook if target has custom JTIs\");\n  }\n\n  /// Returns relocation base for the given PIC jumptable.\n  virtual SDValue getPICJumpTableRelocBase(SDValue Table,\n                                           SelectionDAG &DAG) const;\n\n  /// This returns the relocation base for the given PIC jumptable, the same as\n  /// getPICJumpTableRelocBase, but as an MCExpr.\n  virtual const MCExpr *\n  getPICJumpTableRelocBaseExpr(const MachineFunction *MF,\n                               unsigned JTI, MCContext &Ctx) const;\n\n  /// Return true if folding a constant offset with the given GlobalAddress is\n  /// legal.  It is frequently not legal in PIC relocation models.\n  virtual bool isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const;\n\n  bool isInTailCallPosition(SelectionDAG &DAG, SDNode *Node,\n                            SDValue &Chain) const;\n\n  void softenSetCCOperands(SelectionDAG &DAG, EVT VT, SDValue &NewLHS,\n                           SDValue &NewRHS, ISD::CondCode &CCCode,\n                           const SDLoc &DL, const SDValue OldLHS,\n                           const SDValue OldRHS) const;\n\n  void softenSetCCOperands(SelectionDAG &DAG, EVT VT, SDValue &NewLHS,\n                           SDValue &NewRHS, ISD::CondCode &CCCode,\n                           const SDLoc &DL, const SDValue OldLHS,\n                           const SDValue OldRHS, SDValue &Chain,\n                           bool IsSignaling = false) const;\n\n  /// Returns a pair of (return value, chain).\n  /// It is an error to pass RTLIB::UNKNOWN_LIBCALL as \\p LC.\n  std::pair<SDValue, SDValue> makeLibCall(SelectionDAG &DAG, RTLIB::Libcall LC,\n                                          EVT RetVT, ArrayRef<SDValue> Ops,\n                                          MakeLibCallOptions CallOptions,\n                                          const SDLoc &dl,\n                                          SDValue Chain = SDValue()) const;\n\n  /// Check whether parameters to a call that are passed in callee saved\n  /// registers are the same as from the calling function.  This needs to be\n  /// checked for tail call eligibility.\n  bool parametersInCSRMatch(const MachineRegisterInfo &MRI,\n      const uint32_t *CallerPreservedMask,\n      const SmallVectorImpl<CCValAssign> &ArgLocs,\n      const SmallVectorImpl<SDValue> &OutVals) const;\n\n  //===--------------------------------------------------------------------===//\n  // TargetLowering Optimization Methods\n  //\n\n  /// A convenience struct that encapsulates a DAG, and two SDValues for\n  /// returning information from TargetLowering to its clients that want to\n  /// combine.\n  struct TargetLoweringOpt {\n    SelectionDAG &DAG;\n    bool LegalTys;\n    bool LegalOps;\n    SDValue Old;\n    SDValue New;\n\n    explicit TargetLoweringOpt(SelectionDAG &InDAG,\n                               bool LT, bool LO) :\n      DAG(InDAG), LegalTys(LT), LegalOps(LO) {}\n\n    bool LegalTypes() const { return LegalTys; }\n    bool LegalOperations() const { return LegalOps; }\n\n    bool CombineTo(SDValue O, SDValue N) {\n      Old = O;\n      New = N;\n      return true;\n    }\n  };\n\n  /// Determines the optimal series of memory ops to replace the memset / memcpy.\n  /// Return true if the number of memory ops is below the threshold (Limit).\n  /// It returns the types of the sequence of memory ops to perform\n  /// memset / memcpy by reference.\n  bool findOptimalMemOpLowering(std::vector<EVT> &MemOps, unsigned Limit,\n                                const MemOp &Op, unsigned DstAS, unsigned SrcAS,\n                                const AttributeList &FuncAttributes) const;\n\n  /// Check to see if the specified operand of the specified instruction is a\n  /// constant integer.  If so, check to see if there are any bits set in the\n  /// constant that are not demanded.  If so, shrink the constant and return\n  /// true.\n  bool ShrinkDemandedConstant(SDValue Op, const APInt &DemandedBits,\n                              const APInt &DemandedElts,\n                              TargetLoweringOpt &TLO) const;\n\n  /// Helper wrapper around ShrinkDemandedConstant, demanding all elements.\n  bool ShrinkDemandedConstant(SDValue Op, const APInt &DemandedBits,\n                              TargetLoweringOpt &TLO) const;\n\n  // Target hook to do target-specific const optimization, which is called by\n  // ShrinkDemandedConstant. This function should return true if the target\n  // doesn't want ShrinkDemandedConstant to further optimize the constant.\n  virtual bool targetShrinkDemandedConstant(SDValue Op,\n                                            const APInt &DemandedBits,\n                                            const APInt &DemandedElts,\n                                            TargetLoweringOpt &TLO) const {\n    return false;\n  }\n\n  /// Convert x+y to (VT)((SmallVT)x+(SmallVT)y) if the casts are free.  This\n  /// uses isZExtFree and ZERO_EXTEND for the widening cast, but it could be\n  /// generalized for targets with other types of implicit widening casts.\n  bool ShrinkDemandedOp(SDValue Op, unsigned BitWidth, const APInt &Demanded,\n                        TargetLoweringOpt &TLO) const;\n\n  /// Look at Op.  At this point, we know that only the DemandedBits bits of the\n  /// result of Op are ever used downstream.  If we can use this information to\n  /// simplify Op, create a new simplified DAG node and return true, returning\n  /// the original and new nodes in Old and New.  Otherwise, analyze the\n  /// expression and return a mask of KnownOne and KnownZero bits for the\n  /// expression (used to simplify the caller).  The KnownZero/One bits may only\n  /// be accurate for those bits in the Demanded masks.\n  /// \\p AssumeSingleUse When this parameter is true, this function will\n  ///    attempt to simplify \\p Op even if there are multiple uses.\n  ///    Callers are responsible for correctly updating the DAG based on the\n  ///    results of this function, because simply replacing replacing TLO.Old\n  ///    with TLO.New will be incorrect when this parameter is true and TLO.Old\n  ///    has multiple uses.\n  bool SimplifyDemandedBits(SDValue Op, const APInt &DemandedBits,\n                            const APInt &DemandedElts, KnownBits &Known,\n                            TargetLoweringOpt &TLO, unsigned Depth = 0,\n                            bool AssumeSingleUse = false) const;\n\n  /// Helper wrapper around SimplifyDemandedBits, demanding all elements.\n  /// Adds Op back to the worklist upon success.\n  bool SimplifyDemandedBits(SDValue Op, const APInt &DemandedBits,\n                            KnownBits &Known, TargetLoweringOpt &TLO,\n                            unsigned Depth = 0,\n                            bool AssumeSingleUse = false) const;\n\n  /// Helper wrapper around SimplifyDemandedBits.\n  /// Adds Op back to the worklist upon success.\n  bool SimplifyDemandedBits(SDValue Op, const APInt &DemandedBits,\n                            DAGCombinerInfo &DCI) const;\n\n  /// More limited version of SimplifyDemandedBits that can be used to \"look\n  /// through\" ops that don't contribute to the DemandedBits/DemandedElts -\n  /// bitwise ops etc.\n  SDValue SimplifyMultipleUseDemandedBits(SDValue Op, const APInt &DemandedBits,\n                                          const APInt &DemandedElts,\n                                          SelectionDAG &DAG,\n                                          unsigned Depth) const;\n\n  /// Helper wrapper around SimplifyMultipleUseDemandedBits, demanding all\n  /// elements.\n  SDValue SimplifyMultipleUseDemandedBits(SDValue Op, const APInt &DemandedBits,\n                                          SelectionDAG &DAG,\n                                          unsigned Depth = 0) const;\n\n  /// Helper wrapper around SimplifyMultipleUseDemandedBits, demanding all\n  /// bits from only some vector elements.\n  SDValue SimplifyMultipleUseDemandedVectorElts(SDValue Op,\n                                                const APInt &DemandedElts,\n                                                SelectionDAG &DAG,\n                                                unsigned Depth = 0) const;\n\n  /// Look at Vector Op. At this point, we know that only the DemandedElts\n  /// elements of the result of Op are ever used downstream.  If we can use\n  /// this information to simplify Op, create a new simplified DAG node and\n  /// return true, storing the original and new nodes in TLO.\n  /// Otherwise, analyze the expression and return a mask of KnownUndef and\n  /// KnownZero elements for the expression (used to simplify the caller).\n  /// The KnownUndef/Zero elements may only be accurate for those bits\n  /// in the DemandedMask.\n  /// \\p AssumeSingleUse When this parameter is true, this function will\n  ///    attempt to simplify \\p Op even if there are multiple uses.\n  ///    Callers are responsible for correctly updating the DAG based on the\n  ///    results of this function, because simply replacing replacing TLO.Old\n  ///    with TLO.New will be incorrect when this parameter is true and TLO.Old\n  ///    has multiple uses.\n  bool SimplifyDemandedVectorElts(SDValue Op, const APInt &DemandedEltMask,\n                                  APInt &KnownUndef, APInt &KnownZero,\n                                  TargetLoweringOpt &TLO, unsigned Depth = 0,\n                                  bool AssumeSingleUse = false) const;\n\n  /// Helper wrapper around SimplifyDemandedVectorElts.\n  /// Adds Op back to the worklist upon success.\n  bool SimplifyDemandedVectorElts(SDValue Op, const APInt &DemandedElts,\n                                  APInt &KnownUndef, APInt &KnownZero,\n                                  DAGCombinerInfo &DCI) const;\n\n  /// Determine which of the bits specified in Mask are known to be either zero\n  /// or one and return them in the KnownZero/KnownOne bitsets. The DemandedElts\n  /// argument allows us to only collect the known bits that are shared by the\n  /// requested vector elements.\n  virtual void computeKnownBitsForTargetNode(const SDValue Op,\n                                             KnownBits &Known,\n                                             const APInt &DemandedElts,\n                                             const SelectionDAG &DAG,\n                                             unsigned Depth = 0) const;\n\n  /// Determine which of the bits specified in Mask are known to be either zero\n  /// or one and return them in the KnownZero/KnownOne bitsets. The DemandedElts\n  /// argument allows us to only collect the known bits that are shared by the\n  /// requested vector elements. This is for GISel.\n  virtual void computeKnownBitsForTargetInstr(GISelKnownBits &Analysis,\n                                              Register R, KnownBits &Known,\n                                              const APInt &DemandedElts,\n                                              const MachineRegisterInfo &MRI,\n                                              unsigned Depth = 0) const;\n\n  /// Determine the known alignment for the pointer value \\p R. This is can\n  /// typically be inferred from the number of low known 0 bits. However, for a\n  /// pointer with a non-integral address space, the alignment value may be\n  /// independent from the known low bits.\n  virtual Align computeKnownAlignForTargetInstr(GISelKnownBits &Analysis,\n                                                Register R,\n                                                const MachineRegisterInfo &MRI,\n                                                unsigned Depth = 0) const;\n\n  /// Determine which of the bits of FrameIndex \\p FIOp are known to be 0.\n  /// Default implementation computes low bits based on alignment\n  /// information. This should preserve known bits passed into it.\n  virtual void computeKnownBitsForFrameIndex(int FIOp,\n                                             KnownBits &Known,\n                                             const MachineFunction &MF) const;\n\n  /// This method can be implemented by targets that want to expose additional\n  /// information about sign bits to the DAG Combiner. The DemandedElts\n  /// argument allows us to only collect the minimum sign bits that are shared\n  /// by the requested vector elements.\n  virtual unsigned ComputeNumSignBitsForTargetNode(SDValue Op,\n                                                   const APInt &DemandedElts,\n                                                   const SelectionDAG &DAG,\n                                                   unsigned Depth = 0) const;\n\n  /// This method can be implemented by targets that want to expose additional\n  /// information about sign bits to GlobalISel combiners. The DemandedElts\n  /// argument allows us to only collect the minimum sign bits that are shared\n  /// by the requested vector elements.\n  virtual unsigned computeNumSignBitsForTargetInstr(GISelKnownBits &Analysis,\n                                                    Register R,\n                                                    const APInt &DemandedElts,\n                                                    const MachineRegisterInfo &MRI,\n                                                    unsigned Depth = 0) const;\n\n  /// Attempt to simplify any target nodes based on the demanded vector\n  /// elements, returning true on success. Otherwise, analyze the expression and\n  /// return a mask of KnownUndef and KnownZero elements for the expression\n  /// (used to simplify the caller). The KnownUndef/Zero elements may only be\n  /// accurate for those bits in the DemandedMask.\n  virtual bool SimplifyDemandedVectorEltsForTargetNode(\n      SDValue Op, const APInt &DemandedElts, APInt &KnownUndef,\n      APInt &KnownZero, TargetLoweringOpt &TLO, unsigned Depth = 0) const;\n\n  /// Attempt to simplify any target nodes based on the demanded bits/elts,\n  /// returning true on success. Otherwise, analyze the\n  /// expression and return a mask of KnownOne and KnownZero bits for the\n  /// expression (used to simplify the caller).  The KnownZero/One bits may only\n  /// be accurate for those bits in the Demanded masks.\n  virtual bool SimplifyDemandedBitsForTargetNode(SDValue Op,\n                                                 const APInt &DemandedBits,\n                                                 const APInt &DemandedElts,\n                                                 KnownBits &Known,\n                                                 TargetLoweringOpt &TLO,\n                                                 unsigned Depth = 0) const;\n\n  /// More limited version of SimplifyDemandedBits that can be used to \"look\n  /// through\" ops that don't contribute to the DemandedBits/DemandedElts -\n  /// bitwise ops etc.\n  virtual SDValue SimplifyMultipleUseDemandedBitsForTargetNode(\n      SDValue Op, const APInt &DemandedBits, const APInt &DemandedElts,\n      SelectionDAG &DAG, unsigned Depth) const;\n\n  /// Tries to build a legal vector shuffle using the provided parameters\n  /// or equivalent variations. The Mask argument maybe be modified as the\n  /// function tries different variations.\n  /// Returns an empty SDValue if the operation fails.\n  SDValue buildLegalVectorShuffle(EVT VT, const SDLoc &DL, SDValue N0,\n                                  SDValue N1, MutableArrayRef<int> Mask,\n                                  SelectionDAG &DAG) const;\n\n  /// This method returns the constant pool value that will be loaded by LD.\n  /// NOTE: You must check for implicit extensions of the constant by LD.\n  virtual const Constant *getTargetConstantFromLoad(LoadSDNode *LD) const;\n\n  /// If \\p SNaN is false, \\returns true if \\p Op is known to never be any\n  /// NaN. If \\p sNaN is true, returns if \\p Op is known to never be a signaling\n  /// NaN.\n  virtual bool isKnownNeverNaNForTargetNode(SDValue Op,\n                                            const SelectionDAG &DAG,\n                                            bool SNaN = false,\n                                            unsigned Depth = 0) const;\n  struct DAGCombinerInfo {\n    void *DC;  // The DAG Combiner object.\n    CombineLevel Level;\n    bool CalledByLegalizer;\n\n  public:\n    SelectionDAG &DAG;\n\n    DAGCombinerInfo(SelectionDAG &dag, CombineLevel level,  bool cl, void *dc)\n      : DC(dc), Level(level), CalledByLegalizer(cl), DAG(dag) {}\n\n    bool isBeforeLegalize() const { return Level == BeforeLegalizeTypes; }\n    bool isBeforeLegalizeOps() const { return Level < AfterLegalizeVectorOps; }\n    bool isAfterLegalizeDAG() const { return Level >= AfterLegalizeDAG; }\n    CombineLevel getDAGCombineLevel() { return Level; }\n    bool isCalledByLegalizer() const { return CalledByLegalizer; }\n\n    void AddToWorklist(SDNode *N);\n    SDValue CombineTo(SDNode *N, ArrayRef<SDValue> To, bool AddTo = true);\n    SDValue CombineTo(SDNode *N, SDValue Res, bool AddTo = true);\n    SDValue CombineTo(SDNode *N, SDValue Res0, SDValue Res1, bool AddTo = true);\n\n    bool recursivelyDeleteUnusedNodes(SDNode *N);\n\n    void CommitTargetLoweringOpt(const TargetLoweringOpt &TLO);\n  };\n\n  /// Return if the N is a constant or constant vector equal to the true value\n  /// from getBooleanContents().\n  bool isConstTrueVal(const SDNode *N) const;\n\n  /// Return if the N is a constant or constant vector equal to the false value\n  /// from getBooleanContents().\n  bool isConstFalseVal(const SDNode *N) const;\n\n  /// Return if \\p N is a True value when extended to \\p VT.\n  bool isExtendedTrueVal(const ConstantSDNode *N, EVT VT, bool SExt) const;\n\n  /// Try to simplify a setcc built with the specified operands and cc. If it is\n  /// unable to simplify it, return a null SDValue.\n  SDValue SimplifySetCC(EVT VT, SDValue N0, SDValue N1, ISD::CondCode Cond,\n                        bool foldBooleans, DAGCombinerInfo &DCI,\n                        const SDLoc &dl) const;\n\n  // For targets which wrap address, unwrap for analysis.\n  virtual SDValue unwrapAddress(SDValue N) const { return N; }\n\n  /// Returns true (and the GlobalValue and the offset) if the node is a\n  /// GlobalAddress + offset.\n  virtual bool\n  isGAPlusOffset(SDNode *N, const GlobalValue* &GA, int64_t &Offset) const;\n\n  /// This method will be invoked for all target nodes and for any\n  /// target-independent nodes that the target has registered with invoke it\n  /// for.\n  ///\n  /// The semantics are as follows:\n  /// Return Value:\n  ///   SDValue.Val == 0   - No change was made\n  ///   SDValue.Val == N   - N was replaced, is dead, and is already handled.\n  ///   otherwise          - N should be replaced by the returned Operand.\n  ///\n  /// In addition, methods provided by DAGCombinerInfo may be used to perform\n  /// more complex transformations.\n  ///\n  virtual SDValue PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI) const;\n\n  /// Return true if it is profitable to move this shift by a constant amount\n  /// though its operand, adjusting any immediate operands as necessary to\n  /// preserve semantics. This transformation may not be desirable if it\n  /// disrupts a particularly auspicious target-specific tree (e.g. bitfield\n  /// extraction in AArch64). By default, it returns true.\n  ///\n  /// @param N the shift node\n  /// @param Level the current DAGCombine legalization level.\n  virtual bool isDesirableToCommuteWithShift(const SDNode *N,\n                                             CombineLevel Level) const {\n    return true;\n  }\n\n  /// Return true if the target has native support for the specified value type\n  /// and it is 'desirable' to use the type for the given node type. e.g. On x86\n  /// i16 is legal, but undesirable since i16 instruction encodings are longer\n  /// and some i16 instructions are slow.\n  virtual bool isTypeDesirableForOp(unsigned /*Opc*/, EVT VT) const {\n    // By default, assume all legal types are desirable.\n    return isTypeLegal(VT);\n  }\n\n  /// Return true if it is profitable for dag combiner to transform a floating\n  /// point op of specified opcode to a equivalent op of an integer\n  /// type. e.g. f32 load -> i32 load can be profitable on ARM.\n  virtual bool isDesirableToTransformToIntegerOp(unsigned /*Opc*/,\n                                                 EVT /*VT*/) const {\n    return false;\n  }\n\n  /// This method query the target whether it is beneficial for dag combiner to\n  /// promote the specified node. If true, it should return the desired\n  /// promotion type by reference.\n  virtual bool IsDesirableToPromoteOp(SDValue /*Op*/, EVT &/*PVT*/) const {\n    return false;\n  }\n\n  /// Return true if the target supports swifterror attribute. It optimizes\n  /// loads and stores to reading and writing a specific register.\n  virtual bool supportSwiftError() const {\n    return false;\n  }\n\n  /// Return true if the target supports that a subset of CSRs for the given\n  /// machine function is handled explicitly via copies.\n  virtual bool supportSplitCSR(MachineFunction *MF) const {\n    return false;\n  }\n\n  /// Perform necessary initialization to handle a subset of CSRs explicitly\n  /// via copies. This function is called at the beginning of instruction\n  /// selection.\n  virtual void initializeSplitCSR(MachineBasicBlock *Entry) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Insert explicit copies in entry and exit blocks. We copy a subset of\n  /// CSRs to virtual registers in the entry block, and copy them back to\n  /// physical registers in the exit blocks. This function is called at the end\n  /// of instruction selection.\n  virtual void insertCopiesSplitCSR(\n      MachineBasicBlock *Entry,\n      const SmallVectorImpl<MachineBasicBlock *> &Exits) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Return the newly negated expression if the cost is not expensive and\n  /// set the cost in \\p Cost to indicate that if it is cheaper or neutral to\n  /// do the negation.\n  virtual SDValue getNegatedExpression(SDValue Op, SelectionDAG &DAG,\n                                       bool LegalOps, bool OptForSize,\n                                       NegatibleCost &Cost,\n                                       unsigned Depth = 0) const;\n\n  /// This is the helper function to return the newly negated expression only\n  /// when the cost is cheaper.\n  SDValue getCheaperNegatedExpression(SDValue Op, SelectionDAG &DAG,\n                                      bool LegalOps, bool OptForSize,\n                                      unsigned Depth = 0) const {\n    NegatibleCost Cost = NegatibleCost::Expensive;\n    SDValue Neg =\n        getNegatedExpression(Op, DAG, LegalOps, OptForSize, Cost, Depth);\n    if (Neg && Cost == NegatibleCost::Cheaper)\n      return Neg;\n    // Remove the new created node to avoid the side effect to the DAG.\n    if (Neg && Neg.getNode()->use_empty())\n      DAG.RemoveDeadNode(Neg.getNode());\n    return SDValue();\n  }\n\n  /// This is the helper function to return the newly negated expression if\n  /// the cost is not expensive.\n  SDValue getNegatedExpression(SDValue Op, SelectionDAG &DAG, bool LegalOps,\n                               bool OptForSize, unsigned Depth = 0) const {\n    NegatibleCost Cost = NegatibleCost::Expensive;\n    return getNegatedExpression(Op, DAG, LegalOps, OptForSize, Cost, Depth);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Lowering methods - These methods must be implemented by targets so that\n  // the SelectionDAGBuilder code knows how to lower these.\n  //\n\n  /// Target-specific splitting of values into parts that fit a register\n  /// storing a legal type\n  virtual bool splitValueIntoRegisterParts(SelectionDAG &DAG, const SDLoc &DL,\n                                           SDValue Val, SDValue *Parts,\n                                           unsigned NumParts, MVT PartVT,\n                                           Optional<CallingConv::ID> CC) const {\n    return false;\n  }\n\n  /// Target-specific combining of register parts into its original value\n  virtual SDValue\n  joinRegisterPartsIntoValue(SelectionDAG &DAG, const SDLoc &DL,\n                             const SDValue *Parts, unsigned NumParts,\n                             MVT PartVT, EVT ValueVT,\n                             Optional<CallingConv::ID> CC) const {\n    return SDValue();\n  }\n\n  /// This hook must be implemented to lower the incoming (formal) arguments,\n  /// described by the Ins array, into the specified DAG. The implementation\n  /// should fill in the InVals array with legal-type argument values, and\n  /// return the resulting token chain value.\n  virtual SDValue LowerFormalArguments(\n      SDValue /*Chain*/, CallingConv::ID /*CallConv*/, bool /*isVarArg*/,\n      const SmallVectorImpl<ISD::InputArg> & /*Ins*/, const SDLoc & /*dl*/,\n      SelectionDAG & /*DAG*/, SmallVectorImpl<SDValue> & /*InVals*/) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// This structure contains all information that is necessary for lowering\n  /// calls. It is passed to TLI::LowerCallTo when the SelectionDAG builder\n  /// needs to lower a call, and targets will see this struct in their LowerCall\n  /// implementation.\n  struct CallLoweringInfo {\n    SDValue Chain;\n    Type *RetTy = nullptr;\n    bool RetSExt           : 1;\n    bool RetZExt           : 1;\n    bool IsVarArg          : 1;\n    bool IsInReg           : 1;\n    bool DoesNotReturn     : 1;\n    bool IsReturnValueUsed : 1;\n    bool IsConvergent      : 1;\n    bool IsPatchPoint      : 1;\n    bool IsPreallocated : 1;\n    bool NoMerge           : 1;\n\n    // IsTailCall should be modified by implementations of\n    // TargetLowering::LowerCall that perform tail call conversions.\n    bool IsTailCall = false;\n\n    // Is Call lowering done post SelectionDAG type legalization.\n    bool IsPostTypeLegalization = false;\n\n    unsigned NumFixedArgs = -1;\n    CallingConv::ID CallConv = CallingConv::C;\n    SDValue Callee;\n    ArgListTy Args;\n    SelectionDAG &DAG;\n    SDLoc DL;\n    const CallBase *CB = nullptr;\n    SmallVector<ISD::OutputArg, 32> Outs;\n    SmallVector<SDValue, 32> OutVals;\n    SmallVector<ISD::InputArg, 32> Ins;\n    SmallVector<SDValue, 4> InVals;\n\n    CallLoweringInfo(SelectionDAG &DAG)\n        : RetSExt(false), RetZExt(false), IsVarArg(false), IsInReg(false),\n          DoesNotReturn(false), IsReturnValueUsed(true), IsConvergent(false),\n          IsPatchPoint(false), IsPreallocated(false), NoMerge(false),\n          DAG(DAG) {}\n\n    CallLoweringInfo &setDebugLoc(const SDLoc &dl) {\n      DL = dl;\n      return *this;\n    }\n\n    CallLoweringInfo &setChain(SDValue InChain) {\n      Chain = InChain;\n      return *this;\n    }\n\n    // setCallee with target/module-specific attributes\n    CallLoweringInfo &setLibCallee(CallingConv::ID CC, Type *ResultType,\n                                   SDValue Target, ArgListTy &&ArgsList) {\n      RetTy = ResultType;\n      Callee = Target;\n      CallConv = CC;\n      NumFixedArgs = ArgsList.size();\n      Args = std::move(ArgsList);\n\n      DAG.getTargetLoweringInfo().markLibCallAttributes(\n          &(DAG.getMachineFunction()), CC, Args);\n      return *this;\n    }\n\n    CallLoweringInfo &setCallee(CallingConv::ID CC, Type *ResultType,\n                                SDValue Target, ArgListTy &&ArgsList) {\n      RetTy = ResultType;\n      Callee = Target;\n      CallConv = CC;\n      NumFixedArgs = ArgsList.size();\n      Args = std::move(ArgsList);\n      return *this;\n    }\n\n    CallLoweringInfo &setCallee(Type *ResultType, FunctionType *FTy,\n                                SDValue Target, ArgListTy &&ArgsList,\n                                const CallBase &Call) {\n      RetTy = ResultType;\n\n      IsInReg = Call.hasRetAttr(Attribute::InReg);\n      DoesNotReturn =\n          Call.doesNotReturn() ||\n          (!isa<InvokeInst>(Call) && isa<UnreachableInst>(Call.getNextNode()));\n      IsVarArg = FTy->isVarArg();\n      IsReturnValueUsed = !Call.use_empty();\n      RetSExt = Call.hasRetAttr(Attribute::SExt);\n      RetZExt = Call.hasRetAttr(Attribute::ZExt);\n      NoMerge = Call.hasFnAttr(Attribute::NoMerge);\n      \n      Callee = Target;\n\n      CallConv = Call.getCallingConv();\n      NumFixedArgs = FTy->getNumParams();\n      Args = std::move(ArgsList);\n\n      CB = &Call;\n\n      return *this;\n    }\n\n    CallLoweringInfo &setInRegister(bool Value = true) {\n      IsInReg = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setNoReturn(bool Value = true) {\n      DoesNotReturn = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setVarArg(bool Value = true) {\n      IsVarArg = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setTailCall(bool Value = true) {\n      IsTailCall = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setDiscardResult(bool Value = true) {\n      IsReturnValueUsed = !Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setConvergent(bool Value = true) {\n      IsConvergent = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setSExtResult(bool Value = true) {\n      RetSExt = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setZExtResult(bool Value = true) {\n      RetZExt = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setIsPatchPoint(bool Value = true) {\n      IsPatchPoint = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setIsPreallocated(bool Value = true) {\n      IsPreallocated = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setIsPostTypeLegalization(bool Value=true) {\n      IsPostTypeLegalization = Value;\n      return *this;\n    }\n\n    ArgListTy &getArgs() {\n      return Args;\n    }\n  };\n\n  /// This structure is used to pass arguments to makeLibCall function.\n  struct MakeLibCallOptions {\n    // By passing type list before soften to makeLibCall, the target hook\n    // shouldExtendTypeInLibCall can get the original type before soften.\n    ArrayRef<EVT> OpsVTBeforeSoften;\n    EVT RetVTBeforeSoften;\n    bool IsSExt : 1;\n    bool DoesNotReturn : 1;\n    bool IsReturnValueUsed : 1;\n    bool IsPostTypeLegalization : 1;\n    bool IsSoften : 1;\n\n    MakeLibCallOptions()\n        : IsSExt(false), DoesNotReturn(false), IsReturnValueUsed(true),\n          IsPostTypeLegalization(false), IsSoften(false) {}\n\n    MakeLibCallOptions &setSExt(bool Value = true) {\n      IsSExt = Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setNoReturn(bool Value = true) {\n      DoesNotReturn = Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setDiscardResult(bool Value = true) {\n      IsReturnValueUsed = !Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setIsPostTypeLegalization(bool Value = true) {\n      IsPostTypeLegalization = Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setTypeListBeforeSoften(ArrayRef<EVT> OpsVT, EVT RetVT,\n                                                bool Value = true) {\n      OpsVTBeforeSoften = OpsVT;\n      RetVTBeforeSoften = RetVT;\n      IsSoften = Value;\n      return *this;\n    }\n  };\n\n  /// This function lowers an abstract call to a function into an actual call.\n  /// This returns a pair of operands.  The first element is the return value\n  /// for the function (if RetTy is not VoidTy).  The second element is the\n  /// outgoing token chain. It calls LowerCall to do the actual lowering.\n  std::pair<SDValue, SDValue> LowerCallTo(CallLoweringInfo &CLI) const;\n\n  /// This hook must be implemented to lower calls into the specified\n  /// DAG. The outgoing arguments to the call are described by the Outs array,\n  /// and the values to be returned by the call are described by the Ins\n  /// array. The implementation should fill in the InVals array with legal-type\n  /// return values from the call, and return the resulting token chain value.\n  virtual SDValue\n    LowerCall(CallLoweringInfo &/*CLI*/,\n              SmallVectorImpl<SDValue> &/*InVals*/) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Target-specific cleanup for formal ByVal parameters.\n  virtual void HandleByVal(CCState *, unsigned &, Align) const {}\n\n  /// This hook should be implemented to check whether the return values\n  /// described by the Outs array can fit into the return registers.  If false\n  /// is returned, an sret-demotion is performed.\n  virtual bool CanLowerReturn(CallingConv::ID /*CallConv*/,\n                              MachineFunction &/*MF*/, bool /*isVarArg*/,\n               const SmallVectorImpl<ISD::OutputArg> &/*Outs*/,\n               LLVMContext &/*Context*/) const\n  {\n    // Return true by default to get preexisting behavior.\n    return true;\n  }\n\n  /// This hook must be implemented to lower outgoing return values, described\n  /// by the Outs array, into the specified DAG. The implementation should\n  /// return the resulting token chain value.\n  virtual SDValue LowerReturn(SDValue /*Chain*/, CallingConv::ID /*CallConv*/,\n                              bool /*isVarArg*/,\n                              const SmallVectorImpl<ISD::OutputArg> & /*Outs*/,\n                              const SmallVectorImpl<SDValue> & /*OutVals*/,\n                              const SDLoc & /*dl*/,\n                              SelectionDAG & /*DAG*/) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Return true if result of the specified node is used by a return node\n  /// only. It also compute and return the input chain for the tail call.\n  ///\n  /// This is used to determine whether it is possible to codegen a libcall as\n  /// tail call at legalization time.\n  virtual bool isUsedByReturnOnly(SDNode *, SDValue &/*Chain*/) const {\n    return false;\n  }\n\n  /// Return true if the target may be able emit the call instruction as a tail\n  /// call. This is used by optimization passes to determine if it's profitable\n  /// to duplicate return instructions to enable tailcall optimization.\n  virtual bool mayBeEmittedAsTailCall(const CallInst *) const {\n    return false;\n  }\n\n  /// Return the builtin name for the __builtin___clear_cache intrinsic\n  /// Default is to invoke the clear cache library call\n  virtual const char * getClearCacheBuiltinName() const {\n    return \"__clear_cache\";\n  }\n\n  /// Return the register ID of the name passed in. Used by named register\n  /// global variables extension. There is no target-independent behaviour\n  /// so the default action is to bail.\n  virtual Register getRegisterByName(const char* RegName, LLT Ty,\n                                     const MachineFunction &MF) const {\n    report_fatal_error(\"Named registers not implemented for this target\");\n  }\n\n  /// Return the type that should be used to zero or sign extend a\n  /// zeroext/signext integer return value.  FIXME: Some C calling conventions\n  /// require the return type to be promoted, but this is not true all the time,\n  /// e.g. i1/i8/i16 on x86/x86_64. It is also not necessary for non-C calling\n  /// conventions. The frontend should handle this and include all of the\n  /// necessary information.\n  virtual EVT getTypeForExtReturn(LLVMContext &Context, EVT VT,\n                                       ISD::NodeType /*ExtendKind*/) const {\n    EVT MinVT = getRegisterType(Context, MVT::i32);\n    return VT.bitsLT(MinVT) ? MinVT : VT;\n  }\n\n  /// For some targets, an LLVM struct type must be broken down into multiple\n  /// simple types, but the calling convention specifies that the entire struct\n  /// must be passed in a block of consecutive registers.\n  virtual bool\n  functionArgumentNeedsConsecutiveRegisters(Type *Ty, CallingConv::ID CallConv,\n                                            bool isVarArg) const {\n    return false;\n  }\n\n  /// For most targets, an LLVM type must be broken down into multiple\n  /// smaller types. Usually the halves are ordered according to the endianness\n  /// but for some platform that would break. So this method will default to\n  /// matching the endianness but can be overridden.\n  virtual bool\n  shouldSplitFunctionArgumentsAsLittleEndian(const DataLayout &DL) const {\n    return DL.isLittleEndian();\n  }\n\n  /// Returns a 0 terminated array of registers that can be safely used as\n  /// scratch registers.\n  virtual const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const {\n    return nullptr;\n  }\n\n  /// This callback is used to prepare for a volatile or atomic load.\n  /// It takes a chain node as input and returns the chain for the load itself.\n  ///\n  /// Having a callback like this is necessary for targets like SystemZ,\n  /// which allows a CPU to reuse the result of a previous load indefinitely,\n  /// even if a cache-coherent store is performed by another CPU.  The default\n  /// implementation does nothing.\n  virtual SDValue prepareVolatileOrAtomicLoad(SDValue Chain, const SDLoc &DL,\n                                              SelectionDAG &DAG) const {\n    return Chain;\n  }\n\n  /// Should SelectionDAG lower an atomic store of the given kind as a normal\n  /// StoreSDNode (as opposed to an AtomicSDNode)?  NOTE: The intention is to\n  /// eventually migrate all targets to the using StoreSDNodes, but porting is\n  /// being done target at a time.\n  virtual bool lowerAtomicStoreAsStoreSDNode(const StoreInst &SI) const {\n    assert(SI.isAtomic() && \"violated precondition\");\n    return false;\n  }\n\n  /// Should SelectionDAG lower an atomic load of the given kind as a normal\n  /// LoadSDNode (as opposed to an AtomicSDNode)?  NOTE: The intention is to\n  /// eventually migrate all targets to the using LoadSDNodes, but porting is\n  /// being done target at a time.\n  virtual bool lowerAtomicLoadAsLoadSDNode(const LoadInst &LI) const {\n    assert(LI.isAtomic() && \"violated precondition\");\n    return false;\n  }\n\n\n  /// This callback is invoked by the type legalizer to legalize nodes with an\n  /// illegal operand type but legal result types.  It replaces the\n  /// LowerOperation callback in the type Legalizer.  The reason we can not do\n  /// away with LowerOperation entirely is that LegalizeDAG isn't yet ready to\n  /// use this callback.\n  ///\n  /// TODO: Consider merging with ReplaceNodeResults.\n  ///\n  /// The target places new result values for the node in Results (their number\n  /// and types must exactly match those of the original return values of\n  /// the node), or leaves Results empty, which indicates that the node is not\n  /// to be custom lowered after all.\n  /// The default implementation calls LowerOperation.\n  virtual void LowerOperationWrapper(SDNode *N,\n                                     SmallVectorImpl<SDValue> &Results,\n                                     SelectionDAG &DAG) const;\n\n  /// This callback is invoked for operations that are unsupported by the\n  /// target, which are registered to use 'custom' lowering, and whose defined\n  /// values are all legal.  If the target has no operations that require custom\n  /// lowering, it need not implement this.  The default implementation of this\n  /// aborts.\n  virtual SDValue LowerOperation(SDValue Op, SelectionDAG &DAG) const;\n\n  /// This callback is invoked when a node result type is illegal for the\n  /// target, and the operation was registered to use 'custom' lowering for that\n  /// result type.  The target places new result values for the node in Results\n  /// (their number and types must exactly match those of the original return\n  /// values of the node), or leaves Results empty, which indicates that the\n  /// node is not to be custom lowered after all.\n  ///\n  /// If the target has no operations that require custom lowering, it need not\n  /// implement this.  The default implementation aborts.\n  virtual void ReplaceNodeResults(SDNode * /*N*/,\n                                  SmallVectorImpl<SDValue> &/*Results*/,\n                                  SelectionDAG &/*DAG*/) const {\n    llvm_unreachable(\"ReplaceNodeResults not implemented for this target!\");\n  }\n\n  /// This method returns the name of a target specific DAG node.\n  virtual const char *getTargetNodeName(unsigned Opcode) const;\n\n  /// This method returns a target specific FastISel object, or null if the\n  /// target does not support \"fast\" ISel.\n  virtual FastISel *createFastISel(FunctionLoweringInfo &,\n                                   const TargetLibraryInfo *) const {\n    return nullptr;\n  }\n\n  bool verifyReturnAddressArgumentIsConstant(SDValue Op,\n                                             SelectionDAG &DAG) const;\n\n  //===--------------------------------------------------------------------===//\n  // Inline Asm Support hooks\n  //\n\n  /// This hook allows the target to expand an inline asm call to be explicit\n  /// llvm code if it wants to.  This is useful for turning simple inline asms\n  /// into LLVM intrinsics, which gives the compiler more information about the\n  /// behavior of the code.\n  virtual bool ExpandInlineAsm(CallInst *) const {\n    return false;\n  }\n\n  enum ConstraintType {\n    C_Register,            // Constraint represents specific register(s).\n    C_RegisterClass,       // Constraint represents any of register(s) in class.\n    C_Memory,              // Memory constraint.\n    C_Immediate,           // Requires an immediate.\n    C_Other,               // Something else.\n    C_Unknown              // Unsupported constraint.\n  };\n\n  enum ConstraintWeight {\n    // Generic weights.\n    CW_Invalid  = -1,     // No match.\n    CW_Okay     = 0,      // Acceptable.\n    CW_Good     = 1,      // Good weight.\n    CW_Better   = 2,      // Better weight.\n    CW_Best     = 3,      // Best weight.\n\n    // Well-known weights.\n    CW_SpecificReg  = CW_Okay,    // Specific register operands.\n    CW_Register     = CW_Good,    // Register operands.\n    CW_Memory       = CW_Better,  // Memory operands.\n    CW_Constant     = CW_Best,    // Constant operand.\n    CW_Default      = CW_Okay     // Default or don't know type.\n  };\n\n  /// This contains information for each constraint that we are lowering.\n  struct AsmOperandInfo : public InlineAsm::ConstraintInfo {\n    /// This contains the actual string for the code, like \"m\".  TargetLowering\n    /// picks the 'best' code from ConstraintInfo::Codes that most closely\n    /// matches the operand.\n    std::string ConstraintCode;\n\n    /// Information about the constraint code, e.g. Register, RegisterClass,\n    /// Memory, Other, Unknown.\n    TargetLowering::ConstraintType ConstraintType = TargetLowering::C_Unknown;\n\n    /// If this is the result output operand or a clobber, this is null,\n    /// otherwise it is the incoming operand to the CallInst.  This gets\n    /// modified as the asm is processed.\n    Value *CallOperandVal = nullptr;\n\n    /// The ValueType for the operand value.\n    MVT ConstraintVT = MVT::Other;\n\n    /// Copy constructor for copying from a ConstraintInfo.\n    AsmOperandInfo(InlineAsm::ConstraintInfo Info)\n        : InlineAsm::ConstraintInfo(std::move(Info)) {}\n\n    /// Return true of this is an input operand that is a matching constraint\n    /// like \"4\".\n    bool isMatchingInputConstraint() const;\n\n    /// If this is an input matching constraint, this method returns the output\n    /// operand it matches.\n    unsigned getMatchedOperand() const;\n  };\n\n  using AsmOperandInfoVector = std::vector<AsmOperandInfo>;\n\n  /// Split up the constraint string from the inline assembly value into the\n  /// specific constraints and their prefixes, and also tie in the associated\n  /// operand values.  If this returns an empty vector, and if the constraint\n  /// string itself isn't empty, there was an error parsing.\n  virtual AsmOperandInfoVector ParseConstraints(const DataLayout &DL,\n                                                const TargetRegisterInfo *TRI,\n                                                const CallBase &Call) const;\n\n  /// Examine constraint type and operand type and determine a weight value.\n  /// The operand object must already have been set up with the operand type.\n  virtual ConstraintWeight getMultipleConstraintMatchWeight(\n      AsmOperandInfo &info, int maIndex) const;\n\n  /// Examine constraint string and operand type and determine a weight value.\n  /// The operand object must already have been set up with the operand type.\n  virtual ConstraintWeight getSingleConstraintMatchWeight(\n      AsmOperandInfo &info, const char *constraint) const;\n\n  /// Determines the constraint code and constraint type to use for the specific\n  /// AsmOperandInfo, setting OpInfo.ConstraintCode and OpInfo.ConstraintType.\n  /// If the actual operand being passed in is available, it can be passed in as\n  /// Op, otherwise an empty SDValue can be passed.\n  virtual void ComputeConstraintToUse(AsmOperandInfo &OpInfo,\n                                      SDValue Op,\n                                      SelectionDAG *DAG = nullptr) const;\n\n  /// Given a constraint, return the type of constraint it is for this target.\n  virtual ConstraintType getConstraintType(StringRef Constraint) const;\n\n  /// Given a physical register constraint (e.g.  {edx}), return the register\n  /// number and the register class for the register.\n  ///\n  /// Given a register class constraint, like 'r', if this corresponds directly\n  /// to an LLVM register class, return a register of 0 and the register class\n  /// pointer.\n  ///\n  /// This should only be used for C_Register constraints.  On error, this\n  /// returns a register number of 0 and a null register class pointer.\n  virtual std::pair<unsigned, const TargetRegisterClass *>\n  getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,\n                               StringRef Constraint, MVT VT) const;\n\n  virtual unsigned getInlineAsmMemConstraint(StringRef ConstraintCode) const {\n    if (ConstraintCode == \"m\")\n      return InlineAsm::Constraint_m;\n    return InlineAsm::Constraint_Unknown;\n  }\n\n  /// Try to replace an X constraint, which matches anything, with another that\n  /// has more specific requirements based on the type of the corresponding\n  /// operand.  This returns null if there is no replacement to make.\n  virtual const char *LowerXConstraint(EVT ConstraintVT) const;\n\n  /// Lower the specified operand into the Ops vector.  If it is invalid, don't\n  /// add anything to Ops.\n  virtual void LowerAsmOperandForConstraint(SDValue Op, std::string &Constraint,\n                                            std::vector<SDValue> &Ops,\n                                            SelectionDAG &DAG) const;\n\n  // Lower custom output constraints. If invalid, return SDValue().\n  virtual SDValue LowerAsmOutputForConstraint(SDValue &Chain, SDValue &Flag,\n                                              const SDLoc &DL,\n                                              const AsmOperandInfo &OpInfo,\n                                              SelectionDAG &DAG) const;\n\n  //===--------------------------------------------------------------------===//\n  // Div utility functions\n  //\n  SDValue BuildSDIV(SDNode *N, SelectionDAG &DAG, bool IsAfterLegalization,\n                    SmallVectorImpl<SDNode *> &Created) const;\n  SDValue BuildUDIV(SDNode *N, SelectionDAG &DAG, bool IsAfterLegalization,\n                    SmallVectorImpl<SDNode *> &Created) const;\n\n  /// Targets may override this function to provide custom SDIV lowering for\n  /// power-of-2 denominators.  If the target returns an empty SDValue, LLVM\n  /// assumes SDIV is expensive and replaces it with a series of other integer\n  /// operations.\n  virtual SDValue BuildSDIVPow2(SDNode *N, const APInt &Divisor,\n                                SelectionDAG &DAG,\n                                SmallVectorImpl<SDNode *> &Created) const;\n\n  /// Indicate whether this target prefers to combine FDIVs with the same\n  /// divisor. If the transform should never be done, return zero. If the\n  /// transform should be done, return the minimum number of divisor uses\n  /// that must exist.\n  virtual unsigned combineRepeatedFPDivisors() const {\n    return 0;\n  }\n\n  /// Hooks for building estimates in place of slower divisions and square\n  /// roots.\n\n  /// Return either a square root or its reciprocal estimate value for the input\n  /// operand.\n  /// \\p Enabled is a ReciprocalEstimate enum with value either 'Unspecified' or\n  /// 'Enabled' as set by a potential default override attribute.\n  /// If \\p RefinementSteps is 'Unspecified', the number of Newton-Raphson\n  /// refinement iterations required to generate a sufficient (though not\n  /// necessarily IEEE-754 compliant) estimate is returned in that parameter.\n  /// The boolean UseOneConstNR output is used to select a Newton-Raphson\n  /// algorithm implementation that uses either one or two constants.\n  /// The boolean Reciprocal is used to select whether the estimate is for the\n  /// square root of the input operand or the reciprocal of its square root.\n  /// A target may choose to implement its own refinement within this function.\n  /// If that's true, then return '0' as the number of RefinementSteps to avoid\n  /// any further refinement of the estimate.\n  /// An empty SDValue return means no estimate sequence can be created.\n  virtual SDValue getSqrtEstimate(SDValue Operand, SelectionDAG &DAG,\n                                  int Enabled, int &RefinementSteps,\n                                  bool &UseOneConstNR, bool Reciprocal) const {\n    return SDValue();\n  }\n\n  /// Return a reciprocal estimate value for the input operand.\n  /// \\p Enabled is a ReciprocalEstimate enum with value either 'Unspecified' or\n  /// 'Enabled' as set by a potential default override attribute.\n  /// If \\p RefinementSteps is 'Unspecified', the number of Newton-Raphson\n  /// refinement iterations required to generate a sufficient (though not\n  /// necessarily IEEE-754 compliant) estimate is returned in that parameter.\n  /// A target may choose to implement its own refinement within this function.\n  /// If that's true, then return '0' as the number of RefinementSteps to avoid\n  /// any further refinement of the estimate.\n  /// An empty SDValue return means no estimate sequence can be created.\n  virtual SDValue getRecipEstimate(SDValue Operand, SelectionDAG &DAG,\n                                   int Enabled, int &RefinementSteps) const {\n    return SDValue();\n  }\n\n  /// Return a target-dependent comparison result if the input operand is\n  /// suitable for use with a square root estimate calculation. For example, the\n  /// comparison may check if the operand is NAN, INF, zero, normal, etc. The\n  /// result should be used as the condition operand for a select or branch.\n  virtual SDValue getSqrtInputTest(SDValue Operand, SelectionDAG &DAG,\n                                   const DenormalMode &Mode) const;\n\n  /// Return a target-dependent result if the input operand is not suitable for\n  /// use with a square root estimate calculation.\n  virtual SDValue getSqrtResultForDenormInput(SDValue Operand,\n                                              SelectionDAG &DAG) const {\n    return DAG.getConstantFP(0.0, SDLoc(Operand), Operand.getValueType());\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Legalization utility functions\n  //\n\n  /// Expand a MUL or [US]MUL_LOHI of n-bit values into two or four nodes,\n  /// respectively, each computing an n/2-bit part of the result.\n  /// \\param Result A vector that will be filled with the parts of the result\n  ///        in little-endian order.\n  /// \\param LL Low bits of the LHS of the MUL.  You can use this parameter\n  ///        if you want to control how low bits are extracted from the LHS.\n  /// \\param LH High bits of the LHS of the MUL.  See LL for meaning.\n  /// \\param RL Low bits of the RHS of the MUL.  See LL for meaning\n  /// \\param RH High bits of the RHS of the MUL.  See LL for meaning.\n  /// \\returns true if the node has been expanded, false if it has not\n  bool expandMUL_LOHI(unsigned Opcode, EVT VT, const SDLoc &dl, SDValue LHS,\n                      SDValue RHS, SmallVectorImpl<SDValue> &Result, EVT HiLoVT,\n                      SelectionDAG &DAG, MulExpansionKind Kind,\n                      SDValue LL = SDValue(), SDValue LH = SDValue(),\n                      SDValue RL = SDValue(), SDValue RH = SDValue()) const;\n\n  /// Expand a MUL into two nodes.  One that computes the high bits of\n  /// the result and one that computes the low bits.\n  /// \\param HiLoVT The value type to use for the Lo and Hi nodes.\n  /// \\param LL Low bits of the LHS of the MUL.  You can use this parameter\n  ///        if you want to control how low bits are extracted from the LHS.\n  /// \\param LH High bits of the LHS of the MUL.  See LL for meaning.\n  /// \\param RL Low bits of the RHS of the MUL.  See LL for meaning\n  /// \\param RH High bits of the RHS of the MUL.  See LL for meaning.\n  /// \\returns true if the node has been expanded. false if it has not\n  bool expandMUL(SDNode *N, SDValue &Lo, SDValue &Hi, EVT HiLoVT,\n                 SelectionDAG &DAG, MulExpansionKind Kind,\n                 SDValue LL = SDValue(), SDValue LH = SDValue(),\n                 SDValue RL = SDValue(), SDValue RH = SDValue()) const;\n\n  /// Expand funnel shift.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandFunnelShift(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand rotations.\n  /// \\param N Node to expand\n  /// \\param AllowVectorOps expand vector rotate, this should only be performed\n  ///        if the legalization is happening outside of LegalizeVectorOps\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandROT(SDNode *N, bool AllowVectorOps, SDValue &Result,\n                 SelectionDAG &DAG) const;\n\n  /// Expand float(f32) to SINT(i64) conversion\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandFP_TO_SINT(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand float to UINT conversion\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\param Chain output chain after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandFP_TO_UINT(SDNode *N, SDValue &Result, SDValue &Chain,\n                        SelectionDAG &DAG) const;\n\n  /// Expand UINT(i64) to double(f64) conversion\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\param Chain output chain after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandUINT_TO_FP(SDNode *N, SDValue &Result, SDValue &Chain,\n                        SelectionDAG &DAG) const;\n\n  /// Expand fminnum/fmaxnum into fminnum_ieee/fmaxnum_ieee with quieted inputs.\n  SDValue expandFMINNUM_FMAXNUM(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Expand FP_TO_[US]INT_SAT into FP_TO_[US]INT and selects or min/max.\n  /// \\param N Node to expand\n  /// \\returns The expansion result\n  SDValue expandFP_TO_INT_SAT(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Expand CTPOP nodes. Expands vector/scalar CTPOP nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandCTPOP(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand CTLZ/CTLZ_ZERO_UNDEF nodes. Expands vector/scalar CTLZ nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandCTLZ(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand CTTZ/CTTZ_ZERO_UNDEF nodes. Expands vector/scalar CTTZ nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandCTTZ(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand ABS nodes. Expands vector/scalar ABS nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// (ABS x) -> (XOR (ADD x, (SRA x, type_size)), (SRA x, type_size))\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\param IsNegative indicate negated abs\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandABS(SDNode *N, SDValue &Result, SelectionDAG &DAG,\n                 bool IsNegative = false) const;\n\n  /// Expand BSWAP nodes. Expands scalar/vector BSWAP nodes with i16/i32/i64\n  /// scalar types. Returns SDValue() if expand fails.\n  /// \\param N Node to expand\n  /// \\returns The expansion result or SDValue() if it fails.\n  SDValue expandBSWAP(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Expand BITREVERSE nodes. Expands scalar/vector BITREVERSE nodes.\n  /// Returns SDValue() if expand fails.\n  /// \\param N Node to expand\n  /// \\returns The expansion result or SDValue() if it fails.\n  SDValue expandBITREVERSE(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Turn load of vector type into a load of the individual elements.\n  /// \\param LD load to expand\n  /// \\returns BUILD_VECTOR and TokenFactor nodes.\n  std::pair<SDValue, SDValue> scalarizeVectorLoad(LoadSDNode *LD,\n                                                  SelectionDAG &DAG) const;\n\n  // Turn a store of a vector type into stores of the individual elements.\n  /// \\param ST Store with a vector value type\n  /// \\returns TokenFactor of the individual store chains.\n  SDValue scalarizeVectorStore(StoreSDNode *ST, SelectionDAG &DAG) const;\n\n  /// Expands an unaligned load to 2 half-size loads for an integer, and\n  /// possibly more for vectors.\n  std::pair<SDValue, SDValue> expandUnalignedLoad(LoadSDNode *LD,\n                                                  SelectionDAG &DAG) const;\n\n  /// Expands an unaligned store to 2 half-size stores for integer values, and\n  /// possibly more for vectors.\n  SDValue expandUnalignedStore(StoreSDNode *ST, SelectionDAG &DAG) const;\n\n  /// Increments memory address \\p Addr according to the type of the value\n  /// \\p DataVT that should be stored. If the data is stored in compressed\n  /// form, the memory address should be incremented according to the number of\n  /// the stored elements. This number is equal to the number of '1's bits\n  /// in the \\p Mask.\n  /// \\p DataVT is a vector type. \\p Mask is a vector value.\n  /// \\p DataVT and \\p Mask have the same number of vector elements.\n  SDValue IncrementMemoryAddress(SDValue Addr, SDValue Mask, const SDLoc &DL,\n                                 EVT DataVT, SelectionDAG &DAG,\n                                 bool IsCompressedMemory) const;\n\n  /// Get a pointer to vector element \\p Idx located in memory for a vector of\n  /// type \\p VecVT starting at a base address of \\p VecPtr. If \\p Idx is out of\n  /// bounds the returned pointer is unspecified, but will be within the vector\n  /// bounds.\n  SDValue getVectorElementPointer(SelectionDAG &DAG, SDValue VecPtr, EVT VecVT,\n                                  SDValue Index) const;\n\n  /// Method for building the DAG expansion of ISD::[US][MIN|MAX]. This\n  /// method accepts integers as its arguments.\n  SDValue expandIntMINMAX(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US][ADD|SUB]SAT. This\n  /// method accepts integers as its arguments.\n  SDValue expandAddSubSat(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US]SHLSAT. This\n  /// method accepts integers as its arguments.\n  SDValue expandShlSat(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[U|S]MULFIX[SAT]. This\n  /// method accepts integers as its arguments.\n  SDValue expandFixedPointMul(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US]DIVFIX[SAT]. This\n  /// method accepts integers as its arguments.\n  /// Note: This method may fail if the division could not be performed\n  /// within the type. Clients must retry with a wider type if this happens.\n  SDValue expandFixedPointDiv(unsigned Opcode, const SDLoc &dl,\n                              SDValue LHS, SDValue RHS,\n                              unsigned Scale, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::U(ADD|SUB)O. Expansion\n  /// always suceeds and populates the Result and Overflow arguments.\n  void expandUADDSUBO(SDNode *Node, SDValue &Result, SDValue &Overflow,\n                      SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::S(ADD|SUB)O. Expansion\n  /// always suceeds and populates the Result and Overflow arguments.\n  void expandSADDSUBO(SDNode *Node, SDValue &Result, SDValue &Overflow,\n                      SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US]MULO. Returns whether\n  /// expansion was successful and populates the Result and Overflow arguments.\n  bool expandMULO(SDNode *Node, SDValue &Result, SDValue &Overflow,\n                  SelectionDAG &DAG) const;\n\n  /// Expand a VECREDUCE_* into an explicit calculation. If Count is specified,\n  /// only the first Count elements of the vector are used.\n  SDValue expandVecReduce(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Expand a VECREDUCE_SEQ_* into an explicit ordered calculation.\n  SDValue expandVecReduceSeq(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Expand an SREM or UREM using SDIV/UDIV or SDIVREM/UDIVREM, if legal.\n  /// Returns true if the expansion was successful.\n  bool expandREM(SDNode *Node, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::VECTOR_SPLICE. This\n  /// method accepts vectors as its arguments.\n  SDValue expandVectorSplice(SDNode *Node, SelectionDAG &DAG) const;\n\n  //===--------------------------------------------------------------------===//\n  // Instruction Emitting Hooks\n  //\n\n  /// This method should be implemented by targets that mark instructions with\n  /// the 'usesCustomInserter' flag.  These instructions are special in various\n  /// ways, which require special support to insert.  The specified MachineInstr\n  /// is created but not inserted into any basic blocks, and this method is\n  /// called to expand it into a sequence of instructions, potentially also\n  /// creating new basic blocks and control flow.\n  /// As long as the returned basic block is different (i.e., we created a new\n  /// one), the custom inserter is free to modify the rest of \\p MBB.\n  virtual MachineBasicBlock *\n  EmitInstrWithCustomInserter(MachineInstr &MI, MachineBasicBlock *MBB) const;\n\n  /// This method should be implemented by targets that mark instructions with\n  /// the 'hasPostISelHook' flag. These instructions must be adjusted after\n  /// instruction selection by target hooks.  e.g. To fill in optional defs for\n  /// ARM 's' setting instructions.\n  virtual void AdjustInstrPostInstrSelection(MachineInstr &MI,\n                                             SDNode *Node) const;\n\n  /// If this function returns true, SelectionDAGBuilder emits a\n  /// LOAD_STACK_GUARD node when it is lowering Intrinsic::stackprotector.\n  virtual bool useLoadStackGuardNode() const {\n    return false;\n  }\n\n  virtual SDValue emitStackGuardXorFP(SelectionDAG &DAG, SDValue Val,\n                                      const SDLoc &DL) const {\n    llvm_unreachable(\"not implemented for this target\");\n  }\n\n  /// Lower TLS global address SDNode for target independent emulated TLS model.\n  virtual SDValue LowerToTLSEmulatedModel(const GlobalAddressSDNode *GA,\n                                          SelectionDAG &DAG) const;\n\n  /// Expands target specific indirect branch for the case of JumpTable\n  /// expanasion.\n  virtual SDValue expandIndirectJTBranch(const SDLoc& dl, SDValue Value, SDValue Addr,\n                                         SelectionDAG &DAG) const {\n    return DAG.getNode(ISD::BRIND, dl, MVT::Other, Value, Addr);\n  }\n\n  // seteq(x, 0) -> truncate(srl(ctlz(zext(x)), log2(#bits)))\n  // If we're comparing for equality to zero and isCtlzFast is true, expose the\n  // fact that this can be implemented as a ctlz/srl pair, so that the dag\n  // combiner can fold the new nodes.\n  SDValue lowerCmpEqZeroToCtlzSrl(SDValue Op, SelectionDAG &DAG) const;\n\n  /// Give targets the chance to reduce the number of distinct addresing modes.\n  ISD::MemIndexType getCanonicalIndexType(ISD::MemIndexType IndexType,\n                                          EVT MemVT, SDValue Offsets) const;\n\nprivate:\n  SDValue foldSetCCWithAnd(EVT VT, SDValue N0, SDValue N1, ISD::CondCode Cond,\n                           const SDLoc &DL, DAGCombinerInfo &DCI) const;\n  SDValue foldSetCCWithBinOp(EVT VT, SDValue N0, SDValue N1, ISD::CondCode Cond,\n                             const SDLoc &DL, DAGCombinerInfo &DCI) const;\n\n  SDValue optimizeSetCCOfSignedTruncationCheck(EVT SCCVT, SDValue N0,\n                                               SDValue N1, ISD::CondCode Cond,\n                                               DAGCombinerInfo &DCI,\n                                               const SDLoc &DL) const;\n\n  // (X & (C l>>/<< Y)) ==/!= 0  -->  ((X <</l>> Y) & C) ==/!= 0\n  SDValue optimizeSetCCByHoistingAndByConstFromLogicalShift(\n      EVT SCCVT, SDValue N0, SDValue N1C, ISD::CondCode Cond,\n      DAGCombinerInfo &DCI, const SDLoc &DL) const;\n\n  SDValue prepareUREMEqFold(EVT SETCCVT, SDValue REMNode,\n                            SDValue CompTargetNode, ISD::CondCode Cond,\n                            DAGCombinerInfo &DCI, const SDLoc &DL,\n                            SmallVectorImpl<SDNode *> &Created) const;\n  SDValue buildUREMEqFold(EVT SETCCVT, SDValue REMNode, SDValue CompTargetNode,\n                          ISD::CondCode Cond, DAGCombinerInfo &DCI,\n                          const SDLoc &DL) const;\n\n  SDValue prepareSREMEqFold(EVT SETCCVT, SDValue REMNode,\n                            SDValue CompTargetNode, ISD::CondCode Cond,\n                            DAGCombinerInfo &DCI, const SDLoc &DL,\n                            SmallVectorImpl<SDNode *> &Created) const;\n  SDValue buildSREMEqFold(EVT SETCCVT, SDValue REMNode, SDValue CompTargetNode,\n                          ISD::CondCode Cond, DAGCombinerInfo &DCI,\n                          const SDLoc &DL) const;\n};\n\n/// Given an LLVM IR type and return type attributes, compute the return value\n/// EVTs and flags, and optionally also the offsets, if the return value is\n/// being lowered to memory.\nvoid GetReturnInfo(CallingConv::ID CC, Type *ReturnType, AttributeList attr,\n                   SmallVectorImpl<ISD::OutputArg> &Outs,\n                   const TargetLowering &TLI, const DataLayout &DL);\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_TARGETLOWERING_H\n"}, "60": {"id": 60, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TileShapeInfo.h", "content": "//===- llvm/CodeGen/TileShapeInfo.h - ---------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file Shape utility for AMX.\n/// AMX hardware requires to config the shape of tile data register before use.\n/// The 2D shape includes row and column. In AMX intrinsics interface the shape\n/// is passed as 1st and 2nd parameter and they are lowered as the 1st and 2nd\n/// machine operand of AMX pseudo instructions. ShapeT class is to facilitate\n/// tile config and register allocator. The row and column are machine operand\n/// of AMX pseudo instructions.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_TILESHAPEINFO_H\n#define LLVM_CODEGEN_TILESHAPEINFO_H\n\n#include \"llvm/ADT/DenseMapInfo.h\"\n#include \"llvm/CodeGen/MachineInstr.h\"\n#include \"llvm/CodeGen/MachineOperand.h\"\n#include \"llvm/CodeGen/MachineRegisterInfo.h\"\n#include \"llvm/CodeGen/Register.h\"\n#include <utility>\n\nnamespace llvm {\n\nclass ShapeT {\npublic:\n  ShapeT(MachineOperand *Row, MachineOperand *Col,\n         const MachineRegisterInfo *MRI = nullptr)\n      : Row(Row), Col(Col) {\n    if (MRI)\n      deduceImm(MRI);\n  }\n  ShapeT()\n      : Row(nullptr), Col(nullptr), RowImm(InvalidImmShape),\n        ColImm(InvalidImmShape) {}\n  bool operator==(const ShapeT &Shape) {\n    MachineOperand *R = Shape.Row;\n    MachineOperand *C = Shape.Col;\n    if (!R || !C)\n      return false;\n    if (!Row || !Col)\n      return false;\n    if (Row->getReg() == R->getReg() && Col->getReg() == C->getReg())\n      return true;\n    if ((RowImm != InvalidImmShape) && (ColImm != InvalidImmShape))\n      return RowImm == Shape.getRowImm() && ColImm == Shape.getColImm();\n    return false;\n  }\n\n  bool operator!=(const ShapeT &Shape) { return !(*this == Shape); }\n\n  MachineOperand *getRow() const { return Row; }\n\n  MachineOperand *getCol() const { return Col; }\n\n  int64_t getRowImm() const { return RowImm; }\n\n  int64_t getColImm() const { return ColImm; }\n\n  bool isValid() { return (Row != nullptr) && (Col != nullptr); }\n\n  void deduceImm(const MachineRegisterInfo *MRI) {\n    // All def must be the same value, otherwise it is invalid MIs.\n    // Find the immediate.\n    // TODO copy propagation.\n    auto GetImm = [&](Register Reg) {\n      int64_t Imm = InvalidImmShape;\n      for (const MachineOperand &DefMO : MRI->def_operands(Reg)) {\n        const auto *MI = DefMO.getParent();\n        if (MI->isMoveImmediate()) {\n          Imm = MI->getOperand(1).getImm();\n          break;\n        }\n      }\n      return Imm;\n    };\n    RowImm = GetImm(Row->getReg());\n    ColImm = GetImm(Col->getReg());\n  }\n\nprivate:\n  static constexpr int64_t InvalidImmShape = -1;\n  MachineOperand *Row;\n  MachineOperand *Col;\n  int64_t RowImm;\n  int64_t ColImm;\n};\n\n} // namespace llvm\n\n#endif\n"}, "61": {"id": 61, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/ValueTypes.h", "content": "//===- CodeGen/ValueTypes.h - Low-Level Target independ. types --*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the set of low-level target independent types which various\n// values in the code generator are.  This allows the target specific behavior\n// of instructions to be described to target independent passes.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_VALUETYPES_H\n#define LLVM_CODEGEN_VALUETYPES_H\n\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include \"llvm/Support/MathExtras.h\"\n#include \"llvm/Support/TypeSize.h\"\n#include \"llvm/Support/WithColor.h\"\n#include <cassert>\n#include <cstdint>\n#include <string>\n\nnamespace llvm {\n\n  class LLVMContext;\n  class Type;\n\n  /// Extended Value Type. Capable of holding value types which are not native\n  /// for any processor (such as the i12345 type), as well as the types an MVT\n  /// can represent.\n  struct EVT {\n  private:\n    MVT V = MVT::INVALID_SIMPLE_VALUE_TYPE;\n    Type *LLVMTy = nullptr;\n\n  public:\n    constexpr EVT() = default;\n    constexpr EVT(MVT::SimpleValueType SVT) : V(SVT) {}\n    constexpr EVT(MVT S) : V(S) {}\n\n    bool operator==(EVT VT) const {\n      return !(*this != VT);\n    }\n    bool operator!=(EVT VT) const {\n      if (V.SimpleTy != VT.V.SimpleTy)\n        return true;\n      if (V.SimpleTy == MVT::INVALID_SIMPLE_VALUE_TYPE)\n        return LLVMTy != VT.LLVMTy;\n      return false;\n    }\n\n    /// Returns the EVT that represents a floating-point type with the given\n    /// number of bits. There are two floating-point types with 128 bits - this\n    /// returns f128 rather than ppcf128.\n    static EVT getFloatingPointVT(unsigned BitWidth) {\n      return MVT::getFloatingPointVT(BitWidth);\n    }\n\n    /// Returns the EVT that represents an integer with the given number of\n    /// bits.\n    static EVT getIntegerVT(LLVMContext &Context, unsigned BitWidth) {\n      MVT M = MVT::getIntegerVT(BitWidth);\n      if (M.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE)\n        return M;\n      return getExtendedIntegerVT(Context, BitWidth);\n    }\n\n    /// Returns the EVT that represents a vector NumElements in length, where\n    /// each element is of type VT.\n    static EVT getVectorVT(LLVMContext &Context, EVT VT, unsigned NumElements,\n                           bool IsScalable = false) {\n      MVT M = MVT::getVectorVT(VT.V, NumElements, IsScalable);\n      if (M.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE)\n        return M;\n      return getExtendedVectorVT(Context, VT, NumElements, IsScalable);\n    }\n\n    /// Returns the EVT that represents a vector EC.Min elements in length,\n    /// where each element is of type VT.\n    static EVT getVectorVT(LLVMContext &Context, EVT VT, ElementCount EC) {\n      MVT M = MVT::getVectorVT(VT.V, EC);\n      if (M.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE)\n        return M;\n      return getExtendedVectorVT(Context, VT, EC);\n    }\n\n    /// Return a vector with the same number of elements as this vector, but\n    /// with the element type converted to an integer type with the same\n    /// bitwidth.\n    EVT changeVectorElementTypeToInteger() const {\n      if (isSimple())\n        return getSimpleVT().changeVectorElementTypeToInteger();\n      return changeExtendedVectorElementTypeToInteger();\n    }\n\n    /// Return a VT for a vector type whose attributes match ourselves\n    /// with the exception of the element type that is chosen by the caller.\n    EVT changeVectorElementType(EVT EltVT) const {\n      if (isSimple()) {\n        assert(EltVT.isSimple() &&\n               \"Can't change simple vector VT to have extended element VT\");\n        return getSimpleVT().changeVectorElementType(EltVT.getSimpleVT());\n      }\n      return changeExtendedVectorElementType(EltVT);\n    }\n\n    /// Return the type converted to an equivalently sized integer or vector\n    /// with integer element type. Similar to changeVectorElementTypeToInteger,\n    /// but also handles scalars.\n    EVT changeTypeToInteger() {\n      if (isVector())\n        return changeVectorElementTypeToInteger();\n\n      if (isSimple())\n        return getSimpleVT().changeTypeToInteger();\n      return changeExtendedTypeToInteger();\n    }\n\n    /// Test if the given EVT is simple (as opposed to being extended).\n    bool isSimple() const {\n      return V.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE;\n    }\n\n    /// Test if the given EVT is extended (as opposed to being simple).\n    bool isExtended() const {\n      return !isSimple();\n    }\n\n    /// Return true if this is a FP or a vector FP type.\n    bool isFloatingPoint() const {\n      return isSimple() ? V.isFloatingPoint() : isExtendedFloatingPoint();\n    }\n\n    /// Return true if this is an integer or a vector integer type.\n    bool isInteger() const {\n      return isSimple() ? V.isInteger() : isExtendedInteger();\n    }\n\n    /// Return true if this is an integer, but not a vector.\n    bool isScalarInteger() const {\n      return isSimple() ? V.isScalarInteger() : isExtendedScalarInteger();\n    }\n\n    /// Return true if this is a vector value type.\n    bool isVector() const {\n      return isSimple() ? V.isVector() : isExtendedVector();\n    }\n\n    /// Return true if this is a vector type where the runtime\n    /// length is machine dependent\n    bool isScalableVector() const {\n      return isSimple() ? V.isScalableVector() : isExtendedScalableVector();\n    }\n\n    bool isFixedLengthVector() const {\n      return isSimple() ? V.isFixedLengthVector()\n                        : isExtendedFixedLengthVector();\n    }\n\n    /// Return true if this is a 16-bit vector type.\n    bool is16BitVector() const {\n      return isSimple() ? V.is16BitVector() : isExtended16BitVector();\n    }\n\n    /// Return true if this is a 32-bit vector type.\n    bool is32BitVector() const {\n      return isSimple() ? V.is32BitVector() : isExtended32BitVector();\n    }\n\n    /// Return true if this is a 64-bit vector type.\n    bool is64BitVector() const {\n      return isSimple() ? V.is64BitVector() : isExtended64BitVector();\n    }\n\n    /// Return true if this is a 128-bit vector type.\n    bool is128BitVector() const {\n      return isSimple() ? V.is128BitVector() : isExtended128BitVector();\n    }\n\n    /// Return true if this is a 256-bit vector type.\n    bool is256BitVector() const {\n      return isSimple() ? V.is256BitVector() : isExtended256BitVector();\n    }\n\n    /// Return true if this is a 512-bit vector type.\n    bool is512BitVector() const {\n      return isSimple() ? V.is512BitVector() : isExtended512BitVector();\n    }\n\n    /// Return true if this is a 1024-bit vector type.\n    bool is1024BitVector() const {\n      return isSimple() ? V.is1024BitVector() : isExtended1024BitVector();\n    }\n\n    /// Return true if this is a 2048-bit vector type.\n    bool is2048BitVector() const {\n      return isSimple() ? V.is2048BitVector() : isExtended2048BitVector();\n    }\n\n    /// Return true if this is an overloaded type for TableGen.\n    bool isOverloaded() const {\n      return (V==MVT::iAny || V==MVT::fAny || V==MVT::vAny || V==MVT::iPTRAny);\n    }\n\n    /// Return true if the bit size is a multiple of 8.\n    bool isByteSized() const { return getSizeInBits().isKnownMultipleOf(8); }\n\n    /// Return true if the size is a power-of-two number of bytes.\n    bool isRound() const {\n      if (isScalableVector())\n        return false;\n      unsigned BitSize = getSizeInBits();\n      return BitSize >= 8 && !(BitSize & (BitSize - 1));\n    }\n\n    /// Return true if this has the same number of bits as VT.\n    bool bitsEq(EVT VT) const {\n      if (EVT::operator==(VT)) return true;\n      return getSizeInBits() == VT.getSizeInBits();\n    }\n\n    /// Return true if we know at compile time this has more bits than VT.\n    bool knownBitsGT(EVT VT) const {\n      return TypeSize::isKnownGT(getSizeInBits(), VT.getSizeInBits());\n    }\n\n    /// Return true if we know at compile time this has more than or the same\n    /// bits as VT.\n    bool knownBitsGE(EVT VT) const {\n      return TypeSize::isKnownGE(getSizeInBits(), VT.getSizeInBits());\n    }\n\n    /// Return true if we know at compile time this has fewer bits than VT.\n    bool knownBitsLT(EVT VT) const {\n      return TypeSize::isKnownLT(getSizeInBits(), VT.getSizeInBits());\n    }\n\n    /// Return true if we know at compile time this has fewer than or the same\n    /// bits as VT.\n    bool knownBitsLE(EVT VT) const {\n      return TypeSize::isKnownLE(getSizeInBits(), VT.getSizeInBits());\n    }\n\n    /// Return true if this has more bits than VT.\n    bool bitsGT(EVT VT) const {\n      if (EVT::operator==(VT)) return false;\n      assert(isScalableVector() == VT.isScalableVector() &&\n             \"Comparison between scalable and fixed types\");\n      return knownBitsGT(VT);\n    }\n\n    /// Return true if this has no less bits than VT.\n    bool bitsGE(EVT VT) const {\n      if (EVT::operator==(VT)) return true;\n      assert(isScalableVector() == VT.isScalableVector() &&\n             \"Comparison between scalable and fixed types\");\n      return knownBitsGE(VT);\n    }\n\n    /// Return true if this has less bits than VT.\n    bool bitsLT(EVT VT) const {\n      if (EVT::operator==(VT)) return false;\n      assert(isScalableVector() == VT.isScalableVector() &&\n             \"Comparison between scalable and fixed types\");\n      return knownBitsLT(VT);\n    }\n\n    /// Return true if this has no more bits than VT.\n    bool bitsLE(EVT VT) const {\n      if (EVT::operator==(VT)) return true;\n      assert(isScalableVector() == VT.isScalableVector() &&\n             \"Comparison between scalable and fixed types\");\n      return knownBitsLE(VT);\n    }\n\n    /// Return the SimpleValueType held in the specified simple EVT.\n    MVT getSimpleVT() const {\n      assert(isSimple() && \"Expected a SimpleValueType!\");\n      return V;\n    }\n\n    /// If this is a vector type, return the element type, otherwise return\n    /// this.\n    EVT getScalarType() const {\n      return isVector() ? getVectorElementType() : *this;\n    }\n\n    /// Given a vector type, return the type of each element.\n    EVT getVectorElementType() const {\n      assert(isVector() && \"Invalid vector type!\");\n      if (isSimple())\n        return V.getVectorElementType();\n      return getExtendedVectorElementType();\n    }\n\n    /// Given a vector type, return the number of elements it contains.\n    unsigned getVectorNumElements() const {\n#ifdef STRICT_FIXED_SIZE_VECTORS\n      assert(isFixedLengthVector() && \"Invalid vector type!\");\n#else\n      assert(isVector() && \"Invalid vector type!\");\n      if (isScalableVector())\n        WithColor::warning()\n            << \"Possible incorrect use of EVT::getVectorNumElements() for \"\n               \"scalable vector. Scalable flag may be dropped, use \"\n               \"EVT::getVectorElementCount() instead\\n\";\n#endif\n      if (isSimple())\n        return V.getVectorNumElements();\n      return getExtendedVectorNumElements();\n    }\n\n    // Given a (possibly scalable) vector type, return the ElementCount\n    ElementCount getVectorElementCount() const {\n      assert((isVector()) && \"Invalid vector type!\");\n      if (isSimple())\n        return V.getVectorElementCount();\n\n      return getExtendedVectorElementCount();\n    }\n\n    /// Given a vector type, return the minimum number of elements it contains.\n    unsigned getVectorMinNumElements() const {\n      return getVectorElementCount().getKnownMinValue();\n    }\n\n    /// Return the size of the specified value type in bits.\n    ///\n    /// If the value type is a scalable vector type, the scalable property will\n    /// be set and the runtime size will be a positive integer multiple of the\n    /// base size.\n    TypeSize getSizeInBits() const {\n      if (isSimple())\n        return V.getSizeInBits();\n      return getExtendedSizeInBits();\n    }\n\n    /// Return the size of the specified fixed width value type in bits. The\n    /// function will assert if the type is scalable.\n    uint64_t getFixedSizeInBits() const {\n      return getSizeInBits().getFixedSize();\n    }\n\n    uint64_t getScalarSizeInBits() const {\n      return getScalarType().getSizeInBits().getFixedSize();\n    }\n\n    /// Return the number of bytes overwritten by a store of the specified value\n    /// type.\n    ///\n    /// If the value type is a scalable vector type, the scalable property will\n    /// be set and the runtime size will be a positive integer multiple of the\n    /// base size.\n    TypeSize getStoreSize() const {\n      TypeSize BaseSize = getSizeInBits();\n      return {(BaseSize.getKnownMinSize() + 7) / 8, BaseSize.isScalable()};\n    }\n\n    /// Return the number of bits overwritten by a store of the specified value\n    /// type.\n    ///\n    /// If the value type is a scalable vector type, the scalable property will\n    /// be set and the runtime size will be a positive integer multiple of the\n    /// base size.\n    TypeSize getStoreSizeInBits() const {\n      return getStoreSize() * 8;\n    }\n\n    /// Rounds the bit-width of the given integer EVT up to the nearest power of\n    /// two (and at least to eight), and returns the integer EVT with that\n    /// number of bits.\n    EVT getRoundIntegerType(LLVMContext &Context) const {\n      assert(isInteger() && !isVector() && \"Invalid integer type!\");\n      unsigned BitWidth = getSizeInBits();\n      if (BitWidth <= 8)\n        return EVT(MVT::i8);\n      return getIntegerVT(Context, 1 << Log2_32_Ceil(BitWidth));\n    }\n\n    /// Finds the smallest simple value type that is greater than or equal to\n    /// half the width of this EVT. If no simple value type can be found, an\n    /// extended integer value type of half the size (rounded up) is returned.\n    EVT getHalfSizedIntegerVT(LLVMContext &Context) const {\n      assert(isInteger() && !isVector() && \"Invalid integer type!\");\n      unsigned EVTSize = getSizeInBits();\n      for (unsigned IntVT = MVT::FIRST_INTEGER_VALUETYPE;\n          IntVT <= MVT::LAST_INTEGER_VALUETYPE; ++IntVT) {\n        EVT HalfVT = EVT((MVT::SimpleValueType)IntVT);\n        if (HalfVT.getSizeInBits() * 2 >= EVTSize)\n          return HalfVT;\n      }\n      return getIntegerVT(Context, (EVTSize + 1) / 2);\n    }\n\n    /// Return a VT for an integer vector type with the size of the\n    /// elements doubled. The typed returned may be an extended type.\n    EVT widenIntegerVectorElementType(LLVMContext &Context) const {\n      EVT EltVT = getVectorElementType();\n      EltVT = EVT::getIntegerVT(Context, 2 * EltVT.getSizeInBits());\n      return EVT::getVectorVT(Context, EltVT, getVectorElementCount());\n    }\n\n    // Return a VT for a vector type with the same element type but\n    // half the number of elements. The type returned may be an\n    // extended type.\n    EVT getHalfNumVectorElementsVT(LLVMContext &Context) const {\n      EVT EltVT = getVectorElementType();\n      auto EltCnt = getVectorElementCount();\n      assert(EltCnt.isKnownEven() && \"Splitting vector, but not in half!\");\n      return EVT::getVectorVT(Context, EltVT, EltCnt.divideCoefficientBy(2));\n    }\n\n    // Return a VT for a vector type with the same element type but\n    // double the number of elements. The type returned may be an\n    // extended type.\n    EVT getDoubleNumVectorElementsVT(LLVMContext &Context) const {\n      EVT EltVT = getVectorElementType();\n      auto EltCnt = getVectorElementCount();\n      return EVT::getVectorVT(Context, EltVT, EltCnt * 2);\n    }\n\n    /// Returns true if the given vector is a power of 2.\n    bool isPow2VectorType() const {\n      unsigned NElts = getVectorMinNumElements();\n      return !(NElts & (NElts - 1));\n    }\n\n    /// Widens the length of the given vector EVT up to the nearest power of 2\n    /// and returns that type.\n    EVT getPow2VectorType(LLVMContext &Context) const {\n      if (!isPow2VectorType()) {\n        ElementCount NElts = getVectorElementCount();\n        unsigned NewMinCount = 1 << Log2_32_Ceil(NElts.getKnownMinValue());\n        NElts = ElementCount::get(NewMinCount, NElts.isScalable());\n        return EVT::getVectorVT(Context, getVectorElementType(), NElts);\n      }\n      else {\n        return *this;\n      }\n    }\n\n    /// This function returns value type as a string, e.g. \"i32\".\n    std::string getEVTString() const;\n\n    /// This method returns an LLVM type corresponding to the specified EVT.\n    /// For integer types, this returns an unsigned type. Note that this will\n    /// abort for types that cannot be represented.\n    Type *getTypeForEVT(LLVMContext &Context) const;\n\n    /// Return the value type corresponding to the specified type.\n    /// This returns all pointers as iPTR.  If HandleUnknown is true, unknown\n    /// types are returned as Other, otherwise they are invalid.\n    static EVT getEVT(Type *Ty, bool HandleUnknown = false);\n\n    intptr_t getRawBits() const {\n      if (isSimple())\n        return V.SimpleTy;\n      else\n        return (intptr_t)(LLVMTy);\n    }\n\n    /// A meaningless but well-behaved order, useful for constructing\n    /// containers.\n    struct compareRawBits {\n      bool operator()(EVT L, EVT R) const {\n        if (L.V.SimpleTy == R.V.SimpleTy)\n          return L.LLVMTy < R.LLVMTy;\n        else\n          return L.V.SimpleTy < R.V.SimpleTy;\n      }\n    };\n\n  private:\n    // Methods for handling the Extended-type case in functions above.\n    // These are all out-of-line to prevent users of this header file\n    // from having a dependency on Type.h.\n    EVT changeExtendedTypeToInteger() const;\n    EVT changeExtendedVectorElementType(EVT EltVT) const;\n    EVT changeExtendedVectorElementTypeToInteger() const;\n    static EVT getExtendedIntegerVT(LLVMContext &C, unsigned BitWidth);\n    static EVT getExtendedVectorVT(LLVMContext &C, EVT VT, unsigned NumElements,\n                                   bool IsScalable);\n    static EVT getExtendedVectorVT(LLVMContext &Context, EVT VT,\n                                   ElementCount EC);\n    bool isExtendedFloatingPoint() const LLVM_READONLY;\n    bool isExtendedInteger() const LLVM_READONLY;\n    bool isExtendedScalarInteger() const LLVM_READONLY;\n    bool isExtendedVector() const LLVM_READONLY;\n    bool isExtended16BitVector() const LLVM_READONLY;\n    bool isExtended32BitVector() const LLVM_READONLY;\n    bool isExtended64BitVector() const LLVM_READONLY;\n    bool isExtended128BitVector() const LLVM_READONLY;\n    bool isExtended256BitVector() const LLVM_READONLY;\n    bool isExtended512BitVector() const LLVM_READONLY;\n    bool isExtended1024BitVector() const LLVM_READONLY;\n    bool isExtended2048BitVector() const LLVM_READONLY;\n    bool isExtendedFixedLengthVector() const LLVM_READONLY;\n    bool isExtendedScalableVector() const LLVM_READONLY;\n    EVT getExtendedVectorElementType() const;\n    unsigned getExtendedVectorNumElements() const LLVM_READONLY;\n    ElementCount getExtendedVectorElementCount() const LLVM_READONLY;\n    TypeSize getExtendedSizeInBits() const LLVM_READONLY;\n  };\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_VALUETYPES_H\n"}, "62": {"id": 62, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/VirtRegMap.h", "content": "//===- llvm/CodeGen/VirtRegMap.h - Virtual Register Map ---------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file implements a virtual register map. This maps virtual registers to\n// physical registers and virtual registers to stack slots. It is created and\n// updated by a register allocator and then used by a machine code rewriter that\n// adds spill code and rewrites virtual into physical register references.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_VIRTREGMAP_H\n#define LLVM_CODEGEN_VIRTREGMAP_H\n\n#include \"llvm/ADT/IndexedMap.h\"\n#include \"llvm/CodeGen/MachineFunctionPass.h\"\n#include \"llvm/CodeGen/TargetRegisterInfo.h\"\n#include \"llvm/CodeGen/TileShapeInfo.h\"\n#include \"llvm/Pass.h\"\n#include <cassert>\n\nnamespace llvm {\n\nclass MachineFunction;\nclass MachineRegisterInfo;\nclass raw_ostream;\nclass TargetInstrInfo;\n\n  class VirtRegMap : public MachineFunctionPass {\n  public:\n    enum {\n      NO_PHYS_REG = 0,\n      NO_STACK_SLOT = (1L << 30)-1,\n      MAX_STACK_SLOT = (1L << 18)-1\n    };\n\n  private:\n    MachineRegisterInfo *MRI;\n    const TargetInstrInfo *TII;\n    const TargetRegisterInfo *TRI;\n    MachineFunction *MF;\n\n    /// Virt2PhysMap - This is a virtual to physical register\n    /// mapping. Each virtual register is required to have an entry in\n    /// it; even spilled virtual registers (the register mapped to a\n    /// spilled register is the temporary used to load it from the\n    /// stack).\n    IndexedMap<Register, VirtReg2IndexFunctor> Virt2PhysMap;\n\n    /// Virt2StackSlotMap - This is virtual register to stack slot\n    /// mapping. Each spilled virtual register has an entry in it\n    /// which corresponds to the stack slot this register is spilled\n    /// at.\n    IndexedMap<int, VirtReg2IndexFunctor> Virt2StackSlotMap;\n\n    /// Virt2SplitMap - This is virtual register to splitted virtual register\n    /// mapping.\n    IndexedMap<unsigned, VirtReg2IndexFunctor> Virt2SplitMap;\n\n    /// Virt2ShapeMap - For X86 AMX register whose register is bound shape\n    /// information.\n    DenseMap<unsigned, ShapeT> Virt2ShapeMap;\n\n    /// createSpillSlot - Allocate a spill slot for RC from MFI.\n    unsigned createSpillSlot(const TargetRegisterClass *RC);\n\n  public:\n    static char ID;\n\n    VirtRegMap()\n        : MachineFunctionPass(ID), MRI(nullptr), TII(nullptr), TRI(nullptr),\n          MF(nullptr), Virt2PhysMap(NO_PHYS_REG),\n          Virt2StackSlotMap(NO_STACK_SLOT), Virt2SplitMap(0) {}\n    VirtRegMap(const VirtRegMap &) = delete;\n    VirtRegMap &operator=(const VirtRegMap &) = delete;\n\n    bool runOnMachineFunction(MachineFunction &MF) override;\n\n    void getAnalysisUsage(AnalysisUsage &AU) const override {\n      AU.setPreservesAll();\n      MachineFunctionPass::getAnalysisUsage(AU);\n    }\n\n    MachineFunction &getMachineFunction() const {\n      assert(MF && \"getMachineFunction called before runOnMachineFunction\");\n      return *MF;\n    }\n\n    MachineRegisterInfo &getRegInfo() const { return *MRI; }\n    const TargetRegisterInfo &getTargetRegInfo() const { return *TRI; }\n\n    void grow();\n\n    /// returns true if the specified virtual register is\n    /// mapped to a physical register\n    bool hasPhys(Register virtReg) const {\n      return getPhys(virtReg) != NO_PHYS_REG;\n    }\n\n    /// returns the physical register mapped to the specified\n    /// virtual register\n    MCRegister getPhys(Register virtReg) const {\n      assert(virtReg.isVirtual());\n      return MCRegister::from(Virt2PhysMap[virtReg.id()]);\n    }\n\n    /// creates a mapping for the specified virtual register to\n    /// the specified physical register\n    void assignVirt2Phys(Register virtReg, MCPhysReg physReg);\n\n    bool isShapeMapEmpty() const { return Virt2ShapeMap.empty(); }\n\n    bool hasShape(Register virtReg) const {\n      return getShape(virtReg).isValid();\n    }\n\n    ShapeT getShape(Register virtReg) const {\n      assert(virtReg.isVirtual());\n      return Virt2ShapeMap.lookup(virtReg);\n    }\n\n    void assignVirt2Shape(Register virtReg, ShapeT shape) {\n      Virt2ShapeMap[virtReg.id()] = shape;\n    }\n\n    /// clears the specified virtual register's, physical\n    /// register mapping\n    void clearVirt(Register virtReg) {\n      assert(virtReg.isVirtual());\n      assert(Virt2PhysMap[virtReg.id()] != NO_PHYS_REG &&\n             \"attempt to clear a not assigned virtual register\");\n      Virt2PhysMap[virtReg.id()] = NO_PHYS_REG;\n    }\n\n    /// clears all virtual to physical register mappings\n    void clearAllVirt() {\n      Virt2PhysMap.clear();\n      grow();\n    }\n\n    /// returns true if VirtReg is assigned to its preferred physreg.\n    bool hasPreferredPhys(Register VirtReg) const;\n\n    /// returns true if VirtReg has a known preferred register.\n    /// This returns false if VirtReg has a preference that is a virtual\n    /// register that hasn't been assigned yet.\n    bool hasKnownPreference(Register VirtReg) const;\n\n    /// records virtReg is a split live interval from SReg.\n    void setIsSplitFromReg(Register virtReg, Register SReg) {\n      Virt2SplitMap[virtReg.id()] = SReg;\n      if (hasShape(SReg)) {\n        Virt2ShapeMap[virtReg.id()] = getShape(SReg);\n      }\n    }\n\n    /// returns the live interval virtReg is split from.\n    Register getPreSplitReg(Register virtReg) const {\n      return Virt2SplitMap[virtReg.id()];\n    }\n\n    /// getOriginal - Return the original virtual register that VirtReg descends\n    /// from through splitting.\n    /// A register that was not created by splitting is its own original.\n    /// This operation is idempotent.\n    Register getOriginal(Register VirtReg) const {\n      Register Orig = getPreSplitReg(VirtReg);\n      return Orig ? Orig : VirtReg;\n    }\n\n    /// returns true if the specified virtual register is not\n    /// mapped to a stack slot or rematerialized.\n    bool isAssignedReg(Register virtReg) const {\n      if (getStackSlot(virtReg) == NO_STACK_SLOT)\n        return true;\n      // Split register can be assigned a physical register as well as a\n      // stack slot or remat id.\n      return (Virt2SplitMap[virtReg.id()] &&\n              Virt2PhysMap[virtReg.id()] != NO_PHYS_REG);\n    }\n\n    /// returns the stack slot mapped to the specified virtual\n    /// register\n    int getStackSlot(Register virtReg) const {\n      assert(virtReg.isVirtual());\n      return Virt2StackSlotMap[virtReg.id()];\n    }\n\n    /// create a mapping for the specifed virtual register to\n    /// the next available stack slot\n    int assignVirt2StackSlot(Register virtReg);\n\n    /// create a mapping for the specified virtual register to\n    /// the specified stack slot\n    void assignVirt2StackSlot(Register virtReg, int SS);\n\n    void print(raw_ostream &OS, const Module* M = nullptr) const override;\n    void dump() const;\n  };\n\n  inline raw_ostream &operator<<(raw_ostream &OS, const VirtRegMap &VRM) {\n    VRM.print(OS);\n    return OS;\n  }\n\n} // end llvm namespace\n\n#endif // LLVM_CODEGEN_VIRTREGMAP_H\n"}, "105": {"id": 105, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/FormattedStream.h", "content": "//===-- llvm/Support/FormattedStream.h - Formatted streams ------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains raw_ostream implementations for streams to do\n// things like pretty-print comments.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_SUPPORT_FORMATTEDSTREAM_H\n#define LLVM_SUPPORT_FORMATTEDSTREAM_H\n\n#include \"llvm/ADT/SmallString.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include <utility>\n\nnamespace llvm {\n\n/// formatted_raw_ostream - A raw_ostream that wraps another one and keeps track\n/// of line and column position, allowing padding out to specific column\n/// boundaries and querying the number of lines written to the stream. This\n/// assumes that the contents of the stream is valid UTF-8 encoded text. This\n/// doesn't attempt to handle everything Unicode can do (combining characters,\n/// right-to-left markers, etc), but should cover the cases likely to appear in\n/// source code or diagnostic messages.\nclass formatted_raw_ostream : public raw_ostream {\n  /// TheStream - The real stream we output to. We set it to be\n  /// unbuffered, since we're already doing our own buffering.\n  ///\n  raw_ostream *TheStream;\n\n  /// Position - The current output column and line of the data that's\n  /// been flushed and the portion of the buffer that's been\n  /// scanned.  The line and column scheme is zero-based.\n  ///\n  std::pair<unsigned, unsigned> Position;\n\n  /// Scanned - This points to one past the last character in the\n  /// buffer we've scanned.\n  ///\n  const char *Scanned;\n\n  /// PartialUTF8Char - Either empty or a prefix of a UTF-8 code unit sequence\n  /// for a Unicode scalar value which should be prepended to the buffer for the\n  /// next call to ComputePosition. This is needed when the buffer is flushed\n  /// when it ends part-way through the UTF-8 encoding of a Unicode scalar\n  /// value, so that we can compute the display width of the character once we\n  /// have the rest of it.\n  SmallString<4> PartialUTF8Char;\n\n  void write_impl(const char *Ptr, size_t Size) override;\n\n  /// current_pos - Return the current position within the stream,\n  /// not counting the bytes currently in the buffer.\n  uint64_t current_pos() const override {\n    // Our current position in the stream is all the contents which have been\n    // written to the underlying stream (*not* the current position of the\n    // underlying stream).\n    return TheStream->tell();\n  }\n\n  /// ComputePosition - Examine the given output buffer and figure out the new\n  /// position after output. This is safe to call multiple times on the same\n  /// buffer, as it records the most recently scanned character and resumes from\n  /// there when the buffer has not been flushed.\n  void ComputePosition(const char *Ptr, size_t size);\n\n  /// UpdatePosition - scan the characters in [Ptr, Ptr+Size), and update the\n  /// line and column numbers. Unlike ComputePosition, this must be called\n  /// exactly once on each region of the buffer.\n  void UpdatePosition(const char *Ptr, size_t Size);\n\n  void setStream(raw_ostream &Stream) {\n    releaseStream();\n\n    TheStream = &Stream;\n\n    // This formatted_raw_ostream inherits from raw_ostream, so it'll do its\n    // own buffering, and it doesn't need or want TheStream to do another\n    // layer of buffering underneath. Resize the buffer to what TheStream\n    // had been using, and tell TheStream not to do its own buffering.\n    if (size_t BufferSize = TheStream->GetBufferSize())\n      SetBufferSize(BufferSize);\n    else\n      SetUnbuffered();\n    TheStream->SetUnbuffered();\n\n    Scanned = nullptr;\n  }\n\npublic:\n  /// formatted_raw_ostream - Open the specified file for\n  /// writing. If an error occurs, information about the error is\n  /// put into ErrorInfo, and the stream should be immediately\n  /// destroyed; the string will be empty if no error occurred.\n  ///\n  /// As a side effect, the given Stream is set to be Unbuffered.\n  /// This is because formatted_raw_ostream does its own buffering,\n  /// so it doesn't want another layer of buffering to be happening\n  /// underneath it.\n  ///\n  formatted_raw_ostream(raw_ostream &Stream)\n      : TheStream(nullptr), Position(0, 0) {\n    setStream(Stream);\n  }\n  explicit formatted_raw_ostream() : TheStream(nullptr), Position(0, 0) {\n    Scanned = nullptr;\n  }\n\n  ~formatted_raw_ostream() override {\n    flush();\n    releaseStream();\n  }\n\n  /// PadToColumn - Align the output to some column number.  If the current\n  /// column is already equal to or more than NewCol, PadToColumn inserts one\n  /// space.\n  ///\n  /// \\param NewCol - The column to move to.\n  formatted_raw_ostream &PadToColumn(unsigned NewCol);\n\n  unsigned getColumn() {\n    // Calculate current position, taking buffer contents into account.\n    ComputePosition(getBufferStart(), GetNumBytesInBuffer());\n    return Position.first;\n  }\n\n  unsigned getLine() {\n    // Calculate current position, taking buffer contents into account.\n    ComputePosition(getBufferStart(), GetNumBytesInBuffer());\n    return Position.second;\n  }\n\n  raw_ostream &resetColor() override {\n    TheStream->resetColor();\n    return *this;\n  }\n\n  raw_ostream &reverseColor() override {\n    TheStream->reverseColor();\n    return *this;\n  }\n\n  raw_ostream &changeColor(enum Colors Color, bool Bold, bool BG) override {\n    TheStream->changeColor(Color, Bold, BG);\n    return *this;\n  }\n\n  bool is_displayed() const override {\n    return TheStream->is_displayed();\n  }\n\nprivate:\n  void releaseStream() {\n    // Transfer the buffer settings from this raw_ostream back to the underlying\n    // stream.\n    if (!TheStream)\n      return;\n    if (size_t BufferSize = GetBufferSize())\n      TheStream->SetBufferSize(BufferSize);\n    else\n      TheStream->SetUnbuffered();\n  }\n};\n\n/// fouts() - This returns a reference to a formatted_raw_ostream for\n/// standard output.  Use it like: fouts() << \"foo\" << \"bar\";\nformatted_raw_ostream &fouts();\n\n/// ferrs() - This returns a reference to a formatted_raw_ostream for\n/// standard error.  Use it like: ferrs() << \"foo\" << \"bar\";\nformatted_raw_ostream &ferrs();\n\n/// fdbgs() - This returns a reference to a formatted_raw_ostream for\n/// debug output.  Use it like: fdbgs() << \"foo\" << \"bar\";\nformatted_raw_ostream &fdbgs();\n\n} // end llvm namespace\n\n\n#endif\n"}, "109": {"id": 109, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/RecyclingAllocator.h", "content": "//==- llvm/Support/RecyclingAllocator.h - Recycling Allocator ----*- C++ -*-==//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the RecyclingAllocator class.  See the doxygen comment for\n// RecyclingAllocator for more details on the implementation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_SUPPORT_RECYCLINGALLOCATOR_H\n#define LLVM_SUPPORT_RECYCLINGALLOCATOR_H\n\n#include \"llvm/Support/Recycler.h\"\n\nnamespace llvm {\n\n/// RecyclingAllocator - This class wraps an Allocator, adding the\n/// functionality of recycling deleted objects.\n///\ntemplate <class AllocatorType, class T, size_t Size = sizeof(T),\n          size_t Align = alignof(T)>\nclass RecyclingAllocator {\nprivate:\n  /// Base - Implementation details.\n  ///\n  Recycler<T, Size, Align> Base;\n\n  /// Allocator - The wrapped allocator.\n  ///\n  AllocatorType Allocator;\n\npublic:\n  ~RecyclingAllocator() { Base.clear(Allocator); }\n\n  /// Allocate - Return a pointer to storage for an object of type\n  /// SubClass. The storage may be either newly allocated or recycled.\n  ///\n  template<class SubClass>\n  SubClass *Allocate() { return Base.template Allocate<SubClass>(Allocator); }\n\n  T *Allocate() { return Base.Allocate(Allocator); }\n\n  /// Deallocate - Release storage for the pointed-to object. The\n  /// storage will be kept track of and may be recycled.\n  ///\n  template<class SubClass>\n  void Deallocate(SubClass* E) { return Base.Deallocate(Allocator, E); }\n\n  void PrintStats() {\n    Allocator.PrintStats();\n    Base.PrintStats();\n  }\n};\n\n}\n\ntemplate<class AllocatorType, class T, size_t Size, size_t Align>\ninline void *operator new(size_t size,\n                          llvm::RecyclingAllocator<AllocatorType,\n                                                   T, Size, Align> &Allocator) {\n  assert(size <= Size && \"allocation size exceeded\");\n  return Allocator.Allocate();\n}\n\ntemplate<class AllocatorType, class T, size_t Size, size_t Align>\ninline void operator delete(void *E,\n                            llvm::RecyclingAllocator<AllocatorType,\n                                                     T, Size, Align> &A) {\n  A.Deallocate(E);\n}\n\n#endif\n"}, "112": {"id": 112, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/TargetRegistry.h", "content": "//===- Support/TargetRegistry.h - Target Registration -----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file exposes the TargetRegistry interface, which tools can use to access\n// the appropriate target specific classes (TargetMachine, AsmPrinter, etc.)\n// which have been registered.\n//\n// Target specific class implementations should register themselves using the\n// appropriate TargetRegistry interfaces.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_SUPPORT_TARGETREGISTRY_H\n#define LLVM_SUPPORT_TARGETREGISTRY_H\n\n#include \"llvm-c/DisassemblerTypes.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Triple.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Support/CodeGen.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/FormattedStream.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <iterator>\n#include <memory>\n#include <string>\n\nnamespace llvm {\n\nclass AsmPrinter;\nclass MCAsmBackend;\nclass MCAsmInfo;\nclass MCAsmParser;\nclass MCCodeEmitter;\nclass MCContext;\nclass MCDisassembler;\nclass MCInstPrinter;\nclass MCInstrAnalysis;\nclass MCInstrInfo;\nclass MCObjectWriter;\nclass MCRegisterInfo;\nclass MCRelocationInfo;\nclass MCStreamer;\nclass MCSubtargetInfo;\nclass MCSymbolizer;\nclass MCTargetAsmParser;\nclass MCTargetOptions;\nclass MCTargetStreamer;\nclass raw_ostream;\nclass raw_pwrite_stream;\nclass TargetMachine;\nclass TargetOptions;\n\nMCStreamer *createNullStreamer(MCContext &Ctx);\n// Takes ownership of \\p TAB and \\p CE.\n\n/// Create a machine code streamer which will print out assembly for the native\n/// target, suitable for compiling with a native assembler.\n///\n/// \\param InstPrint - If given, the instruction printer to use. If not given\n/// the MCInst representation will be printed.  This method takes ownership of\n/// InstPrint.\n///\n/// \\param CE - If given, a code emitter to use to show the instruction\n/// encoding inline with the assembly. This method takes ownership of \\p CE.\n///\n/// \\param TAB - If given, a target asm backend to use to show the fixup\n/// information in conjunction with encoding information. This method takes\n/// ownership of \\p TAB.\n///\n/// \\param ShowInst - Whether to show the MCInst representation inline with\n/// the assembly.\nMCStreamer *\ncreateAsmStreamer(MCContext &Ctx, std::unique_ptr<formatted_raw_ostream> OS,\n                  bool isVerboseAsm, bool useDwarfDirectory,\n                  MCInstPrinter *InstPrint, std::unique_ptr<MCCodeEmitter> &&CE,\n                  std::unique_ptr<MCAsmBackend> &&TAB, bool ShowInst);\n\nMCStreamer *createELFStreamer(MCContext &Ctx,\n                              std::unique_ptr<MCAsmBackend> &&TAB,\n                              std::unique_ptr<MCObjectWriter> &&OW,\n                              std::unique_ptr<MCCodeEmitter> &&CE,\n                              bool RelaxAll);\nMCStreamer *createMachOStreamer(MCContext &Ctx,\n                                std::unique_ptr<MCAsmBackend> &&TAB,\n                                std::unique_ptr<MCObjectWriter> &&OW,\n                                std::unique_ptr<MCCodeEmitter> &&CE,\n                                bool RelaxAll, bool DWARFMustBeAtTheEnd,\n                                bool LabelSections = false);\nMCStreamer *createWasmStreamer(MCContext &Ctx,\n                               std::unique_ptr<MCAsmBackend> &&TAB,\n                               std::unique_ptr<MCObjectWriter> &&OW,\n                               std::unique_ptr<MCCodeEmitter> &&CE,\n                               bool RelaxAll);\nMCStreamer *createXCOFFStreamer(MCContext &Ctx,\n                                std::unique_ptr<MCAsmBackend> &&TAB,\n                                std::unique_ptr<MCObjectWriter> &&OW,\n                                std::unique_ptr<MCCodeEmitter> &&CE,\n                                bool RelaxAll);\n\nMCRelocationInfo *createMCRelocationInfo(const Triple &TT, MCContext &Ctx);\n\nMCSymbolizer *createMCSymbolizer(const Triple &TT, LLVMOpInfoCallback GetOpInfo,\n                                 LLVMSymbolLookupCallback SymbolLookUp,\n                                 void *DisInfo, MCContext *Ctx,\n                                 std::unique_ptr<MCRelocationInfo> &&RelInfo);\n\n/// Target - Wrapper for Target specific information.\n///\n/// For registration purposes, this is a POD type so that targets can be\n/// registered without the use of static constructors.\n///\n/// Targets should implement a single global instance of this class (which\n/// will be zero initialized), and pass that instance to the TargetRegistry as\n/// part of their initialization.\nclass Target {\npublic:\n  friend struct TargetRegistry;\n\n  using ArchMatchFnTy = bool (*)(Triple::ArchType Arch);\n\n  using MCAsmInfoCtorFnTy = MCAsmInfo *(*)(const MCRegisterInfo &MRI,\n                                           const Triple &TT,\n                                           const MCTargetOptions &Options);\n  using MCInstrInfoCtorFnTy = MCInstrInfo *(*)();\n  using MCInstrAnalysisCtorFnTy = MCInstrAnalysis *(*)(const MCInstrInfo *Info);\n  using MCRegInfoCtorFnTy = MCRegisterInfo *(*)(const Triple &TT);\n  using MCSubtargetInfoCtorFnTy = MCSubtargetInfo *(*)(const Triple &TT,\n                                                       StringRef CPU,\n                                                       StringRef Features);\n  using TargetMachineCtorTy = TargetMachine\n      *(*)(const Target &T, const Triple &TT, StringRef CPU, StringRef Features,\n           const TargetOptions &Options, Optional<Reloc::Model> RM,\n           Optional<CodeModel::Model> CM, CodeGenOpt::Level OL, bool JIT);\n  // If it weren't for layering issues (this header is in llvm/Support, but\n  // depends on MC?) this should take the Streamer by value rather than rvalue\n  // reference.\n  using AsmPrinterCtorTy = AsmPrinter *(*)(\n      TargetMachine &TM, std::unique_ptr<MCStreamer> &&Streamer);\n  using MCAsmBackendCtorTy = MCAsmBackend *(*)(const Target &T,\n                                               const MCSubtargetInfo &STI,\n                                               const MCRegisterInfo &MRI,\n                                               const MCTargetOptions &Options);\n  using MCAsmParserCtorTy = MCTargetAsmParser *(*)(\n      const MCSubtargetInfo &STI, MCAsmParser &P, const MCInstrInfo &MII,\n      const MCTargetOptions &Options);\n  using MCDisassemblerCtorTy = MCDisassembler *(*)(const Target &T,\n                                                   const MCSubtargetInfo &STI,\n                                                   MCContext &Ctx);\n  using MCInstPrinterCtorTy = MCInstPrinter *(*)(const Triple &T,\n                                                 unsigned SyntaxVariant,\n                                                 const MCAsmInfo &MAI,\n                                                 const MCInstrInfo &MII,\n                                                 const MCRegisterInfo &MRI);\n  using MCCodeEmitterCtorTy = MCCodeEmitter *(*)(const MCInstrInfo &II,\n                                                 const MCRegisterInfo &MRI,\n                                                 MCContext &Ctx);\n  using ELFStreamerCtorTy =\n      MCStreamer *(*)(const Triple &T, MCContext &Ctx,\n                      std::unique_ptr<MCAsmBackend> &&TAB,\n                      std::unique_ptr<MCObjectWriter> &&OW,\n                      std::unique_ptr<MCCodeEmitter> &&Emitter, bool RelaxAll);\n  using MachOStreamerCtorTy =\n      MCStreamer *(*)(MCContext &Ctx, std::unique_ptr<MCAsmBackend> &&TAB,\n                      std::unique_ptr<MCObjectWriter> &&OW,\n                      std::unique_ptr<MCCodeEmitter> &&Emitter, bool RelaxAll,\n                      bool DWARFMustBeAtTheEnd);\n  using COFFStreamerCtorTy =\n      MCStreamer *(*)(MCContext &Ctx, std::unique_ptr<MCAsmBackend> &&TAB,\n                      std::unique_ptr<MCObjectWriter> &&OW,\n                      std::unique_ptr<MCCodeEmitter> &&Emitter, bool RelaxAll,\n                      bool IncrementalLinkerCompatible);\n  using WasmStreamerCtorTy =\n      MCStreamer *(*)(const Triple &T, MCContext &Ctx,\n                      std::unique_ptr<MCAsmBackend> &&TAB,\n                      std::unique_ptr<MCObjectWriter> &&OW,\n                      std::unique_ptr<MCCodeEmitter> &&Emitter, bool RelaxAll);\n  using NullTargetStreamerCtorTy = MCTargetStreamer *(*)(MCStreamer &S);\n  using AsmTargetStreamerCtorTy = MCTargetStreamer *(*)(\n      MCStreamer &S, formatted_raw_ostream &OS, MCInstPrinter *InstPrint,\n      bool IsVerboseAsm);\n  using ObjectTargetStreamerCtorTy = MCTargetStreamer *(*)(\n      MCStreamer &S, const MCSubtargetInfo &STI);\n  using MCRelocationInfoCtorTy = MCRelocationInfo *(*)(const Triple &TT,\n                                                       MCContext &Ctx);\n  using MCSymbolizerCtorTy = MCSymbolizer *(*)(\n      const Triple &TT, LLVMOpInfoCallback GetOpInfo,\n      LLVMSymbolLookupCallback SymbolLookUp, void *DisInfo, MCContext *Ctx,\n      std::unique_ptr<MCRelocationInfo> &&RelInfo);\n\nprivate:\n  /// Next - The next registered target in the linked list, maintained by the\n  /// TargetRegistry.\n  Target *Next;\n\n  /// The target function for checking if an architecture is supported.\n  ArchMatchFnTy ArchMatchFn;\n\n  /// Name - The target name.\n  const char *Name;\n\n  /// ShortDesc - A short description of the target.\n  const char *ShortDesc;\n\n  /// BackendName - The name of the backend implementation. This must match the\n  /// name of the 'def X : Target ...' in TableGen.\n  const char *BackendName;\n\n  /// HasJIT - Whether this target supports the JIT.\n  bool HasJIT;\n\n  /// MCAsmInfoCtorFn - Constructor function for this target's MCAsmInfo, if\n  /// registered.\n  MCAsmInfoCtorFnTy MCAsmInfoCtorFn;\n\n  /// MCInstrInfoCtorFn - Constructor function for this target's MCInstrInfo,\n  /// if registered.\n  MCInstrInfoCtorFnTy MCInstrInfoCtorFn;\n\n  /// MCInstrAnalysisCtorFn - Constructor function for this target's\n  /// MCInstrAnalysis, if registered.\n  MCInstrAnalysisCtorFnTy MCInstrAnalysisCtorFn;\n\n  /// MCRegInfoCtorFn - Constructor function for this target's MCRegisterInfo,\n  /// if registered.\n  MCRegInfoCtorFnTy MCRegInfoCtorFn;\n\n  /// MCSubtargetInfoCtorFn - Constructor function for this target's\n  /// MCSubtargetInfo, if registered.\n  MCSubtargetInfoCtorFnTy MCSubtargetInfoCtorFn;\n\n  /// TargetMachineCtorFn - Construction function for this target's\n  /// TargetMachine, if registered.\n  TargetMachineCtorTy TargetMachineCtorFn;\n\n  /// MCAsmBackendCtorFn - Construction function for this target's\n  /// MCAsmBackend, if registered.\n  MCAsmBackendCtorTy MCAsmBackendCtorFn;\n\n  /// MCAsmParserCtorFn - Construction function for this target's\n  /// MCTargetAsmParser, if registered.\n  MCAsmParserCtorTy MCAsmParserCtorFn;\n\n  /// AsmPrinterCtorFn - Construction function for this target's AsmPrinter,\n  /// if registered.\n  AsmPrinterCtorTy AsmPrinterCtorFn;\n\n  /// MCDisassemblerCtorFn - Construction function for this target's\n  /// MCDisassembler, if registered.\n  MCDisassemblerCtorTy MCDisassemblerCtorFn;\n\n  /// MCInstPrinterCtorFn - Construction function for this target's\n  /// MCInstPrinter, if registered.\n  MCInstPrinterCtorTy MCInstPrinterCtorFn;\n\n  /// MCCodeEmitterCtorFn - Construction function for this target's\n  /// CodeEmitter, if registered.\n  MCCodeEmitterCtorTy MCCodeEmitterCtorFn;\n\n  // Construction functions for the various object formats, if registered.\n  COFFStreamerCtorTy COFFStreamerCtorFn = nullptr;\n  MachOStreamerCtorTy MachOStreamerCtorFn = nullptr;\n  ELFStreamerCtorTy ELFStreamerCtorFn = nullptr;\n  WasmStreamerCtorTy WasmStreamerCtorFn = nullptr;\n\n  /// Construction function for this target's null TargetStreamer, if\n  /// registered (default = nullptr).\n  NullTargetStreamerCtorTy NullTargetStreamerCtorFn = nullptr;\n\n  /// Construction function for this target's asm TargetStreamer, if\n  /// registered (default = nullptr).\n  AsmTargetStreamerCtorTy AsmTargetStreamerCtorFn = nullptr;\n\n  /// Construction function for this target's obj TargetStreamer, if\n  /// registered (default = nullptr).\n  ObjectTargetStreamerCtorTy ObjectTargetStreamerCtorFn = nullptr;\n\n  /// MCRelocationInfoCtorFn - Construction function for this target's\n  /// MCRelocationInfo, if registered (default = llvm::createMCRelocationInfo)\n  MCRelocationInfoCtorTy MCRelocationInfoCtorFn = nullptr;\n\n  /// MCSymbolizerCtorFn - Construction function for this target's\n  /// MCSymbolizer, if registered (default = llvm::createMCSymbolizer)\n  MCSymbolizerCtorTy MCSymbolizerCtorFn = nullptr;\n\npublic:\n  Target() = default;\n\n  /// @name Target Information\n  /// @{\n\n  // getNext - Return the next registered target.\n  const Target *getNext() const { return Next; }\n\n  /// getName - Get the target name.\n  const char *getName() const { return Name; }\n\n  /// getShortDescription - Get a short description of the target.\n  const char *getShortDescription() const { return ShortDesc; }\n\n  /// getBackendName - Get the backend name.\n  const char *getBackendName() const { return BackendName; }\n\n  /// @}\n  /// @name Feature Predicates\n  /// @{\n\n  /// hasJIT - Check if this targets supports the just-in-time compilation.\n  bool hasJIT() const { return HasJIT; }\n\n  /// hasTargetMachine - Check if this target supports code generation.\n  bool hasTargetMachine() const { return TargetMachineCtorFn != nullptr; }\n\n  /// hasMCAsmBackend - Check if this target supports .o generation.\n  bool hasMCAsmBackend() const { return MCAsmBackendCtorFn != nullptr; }\n\n  /// hasMCAsmParser - Check if this target supports assembly parsing.\n  bool hasMCAsmParser() const { return MCAsmParserCtorFn != nullptr; }\n\n  /// @}\n  /// @name Feature Constructors\n  /// @{\n\n  /// createMCAsmInfo - Create a MCAsmInfo implementation for the specified\n  /// target triple.\n  ///\n  /// \\param TheTriple This argument is used to determine the target machine\n  /// feature set; it should always be provided. Generally this should be\n  /// either the target triple from the module, or the target triple of the\n  /// host if that does not exist.\n  MCAsmInfo *createMCAsmInfo(const MCRegisterInfo &MRI, StringRef TheTriple,\n                             const MCTargetOptions &Options) const {\n    if (!MCAsmInfoCtorFn)\n      return nullptr;\n    return MCAsmInfoCtorFn(MRI, Triple(TheTriple), Options);\n  }\n\n  /// createMCInstrInfo - Create a MCInstrInfo implementation.\n  ///\n  MCInstrInfo *createMCInstrInfo() const {\n    if (!MCInstrInfoCtorFn)\n      return nullptr;\n    return MCInstrInfoCtorFn();\n  }\n\n  /// createMCInstrAnalysis - Create a MCInstrAnalysis implementation.\n  ///\n  MCInstrAnalysis *createMCInstrAnalysis(const MCInstrInfo *Info) const {\n    if (!MCInstrAnalysisCtorFn)\n      return nullptr;\n    return MCInstrAnalysisCtorFn(Info);\n  }\n\n  /// createMCRegInfo - Create a MCRegisterInfo implementation.\n  ///\n  MCRegisterInfo *createMCRegInfo(StringRef TT) const {\n    if (!MCRegInfoCtorFn)\n      return nullptr;\n    return MCRegInfoCtorFn(Triple(TT));\n  }\n\n  /// createMCSubtargetInfo - Create a MCSubtargetInfo implementation.\n  ///\n  /// \\param TheTriple This argument is used to determine the target machine\n  /// feature set; it should always be provided. Generally this should be\n  /// either the target triple from the module, or the target triple of the\n  /// host if that does not exist.\n  /// \\param CPU This specifies the name of the target CPU.\n  /// \\param Features This specifies the string representation of the\n  /// additional target features.\n  MCSubtargetInfo *createMCSubtargetInfo(StringRef TheTriple, StringRef CPU,\n                                         StringRef Features) const {\n    if (!MCSubtargetInfoCtorFn)\n      return nullptr;\n    return MCSubtargetInfoCtorFn(Triple(TheTriple), CPU, Features);\n  }\n\n  /// createTargetMachine - Create a target specific machine implementation\n  /// for the specified \\p Triple.\n  ///\n  /// \\param TT This argument is used to determine the target machine\n  /// feature set; it should always be provided. Generally this should be\n  /// either the target triple from the module, or the target triple of the\n  /// host if that does not exist.\n  TargetMachine *createTargetMachine(StringRef TT, StringRef CPU,\n                                     StringRef Features,\n                                     const TargetOptions &Options,\n                                     Optional<Reloc::Model> RM,\n                                     Optional<CodeModel::Model> CM = None,\n                                     CodeGenOpt::Level OL = CodeGenOpt::Default,\n                                     bool JIT = false) const {\n    if (!TargetMachineCtorFn)\n      return nullptr;\n    return TargetMachineCtorFn(*this, Triple(TT), CPU, Features, Options, RM,\n                               CM, OL, JIT);\n  }\n\n  /// createMCAsmBackend - Create a target specific assembly parser.\n  MCAsmBackend *createMCAsmBackend(const MCSubtargetInfo &STI,\n                                   const MCRegisterInfo &MRI,\n                                   const MCTargetOptions &Options) const {\n    if (!MCAsmBackendCtorFn)\n      return nullptr;\n    return MCAsmBackendCtorFn(*this, STI, MRI, Options);\n  }\n\n  /// createMCAsmParser - Create a target specific assembly parser.\n  ///\n  /// \\param Parser The target independent parser implementation to use for\n  /// parsing and lexing.\n  MCTargetAsmParser *createMCAsmParser(const MCSubtargetInfo &STI,\n                                       MCAsmParser &Parser,\n                                       const MCInstrInfo &MII,\n                                       const MCTargetOptions &Options) const {\n    if (!MCAsmParserCtorFn)\n      return nullptr;\n    return MCAsmParserCtorFn(STI, Parser, MII, Options);\n  }\n\n  /// createAsmPrinter - Create a target specific assembly printer pass.  This\n  /// takes ownership of the MCStreamer object.\n  AsmPrinter *createAsmPrinter(TargetMachine &TM,\n                               std::unique_ptr<MCStreamer> &&Streamer) const {\n    if (!AsmPrinterCtorFn)\n      return nullptr;\n    return AsmPrinterCtorFn(TM, std::move(Streamer));\n  }\n\n  MCDisassembler *createMCDisassembler(const MCSubtargetInfo &STI,\n                                       MCContext &Ctx) const {\n    if (!MCDisassemblerCtorFn)\n      return nullptr;\n    return MCDisassemblerCtorFn(*this, STI, Ctx);\n  }\n\n  MCInstPrinter *createMCInstPrinter(const Triple &T, unsigned SyntaxVariant,\n                                     const MCAsmInfo &MAI,\n                                     const MCInstrInfo &MII,\n                                     const MCRegisterInfo &MRI) const {\n    if (!MCInstPrinterCtorFn)\n      return nullptr;\n    return MCInstPrinterCtorFn(T, SyntaxVariant, MAI, MII, MRI);\n  }\n\n  /// createMCCodeEmitter - Create a target specific code emitter.\n  MCCodeEmitter *createMCCodeEmitter(const MCInstrInfo &II,\n                                     const MCRegisterInfo &MRI,\n                                     MCContext &Ctx) const {\n    if (!MCCodeEmitterCtorFn)\n      return nullptr;\n    return MCCodeEmitterCtorFn(II, MRI, Ctx);\n  }\n\n  /// Create a target specific MCStreamer.\n  ///\n  /// \\param T The target triple.\n  /// \\param Ctx The target context.\n  /// \\param TAB The target assembler backend object. Takes ownership.\n  /// \\param OW The stream object.\n  /// \\param Emitter The target independent assembler object.Takes ownership.\n  /// \\param RelaxAll Relax all fixups?\n  MCStreamer *createMCObjectStreamer(const Triple &T, MCContext &Ctx,\n                                     std::unique_ptr<MCAsmBackend> &&TAB,\n                                     std::unique_ptr<MCObjectWriter> &&OW,\n                                     std::unique_ptr<MCCodeEmitter> &&Emitter,\n                                     const MCSubtargetInfo &STI, bool RelaxAll,\n                                     bool IncrementalLinkerCompatible,\n                                     bool DWARFMustBeAtTheEnd) const {\n    MCStreamer *S = nullptr;\n    switch (T.getObjectFormat()) {\n    case Triple::UnknownObjectFormat:\n      llvm_unreachable(\"Unknown object format\");\n    case Triple::COFF:\n      assert(T.isOSWindows() && \"only Windows COFF is supported\");\n      S = COFFStreamerCtorFn(Ctx, std::move(TAB), std::move(OW),\n                             std::move(Emitter), RelaxAll,\n                             IncrementalLinkerCompatible);\n      break;\n    case Triple::MachO:\n      if (MachOStreamerCtorFn)\n        S = MachOStreamerCtorFn(Ctx, std::move(TAB), std::move(OW),\n                                std::move(Emitter), RelaxAll,\n                                DWARFMustBeAtTheEnd);\n      else\n        S = createMachOStreamer(Ctx, std::move(TAB), std::move(OW),\n                                std::move(Emitter), RelaxAll,\n                                DWARFMustBeAtTheEnd);\n      break;\n    case Triple::ELF:\n      if (ELFStreamerCtorFn)\n        S = ELFStreamerCtorFn(T, Ctx, std::move(TAB), std::move(OW),\n                              std::move(Emitter), RelaxAll);\n      else\n        S = createELFStreamer(Ctx, std::move(TAB), std::move(OW),\n                              std::move(Emitter), RelaxAll);\n      break;\n    case Triple::Wasm:\n      if (WasmStreamerCtorFn)\n        S = WasmStreamerCtorFn(T, Ctx, std::move(TAB), std::move(OW),\n                               std::move(Emitter), RelaxAll);\n      else\n        S = createWasmStreamer(Ctx, std::move(TAB), std::move(OW),\n                               std::move(Emitter), RelaxAll);\n      break;\n    case Triple::GOFF:\n      report_fatal_error(\"GOFF MCObjectStreamer not implemented yet\");\n    case Triple::XCOFF:\n      S = createXCOFFStreamer(Ctx, std::move(TAB), std::move(OW),\n                              std::move(Emitter), RelaxAll);\n      break;\n    }\n    if (ObjectTargetStreamerCtorFn)\n      ObjectTargetStreamerCtorFn(*S, STI);\n    return S;\n  }\n\n  MCStreamer *createAsmStreamer(MCContext &Ctx,\n                                std::unique_ptr<formatted_raw_ostream> OS,\n                                bool IsVerboseAsm, bool UseDwarfDirectory,\n                                MCInstPrinter *InstPrint,\n                                std::unique_ptr<MCCodeEmitter> &&CE,\n                                std::unique_ptr<MCAsmBackend> &&TAB,\n                                bool ShowInst) const {\n    formatted_raw_ostream &OSRef = *OS;\n    MCStreamer *S = llvm::createAsmStreamer(\n        Ctx, std::move(OS), IsVerboseAsm, UseDwarfDirectory, InstPrint,\n        std::move(CE), std::move(TAB), ShowInst);\n    createAsmTargetStreamer(*S, OSRef, InstPrint, IsVerboseAsm);\n    return S;\n  }\n\n  MCTargetStreamer *createAsmTargetStreamer(MCStreamer &S,\n                                            formatted_raw_ostream &OS,\n                                            MCInstPrinter *InstPrint,\n                                            bool IsVerboseAsm) const {\n    if (AsmTargetStreamerCtorFn)\n      return AsmTargetStreamerCtorFn(S, OS, InstPrint, IsVerboseAsm);\n    return nullptr;\n  }\n\n  MCStreamer *createNullStreamer(MCContext &Ctx) const {\n    MCStreamer *S = llvm::createNullStreamer(Ctx);\n    createNullTargetStreamer(*S);\n    return S;\n  }\n\n  MCTargetStreamer *createNullTargetStreamer(MCStreamer &S) const {\n    if (NullTargetStreamerCtorFn)\n      return NullTargetStreamerCtorFn(S);\n    return nullptr;\n  }\n\n  /// createMCRelocationInfo - Create a target specific MCRelocationInfo.\n  ///\n  /// \\param TT The target triple.\n  /// \\param Ctx The target context.\n  MCRelocationInfo *createMCRelocationInfo(StringRef TT, MCContext &Ctx) const {\n    MCRelocationInfoCtorTy Fn = MCRelocationInfoCtorFn\n                                    ? MCRelocationInfoCtorFn\n                                    : llvm::createMCRelocationInfo;\n    return Fn(Triple(TT), Ctx);\n  }\n\n  /// createMCSymbolizer - Create a target specific MCSymbolizer.\n  ///\n  /// \\param TT The target triple.\n  /// \\param GetOpInfo The function to get the symbolic information for\n  /// operands.\n  /// \\param SymbolLookUp The function to lookup a symbol name.\n  /// \\param DisInfo The pointer to the block of symbolic information for above\n  /// call\n  /// back.\n  /// \\param Ctx The target context.\n  /// \\param RelInfo The relocation information for this target. Takes\n  /// ownership.\n  MCSymbolizer *\n  createMCSymbolizer(StringRef TT, LLVMOpInfoCallback GetOpInfo,\n                     LLVMSymbolLookupCallback SymbolLookUp, void *DisInfo,\n                     MCContext *Ctx,\n                     std::unique_ptr<MCRelocationInfo> &&RelInfo) const {\n    MCSymbolizerCtorTy Fn =\n        MCSymbolizerCtorFn ? MCSymbolizerCtorFn : llvm::createMCSymbolizer;\n    return Fn(Triple(TT), GetOpInfo, SymbolLookUp, DisInfo, Ctx,\n              std::move(RelInfo));\n  }\n\n  /// @}\n};\n\n/// TargetRegistry - Generic interface to target specific features.\nstruct TargetRegistry {\n  // FIXME: Make this a namespace, probably just move all the Register*\n  // functions into Target (currently they all just set members on the Target\n  // anyway, and Target friends this class so those functions can...\n  // function).\n  TargetRegistry() = delete;\n\n  class iterator\n      : public std::iterator<std::forward_iterator_tag, Target, ptrdiff_t> {\n    friend struct TargetRegistry;\n\n    const Target *Current = nullptr;\n\n    explicit iterator(Target *T) : Current(T) {}\n\n  public:\n    iterator() = default;\n\n    bool operator==(const iterator &x) const { return Current == x.Current; }\n    bool operator!=(const iterator &x) const { return !operator==(x); }\n\n    // Iterator traversal: forward iteration only\n    iterator &operator++() { // Preincrement\n      assert(Current && \"Cannot increment end iterator!\");\n      Current = Current->getNext();\n      return *this;\n    }\n    iterator operator++(int) { // Postincrement\n      iterator tmp = *this;\n      ++*this;\n      return tmp;\n    }\n\n    const Target &operator*() const {\n      assert(Current && \"Cannot dereference end iterator!\");\n      return *Current;\n    }\n\n    const Target *operator->() const { return &operator*(); }\n  };\n\n  /// printRegisteredTargetsForVersion - Print the registered targets\n  /// appropriately for inclusion in a tool's version output.\n  static void printRegisteredTargetsForVersion(raw_ostream &OS);\n\n  /// @name Registry Access\n  /// @{\n\n  static iterator_range<iterator> targets();\n\n  /// lookupTarget - Lookup a target based on a target triple.\n  ///\n  /// \\param Triple - The triple to use for finding a target.\n  /// \\param Error - On failure, an error string describing why no target was\n  /// found.\n  static const Target *lookupTarget(const std::string &Triple,\n                                    std::string &Error);\n\n  /// lookupTarget - Lookup a target based on an architecture name\n  /// and a target triple.  If the architecture name is non-empty,\n  /// then the lookup is done by architecture.  Otherwise, the target\n  /// triple is used.\n  ///\n  /// \\param ArchName - The architecture to use for finding a target.\n  /// \\param TheTriple - The triple to use for finding a target.  The\n  /// triple is updated with canonical architecture name if a lookup\n  /// by architecture is done.\n  /// \\param Error - On failure, an error string describing why no target was\n  /// found.\n  static const Target *lookupTarget(const std::string &ArchName,\n                                    Triple &TheTriple, std::string &Error);\n\n  /// @}\n  /// @name Target Registration\n  /// @{\n\n  /// RegisterTarget - Register the given target. Attempts to register a\n  /// target which has already been registered will be ignored.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Name - The target name. This should be a static string.\n  /// @param ShortDesc - A short target description. This should be a static\n  /// string.\n  /// @param BackendName - The name of the backend. This should be a static\n  /// string that is the same for all targets that share a backend\n  /// implementation and must match the name used in the 'def X : Target ...' in\n  /// TableGen.\n  /// @param ArchMatchFn - The arch match checking function for this target.\n  /// @param HasJIT - Whether the target supports JIT code\n  /// generation.\n  static void RegisterTarget(Target &T, const char *Name, const char *ShortDesc,\n                             const char *BackendName,\n                             Target::ArchMatchFnTy ArchMatchFn,\n                             bool HasJIT = false);\n\n  /// RegisterMCAsmInfo - Register a MCAsmInfo implementation for the\n  /// given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct a MCAsmInfo for the target.\n  static void RegisterMCAsmInfo(Target &T, Target::MCAsmInfoCtorFnTy Fn) {\n    T.MCAsmInfoCtorFn = Fn;\n  }\n\n  /// RegisterMCInstrInfo - Register a MCInstrInfo implementation for the\n  /// given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct a MCInstrInfo for the target.\n  static void RegisterMCInstrInfo(Target &T, Target::MCInstrInfoCtorFnTy Fn) {\n    T.MCInstrInfoCtorFn = Fn;\n  }\n\n  /// RegisterMCInstrAnalysis - Register a MCInstrAnalysis implementation for\n  /// the given target.\n  static void RegisterMCInstrAnalysis(Target &T,\n                                      Target::MCInstrAnalysisCtorFnTy Fn) {\n    T.MCInstrAnalysisCtorFn = Fn;\n  }\n\n  /// RegisterMCRegInfo - Register a MCRegisterInfo implementation for the\n  /// given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct a MCRegisterInfo for the target.\n  static void RegisterMCRegInfo(Target &T, Target::MCRegInfoCtorFnTy Fn) {\n    T.MCRegInfoCtorFn = Fn;\n  }\n\n  /// RegisterMCSubtargetInfo - Register a MCSubtargetInfo implementation for\n  /// the given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct a MCSubtargetInfo for the target.\n  static void RegisterMCSubtargetInfo(Target &T,\n                                      Target::MCSubtargetInfoCtorFnTy Fn) {\n    T.MCSubtargetInfoCtorFn = Fn;\n  }\n\n  /// RegisterTargetMachine - Register a TargetMachine implementation for the\n  /// given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct a TargetMachine for the target.\n  static void RegisterTargetMachine(Target &T, Target::TargetMachineCtorTy Fn) {\n    T.TargetMachineCtorFn = Fn;\n  }\n\n  /// RegisterMCAsmBackend - Register a MCAsmBackend implementation for the\n  /// given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an AsmBackend for the target.\n  static void RegisterMCAsmBackend(Target &T, Target::MCAsmBackendCtorTy Fn) {\n    T.MCAsmBackendCtorFn = Fn;\n  }\n\n  /// RegisterMCAsmParser - Register a MCTargetAsmParser implementation for\n  /// the given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an MCTargetAsmParser for the target.\n  static void RegisterMCAsmParser(Target &T, Target::MCAsmParserCtorTy Fn) {\n    T.MCAsmParserCtorFn = Fn;\n  }\n\n  /// RegisterAsmPrinter - Register an AsmPrinter implementation for the given\n  /// target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an AsmPrinter for the target.\n  static void RegisterAsmPrinter(Target &T, Target::AsmPrinterCtorTy Fn) {\n    T.AsmPrinterCtorFn = Fn;\n  }\n\n  /// RegisterMCDisassembler - Register a MCDisassembler implementation for\n  /// the given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an MCDisassembler for the target.\n  static void RegisterMCDisassembler(Target &T,\n                                     Target::MCDisassemblerCtorTy Fn) {\n    T.MCDisassemblerCtorFn = Fn;\n  }\n\n  /// RegisterMCInstPrinter - Register a MCInstPrinter implementation for the\n  /// given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an MCInstPrinter for the target.\n  static void RegisterMCInstPrinter(Target &T, Target::MCInstPrinterCtorTy Fn) {\n    T.MCInstPrinterCtorFn = Fn;\n  }\n\n  /// RegisterMCCodeEmitter - Register a MCCodeEmitter implementation for the\n  /// given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an MCCodeEmitter for the target.\n  static void RegisterMCCodeEmitter(Target &T, Target::MCCodeEmitterCtorTy Fn) {\n    T.MCCodeEmitterCtorFn = Fn;\n  }\n\n  static void RegisterCOFFStreamer(Target &T, Target::COFFStreamerCtorTy Fn) {\n    T.COFFStreamerCtorFn = Fn;\n  }\n\n  static void RegisterMachOStreamer(Target &T, Target::MachOStreamerCtorTy Fn) {\n    T.MachOStreamerCtorFn = Fn;\n  }\n\n  static void RegisterELFStreamer(Target &T, Target::ELFStreamerCtorTy Fn) {\n    T.ELFStreamerCtorFn = Fn;\n  }\n\n  static void RegisterWasmStreamer(Target &T, Target::WasmStreamerCtorTy Fn) {\n    T.WasmStreamerCtorFn = Fn;\n  }\n\n  static void RegisterNullTargetStreamer(Target &T,\n                                         Target::NullTargetStreamerCtorTy Fn) {\n    T.NullTargetStreamerCtorFn = Fn;\n  }\n\n  static void RegisterAsmTargetStreamer(Target &T,\n                                        Target::AsmTargetStreamerCtorTy Fn) {\n    T.AsmTargetStreamerCtorFn = Fn;\n  }\n\n  static void\n  RegisterObjectTargetStreamer(Target &T,\n                               Target::ObjectTargetStreamerCtorTy Fn) {\n    T.ObjectTargetStreamerCtorFn = Fn;\n  }\n\n  /// RegisterMCRelocationInfo - Register an MCRelocationInfo\n  /// implementation for the given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an MCRelocationInfo for the target.\n  static void RegisterMCRelocationInfo(Target &T,\n                                       Target::MCRelocationInfoCtorTy Fn) {\n    T.MCRelocationInfoCtorFn = Fn;\n  }\n\n  /// RegisterMCSymbolizer - Register an MCSymbolizer\n  /// implementation for the given target.\n  ///\n  /// Clients are responsible for ensuring that registration doesn't occur\n  /// while another thread is attempting to access the registry. Typically\n  /// this is done by initializing all targets at program startup.\n  ///\n  /// @param T - The target being registered.\n  /// @param Fn - A function to construct an MCSymbolizer for the target.\n  static void RegisterMCSymbolizer(Target &T, Target::MCSymbolizerCtorTy Fn) {\n    T.MCSymbolizerCtorFn = Fn;\n  }\n\n  /// @}\n};\n\n//===--------------------------------------------------------------------===//\n\n/// RegisterTarget - Helper template for registering a target, for use in the\n/// target's initialization function. Usage:\n///\n///\n/// Target &getTheFooTarget() { // The global target instance.\n///   static Target TheFooTarget;\n///   return TheFooTarget;\n/// }\n/// extern \"C\" void LLVMInitializeFooTargetInfo() {\n///   RegisterTarget<Triple::foo> X(getTheFooTarget(), \"foo\", \"Foo\n///   description\", \"Foo\" /* Backend Name */);\n/// }\ntemplate <Triple::ArchType TargetArchType = Triple::UnknownArch,\n          bool HasJIT = false>\nstruct RegisterTarget {\n  RegisterTarget(Target &T, const char *Name, const char *Desc,\n                 const char *BackendName) {\n    TargetRegistry::RegisterTarget(T, Name, Desc, BackendName, &getArchMatch,\n                                   HasJIT);\n  }\n\n  static bool getArchMatch(Triple::ArchType Arch) {\n    return Arch == TargetArchType;\n  }\n};\n\n/// RegisterMCAsmInfo - Helper template for registering a target assembly info\n/// implementation.  This invokes the static \"Create\" method on the class to\n/// actually do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCAsmInfo<FooMCAsmInfo> X(TheFooTarget);\n/// }\ntemplate <class MCAsmInfoImpl> struct RegisterMCAsmInfo {\n  RegisterMCAsmInfo(Target &T) {\n    TargetRegistry::RegisterMCAsmInfo(T, &Allocator);\n  }\n\nprivate:\n  static MCAsmInfo *Allocator(const MCRegisterInfo & /*MRI*/, const Triple &TT,\n                              const MCTargetOptions &Options) {\n    return new MCAsmInfoImpl(TT, Options);\n  }\n};\n\n/// RegisterMCAsmInfoFn - Helper template for registering a target assembly info\n/// implementation.  This invokes the specified function to do the\n/// construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCAsmInfoFn X(TheFooTarget, TheFunction);\n/// }\nstruct RegisterMCAsmInfoFn {\n  RegisterMCAsmInfoFn(Target &T, Target::MCAsmInfoCtorFnTy Fn) {\n    TargetRegistry::RegisterMCAsmInfo(T, Fn);\n  }\n};\n\n/// RegisterMCInstrInfo - Helper template for registering a target instruction\n/// info implementation.  This invokes the static \"Create\" method on the class\n/// to actually do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCInstrInfo<FooMCInstrInfo> X(TheFooTarget);\n/// }\ntemplate <class MCInstrInfoImpl> struct RegisterMCInstrInfo {\n  RegisterMCInstrInfo(Target &T) {\n    TargetRegistry::RegisterMCInstrInfo(T, &Allocator);\n  }\n\nprivate:\n  static MCInstrInfo *Allocator() { return new MCInstrInfoImpl(); }\n};\n\n/// RegisterMCInstrInfoFn - Helper template for registering a target\n/// instruction info implementation.  This invokes the specified function to\n/// do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCInstrInfoFn X(TheFooTarget, TheFunction);\n/// }\nstruct RegisterMCInstrInfoFn {\n  RegisterMCInstrInfoFn(Target &T, Target::MCInstrInfoCtorFnTy Fn) {\n    TargetRegistry::RegisterMCInstrInfo(T, Fn);\n  }\n};\n\n/// RegisterMCInstrAnalysis - Helper template for registering a target\n/// instruction analyzer implementation.  This invokes the static \"Create\"\n/// method on the class to actually do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCInstrAnalysis<FooMCInstrAnalysis> X(TheFooTarget);\n/// }\ntemplate <class MCInstrAnalysisImpl> struct RegisterMCInstrAnalysis {\n  RegisterMCInstrAnalysis(Target &T) {\n    TargetRegistry::RegisterMCInstrAnalysis(T, &Allocator);\n  }\n\nprivate:\n  static MCInstrAnalysis *Allocator(const MCInstrInfo *Info) {\n    return new MCInstrAnalysisImpl(Info);\n  }\n};\n\n/// RegisterMCInstrAnalysisFn - Helper template for registering a target\n/// instruction analyzer implementation.  This invokes the specified function\n/// to do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCInstrAnalysisFn X(TheFooTarget, TheFunction);\n/// }\nstruct RegisterMCInstrAnalysisFn {\n  RegisterMCInstrAnalysisFn(Target &T, Target::MCInstrAnalysisCtorFnTy Fn) {\n    TargetRegistry::RegisterMCInstrAnalysis(T, Fn);\n  }\n};\n\n/// RegisterMCRegInfo - Helper template for registering a target register info\n/// implementation.  This invokes the static \"Create\" method on the class to\n/// actually do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCRegInfo<FooMCRegInfo> X(TheFooTarget);\n/// }\ntemplate <class MCRegisterInfoImpl> struct RegisterMCRegInfo {\n  RegisterMCRegInfo(Target &T) {\n    TargetRegistry::RegisterMCRegInfo(T, &Allocator);\n  }\n\nprivate:\n  static MCRegisterInfo *Allocator(const Triple & /*TT*/) {\n    return new MCRegisterInfoImpl();\n  }\n};\n\n/// RegisterMCRegInfoFn - Helper template for registering a target register\n/// info implementation.  This invokes the specified function to do the\n/// construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCRegInfoFn X(TheFooTarget, TheFunction);\n/// }\nstruct RegisterMCRegInfoFn {\n  RegisterMCRegInfoFn(Target &T, Target::MCRegInfoCtorFnTy Fn) {\n    TargetRegistry::RegisterMCRegInfo(T, Fn);\n  }\n};\n\n/// RegisterMCSubtargetInfo - Helper template for registering a target\n/// subtarget info implementation.  This invokes the static \"Create\" method\n/// on the class to actually do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCSubtargetInfo<FooMCSubtargetInfo> X(TheFooTarget);\n/// }\ntemplate <class MCSubtargetInfoImpl> struct RegisterMCSubtargetInfo {\n  RegisterMCSubtargetInfo(Target &T) {\n    TargetRegistry::RegisterMCSubtargetInfo(T, &Allocator);\n  }\n\nprivate:\n  static MCSubtargetInfo *Allocator(const Triple & /*TT*/, StringRef /*CPU*/,\n                                    StringRef /*FS*/) {\n    return new MCSubtargetInfoImpl();\n  }\n};\n\n/// RegisterMCSubtargetInfoFn - Helper template for registering a target\n/// subtarget info implementation.  This invokes the specified function to\n/// do the construction.  Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterMCSubtargetInfoFn X(TheFooTarget, TheFunction);\n/// }\nstruct RegisterMCSubtargetInfoFn {\n  RegisterMCSubtargetInfoFn(Target &T, Target::MCSubtargetInfoCtorFnTy Fn) {\n    TargetRegistry::RegisterMCSubtargetInfo(T, Fn);\n  }\n};\n\n/// RegisterTargetMachine - Helper template for registering a target machine\n/// implementation, for use in the target machine initialization\n/// function. Usage:\n///\n/// extern \"C\" void LLVMInitializeFooTarget() {\n///   extern Target TheFooTarget;\n///   RegisterTargetMachine<FooTargetMachine> X(TheFooTarget);\n/// }\ntemplate <class TargetMachineImpl> struct RegisterTargetMachine {\n  RegisterTargetMachine(Target &T) {\n    TargetRegistry::RegisterTargetMachine(T, &Allocator);\n  }\n\nprivate:\n  static TargetMachine *\n  Allocator(const Target &T, const Triple &TT, StringRef CPU, StringRef FS,\n            const TargetOptions &Options, Optional<Reloc::Model> RM,\n            Optional<CodeModel::Model> CM, CodeGenOpt::Level OL, bool JIT) {\n    return new TargetMachineImpl(T, TT, CPU, FS, Options, RM, CM, OL, JIT);\n  }\n};\n\n/// RegisterMCAsmBackend - Helper template for registering a target specific\n/// assembler backend. Usage:\n///\n/// extern \"C\" void LLVMInitializeFooMCAsmBackend() {\n///   extern Target TheFooTarget;\n///   RegisterMCAsmBackend<FooAsmLexer> X(TheFooTarget);\n/// }\ntemplate <class MCAsmBackendImpl> struct RegisterMCAsmBackend {\n  RegisterMCAsmBackend(Target &T) {\n    TargetRegistry::RegisterMCAsmBackend(T, &Allocator);\n  }\n\nprivate:\n  static MCAsmBackend *Allocator(const Target &T, const MCSubtargetInfo &STI,\n                                 const MCRegisterInfo &MRI,\n                                 const MCTargetOptions &Options) {\n    return new MCAsmBackendImpl(T, STI, MRI);\n  }\n};\n\n/// RegisterMCAsmParser - Helper template for registering a target specific\n/// assembly parser, for use in the target machine initialization\n/// function. Usage:\n///\n/// extern \"C\" void LLVMInitializeFooMCAsmParser() {\n///   extern Target TheFooTarget;\n///   RegisterMCAsmParser<FooAsmParser> X(TheFooTarget);\n/// }\ntemplate <class MCAsmParserImpl> struct RegisterMCAsmParser {\n  RegisterMCAsmParser(Target &T) {\n    TargetRegistry::RegisterMCAsmParser(T, &Allocator);\n  }\n\nprivate:\n  static MCTargetAsmParser *Allocator(const MCSubtargetInfo &STI,\n                                      MCAsmParser &P, const MCInstrInfo &MII,\n                                      const MCTargetOptions &Options) {\n    return new MCAsmParserImpl(STI, P, MII, Options);\n  }\n};\n\n/// RegisterAsmPrinter - Helper template for registering a target specific\n/// assembly printer, for use in the target machine initialization\n/// function. Usage:\n///\n/// extern \"C\" void LLVMInitializeFooAsmPrinter() {\n///   extern Target TheFooTarget;\n///   RegisterAsmPrinter<FooAsmPrinter> X(TheFooTarget);\n/// }\ntemplate <class AsmPrinterImpl> struct RegisterAsmPrinter {\n  RegisterAsmPrinter(Target &T) {\n    TargetRegistry::RegisterAsmPrinter(T, &Allocator);\n  }\n\nprivate:\n  static AsmPrinter *Allocator(TargetMachine &TM,\n                               std::unique_ptr<MCStreamer> &&Streamer) {\n    return new AsmPrinterImpl(TM, std::move(Streamer));\n  }\n};\n\n/// RegisterMCCodeEmitter - Helper template for registering a target specific\n/// machine code emitter, for use in the target initialization\n/// function. Usage:\n///\n/// extern \"C\" void LLVMInitializeFooMCCodeEmitter() {\n///   extern Target TheFooTarget;\n///   RegisterMCCodeEmitter<FooCodeEmitter> X(TheFooTarget);\n/// }\ntemplate <class MCCodeEmitterImpl> struct RegisterMCCodeEmitter {\n  RegisterMCCodeEmitter(Target &T) {\n    TargetRegistry::RegisterMCCodeEmitter(T, &Allocator);\n  }\n\nprivate:\n  static MCCodeEmitter *Allocator(const MCInstrInfo & /*II*/,\n                                  const MCRegisterInfo & /*MRI*/,\n                                  MCContext & /*Ctx*/) {\n    return new MCCodeEmitterImpl();\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_SUPPORT_TARGETREGISTRY_H\n"}, "121": {"id": 121, "path": "/home/vsts/work/1/llvm-project/llvm/utils/unittest/googletest/include/gtest/gtest.h", "content": "// Copyright 2005, Google Inc.\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Author: wan@google.com (Zhanyong Wan)\n//\n// The Google C++ Testing Framework (Google Test)\n//\n// This header file defines the public API for Google Test.  It should be\n// included by any test program that uses Google Test.\n//\n// IMPORTANT NOTE: Due to limitation of the C++ language, we have to\n// leave some internal implementation details in this header file.\n// They are clearly marked by comments like this:\n//\n//   // INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\n//\n// Such code is NOT meant to be used by a user directly, and is subject\n// to CHANGE WITHOUT NOTICE.  Therefore DO NOT DEPEND ON IT in a user\n// program!\n//\n// Acknowledgment: Google Test borrowed the idea of automatic test\n// registration from Barthelemy Dagenais' (barthelemy@prologique.com)\n// easyUnit framework.\n\n#ifndef GTEST_INCLUDE_GTEST_GTEST_H_\n#define GTEST_INCLUDE_GTEST_GTEST_H_\n\n#include <limits>\n#include <ostream>\n#include <vector>\n\n#include \"gtest/internal/gtest-internal.h\"\n#include \"gtest/internal/gtest-string.h\"\n#include \"gtest/gtest-death-test.h\"\n#include \"gtest/gtest-message.h\"\n#include \"gtest/gtest-param-test.h\"\n#include \"gtest/gtest-printers.h\"\n#include \"gtest/gtest_prod.h\"\n#include \"gtest/gtest-test-part.h\"\n#include \"gtest/gtest-typed-test.h\"\n\n// Depending on the platform, different string classes are available.\n// On Linux, in addition to ::std::string, Google also makes use of\n// class ::string, which has the same interface as ::std::string, but\n// has a different implementation.\n//\n// You can define GTEST_HAS_GLOBAL_STRING to 1 to indicate that\n// ::string is available AND is a distinct type to ::std::string, or\n// define it to 0 to indicate otherwise.\n//\n// If ::std::string and ::string are the same class on your platform\n// due to aliasing, you should define GTEST_HAS_GLOBAL_STRING to 0.\n//\n// If you do not define GTEST_HAS_GLOBAL_STRING, it is defined\n// heuristically.\n\nnamespace testing {\n\n// Declares the flags.\n\n// This flag temporary enables the disabled tests.\nGTEST_DECLARE_bool_(also_run_disabled_tests);\n\n// This flag brings the debugger on an assertion failure.\nGTEST_DECLARE_bool_(break_on_failure);\n\n// This flag controls whether Google Test catches all test-thrown exceptions\n// and logs them as failures.\nGTEST_DECLARE_bool_(catch_exceptions);\n\n// This flag enables using colors in terminal output. Available values are\n// \"yes\" to enable colors, \"no\" (disable colors), or \"auto\" (the default)\n// to let Google Test decide.\nGTEST_DECLARE_string_(color);\n\n// This flag sets up the filter to select by name using a glob pattern\n// the tests to run. If the filter is not given all tests are executed.\nGTEST_DECLARE_string_(filter);\n\n// This flag causes the Google Test to list tests. None of the tests listed\n// are actually run if the flag is provided.\nGTEST_DECLARE_bool_(list_tests);\n\n// This flag controls whether Google Test emits a detailed XML report to a file\n// in addition to its normal textual output.\nGTEST_DECLARE_string_(output);\n\n// This flags control whether Google Test prints the elapsed time for each\n// test.\nGTEST_DECLARE_bool_(print_time);\n\n// This flag specifies the random number seed.\nGTEST_DECLARE_int32_(random_seed);\n\n// This flag sets how many times the tests are repeated. The default value\n// is 1. If the value is -1 the tests are repeating forever.\nGTEST_DECLARE_int32_(repeat);\n\n// This flag controls whether Google Test includes Google Test internal\n// stack frames in failure stack traces.\nGTEST_DECLARE_bool_(show_internal_stack_frames);\n\n// When this flag is specified, tests' order is randomized on every iteration.\nGTEST_DECLARE_bool_(shuffle);\n\n// This flag specifies the maximum number of stack frames to be\n// printed in a failure message.\nGTEST_DECLARE_int32_(stack_trace_depth);\n\n// When this flag is specified, a failed assertion will throw an\n// exception if exceptions are enabled, or exit the program with a\n// non-zero code otherwise.\nGTEST_DECLARE_bool_(throw_on_failure);\n\n// When this flag is set with a \"host:port\" string, on supported\n// platforms test results are streamed to the specified port on\n// the specified host machine.\nGTEST_DECLARE_string_(stream_result_to);\n\n// The upper limit for valid stack trace depths.\nconst int kMaxStackTraceDepth = 100;\n\nnamespace internal {\n\nclass AssertHelper;\nclass DefaultGlobalTestPartResultReporter;\nclass ExecDeathTest;\nclass NoExecDeathTest;\nclass FinalSuccessChecker;\nclass GTestFlagSaver;\nclass StreamingListenerTest;\nclass TestResultAccessor;\nclass TestEventListenersAccessor;\nclass TestEventRepeater;\nclass UnitTestRecordPropertyTestHelper;\nclass WindowsDeathTest;\nclass UnitTestImpl* GetUnitTestImpl();\nvoid ReportFailureInUnknownLocation(TestPartResult::Type result_type,\n                                    const std::string& message);\n\n}  // namespace internal\n\n// The friend relationship of some of these classes is cyclic.\n// If we don't forward declare them the compiler might confuse the classes\n// in friendship clauses with same named classes on the scope.\nclass Test;\nclass TestCase;\nclass TestInfo;\nclass UnitTest;\n\n// A class for indicating whether an assertion was successful.  When\n// the assertion wasn't successful, the AssertionResult object\n// remembers a non-empty message that describes how it failed.\n//\n// To create an instance of this class, use one of the factory functions\n// (AssertionSuccess() and AssertionFailure()).\n//\n// This class is useful for two purposes:\n//   1. Defining predicate functions to be used with Boolean test assertions\n//      EXPECT_TRUE/EXPECT_FALSE and their ASSERT_ counterparts\n//   2. Defining predicate-format functions to be\n//      used with predicate assertions (ASSERT_PRED_FORMAT*, etc).\n//\n// For example, if you define IsEven predicate:\n//\n//   testing::AssertionResult IsEven(int n) {\n//     if ((n % 2) == 0)\n//       return testing::AssertionSuccess();\n//     else\n//       return testing::AssertionFailure() << n << \" is odd\";\n//   }\n//\n// Then the failed expectation EXPECT_TRUE(IsEven(Fib(5)))\n// will print the message\n//\n//   Value of: IsEven(Fib(5))\n//     Actual: false (5 is odd)\n//   Expected: true\n//\n// instead of a more opaque\n//\n//   Value of: IsEven(Fib(5))\n//     Actual: false\n//   Expected: true\n//\n// in case IsEven is a simple Boolean predicate.\n//\n// If you expect your predicate to be reused and want to support informative\n// messages in EXPECT_FALSE and ASSERT_FALSE (negative assertions show up\n// about half as often as positive ones in our tests), supply messages for\n// both success and failure cases:\n//\n//   testing::AssertionResult IsEven(int n) {\n//     if ((n % 2) == 0)\n//       return testing::AssertionSuccess() << n << \" is even\";\n//     else\n//       return testing::AssertionFailure() << n << \" is odd\";\n//   }\n//\n// Then a statement EXPECT_FALSE(IsEven(Fib(6))) will print\n//\n//   Value of: IsEven(Fib(6))\n//     Actual: true (8 is even)\n//   Expected: false\n//\n// NB: Predicates that support negative Boolean assertions have reduced\n// performance in positive ones so be careful not to use them in tests\n// that have lots (tens of thousands) of positive Boolean assertions.\n//\n// To use this class with EXPECT_PRED_FORMAT assertions such as:\n//\n//   // Verifies that Foo() returns an even number.\n//   EXPECT_PRED_FORMAT1(IsEven, Foo());\n//\n// you need to define:\n//\n//   testing::AssertionResult IsEven(const char* expr, int n) {\n//     if ((n % 2) == 0)\n//       return testing::AssertionSuccess();\n//     else\n//       return testing::AssertionFailure()\n//         << \"Expected: \" << expr << \" is even\\n  Actual: it's \" << n;\n//   }\n//\n// If Foo() returns 5, you will see the following message:\n//\n//   Expected: Foo() is even\n//     Actual: it's 5\n//\nclass GTEST_API_ AssertionResult {\n public:\n  // Copy constructor.\n  // Used in EXPECT_TRUE/FALSE(assertion_result).\n  AssertionResult(const AssertionResult& other);\n\n  GTEST_DISABLE_MSC_WARNINGS_PUSH_(4800 /* forcing value to bool */)\n\n  // Used in the EXPECT_TRUE/FALSE(bool_expression).\n  //\n  // T must be contextually convertible to bool.\n  //\n  // The second parameter prevents this overload from being considered if\n  // the argument is implicitly convertible to AssertionResult. In that case\n  // we want AssertionResult's copy constructor to be used.\n  template <typename T>\n  explicit AssertionResult(\n      const T& success,\n      typename internal::EnableIf<\n          !internal::ImplicitlyConvertible<T, AssertionResult>::value>::type*\n          /*enabler*/ = NULL)\n      : success_(success) {}\n\n  GTEST_DISABLE_MSC_WARNINGS_POP_()\n\n  // Assignment operator.\n  AssertionResult& operator=(AssertionResult other) {\n    swap(other);\n    return *this;\n  }\n\n  // Returns true iff the assertion succeeded.\n  operator bool() const { return success_; }  // NOLINT\n\n  // Returns the assertion's negation. Used with EXPECT/ASSERT_FALSE.\n  AssertionResult operator!() const;\n\n  // Returns the text streamed into this AssertionResult. Test assertions\n  // use it when they fail (i.e., the predicate's outcome doesn't match the\n  // assertion's expectation). When nothing has been streamed into the\n  // object, returns an empty string.\n  const char* message() const {\n    return message_.get() != NULL ?  message_->c_str() : \"\";\n  }\n  // TODO(vladl@google.com): Remove this after making sure no clients use it.\n  // Deprecated; please use message() instead.\n  const char* failure_message() const { return message(); }\n\n  // Streams a custom failure message into this object.\n  template <typename T> AssertionResult& operator<<(const T& value) {\n    AppendMessage(Message() << value);\n    return *this;\n  }\n\n  // Allows streaming basic output manipulators such as endl or flush into\n  // this object.\n  AssertionResult& operator<<(\n      ::std::ostream& (*basic_manipulator)(::std::ostream& stream)) {\n    AppendMessage(Message() << basic_manipulator);\n    return *this;\n  }\n\n private:\n  // Appends the contents of message to message_.\n  void AppendMessage(const Message& a_message) {\n    if (message_.get() == NULL)\n      message_.reset(new ::std::string);\n    message_->append(a_message.GetString().c_str());\n  }\n\n  // Swap the contents of this AssertionResult with other.\n  void swap(AssertionResult& other);\n\n  // Stores result of the assertion predicate.\n  bool success_;\n  // Stores the message describing the condition in case the expectation\n  // construct is not satisfied with the predicate's outcome.\n  // Referenced via a pointer to avoid taking too much stack frame space\n  // with test assertions.\n  internal::scoped_ptr< ::std::string> message_;\n};\n\n// Makes a successful assertion result.\nGTEST_API_ AssertionResult AssertionSuccess();\n\n// Makes a failed assertion result.\nGTEST_API_ AssertionResult AssertionFailure();\n\n// Makes a failed assertion result with the given failure message.\n// Deprecated; use AssertionFailure() << msg.\nGTEST_API_ AssertionResult AssertionFailure(const Message& msg);\n\n// The abstract class that all tests inherit from.\n//\n// In Google Test, a unit test program contains one or many TestCases, and\n// each TestCase contains one or many Tests.\n//\n// When you define a test using the TEST macro, you don't need to\n// explicitly derive from Test - the TEST macro automatically does\n// this for you.\n//\n// The only time you derive from Test is when defining a test fixture\n// to be used a TEST_F.  For example:\n//\n//   class FooTest : public testing::Test {\n//    protected:\n//     void SetUp() override { ... }\n//     void TearDown() override { ... }\n//     ...\n//   };\n//\n//   TEST_F(FooTest, Bar) { ... }\n//   TEST_F(FooTest, Baz) { ... }\n//\n// Test is not copyable.\nclass GTEST_API_ Test {\n public:\n  friend class TestInfo;\n\n  // Defines types for pointers to functions that set up and tear down\n  // a test case.\n  typedef internal::SetUpTestCaseFunc SetUpTestCaseFunc;\n  typedef internal::TearDownTestCaseFunc TearDownTestCaseFunc;\n\n  // The d'tor is virtual as we intend to inherit from Test.\n  virtual ~Test();\n\n  // Sets up the stuff shared by all tests in this test case.\n  //\n  // Google Test will call Foo::SetUpTestCase() before running the first\n  // test in test case Foo.  Hence a sub-class can define its own\n  // SetUpTestCase() method to shadow the one defined in the super\n  // class.\n  static void SetUpTestCase() {}\n\n  // Tears down the stuff shared by all tests in this test case.\n  //\n  // Google Test will call Foo::TearDownTestCase() after running the last\n  // test in test case Foo.  Hence a sub-class can define its own\n  // TearDownTestCase() method to shadow the one defined in the super\n  // class.\n  static void TearDownTestCase() {}\n\n  // Returns true iff the current test has a fatal failure.\n  static bool HasFatalFailure();\n\n  // Returns true iff the current test has a non-fatal failure.\n  static bool HasNonfatalFailure();\n\n  // Returns true iff the current test has a (either fatal or\n  // non-fatal) failure.\n  static bool HasFailure() { return HasFatalFailure() || HasNonfatalFailure(); }\n\n  // Logs a property for the current test, test case, or for the entire\n  // invocation of the test program when used outside of the context of a\n  // test case.  Only the last value for a given key is remembered.  These\n  // are public static so they can be called from utility functions that are\n  // not members of the test fixture.  Calls to RecordProperty made during\n  // lifespan of the test (from the moment its constructor starts to the\n  // moment its destructor finishes) will be output in XML as attributes of\n  // the <testcase> element.  Properties recorded from fixture's\n  // SetUpTestCase or TearDownTestCase are logged as attributes of the\n  // corresponding <testsuite> element.  Calls to RecordProperty made in the\n  // global context (before or after invocation of RUN_ALL_TESTS and from\n  // SetUp/TearDown method of Environment objects registered with Google\n  // Test) will be output as attributes of the <testsuites> element.\n  static void RecordProperty(const std::string& key, const std::string& value);\n  static void RecordProperty(const std::string& key, int value);\n\n protected:\n  // Creates a Test object.\n  Test();\n\n  // Sets up the test fixture.\n  virtual void SetUp();\n\n  // Tears down the test fixture.\n  virtual void TearDown();\n\n private:\n  // Returns true iff the current test has the same fixture class as\n  // the first test in the current test case.\n  static bool HasSameFixtureClass();\n\n  // Runs the test after the test fixture has been set up.\n  //\n  // A sub-class must implement this to define the test logic.\n  //\n  // DO NOT OVERRIDE THIS FUNCTION DIRECTLY IN A USER PROGRAM.\n  // Instead, use the TEST or TEST_F macro.\n  virtual void TestBody() = 0;\n\n  // Sets up, executes, and tears down the test.\n  void Run();\n\n  // Deletes self.  We deliberately pick an unusual name for this\n  // internal method to avoid clashing with names used in user TESTs.\n  void DeleteSelf_() { delete this; }\n\n  const internal::scoped_ptr< GTEST_FLAG_SAVER_ > gtest_flag_saver_;\n\n  // Often a user misspells SetUp() as Setup() and spends a long time\n  // wondering why it is never called by Google Test.  The declaration of\n  // the following method is solely for catching such an error at\n  // compile time:\n  //\n  //   - The return type is deliberately chosen to be not void, so it\n  //   will be a conflict if void Setup() is declared in the user's\n  //   test fixture.\n  //\n  //   - This method is private, so it will be another compiler error\n  //   if the method is called from the user's test fixture.\n  //\n  // DO NOT OVERRIDE THIS FUNCTION.\n  //\n  // If you see an error about overriding the following function or\n  // about it being private, you have mis-spelled SetUp() as Setup().\n  struct Setup_should_be_spelled_SetUp {};\n  virtual Setup_should_be_spelled_SetUp* Setup() { return NULL; }\n\n  // We disallow copying Tests.\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(Test);\n};\n\ntypedef internal::TimeInMillis TimeInMillis;\n\n// A copyable object representing a user specified test property which can be\n// output as a key/value string pair.\n//\n// Don't inherit from TestProperty as its destructor is not virtual.\nclass TestProperty {\n public:\n  // C'tor.  TestProperty does NOT have a default constructor.\n  // Always use this constructor (with parameters) to create a\n  // TestProperty object.\n  TestProperty(const std::string& a_key, const std::string& a_value) :\n    key_(a_key), value_(a_value) {\n  }\n\n  // Gets the user supplied key.\n  const char* key() const {\n    return key_.c_str();\n  }\n\n  // Gets the user supplied value.\n  const char* value() const {\n    return value_.c_str();\n  }\n\n  // Sets a new value, overriding the one supplied in the constructor.\n  void SetValue(const std::string& new_value) {\n    value_ = new_value;\n  }\n\n private:\n  // The key supplied by the user.\n  std::string key_;\n  // The value supplied by the user.\n  std::string value_;\n};\n\n// The result of a single Test.  This includes a list of\n// TestPartResults, a list of TestProperties, a count of how many\n// death tests there are in the Test, and how much time it took to run\n// the Test.\n//\n// TestResult is not copyable.\nclass GTEST_API_ TestResult {\n public:\n  // Creates an empty TestResult.\n  TestResult();\n\n  // D'tor.  Do not inherit from TestResult.\n  ~TestResult();\n\n  // Gets the number of all test parts.  This is the sum of the number\n  // of successful test parts and the number of failed test parts.\n  int total_part_count() const;\n\n  // Returns the number of the test properties.\n  int test_property_count() const;\n\n  // Returns true iff the test passed (i.e. no test part failed).\n  bool Passed() const { return !Failed(); }\n\n  // Returns true iff the test failed.\n  bool Failed() const;\n\n  // Returns true iff the test fatally failed.\n  bool HasFatalFailure() const;\n\n  // Returns true iff the test has a non-fatal failure.\n  bool HasNonfatalFailure() const;\n\n  // Returns the elapsed time, in milliseconds.\n  TimeInMillis elapsed_time() const { return elapsed_time_; }\n\n  // Returns the i-th test part result among all the results. i can range\n  // from 0 to test_property_count() - 1. If i is not in that range, aborts\n  // the program.\n  const TestPartResult& GetTestPartResult(int i) const;\n\n  // Returns the i-th test property. i can range from 0 to\n  // test_property_count() - 1. If i is not in that range, aborts the\n  // program.\n  const TestProperty& GetTestProperty(int i) const;\n\n private:\n  friend class TestInfo;\n  friend class TestCase;\n  friend class UnitTest;\n  friend class internal::DefaultGlobalTestPartResultReporter;\n  friend class internal::ExecDeathTest;\n  friend class internal::TestResultAccessor;\n  friend class internal::UnitTestImpl;\n  friend class internal::WindowsDeathTest;\n\n  // Gets the vector of TestPartResults.\n  const std::vector<TestPartResult>& test_part_results() const {\n    return test_part_results_;\n  }\n\n  // Gets the vector of TestProperties.\n  const std::vector<TestProperty>& test_properties() const {\n    return test_properties_;\n  }\n\n  // Sets the elapsed time.\n  void set_elapsed_time(TimeInMillis elapsed) { elapsed_time_ = elapsed; }\n\n  // Adds a test property to the list. The property is validated and may add\n  // a non-fatal failure if invalid (e.g., if it conflicts with reserved\n  // key names). If a property is already recorded for the same key, the\n  // value will be updated, rather than storing multiple values for the same\n  // key.  xml_element specifies the element for which the property is being\n  // recorded and is used for validation.\n  void RecordProperty(const std::string& xml_element,\n                      const TestProperty& test_property);\n\n  // Adds a failure if the key is a reserved attribute of Google Test\n  // testcase tags.  Returns true if the property is valid.\n  // TODO(russr): Validate attribute names are legal and human readable.\n  static bool ValidateTestProperty(const std::string& xml_element,\n                                   const TestProperty& test_property);\n\n  // Adds a test part result to the list.\n  void AddTestPartResult(const TestPartResult& test_part_result);\n\n  // Returns the death test count.\n  int death_test_count() const { return death_test_count_; }\n\n  // Increments the death test count, returning the new count.\n  int increment_death_test_count() { return ++death_test_count_; }\n\n  // Clears the test part results.\n  void ClearTestPartResults();\n\n  // Clears the object.\n  void Clear();\n\n  // Protects mutable state of the property vector and of owned\n  // properties, whose values may be updated.\n  internal::Mutex test_properites_mutex_;\n\n  // The vector of TestPartResults\n  std::vector<TestPartResult> test_part_results_;\n  // The vector of TestProperties\n  std::vector<TestProperty> test_properties_;\n  // Running count of death tests.\n  int death_test_count_;\n  // The elapsed time, in milliseconds.\n  TimeInMillis elapsed_time_;\n\n  // We disallow copying TestResult.\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(TestResult);\n};  // class TestResult\n\n// A TestInfo object stores the following information about a test:\n//\n//   Test case name\n//   Test name\n//   Whether the test should be run\n//   A function pointer that creates the test object when invoked\n//   Test result\n//\n// The constructor of TestInfo registers itself with the UnitTest\n// singleton such that the RUN_ALL_TESTS() macro knows which tests to\n// run.\nclass GTEST_API_ TestInfo {\n public:\n  // Destructs a TestInfo object.  This function is not virtual, so\n  // don't inherit from TestInfo.\n  ~TestInfo();\n\n  // Returns the test case name.\n  const char* test_case_name() const { return test_case_name_.c_str(); }\n\n  // Returns the test name.\n  const char* name() const { return name_.c_str(); }\n\n  // Returns the name of the parameter type, or NULL if this is not a typed\n  // or a type-parameterized test.\n  const char* type_param() const {\n    if (type_param_.get() != NULL)\n      return type_param_->c_str();\n    return NULL;\n  }\n\n  // Returns the text representation of the value parameter, or NULL if this\n  // is not a value-parameterized test.\n  const char* value_param() const {\n    if (value_param_.get() != NULL)\n      return value_param_->c_str();\n    return NULL;\n  }\n\n  // Returns the file name where this test is defined.\n  const char* file() const { return location_.file.c_str(); }\n\n  // Returns the line where this test is defined.\n  int line() const { return location_.line; }\n\n  // Returns true if this test should run, that is if the test is not\n  // disabled (or it is disabled but the also_run_disabled_tests flag has\n  // been specified) and its full name matches the user-specified filter.\n  //\n  // Google Test allows the user to filter the tests by their full names.\n  // The full name of a test Bar in test case Foo is defined as\n  // \"Foo.Bar\".  Only the tests that match the filter will run.\n  //\n  // A filter is a colon-separated list of glob (not regex) patterns,\n  // optionally followed by a '-' and a colon-separated list of\n  // negative patterns (tests to exclude).  A test is run if it\n  // matches one of the positive patterns and does not match any of\n  // the negative patterns.\n  //\n  // For example, *A*:Foo.* is a filter that matches any string that\n  // contains the character 'A' or starts with \"Foo.\".\n  bool should_run() const { return should_run_; }\n\n  // Returns true iff this test will appear in the XML report.\n  bool is_reportable() const {\n    // For now, the XML report includes all tests matching the filter.\n    // In the future, we may trim tests that are excluded because of\n    // sharding.\n    return matches_filter_;\n  }\n\n  // Returns the result of the test.\n  const TestResult* result() const { return &result_; }\n\n private:\n#if GTEST_HAS_DEATH_TEST\n  friend class internal::DefaultDeathTestFactory;\n#endif  // GTEST_HAS_DEATH_TEST\n  friend class Test;\n  friend class TestCase;\n  friend class internal::UnitTestImpl;\n  friend class internal::StreamingListenerTest;\n  friend TestInfo* internal::MakeAndRegisterTestInfo(\n      const char* test_case_name,\n      const char* name,\n      const char* type_param,\n      const char* value_param,\n      internal::CodeLocation code_location,\n      internal::TypeId fixture_class_id,\n      Test::SetUpTestCaseFunc set_up_tc,\n      Test::TearDownTestCaseFunc tear_down_tc,\n      internal::TestFactoryBase* factory);\n\n  // Constructs a TestInfo object. The newly constructed instance assumes\n  // ownership of the factory object.\n  TestInfo(const std::string& test_case_name,\n           const std::string& name,\n           const char* a_type_param,   // NULL if not a type-parameterized test\n           const char* a_value_param,  // NULL if not a value-parameterized test\n           internal::CodeLocation a_code_location,\n           internal::TypeId fixture_class_id,\n           internal::TestFactoryBase* factory);\n\n  // Increments the number of death tests encountered in this test so\n  // far.\n  int increment_death_test_count() {\n    return result_.increment_death_test_count();\n  }\n\n  // Creates the test object, runs it, records its result, and then\n  // deletes it.\n  void Run();\n\n  static void ClearTestResult(TestInfo* test_info) {\n    test_info->result_.Clear();\n  }\n\n  // These fields are immutable properties of the test.\n  const std::string test_case_name_;     // Test case name\n  const std::string name_;               // Test name\n  // Name of the parameter type, or NULL if this is not a typed or a\n  // type-parameterized test.\n  const internal::scoped_ptr<const ::std::string> type_param_;\n  // Text representation of the value parameter, or NULL if this is not a\n  // value-parameterized test.\n  const internal::scoped_ptr<const ::std::string> value_param_;\n  internal::CodeLocation location_;\n  const internal::TypeId fixture_class_id_;   // ID of the test fixture class\n  bool should_run_;                 // True iff this test should run\n  bool is_disabled_;                // True iff this test is disabled\n  bool matches_filter_;             // True if this test matches the\n                                    // user-specified filter.\n  internal::TestFactoryBase* const factory_;  // The factory that creates\n                                              // the test object\n\n  // This field is mutable and needs to be reset before running the\n  // test for the second time.\n  TestResult result_;\n\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(TestInfo);\n};\n\n// A test case, which consists of a vector of TestInfos.\n//\n// TestCase is not copyable.\nclass GTEST_API_ TestCase {\n public:\n  // Creates a TestCase with the given name.\n  //\n  // TestCase does NOT have a default constructor.  Always use this\n  // constructor to create a TestCase object.\n  //\n  // Arguments:\n  //\n  //   name:         name of the test case\n  //   a_type_param: the name of the test's type parameter, or NULL if\n  //                 this is not a type-parameterized test.\n  //   set_up_tc:    pointer to the function that sets up the test case\n  //   tear_down_tc: pointer to the function that tears down the test case\n  TestCase(const char* name, const char* a_type_param,\n           Test::SetUpTestCaseFunc set_up_tc,\n           Test::TearDownTestCaseFunc tear_down_tc);\n\n  // Destructor of TestCase.\n  virtual ~TestCase();\n\n  // Gets the name of the TestCase.\n  const char* name() const { return name_.c_str(); }\n\n  // Returns the name of the parameter type, or NULL if this is not a\n  // type-parameterized test case.\n  const char* type_param() const {\n    if (type_param_.get() != NULL)\n      return type_param_->c_str();\n    return NULL;\n  }\n\n  // Returns true if any test in this test case should run.\n  bool should_run() const { return should_run_; }\n\n  // Gets the number of successful tests in this test case.\n  int successful_test_count() const;\n\n  // Gets the number of failed tests in this test case.\n  int failed_test_count() const;\n\n  // Gets the number of disabled tests that will be reported in the XML report.\n  int reportable_disabled_test_count() const;\n\n  // Gets the number of disabled tests in this test case.\n  int disabled_test_count() const;\n\n  // Gets the number of tests to be printed in the XML report.\n  int reportable_test_count() const;\n\n  // Get the number of tests in this test case that should run.\n  int test_to_run_count() const;\n\n  // Gets the number of all tests in this test case.\n  int total_test_count() const;\n\n  // Returns true iff the test case passed.\n  bool Passed() const { return !Failed(); }\n\n  // Returns true iff the test case failed.\n  bool Failed() const { return failed_test_count() > 0; }\n\n  // Returns the elapsed time, in milliseconds.\n  TimeInMillis elapsed_time() const { return elapsed_time_; }\n\n  // Returns the i-th test among all the tests. i can range from 0 to\n  // total_test_count() - 1. If i is not in that range, returns NULL.\n  const TestInfo* GetTestInfo(int i) const;\n\n  // Returns the TestResult that holds test properties recorded during\n  // execution of SetUpTestCase and TearDownTestCase.\n  const TestResult& ad_hoc_test_result() const { return ad_hoc_test_result_; }\n\n private:\n  friend class Test;\n  friend class internal::UnitTestImpl;\n\n  // Gets the (mutable) vector of TestInfos in this TestCase.\n  std::vector<TestInfo*>& test_info_list() { return test_info_list_; }\n\n  // Gets the (immutable) vector of TestInfos in this TestCase.\n  const std::vector<TestInfo*>& test_info_list() const {\n    return test_info_list_;\n  }\n\n  // Returns the i-th test among all the tests. i can range from 0 to\n  // total_test_count() - 1. If i is not in that range, returns NULL.\n  TestInfo* GetMutableTestInfo(int i);\n\n  // Sets the should_run member.\n  void set_should_run(bool should) { should_run_ = should; }\n\n  // Adds a TestInfo to this test case.  Will delete the TestInfo upon\n  // destruction of the TestCase object.\n  void AddTestInfo(TestInfo * test_info);\n\n  // Clears the results of all tests in this test case.\n  void ClearResult();\n\n  // Clears the results of all tests in the given test case.\n  static void ClearTestCaseResult(TestCase* test_case) {\n    test_case->ClearResult();\n  }\n\n  // Runs every test in this TestCase.\n  void Run();\n\n  // Runs SetUpTestCase() for this TestCase.  This wrapper is needed\n  // for catching exceptions thrown from SetUpTestCase().\n  void RunSetUpTestCase() { (*set_up_tc_)(); }\n\n  // Runs TearDownTestCase() for this TestCase.  This wrapper is\n  // needed for catching exceptions thrown from TearDownTestCase().\n  void RunTearDownTestCase() { (*tear_down_tc_)(); }\n\n  // Returns true iff test passed.\n  static bool TestPassed(const TestInfo* test_info) {\n    return test_info->should_run() && test_info->result()->Passed();\n  }\n\n  // Returns true iff test failed.\n  static bool TestFailed(const TestInfo* test_info) {\n    return test_info->should_run() && test_info->result()->Failed();\n  }\n\n  // Returns true iff the test is disabled and will be reported in the XML\n  // report.\n  static bool TestReportableDisabled(const TestInfo* test_info) {\n    return test_info->is_reportable() && test_info->is_disabled_;\n  }\n\n  // Returns true iff test is disabled.\n  static bool TestDisabled(const TestInfo* test_info) {\n    return test_info->is_disabled_;\n  }\n\n  // Returns true iff this test will appear in the XML report.\n  static bool TestReportable(const TestInfo* test_info) {\n    return test_info->is_reportable();\n  }\n\n  // Returns true if the given test should run.\n  static bool ShouldRunTest(const TestInfo* test_info) {\n    return test_info->should_run();\n  }\n\n  // Shuffles the tests in this test case.\n  void ShuffleTests(internal::Random* random);\n\n  // Restores the test order to before the first shuffle.\n  void UnshuffleTests();\n\n  // Name of the test case.\n  std::string name_;\n  // Name of the parameter type, or NULL if this is not a typed or a\n  // type-parameterized test.\n  const internal::scoped_ptr<const ::std::string> type_param_;\n  // The vector of TestInfos in their original order.  It owns the\n  // elements in the vector.\n  std::vector<TestInfo*> test_info_list_;\n  // Provides a level of indirection for the test list to allow easy\n  // shuffling and restoring the test order.  The i-th element in this\n  // vector is the index of the i-th test in the shuffled test list.\n  std::vector<int> test_indices_;\n  // Pointer to the function that sets up the test case.\n  Test::SetUpTestCaseFunc set_up_tc_;\n  // Pointer to the function that tears down the test case.\n  Test::TearDownTestCaseFunc tear_down_tc_;\n  // True iff any test in this test case should run.\n  bool should_run_;\n  // Elapsed time, in milliseconds.\n  TimeInMillis elapsed_time_;\n  // Holds test properties recorded during execution of SetUpTestCase and\n  // TearDownTestCase.\n  TestResult ad_hoc_test_result_;\n\n  // We disallow copying TestCases.\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(TestCase);\n};\n\n// An Environment object is capable of setting up and tearing down an\n// environment.  You should subclass this to define your own\n// environment(s).\n//\n// An Environment object does the set-up and tear-down in virtual\n// methods SetUp() and TearDown() instead of the constructor and the\n// destructor, as:\n//\n//   1. You cannot safely throw from a destructor.  This is a problem\n//      as in some cases Google Test is used where exceptions are enabled, and\n//      we may want to implement ASSERT_* using exceptions where they are\n//      available.\n//   2. You cannot use ASSERT_* directly in a constructor or\n//      destructor.\nclass Environment {\n public:\n  // The d'tor is virtual as we need to subclass Environment.\n  virtual ~Environment() {}\n\n  // Override this to define how to set up the environment.\n  virtual void SetUp() {}\n\n  // Override this to define how to tear down the environment.\n  virtual void TearDown() {}\n private:\n  // If you see an error about overriding the following function or\n  // about it being private, you have mis-spelled SetUp() as Setup().\n  struct Setup_should_be_spelled_SetUp {};\n  virtual Setup_should_be_spelled_SetUp* Setup() { return NULL; }\n};\n\n// The interface for tracing execution of tests. The methods are organized in\n// the order the corresponding events are fired.\nclass TestEventListener {\n public:\n  virtual ~TestEventListener() {}\n\n  // Fired before any test activity starts.\n  virtual void OnTestProgramStart(const UnitTest& unit_test) = 0;\n\n  // Fired before each iteration of tests starts.  There may be more than\n  // one iteration if GTEST_FLAG(repeat) is set. iteration is the iteration\n  // index, starting from 0.\n  virtual void OnTestIterationStart(const UnitTest& unit_test,\n                                    int iteration) = 0;\n\n  // Fired before environment set-up for each iteration of tests starts.\n  virtual void OnEnvironmentsSetUpStart(const UnitTest& unit_test) = 0;\n\n  // Fired after environment set-up for each iteration of tests ends.\n  virtual void OnEnvironmentsSetUpEnd(const UnitTest& unit_test) = 0;\n\n  // Fired before the test case starts.\n  virtual void OnTestCaseStart(const TestCase& test_case) = 0;\n\n  // Fired before the test starts.\n  virtual void OnTestStart(const TestInfo& test_info) = 0;\n\n  // Fired after a failed assertion or a SUCCEED() invocation.\n  virtual void OnTestPartResult(const TestPartResult& test_part_result) = 0;\n\n  // Fired after the test ends.\n  virtual void OnTestEnd(const TestInfo& test_info) = 0;\n\n  // Fired after the test case ends.\n  virtual void OnTestCaseEnd(const TestCase& test_case) = 0;\n\n  // Fired before environment tear-down for each iteration of tests starts.\n  virtual void OnEnvironmentsTearDownStart(const UnitTest& unit_test) = 0;\n\n  // Fired after environment tear-down for each iteration of tests ends.\n  virtual void OnEnvironmentsTearDownEnd(const UnitTest& unit_test) = 0;\n\n  // Fired after each iteration of tests finishes.\n  virtual void OnTestIterationEnd(const UnitTest& unit_test,\n                                  int iteration) = 0;\n\n  // Fired after all test activities have ended.\n  virtual void OnTestProgramEnd(const UnitTest& unit_test) = 0;\n};\n\n// The convenience class for users who need to override just one or two\n// methods and are not concerned that a possible change to a signature of\n// the methods they override will not be caught during the build.  For\n// comments about each method please see the definition of TestEventListener\n// above.\nclass EmptyTestEventListener : public TestEventListener {\n public:\n  virtual void OnTestProgramStart(const UnitTest& /*unit_test*/) {}\n  virtual void OnTestIterationStart(const UnitTest& /*unit_test*/,\n                                    int /*iteration*/) {}\n  virtual void OnEnvironmentsSetUpStart(const UnitTest& /*unit_test*/) {}\n  virtual void OnEnvironmentsSetUpEnd(const UnitTest& /*unit_test*/) {}\n  virtual void OnTestCaseStart(const TestCase& /*test_case*/) {}\n  virtual void OnTestStart(const TestInfo& /*test_info*/) {}\n  virtual void OnTestPartResult(const TestPartResult& /*test_part_result*/) {}\n  virtual void OnTestEnd(const TestInfo& /*test_info*/) {}\n  virtual void OnTestCaseEnd(const TestCase& /*test_case*/) {}\n  virtual void OnEnvironmentsTearDownStart(const UnitTest& /*unit_test*/) {}\n  virtual void OnEnvironmentsTearDownEnd(const UnitTest& /*unit_test*/) {}\n  virtual void OnTestIterationEnd(const UnitTest& /*unit_test*/,\n                                  int /*iteration*/) {}\n  virtual void OnTestProgramEnd(const UnitTest& /*unit_test*/) {}\n};\n\n// TestEventListeners lets users add listeners to track events in Google Test.\nclass GTEST_API_ TestEventListeners {\n public:\n  TestEventListeners();\n  ~TestEventListeners();\n\n  // Appends an event listener to the end of the list. Google Test assumes\n  // the ownership of the listener (i.e. it will delete the listener when\n  // the test program finishes).\n  void Append(TestEventListener* listener);\n\n  // Removes the given event listener from the list and returns it.  It then\n  // becomes the caller's responsibility to delete the listener. Returns\n  // NULL if the listener is not found in the list.\n  TestEventListener* Release(TestEventListener* listener);\n\n  // Returns the standard listener responsible for the default console\n  // output.  Can be removed from the listeners list to shut down default\n  // console output.  Note that removing this object from the listener list\n  // with Release transfers its ownership to the caller and makes this\n  // function return NULL the next time.\n  TestEventListener* default_result_printer() const {\n    return default_result_printer_;\n  }\n\n  // Returns the standard listener responsible for the default XML output\n  // controlled by the --gtest_output=xml flag.  Can be removed from the\n  // listeners list by users who want to shut down the default XML output\n  // controlled by this flag and substitute it with custom one.  Note that\n  // removing this object from the listener list with Release transfers its\n  // ownership to the caller and makes this function return NULL the next\n  // time.\n  TestEventListener* default_xml_generator() const {\n    return default_xml_generator_;\n  }\n\n private:\n  friend class TestCase;\n  friend class TestInfo;\n  friend class internal::DefaultGlobalTestPartResultReporter;\n  friend class internal::NoExecDeathTest;\n  friend class internal::TestEventListenersAccessor;\n  friend class internal::UnitTestImpl;\n\n  // Returns repeater that broadcasts the TestEventListener events to all\n  // subscribers.\n  TestEventListener* repeater();\n\n  // Sets the default_result_printer attribute to the provided listener.\n  // The listener is also added to the listener list and previous\n  // default_result_printer is removed from it and deleted. The listener can\n  // also be NULL in which case it will not be added to the list. Does\n  // nothing if the previous and the current listener objects are the same.\n  void SetDefaultResultPrinter(TestEventListener* listener);\n\n  // Sets the default_xml_generator attribute to the provided listener.  The\n  // listener is also added to the listener list and previous\n  // default_xml_generator is removed from it and deleted. The listener can\n  // also be NULL in which case it will not be added to the list. Does\n  // nothing if the previous and the current listener objects are the same.\n  void SetDefaultXmlGenerator(TestEventListener* listener);\n\n  // Controls whether events will be forwarded by the repeater to the\n  // listeners in the list.\n  bool EventForwardingEnabled() const;\n  void SuppressEventForwarding();\n\n  // The actual list of listeners.\n  internal::TestEventRepeater* repeater_;\n  // Listener responsible for the standard result output.\n  TestEventListener* default_result_printer_;\n  // Listener responsible for the creation of the XML output file.\n  TestEventListener* default_xml_generator_;\n\n  // We disallow copying TestEventListeners.\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(TestEventListeners);\n};\n\n// A UnitTest consists of a vector of TestCases.\n//\n// This is a singleton class.  The only instance of UnitTest is\n// created when UnitTest::GetInstance() is first called.  This\n// instance is never deleted.\n//\n// UnitTest is not copyable.\n//\n// This class is thread-safe as long as the methods are called\n// according to their specification.\nclass GTEST_API_ UnitTest {\n public:\n  // Gets the singleton UnitTest object.  The first time this method\n  // is called, a UnitTest object is constructed and returned.\n  // Consecutive calls will return the same object.\n  static UnitTest* GetInstance();\n\n  // Runs all tests in this UnitTest object and prints the result.\n  // Returns 0 if successful, or 1 otherwise.\n  //\n  // This method can only be called from the main thread.\n  //\n  // INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\n  int Run() GTEST_MUST_USE_RESULT_;\n\n  // Returns the working directory when the first TEST() or TEST_F()\n  // was executed.  The UnitTest object owns the string.\n  const char* original_working_dir() const;\n\n  // Returns the TestCase object for the test that's currently running,\n  // or NULL if no test is running.\n  const TestCase* current_test_case() const\n      GTEST_LOCK_EXCLUDED_(mutex_);\n\n  // Returns the TestInfo object for the test that's currently running,\n  // or NULL if no test is running.\n  const TestInfo* current_test_info() const\n      GTEST_LOCK_EXCLUDED_(mutex_);\n\n  // Returns the random seed used at the start of the current test run.\n  int random_seed() const;\n\n#if GTEST_HAS_PARAM_TEST\n  // Returns the ParameterizedTestCaseRegistry object used to keep track of\n  // value-parameterized tests and instantiate and register them.\n  //\n  // INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\n  internal::ParameterizedTestCaseRegistry& parameterized_test_registry()\n      GTEST_LOCK_EXCLUDED_(mutex_);\n#endif  // GTEST_HAS_PARAM_TEST\n\n  // Gets the number of successful test cases.\n  int successful_test_case_count() const;\n\n  // Gets the number of failed test cases.\n  int failed_test_case_count() const;\n\n  // Gets the number of all test cases.\n  int total_test_case_count() const;\n\n  // Gets the number of all test cases that contain at least one test\n  // that should run.\n  int test_case_to_run_count() const;\n\n  // Gets the number of successful tests.\n  int successful_test_count() const;\n\n  // Gets the number of failed tests.\n  int failed_test_count() const;\n\n  // Gets the number of disabled tests that will be reported in the XML report.\n  int reportable_disabled_test_count() const;\n\n  // Gets the number of disabled tests.\n  int disabled_test_count() const;\n\n  // Gets the number of tests to be printed in the XML report.\n  int reportable_test_count() const;\n\n  // Gets the number of all tests.\n  int total_test_count() const;\n\n  // Gets the number of tests that should run.\n  int test_to_run_count() const;\n\n  // Gets the time of the test program start, in ms from the start of the\n  // UNIX epoch.\n  TimeInMillis start_timestamp() const;\n\n  // Gets the elapsed time, in milliseconds.\n  TimeInMillis elapsed_time() const;\n\n  // Returns true iff the unit test passed (i.e. all test cases passed).\n  bool Passed() const;\n\n  // Returns true iff the unit test failed (i.e. some test case failed\n  // or something outside of all tests failed).\n  bool Failed() const;\n\n  // Gets the i-th test case among all the test cases. i can range from 0 to\n  // total_test_case_count() - 1. If i is not in that range, returns NULL.\n  const TestCase* GetTestCase(int i) const;\n\n  // Returns the TestResult containing information on test failures and\n  // properties logged outside of individual test cases.\n  const TestResult& ad_hoc_test_result() const;\n\n  // Returns the list of event listeners that can be used to track events\n  // inside Google Test.\n  TestEventListeners& listeners();\n\n private:\n  // Registers and returns a global test environment.  When a test\n  // program is run, all global test environments will be set-up in\n  // the order they were registered.  After all tests in the program\n  // have finished, all global test environments will be torn-down in\n  // the *reverse* order they were registered.\n  //\n  // The UnitTest object takes ownership of the given environment.\n  //\n  // This method can only be called from the main thread.\n  Environment* AddEnvironment(Environment* env);\n\n  // Adds a TestPartResult to the current TestResult object.  All\n  // Google Test assertion macros (e.g. ASSERT_TRUE, EXPECT_EQ, etc)\n  // eventually call this to report their results.  The user code\n  // should use the assertion macros instead of calling this directly.\n  void AddTestPartResult(TestPartResult::Type result_type,\n                         const char* file_name,\n                         int line_number,\n                         const std::string& message,\n                         const std::string& os_stack_trace)\n      GTEST_LOCK_EXCLUDED_(mutex_);\n\n  // Adds a TestProperty to the current TestResult object when invoked from\n  // inside a test, to current TestCase's ad_hoc_test_result_ when invoked\n  // from SetUpTestCase or TearDownTestCase, or to the global property set\n  // when invoked elsewhere.  If the result already contains a property with\n  // the same key, the value will be updated.\n  void RecordProperty(const std::string& key, const std::string& value);\n\n  // Gets the i-th test case among all the test cases. i can range from 0 to\n  // total_test_case_count() - 1. If i is not in that range, returns NULL.\n  TestCase* GetMutableTestCase(int i);\n\n  // Accessors for the implementation object.\n  internal::UnitTestImpl* impl() { return impl_; }\n  const internal::UnitTestImpl* impl() const { return impl_; }\n\n  // These classes and funcions are friends as they need to access private\n  // members of UnitTest.\n  friend class Test;\n  friend class internal::AssertHelper;\n  friend class internal::ScopedTrace;\n  friend class internal::StreamingListenerTest;\n  friend class internal::UnitTestRecordPropertyTestHelper;\n  friend Environment* AddGlobalTestEnvironment(Environment* env);\n  friend internal::UnitTestImpl* internal::GetUnitTestImpl();\n  friend void internal::ReportFailureInUnknownLocation(\n      TestPartResult::Type result_type,\n      const std::string& message);\n\n  // Creates an empty UnitTest.\n  UnitTest();\n\n  // D'tor\n  virtual ~UnitTest();\n\n  // Pushes a trace defined by SCOPED_TRACE() on to the per-thread\n  // Google Test trace stack.\n  void PushGTestTrace(const internal::TraceInfo& trace)\n      GTEST_LOCK_EXCLUDED_(mutex_);\n\n  // Pops a trace from the per-thread Google Test trace stack.\n  void PopGTestTrace()\n      GTEST_LOCK_EXCLUDED_(mutex_);\n\n  // Protects mutable state in *impl_.  This is mutable as some const\n  // methods need to lock it too.\n  mutable internal::Mutex mutex_;\n\n  // Opaque implementation object.  This field is never changed once\n  // the object is constructed.  We don't mark it as const here, as\n  // doing so will cause a warning in the constructor of UnitTest.\n  // Mutable state in *impl_ is protected by mutex_.\n  internal::UnitTestImpl* impl_;\n\n  // We disallow copying UnitTest.\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(UnitTest);\n};\n\n// A convenient wrapper for adding an environment for the test\n// program.\n//\n// You should call this before RUN_ALL_TESTS() is called, probably in\n// main().  If you use gtest_main, you need to call this before main()\n// starts for it to take effect.  For example, you can define a global\n// variable like this:\n//\n//   testing::Environment* const foo_env =\n//       testing::AddGlobalTestEnvironment(new FooEnvironment);\n//\n// However, we strongly recommend you to write your own main() and\n// call AddGlobalTestEnvironment() there, as relying on initialization\n// of global variables makes the code harder to read and may cause\n// problems when you register multiple environments from different\n// translation units and the environments have dependencies among them\n// (remember that the compiler doesn't guarantee the order in which\n// global variables from different translation units are initialized).\ninline Environment* AddGlobalTestEnvironment(Environment* env) {\n  return UnitTest::GetInstance()->AddEnvironment(env);\n}\n\n// Initializes Google Test.  This must be called before calling\n// RUN_ALL_TESTS().  In particular, it parses a command line for the\n// flags that Google Test recognizes.  Whenever a Google Test flag is\n// seen, it is removed from argv, and *argc is decremented.\n//\n// No value is returned.  Instead, the Google Test flag variables are\n// updated.\n//\n// Calling the function for the second time has no user-visible effect.\nGTEST_API_ void InitGoogleTest(int* argc, char** argv);\n\n// This overloaded version can be used in Windows programs compiled in\n// UNICODE mode.\nGTEST_API_ void InitGoogleTest(int* argc, wchar_t** argv);\n\nnamespace internal {\n\n// Separate the error generating code from the code path to reduce the stack\n// frame size of CmpHelperEQ. This helps reduce the overhead of some sanitizers\n// when calling EXPECT_* in a tight loop.\ntemplate <typename T1, typename T2>\nAssertionResult CmpHelperEQFailure(const char* lhs_expression,\n                                   const char* rhs_expression,\n                                   const T1& lhs, const T2& rhs) {\n  return EqFailure(lhs_expression,\n                   rhs_expression,\n                   FormatForComparisonFailureMessage(lhs, rhs),\n                   FormatForComparisonFailureMessage(rhs, lhs),\n                   false);\n}\n\n// The helper function for {ASSERT|EXPECT}_EQ.\ntemplate <typename T1, typename T2>\nAssertionResult CmpHelperEQ(const char* lhs_expression,\n                            const char* rhs_expression,\n                            const T1& lhs,\n                            const T2& rhs) {\nGTEST_DISABLE_MSC_WARNINGS_PUSH_(4389 /* signed/unsigned mismatch */)\n  if (lhs == rhs) {\n    return AssertionSuccess();\n  }\nGTEST_DISABLE_MSC_WARNINGS_POP_()\n\n  return CmpHelperEQFailure(lhs_expression, rhs_expression, lhs, rhs);\n}\n\n// With this overloaded version, we allow anonymous enums to be used\n// in {ASSERT|EXPECT}_EQ when compiled with gcc 4, as anonymous enums\n// can be implicitly cast to BiggestInt.\nGTEST_API_ AssertionResult CmpHelperEQ(const char* lhs_expression,\n                                       const char* rhs_expression,\n                                       BiggestInt lhs,\n                                       BiggestInt rhs);\n\n// The helper class for {ASSERT|EXPECT}_EQ.  The template argument\n// lhs_is_null_literal is true iff the first argument to ASSERT_EQ()\n// is a null pointer literal.  The following default implementation is\n// for lhs_is_null_literal being false.\ntemplate <bool lhs_is_null_literal>\nclass EqHelper {\n public:\n  // This templatized version is for the general case.\n  template <typename T1, typename T2>\n  static AssertionResult Compare(const char* lhs_expression,\n                                 const char* rhs_expression,\n                                 const T1& lhs,\n                                 const T2& rhs) {\n    return CmpHelperEQ(lhs_expression, rhs_expression, lhs, rhs);\n  }\n\n  // With this overloaded version, we allow anonymous enums to be used\n  // in {ASSERT|EXPECT}_EQ when compiled with gcc 4, as anonymous\n  // enums can be implicitly cast to BiggestInt.\n  //\n  // Even though its body looks the same as the above version, we\n  // cannot merge the two, as it will make anonymous enums unhappy.\n  static AssertionResult Compare(const char* lhs_expression,\n                                 const char* rhs_expression,\n                                 BiggestInt lhs,\n                                 BiggestInt rhs) {\n    return CmpHelperEQ(lhs_expression, rhs_expression, lhs, rhs);\n  }\n};\n\n// This specialization is used when the first argument to ASSERT_EQ()\n// is a null pointer literal, like NULL, false, or 0.\ntemplate <>\nclass EqHelper<true> {\n public:\n  // We define two overloaded versions of Compare().  The first\n  // version will be picked when the second argument to ASSERT_EQ() is\n  // NOT a pointer, e.g. ASSERT_EQ(0, AnIntFunction()) or\n  // EXPECT_EQ(false, a_bool).\n  template <typename T1, typename T2>\n  static AssertionResult Compare(\n      const char* lhs_expression,\n      const char* rhs_expression,\n      const T1& lhs,\n      const T2& rhs,\n      // The following line prevents this overload from being considered if T2\n      // is not a pointer type.  We need this because ASSERT_EQ(NULL, my_ptr)\n      // expands to Compare(\"\", \"\", NULL, my_ptr), which requires a conversion\n      // to match the Secret* in the other overload, which would otherwise make\n      // this template match better.\n      typename EnableIf<!is_pointer<T2>::value>::type* = 0) {\n    return CmpHelperEQ(lhs_expression, rhs_expression, lhs, rhs);\n  }\n\n  // This version will be picked when the second argument to ASSERT_EQ() is a\n  // pointer, e.g. ASSERT_EQ(NULL, a_pointer).\n  template <typename T>\n  static AssertionResult Compare(\n      const char* lhs_expression,\n      const char* rhs_expression,\n      // We used to have a second template parameter instead of Secret*.  That\n      // template parameter would deduce to 'long', making this a better match\n      // than the first overload even without the first overload's EnableIf.\n      // Unfortunately, gcc with -Wconversion-null warns when \"passing NULL to\n      // non-pointer argument\" (even a deduced integral argument), so the old\n      // implementation caused warnings in user code.\n      Secret* /* lhs (NULL) */,\n      T* rhs) {\n    // We already know that 'lhs' is a null pointer.\n    return CmpHelperEQ(lhs_expression, rhs_expression,\n                       static_cast<T*>(NULL), rhs);\n  }\n};\n\n// Separate the error generating code from the code path to reduce the stack\n// frame size of CmpHelperOP. This helps reduce the overhead of some sanitizers\n// when calling EXPECT_OP in a tight loop.\ntemplate <typename T1, typename T2>\nAssertionResult CmpHelperOpFailure(const char* expr1, const char* expr2,\n                                   const T1& val1, const T2& val2,\n                                   const char* op) {\n  return AssertionFailure()\n         << \"Expected: (\" << expr1 << \") \" << op << \" (\" << expr2\n         << \"), actual: \" << FormatForComparisonFailureMessage(val1, val2)\n         << \" vs \" << FormatForComparisonFailureMessage(val2, val1);\n}\n\n// A macro for implementing the helper functions needed to implement\n// ASSERT_?? and EXPECT_??.  It is here just to avoid copy-and-paste\n// of similar code.\n//\n// For each templatized helper function, we also define an overloaded\n// version for BiggestInt in order to reduce code bloat and allow\n// anonymous enums to be used with {ASSERT|EXPECT}_?? when compiled\n// with gcc 4.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\n\n#define GTEST_IMPL_CMP_HELPER_(op_name, op)\\\ntemplate <typename T1, typename T2>\\\nAssertionResult CmpHelper##op_name(const char* expr1, const char* expr2, \\\n                                   const T1& val1, const T2& val2) {\\\n  if (val1 op val2) {\\\n    return AssertionSuccess();\\\n  } else {\\\n    return CmpHelperOpFailure(expr1, expr2, val1, val2, #op);\\\n  }\\\n}\\\nGTEST_API_ AssertionResult CmpHelper##op_name(\\\n    const char* expr1, const char* expr2, BiggestInt val1, BiggestInt val2)\n\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\n\n// Implements the helper function for {ASSERT|EXPECT}_NE\nGTEST_IMPL_CMP_HELPER_(NE, !=);\n// Implements the helper function for {ASSERT|EXPECT}_LE\nGTEST_IMPL_CMP_HELPER_(LE, <=);\n// Implements the helper function for {ASSERT|EXPECT}_LT\nGTEST_IMPL_CMP_HELPER_(LT, <);\n// Implements the helper function for {ASSERT|EXPECT}_GE\nGTEST_IMPL_CMP_HELPER_(GE, >=);\n// Implements the helper function for {ASSERT|EXPECT}_GT\nGTEST_IMPL_CMP_HELPER_(GT, >);\n\n#undef GTEST_IMPL_CMP_HELPER_\n\n// The helper function for {ASSERT|EXPECT}_STREQ.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\nGTEST_API_ AssertionResult CmpHelperSTREQ(const char* s1_expression,\n                                          const char* s2_expression,\n                                          const char* s1,\n                                          const char* s2);\n\n// The helper function for {ASSERT|EXPECT}_STRCASEEQ.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\nGTEST_API_ AssertionResult CmpHelperSTRCASEEQ(const char* s1_expression,\n                                              const char* s2_expression,\n                                              const char* s1,\n                                              const char* s2);\n\n// The helper function for {ASSERT|EXPECT}_STRNE.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\nGTEST_API_ AssertionResult CmpHelperSTRNE(const char* s1_expression,\n                                          const char* s2_expression,\n                                          const char* s1,\n                                          const char* s2);\n\n// The helper function for {ASSERT|EXPECT}_STRCASENE.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\nGTEST_API_ AssertionResult CmpHelperSTRCASENE(const char* s1_expression,\n                                              const char* s2_expression,\n                                              const char* s1,\n                                              const char* s2);\n\n\n// Helper function for *_STREQ on wide strings.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\nGTEST_API_ AssertionResult CmpHelperSTREQ(const char* s1_expression,\n                                          const char* s2_expression,\n                                          const wchar_t* s1,\n                                          const wchar_t* s2);\n\n// Helper function for *_STRNE on wide strings.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\nGTEST_API_ AssertionResult CmpHelperSTRNE(const char* s1_expression,\n                                          const char* s2_expression,\n                                          const wchar_t* s1,\n                                          const wchar_t* s2);\n\n}  // namespace internal\n\n// IsSubstring() and IsNotSubstring() are intended to be used as the\n// first argument to {EXPECT,ASSERT}_PRED_FORMAT2(), not by\n// themselves.  They check whether needle is a substring of haystack\n// (NULL is considered a substring of itself only), and return an\n// appropriate error message when they fail.\n//\n// The {needle,haystack}_expr arguments are the stringified\n// expressions that generated the two real arguments.\nGTEST_API_ AssertionResult IsSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const char* needle, const char* haystack);\nGTEST_API_ AssertionResult IsSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const wchar_t* needle, const wchar_t* haystack);\nGTEST_API_ AssertionResult IsNotSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const char* needle, const char* haystack);\nGTEST_API_ AssertionResult IsNotSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const wchar_t* needle, const wchar_t* haystack);\nGTEST_API_ AssertionResult IsSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const ::std::string& needle, const ::std::string& haystack);\nGTEST_API_ AssertionResult IsNotSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const ::std::string& needle, const ::std::string& haystack);\n\n#if GTEST_HAS_STD_WSTRING\nGTEST_API_ AssertionResult IsSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const ::std::wstring& needle, const ::std::wstring& haystack);\nGTEST_API_ AssertionResult IsNotSubstring(\n    const char* needle_expr, const char* haystack_expr,\n    const ::std::wstring& needle, const ::std::wstring& haystack);\n#endif  // GTEST_HAS_STD_WSTRING\n\nnamespace internal {\n\n// Helper template function for comparing floating-points.\n//\n// Template parameter:\n//\n//   RawType: the raw floating-point type (either float or double)\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\ntemplate <typename RawType>\nAssertionResult CmpHelperFloatingPointEQ(const char* lhs_expression,\n                                         const char* rhs_expression,\n                                         RawType lhs_value,\n                                         RawType rhs_value) {\n  const FloatingPoint<RawType> lhs(lhs_value), rhs(rhs_value);\n\n  if (lhs.AlmostEquals(rhs)) {\n    return AssertionSuccess();\n  }\n\n  ::std::stringstream lhs_ss;\n  lhs_ss << std::setprecision(std::numeric_limits<RawType>::digits10 + 2)\n         << lhs_value;\n\n  ::std::stringstream rhs_ss;\n  rhs_ss << std::setprecision(std::numeric_limits<RawType>::digits10 + 2)\n         << rhs_value;\n\n  return EqFailure(lhs_expression,\n                   rhs_expression,\n                   StringStreamToString(&lhs_ss),\n                   StringStreamToString(&rhs_ss),\n                   false);\n}\n\n// Helper function for implementing ASSERT_NEAR.\n//\n// INTERNAL IMPLEMENTATION - DO NOT USE IN A USER PROGRAM.\nGTEST_API_ AssertionResult DoubleNearPredFormat(const char* expr1,\n                                                const char* expr2,\n                                                const char* abs_error_expr,\n                                                double val1,\n                                                double val2,\n                                                double abs_error);\n\n// INTERNAL IMPLEMENTATION - DO NOT USE IN USER CODE.\n// A class that enables one to stream messages to assertion macros\nclass GTEST_API_ AssertHelper {\n public:\n  // Constructor.\n  AssertHelper(TestPartResult::Type type,\n               const char* file,\n               int line,\n               const char* message);\n  ~AssertHelper();\n\n  // Message assignment is a semantic trick to enable assertion\n  // streaming; see the GTEST_MESSAGE_ macro below.\n  void operator=(const Message& message) const;\n\n private:\n  // We put our data in a struct so that the size of the AssertHelper class can\n  // be as small as possible.  This is important because gcc is incapable of\n  // re-using stack space even for temporary variables, so every EXPECT_EQ\n  // reserves stack space for another AssertHelper.\n  struct AssertHelperData {\n    AssertHelperData(TestPartResult::Type t,\n                     const char* srcfile,\n                     int line_num,\n                     const char* msg)\n        : type(t), file(srcfile), line(line_num), message(msg) { }\n\n    TestPartResult::Type const type;\n    const char* const file;\n    int const line;\n    std::string const message;\n\n   private:\n    GTEST_DISALLOW_COPY_AND_ASSIGN_(AssertHelperData);\n  };\n\n  AssertHelperData* const data_;\n\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(AssertHelper);\n};\n\n}  // namespace internal\n\n#if GTEST_HAS_PARAM_TEST\n// The pure interface class that all value-parameterized tests inherit from.\n// A value-parameterized class must inherit from both ::testing::Test and\n// ::testing::WithParamInterface. In most cases that just means inheriting\n// from ::testing::TestWithParam, but more complicated test hierarchies\n// may need to inherit from Test and WithParamInterface at different levels.\n//\n// This interface has support for accessing the test parameter value via\n// the GetParam() method.\n//\n// Use it with one of the parameter generator defining functions, like Range(),\n// Values(), ValuesIn(), Bool(), and Combine().\n//\n// class FooTest : public ::testing::TestWithParam<int> {\n//  protected:\n//   FooTest() {\n//     // Can use GetParam() here.\n//   }\n//   virtual ~FooTest() {\n//     // Can use GetParam() here.\n//   }\n//   virtual void SetUp() {\n//     // Can use GetParam() here.\n//   }\n//   virtual void TearDown {\n//     // Can use GetParam() here.\n//   }\n// };\n// TEST_P(FooTest, DoesBar) {\n//   // Can use GetParam() method here.\n//   Foo foo;\n//   ASSERT_TRUE(foo.DoesBar(GetParam()));\n// }\n// INSTANTIATE_TEST_CASE_P(OneToTenRange, FooTest, ::testing::Range(1, 10));\n\ntemplate <typename T>\nclass WithParamInterface {\n public:\n  typedef T ParamType;\n  virtual ~WithParamInterface() {}\n\n  // The current parameter value. Is also available in the test fixture's\n  // constructor. This member function is non-static, even though it only\n  // references static data, to reduce the opportunity for incorrect uses\n  // like writing 'WithParamInterface<bool>::GetParam()' for a test that\n  // uses a fixture whose parameter type is int.\n  const ParamType& GetParam() const {\n    GTEST_CHECK_(parameter_ != NULL)\n        << \"GetParam() can only be called inside a value-parameterized test \"\n        << \"-- did you intend to write TEST_P instead of TEST_F?\";\n    return *parameter_;\n  }\n\n private:\n  // Sets parameter value. The caller is responsible for making sure the value\n  // remains alive and unchanged throughout the current test.\n  static void SetParam(const ParamType* parameter) {\n    parameter_ = parameter;\n  }\n\n  // Static value used for accessing parameter during a test lifetime.\n  static const ParamType* parameter_;\n\n  // TestClass must be a subclass of WithParamInterface<T> and Test.\n  template <class TestClass> friend class internal::ParameterizedTestFactory;\n};\n\ntemplate <typename T>\nconst T* WithParamInterface<T>::parameter_ = NULL;\n\n// Most value-parameterized classes can ignore the existence of\n// WithParamInterface, and can just inherit from ::testing::TestWithParam.\n\ntemplate <typename T>\nclass TestWithParam : public Test, public WithParamInterface<T> {\n};\n\n#endif  // GTEST_HAS_PARAM_TEST\n\n// Macros for indicating success/failure in test code.\n\n// ADD_FAILURE unconditionally adds a failure to the current test.\n// SUCCEED generates a success - it doesn't automatically make the\n// current test successful, as a test is only successful when it has\n// no failure.\n//\n// EXPECT_* verifies that a certain condition is satisfied.  If not,\n// it behaves like ADD_FAILURE.  In particular:\n//\n//   EXPECT_TRUE  verifies that a Boolean condition is true.\n//   EXPECT_FALSE verifies that a Boolean condition is false.\n//\n// FAIL and ASSERT_* are similar to ADD_FAILURE and EXPECT_*, except\n// that they will also abort the current function on failure.  People\n// usually want the fail-fast behavior of FAIL and ASSERT_*, but those\n// writing data-driven tests often find themselves using ADD_FAILURE\n// and EXPECT_* more.\n\n// Generates a nonfatal failure with a generic message.\n#define ADD_FAILURE() GTEST_NONFATAL_FAILURE_(\"Failed\")\n\n// Generates a nonfatal failure at the given source file location with\n// a generic message.\n#define ADD_FAILURE_AT(file, line) \\\n  GTEST_MESSAGE_AT_(file, line, \"Failed\", \\\n                    ::testing::TestPartResult::kNonFatalFailure)\n\n// Generates a fatal failure with a generic message.\n#define GTEST_FAIL() GTEST_FATAL_FAILURE_(\"Failed\")\n\n// Define this macro to 1 to omit the definition of FAIL(), which is a\n// generic name and clashes with some other libraries.\n#if !GTEST_DONT_DEFINE_FAIL\n# define FAIL() GTEST_FAIL()\n#endif\n\n// Generates a success with a generic message.\n#define GTEST_SUCCEED() GTEST_SUCCESS_(\"Succeeded\")\n\n// Define this macro to 1 to omit the definition of SUCCEED(), which\n// is a generic name and clashes with some other libraries.\n#if !GTEST_DONT_DEFINE_SUCCEED\n# define SUCCEED() GTEST_SUCCEED()\n#endif\n\n// Macros for testing exceptions.\n//\n//    * {ASSERT|EXPECT}_THROW(statement, expected_exception):\n//         Tests that the statement throws the expected exception.\n//    * {ASSERT|EXPECT}_NO_THROW(statement):\n//         Tests that the statement doesn't throw any exception.\n//    * {ASSERT|EXPECT}_ANY_THROW(statement):\n//         Tests that the statement throws an exception.\n\n#define EXPECT_THROW(statement, expected_exception) \\\n  GTEST_TEST_THROW_(statement, expected_exception, GTEST_NONFATAL_FAILURE_)\n#define EXPECT_NO_THROW(statement) \\\n  GTEST_TEST_NO_THROW_(statement, GTEST_NONFATAL_FAILURE_)\n#define EXPECT_ANY_THROW(statement) \\\n  GTEST_TEST_ANY_THROW_(statement, GTEST_NONFATAL_FAILURE_)\n#define ASSERT_THROW(statement, expected_exception) \\\n  GTEST_TEST_THROW_(statement, expected_exception, GTEST_FATAL_FAILURE_)\n#define ASSERT_NO_THROW(statement) \\\n  GTEST_TEST_NO_THROW_(statement, GTEST_FATAL_FAILURE_)\n#define ASSERT_ANY_THROW(statement) \\\n  GTEST_TEST_ANY_THROW_(statement, GTEST_FATAL_FAILURE_)\n\n// Boolean assertions. Condition can be either a Boolean expression or an\n// AssertionResult. For more information on how to use AssertionResult with\n// these macros see comments on that class.\n#define EXPECT_TRUE(condition) \\\n  GTEST_TEST_BOOLEAN_((condition), #condition, false, true, \\\n                      GTEST_NONFATAL_FAILURE_)\n#define EXPECT_FALSE(condition) \\\n  GTEST_TEST_BOOLEAN_(!(condition), #condition, true, false, \\\n                      GTEST_NONFATAL_FAILURE_)\n#define ASSERT_TRUE(condition) \\\n  GTEST_TEST_BOOLEAN_((condition), #condition, false, true, \\\n                      GTEST_FATAL_FAILURE_)\n#define ASSERT_FALSE(condition) \\\n  GTEST_TEST_BOOLEAN_(!(condition), #condition, true, false, \\\n                      GTEST_FATAL_FAILURE_)\n\n// Includes the auto-generated header that implements a family of\n// generic predicate assertion macros.\n#include \"gtest/gtest_pred_impl.h\"\n\n// Macros for testing equalities and inequalities.\n//\n//    * {ASSERT|EXPECT}_EQ(v1, v2): Tests that v1 == v2\n//    * {ASSERT|EXPECT}_NE(v1, v2): Tests that v1 != v2\n//    * {ASSERT|EXPECT}_LT(v1, v2): Tests that v1 < v2\n//    * {ASSERT|EXPECT}_LE(v1, v2): Tests that v1 <= v2\n//    * {ASSERT|EXPECT}_GT(v1, v2): Tests that v1 > v2\n//    * {ASSERT|EXPECT}_GE(v1, v2): Tests that v1 >= v2\n//\n// When they are not, Google Test prints both the tested expressions and\n// their actual values.  The values must be compatible built-in types,\n// or you will get a compiler error.  By \"compatible\" we mean that the\n// values can be compared by the respective operator.\n//\n// Note:\n//\n//   1. It is possible to make a user-defined type work with\n//   {ASSERT|EXPECT}_??(), but that requires overloading the\n//   comparison operators and is thus discouraged by the Google C++\n//   Usage Guide.  Therefore, you are advised to use the\n//   {ASSERT|EXPECT}_TRUE() macro to assert that two objects are\n//   equal.\n//\n//   2. The {ASSERT|EXPECT}_??() macros do pointer comparisons on\n//   pointers (in particular, C strings).  Therefore, if you use it\n//   with two C strings, you are testing how their locations in memory\n//   are related, not how their content is related.  To compare two C\n//   strings by content, use {ASSERT|EXPECT}_STR*().\n//\n//   3. {ASSERT|EXPECT}_EQ(v1, v2) is preferred to\n//   {ASSERT|EXPECT}_TRUE(v1 == v2), as the former tells you\n//   what the actual value is when it fails, and similarly for the\n//   other comparisons.\n//\n//   4. Do not depend on the order in which {ASSERT|EXPECT}_??()\n//   evaluate their arguments, which is undefined.\n//\n//   5. These macros evaluate their arguments exactly once.\n//\n// Examples:\n//\n//   EXPECT_NE(5, Foo());\n//   EXPECT_EQ(NULL, a_pointer);\n//   ASSERT_LT(i, array_size);\n//   ASSERT_GT(records.size(), 0) << \"There is no record left.\";\n\n#define EXPECT_EQ(val1, val2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal:: \\\n                      EqHelper<GTEST_IS_NULL_LITERAL_(val1)>::Compare, \\\n                      val1, val2)\n#define EXPECT_NE(val1, val2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\n#define EXPECT_LE(val1, val2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperLE, val1, val2)\n#define EXPECT_LT(val1, val2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperLT, val1, val2)\n#define EXPECT_GE(val1, val2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperGE, val1, val2)\n#define EXPECT_GT(val1, val2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperGT, val1, val2)\n\n#define GTEST_ASSERT_EQ(val1, val2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal:: \\\n                      EqHelper<GTEST_IS_NULL_LITERAL_(val1)>::Compare, \\\n                      val1, val2)\n#define GTEST_ASSERT_NE(val1, val2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\n#define GTEST_ASSERT_LE(val1, val2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperLE, val1, val2)\n#define GTEST_ASSERT_LT(val1, val2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperLT, val1, val2)\n#define GTEST_ASSERT_GE(val1, val2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperGE, val1, val2)\n#define GTEST_ASSERT_GT(val1, val2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperGT, val1, val2)\n\n// Define macro GTEST_DONT_DEFINE_ASSERT_XY to 1 to omit the definition of\n// ASSERT_XY(), which clashes with some users' own code.\n\n#if !GTEST_DONT_DEFINE_ASSERT_EQ\n# define ASSERT_EQ(val1, val2) GTEST_ASSERT_EQ(val1, val2)\n#endif\n\n#if !GTEST_DONT_DEFINE_ASSERT_NE\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\n#endif\n\n#if !GTEST_DONT_DEFINE_ASSERT_LE\n# define ASSERT_LE(val1, val2) GTEST_ASSERT_LE(val1, val2)\n#endif\n\n#if !GTEST_DONT_DEFINE_ASSERT_LT\n# define ASSERT_LT(val1, val2) GTEST_ASSERT_LT(val1, val2)\n#endif\n\n#if !GTEST_DONT_DEFINE_ASSERT_GE\n# define ASSERT_GE(val1, val2) GTEST_ASSERT_GE(val1, val2)\n#endif\n\n#if !GTEST_DONT_DEFINE_ASSERT_GT\n# define ASSERT_GT(val1, val2) GTEST_ASSERT_GT(val1, val2)\n#endif\n\n// C-string Comparisons.  All tests treat NULL and any non-NULL string\n// as different.  Two NULLs are equal.\n//\n//    * {ASSERT|EXPECT}_STREQ(s1, s2):     Tests that s1 == s2\n//    * {ASSERT|EXPECT}_STRNE(s1, s2):     Tests that s1 != s2\n//    * {ASSERT|EXPECT}_STRCASEEQ(s1, s2): Tests that s1 == s2, ignoring case\n//    * {ASSERT|EXPECT}_STRCASENE(s1, s2): Tests that s1 != s2, ignoring case\n//\n// For wide or narrow string objects, you can use the\n// {ASSERT|EXPECT}_??() macros.\n//\n// Don't depend on the order in which the arguments are evaluated,\n// which is undefined.\n//\n// These macros evaluate their arguments exactly once.\n\n#define EXPECT_STREQ(s1, s2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperSTREQ, s1, s2)\n#define EXPECT_STRNE(s1, s2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperSTRNE, s1, s2)\n#define EXPECT_STRCASEEQ(s1, s2) \\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperSTRCASEEQ, s1, s2)\n#define EXPECT_STRCASENE(s1, s2)\\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperSTRCASENE, s1, s2)\n\n#define ASSERT_STREQ(s1, s2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperSTREQ, s1, s2)\n#define ASSERT_STRNE(s1, s2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperSTRNE, s1, s2)\n#define ASSERT_STRCASEEQ(s1, s2) \\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperSTRCASEEQ, s1, s2)\n#define ASSERT_STRCASENE(s1, s2)\\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperSTRCASENE, s1, s2)\n\n// Macros for comparing floating-point numbers.\n//\n//    * {ASSERT|EXPECT}_FLOAT_EQ(val1, val2):\n//         Tests that two float values are almost equal.\n//    * {ASSERT|EXPECT}_DOUBLE_EQ(val1, val2):\n//         Tests that two double values are almost equal.\n//    * {ASSERT|EXPECT}_NEAR(v1, v2, abs_error):\n//         Tests that v1 and v2 are within the given distance to each other.\n//\n// Google Test uses ULP-based comparison to automatically pick a default\n// error bound that is appropriate for the operands.  See the\n// FloatingPoint template class in gtest-internal.h if you are\n// interested in the implementation details.\n\n#define EXPECT_FLOAT_EQ(val1, val2)\\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperFloatingPointEQ<float>, \\\n                      val1, val2)\n\n#define EXPECT_DOUBLE_EQ(val1, val2)\\\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperFloatingPointEQ<double>, \\\n                      val1, val2)\n\n#define ASSERT_FLOAT_EQ(val1, val2)\\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperFloatingPointEQ<float>, \\\n                      val1, val2)\n\n#define ASSERT_DOUBLE_EQ(val1, val2)\\\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperFloatingPointEQ<double>, \\\n                      val1, val2)\n\n#define EXPECT_NEAR(val1, val2, abs_error)\\\n  EXPECT_PRED_FORMAT3(::testing::internal::DoubleNearPredFormat, \\\n                      val1, val2, abs_error)\n\n#define ASSERT_NEAR(val1, val2, abs_error)\\\n  ASSERT_PRED_FORMAT3(::testing::internal::DoubleNearPredFormat, \\\n                      val1, val2, abs_error)\n\n// These predicate format functions work on floating-point values, and\n// can be used in {ASSERT|EXPECT}_PRED_FORMAT2*(), e.g.\n//\n//   EXPECT_PRED_FORMAT2(testing::DoubleLE, Foo(), 5.0);\n\n// Asserts that val1 is less than, or almost equal to, val2.  Fails\n// otherwise.  In particular, it fails if either val1 or val2 is NaN.\nGTEST_API_ AssertionResult FloatLE(const char* expr1, const char* expr2,\n                                   float val1, float val2);\nGTEST_API_ AssertionResult DoubleLE(const char* expr1, const char* expr2,\n                                    double val1, double val2);\n\n\n#if GTEST_OS_WINDOWS\n\n// Macros that test for HRESULT failure and success, these are only useful\n// on Windows, and rely on Windows SDK macros and APIs to compile.\n//\n//    * {ASSERT|EXPECT}_HRESULT_{SUCCEEDED|FAILED}(expr)\n//\n// When expr unexpectedly fails or succeeds, Google Test prints the\n// expected result and the actual result with both a human-readable\n// string representation of the error, if available, as well as the\n// hex result code.\n# define EXPECT_HRESULT_SUCCEEDED(expr) \\\n    EXPECT_PRED_FORMAT1(::testing::internal::IsHRESULTSuccess, (expr))\n\n# define ASSERT_HRESULT_SUCCEEDED(expr) \\\n    ASSERT_PRED_FORMAT1(::testing::internal::IsHRESULTSuccess, (expr))\n\n# define EXPECT_HRESULT_FAILED(expr) \\\n    EXPECT_PRED_FORMAT1(::testing::internal::IsHRESULTFailure, (expr))\n\n# define ASSERT_HRESULT_FAILED(expr) \\\n    ASSERT_PRED_FORMAT1(::testing::internal::IsHRESULTFailure, (expr))\n\n#endif  // GTEST_OS_WINDOWS\n\n// Macros that execute statement and check that it doesn't generate new fatal\n// failures in the current thread.\n//\n//   * {ASSERT|EXPECT}_NO_FATAL_FAILURE(statement);\n//\n// Examples:\n//\n//   EXPECT_NO_FATAL_FAILURE(Process());\n//   ASSERT_NO_FATAL_FAILURE(Process()) << \"Process() failed\";\n//\n#define ASSERT_NO_FATAL_FAILURE(statement) \\\n    GTEST_TEST_NO_FATAL_FAILURE_(statement, GTEST_FATAL_FAILURE_)\n#define EXPECT_NO_FATAL_FAILURE(statement) \\\n    GTEST_TEST_NO_FATAL_FAILURE_(statement, GTEST_NONFATAL_FAILURE_)\n\n// Causes a trace (including the source file path, the current line\n// number, and the given message) to be included in every test failure\n// message generated by code in the current scope.  The effect is\n// undone when the control leaves the current scope.\n//\n// The message argument can be anything streamable to std::ostream.\n//\n// In the implementation, we include the current line number as part\n// of the dummy variable name, thus allowing multiple SCOPED_TRACE()s\n// to appear in the same block - as long as they are on different\n// lines.\n#define SCOPED_TRACE(message) \\\n  ::testing::internal::ScopedTrace GTEST_CONCAT_TOKEN_(gtest_trace_, __LINE__)(\\\n    __FILE__, __LINE__, ::testing::Message() << (message))\n\n// Compile-time assertion for type equality.\n// StaticAssertTypeEq<type1, type2>() compiles iff type1 and type2 are\n// the same type.  The value it returns is not interesting.\n//\n// Instead of making StaticAssertTypeEq a class template, we make it a\n// function template that invokes a helper class template.  This\n// prevents a user from misusing StaticAssertTypeEq<T1, T2> by\n// defining objects of that type.\n//\n// CAVEAT:\n//\n// When used inside a method of a class template,\n// StaticAssertTypeEq<T1, T2>() is effective ONLY IF the method is\n// instantiated.  For example, given:\n//\n//   template <typename T> class Foo {\n//    public:\n//     void Bar() { testing::StaticAssertTypeEq<int, T>(); }\n//   };\n//\n// the code:\n//\n//   void Test1() { Foo<bool> foo; }\n//\n// will NOT generate a compiler error, as Foo<bool>::Bar() is never\n// actually instantiated.  Instead, you need:\n//\n//   void Test2() { Foo<bool> foo; foo.Bar(); }\n//\n// to cause a compiler error.\ntemplate <typename T1, typename T2>\nbool StaticAssertTypeEq() {\n  (void)internal::StaticAssertTypeEqHelper<T1, T2>();\n  return true;\n}\n\n// Defines a test.\n//\n// The first parameter is the name of the test case, and the second\n// parameter is the name of the test within the test case.\n//\n// The convention is to end the test case name with \"Test\".  For\n// example, a test case for the Foo class can be named FooTest.\n//\n// Test code should appear between braces after an invocation of\n// this macro.  Example:\n//\n//   TEST(FooTest, InitializesCorrectly) {\n//     Foo foo;\n//     EXPECT_TRUE(foo.StatusIsOK());\n//   }\n\n// Note that we call GetTestTypeId() instead of GetTypeId<\n// ::testing::Test>() here to get the type ID of testing::Test.  This\n// is to work around a suspected linker bug when using Google Test as\n// a framework on Mac OS X.  The bug causes GetTypeId<\n// ::testing::Test>() to return different values depending on whether\n// the call is from the Google Test framework itself or from user test\n// code.  GetTestTypeId() is guaranteed to always return the same\n// value, as it always calls GetTypeId<>() from the Google Test\n// framework.\n#define GTEST_TEST(test_case_name, test_name)\\\n  GTEST_TEST_(test_case_name, test_name, \\\n              ::testing::Test, ::testing::internal::GetTestTypeId())\n\n// Define this macro to 1 to omit the definition of TEST(), which\n// is a generic name and clashes with some other libraries.\n#if !GTEST_DONT_DEFINE_TEST\n# define TEST(test_case_name, test_name) GTEST_TEST(test_case_name, test_name)\n#endif\n\n// Defines a test that uses a test fixture.\n//\n// The first parameter is the name of the test fixture class, which\n// also doubles as the test case name.  The second parameter is the\n// name of the test within the test case.\n//\n// A test fixture class must be declared earlier.  The user should put\n// his test code between braces after using this macro.  Example:\n//\n//   class FooTest : public testing::Test {\n//    protected:\n//     virtual void SetUp() { b_.AddElement(3); }\n//\n//     Foo a_;\n//     Foo b_;\n//   };\n//\n//   TEST_F(FooTest, InitializesCorrectly) {\n//     EXPECT_TRUE(a_.StatusIsOK());\n//   }\n//\n//   TEST_F(FooTest, ReturnsElementCountCorrectly) {\n//     EXPECT_EQ(0, a_.size());\n//     EXPECT_EQ(1, b_.size());\n//   }\n\n#define TEST_F(test_fixture, test_name)\\\n  GTEST_TEST_(test_fixture, test_name, test_fixture, \\\n              ::testing::internal::GetTypeId<test_fixture>())\n\n}  // namespace testing\n\n// Use this function in main() to run all tests.  It returns 0 if all\n// tests are successful, or 1 otherwise.\n//\n// RUN_ALL_TESTS() should be invoked after the command line has been\n// parsed by InitGoogleTest().\n//\n// This function was formerly a macro; thus, it is in the global\n// namespace and has an all-caps name.\nint RUN_ALL_TESTS() GTEST_MUST_USE_RESULT_;\n\ninline int RUN_ALL_TESTS() {\n  return ::testing::UnitTest::GetInstance()->Run();\n}\n\n#endif  // GTEST_INCLUDE_GTEST_GTEST_H_\n"}, "122": {"id": 122, "path": "/home/vsts/work/1/llvm-project/llvm/utils/unittest/googletest/include/gtest/internal/gtest-internal.h", "content": "// Copyright 2005, Google Inc.\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Authors: wan@google.com (Zhanyong Wan), eefacm@gmail.com (Sean Mcafee)\n//\n// The Google C++ Testing Framework (Google Test)\n//\n// This header file declares functions and macros used internally by\n// Google Test.  They are subject to change without notice.\n\n#ifndef GTEST_INCLUDE_GTEST_INTERNAL_GTEST_INTERNAL_H_\n#define GTEST_INCLUDE_GTEST_INTERNAL_GTEST_INTERNAL_H_\n\n#include \"gtest/internal/gtest-port.h\"\n\n#if GTEST_OS_LINUX\n# include <stdlib.h>\n# include <sys/types.h>\n# include <sys/wait.h>\n# include <unistd.h>\n#endif  // GTEST_OS_LINUX\n\n#if GTEST_HAS_EXCEPTIONS\n# include <stdexcept>\n#endif\n\n#include <ctype.h>\n#include <float.h>\n#include <string.h>\n#include <iomanip>\n#include <limits>\n#include <map>\n#include <set>\n#include <string>\n#include <vector>\n\n#include \"gtest/gtest-message.h\"\n#include \"gtest/internal/gtest-string.h\"\n#include \"gtest/internal/gtest-filepath.h\"\n#include \"gtest/internal/gtest-type-util.h\"\n\n// Due to C++ preprocessor weirdness, we need double indirection to\n// concatenate two tokens when one of them is __LINE__.  Writing\n//\n//   foo ## __LINE__\n//\n// will result in the token foo__LINE__, instead of foo followed by\n// the current line number.  For more details, see\n// http://www.parashift.com/c++-faq-lite/misc-technical-issues.html#faq-39.6\n#define GTEST_CONCAT_TOKEN_(foo, bar) GTEST_CONCAT_TOKEN_IMPL_(foo, bar)\n#define GTEST_CONCAT_TOKEN_IMPL_(foo, bar) foo ## bar\n\nclass ProtocolMessage;\nnamespace proto2 { class Message; }\n\nnamespace testing {\n\n// Forward declarations.\n\nclass AssertionResult;                 // Result of an assertion.\nclass Message;                         // Represents a failure message.\nclass Test;                            // Represents a test.\nclass TestInfo;                        // Information about a test.\nclass TestPartResult;                  // Result of a test part.\nclass UnitTest;                        // A collection of test cases.\n\ntemplate <typename T>\n::std::string PrintToString(const T& value);\n\nnamespace internal {\n\nstruct TraceInfo;                      // Information about a trace point.\nclass ScopedTrace;                     // Implements scoped trace.\nclass TestInfoImpl;                    // Opaque implementation of TestInfo\nclass UnitTestImpl;                    // Opaque implementation of UnitTest\n\n// The text used in failure messages to indicate the start of the\n// stack trace.\nGTEST_API_ extern const char kStackTraceMarker[];\n\n// Two overloaded helpers for checking at compile time whether an\n// expression is a null pointer literal (i.e. NULL or any 0-valued\n// compile-time integral constant).  Their return values have\n// different sizes, so we can use sizeof() to test which version is\n// picked by the compiler.  These helpers have no implementations, as\n// we only need their signatures.\n//\n// Given IsNullLiteralHelper(x), the compiler will pick the first\n// version if x can be implicitly converted to Secret*, and pick the\n// second version otherwise.  Since Secret is a secret and incomplete\n// type, the only expression a user can write that has type Secret* is\n// a null pointer literal.  Therefore, we know that x is a null\n// pointer literal if and only if the first version is picked by the\n// compiler.\nchar IsNullLiteralHelper(Secret* p);\nchar (&IsNullLiteralHelper(...))[2];  // NOLINT\n\n// A compile-time bool constant that is true if and only if x is a\n// null pointer literal (i.e. NULL or any 0-valued compile-time\n// integral constant).\n#ifdef GTEST_ELLIPSIS_NEEDS_POD_\n// We lose support for NULL detection where the compiler doesn't like\n// passing non-POD classes through ellipsis (...).\n# define GTEST_IS_NULL_LITERAL_(x) false\n#else\n# define GTEST_IS_NULL_LITERAL_(x) \\\n    (sizeof(::testing::internal::IsNullLiteralHelper(x)) == 1)\n#endif  // GTEST_ELLIPSIS_NEEDS_POD_\n\n// Appends the user-supplied message to the Google-Test-generated message.\nGTEST_API_ std::string AppendUserMessage(\n    const std::string& gtest_msg, const Message& user_msg);\n\n#if GTEST_HAS_EXCEPTIONS\n\n// This exception is thrown by (and only by) a failed Google Test\n// assertion when GTEST_FLAG(throw_on_failure) is true (if exceptions\n// are enabled).  We derive it from std::runtime_error, which is for\n// errors presumably detectable only at run time.  Since\n// std::runtime_error inherits from std::exception, many testing\n// frameworks know how to extract and print the message inside it.\nclass GTEST_API_ GoogleTestFailureException : public ::std::runtime_error {\n public:\n  explicit GoogleTestFailureException(const TestPartResult& failure);\n};\n\n#endif  // GTEST_HAS_EXCEPTIONS\n\n// A helper class for creating scoped traces in user programs.\nclass GTEST_API_ ScopedTrace {\n public:\n  // The c'tor pushes the given source file location and message onto\n  // a trace stack maintained by Google Test.\n  ScopedTrace(const char* file, int line, const Message& message);\n\n  // The d'tor pops the info pushed by the c'tor.\n  //\n  // Note that the d'tor is not virtual in order to be efficient.\n  // Don't inherit from ScopedTrace!\n  ~ScopedTrace();\n\n private:\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(ScopedTrace);\n} GTEST_ATTRIBUTE_UNUSED_;  // A ScopedTrace object does its job in its\n                            // c'tor and d'tor.  Therefore it doesn't\n                            // need to be used otherwise.\n\nnamespace edit_distance {\n// Returns the optimal edits to go from 'left' to 'right'.\n// All edits cost the same, with replace having lower priority than\n// add/remove.\n// Simple implementation of the Wagner\u2013Fischer algorithm.\n// See http://en.wikipedia.org/wiki/Wagner-Fischer_algorithm\nenum EditType { kMatch, kAdd, kRemove, kReplace };\nGTEST_API_ std::vector<EditType> CalculateOptimalEdits(\n    const std::vector<size_t>& left, const std::vector<size_t>& right);\n\n// Same as above, but the input is represented as strings.\nGTEST_API_ std::vector<EditType> CalculateOptimalEdits(\n    const std::vector<std::string>& left,\n    const std::vector<std::string>& right);\n\n// Create a diff of the input strings in Unified diff format.\nGTEST_API_ std::string CreateUnifiedDiff(const std::vector<std::string>& left,\n                                         const std::vector<std::string>& right,\n                                         size_t context = 2);\n\n}  // namespace edit_distance\n\n// Calculate the diff between 'left' and 'right' and return it in unified diff\n// format.\n// If not null, stores in 'total_line_count' the total number of lines found\n// in left + right.\nGTEST_API_ std::string DiffStrings(const std::string& left,\n                                   const std::string& right,\n                                   size_t* total_line_count);\n\n// Constructs and returns the message for an equality assertion\n// (e.g. ASSERT_EQ, EXPECT_STREQ, etc) failure.\n//\n// The first four parameters are the expressions used in the assertion\n// and their values, as strings.  For example, for ASSERT_EQ(foo, bar)\n// where foo is 5 and bar is 6, we have:\n//\n//   expected_expression: \"foo\"\n//   actual_expression:   \"bar\"\n//   expected_value:      \"5\"\n//   actual_value:        \"6\"\n//\n// The ignoring_case parameter is true iff the assertion is a\n// *_STRCASEEQ*.  When it's true, the string \" (ignoring case)\" will\n// be inserted into the message.\nGTEST_API_ AssertionResult EqFailure(const char* expected_expression,\n                                     const char* actual_expression,\n                                     const std::string& expected_value,\n                                     const std::string& actual_value,\n                                     bool ignoring_case);\n\n// Constructs a failure message for Boolean assertions such as EXPECT_TRUE.\nGTEST_API_ std::string GetBoolAssertionFailureMessage(\n    const AssertionResult& assertion_result,\n    const char* expression_text,\n    const char* actual_predicate_value,\n    const char* expected_predicate_value);\n\n// This template class represents an IEEE floating-point number\n// (either single-precision or double-precision, depending on the\n// template parameters).\n//\n// The purpose of this class is to do more sophisticated number\n// comparison.  (Due to round-off error, etc, it's very unlikely that\n// two floating-points will be equal exactly.  Hence a naive\n// comparison by the == operation often doesn't work.)\n//\n// Format of IEEE floating-point:\n//\n//   The most-significant bit being the leftmost, an IEEE\n//   floating-point looks like\n//\n//     sign_bit exponent_bits fraction_bits\n//\n//   Here, sign_bit is a single bit that designates the sign of the\n//   number.\n//\n//   For float, there are 8 exponent bits and 23 fraction bits.\n//\n//   For double, there are 11 exponent bits and 52 fraction bits.\n//\n//   More details can be found at\n//   http://en.wikipedia.org/wiki/IEEE_floating-point_standard.\n//\n// Template parameter:\n//\n//   RawType: the raw floating-point type (either float or double)\ntemplate <typename RawType>\nclass FloatingPoint {\n public:\n  // Defines the unsigned integer type that has the same size as the\n  // floating point number.\n  typedef typename TypeWithSize<sizeof(RawType)>::UInt Bits;\n\n  // Constants.\n\n  // # of bits in a number.\n  static const size_t kBitCount = 8*sizeof(RawType);\n\n  // # of fraction bits in a number.\n  static const size_t kFractionBitCount =\n    std::numeric_limits<RawType>::digits - 1;\n\n  // # of exponent bits in a number.\n  static const size_t kExponentBitCount = kBitCount - 1 - kFractionBitCount;\n\n  // The mask for the sign bit.\n  static const Bits kSignBitMask = static_cast<Bits>(1) << (kBitCount - 1);\n\n  // The mask for the fraction bits.\n  static const Bits kFractionBitMask =\n    ~static_cast<Bits>(0) >> (kExponentBitCount + 1);\n\n  // The mask for the exponent bits.\n  static const Bits kExponentBitMask = ~(kSignBitMask | kFractionBitMask);\n\n  // How many ULP's (Units in the Last Place) we want to tolerate when\n  // comparing two numbers.  The larger the value, the more error we\n  // allow.  A 0 value means that two numbers must be exactly the same\n  // to be considered equal.\n  //\n  // The maximum error of a single floating-point operation is 0.5\n  // units in the last place.  On Intel CPU's, all floating-point\n  // calculations are done with 80-bit precision, while double has 64\n  // bits.  Therefore, 4 should be enough for ordinary use.\n  //\n  // See the following article for more details on ULP:\n  // http://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/\n  static const size_t kMaxUlps = 4;\n\n  // Constructs a FloatingPoint from a raw floating-point number.\n  //\n  // On an Intel CPU, passing a non-normalized NAN (Not a Number)\n  // around may change its bits, although the new value is guaranteed\n  // to be also a NAN.  Therefore, don't expect this constructor to\n  // preserve the bits in x when x is a NAN.\n  explicit FloatingPoint(const RawType& x) { u_.value_ = x; }\n\n  // Static methods\n\n  // Reinterprets a bit pattern as a floating-point number.\n  //\n  // This function is needed to test the AlmostEquals() method.\n  static RawType ReinterpretBits(const Bits bits) {\n    FloatingPoint fp(0);\n    fp.u_.bits_ = bits;\n    return fp.u_.value_;\n  }\n\n  // Returns the floating-point number that represent positive infinity.\n  static RawType Infinity() {\n    return ReinterpretBits(kExponentBitMask);\n  }\n\n  // Returns the maximum representable finite floating-point number.\n  static RawType Max();\n\n  // Non-static methods\n\n  // Returns the bits that represents this number.\n  const Bits &bits() const { return u_.bits_; }\n\n  // Returns the exponent bits of this number.\n  Bits exponent_bits() const { return kExponentBitMask & u_.bits_; }\n\n  // Returns the fraction bits of this number.\n  Bits fraction_bits() const { return kFractionBitMask & u_.bits_; }\n\n  // Returns the sign bit of this number.\n  Bits sign_bit() const { return kSignBitMask & u_.bits_; }\n\n  // Returns true iff this is NAN (not a number).\n  bool is_nan() const {\n    // It's a NAN if the exponent bits are all ones and the fraction\n    // bits are not entirely zeros.\n    return (exponent_bits() == kExponentBitMask) && (fraction_bits() != 0);\n  }\n\n  // Returns true iff this number is at most kMaxUlps ULP's away from\n  // rhs.  In particular, this function:\n  //\n  //   - returns false if either number is (or both are) NAN.\n  //   - treats really large numbers as almost equal to infinity.\n  //   - thinks +0.0 and -0.0 are 0 DLP's apart.\n  bool AlmostEquals(const FloatingPoint& rhs) const {\n    // The IEEE standard says that any comparison operation involving\n    // a NAN must return false.\n    if (is_nan() || rhs.is_nan()) return false;\n\n    return DistanceBetweenSignAndMagnitudeNumbers(u_.bits_, rhs.u_.bits_)\n        <= kMaxUlps;\n  }\n\n private:\n  // The data type used to store the actual floating-point number.\n  union FloatingPointUnion {\n    RawType value_;  // The raw floating-point number.\n    Bits bits_;      // The bits that represent the number.\n  };\n\n  // Converts an integer from the sign-and-magnitude representation to\n  // the biased representation.  More precisely, let N be 2 to the\n  // power of (kBitCount - 1), an integer x is represented by the\n  // unsigned number x + N.\n  //\n  // For instance,\n  //\n  //   -N + 1 (the most negative number representable using\n  //          sign-and-magnitude) is represented by 1;\n  //   0      is represented by N; and\n  //   N - 1  (the biggest number representable using\n  //          sign-and-magnitude) is represented by 2N - 1.\n  //\n  // Read http://en.wikipedia.org/wiki/Signed_number_representations\n  // for more details on signed number representations.\n  static Bits SignAndMagnitudeToBiased(const Bits &sam) {\n    if (kSignBitMask & sam) {\n      // sam represents a negative number.\n      return ~sam + 1;\n    } else {\n      // sam represents a positive number.\n      return kSignBitMask | sam;\n    }\n  }\n\n  // Given two numbers in the sign-and-magnitude representation,\n  // returns the distance between them as an unsigned number.\n  static Bits DistanceBetweenSignAndMagnitudeNumbers(const Bits &sam1,\n                                                     const Bits &sam2) {\n    const Bits biased1 = SignAndMagnitudeToBiased(sam1);\n    const Bits biased2 = SignAndMagnitudeToBiased(sam2);\n    return (biased1 >= biased2) ? (biased1 - biased2) : (biased2 - biased1);\n  }\n\n  FloatingPointUnion u_;\n};\n\n// We cannot use std::numeric_limits<T>::max() as it clashes with the max()\n// macro defined by <windows.h>.\ntemplate <>\ninline float FloatingPoint<float>::Max() { return FLT_MAX; }\ntemplate <>\ninline double FloatingPoint<double>::Max() { return DBL_MAX; }\n\n// Typedefs the instances of the FloatingPoint template class that we\n// care to use.\ntypedef FloatingPoint<float> Float;\ntypedef FloatingPoint<double> Double;\n\n// In order to catch the mistake of putting tests that use different\n// test fixture classes in the same test case, we need to assign\n// unique IDs to fixture classes and compare them.  The TypeId type is\n// used to hold such IDs.  The user should treat TypeId as an opaque\n// type: the only operation allowed on TypeId values is to compare\n// them for equality using the == operator.\ntypedef const void* TypeId;\n\ntemplate <typename T>\nclass TypeIdHelper {\n public:\n  // dummy_ must not have a const type.  Otherwise an overly eager\n  // compiler (e.g. MSVC 7.1 & 8.0) may try to merge\n  // TypeIdHelper<T>::dummy_ for different Ts as an \"optimization\".\n  static bool dummy_;\n};\n\ntemplate <typename T>\nbool TypeIdHelper<T>::dummy_ = false;\n\n// GetTypeId<T>() returns the ID of type T.  Different values will be\n// returned for different types.  Calling the function twice with the\n// same type argument is guaranteed to return the same ID.\ntemplate <typename T>\nTypeId GetTypeId() {\n  // The compiler is required to allocate a different\n  // TypeIdHelper<T>::dummy_ variable for each T used to instantiate\n  // the template.  Therefore, the address of dummy_ is guaranteed to\n  // be unique.\n  return &(TypeIdHelper<T>::dummy_);\n}\n\n// Returns the type ID of ::testing::Test.  Always call this instead\n// of GetTypeId< ::testing::Test>() to get the type ID of\n// ::testing::Test, as the latter may give the wrong result due to a\n// suspected linker bug when compiling Google Test as a Mac OS X\n// framework.\nGTEST_API_ TypeId GetTestTypeId();\n\n// Defines the abstract factory interface that creates instances\n// of a Test object.\nclass TestFactoryBase {\n public:\n  virtual ~TestFactoryBase() {}\n\n  // Creates a test instance to run. The instance is both created and destroyed\n  // within TestInfoImpl::Run()\n  virtual Test* CreateTest() = 0;\n\n protected:\n  TestFactoryBase() {}\n\n private:\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(TestFactoryBase);\n};\n\n// This class provides implementation of TeastFactoryBase interface.\n// It is used in TEST and TEST_F macros.\ntemplate <class TestClass>\nclass TestFactoryImpl : public TestFactoryBase {\n public:\n  virtual Test* CreateTest() { return new TestClass; }\n};\n\n#if GTEST_OS_WINDOWS\n\n// Predicate-formatters for implementing the HRESULT checking macros\n// {ASSERT|EXPECT}_HRESULT_{SUCCEEDED|FAILED}\n// We pass a long instead of HRESULT to avoid causing an\n// include dependency for the HRESULT type.\nGTEST_API_ AssertionResult IsHRESULTSuccess(const char* expr,\n                                            long hr);  // NOLINT\nGTEST_API_ AssertionResult IsHRESULTFailure(const char* expr,\n                                            long hr);  // NOLINT\n\n#endif  // GTEST_OS_WINDOWS\n\n// Types of SetUpTestCase() and TearDownTestCase() functions.\ntypedef void (*SetUpTestCaseFunc)();\ntypedef void (*TearDownTestCaseFunc)();\n\nstruct CodeLocation {\n  CodeLocation(const string& a_file, int a_line) : file(a_file), line(a_line) {}\n\n  string file;\n  int line;\n};\n\n// Creates a new TestInfo object and registers it with Google Test;\n// returns the created object.\n//\n// Arguments:\n//\n//   test_case_name:   name of the test case\n//   name:             name of the test\n//   type_param        the name of the test's type parameter, or NULL if\n//                     this is not a typed or a type-parameterized test.\n//   value_param       text representation of the test's value parameter,\n//                     or NULL if this is not a type-parameterized test.\n//   code_location:    code location where the test is defined\n//   fixture_class_id: ID of the test fixture class\n//   set_up_tc:        pointer to the function that sets up the test case\n//   tear_down_tc:     pointer to the function that tears down the test case\n//   factory:          pointer to the factory that creates a test object.\n//                     The newly created TestInfo instance will assume\n//                     ownership of the factory object.\nGTEST_API_ TestInfo* MakeAndRegisterTestInfo(\n    const char* test_case_name,\n    const char* name,\n    const char* type_param,\n    const char* value_param,\n    CodeLocation code_location,\n    TypeId fixture_class_id,\n    SetUpTestCaseFunc set_up_tc,\n    TearDownTestCaseFunc tear_down_tc,\n    TestFactoryBase* factory);\n\n// If *pstr starts with the given prefix, modifies *pstr to be right\n// past the prefix and returns true; otherwise leaves *pstr unchanged\n// and returns false.  None of pstr, *pstr, and prefix can be NULL.\nGTEST_API_ bool SkipPrefix(const char* prefix, const char** pstr);\n\n#if GTEST_HAS_TYPED_TEST || GTEST_HAS_TYPED_TEST_P\n\n// State of the definition of a type-parameterized test case.\nclass GTEST_API_ TypedTestCasePState {\n public:\n  TypedTestCasePState() : registered_(false) {}\n\n  // Adds the given test name to defined_test_names_ and return true\n  // if the test case hasn't been registered; otherwise aborts the\n  // program.\n  bool AddTestName(const char* file, int line, const char* case_name,\n                   const char* test_name) {\n    if (registered_) {\n      fprintf(stderr, \"%s Test %s must be defined before \"\n              \"REGISTER_TYPED_TEST_CASE_P(%s, ...).\\n\",\n              FormatFileLocation(file, line).c_str(), test_name, case_name);\n      fflush(stderr);\n      posix::Abort();\n    }\n    registered_tests_.insert(\n        ::std::make_pair(test_name, CodeLocation(file, line)));\n    return true;\n  }\n\n  bool TestExists(const std::string& test_name) const {\n    return registered_tests_.count(test_name) > 0;\n  }\n\n  const CodeLocation& GetCodeLocation(const std::string& test_name) const {\n    RegisteredTestsMap::const_iterator it = registered_tests_.find(test_name);\n    GTEST_CHECK_(it != registered_tests_.end());\n    return it->second;\n  }\n\n  // Verifies that registered_tests match the test names in\n  // defined_test_names_; returns registered_tests if successful, or\n  // aborts the program otherwise.\n  const char* VerifyRegisteredTestNames(\n      const char* file, int line, const char* registered_tests);\n\n private:\n  typedef ::std::map<std::string, CodeLocation> RegisteredTestsMap;\n\n  bool registered_;\n  RegisteredTestsMap registered_tests_;\n};\n\n// Skips to the first non-space char after the first comma in 'str';\n// returns NULL if no comma is found in 'str'.\ninline const char* SkipComma(const char* str) {\n  const char* comma = strchr(str, ',');\n  if (comma == NULL) {\n    return NULL;\n  }\n  while (IsSpace(*(++comma))) {}\n  return comma;\n}\n\n// Returns the prefix of 'str' before the first comma in it; returns\n// the entire string if it contains no comma.\ninline std::string GetPrefixUntilComma(const char* str) {\n  const char* comma = strchr(str, ',');\n  return comma == NULL ? str : std::string(str, comma);\n}\n\n// Splits a given string on a given delimiter, populating a given\n// vector with the fields.\nvoid SplitString(const ::std::string& str, char delimiter,\n                 ::std::vector< ::std::string>* dest);\n\n// TypeParameterizedTest<Fixture, TestSel, Types>::Register()\n// registers a list of type-parameterized tests with Google Test.  The\n// return value is insignificant - we just need to return something\n// such that we can call this function in a namespace scope.\n//\n// Implementation note: The GTEST_TEMPLATE_ macro declares a template\n// template parameter.  It's defined in gtest-type-util.h.\ntemplate <GTEST_TEMPLATE_ Fixture, class TestSel, typename Types>\nclass TypeParameterizedTest {\n public:\n  // 'index' is the index of the test in the type list 'Types'\n  // specified in INSTANTIATE_TYPED_TEST_CASE_P(Prefix, TestCase,\n  // Types).  Valid values for 'index' are [0, N - 1] where N is the\n  // length of Types.\n  static bool Register(const char* prefix,\n                       CodeLocation code_location,\n                       const char* case_name, const char* test_names,\n                       int index) {\n    typedef typename Types::Head Type;\n    typedef Fixture<Type> FixtureClass;\n    typedef typename GTEST_BIND_(TestSel, Type) TestClass;\n\n    // First, registers the first type-parameterized test in the type\n    // list.\n    MakeAndRegisterTestInfo(\n        (std::string(prefix) + (prefix[0] == '\\0' ? \"\" : \"/\") + case_name + \"/\"\n         + StreamableToString(index)).c_str(),\n        StripTrailingSpaces(GetPrefixUntilComma(test_names)).c_str(),\n        GetTypeName<Type>().c_str(),\n        NULL,  // No value parameter.\n        code_location,\n        GetTypeId<FixtureClass>(),\n        TestClass::SetUpTestCase,\n        TestClass::TearDownTestCase,\n        new TestFactoryImpl<TestClass>);\n\n    // Next, recurses (at compile time) with the tail of the type list.\n    return TypeParameterizedTest<Fixture, TestSel, typename Types::Tail>\n        ::Register(prefix, code_location, case_name, test_names, index + 1);\n  }\n};\n\n// The base case for the compile time recursion.\ntemplate <GTEST_TEMPLATE_ Fixture, class TestSel>\nclass TypeParameterizedTest<Fixture, TestSel, Types0> {\n public:\n  static bool Register(const char* /*prefix*/, CodeLocation,\n                       const char* /*case_name*/, const char* /*test_names*/,\n                       int /*index*/) {\n    return true;\n  }\n};\n\n// TypeParameterizedTestCase<Fixture, Tests, Types>::Register()\n// registers *all combinations* of 'Tests' and 'Types' with Google\n// Test.  The return value is insignificant - we just need to return\n// something such that we can call this function in a namespace scope.\ntemplate <GTEST_TEMPLATE_ Fixture, typename Tests, typename Types>\nclass TypeParameterizedTestCase {\n public:\n  static bool Register(const char* prefix, CodeLocation code_location,\n                       const TypedTestCasePState* state,\n                       const char* case_name, const char* test_names) {\n    std::string test_name = StripTrailingSpaces(\n        GetPrefixUntilComma(test_names));\n    if (!state->TestExists(test_name)) {\n      fprintf(stderr, \"Failed to get code location for test %s.%s at %s.\",\n              case_name, test_name.c_str(),\n              FormatFileLocation(code_location.file.c_str(),\n                                 code_location.line).c_str());\n      fflush(stderr);\n      posix::Abort();\n    }\n    const CodeLocation& test_location = state->GetCodeLocation(test_name);\n\n    typedef typename Tests::Head Head;\n\n    // First, register the first test in 'Test' for each type in 'Types'.\n    TypeParameterizedTest<Fixture, Head, Types>::Register(\n        prefix, test_location, case_name, test_names, 0);\n\n    // Next, recurses (at compile time) with the tail of the test list.\n    return TypeParameterizedTestCase<Fixture, typename Tests::Tail, Types>\n        ::Register(prefix, code_location, state,\n                   case_name, SkipComma(test_names));\n  }\n};\n\n// The base case for the compile time recursion.\ntemplate <GTEST_TEMPLATE_ Fixture, typename Types>\nclass TypeParameterizedTestCase<Fixture, Templates0, Types> {\n public:\n  static bool Register(const char* /*prefix*/, CodeLocation,\n                       const TypedTestCasePState* /*state*/,\n                       const char* /*case_name*/, const char* /*test_names*/) {\n    return true;\n  }\n};\n\n#endif  // GTEST_HAS_TYPED_TEST || GTEST_HAS_TYPED_TEST_P\n\n// Returns the current OS stack trace as an std::string.\n//\n// The maximum number of stack frames to be included is specified by\n// the gtest_stack_trace_depth flag.  The skip_count parameter\n// specifies the number of top frames to be skipped, which doesn't\n// count against the number of frames to be included.\n//\n// For example, if Foo() calls Bar(), which in turn calls\n// GetCurrentOsStackTraceExceptTop(..., 1), Foo() will be included in\n// the trace but Bar() and GetCurrentOsStackTraceExceptTop() won't.\nGTEST_API_ std::string GetCurrentOsStackTraceExceptTop(\n    UnitTest* unit_test, int skip_count);\n\n// Helpers for suppressing warnings on unreachable code or constant\n// condition.\n\n// Always returns true.\nGTEST_API_ bool AlwaysTrue();\n\n// Always returns false.\ninline bool AlwaysFalse() { return !AlwaysTrue(); }\n\n// Helper for suppressing false warning from Clang on a const char*\n// variable declared in a conditional expression always being NULL in\n// the else branch.\nstruct GTEST_API_ ConstCharPtr {\n  ConstCharPtr(const char* str) : value(str) {}\n  operator bool() const { return true; }\n  const char* value;\n};\n\n// A simple Linear Congruential Generator for generating random\n// numbers with a uniform distribution.  Unlike rand() and srand(), it\n// doesn't use global state (and therefore can't interfere with user\n// code).  Unlike rand_r(), it's portable.  An LCG isn't very random,\n// but it's good enough for our purposes.\nclass GTEST_API_ Random {\n public:\n  static const UInt32 kMaxRange = 1u << 31;\n\n  explicit Random(UInt32 seed) : state_(seed) {}\n\n  void Reseed(UInt32 seed) { state_ = seed; }\n\n  // Generates a random number from [0, range).  Crashes if 'range' is\n  // 0 or greater than kMaxRange.\n  UInt32 Generate(UInt32 range);\n\n private:\n  UInt32 state_;\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(Random);\n};\n\n// Defining a variable of type CompileAssertTypesEqual<T1, T2> will cause a\n// compiler error iff T1 and T2 are different types.\ntemplate <typename T1, typename T2>\nstruct CompileAssertTypesEqual;\n\ntemplate <typename T>\nstruct CompileAssertTypesEqual<T, T> {\n};\n\n// Removes the reference from a type if it is a reference type,\n// otherwise leaves it unchanged.  This is the same as\n// tr1::remove_reference, which is not widely available yet.\ntemplate <typename T>\nstruct RemoveReference { typedef T type; };  // NOLINT\ntemplate <typename T>\nstruct RemoveReference<T&> { typedef T type; };  // NOLINT\n\n// A handy wrapper around RemoveReference that works when the argument\n// T depends on template parameters.\n#define GTEST_REMOVE_REFERENCE_(T) \\\n    typename ::testing::internal::RemoveReference<T>::type\n\n// Removes const from a type if it is a const type, otherwise leaves\n// it unchanged.  This is the same as tr1::remove_const, which is not\n// widely available yet.\ntemplate <typename T>\nstruct RemoveConst { typedef T type; };  // NOLINT\ntemplate <typename T>\nstruct RemoveConst<const T> { typedef T type; };  // NOLINT\n\n// MSVC 8.0, Sun C++, and IBM XL C++ have a bug which causes the above\n// definition to fail to remove the const in 'const int[3]' and 'const\n// char[3][4]'.  The following specialization works around the bug.\ntemplate <typename T, size_t N>\nstruct RemoveConst<const T[N]> {\n  typedef typename RemoveConst<T>::type type[N];\n};\n\n#if defined(_MSC_VER) && _MSC_VER < 1400\n// This is the only specialization that allows VC++ 7.1 to remove const in\n// 'const int[3] and 'const int[3][4]'.  However, it causes trouble with GCC\n// and thus needs to be conditionally compiled.\ntemplate <typename T, size_t N>\nstruct RemoveConst<T[N]> {\n  typedef typename RemoveConst<T>::type type[N];\n};\n#endif\n\n// A handy wrapper around RemoveConst that works when the argument\n// T depends on template parameters.\n#define GTEST_REMOVE_CONST_(T) \\\n    typename ::testing::internal::RemoveConst<T>::type\n\n// Turns const U&, U&, const U, and U all into U.\n#define GTEST_REMOVE_REFERENCE_AND_CONST_(T) \\\n    GTEST_REMOVE_CONST_(GTEST_REMOVE_REFERENCE_(T))\n\n// Adds reference to a type if it is not a reference type,\n// otherwise leaves it unchanged.  This is the same as\n// tr1::add_reference, which is not widely available yet.\ntemplate <typename T>\nstruct AddReference { typedef T& type; };  // NOLINT\ntemplate <typename T>\nstruct AddReference<T&> { typedef T& type; };  // NOLINT\n\n// A handy wrapper around AddReference that works when the argument T\n// depends on template parameters.\n#define GTEST_ADD_REFERENCE_(T) \\\n    typename ::testing::internal::AddReference<T>::type\n\n// Adds a reference to const on top of T as necessary.  For example,\n// it transforms\n//\n//   char         ==> const char&\n//   const char   ==> const char&\n//   char&        ==> const char&\n//   const char&  ==> const char&\n//\n// The argument T must depend on some template parameters.\n#define GTEST_REFERENCE_TO_CONST_(T) \\\n    GTEST_ADD_REFERENCE_(const GTEST_REMOVE_REFERENCE_(T))\n\n// ImplicitlyConvertible<From, To>::value is a compile-time bool\n// constant that's true iff type From can be implicitly converted to\n// type To.\ntemplate <typename From, typename To>\nclass ImplicitlyConvertible {\n private:\n  // We need the following helper functions only for their types.\n  // They have no implementations.\n\n  // MakeFrom() is an expression whose type is From.  We cannot simply\n  // use From(), as the type From may not have a public default\n  // constructor.\n  static typename AddReference<From>::type MakeFrom();\n\n  // These two functions are overloaded.  Given an expression\n  // Helper(x), the compiler will pick the first version if x can be\n  // implicitly converted to type To; otherwise it will pick the\n  // second version.\n  //\n  // The first version returns a value of size 1, and the second\n  // version returns a value of size 2.  Therefore, by checking the\n  // size of Helper(x), which can be done at compile time, we can tell\n  // which version of Helper() is used, and hence whether x can be\n  // implicitly converted to type To.\n  static char Helper(To);\n  static char (&Helper(...))[2];  // NOLINT\n\n  // We have to put the 'public' section after the 'private' section,\n  // or MSVC refuses to compile the code.\n public:\n#if defined(__BORLANDC__)\n  // C++Builder cannot use member overload resolution during template\n  // instantiation.  The simplest workaround is to use its C++0x type traits\n  // functions (C++Builder 2009 and above only).\n  static const bool value = __is_convertible(From, To);\n#else\n  // MSVC warns about implicitly converting from double to int for\n  // possible loss of data, so we need to temporarily disable the\n  // warning.\n  GTEST_DISABLE_MSC_WARNINGS_PUSH_(4244)\n  static const bool value =\n      sizeof(Helper(ImplicitlyConvertible::MakeFrom())) == 1;\n  GTEST_DISABLE_MSC_WARNINGS_POP_()\n#endif  // __BORLANDC__\n};\ntemplate <typename From, typename To>\nconst bool ImplicitlyConvertible<From, To>::value;\n\n// IsAProtocolMessage<T>::value is a compile-time bool constant that's\n// true iff T is type ProtocolMessage, proto2::Message, or a subclass\n// of those.\ntemplate <typename T>\nstruct IsAProtocolMessage\n    : public bool_constant<\n  ImplicitlyConvertible<const T*, const ::ProtocolMessage*>::value ||\n  ImplicitlyConvertible<const T*, const ::proto2::Message*>::value> {\n};\n\n// When the compiler sees expression IsContainerTest<C>(0), if C is an\n// STL-style container class, the first overload of IsContainerTest\n// will be viable (since both C::iterator* and C::const_iterator* are\n// valid types and NULL can be implicitly converted to them).  It will\n// be picked over the second overload as 'int' is a perfect match for\n// the type of argument 0.  If C::iterator or C::const_iterator is not\n// a valid type, the first overload is not viable, and the second\n// overload will be picked.  Therefore, we can determine whether C is\n// a container class by checking the type of IsContainerTest<C>(0).\n// The value of the expression is insignificant.\n//\n// Note that we look for both C::iterator and C::const_iterator.  The\n// reason is that C++ injects the name of a class as a member of the\n// class itself (e.g. you can refer to class iterator as either\n// 'iterator' or 'iterator::iterator').  If we look for C::iterator\n// only, for example, we would mistakenly think that a class named\n// iterator is an STL container.\n//\n// Also note that the simpler approach of overloading\n// IsContainerTest(typename C::const_iterator*) and\n// IsContainerTest(...) doesn't work with Visual Age C++ and Sun C++.\ntypedef int IsContainer;\ntemplate <class C>\nIsContainer IsContainerTest(int /* dummy */,\n                            typename C::iterator* /* it */ = NULL,\n                            typename C::const_iterator* /* const_it */ = NULL) {\n  return 0;\n}\n\ntypedef char IsNotContainer;\ntemplate <class C>\nIsNotContainer IsContainerTest(long /* dummy */) { return '\\0'; }\n\n// EnableIf<condition>::type is void when 'Cond' is true, and\n// undefined when 'Cond' is false.  To use SFINAE to make a function\n// overload only apply when a particular expression is true, add\n// \"typename EnableIf<expression>::type* = 0\" as the last parameter.\ntemplate<bool> struct EnableIf;\ntemplate<> struct EnableIf<true> { typedef void type; };  // NOLINT\n\n// Utilities for native arrays.\n\n// ArrayEq() compares two k-dimensional native arrays using the\n// elements' operator==, where k can be any integer >= 0.  When k is\n// 0, ArrayEq() degenerates into comparing a single pair of values.\n\ntemplate <typename T, typename U>\nbool ArrayEq(const T* lhs, size_t size, const U* rhs);\n\n// This generic version is used when k is 0.\ntemplate <typename T, typename U>\ninline bool ArrayEq(const T& lhs, const U& rhs) { return lhs == rhs; }\n\n// This overload is used when k >= 1.\ntemplate <typename T, typename U, size_t N>\ninline bool ArrayEq(const T(&lhs)[N], const U(&rhs)[N]) {\n  return internal::ArrayEq(lhs, N, rhs);\n}\n\n// This helper reduces code bloat.  If we instead put its logic inside\n// the previous ArrayEq() function, arrays with different sizes would\n// lead to different copies of the template code.\ntemplate <typename T, typename U>\nbool ArrayEq(const T* lhs, size_t size, const U* rhs) {\n  for (size_t i = 0; i != size; i++) {\n    if (!internal::ArrayEq(lhs[i], rhs[i]))\n      return false;\n  }\n  return true;\n}\n\n// Finds the first element in the iterator range [begin, end) that\n// equals elem.  Element may be a native array type itself.\ntemplate <typename Iter, typename Element>\nIter ArrayAwareFind(Iter begin, Iter end, const Element& elem) {\n  for (Iter it = begin; it != end; ++it) {\n    if (internal::ArrayEq(*it, elem))\n      return it;\n  }\n  return end;\n}\n\n// CopyArray() copies a k-dimensional native array using the elements'\n// operator=, where k can be any integer >= 0.  When k is 0,\n// CopyArray() degenerates into copying a single value.\n\ntemplate <typename T, typename U>\nvoid CopyArray(const T* from, size_t size, U* to);\n\n// This generic version is used when k is 0.\ntemplate <typename T, typename U>\ninline void CopyArray(const T& from, U* to) { *to = from; }\n\n// This overload is used when k >= 1.\ntemplate <typename T, typename U, size_t N>\ninline void CopyArray(const T(&from)[N], U(*to)[N]) {\n  internal::CopyArray(from, N, *to);\n}\n\n// This helper reduces code bloat.  If we instead put its logic inside\n// the previous CopyArray() function, arrays with different sizes\n// would lead to different copies of the template code.\ntemplate <typename T, typename U>\nvoid CopyArray(const T* from, size_t size, U* to) {\n  for (size_t i = 0; i != size; i++) {\n    internal::CopyArray(from[i], to + i);\n  }\n}\n\n// The relation between an NativeArray object (see below) and the\n// native array it represents.\n// We use 2 different structs to allow non-copyable types to be used, as long\n// as RelationToSourceReference() is passed.\nstruct RelationToSourceReference {};\nstruct RelationToSourceCopy {};\n\n// Adapts a native array to a read-only STL-style container.  Instead\n// of the complete STL container concept, this adaptor only implements\n// members useful for Google Mock's container matchers.  New members\n// should be added as needed.  To simplify the implementation, we only\n// support Element being a raw type (i.e. having no top-level const or\n// reference modifier).  It's the client's responsibility to satisfy\n// this requirement.  Element can be an array type itself (hence\n// multi-dimensional arrays are supported).\ntemplate <typename Element>\nclass NativeArray {\n public:\n  // STL-style container typedefs.\n  typedef Element value_type;\n  typedef Element* iterator;\n  typedef const Element* const_iterator;\n\n  // Constructs from a native array. References the source.\n  NativeArray(const Element* array, size_t count, RelationToSourceReference) {\n    InitRef(array, count);\n  }\n\n  // Constructs from a native array. Copies the source.\n  NativeArray(const Element* array, size_t count, RelationToSourceCopy) {\n    InitCopy(array, count);\n  }\n\n  // Copy constructor.\n  NativeArray(const NativeArray& rhs) {\n    (this->*rhs.clone_)(rhs.array_, rhs.size_);\n  }\n\n  ~NativeArray() {\n    if (clone_ != &NativeArray::InitRef)\n      delete[] array_;\n  }\n\n  // STL-style container methods.\n  size_t size() const { return size_; }\n  const_iterator begin() const { return array_; }\n  const_iterator end() const { return array_ + size_; }\n  bool operator==(const NativeArray& rhs) const {\n    return size() == rhs.size() &&\n        ArrayEq(begin(), size(), rhs.begin());\n  }\n\n private:\n  enum {\n    kCheckTypeIsNotConstOrAReference = StaticAssertTypeEqHelper<\n        Element, GTEST_REMOVE_REFERENCE_AND_CONST_(Element)>::value,\n  };\n\n  // Initializes this object with a copy of the input.\n  void InitCopy(const Element* array, size_t a_size) {\n    Element* const copy = new Element[a_size];\n    CopyArray(array, a_size, copy);\n    array_ = copy;\n    size_ = a_size;\n    clone_ = &NativeArray::InitCopy;\n  }\n\n  // Initializes this object with a reference of the input.\n  void InitRef(const Element* array, size_t a_size) {\n    array_ = array;\n    size_ = a_size;\n    clone_ = &NativeArray::InitRef;\n  }\n\n  const Element* array_;\n  size_t size_;\n  void (NativeArray::*clone_)(const Element*, size_t);\n\n  GTEST_DISALLOW_ASSIGN_(NativeArray);\n};\n\n}  // namespace internal\n}  // namespace testing\n\n#define GTEST_MESSAGE_AT_(file, line, message, result_type) \\\n  ::testing::internal::AssertHelper(result_type, file, line, message) \\\n    = ::testing::Message()\n\n#define GTEST_MESSAGE_(message, result_type) \\\n  GTEST_MESSAGE_AT_(__FILE__, __LINE__, message, result_type)\n\n#define GTEST_FATAL_FAILURE_(message) \\\n  return GTEST_MESSAGE_(message, ::testing::TestPartResult::kFatalFailure)\n\n#define GTEST_NONFATAL_FAILURE_(message) \\\n  GTEST_MESSAGE_(message, ::testing::TestPartResult::kNonFatalFailure)\n\n#define GTEST_SUCCESS_(message) \\\n  GTEST_MESSAGE_(message, ::testing::TestPartResult::kSuccess)\n\n// Suppresses MSVC warnings 4072 (unreachable code) for the code following\n// statement if it returns or throws (or doesn't return or throw in some\n// situations).\n#define GTEST_SUPPRESS_UNREACHABLE_CODE_WARNING_BELOW_(statement) \\\n  if (::testing::internal::AlwaysTrue()) { statement; }\n\n#define GTEST_TEST_THROW_(statement, expected_exception, fail) \\\n  GTEST_AMBIGUOUS_ELSE_BLOCKER_ \\\n  if (::testing::internal::ConstCharPtr gtest_msg = \"\") { \\\n    bool gtest_caught_expected = false; \\\n    try { \\\n      GTEST_SUPPRESS_UNREACHABLE_CODE_WARNING_BELOW_(statement); \\\n    } \\\n    catch (expected_exception const&) { \\\n      gtest_caught_expected = true; \\\n    } \\\n    catch (...) { \\\n      gtest_msg.value = \\\n          \"Expected: \" #statement \" throws an exception of type \" \\\n          #expected_exception \".\\n  Actual: it throws a different type.\"; \\\n      goto GTEST_CONCAT_TOKEN_(gtest_label_testthrow_, __LINE__); \\\n    } \\\n    if (!gtest_caught_expected) { \\\n      gtest_msg.value = \\\n          \"Expected: \" #statement \" throws an exception of type \" \\\n          #expected_exception \".\\n  Actual: it throws nothing.\"; \\\n      goto GTEST_CONCAT_TOKEN_(gtest_label_testthrow_, __LINE__); \\\n    } \\\n  } else \\\n    GTEST_CONCAT_TOKEN_(gtest_label_testthrow_, __LINE__): \\\n      fail(gtest_msg.value)\n\n#define GTEST_TEST_NO_THROW_(statement, fail) \\\n  GTEST_AMBIGUOUS_ELSE_BLOCKER_ \\\n  if (::testing::internal::AlwaysTrue()) { \\\n    try { \\\n      GTEST_SUPPRESS_UNREACHABLE_CODE_WARNING_BELOW_(statement); \\\n    } \\\n    catch (...) { \\\n      goto GTEST_CONCAT_TOKEN_(gtest_label_testnothrow_, __LINE__); \\\n    } \\\n  } else \\\n    GTEST_CONCAT_TOKEN_(gtest_label_testnothrow_, __LINE__): \\\n      fail(\"Expected: \" #statement \" doesn't throw an exception.\\n\" \\\n           \"  Actual: it throws.\")\n\n#define GTEST_TEST_ANY_THROW_(statement, fail) \\\n  GTEST_AMBIGUOUS_ELSE_BLOCKER_ \\\n  if (::testing::internal::AlwaysTrue()) { \\\n    bool gtest_caught_any = false; \\\n    try { \\\n      GTEST_SUPPRESS_UNREACHABLE_CODE_WARNING_BELOW_(statement); \\\n    } \\\n    catch (...) { \\\n      gtest_caught_any = true; \\\n    } \\\n    if (!gtest_caught_any) { \\\n      goto GTEST_CONCAT_TOKEN_(gtest_label_testanythrow_, __LINE__); \\\n    } \\\n  } else \\\n    GTEST_CONCAT_TOKEN_(gtest_label_testanythrow_, __LINE__): \\\n      fail(\"Expected: \" #statement \" throws an exception.\\n\" \\\n           \"  Actual: it doesn't.\")\n\n\n// Implements Boolean test assertions such as EXPECT_TRUE. expression can be\n// either a boolean expression or an AssertionResult. text is a textual\n// represenation of expression as it was passed into the EXPECT_TRUE.\n#define GTEST_TEST_BOOLEAN_(expression, text, actual, expected, fail) \\\n  GTEST_AMBIGUOUS_ELSE_BLOCKER_ \\\n  if (const ::testing::AssertionResult gtest_ar_ = \\\n      ::testing::AssertionResult(expression)) \\\n    ; \\\n  else \\\n    fail(::testing::internal::GetBoolAssertionFailureMessage(\\\n        gtest_ar_, text, #actual, #expected).c_str())\n\n#define GTEST_TEST_NO_FATAL_FAILURE_(statement, fail) \\\n  GTEST_AMBIGUOUS_ELSE_BLOCKER_ \\\n  if (::testing::internal::AlwaysTrue()) { \\\n    ::testing::internal::HasNewFatalFailureHelper gtest_fatal_failure_checker; \\\n    GTEST_SUPPRESS_UNREACHABLE_CODE_WARNING_BELOW_(statement); \\\n    if (gtest_fatal_failure_checker.has_new_fatal_failure()) { \\\n      goto GTEST_CONCAT_TOKEN_(gtest_label_testnofatal_, __LINE__); \\\n    } \\\n  } else \\\n    GTEST_CONCAT_TOKEN_(gtest_label_testnofatal_, __LINE__): \\\n      fail(\"Expected: \" #statement \" doesn't generate new fatal \" \\\n           \"failures in the current thread.\\n\" \\\n           \"  Actual: it does.\")\n\n// Expands to the name of the class that implements the given test.\n#define GTEST_TEST_CLASS_NAME_(test_case_name, test_name) \\\n  test_case_name##_##test_name##_Test\n\n// Helper macro for defining tests.\n#define GTEST_TEST_(test_case_name, test_name, parent_class, parent_id)\\\nclass GTEST_TEST_CLASS_NAME_(test_case_name, test_name) : public parent_class {\\\n public:\\\n  GTEST_TEST_CLASS_NAME_(test_case_name, test_name)() {}\\\n private:\\\n  virtual void TestBody();\\\n  static ::testing::TestInfo* const test_info_ GTEST_ATTRIBUTE_UNUSED_;\\\n  GTEST_DISALLOW_COPY_AND_ASSIGN_(\\\n      GTEST_TEST_CLASS_NAME_(test_case_name, test_name));\\\n};\\\n\\\n::testing::TestInfo* const GTEST_TEST_CLASS_NAME_(test_case_name, test_name)\\\n  ::test_info_ =\\\n    ::testing::internal::MakeAndRegisterTestInfo(\\\n        #test_case_name, #test_name, NULL, NULL, \\\n        ::testing::internal::CodeLocation(__FILE__, __LINE__), \\\n        (parent_id), \\\n        parent_class::SetUpTestCase, \\\n        parent_class::TearDownTestCase, \\\n        new ::testing::internal::TestFactoryImpl<\\\n            GTEST_TEST_CLASS_NAME_(test_case_name, test_name)>);\\\nvoid GTEST_TEST_CLASS_NAME_(test_case_name, test_name)::TestBody()\n\n#endif  // GTEST_INCLUDE_GTEST_INTERNAL_GTEST_INTERNAL_H_\n\n"}, "120": {"id": 120, "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/ConstantFoldingTest.cpp", "content": "//===- ConstantFoldingTest.cpp -------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#include \"GISelMITest.h\"\n#include \"llvm/CodeGen/GlobalISel/ConstantFoldingMIRBuilder.h\"\n#include \"llvm/CodeGen/GlobalISel/MachineIRBuilder.h\"\n#include \"llvm/CodeGen/GlobalISel/Utils.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"gtest/gtest.h\"\n\nusing namespace llvm;\n\nnamespace {\n\nTEST_F(AArch64GISelMITest, FoldWithBuilder) {\n  setUp();\n  if (!TM)\n    return;\n  // Try to use the FoldableInstructionsBuilder to build binary ops.\n  ConstantFoldingMIRBuilder CFB(B.getState());\n  LLT s32 = LLT::scalar(32);\n  int64_t Cst;\n  auto MIBCAdd =\n      CFB.buildAdd(s32, CFB.buildConstant(s32, 0), CFB.buildConstant(s32, 1));\n  // This should be a constant now.\n  bool match = mi_match(MIBCAdd.getReg(0), *MRI, m_ICst(Cst));\n  EXPECT_TRUE(match);\n  EXPECT_EQ(Cst, 1);\n  auto MIBCAdd1 =\n      CFB.buildInstr(TargetOpcode::G_ADD, {s32},\n                     {CFB.buildConstant(s32, 0), CFB.buildConstant(s32, 1)});\n  // This should be a constant now.\n  match = mi_match(MIBCAdd1.getReg(0), *MRI, m_ICst(Cst));\n  EXPECT_TRUE(match);\n  EXPECT_EQ(Cst, 1);\n\n  // Try one of the other constructors of MachineIRBuilder to make sure it's\n  // compatible.\n  ConstantFoldingMIRBuilder CFB1(*MF);\n  CFB1.setInsertPt(*EntryMBB, EntryMBB->end());\n  auto MIBCSub =\n      CFB1.buildInstr(TargetOpcode::G_SUB, {s32},\n                      {CFB1.buildConstant(s32, 1), CFB1.buildConstant(s32, 1)});\n  // This should be a constant now.\n  match = mi_match(MIBCSub.getReg(0), *MRI, m_ICst(Cst));\n  EXPECT_TRUE(match);\n  EXPECT_EQ(Cst, 0);\n\n  auto MIBCSext1 =\n      CFB1.buildInstr(TargetOpcode::G_SEXT_INREG, {s32},\n                      {CFB1.buildConstant(s32, 0x01), uint64_t(8)});\n  // This should be a constant now.\n  match = mi_match(MIBCSext1.getReg(0), *MRI, m_ICst(Cst));\n  EXPECT_TRUE(match);\n  EXPECT_EQ(1, Cst);\n\n  auto MIBCSext2 =\n      CFB1.buildInstr(TargetOpcode::G_SEXT_INREG, {s32},\n                      {CFB1.buildConstant(s32, 0x80), uint64_t(8)});\n  // This should be a constant now.\n  match = mi_match(MIBCSext2.getReg(0), *MRI, m_ICst(Cst));\n  EXPECT_TRUE(match);\n  EXPECT_EQ(-0x80, Cst);\n}\n\nTEST_F(AArch64GISelMITest, FoldBinOp) {\n  setUp();\n  if (!TM)\n    return;\n\n  LLT s32{LLT::scalar(32)};\n  auto MIBCst1 = B.buildConstant(s32, 16);\n  auto MIBCst2 = B.buildConstant(s32, 9);\n  auto MIBFCst1 = B.buildFConstant(s32, 1.0000001);\n  auto MIBFCst2 = B.buildFConstant(s32, 2.0);\n\n  // Test G_ADD folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGAddInt =\n      ConstantFoldBinOp(TargetOpcode::G_ADD, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGAddInt.hasValue());\n  EXPECT_EQ(25ULL, FoldGAddInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGAddMix =\n      ConstantFoldBinOp(TargetOpcode::G_ADD, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGAddMix.hasValue());\n  EXPECT_EQ(1073741840ULL, FoldGAddMix.getValue().getLimitedValue());\n\n  // Test G_AND folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGAndInt =\n      ConstantFoldBinOp(TargetOpcode::G_AND, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGAndInt.hasValue());\n  EXPECT_EQ(0ULL, FoldGAndInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGAndMix =\n      ConstantFoldBinOp(TargetOpcode::G_AND, MIBCst2.getReg(0),\n                        MIBFCst1.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGAndMix.hasValue());\n  EXPECT_EQ(1ULL, FoldGAndMix.getValue().getLimitedValue());\n\n  // Test G_ASHR folding Integer + Mixed cases\n  Optional<APInt> FoldGAShrInt =\n      ConstantFoldBinOp(TargetOpcode::G_ASHR, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGAShrInt.hasValue());\n  EXPECT_EQ(0ULL, FoldGAShrInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGAShrMix =\n      ConstantFoldBinOp(TargetOpcode::G_ASHR, MIBFCst2.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGAShrMix.hasValue());\n  EXPECT_EQ(2097152ULL, FoldGAShrMix.getValue().getLimitedValue());\n\n  // Test G_LSHR folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGLShrInt =\n      ConstantFoldBinOp(TargetOpcode::G_LSHR, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGLShrInt.hasValue());\n  EXPECT_EQ(0ULL, FoldGLShrInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGLShrMix =\n      ConstantFoldBinOp(TargetOpcode::G_LSHR, MIBFCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGLShrMix.hasValue());\n  EXPECT_EQ(2080768ULL, FoldGLShrMix.getValue().getLimitedValue());\n\n  // Test G_MUL folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGMulInt =\n      ConstantFoldBinOp(TargetOpcode::G_MUL, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGMulInt.hasValue());\n  EXPECT_EQ(144ULL, FoldGMulInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGMulMix =\n      ConstantFoldBinOp(TargetOpcode::G_MUL, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGMulMix.hasValue());\n  EXPECT_EQ(0ULL, FoldGMulMix.getValue().getLimitedValue());\n\n  // Test G_OR folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGOrInt =\n      ConstantFoldBinOp(TargetOpcode::G_OR, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGOrInt.hasValue());\n  EXPECT_EQ(25ULL, FoldGOrInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGOrMix =\n      ConstantFoldBinOp(TargetOpcode::G_OR, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGOrMix.hasValue());\n  EXPECT_EQ(1073741840ULL, FoldGOrMix.getValue().getLimitedValue());\n\n  // Test G_SHL folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGShlInt =\n      ConstantFoldBinOp(TargetOpcode::G_SHL, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGShlInt.hasValue());\n  EXPECT_EQ(8192ULL, FoldGShlInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGShlMix =\n      ConstantFoldBinOp(TargetOpcode::G_SHL, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGShlMix.hasValue());\n  EXPECT_EQ(0ULL, FoldGShlMix.getValue().getLimitedValue());\n\n  // Test G_SUB folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGSubInt =\n      ConstantFoldBinOp(TargetOpcode::G_SUB, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGSubInt.hasValue());\n  EXPECT_EQ(7ULL, FoldGSubInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGSubMix =\n      ConstantFoldBinOp(TargetOpcode::G_SUB, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGSubMix.hasValue());\n  EXPECT_EQ(3221225488ULL, FoldGSubMix.getValue().getLimitedValue());\n\n  // Test G_XOR folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGXorInt =\n      ConstantFoldBinOp(TargetOpcode::G_XOR, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGXorInt.hasValue());\n  EXPECT_EQ(25ULL, FoldGXorInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGXorMix =\n      ConstantFoldBinOp(TargetOpcode::G_XOR, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGXorMix.hasValue());\n  EXPECT_EQ(1073741840ULL, FoldGXorMix.getValue().getLimitedValue());\n\n  // Test G_UDIV folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGUdivInt =\n      ConstantFoldBinOp(TargetOpcode::G_UDIV, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGUdivInt.hasValue());\n  EXPECT_EQ(1ULL, FoldGUdivInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGUdivMix =\n      ConstantFoldBinOp(TargetOpcode::G_UDIV, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGUdivMix.hasValue());\n  EXPECT_EQ(0ULL, FoldGUdivMix.getValue().getLimitedValue());\n\n  // Test G_SDIV folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGSdivInt =\n      ConstantFoldBinOp(TargetOpcode::G_SDIV, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGSdivInt.hasValue());\n  EXPECT_EQ(1ULL, FoldGSdivInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGSdivMix =\n      ConstantFoldBinOp(TargetOpcode::G_SDIV, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGSdivMix.hasValue());\n  EXPECT_EQ(0ULL, FoldGSdivMix.getValue().getLimitedValue());\n\n  // Test G_UREM folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGUremInt =\n      ConstantFoldBinOp(TargetOpcode::G_UDIV, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGUremInt.hasValue());\n  EXPECT_EQ(1ULL, FoldGUremInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGUremMix =\n      ConstantFoldBinOp(TargetOpcode::G_UDIV, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGUremMix.hasValue());\n  EXPECT_EQ(0ULL, FoldGUremMix.getValue().getLimitedValue());\n\n  // Test G_SREM folding Integer + Mixed Int-Float cases\n  Optional<APInt> FoldGSremInt =\n      ConstantFoldBinOp(TargetOpcode::G_SREM, MIBCst1.getReg(0),\n                        MIBCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGSremInt.hasValue());\n  EXPECT_EQ(7ULL, FoldGSremInt.getValue().getLimitedValue());\n  Optional<APInt> FoldGSremMix =\n      ConstantFoldBinOp(TargetOpcode::G_SREM, MIBCst1.getReg(0),\n                        MIBFCst2.getReg(0), *MRI);\n  EXPECT_TRUE(FoldGSremMix.hasValue());\n  EXPECT_EQ(16ULL, FoldGSremMix.getValue().getLimitedValue());\n}\n\n} // namespace"}, "123": {"id": 123, "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/GISelMITest.h", "content": "//===- GISelMITest.h --------------------------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n#ifndef LLVM_UNITTEST_CODEGEN_GLOBALISEL_GISELMI_H\n#define LLVM_UNITTEST_CODEGEN_GLOBALISEL_GISELMI_H\n\n#include \"llvm/CodeGen/GlobalISel/GISelChangeObserver.h\"\n#include \"llvm/CodeGen/GlobalISel/LegalizerHelper.h\"\n#include \"llvm/CodeGen/GlobalISel/LegalizerInfo.h\"\n#include \"llvm/CodeGen/GlobalISel/MIPatternMatch.h\"\n#include \"llvm/CodeGen/GlobalISel/MachineIRBuilder.h\"\n#include \"llvm/CodeGen/GlobalISel/Utils.h\"\n#include \"llvm/CodeGen/MIRParser/MIRParser.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineModuleInfo.h\"\n#include \"llvm/CodeGen/TargetFrameLowering.h\"\n#include \"llvm/CodeGen/TargetInstrInfo.h\"\n#include \"llvm/CodeGen/TargetLowering.h\"\n#include \"llvm/CodeGen/TargetSubtargetInfo.h\"\n#include \"llvm/FileCheck/FileCheck.h\"\n#include \"llvm/InitializePasses.h\"\n#include \"llvm/Support/SourceMgr.h\"\n#include \"llvm/Support/TargetRegistry.h\"\n#include \"llvm/Support/TargetSelect.h\"\n#include \"llvm/Target/TargetMachine.h\"\n#include \"llvm/Target/TargetOptions.h\"\n#include \"gtest/gtest.h\"\n\nusing namespace llvm;\nusing namespace MIPatternMatch;\n\nstatic inline void initLLVM() {\n  InitializeAllTargets();\n  InitializeAllTargetMCs();\n  InitializeAllAsmPrinters();\n  InitializeAllAsmParsers();\n\n  PassRegistry *Registry = PassRegistry::getPassRegistry();\n  initializeCore(*Registry);\n  initializeCodeGen(*Registry);\n}\n\n// Define a printers to help debugging when things go wrong.\nnamespace llvm {\nstd::ostream &\noperator<<(std::ostream &OS, const LLT Ty);\n\nstd::ostream &\noperator<<(std::ostream &OS, const MachineFunction &MF);\n}\n\nstatic std::unique_ptr<Module> parseMIR(LLVMContext &Context,\n                                        std::unique_ptr<MIRParser> &MIR,\n                                        const TargetMachine &TM,\n                                        StringRef MIRCode, const char *FuncName,\n                                        MachineModuleInfo &MMI) {\n  SMDiagnostic Diagnostic;\n  std::unique_ptr<MemoryBuffer> MBuffer = MemoryBuffer::getMemBuffer(MIRCode);\n  MIR = createMIRParser(std::move(MBuffer), Context);\n  if (!MIR)\n    return nullptr;\n\n  std::unique_ptr<Module> M = MIR->parseIRModule();\n  if (!M)\n    return nullptr;\n\n  M->setDataLayout(TM.createDataLayout());\n\n  if (MIR->parseMachineFunctions(*M, MMI))\n    return nullptr;\n\n  return M;\n}\nstatic std::pair<std::unique_ptr<Module>, std::unique_ptr<MachineModuleInfo>>\ncreateDummyModule(LLVMContext &Context, const LLVMTargetMachine &TM,\n                  StringRef MIRString, const char *FuncName) {\n  std::unique_ptr<MIRParser> MIR;\n  auto MMI = std::make_unique<MachineModuleInfo>(&TM);\n  std::unique_ptr<Module> M =\n      parseMIR(Context, MIR, TM, MIRString, FuncName, *MMI);\n  return make_pair(std::move(M), std::move(MMI));\n}\n\nstatic MachineFunction *getMFFromMMI(const Module *M,\n                                     const MachineModuleInfo *MMI) {\n  Function *F = M->getFunction(\"func\");\n  auto *MF = MMI->getMachineFunction(*F);\n  return MF;\n}\n\nstatic void collectCopies(SmallVectorImpl<Register> &Copies,\n                          MachineFunction *MF) {\n  for (auto &MBB : *MF)\n    for (MachineInstr &MI : MBB) {\n      if (MI.getOpcode() == TargetOpcode::COPY)\n        Copies.push_back(MI.getOperand(0).getReg());\n    }\n}\n\nclass GISelMITest : public ::testing::Test {\nprotected:\n  GISelMITest() : ::testing::Test() {}\n\n  /// Prepare a target specific LLVMTargetMachine.\n  virtual std::unique_ptr<LLVMTargetMachine> createTargetMachine() const = 0;\n\n  /// Get the stub sample MIR test function.\n  virtual void getTargetTestModuleString(SmallString<512> &S,\n                                         StringRef MIRFunc) const = 0;\n\n  void setUp(StringRef ExtraAssembly = \"\") {\n    TM = createTargetMachine();\n    if (!TM)\n      return;\n\n    SmallString<512> MIRString;\n    getTargetTestModuleString(MIRString, ExtraAssembly);\n\n    ModuleMMIPair = createDummyModule(Context, *TM, MIRString, \"func\");\n    MF = getMFFromMMI(ModuleMMIPair.first.get(), ModuleMMIPair.second.get());\n    collectCopies(Copies, MF);\n    EntryMBB = &*MF->begin();\n    B.setMF(*MF);\n    MRI = &MF->getRegInfo();\n    B.setInsertPt(*EntryMBB, EntryMBB->end());\n  }\n\n  LLVMContext Context;\n  std::unique_ptr<LLVMTargetMachine> TM;\n  MachineFunction *MF;\n  std::pair<std::unique_ptr<Module>, std::unique_ptr<MachineModuleInfo>>\n      ModuleMMIPair;\n  SmallVector<Register, 4> Copies;\n  MachineBasicBlock *EntryMBB;\n  MachineIRBuilder B;\n  MachineRegisterInfo *MRI;\n};\n\nclass AArch64GISelMITest : public GISelMITest {\n  std::unique_ptr<LLVMTargetMachine> createTargetMachine() const override;\n  void getTargetTestModuleString(SmallString<512> &S,\n                                 StringRef MIRFunc) const override;\n};\n\nclass AMDGPUGISelMITest : public GISelMITest {\n  std::unique_ptr<LLVMTargetMachine> createTargetMachine() const override;\n  void getTargetTestModuleString(SmallString<512> &S,\n                                 StringRef MIRFunc) const override;\n};\n\n#define DefineLegalizerInfo(Name, SettingUpActionsBlock)                       \\\n  class Name##Info : public LegalizerInfo {                                    \\\n  public:                                                                      \\\n    Name##Info(const TargetSubtargetInfo &ST) {                                \\\n      using namespace TargetOpcode;                                            \\\n      const LLT s8 = LLT::scalar(8);                                           \\\n      (void)s8;                                                                \\\n      const LLT s16 = LLT::scalar(16);                                         \\\n      (void)s16;                                                               \\\n      const LLT s32 = LLT::scalar(32);                                         \\\n      (void)s32;                                                               \\\n      const LLT s64 = LLT::scalar(64);                                         \\\n      (void)s64;                                                               \\\n      const LLT s128 = LLT::scalar(128);                                       \\\n      (void)s128;                                                              \\\n      do                                                                       \\\n        SettingUpActionsBlock while (0);                                       \\\n      computeTables();                                                         \\\n      verify(*ST.getInstrInfo());                                              \\\n    }                                                                          \\\n  };\n\nstatic inline bool CheckMachineFunction(const MachineFunction &MF,\n                                        StringRef CheckStr) {\n  SmallString<512> Msg;\n  raw_svector_ostream OS(Msg);\n  MF.print(OS);\n  auto OutputBuf = MemoryBuffer::getMemBuffer(Msg, \"Output\", false);\n  auto CheckBuf = MemoryBuffer::getMemBuffer(CheckStr, \"\");\n  SmallString<4096> CheckFileBuffer;\n  FileCheckRequest Req;\n  FileCheck FC(Req);\n  StringRef CheckFileText =\n      FC.CanonicalizeFile(*CheckBuf.get(), CheckFileBuffer);\n  SourceMgr SM;\n  SM.AddNewSourceBuffer(MemoryBuffer::getMemBuffer(CheckFileText, \"CheckFile\"),\n                        SMLoc());\n  Regex PrefixRE = FC.buildCheckPrefixRegex();\n  if (FC.readCheckFile(SM, CheckFileText, PrefixRE))\n    return false;\n\n  auto OutBuffer = OutputBuf->getBuffer();\n  SM.AddNewSourceBuffer(std::move(OutputBuf), SMLoc());\n  return FC.checkInput(SM, OutBuffer);\n}\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 5, "file": 10, "line": 41}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 10, "line": 41}, "message": "default constructor 'IndexedMap<T, ToIndexT>' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/IndexedMap.h", "reportHash": "6cebf63dabdaffcd25d2ea460576e1f0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 25, "line": 144}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 25, "line": 144}, "message": "default constructor 'SparseSet<ValueT, KeyFunctorT, SparseT>' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/SparseSet.h", "reportHash": "6fa7f8e9badaa39640a63b6d794ece59", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 25, "line": 147}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 25, "line": 147}, "message": "destructor '~SparseSet<ValueT, KeyFunctorT, SparseT>' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/SparseSet.h", "reportHash": "03dc5378eaa77494d717ab897b221f25", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 38, "line": 225}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 38, "line": 225}, "message": "default constructor 'GISelCSEAnalysisWrapperPass' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CSEInfo.h", "reportHash": "16967ae93c550eda78c58d2f19abc178", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 39, "line": 57}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 39, "line": 57}, "message": "default constructor 'BaseArgInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CallLowering.h", "reportHash": "91428bafcd69d8ecf83dbd11e850b8df", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 39, "line": 79}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 39, "line": 79}, "message": "default constructor 'ArgInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CallLowering.h", "reportHash": "fb91dba257d0619d29c5c22b43dfa84f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 40, "line": 37}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 40, "line": 37}, "message": "default constructor 'GISelWorkList<N>' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/GISelWorkList.h", "reportHash": "951e1eb28dfe0e83b12e9db87d9feb54", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 41, "line": 518}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 41, "line": 518}, "message": "default constructor 'LegalizeRuleSet' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "reportHash": "b3e18d61d5b0be523519d0c0565ea1cf", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 41, "line": 1043}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 41, "line": 1043}, "message": "default constructor 'LegalizerInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "reportHash": "f63cda1a5f9d09e05d5b8eb1181db5d4", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 42, "line": 213}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 42, "line": 213}, "message": "default constructor 'FlagsOp' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "reportHash": "ba87b556e2c059969978a42ee42a7214", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 43, "line": 36}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 43, "line": 36}, "message": "default constructor 'LiveRegUnits' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/LiveRegUnits.h", "reportHash": "3e7526b3429be38ec5dd3d0790528779", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 44, "line": 35}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 44, "line": 35}, "message": "default constructor 'MIRFormatter' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MIRFormatter.h", "reportHash": "d145413254d642f9305e5e8b224c61b4", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 22, "file": 48, "line": 150}, "message": "mark 'noexcept'"}, {"location": {"col": 22, "file": 48, "line": 150}, "message": "move assignment operator 'operator=' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "reportHash": "38093c86c8de0e02285ced8ff5f37426", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 48, "line": 153}, "message": "mark 'noexcept'"}, {"location": {"col": 12, "file": 48, "line": 153}, "message": "default constructor 'MachineModuleInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "reportHash": "6a833bda075f94f59b2870e9345f4501", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 48, "line": 158}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 48, "line": 158}, "message": "move constructor 'MachineModuleInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "reportHash": "ed47b2454d6bfdbe162a3ade4a7e9037", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 48, "line": 277}, "message": "mark 'noexcept'"}, {"location": {"col": 12, "file": 48, "line": 277}, "message": "default constructor 'MachineModuleInfoWrapperPass' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "reportHash": "231913576730ade04b0d2f81e0519359", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 50, "line": 127}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 50, "line": 127}, "message": "default constructor 'Candidate' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineOutliner.h", "reportHash": "e510bed5d496e0ac291ef1f9ac971cca", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 50, "line": 221}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 50, "line": 221}, "message": "default constructor 'OutlinedFunction' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineOutliner.h", "reportHash": "549abe992e0b5d1565aa28346d255715", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 52, "line": 1016}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 52, "line": 1016}, "message": "default constructor 'defusechain_iterator<Uses, Defs, SkipDebug, ByOperand, ByInstr, ByBundle>' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineRegisterInfo.h", "reportHash": "bd94e8ec1944a30a041834557878fb31", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 52, "line": 1122}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 52, "line": 1122}, "message": "default constructor 'defusechain_instr_iterator<Uses, Defs, SkipDebug, ByOperand, ByInstr, ByBundle>' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineRegisterInfo.h", "reportHash": "abb6fc6fd519b2ea347f6af8894ebd0c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 54, "line": 84}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 54, "line": 84}, "message": "default constructor 'RegisterClassInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/RegisterClassInfo.h", "reportHash": "8e9f758784e7ef7a89998bbfb0fdc4d6", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 55, "line": 393}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 55, "line": 393}, "message": "default constructor 'SDNodeFlags' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "dffeba1ff1a1500ee3b80128febae350", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 14, "file": 55, "line": 913}, "message": "mark 'noexcept'"}, {"location": {"col": 14, "file": 55, "line": 913}, "message": "default constructor 'value_op_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "6604462927066afb82fd6c18ebda7c4b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 55, "line": 1919}, "message": "mark 'noexcept'"}, {"location": {"col": 12, "file": 55, "line": 1919}, "message": "default constructor 'BuildVectorSDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "13ff910ce21c94c82bad2d9033a4e008", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 56, "line": 65}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 56, "line": 65}, "message": "default constructor 'ArgFlagsTy' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetCallingConv.h", "reportHash": "797bae13f31df5a5ab10c31a917309fc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 57, "line": 99}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 57, "line": 99}, "message": "default constructor 'TargetInstrInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetInstrInfo.h", "reportHash": "344345442385453c734342ed914ac1b1", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 57, "line": 468}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 57, "line": 468}, "message": "default constructor 'RegSubRegPair' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetInstrInfo.h", "reportHash": "efd8b147378cdfc709234c486b61b021", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 57, "line": 485}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 57, "line": 485}, "message": "default constructor 'RegSubRegPairAndIdx' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetInstrInfo.h", "reportHash": "7f3478bd4e8e3c76c5635da3d49f7fd1", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 58, "line": 292}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 58, "line": 292}, "message": "default constructor 'ArgListEntry' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "reportHash": "07ef7e2b322cfb371a710f40c5cbf6ec", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 58, "line": 897}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 58, "line": 897}, "message": "default constructor 'ValueTypeActionImpl' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "reportHash": "f66b1feb31bc7d43815ae0a42f309cdd", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 58, "line": 3865}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 58, "line": 3865}, "message": "default constructor 'MakeLibCallOptions' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "reportHash": "0b8aec2dac81907966f6566419ebf07c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 60, "line": 39}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 60, "line": 39}, "message": "default constructor 'ShapeT' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TileShapeInfo.h", "reportHash": "3a1a0afb3c0e3147819667f785cd9619", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 15, "file": 61, "line": 41}, "message": "mark 'noexcept'"}, {"location": {"col": 15, "file": 61, "line": 41}, "message": "default constructor 'EVT' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/ValueTypes.h", "reportHash": "3a4943fdcf38fcd3288f08781fe24efb", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 62, "line": 74}, "message": "mark 'noexcept'"}, {"location": {"col": 5, "file": 62, "line": 74}, "message": "default constructor 'VirtRegMap' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/VirtRegMap.h", "reportHash": "6adc812ab6c865f5679b32d63a20d92e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 105, "line": 110}, "message": "mark 'noexcept'"}, {"location": {"col": 12, "file": 105, "line": 110}, "message": "default constructor 'formatted_raw_ostream' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/FormattedStream.h", "reportHash": "13694ac5e0da9d231e6673c1eb1e4377", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 109, "line": 37}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 109, "line": 37}, "message": "destructor '~RecyclingAllocator<AllocatorType, T, Size, Align>' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/RecyclingAllocator.h", "reportHash": "5205eb22e3b1ae477796454de1d1bd9c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 112, "line": 604}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 112, "line": 604}, "message": "default constructor 'TargetRegistry' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/TargetRegistry.h", "reportHash": "14804cc2976d2ac5cee7a644c74bd310", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 121, "line": 2217}, "message": "expanded from macro 'TEST_F'"}, {"location": {"col": 3, "file": 122, "line": 1217}, "message": "expanded from macro 'GTEST_TEST_'"}, {"location": {"col": 3, "file": 122, "line": 1211}, "message": "expanded from macro 'GTEST_TEST_CLASS_NAME_'"}, {"location": {"col": 1, "file": 120, "line": 20}, "message": "default constructor 'AArch64GISelMITest_FoldWithBuilder_Test' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/ConstantFoldingTest.cpp", "reportHash": "0e157051cbc974f40299e59b0a8cc422", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 121, "line": 2217}, "message": "expanded from macro 'TEST_F'"}, {"location": {"col": 3, "file": 122, "line": 1217}, "message": "expanded from macro 'GTEST_TEST_'"}, {"location": {"col": 3, "file": 122, "line": 1211}, "message": "expanded from macro 'GTEST_TEST_CLASS_NAME_'"}, {"location": {"col": 1, "file": 120, "line": 71}, "message": "default constructor 'AArch64GISelMITest_FoldBinOp_Test' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/ConstantFoldingTest.cpp", "reportHash": "8dabc8a37ea8ebb5364dbb01a3abcf3a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 123, "line": 106}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 123, "line": 106}, "message": "default constructor 'GISelMITest' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/GISelMITest.h", "reportHash": "6ba96e280bcb60b610c84c02337b04a7", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
