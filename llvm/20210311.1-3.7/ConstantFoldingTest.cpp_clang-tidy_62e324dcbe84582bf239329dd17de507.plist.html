<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"32": {"id": 32, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/CallingConvLower.h", "content": "//===- llvm/CallingConvLower.h - Calling Conventions ------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file declares the CCState and CCValAssign classes, used for lowering\n// and implementing calling conventions.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_CALLINGCONVLOWER_H\n#define LLVM_CODEGEN_CALLINGCONVLOWER_H\n\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/MachineFrameInfo.h\"\n#include \"llvm/CodeGen/Register.h\"\n#include \"llvm/CodeGen/TargetCallingConv.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/MC/MCRegisterInfo.h\"\n#include \"llvm/Support/Alignment.h\"\n\nnamespace llvm {\n\nclass CCState;\nclass MachineFunction;\nclass MVT;\nclass TargetRegisterInfo;\n\n/// CCValAssign - Represent assignment of one arg/retval to a location.\nclass CCValAssign {\npublic:\n  enum LocInfo {\n    Full,      // The value fills the full location.\n    SExt,      // The value is sign extended in the location.\n    ZExt,      // The value is zero extended in the location.\n    AExt,      // The value is extended with undefined upper bits.\n    SExtUpper, // The value is in the upper bits of the location and should be\n               // sign extended when retrieved.\n    ZExtUpper, // The value is in the upper bits of the location and should be\n               // zero extended when retrieved.\n    AExtUpper, // The value is in the upper bits of the location and should be\n               // extended with undefined upper bits when retrieved.\n    BCvt,      // The value is bit-converted in the location.\n    Trunc,     // The value is truncated in the location.\n    VExt,      // The value is vector-widened in the location.\n               // FIXME: Not implemented yet. Code that uses AExt to mean\n               // vector-widen should be fixed to use VExt instead.\n    FPExt,     // The floating-point value is fp-extended in the location.\n    Indirect   // The location contains pointer to the value.\n    // TODO: a subset of the value is in the location.\n  };\n\nprivate:\n  /// ValNo - This is the value number being assigned (e.g. an argument number).\n  unsigned ValNo;\n\n  /// Loc is either a stack offset or a register number.\n  unsigned Loc;\n\n  /// isMem - True if this is a memory loc, false if it is a register loc.\n  unsigned isMem : 1;\n\n  /// isCustom - True if this arg/retval requires special handling.\n  unsigned isCustom : 1;\n\n  /// Information about how the value is assigned.\n  LocInfo HTP : 6;\n\n  /// ValVT - The type of the value being assigned.\n  MVT ValVT;\n\n  /// LocVT - The type of the location being assigned to.\n  MVT LocVT;\npublic:\n\n  static CCValAssign getReg(unsigned ValNo, MVT ValVT,\n                            unsigned RegNo, MVT LocVT,\n                            LocInfo HTP) {\n    CCValAssign Ret;\n    Ret.ValNo = ValNo;\n    Ret.Loc = RegNo;\n    Ret.isMem = false;\n    Ret.isCustom = false;\n    Ret.HTP = HTP;\n    Ret.ValVT = ValVT;\n    Ret.LocVT = LocVT;\n    return Ret;\n  }\n\n  static CCValAssign getCustomReg(unsigned ValNo, MVT ValVT,\n                                  unsigned RegNo, MVT LocVT,\n                                  LocInfo HTP) {\n    CCValAssign Ret;\n    Ret = getReg(ValNo, ValVT, RegNo, LocVT, HTP);\n    Ret.isCustom = true;\n    return Ret;\n  }\n\n  static CCValAssign getMem(unsigned ValNo, MVT ValVT,\n                            unsigned Offset, MVT LocVT,\n                            LocInfo HTP) {\n    CCValAssign Ret;\n    Ret.ValNo = ValNo;\n    Ret.Loc = Offset;\n    Ret.isMem = true;\n    Ret.isCustom = false;\n    Ret.HTP = HTP;\n    Ret.ValVT = ValVT;\n    Ret.LocVT = LocVT;\n    return Ret;\n  }\n\n  static CCValAssign getCustomMem(unsigned ValNo, MVT ValVT,\n                                  unsigned Offset, MVT LocVT,\n                                  LocInfo HTP) {\n    CCValAssign Ret;\n    Ret = getMem(ValNo, ValVT, Offset, LocVT, HTP);\n    Ret.isCustom = true;\n    return Ret;\n  }\n\n  // There is no need to differentiate between a pending CCValAssign and other\n  // kinds, as they are stored in a different list.\n  static CCValAssign getPending(unsigned ValNo, MVT ValVT, MVT LocVT,\n                                LocInfo HTP, unsigned ExtraInfo = 0) {\n    return getReg(ValNo, ValVT, ExtraInfo, LocVT, HTP);\n  }\n\n  void convertToReg(unsigned RegNo) {\n    Loc = RegNo;\n    isMem = false;\n  }\n\n  void convertToMem(unsigned Offset) {\n    Loc = Offset;\n    isMem = true;\n  }\n\n  unsigned getValNo() const { return ValNo; }\n  MVT getValVT() const { return ValVT; }\n\n  bool isRegLoc() const { return !isMem; }\n  bool isMemLoc() const { return isMem; }\n\n  bool needsCustom() const { return isCustom; }\n\n  Register getLocReg() const { assert(isRegLoc()); return Loc; }\n  unsigned getLocMemOffset() const { assert(isMemLoc()); return Loc; }\n  unsigned getExtraInfo() const { return Loc; }\n  MVT getLocVT() const { return LocVT; }\n\n  LocInfo getLocInfo() const { return HTP; }\n  bool isExtInLoc() const {\n    return (HTP == AExt || HTP == SExt || HTP == ZExt);\n  }\n\n  bool isUpperBitsInLoc() const {\n    return HTP == AExtUpper || HTP == SExtUpper || HTP == ZExtUpper;\n  }\n};\n\n/// Describes a register that needs to be forwarded from the prologue to a\n/// musttail call.\nstruct ForwardedRegister {\n  ForwardedRegister(Register VReg, MCPhysReg PReg, MVT VT)\n      : VReg(VReg), PReg(PReg), VT(VT) {}\n  Register VReg;\n  MCPhysReg PReg;\n  MVT VT;\n};\n\n/// CCAssignFn - This function assigns a location for Val, updating State to\n/// reflect the change.  It returns 'true' if it failed to handle Val.\ntypedef bool CCAssignFn(unsigned ValNo, MVT ValVT,\n                        MVT LocVT, CCValAssign::LocInfo LocInfo,\n                        ISD::ArgFlagsTy ArgFlags, CCState &State);\n\n/// CCCustomFn - This function assigns a location for Val, possibly updating\n/// all args to reflect changes and indicates if it handled it. It must set\n/// isCustom if it handles the arg and returns true.\ntypedef bool CCCustomFn(unsigned &ValNo, MVT &ValVT,\n                        MVT &LocVT, CCValAssign::LocInfo &LocInfo,\n                        ISD::ArgFlagsTy &ArgFlags, CCState &State);\n\n/// CCState - This class holds information needed while lowering arguments and\n/// return values.  It captures which registers are already assigned and which\n/// stack slots are used.  It provides accessors to allocate these values.\nclass CCState {\nprivate:\n  CallingConv::ID CallingConv;\n  bool IsVarArg;\n  bool AnalyzingMustTailForwardedRegs = false;\n  MachineFunction &MF;\n  const TargetRegisterInfo &TRI;\n  SmallVectorImpl<CCValAssign> &Locs;\n  LLVMContext &Context;\n\n  unsigned StackOffset;\n  Align MaxStackArgAlign;\n  SmallVector<uint32_t, 16> UsedRegs;\n  SmallVector<CCValAssign, 4> PendingLocs;\n  SmallVector<ISD::ArgFlagsTy, 4> PendingArgFlags;\n\n  // ByValInfo and SmallVector<ByValInfo, 4> ByValRegs:\n  //\n  // Vector of ByValInfo instances (ByValRegs) is introduced for byval registers\n  // tracking.\n  // Or, in another words it tracks byval parameters that are stored in\n  // general purpose registers.\n  //\n  // For 4 byte stack alignment,\n  // instance index means byval parameter number in formal\n  // arguments set. Assume, we have some \"struct_type\" with size = 4 bytes,\n  // then, for function \"foo\":\n  //\n  // i32 foo(i32 %p, %struct_type* %r, i32 %s, %struct_type* %t)\n  //\n  // ByValRegs[0] describes how \"%r\" is stored (Begin == r1, End == r2)\n  // ByValRegs[1] describes how \"%t\" is stored (Begin == r3, End == r4).\n  //\n  // In case of 8 bytes stack alignment,\n  // In function shown above, r3 would be wasted according to AAPCS rules.\n  // ByValRegs vector size still would be 2,\n  // while \"%t\" goes to the stack: it wouldn't be described in ByValRegs.\n  //\n  // Supposed use-case for this collection:\n  // 1. Initially ByValRegs is empty, InRegsParamsProcessed is 0.\n  // 2. HandleByVal fills up ByValRegs.\n  // 3. Argument analysis (LowerFormatArguments, for example). After\n  // some byval argument was analyzed, InRegsParamsProcessed is increased.\n  struct ByValInfo {\n    ByValInfo(unsigned B, unsigned E) : Begin(B), End(E) {}\n\n    // First register allocated for current parameter.\n    unsigned Begin;\n\n    // First after last register allocated for current parameter.\n    unsigned End;\n  };\n  SmallVector<ByValInfo, 4 > ByValRegs;\n\n  // InRegsParamsProcessed - shows how many instances of ByValRegs was proceed\n  // during argument analysis.\n  unsigned InRegsParamsProcessed;\n\npublic:\n  CCState(CallingConv::ID CC, bool isVarArg, MachineFunction &MF,\n          SmallVectorImpl<CCValAssign> &locs, LLVMContext &C);\n\n  void addLoc(const CCValAssign &V) {\n    Locs.push_back(V);\n  }\n\n  LLVMContext &getContext() const { return Context; }\n  MachineFunction &getMachineFunction() const { return MF; }\n  CallingConv::ID getCallingConv() const { return CallingConv; }\n  bool isVarArg() const { return IsVarArg; }\n\n  /// getNextStackOffset - Return the next stack offset such that all stack\n  /// slots satisfy their alignment requirements.\n  unsigned getNextStackOffset() const {\n    return StackOffset;\n  }\n\n  /// getAlignedCallFrameSize - Return the size of the call frame needed to\n  /// be able to store all arguments and such that the alignment requirement\n  /// of each of the arguments is satisfied.\n  unsigned getAlignedCallFrameSize() const {\n    return alignTo(StackOffset, MaxStackArgAlign);\n  }\n\n  /// isAllocated - Return true if the specified register (or an alias) is\n  /// allocated.\n  bool isAllocated(MCRegister Reg) const {\n    return UsedRegs[Reg / 32] & (1 << (Reg & 31));\n  }\n\n  /// AnalyzeFormalArguments - Analyze an array of argument values,\n  /// incorporating info about the formals into this state.\n  void AnalyzeFormalArguments(const SmallVectorImpl<ISD::InputArg> &Ins,\n                              CCAssignFn Fn);\n\n  /// The function will invoke AnalyzeFormalArguments.\n  void AnalyzeArguments(const SmallVectorImpl<ISD::InputArg> &Ins,\n                        CCAssignFn Fn) {\n    AnalyzeFormalArguments(Ins, Fn);\n  }\n\n  /// AnalyzeReturn - Analyze the returned values of a return,\n  /// incorporating info about the result values into this state.\n  void AnalyzeReturn(const SmallVectorImpl<ISD::OutputArg> &Outs,\n                     CCAssignFn Fn);\n\n  /// CheckReturn - Analyze the return values of a function, returning\n  /// true if the return can be performed without sret-demotion, and\n  /// false otherwise.\n  bool CheckReturn(const SmallVectorImpl<ISD::OutputArg> &Outs,\n                   CCAssignFn Fn);\n\n  /// AnalyzeCallOperands - Analyze the outgoing arguments to a call,\n  /// incorporating info about the passed values into this state.\n  void AnalyzeCallOperands(const SmallVectorImpl<ISD::OutputArg> &Outs,\n                           CCAssignFn Fn);\n\n  /// AnalyzeCallOperands - Same as above except it takes vectors of types\n  /// and argument flags.\n  void AnalyzeCallOperands(SmallVectorImpl<MVT> &ArgVTs,\n                           SmallVectorImpl<ISD::ArgFlagsTy> &Flags,\n                           CCAssignFn Fn);\n\n  /// The function will invoke AnalyzeCallOperands.\n  void AnalyzeArguments(const SmallVectorImpl<ISD::OutputArg> &Outs,\n                        CCAssignFn Fn) {\n    AnalyzeCallOperands(Outs, Fn);\n  }\n\n  /// AnalyzeCallResult - Analyze the return values of a call,\n  /// incorporating info about the passed values into this state.\n  void AnalyzeCallResult(const SmallVectorImpl<ISD::InputArg> &Ins,\n                         CCAssignFn Fn);\n\n  /// A shadow allocated register is a register that was allocated\n  /// but wasn't added to the location list (Locs).\n  /// \\returns true if the register was allocated as shadow or false otherwise.\n  bool IsShadowAllocatedReg(MCRegister Reg) const;\n\n  /// AnalyzeCallResult - Same as above except it's specialized for calls which\n  /// produce a single value.\n  void AnalyzeCallResult(MVT VT, CCAssignFn Fn);\n\n  /// getFirstUnallocated - Return the index of the first unallocated register\n  /// in the set, or Regs.size() if they are all allocated.\n  unsigned getFirstUnallocated(ArrayRef<MCPhysReg> Regs) const {\n    for (unsigned i = 0; i < Regs.size(); ++i)\n      if (!isAllocated(Regs[i]))\n        return i;\n    return Regs.size();\n  }\n\n  void DeallocateReg(MCPhysReg Reg) {\n    assert(isAllocated(Reg) && \"Trying to deallocate an unallocated register\");\n    MarkUnallocated(Reg);\n  }\n\n  /// AllocateReg - Attempt to allocate one register.  If it is not available,\n  /// return zero.  Otherwise, return the register, marking it and any aliases\n  /// as allocated.\n  MCRegister AllocateReg(MCPhysReg Reg) {\n    if (isAllocated(Reg))\n      return MCRegister();\n    MarkAllocated(Reg);\n    return Reg;\n  }\n\n  /// Version of AllocateReg with extra register to be shadowed.\n  MCRegister AllocateReg(MCPhysReg Reg, MCPhysReg ShadowReg) {\n    if (isAllocated(Reg))\n      return MCRegister();\n    MarkAllocated(Reg);\n    MarkAllocated(ShadowReg);\n    return Reg;\n  }\n\n  /// AllocateReg - Attempt to allocate one of the specified registers.  If none\n  /// are available, return zero.  Otherwise, return the first one available,\n  /// marking it and any aliases as allocated.\n  MCPhysReg AllocateReg(ArrayRef<MCPhysReg> Regs) {\n    unsigned FirstUnalloc = getFirstUnallocated(Regs);\n    if (FirstUnalloc == Regs.size())\n      return MCRegister();    // Didn't find the reg.\n\n    // Mark the register and any aliases as allocated.\n    MCPhysReg Reg = Regs[FirstUnalloc];\n    MarkAllocated(Reg);\n    return Reg;\n  }\n\n  /// AllocateRegBlock - Attempt to allocate a block of RegsRequired consecutive\n  /// registers. If this is not possible, return zero. Otherwise, return the first\n  /// register of the block that were allocated, marking the entire block as allocated.\n  MCPhysReg AllocateRegBlock(ArrayRef<MCPhysReg> Regs, unsigned RegsRequired) {\n    if (RegsRequired > Regs.size())\n      return 0;\n\n    for (unsigned StartIdx = 0; StartIdx <= Regs.size() - RegsRequired;\n         ++StartIdx) {\n      bool BlockAvailable = true;\n      // Check for already-allocated regs in this block\n      for (unsigned BlockIdx = 0; BlockIdx < RegsRequired; ++BlockIdx) {\n        if (isAllocated(Regs[StartIdx + BlockIdx])) {\n          BlockAvailable = false;\n          break;\n        }\n      }\n      if (BlockAvailable) {\n        // Mark the entire block as allocated\n        for (unsigned BlockIdx = 0; BlockIdx < RegsRequired; ++BlockIdx) {\n          MarkAllocated(Regs[StartIdx + BlockIdx]);\n        }\n        return Regs[StartIdx];\n      }\n    }\n    // No block was available\n    return 0;\n  }\n\n  /// Version of AllocateReg with list of registers to be shadowed.\n  MCRegister AllocateReg(ArrayRef<MCPhysReg> Regs, const MCPhysReg *ShadowRegs) {\n    unsigned FirstUnalloc = getFirstUnallocated(Regs);\n    if (FirstUnalloc == Regs.size())\n      return MCRegister();    // Didn't find the reg.\n\n    // Mark the register and any aliases as allocated.\n    MCRegister Reg = Regs[FirstUnalloc], ShadowReg = ShadowRegs[FirstUnalloc];\n    MarkAllocated(Reg);\n    MarkAllocated(ShadowReg);\n    return Reg;\n  }\n\n  /// AllocateStack - Allocate a chunk of stack space with the specified size\n  /// and alignment.\n  unsigned AllocateStack(unsigned Size, Align Alignment) {\n    StackOffset = alignTo(StackOffset, Alignment);\n    unsigned Result = StackOffset;\n    StackOffset += Size;\n    MaxStackArgAlign = std::max(Alignment, MaxStackArgAlign);\n    ensureMaxAlignment(Alignment);\n    return Result;\n  }\n\n  // FIXME: Deprecate this function when transition to Align is over.\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned AllocateStack(unsigned Size,\n                                                   unsigned Alignment),\n                            \"Use the version that takes Align instead.\") {\n    return AllocateStack(Size, Align(Alignment));\n  }\n\n  void ensureMaxAlignment(Align Alignment);\n\n  /// Version of AllocateStack with extra register to be shadowed.\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned AllocateStack(unsigned Size,\n                                                   unsigned Alignment,\n                                                   unsigned ShadowReg),\n                            \"Use the version that takes Align instead.\") {\n    MarkAllocated(ShadowReg);\n    return AllocateStack(Size, Align(Alignment));\n  }\n\n  /// Version of AllocateStack with list of extra registers to be shadowed.\n  /// Note that, unlike AllocateReg, this shadows ALL of the shadow registers.\n  unsigned AllocateStack(unsigned Size, Align Alignment,\n                         ArrayRef<MCPhysReg> ShadowRegs) {\n    for (unsigned i = 0; i < ShadowRegs.size(); ++i)\n      MarkAllocated(ShadowRegs[i]);\n    return AllocateStack(Size, Alignment);\n  }\n\n  // HandleByVal - Allocate a stack slot large enough to pass an argument by\n  // value. The size and alignment information of the argument is encoded in its\n  // parameter attribute.\n  void HandleByVal(unsigned ValNo, MVT ValVT, MVT LocVT,\n                   CCValAssign::LocInfo LocInfo, int MinSize, Align MinAlign,\n                   ISD::ArgFlagsTy ArgFlags);\n\n  // Returns count of byval arguments that are to be stored (even partly)\n  // in registers.\n  unsigned getInRegsParamsCount() const { return ByValRegs.size(); }\n\n  // Returns count of byval in-regs arguments processed.\n  unsigned getInRegsParamsProcessed() const { return InRegsParamsProcessed; }\n\n  // Get information about N-th byval parameter that is stored in registers.\n  // Here \"ByValParamIndex\" is N.\n  void getInRegsParamInfo(unsigned InRegsParamRecordIndex,\n                          unsigned& BeginReg, unsigned& EndReg) const {\n    assert(InRegsParamRecordIndex < ByValRegs.size() &&\n           \"Wrong ByVal parameter index\");\n\n    const ByValInfo& info = ByValRegs[InRegsParamRecordIndex];\n    BeginReg = info.Begin;\n    EndReg = info.End;\n  }\n\n  // Add information about parameter that is kept in registers.\n  void addInRegsParamInfo(unsigned RegBegin, unsigned RegEnd) {\n    ByValRegs.push_back(ByValInfo(RegBegin, RegEnd));\n  }\n\n  // Goes either to next byval parameter (excluding \"waste\" record), or\n  // to the end of collection.\n  // Returns false, if end is reached.\n  bool nextInRegsParam() {\n    unsigned e = ByValRegs.size();\n    if (InRegsParamsProcessed < e)\n      ++InRegsParamsProcessed;\n    return InRegsParamsProcessed < e;\n  }\n\n  // Clear byval registers tracking info.\n  void clearByValRegsInfo() {\n    InRegsParamsProcessed = 0;\n    ByValRegs.clear();\n  }\n\n  // Rewind byval registers tracking info.\n  void rewindByValRegsInfo() {\n    InRegsParamsProcessed = 0;\n  }\n\n  // Get list of pending assignments\n  SmallVectorImpl<CCValAssign> &getPendingLocs() {\n    return PendingLocs;\n  }\n\n  // Get a list of argflags for pending assignments.\n  SmallVectorImpl<ISD::ArgFlagsTy> &getPendingArgFlags() {\n    return PendingArgFlags;\n  }\n\n  /// Compute the remaining unused register parameters that would be used for\n  /// the given value type. This is useful when varargs are passed in the\n  /// registers that normal prototyped parameters would be passed in, or for\n  /// implementing perfect forwarding.\n  void getRemainingRegParmsForType(SmallVectorImpl<MCPhysReg> &Regs, MVT VT,\n                                   CCAssignFn Fn);\n\n  /// Compute the set of registers that need to be preserved and forwarded to\n  /// any musttail calls.\n  void analyzeMustTailForwardedRegisters(\n      SmallVectorImpl<ForwardedRegister> &Forwards, ArrayRef<MVT> RegParmTypes,\n      CCAssignFn Fn);\n\n  /// Returns true if the results of the two calling conventions are compatible.\n  /// This is usually part of the check for tailcall eligibility.\n  static bool resultsCompatible(CallingConv::ID CalleeCC,\n                                CallingConv::ID CallerCC, MachineFunction &MF,\n                                LLVMContext &C,\n                                const SmallVectorImpl<ISD::InputArg> &Ins,\n                                CCAssignFn CalleeFn, CCAssignFn CallerFn);\n\n  /// The function runs an additional analysis pass over function arguments.\n  /// It will mark each argument with the attribute flag SecArgPass.\n  /// After running, it will sort the locs list.\n  template <class T>\n  void AnalyzeArgumentsSecondPass(const SmallVectorImpl<T> &Args,\n                                  CCAssignFn Fn) {\n    unsigned NumFirstPassLocs = Locs.size();\n\n    /// Creates similar argument list to \\p Args in which each argument is\n    /// marked using SecArgPass flag.\n    SmallVector<T, 16> SecPassArg;\n    // SmallVector<ISD::InputArg, 16> SecPassArg;\n    for (auto Arg : Args) {\n      Arg.Flags.setSecArgPass();\n      SecPassArg.push_back(Arg);\n    }\n\n    // Run the second argument pass\n    AnalyzeArguments(SecPassArg, Fn);\n\n    // Sort the locations of the arguments according to their original position.\n    SmallVector<CCValAssign, 16> TmpArgLocs;\n    TmpArgLocs.swap(Locs);\n    auto B = TmpArgLocs.begin(), E = TmpArgLocs.end();\n    std::merge(B, B + NumFirstPassLocs, B + NumFirstPassLocs, E,\n               std::back_inserter(Locs),\n               [](const CCValAssign &A, const CCValAssign &B) -> bool {\n                 return A.getValNo() < B.getValNo();\n               });\n  }\n\nprivate:\n  /// MarkAllocated - Mark a register and all of its aliases as allocated.\n  void MarkAllocated(MCPhysReg Reg);\n\n  void MarkUnallocated(MCPhysReg Reg);\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_CALLINGCONVLOWER_H\n"}, "33": {"id": 33, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CSEInfo.h", "content": "//===- llvm/CodeGen/GlobalISel/CSEInfo.h ------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// Provides analysis for continuously CSEing during GISel passes.\n//\n//===----------------------------------------------------------------------===//\n#ifndef LLVM_CODEGEN_GLOBALISEL_CSEINFO_H\n#define LLVM_CODEGEN_GLOBALISEL_CSEINFO_H\n\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/CodeGen/CSEConfigBase.h\"\n#include \"llvm/CodeGen/GlobalISel/GISelChangeObserver.h\"\n#include \"llvm/CodeGen/GlobalISel/GISelWorkList.h\"\n#include \"llvm/CodeGen/MachineFunctionPass.h\"\n#include \"llvm/Support/Allocator.h\"\n#include \"llvm/Support/CodeGen.h\"\n\nnamespace llvm {\nclass MachineBasicBlock;\n\n/// A class that wraps MachineInstrs and derives from FoldingSetNode in order to\n/// be uniqued in a CSEMap. The tradeoff here is extra memory allocations for\n/// UniqueMachineInstr vs making MachineInstr bigger.\nclass UniqueMachineInstr : public FoldingSetNode {\n  friend class GISelCSEInfo;\n  const MachineInstr *MI;\n  explicit UniqueMachineInstr(const MachineInstr *MI) : MI(MI) {}\n\npublic:\n  void Profile(FoldingSetNodeID &ID);\n};\n\n// A CSE config for fully optimized builds.\nclass CSEConfigFull : public CSEConfigBase {\npublic:\n  virtual ~CSEConfigFull() = default;\n  virtual bool shouldCSEOpc(unsigned Opc) override;\n};\n\n// Commonly used for O0 config.\nclass CSEConfigConstantOnly : public CSEConfigBase {\npublic:\n  virtual ~CSEConfigConstantOnly() = default;\n  virtual bool shouldCSEOpc(unsigned Opc) override;\n};\n\n// Returns the standard expected CSEConfig for the given optimization level.\n// We have this logic here so targets can make use of it from their derived\n// TargetPassConfig, but can't put this logic into TargetPassConfig directly\n// because the CodeGen library can't depend on GlobalISel.\nstd::unique_ptr<CSEConfigBase>\ngetStandardCSEConfigForOpt(CodeGenOpt::Level Level);\n\n/// The CSE Analysis object.\n/// This installs itself as a delegate to the MachineFunction to track\n/// new instructions as well as deletions. It however will not be able to\n/// track instruction mutations. In such cases, recordNewInstruction should be\n/// called (for eg inside MachineIRBuilder::recordInsertion).\n/// Also because of how just the instruction can be inserted without adding any\n/// operands to the instruction, instructions are uniqued and inserted lazily.\n/// CSEInfo should assert when trying to enter an incomplete instruction into\n/// the CSEMap. There is Opcode level granularity on which instructions can be\n/// CSE'd and for now, only Generic instructions are CSEable.\nclass GISelCSEInfo : public GISelChangeObserver {\n  // Make it accessible only to CSEMIRBuilder.\n  friend class CSEMIRBuilder;\n\n  BumpPtrAllocator UniqueInstrAllocator;\n  FoldingSet<UniqueMachineInstr> CSEMap;\n  MachineRegisterInfo *MRI = nullptr;\n  MachineFunction *MF = nullptr;\n  std::unique_ptr<CSEConfigBase> CSEOpt;\n  /// Keep a cache of UniqueInstrs for each MachineInstr. In GISel,\n  /// often instructions are mutated (while their ID has completely changed).\n  /// Whenever mutation happens, invalidate the UniqueMachineInstr for the\n  /// MachineInstr\n  DenseMap<const MachineInstr *, UniqueMachineInstr *> InstrMapping;\n\n  /// Store instructions that are not fully formed in TemporaryInsts.\n  /// Also because CSE insertion happens lazily, we can remove insts from this\n  /// list and avoid inserting and then removing from the CSEMap.\n  GISelWorkList<8> TemporaryInsts;\n\n  // Only used in asserts.\n  DenseMap<unsigned, unsigned> OpcodeHitTable;\n\n  bool isUniqueMachineInstValid(const UniqueMachineInstr &UMI) const;\n\n  void invalidateUniqueMachineInstr(UniqueMachineInstr *UMI);\n\n  UniqueMachineInstr *getNodeIfExists(FoldingSetNodeID &ID,\n                                      MachineBasicBlock *MBB, void *&InsertPos);\n\n  /// Allocate and construct a new UniqueMachineInstr for MI and return.\n  UniqueMachineInstr *getUniqueInstrForMI(const MachineInstr *MI);\n\n  void insertNode(UniqueMachineInstr *UMI, void *InsertPos = nullptr);\n\n  /// Get the MachineInstr(Unique) if it exists already in the CSEMap and the\n  /// same MachineBasicBlock.\n  MachineInstr *getMachineInstrIfExists(FoldingSetNodeID &ID,\n                                        MachineBasicBlock *MBB,\n                                        void *&InsertPos);\n\n  /// Use this method to allocate a new UniqueMachineInstr for MI and insert it\n  /// into the CSEMap. MI should return true for shouldCSE(MI->getOpcode())\n  void insertInstr(MachineInstr *MI, void *InsertPos = nullptr);\n\npublic:\n  GISelCSEInfo() = default;\n\n  virtual ~GISelCSEInfo();\n\n  void setMF(MachineFunction &MF);\n\n  Error verify();\n\n  /// Records a newly created inst in a list and lazily insert it to the CSEMap.\n  /// Sometimes, this method might be called with a partially constructed\n  /// MachineInstr,\n  //  (right after BuildMI without adding any operands) - and in such cases,\n  //  defer the hashing of the instruction to a later stage.\n  void recordNewInstruction(MachineInstr *MI);\n\n  /// Use this callback to inform CSE about a newly fully created instruction.\n  void handleRecordedInst(MachineInstr *MI);\n\n  /// Use this callback to insert all the recorded instructions. At this point,\n  /// all of these insts need to be fully constructed and should not be missing\n  /// any operands.\n  void handleRecordedInsts();\n\n  /// Remove this inst from the CSE map. If this inst has not been inserted yet,\n  /// it will be removed from the Tempinsts list if it exists.\n  void handleRemoveInst(MachineInstr *MI);\n\n  void releaseMemory();\n\n  void setCSEConfig(std::unique_ptr<CSEConfigBase> Opt) {\n    CSEOpt = std::move(Opt);\n  }\n\n  bool shouldCSE(unsigned Opc) const;\n\n  void analyze(MachineFunction &MF);\n\n  void countOpcodeHit(unsigned Opc);\n\n  void print();\n\n  // Observer API\n  void erasingInstr(MachineInstr &MI) override;\n  void createdInstr(MachineInstr &MI) override;\n  void changingInstr(MachineInstr &MI) override;\n  void changedInstr(MachineInstr &MI) override;\n};\n\nclass TargetRegisterClass;\nclass RegisterBank;\n\n// Simple builder class to easily profile properties about MIs.\nclass GISelInstProfileBuilder {\n  FoldingSetNodeID &ID;\n  const MachineRegisterInfo &MRI;\n\npublic:\n  GISelInstProfileBuilder(FoldingSetNodeID &ID, const MachineRegisterInfo &MRI)\n      : ID(ID), MRI(MRI) {}\n  // Profiling methods.\n  const GISelInstProfileBuilder &addNodeIDOpcode(unsigned Opc) const;\n  const GISelInstProfileBuilder &addNodeIDRegType(const LLT Ty) const;\n  const GISelInstProfileBuilder &addNodeIDRegType(const Register) const;\n\n  const GISelInstProfileBuilder &\n  addNodeIDRegType(const TargetRegisterClass *RC) const;\n  const GISelInstProfileBuilder &addNodeIDRegType(const RegisterBank *RB) const;\n\n  const GISelInstProfileBuilder &addNodeIDRegNum(Register Reg) const;\n\n  const GISelInstProfileBuilder &addNodeIDReg(Register Reg) const;\n\n  const GISelInstProfileBuilder &addNodeIDImmediate(int64_t Imm) const;\n  const GISelInstProfileBuilder &\n  addNodeIDMBB(const MachineBasicBlock *MBB) const;\n\n  const GISelInstProfileBuilder &\n  addNodeIDMachineOperand(const MachineOperand &MO) const;\n\n  const GISelInstProfileBuilder &addNodeIDFlag(unsigned Flag) const;\n  const GISelInstProfileBuilder &addNodeID(const MachineInstr *MI) const;\n};\n\n/// Simple wrapper that does the following.\n/// 1) Lazily evaluate the MachineFunction to compute CSEable instructions.\n/// 2) Allows configuration of which instructions are CSEd through CSEConfig\n/// object. Provides a method called get which takes a CSEConfig object.\nclass GISelCSEAnalysisWrapper {\n  GISelCSEInfo Info;\n  MachineFunction *MF = nullptr;\n  bool AlreadyComputed = false;\n\npublic:\n  /// Takes a CSEConfigBase object that defines what opcodes get CSEd.\n  /// If CSEConfig is already set, and the CSE Analysis has been preserved,\n  /// it will not use the new CSEOpt(use Recompute to force using the new\n  /// CSEOpt).\n  GISelCSEInfo &get(std::unique_ptr<CSEConfigBase> CSEOpt,\n                    bool ReCompute = false);\n  void setMF(MachineFunction &MFunc) { MF = &MFunc; }\n  void setComputed(bool Computed) { AlreadyComputed = Computed; }\n  void releaseMemory() { Info.releaseMemory(); }\n};\n\n/// The actual analysis pass wrapper.\nclass GISelCSEAnalysisWrapperPass : public MachineFunctionPass {\n  GISelCSEAnalysisWrapper Wrapper;\n\npublic:\n  static char ID;\n  GISelCSEAnalysisWrapperPass();\n\n  void getAnalysisUsage(AnalysisUsage &AU) const override;\n\n  const GISelCSEAnalysisWrapper &getCSEWrapper() const { return Wrapper; }\n  GISelCSEAnalysisWrapper &getCSEWrapper() { return Wrapper; }\n\n  bool runOnMachineFunction(MachineFunction &MF) override;\n\n  void releaseMemory() override {\n    Wrapper.releaseMemory();\n    Wrapper.setComputed(false);\n  }\n};\n\n} // namespace llvm\n\n#endif\n"}, "34": {"id": 34, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CallLowering.h", "content": "//===- llvm/CodeGen/GlobalISel/CallLowering.h - Call lowering ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n///\n/// \\file\n/// This file describes how to lower LLVM calls to machine code calls.\n///\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_CALLLOWERING_H\n#define LLVM_CODEGEN_GLOBALISEL_CALLLOWERING_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/CallingConvLower.h\"\n#include \"llvm/CodeGen/MachineOperand.h\"\n#include \"llvm/CodeGen/TargetCallingConv.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include <cstdint>\n#include <functional>\n\nnamespace llvm {\n\nclass CallBase;\nclass DataLayout;\nclass Function;\nclass FunctionLoweringInfo;\nclass MachineIRBuilder;\nstruct MachinePointerInfo;\nclass MachineRegisterInfo;\nclass TargetLowering;\nclass Value;\n\nclass CallLowering {\n  const TargetLowering *TLI;\n\n  virtual void anchor();\npublic:\n  struct BaseArgInfo {\n    Type *Ty;\n    SmallVector<ISD::ArgFlagsTy, 4> Flags;\n    bool IsFixed;\n\n    BaseArgInfo(Type *Ty,\n                ArrayRef<ISD::ArgFlagsTy> Flags = ArrayRef<ISD::ArgFlagsTy>(),\n                bool IsFixed = true)\n        : Ty(Ty), Flags(Flags.begin(), Flags.end()), IsFixed(IsFixed) {}\n\n    BaseArgInfo() : Ty(nullptr), IsFixed(false) {}\n  };\n\n  struct ArgInfo : public BaseArgInfo {\n    SmallVector<Register, 4> Regs;\n    // If the argument had to be split into multiple parts according to the\n    // target calling convention, then this contains the original vregs\n    // if the argument was an incoming arg.\n    SmallVector<Register, 2> OrigRegs;\n\n    ArgInfo(ArrayRef<Register> Regs, Type *Ty,\n            ArrayRef<ISD::ArgFlagsTy> Flags = ArrayRef<ISD::ArgFlagsTy>(),\n            bool IsFixed = true)\n        : BaseArgInfo(Ty, Flags, IsFixed), Regs(Regs.begin(), Regs.end()) {\n      if (!Regs.empty() && Flags.empty())\n        this->Flags.push_back(ISD::ArgFlagsTy());\n      // FIXME: We should have just one way of saying \"no register\".\n      assert(((Ty->isVoidTy() || Ty->isEmptyTy()) ==\n              (Regs.empty() || Regs[0] == 0)) &&\n             \"only void types should have no register\");\n    }\n\n    ArgInfo() : BaseArgInfo() {}\n  };\n\n  struct CallLoweringInfo {\n    /// Calling convention to be used for the call.\n    CallingConv::ID CallConv = CallingConv::C;\n\n    /// Destination of the call. It should be either a register, globaladdress,\n    /// or externalsymbol.\n    MachineOperand Callee = MachineOperand::CreateImm(0);\n\n    /// Descriptor for the return type of the function.\n    ArgInfo OrigRet;\n\n    /// List of descriptors of the arguments passed to the function.\n    SmallVector<ArgInfo, 8> OrigArgs;\n\n    /// Valid if the call has a swifterror inout parameter, and contains the\n    /// vreg that the swifterror should be copied into after the call.\n    Register SwiftErrorVReg;\n\n    MDNode *KnownCallees = nullptr;\n\n    /// True if the call must be tail call optimized.\n    bool IsMustTailCall = false;\n\n    /// True if the call passes all target-independent checks for tail call\n    /// optimization.\n    bool IsTailCall = false;\n\n    /// True if the call was lowered as a tail call. This is consumed by the\n    /// legalizer. This allows the legalizer to lower libcalls as tail calls.\n    bool LoweredTailCall = false;\n\n    /// True if the call is to a vararg function.\n    bool IsVarArg = false;\n\n    /// True if the function's return value can be lowered to registers.\n    bool CanLowerReturn = true;\n\n    /// VReg to hold the hidden sret parameter.\n    Register DemoteRegister;\n\n    /// The stack index for sret demotion.\n    int DemoteStackIndex;\n  };\n\n  /// Argument handling is mostly uniform between the four places that\n  /// make these decisions: function formal arguments, call\n  /// instruction args, call instruction returns and function\n  /// returns. However, once a decision has been made on where an\n  /// argument should go, exactly what happens can vary slightly. This\n  /// class abstracts the differences.\n  struct ValueHandler {\n    ValueHandler(bool IsIncoming, MachineIRBuilder &MIRBuilder,\n                 MachineRegisterInfo &MRI, CCAssignFn *AssignFn)\n        : MIRBuilder(MIRBuilder), MRI(MRI), AssignFn(AssignFn),\n          IsIncomingArgumentHandler(IsIncoming) {}\n\n    virtual ~ValueHandler() = default;\n\n    /// Returns true if the handler is dealing with incoming arguments,\n    /// i.e. those that move values from some physical location to vregs.\n    bool isIncomingArgumentHandler() const {\n      return IsIncomingArgumentHandler;\n    }\n\n    /// Materialize a VReg containing the address of the specified\n    /// stack-based object. This is either based on a FrameIndex or\n    /// direct SP manipulation, depending on the context. \\p MPO\n    /// should be initialized to an appropriate description of the\n    /// address created.\n    virtual Register getStackAddress(uint64_t Size, int64_t Offset,\n                                     MachinePointerInfo &MPO) = 0;\n\n    /// The specified value has been assigned to a physical register,\n    /// handle the appropriate COPY (either to or from) and mark any\n    /// relevant uses/defines as needed.\n    virtual void assignValueToReg(Register ValVReg, Register PhysReg,\n                                  CCValAssign &VA) = 0;\n\n    /// The specified value has been assigned to a stack\n    /// location. Load or store it there, with appropriate extension\n    /// if necessary.\n    virtual void assignValueToAddress(Register ValVReg, Register Addr,\n                                      uint64_t Size, MachinePointerInfo &MPO,\n                                      CCValAssign &VA) = 0;\n\n    /// An overload which takes an ArgInfo if additional information about the\n    /// arg is needed. \\p ValRegIndex is the index in \\p Arg.Regs for the value\n    /// to store.\n    virtual void assignValueToAddress(const ArgInfo &Arg, unsigned ValRegIndex,\n                                      Register Addr, uint64_t Size,\n                                      MachinePointerInfo &MPO,\n                                      CCValAssign &VA) {\n      assignValueToAddress(Arg.Regs[ValRegIndex], Addr, Size, MPO, VA);\n    }\n\n    /// Handle custom values, which may be passed into one or more of \\p VAs.\n    /// \\return The number of \\p VAs that have been assigned after the first\n    ///         one, and which should therefore be skipped from further\n    ///         processing.\n    virtual unsigned assignCustomValue(const ArgInfo &Arg,\n                                       ArrayRef<CCValAssign> VAs) {\n      // This is not a pure virtual method because not all targets need to worry\n      // about custom values.\n      llvm_unreachable(\"Custom values not supported\");\n    }\n\n    /// Extend a register to the location type given in VA, capped at extending\n    /// to at most MaxSize bits. If MaxSizeBits is 0 then no maximum is set.\n    Register extendRegister(Register ValReg, CCValAssign &VA,\n                            unsigned MaxSizeBits = 0);\n\n    virtual bool assignArg(unsigned ValNo, MVT ValVT, MVT LocVT,\n                           CCValAssign::LocInfo LocInfo, const ArgInfo &Info,\n                           ISD::ArgFlagsTy Flags, CCState &State) {\n      return AssignFn(ValNo, ValVT, LocVT, LocInfo, Flags, State);\n    }\n\n    MachineIRBuilder &MIRBuilder;\n    MachineRegisterInfo &MRI;\n    CCAssignFn *AssignFn;\n\n  private:\n    bool IsIncomingArgumentHandler;\n    virtual void anchor();\n  };\n\n  struct IncomingValueHandler : public ValueHandler {\n    IncomingValueHandler(MachineIRBuilder &MIRBuilder, MachineRegisterInfo &MRI,\n                         CCAssignFn *AssignFn)\n        : ValueHandler(true, MIRBuilder, MRI, AssignFn) {}\n\n    /// Insert G_ASSERT_ZEXT/G_ASSERT_SEXT or other hint instruction based on \\p\n    /// VA, returning the new register if a hint was inserted.\n    Register buildExtensionHint(CCValAssign &VA, Register SrcReg, LLT NarrowTy);\n\n    /// Provides a default implementation for argument handling.\n    void assignValueToReg(Register ValVReg, Register PhysReg,\n                          CCValAssign &VA) override;\n  };\n\n  struct OutgoingValueHandler : public ValueHandler {\n    OutgoingValueHandler(MachineIRBuilder &MIRBuilder, MachineRegisterInfo &MRI,\n                         CCAssignFn *AssignFn)\n        : ValueHandler(false, MIRBuilder, MRI, AssignFn) {}\n  };\n\nprotected:\n  /// Getter for generic TargetLowering class.\n  const TargetLowering *getTLI() const {\n    return TLI;\n  }\n\n  /// Getter for target specific TargetLowering class.\n  template <class XXXTargetLowering>\n    const XXXTargetLowering *getTLI() const {\n    return static_cast<const XXXTargetLowering *>(TLI);\n  }\n\n  /// \\returns Flags corresponding to the attributes on the \\p ArgIdx-th\n  /// parameter of \\p Call.\n  ISD::ArgFlagsTy getAttributesForArgIdx(const CallBase &Call,\n                                         unsigned ArgIdx) const;\n\n  /// Adds flags to \\p Flags based off of the attributes in \\p Attrs.\n  /// \\p OpIdx is the index in \\p Attrs to add flags from.\n  void addArgFlagsFromAttributes(ISD::ArgFlagsTy &Flags,\n                                 const AttributeList &Attrs,\n                                 unsigned OpIdx) const;\n\n  template <typename FuncInfoTy>\n  void setArgFlags(ArgInfo &Arg, unsigned OpIdx, const DataLayout &DL,\n                   const FuncInfoTy &FuncInfo) const;\n\n  /// Break \\p OrigArgInfo into one or more pieces the calling convention can\n  /// process, returned in \\p SplitArgs. For example, this should break structs\n  /// down into individual fields.\n  void splitToValueTypes(const ArgInfo &OrigArgInfo,\n                         SmallVectorImpl<ArgInfo> &SplitArgs,\n                         const DataLayout &DL, CallingConv::ID CallConv) const;\n\n  /// Generate instructions for unpacking \\p SrcReg into the \\p DstRegs\n  /// corresponding to the aggregate type \\p PackedTy.\n  ///\n  /// \\param DstRegs should contain one virtual register for each base type in\n  ///        \\p PackedTy, as returned by computeValueLLTs.\n  void unpackRegs(ArrayRef<Register> DstRegs, Register SrcReg, Type *PackedTy,\n                  MachineIRBuilder &MIRBuilder) const;\n\n  /// Invoke Handler::assignArg on each of the given \\p Args and then use\n  /// \\p Handler to move them to the assigned locations.\n  ///\n  /// \\return True if everything has succeeded, false otherwise.\n  bool handleAssignments(MachineIRBuilder &MIRBuilder,\n                         SmallVectorImpl<ArgInfo> &Args, ValueHandler &Handler,\n                         CallingConv::ID CallConv, bool IsVarArg,\n                         Register ThisReturnReg = Register()) const;\n  bool handleAssignments(CCState &CCState,\n                         SmallVectorImpl<CCValAssign> &ArgLocs,\n                         MachineIRBuilder &MIRBuilder,\n                         SmallVectorImpl<ArgInfo> &Args, ValueHandler &Handler,\n                         Register ThisReturnReg = Register()) const;\n\n  /// Analyze passed or returned values from a call, supplied in \\p ArgInfo,\n  /// incorporating info about the passed values into \\p CCState.\n  ///\n  /// Used to check if arguments are suitable for tail call lowering.\n  bool analyzeArgInfo(CCState &CCState, SmallVectorImpl<ArgInfo> &Args,\n                      CCAssignFn &AssignFnFixed,\n                      CCAssignFn &AssignFnVarArg) const;\n\n  /// Check whether parameters to a call that are passed in callee saved\n  /// registers are the same as from the calling function.  This needs to be\n  /// checked for tail call eligibility.\n  bool parametersInCSRMatch(const MachineRegisterInfo &MRI,\n                            const uint32_t *CallerPreservedMask,\n                            const SmallVectorImpl<CCValAssign> &ArgLocs,\n                            const SmallVectorImpl<ArgInfo> &OutVals) const;\n\n  /// \\returns True if the calling convention for a callee and its caller pass\n  /// results in the same way. Typically used for tail call eligibility checks.\n  ///\n  /// \\p Info is the CallLoweringInfo for the call.\n  /// \\p MF is the MachineFunction for the caller.\n  /// \\p InArgs contains the results of the call.\n  /// \\p CalleeAssignFnFixed is the CCAssignFn to be used for the callee for\n  /// fixed arguments.\n  /// \\p CalleeAssignFnVarArg is similar, but for varargs.\n  /// \\p CallerAssignFnFixed is the CCAssignFn to be used for the caller for\n  /// fixed arguments.\n  /// \\p CallerAssignFnVarArg is similar, but for varargs.\n  bool resultsCompatible(CallLoweringInfo &Info, MachineFunction &MF,\n                         SmallVectorImpl<ArgInfo> &InArgs,\n                         CCAssignFn &CalleeAssignFnFixed,\n                         CCAssignFn &CalleeAssignFnVarArg,\n                         CCAssignFn &CallerAssignFnFixed,\n                         CCAssignFn &CallerAssignFnVarArg) const;\n\npublic:\n  CallLowering(const TargetLowering *TLI) : TLI(TLI) {}\n  virtual ~CallLowering() = default;\n\n  /// \\return true if the target is capable of handling swifterror values that\n  /// have been promoted to a specified register. The extended versions of\n  /// lowerReturn and lowerCall should be implemented.\n  virtual bool supportSwiftError() const {\n    return false;\n  }\n\n  /// Load the returned value from the stack into virtual registers in \\p VRegs.\n  /// It uses the frame index \\p FI and the start offset from \\p DemoteReg.\n  /// The loaded data size will be determined from \\p RetTy.\n  void insertSRetLoads(MachineIRBuilder &MIRBuilder, Type *RetTy,\n                       ArrayRef<Register> VRegs, Register DemoteReg,\n                       int FI) const;\n\n  /// Store the return value given by \\p VRegs into stack starting at the offset\n  /// specified in \\p DemoteReg.\n  void insertSRetStores(MachineIRBuilder &MIRBuilder, Type *RetTy,\n                        ArrayRef<Register> VRegs, Register DemoteReg) const;\n\n  /// Insert the hidden sret ArgInfo to the beginning of \\p SplitArgs.\n  /// This function should be called from the target specific\n  /// lowerFormalArguments when \\p F requires the sret demotion.\n  void insertSRetIncomingArgument(const Function &F,\n                                  SmallVectorImpl<ArgInfo> &SplitArgs,\n                                  Register &DemoteReg, MachineRegisterInfo &MRI,\n                                  const DataLayout &DL) const;\n\n  /// For the call-base described by \\p CB, insert the hidden sret ArgInfo to\n  /// the OrigArgs field of \\p Info.\n  void insertSRetOutgoingArgument(MachineIRBuilder &MIRBuilder,\n                                  const CallBase &CB,\n                                  CallLoweringInfo &Info) const;\n\n  /// \\return True if the return type described by \\p Outs can be returned\n  /// without performing sret demotion.\n  bool checkReturn(CCState &CCInfo, SmallVectorImpl<BaseArgInfo> &Outs,\n                   CCAssignFn *Fn) const;\n\n  /// Get the type and the ArgFlags for the split components of \\p RetTy as\n  /// returned by \\c ComputeValueVTs.\n  void getReturnInfo(CallingConv::ID CallConv, Type *RetTy, AttributeList Attrs,\n                     SmallVectorImpl<BaseArgInfo> &Outs,\n                     const DataLayout &DL) const;\n\n  /// Toplevel function to check the return type based on the target calling\n  /// convention. \\return True if the return value of \\p MF can be returned\n  /// without performing sret demotion.\n  bool checkReturnTypeForCallConv(MachineFunction &MF) const;\n\n  /// This hook must be implemented to check whether the return values\n  /// described by \\p Outs can fit into the return registers. If false\n  /// is returned, an sret-demotion is performed.\n  virtual bool canLowerReturn(MachineFunction &MF, CallingConv::ID CallConv,\n                              SmallVectorImpl<BaseArgInfo> &Outs,\n                              bool IsVarArg) const {\n    return true;\n  }\n\n  /// This hook must be implemented to lower outgoing return values, described\n  /// by \\p Val, into the specified virtual registers \\p VRegs.\n  /// This hook is used by GlobalISel.\n  ///\n  /// \\p FLI is required for sret demotion.\n  ///\n  /// \\p SwiftErrorVReg is non-zero if the function has a swifterror parameter\n  /// that needs to be implicitly returned.\n  ///\n  /// \\return True if the lowering succeeds, false otherwise.\n  virtual bool lowerReturn(MachineIRBuilder &MIRBuilder, const Value *Val,\n                           ArrayRef<Register> VRegs, FunctionLoweringInfo &FLI,\n                           Register SwiftErrorVReg) const {\n    if (!supportSwiftError()) {\n      assert(SwiftErrorVReg == 0 && \"attempt to use unsupported swifterror\");\n      return lowerReturn(MIRBuilder, Val, VRegs, FLI);\n    }\n    return false;\n  }\n\n  /// This hook behaves as the extended lowerReturn function, but for targets\n  /// that do not support swifterror value promotion.\n  virtual bool lowerReturn(MachineIRBuilder &MIRBuilder, const Value *Val,\n                           ArrayRef<Register> VRegs,\n                           FunctionLoweringInfo &FLI) const {\n    return false;\n  }\n\n  virtual bool fallBackToDAGISel(const Function &F) const { return false; }\n\n  /// This hook must be implemented to lower the incoming (formal)\n  /// arguments, described by \\p VRegs, for GlobalISel. Each argument\n  /// must end up in the related virtual registers described by \\p VRegs.\n  /// In other words, the first argument should end up in \\c VRegs[0],\n  /// the second in \\c VRegs[1], and so on. For each argument, there will be one\n  /// register for each non-aggregate type, as returned by \\c computeValueLLTs.\n  /// \\p MIRBuilder is set to the proper insertion for the argument\n  /// lowering. \\p FLI is required for sret demotion.\n  ///\n  /// \\return True if the lowering succeeded, false otherwise.\n  virtual bool lowerFormalArguments(MachineIRBuilder &MIRBuilder,\n                                    const Function &F,\n                                    ArrayRef<ArrayRef<Register>> VRegs,\n                                    FunctionLoweringInfo &FLI) const {\n    return false;\n  }\n\n  /// This hook must be implemented to lower the given call instruction,\n  /// including argument and return value marshalling.\n  ///\n  ///\n  /// \\return true if the lowering succeeded, false otherwise.\n  virtual bool lowerCall(MachineIRBuilder &MIRBuilder,\n                         CallLoweringInfo &Info) const {\n    return false;\n  }\n\n  /// Lower the given call instruction, including argument and return value\n  /// marshalling.\n  ///\n  /// \\p CI is the call/invoke instruction.\n  ///\n  /// \\p ResRegs are the registers where the call's return value should be\n  /// stored (or 0 if there is no return value). There will be one register for\n  /// each non-aggregate type, as returned by \\c computeValueLLTs.\n  ///\n  /// \\p ArgRegs is a list of lists of virtual registers containing each\n  /// argument that needs to be passed (argument \\c i should be placed in \\c\n  /// ArgRegs[i]). For each argument, there will be one register for each\n  /// non-aggregate type, as returned by \\c computeValueLLTs.\n  ///\n  /// \\p SwiftErrorVReg is non-zero if the call has a swifterror inout\n  /// parameter, and contains the vreg that the swifterror should be copied into\n  /// after the call.\n  ///\n  /// \\p GetCalleeReg is a callback to materialize a register for the callee if\n  /// the target determines it cannot jump to the destination based purely on \\p\n  /// CI. This might be because \\p CI is indirect, or because of the limited\n  /// range of an immediate jump.\n  ///\n  /// \\return true if the lowering succeeded, false otherwise.\n  bool lowerCall(MachineIRBuilder &MIRBuilder, const CallBase &Call,\n                 ArrayRef<Register> ResRegs,\n                 ArrayRef<ArrayRef<Register>> ArgRegs, Register SwiftErrorVReg,\n                 std::function<unsigned()> GetCalleeReg) const;\n\n  /// For targets which support the \"returned\" parameter attribute, returns\n  /// true if the given type is a valid one to use with \"returned\".\n  virtual bool isTypeIsValidForThisReturn(EVT Ty) const { return false; }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_GLOBALISEL_CALLLOWERING_H\n"}, "35": {"id": 35, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/GISelChangeObserver.h", "content": "//===----- llvm/CodeGen/GlobalISel/GISelChangeObserver.h --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// This contains common code to allow clients to notify changes to machine\n/// instr.\n//\n//===----------------------------------------------------------------------===//\n#ifndef LLVM_CODEGEN_GLOBALISEL_GISELCHANGEOBSERVER_H\n#define LLVM_CODEGEN_GLOBALISEL_GISELCHANGEOBSERVER_H\n\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n\nnamespace llvm {\nclass MachineInstr;\nclass MachineRegisterInfo;\n\n/// Abstract class that contains various methods for clients to notify about\n/// changes. This should be the preferred way for APIs to notify changes.\n/// Typically calling erasingInstr/createdInstr multiple times should not affect\n/// the result. The observer would likely need to check if it was already\n/// notified earlier (consider using GISelWorkList).\nclass GISelChangeObserver {\n  SmallPtrSet<MachineInstr *, 4> ChangingAllUsesOfReg;\n\npublic:\n  virtual ~GISelChangeObserver() {}\n\n  /// An instruction is about to be erased.\n  virtual void erasingInstr(MachineInstr &MI) = 0;\n\n  /// An instruction has been created and inserted into the function.\n  /// Note that the instruction might not be a fully fledged instruction at this\n  /// point and won't be if the MachineFunction::Delegate is calling it. This is\n  /// because the delegate only sees the construction of the MachineInstr before\n  /// operands have been added.\n  virtual void createdInstr(MachineInstr &MI) = 0;\n\n  /// This instruction is about to be mutated in some way.\n  virtual void changingInstr(MachineInstr &MI) = 0;\n\n  /// This instruction was mutated in some way.\n  virtual void changedInstr(MachineInstr &MI) = 0;\n\n  /// All the instructions using the given register are being changed.\n  /// For convenience, finishedChangingAllUsesOfReg() will report the completion\n  /// of the changes. The use list may change between this call and\n  /// finishedChangingAllUsesOfReg().\n  void changingAllUsesOfReg(const MachineRegisterInfo &MRI, Register Reg);\n  /// All instructions reported as changing by changingAllUsesOfReg() have\n  /// finished being changed.\n  void finishedChangingAllUsesOfReg();\n\n};\n\n/// Simple wrapper observer that takes several observers, and calls\n/// each one for each event. If there are multiple observers (say CSE,\n/// Legalizer, Combiner), it's sufficient to register this to the machine\n/// function as the delegate.\nclass GISelObserverWrapper : public MachineFunction::Delegate,\n                             public GISelChangeObserver {\n  SmallVector<GISelChangeObserver *, 4> Observers;\n\npublic:\n  GISelObserverWrapper() = default;\n  GISelObserverWrapper(ArrayRef<GISelChangeObserver *> Obs)\n      : Observers(Obs.begin(), Obs.end()) {}\n  // Adds an observer.\n  void addObserver(GISelChangeObserver *O) { Observers.push_back(O); }\n  // Removes an observer from the list and does nothing if observer is not\n  // present.\n  void removeObserver(GISelChangeObserver *O) {\n    auto It = std::find(Observers.begin(), Observers.end(), O);\n    if (It != Observers.end())\n      Observers.erase(It);\n  }\n  // API for Observer.\n  void erasingInstr(MachineInstr &MI) override {\n    for (auto &O : Observers)\n      O->erasingInstr(MI);\n  }\n  void createdInstr(MachineInstr &MI) override {\n    for (auto &O : Observers)\n      O->createdInstr(MI);\n  }\n  void changingInstr(MachineInstr &MI) override {\n    for (auto &O : Observers)\n      O->changingInstr(MI);\n  }\n  void changedInstr(MachineInstr &MI) override {\n    for (auto &O : Observers)\n      O->changedInstr(MI);\n  }\n  // API for MachineFunction::Delegate\n  void MF_HandleInsertion(MachineInstr &MI) override { createdInstr(MI); }\n  void MF_HandleRemoval(MachineInstr &MI) override { erasingInstr(MI); }\n};\n\n/// A simple RAII based Delegate installer.\n/// Use this in a scope to install a delegate to the MachineFunction and reset\n/// it at the end of the scope.\nclass RAIIDelegateInstaller {\n  MachineFunction &MF;\n  MachineFunction::Delegate *Delegate;\n\npublic:\n  RAIIDelegateInstaller(MachineFunction &MF, MachineFunction::Delegate *Del);\n  ~RAIIDelegateInstaller();\n};\n\n/// A simple RAII based Observer installer.\n/// Use this in a scope to install the Observer to the MachineFunction and reset\n/// it at the end of the scope.\nclass RAIIMFObserverInstaller {\n  MachineFunction &MF;\n\npublic:\n  RAIIMFObserverInstaller(MachineFunction &MF, GISelChangeObserver &Observer);\n  ~RAIIMFObserverInstaller();\n};\n\n/// Class to install both of the above.\nclass RAIIMFObsDelInstaller {\n  RAIIDelegateInstaller DelI;\n  RAIIMFObserverInstaller ObsI;\n\npublic:\n  RAIIMFObsDelInstaller(MachineFunction &MF, GISelObserverWrapper &Wrapper)\n      : DelI(MF, &Wrapper), ObsI(MF, Wrapper) {}\n  ~RAIIMFObsDelInstaller() = default;\n};\n\n} // namespace llvm\n#endif\n"}, "36": {"id": 36, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "content": "//===- llvm/CodeGen/GlobalISel/LegalizerInfo.h ------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// Interface for Targets to specify which operations they can successfully\n/// select and how the others should be expanded most efficiently.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_LEGALIZERINFO_H\n#define LLVM_CODEGEN_GLOBALISEL_LEGALIZERINFO_H\n\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallBitVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/TargetOpcodes.h\"\n#include \"llvm/Support/CommandLine.h\"\n#include \"llvm/Support/LowLevelTypeImpl.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include <cassert>\n#include <cstdint>\n#include <tuple>\n#include <unordered_map>\n#include <utility>\n\nnamespace llvm {\n\nextern cl::opt<bool> DisableGISelLegalityCheck;\n\nclass LegalizerHelper;\nclass MachineInstr;\nclass MachineRegisterInfo;\nclass MCInstrInfo;\nclass GISelChangeObserver;\n\nnamespace LegalizeActions {\nenum LegalizeAction : std::uint8_t {\n  /// The operation is expected to be selectable directly by the target, and\n  /// no transformation is necessary.\n  Legal,\n\n  /// The operation should be synthesized from multiple instructions acting on\n  /// a narrower scalar base-type. For example a 64-bit add might be\n  /// implemented in terms of 32-bit add-with-carry.\n  NarrowScalar,\n\n  /// The operation should be implemented in terms of a wider scalar\n  /// base-type. For example a <2 x s8> add could be implemented as a <2\n  /// x s32> add (ignoring the high bits).\n  WidenScalar,\n\n  /// The (vector) operation should be implemented by splitting it into\n  /// sub-vectors where the operation is legal. For example a <8 x s64> add\n  /// might be implemented as 4 separate <2 x s64> adds.\n  FewerElements,\n\n  /// The (vector) operation should be implemented by widening the input\n  /// vector and ignoring the lanes added by doing so. For example <2 x i8> is\n  /// rarely legal, but you might perform an <8 x i8> and then only look at\n  /// the first two results.\n  MoreElements,\n\n  /// Perform the operation on a different, but equivalently sized type.\n  Bitcast,\n\n  /// The operation itself must be expressed in terms of simpler actions on\n  /// this target. E.g. a SREM replaced by an SDIV and subtraction.\n  Lower,\n\n  /// The operation should be implemented as a call to some kind of runtime\n  /// support library. For example this usually happens on machines that don't\n  /// support floating-point operations natively.\n  Libcall,\n\n  /// The target wants to do something special with this combination of\n  /// operand and type. A callback will be issued when it is needed.\n  Custom,\n\n  /// This operation is completely unsupported on the target. A programming\n  /// error has occurred.\n  Unsupported,\n\n  /// Sentinel value for when no action was found in the specified table.\n  NotFound,\n\n  /// Fall back onto the old rules.\n  /// TODO: Remove this once we've migrated\n  UseLegacyRules,\n};\n} // end namespace LegalizeActions\nraw_ostream &operator<<(raw_ostream &OS, LegalizeActions::LegalizeAction Action);\n\nusing LegalizeActions::LegalizeAction;\n\n/// Legalization is decided based on an instruction's opcode, which type slot\n/// we're considering, and what the existing type is. These aspects are gathered\n/// together for convenience in the InstrAspect class.\nstruct InstrAspect {\n  unsigned Opcode;\n  unsigned Idx = 0;\n  LLT Type;\n\n  InstrAspect(unsigned Opcode, LLT Type) : Opcode(Opcode), Type(Type) {}\n  InstrAspect(unsigned Opcode, unsigned Idx, LLT Type)\n      : Opcode(Opcode), Idx(Idx), Type(Type) {}\n\n  bool operator==(const InstrAspect &RHS) const {\n    return Opcode == RHS.Opcode && Idx == RHS.Idx && Type == RHS.Type;\n  }\n};\n\n/// The LegalityQuery object bundles together all the information that's needed\n/// to decide whether a given operation is legal or not.\n/// For efficiency, it doesn't make a copy of Types so care must be taken not\n/// to free it before using the query.\nstruct LegalityQuery {\n  unsigned Opcode;\n  ArrayRef<LLT> Types;\n\n  struct MemDesc {\n    uint64_t SizeInBits;\n    uint64_t AlignInBits;\n    AtomicOrdering Ordering;\n  };\n\n  /// Operations which require memory can use this to place requirements on the\n  /// memory type for each MMO.\n  ArrayRef<MemDesc> MMODescrs;\n\n  constexpr LegalityQuery(unsigned Opcode, const ArrayRef<LLT> Types,\n                          const ArrayRef<MemDesc> MMODescrs)\n      : Opcode(Opcode), Types(Types), MMODescrs(MMODescrs) {}\n  constexpr LegalityQuery(unsigned Opcode, const ArrayRef<LLT> Types)\n      : LegalityQuery(Opcode, Types, {}) {}\n\n  raw_ostream &print(raw_ostream &OS) const;\n};\n\n/// The result of a query. It either indicates a final answer of Legal or\n/// Unsupported or describes an action that must be taken to make an operation\n/// more legal.\nstruct LegalizeActionStep {\n  /// The action to take or the final answer.\n  LegalizeAction Action;\n  /// If describing an action, the type index to change. Otherwise zero.\n  unsigned TypeIdx;\n  /// If describing an action, the new type for TypeIdx. Otherwise LLT{}.\n  LLT NewType;\n\n  LegalizeActionStep(LegalizeAction Action, unsigned TypeIdx,\n                     const LLT NewType)\n      : Action(Action), TypeIdx(TypeIdx), NewType(NewType) {}\n\n  bool operator==(const LegalizeActionStep &RHS) const {\n    return std::tie(Action, TypeIdx, NewType) ==\n        std::tie(RHS.Action, RHS.TypeIdx, RHS.NewType);\n  }\n};\n\nusing LegalityPredicate = std::function<bool (const LegalityQuery &)>;\nusing LegalizeMutation =\n    std::function<std::pair<unsigned, LLT>(const LegalityQuery &)>;\n\nnamespace LegalityPredicates {\nstruct TypePairAndMemDesc {\n  LLT Type0;\n  LLT Type1;\n  uint64_t MemSize;\n  uint64_t Align;\n\n  bool operator==(const TypePairAndMemDesc &Other) const {\n    return Type0 == Other.Type0 && Type1 == Other.Type1 &&\n           Align == Other.Align &&\n           MemSize == Other.MemSize;\n  }\n\n  /// \\returns true if this memory access is legal with for the access described\n  /// by \\p Other (The alignment is sufficient for the size and result type).\n  bool isCompatible(const TypePairAndMemDesc &Other) const {\n    return Type0 == Other.Type0 && Type1 == Other.Type1 &&\n           Align >= Other.Align &&\n           MemSize == Other.MemSize;\n  }\n};\n\n/// True iff P0 and P1 are true.\ntemplate<typename Predicate>\nPredicate all(Predicate P0, Predicate P1) {\n  return [=](const LegalityQuery &Query) {\n    return P0(Query) && P1(Query);\n  };\n}\n/// True iff all given predicates are true.\ntemplate<typename Predicate, typename... Args>\nPredicate all(Predicate P0, Predicate P1, Args... args) {\n  return all(all(P0, P1), args...);\n}\n\n/// True iff P0 or P1 are true.\ntemplate<typename Predicate>\nPredicate any(Predicate P0, Predicate P1) {\n  return [=](const LegalityQuery &Query) {\n    return P0(Query) || P1(Query);\n  };\n}\n/// True iff any given predicates are true.\ntemplate<typename Predicate, typename... Args>\nPredicate any(Predicate P0, Predicate P1, Args... args) {\n  return any(any(P0, P1), args...);\n}\n\n/// True iff the given type index is the specified type.\nLegalityPredicate typeIs(unsigned TypeIdx, LLT TypesInit);\n/// True iff the given type index is one of the specified types.\nLegalityPredicate typeInSet(unsigned TypeIdx,\n                            std::initializer_list<LLT> TypesInit);\n\n/// True iff the given type index is not the specified type.\ninline LegalityPredicate typeIsNot(unsigned TypeIdx, LLT Type) {\n  return [=](const LegalityQuery &Query) {\n           return Query.Types[TypeIdx] != Type;\n         };\n}\n\n/// True iff the given types for the given pair of type indexes is one of the\n/// specified type pairs.\nLegalityPredicate\ntypePairInSet(unsigned TypeIdx0, unsigned TypeIdx1,\n              std::initializer_list<std::pair<LLT, LLT>> TypesInit);\n/// True iff the given types for the given pair of type indexes is one of the\n/// specified type pairs.\nLegalityPredicate typePairAndMemDescInSet(\n    unsigned TypeIdx0, unsigned TypeIdx1, unsigned MMOIdx,\n    std::initializer_list<TypePairAndMemDesc> TypesAndMemDescInit);\n/// True iff the specified type index is a scalar.\nLegalityPredicate isScalar(unsigned TypeIdx);\n/// True iff the specified type index is a vector.\nLegalityPredicate isVector(unsigned TypeIdx);\n/// True iff the specified type index is a pointer (with any address space).\nLegalityPredicate isPointer(unsigned TypeIdx);\n/// True iff the specified type index is a pointer with the specified address\n/// space.\nLegalityPredicate isPointer(unsigned TypeIdx, unsigned AddrSpace);\n\n/// True if the type index is a vector with element type \\p EltTy\nLegalityPredicate elementTypeIs(unsigned TypeIdx, LLT EltTy);\n\n/// True iff the specified type index is a scalar that's narrower than the given\n/// size.\nLegalityPredicate scalarNarrowerThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar that's wider than the given\n/// size.\nLegalityPredicate scalarWiderThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar or vector with an element type\n/// that's narrower than the given size.\nLegalityPredicate scalarOrEltNarrowerThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar or a vector with an element\n/// type that's wider than the given size.\nLegalityPredicate scalarOrEltWiderThan(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type index is a scalar whose size is not a power of\n/// 2.\nLegalityPredicate sizeNotPow2(unsigned TypeIdx);\n\n/// True iff the specified type index is a scalar or vector whose element size\n/// is not a power of 2.\nLegalityPredicate scalarOrEltSizeNotPow2(unsigned TypeIdx);\n\n/// True if the total bitwidth of the specified type index is \\p Size bits.\nLegalityPredicate sizeIs(unsigned TypeIdx, unsigned Size);\n\n/// True iff the specified type indices are both the same bit size.\nLegalityPredicate sameSize(unsigned TypeIdx0, unsigned TypeIdx1);\n\n/// True iff the first type index has a larger total bit size than second type\n/// index.\nLegalityPredicate largerThan(unsigned TypeIdx0, unsigned TypeIdx1);\n\n/// True iff the first type index has a smaller total bit size than second type\n/// index.\nLegalityPredicate smallerThan(unsigned TypeIdx0, unsigned TypeIdx1);\n\n/// True iff the specified MMO index has a size that is not a power of 2\nLegalityPredicate memSizeInBytesNotPow2(unsigned MMOIdx);\n/// True iff the specified type index is a vector whose element count is not a\n/// power of 2.\nLegalityPredicate numElementsNotPow2(unsigned TypeIdx);\n/// True iff the specified MMO index has at an atomic ordering of at Ordering or\n/// stronger.\nLegalityPredicate atomicOrderingAtLeastOrStrongerThan(unsigned MMOIdx,\n                                                      AtomicOrdering Ordering);\n} // end namespace LegalityPredicates\n\nnamespace LegalizeMutations {\n/// Select this specific type for the given type index.\nLegalizeMutation changeTo(unsigned TypeIdx, LLT Ty);\n\n/// Keep the same type as the given type index.\nLegalizeMutation changeTo(unsigned TypeIdx, unsigned FromTypeIdx);\n\n/// Keep the same scalar or element type as the given type index.\nLegalizeMutation changeElementTo(unsigned TypeIdx, unsigned FromTypeIdx);\n\n/// Keep the same scalar or element type as the given type.\nLegalizeMutation changeElementTo(unsigned TypeIdx, LLT Ty);\n\n/// Change the scalar size or element size to have the same scalar size as type\n/// index \\p FromIndex. Unlike changeElementTo, this discards pointer types and\n/// only changes the size.\nLegalizeMutation changeElementSizeTo(unsigned TypeIdx, unsigned FromTypeIdx);\n\n/// Widen the scalar type or vector element type for the given type index to the\n/// next power of 2.\nLegalizeMutation widenScalarOrEltToNextPow2(unsigned TypeIdx, unsigned Min = 0);\n\n/// Add more elements to the type for the given type index to the next power of\n/// 2.\nLegalizeMutation moreElementsToNextPow2(unsigned TypeIdx, unsigned Min = 0);\n/// Break up the vector type for the given type index into the element type.\nLegalizeMutation scalarize(unsigned TypeIdx);\n} // end namespace LegalizeMutations\n\n/// A single rule in a legalizer info ruleset.\n/// The specified action is chosen when the predicate is true. Where appropriate\n/// for the action (e.g. for WidenScalar) the new type is selected using the\n/// given mutator.\nclass LegalizeRule {\n  LegalityPredicate Predicate;\n  LegalizeAction Action;\n  LegalizeMutation Mutation;\n\npublic:\n  LegalizeRule(LegalityPredicate Predicate, LegalizeAction Action,\n               LegalizeMutation Mutation = nullptr)\n      : Predicate(Predicate), Action(Action), Mutation(Mutation) {}\n\n  /// Test whether the LegalityQuery matches.\n  bool match(const LegalityQuery &Query) const {\n    return Predicate(Query);\n  }\n\n  LegalizeAction getAction() const { return Action; }\n\n  /// Determine the change to make.\n  std::pair<unsigned, LLT> determineMutation(const LegalityQuery &Query) const {\n    if (Mutation)\n      return Mutation(Query);\n    return std::make_pair(0, LLT{});\n  }\n};\n\nclass LegalizeRuleSet {\n  /// When non-zero, the opcode we are an alias of\n  unsigned AliasOf;\n  /// If true, there is another opcode that aliases this one\n  bool IsAliasedByAnother;\n  SmallVector<LegalizeRule, 2> Rules;\n\n#ifndef NDEBUG\n  /// If bit I is set, this rule set contains a rule that may handle (predicate\n  /// or perform an action upon (or both)) the type index I. The uncertainty\n  /// comes from free-form rules executing user-provided lambda functions. We\n  /// conservatively assume such rules do the right thing and cover all type\n  /// indices. The bitset is intentionally 1 bit wider than it absolutely needs\n  /// to be to distinguish such cases from the cases where all type indices are\n  /// individually handled.\n  SmallBitVector TypeIdxsCovered{MCOI::OPERAND_LAST_GENERIC -\n                                 MCOI::OPERAND_FIRST_GENERIC + 2};\n  SmallBitVector ImmIdxsCovered{MCOI::OPERAND_LAST_GENERIC_IMM -\n                                MCOI::OPERAND_FIRST_GENERIC_IMM + 2};\n#endif\n\n  unsigned typeIdx(unsigned TypeIdx) {\n    assert(TypeIdx <=\n               (MCOI::OPERAND_LAST_GENERIC - MCOI::OPERAND_FIRST_GENERIC) &&\n           \"Type Index is out of bounds\");\n#ifndef NDEBUG\n    TypeIdxsCovered.set(TypeIdx);\n#endif\n    return TypeIdx;\n  }\n\n  unsigned immIdx(unsigned ImmIdx) {\n    assert(ImmIdx <= (MCOI::OPERAND_LAST_GENERIC_IMM -\n                      MCOI::OPERAND_FIRST_GENERIC_IMM) &&\n           \"Imm Index is out of bounds\");\n#ifndef NDEBUG\n    ImmIdxsCovered.set(ImmIdx);\n#endif\n    return ImmIdx;\n  }\n\n  void markAllIdxsAsCovered() {\n#ifndef NDEBUG\n    TypeIdxsCovered.set();\n    ImmIdxsCovered.set();\n#endif\n  }\n\n  void add(const LegalizeRule &Rule) {\n    assert(AliasOf == 0 &&\n           \"RuleSet is aliased, change the representative opcode instead\");\n    Rules.push_back(Rule);\n  }\n\n  static bool always(const LegalityQuery &) { return true; }\n\n  /// Use the given action when the predicate is true.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionIf(LegalizeAction Action,\n                            LegalityPredicate Predicate) {\n    add({Predicate, Action});\n    return *this;\n  }\n  /// Use the given action when the predicate is true.\n  /// Action should be an action that requires mutation.\n  LegalizeRuleSet &actionIf(LegalizeAction Action, LegalityPredicate Predicate,\n                            LegalizeMutation Mutation) {\n    add({Predicate, Action, Mutation});\n    return *this;\n  }\n  /// Use the given action when type index 0 is any type in the given list.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<LLT> Types) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typeInSet(typeIdx(0), Types));\n  }\n  /// Use the given action when type index 0 is any type in the given list.\n  /// Action should be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<LLT> Types,\n                             LegalizeMutation Mutation) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typeInSet(typeIdx(0), Types), Mutation);\n  }\n  /// Use the given action when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<std::pair<LLT, LLT>> Types) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typePairInSet(typeIdx(0), typeIdx(1), Types));\n  }\n  /// Use the given action when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  /// Action should be an action that requires mutation.\n  LegalizeRuleSet &actionFor(LegalizeAction Action,\n                             std::initializer_list<std::pair<LLT, LLT>> Types,\n                             LegalizeMutation Mutation) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, typePairInSet(typeIdx(0), typeIdx(1), Types),\n                    Mutation);\n  }\n  /// Use the given action when type index 0 is any type in the given list and\n  /// imm index 0 is anything. Action should not be an action that requires\n  /// mutation.\n  LegalizeRuleSet &actionForTypeWithAnyImm(LegalizeAction Action,\n                                           std::initializer_list<LLT> Types) {\n    using namespace LegalityPredicates;\n    immIdx(0); // Inform verifier imm idx 0 is handled.\n    return actionIf(Action, typeInSet(typeIdx(0), Types));\n  }\n\n  LegalizeRuleSet &actionForTypeWithAnyImm(\n    LegalizeAction Action, std::initializer_list<std::pair<LLT, LLT>> Types) {\n    using namespace LegalityPredicates;\n    immIdx(0); // Inform verifier imm idx 0 is handled.\n    return actionIf(Action, typePairInSet(typeIdx(0), typeIdx(1), Types));\n  }\n\n  /// Use the given action when type indexes 0 and 1 are both in the given list.\n  /// That is, the type pair is in the cartesian product of the list.\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionForCartesianProduct(LegalizeAction Action,\n                                             std::initializer_list<LLT> Types) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, all(typeInSet(typeIdx(0), Types),\n                                typeInSet(typeIdx(1), Types)));\n  }\n  /// Use the given action when type indexes 0 and 1 are both in their\n  /// respective lists.\n  /// That is, the type pair is in the cartesian product of the lists\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &\n  actionForCartesianProduct(LegalizeAction Action,\n                            std::initializer_list<LLT> Types0,\n                            std::initializer_list<LLT> Types1) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, all(typeInSet(typeIdx(0), Types0),\n                                typeInSet(typeIdx(1), Types1)));\n  }\n  /// Use the given action when type indexes 0, 1, and 2 are all in their\n  /// respective lists.\n  /// That is, the type triple is in the cartesian product of the lists\n  /// Action should not be an action that requires mutation.\n  LegalizeRuleSet &actionForCartesianProduct(\n      LegalizeAction Action, std::initializer_list<LLT> Types0,\n      std::initializer_list<LLT> Types1, std::initializer_list<LLT> Types2) {\n    using namespace LegalityPredicates;\n    return actionIf(Action, all(typeInSet(typeIdx(0), Types0),\n                                all(typeInSet(typeIdx(1), Types1),\n                                    typeInSet(typeIdx(2), Types2))));\n  }\n\npublic:\n  LegalizeRuleSet() : AliasOf(0), IsAliasedByAnother(false), Rules() {}\n\n  bool isAliasedByAnother() { return IsAliasedByAnother; }\n  void setIsAliasedByAnother() { IsAliasedByAnother = true; }\n  void aliasTo(unsigned Opcode) {\n    assert((AliasOf == 0 || AliasOf == Opcode) &&\n           \"Opcode is already aliased to another opcode\");\n    assert(Rules.empty() && \"Aliasing will discard rules\");\n    AliasOf = Opcode;\n  }\n  unsigned getAlias() const { return AliasOf; }\n\n  /// The instruction is legal if predicate is true.\n  LegalizeRuleSet &legalIf(LegalityPredicate Predicate) {\n    // We have no choice but conservatively assume that the free-form\n    // user-provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Legal, Predicate);\n  }\n  /// The instruction is legal when type index 0 is any type in the given list.\n  LegalizeRuleSet &legalFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Legal, Types);\n  }\n  /// The instruction is legal when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  LegalizeRuleSet &legalFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Legal, Types);\n  }\n  /// The instruction is legal when type index 0 is any type in the given list\n  /// and imm index 0 is anything.\n  LegalizeRuleSet &legalForTypeWithAnyImm(std::initializer_list<LLT> Types) {\n    markAllIdxsAsCovered();\n    return actionForTypeWithAnyImm(LegalizeAction::Legal, Types);\n  }\n\n  LegalizeRuleSet &legalForTypeWithAnyImm(\n    std::initializer_list<std::pair<LLT, LLT>> Types) {\n    markAllIdxsAsCovered();\n    return actionForTypeWithAnyImm(LegalizeAction::Legal, Types);\n  }\n\n  /// The instruction is legal when type indexes 0 and 1 along with the memory\n  /// size and minimum alignment is any type and size tuple in the given list.\n  LegalizeRuleSet &legalForTypesWithMemDesc(\n      std::initializer_list<LegalityPredicates::TypePairAndMemDesc>\n          TypesAndMemDesc) {\n    return actionIf(LegalizeAction::Legal,\n                    LegalityPredicates::typePairAndMemDescInSet(\n                        typeIdx(0), typeIdx(1), /*MMOIdx*/ 0, TypesAndMemDesc));\n  }\n  /// The instruction is legal when type indexes 0 and 1 are both in the given\n  /// list. That is, the type pair is in the cartesian product of the list.\n  LegalizeRuleSet &legalForCartesianProduct(std::initializer_list<LLT> Types) {\n    return actionForCartesianProduct(LegalizeAction::Legal, Types);\n  }\n  /// The instruction is legal when type indexes 0 and 1 are both their\n  /// respective lists.\n  LegalizeRuleSet &legalForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1) {\n    return actionForCartesianProduct(LegalizeAction::Legal, Types0, Types1);\n  }\n  /// The instruction is legal when type indexes 0, 1, and 2 are both their\n  /// respective lists.\n  LegalizeRuleSet &legalForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1,\n                                            std::initializer_list<LLT> Types2) {\n    return actionForCartesianProduct(LegalizeAction::Legal, Types0, Types1,\n                                     Types2);\n  }\n\n  LegalizeRuleSet &alwaysLegal() {\n    using namespace LegalizeMutations;\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Legal, always);\n  }\n\n  /// The specified type index is coerced if predicate is true.\n  LegalizeRuleSet &bitcastIf(LegalityPredicate Predicate,\n                             LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that lowering with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Bitcast, Predicate, Mutation);\n  }\n\n  /// The instruction is lowered.\n  LegalizeRuleSet &lower() {\n    using namespace LegalizeMutations;\n    // We have no choice but conservatively assume that predicate-less lowering\n    // properly handles all type indices by design:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Lower, always);\n  }\n  /// The instruction is lowered if predicate is true. Keep type index 0 as the\n  /// same type.\n  LegalizeRuleSet &lowerIf(LegalityPredicate Predicate) {\n    using namespace LegalizeMutations;\n    // We have no choice but conservatively assume that lowering with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Lower, Predicate);\n  }\n  /// The instruction is lowered if predicate is true.\n  LegalizeRuleSet &lowerIf(LegalityPredicate Predicate,\n                           LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that lowering with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Lower, Predicate, Mutation);\n  }\n  /// The instruction is lowered when type index 0 is any type in the given\n  /// list. Keep type index 0 as the same type.\n  LegalizeRuleSet &lowerFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Lower, Types);\n  }\n  /// The instruction is lowered when type index 0 is any type in the given\n  /// list.\n  LegalizeRuleSet &lowerFor(std::initializer_list<LLT> Types,\n                            LegalizeMutation Mutation) {\n    return actionFor(LegalizeAction::Lower, Types, Mutation);\n  }\n  /// The instruction is lowered when type indexes 0 and 1 is any type pair in\n  /// the given list. Keep type index 0 as the same type.\n  LegalizeRuleSet &lowerFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Lower, Types);\n  }\n  /// The instruction is lowered when type indexes 0 and 1 is any type pair in\n  /// the given list.\n  LegalizeRuleSet &lowerFor(std::initializer_list<std::pair<LLT, LLT>> Types,\n                            LegalizeMutation Mutation) {\n    return actionFor(LegalizeAction::Lower, Types, Mutation);\n  }\n  /// The instruction is lowered when type indexes 0 and 1 are both in their\n  /// respective lists.\n  LegalizeRuleSet &lowerForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1) {\n    using namespace LegalityPredicates;\n    return actionForCartesianProduct(LegalizeAction::Lower, Types0, Types1);\n  }\n  /// The instruction is lowered when when type indexes 0, 1, and 2 are all in\n  /// their respective lists.\n  LegalizeRuleSet &lowerForCartesianProduct(std::initializer_list<LLT> Types0,\n                                            std::initializer_list<LLT> Types1,\n                                            std::initializer_list<LLT> Types2) {\n    using namespace LegalityPredicates;\n    return actionForCartesianProduct(LegalizeAction::Lower, Types0, Types1,\n                                     Types2);\n  }\n\n  /// The instruction is emitted as a library call.\n  LegalizeRuleSet &libcall() {\n    using namespace LegalizeMutations;\n    // We have no choice but conservatively assume that predicate-less lowering\n    // properly handles all type indices by design:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Libcall, always);\n  }\n\n  /// Like legalIf, but for the Libcall action.\n  LegalizeRuleSet &libcallIf(LegalityPredicate Predicate) {\n    // We have no choice but conservatively assume that a libcall with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Libcall, Predicate);\n  }\n  LegalizeRuleSet &libcallFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Libcall, Types);\n  }\n  LegalizeRuleSet &\n  libcallFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Libcall, Types);\n  }\n  LegalizeRuleSet &\n  libcallForCartesianProduct(std::initializer_list<LLT> Types) {\n    return actionForCartesianProduct(LegalizeAction::Libcall, Types);\n  }\n  LegalizeRuleSet &\n  libcallForCartesianProduct(std::initializer_list<LLT> Types0,\n                             std::initializer_list<LLT> Types1) {\n    return actionForCartesianProduct(LegalizeAction::Libcall, Types0, Types1);\n  }\n\n  /// Widen the scalar to the one selected by the mutation if the predicate is\n  /// true.\n  LegalizeRuleSet &widenScalarIf(LegalityPredicate Predicate,\n                                 LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::WidenScalar, Predicate, Mutation);\n  }\n  /// Narrow the scalar to the one selected by the mutation if the predicate is\n  /// true.\n  LegalizeRuleSet &narrowScalarIf(LegalityPredicate Predicate,\n                                  LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::NarrowScalar, Predicate, Mutation);\n  }\n  /// Narrow the scalar, specified in mutation, when type indexes 0 and 1 is any\n  /// type pair in the given list.\n  LegalizeRuleSet &\n  narrowScalarFor(std::initializer_list<std::pair<LLT, LLT>> Types,\n                  LegalizeMutation Mutation) {\n    return actionFor(LegalizeAction::NarrowScalar, Types, Mutation);\n  }\n\n  /// Add more elements to reach the type selected by the mutation if the\n  /// predicate is true.\n  LegalizeRuleSet &moreElementsIf(LegalityPredicate Predicate,\n                                  LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::MoreElements, Predicate, Mutation);\n  }\n  /// Remove elements to reach the type selected by the mutation if the\n  /// predicate is true.\n  LegalizeRuleSet &fewerElementsIf(LegalityPredicate Predicate,\n                                   LegalizeMutation Mutation) {\n    // We have no choice but conservatively assume that an action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::FewerElements, Predicate, Mutation);\n  }\n\n  /// The instruction is unsupported.\n  LegalizeRuleSet &unsupported() {\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Unsupported, always);\n  }\n  LegalizeRuleSet &unsupportedIf(LegalityPredicate Predicate) {\n    return actionIf(LegalizeAction::Unsupported, Predicate);\n  }\n\n  LegalizeRuleSet &unsupportedFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Unsupported, Types);\n  }\n\n  LegalizeRuleSet &unsupportedIfMemSizeNotPow2() {\n    return actionIf(LegalizeAction::Unsupported,\n                    LegalityPredicates::memSizeInBytesNotPow2(0));\n  }\n  LegalizeRuleSet &lowerIfMemSizeNotPow2() {\n    return actionIf(LegalizeAction::Lower,\n                    LegalityPredicates::memSizeInBytesNotPow2(0));\n  }\n\n  LegalizeRuleSet &customIf(LegalityPredicate Predicate) {\n    // We have no choice but conservatively assume that a custom action with a\n    // free-form user provided Predicate properly handles all type indices:\n    markAllIdxsAsCovered();\n    return actionIf(LegalizeAction::Custom, Predicate);\n  }\n  LegalizeRuleSet &customFor(std::initializer_list<LLT> Types) {\n    return actionFor(LegalizeAction::Custom, Types);\n  }\n\n  /// The instruction is custom when type indexes 0 and 1 is any type pair in the\n  /// given list.\n  LegalizeRuleSet &customFor(std::initializer_list<std::pair<LLT, LLT>> Types) {\n    return actionFor(LegalizeAction::Custom, Types);\n  }\n\n  LegalizeRuleSet &customForCartesianProduct(std::initializer_list<LLT> Types) {\n    return actionForCartesianProduct(LegalizeAction::Custom, Types);\n  }\n  LegalizeRuleSet &\n  customForCartesianProduct(std::initializer_list<LLT> Types0,\n                            std::initializer_list<LLT> Types1) {\n    return actionForCartesianProduct(LegalizeAction::Custom, Types0, Types1);\n  }\n\n  /// Unconditionally custom lower.\n  LegalizeRuleSet &custom() {\n    return customIf(always);\n  }\n\n  /// Widen the scalar to the next power of two that is at least MinSize.\n  /// No effect if the type is not a scalar or is a power of two.\n  LegalizeRuleSet &widenScalarToNextPow2(unsigned TypeIdx,\n                                         unsigned MinSize = 0) {\n    using namespace LegalityPredicates;\n    return actionIf(\n        LegalizeAction::WidenScalar, sizeNotPow2(typeIdx(TypeIdx)),\n        LegalizeMutations::widenScalarOrEltToNextPow2(TypeIdx, MinSize));\n  }\n\n  /// Widen the scalar or vector element type to the next power of two that is\n  /// at least MinSize.  No effect if the scalar size is a power of two.\n  LegalizeRuleSet &widenScalarOrEltToNextPow2(unsigned TypeIdx,\n                                              unsigned MinSize = 0) {\n    using namespace LegalityPredicates;\n    return actionIf(\n        LegalizeAction::WidenScalar, scalarOrEltSizeNotPow2(typeIdx(TypeIdx)),\n        LegalizeMutations::widenScalarOrEltToNextPow2(TypeIdx, MinSize));\n  }\n\n  LegalizeRuleSet &narrowScalar(unsigned TypeIdx, LegalizeMutation Mutation) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::NarrowScalar, isScalar(typeIdx(TypeIdx)),\n                    Mutation);\n  }\n\n  LegalizeRuleSet &scalarize(unsigned TypeIdx) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::FewerElements, isVector(typeIdx(TypeIdx)),\n                    LegalizeMutations::scalarize(TypeIdx));\n  }\n\n  LegalizeRuleSet &scalarizeIf(LegalityPredicate Predicate, unsigned TypeIdx) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::FewerElements,\n                    all(Predicate, isVector(typeIdx(TypeIdx))),\n                    LegalizeMutations::scalarize(TypeIdx));\n  }\n\n  /// Ensure the scalar or element is at least as wide as Ty.\n  LegalizeRuleSet &minScalarOrElt(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::WidenScalar,\n                    scalarOrEltNarrowerThan(TypeIdx, Ty.getScalarSizeInBits()),\n                    changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar or element is at least as wide as Ty.\n  LegalizeRuleSet &minScalarOrEltIf(LegalityPredicate Predicate,\n                                    unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::WidenScalar,\n                    all(Predicate, scalarOrEltNarrowerThan(\n                                       TypeIdx, Ty.getScalarSizeInBits())),\n                    changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar is at least as wide as Ty.\n  LegalizeRuleSet &minScalar(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::WidenScalar,\n                    scalarNarrowerThan(TypeIdx, Ty.getSizeInBits()),\n                    changeTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar is at most as wide as Ty.\n  LegalizeRuleSet &maxScalarOrElt(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::NarrowScalar,\n                    scalarOrEltWiderThan(TypeIdx, Ty.getScalarSizeInBits()),\n                    changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Ensure the scalar is at most as wide as Ty.\n  LegalizeRuleSet &maxScalar(unsigned TypeIdx, const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(LegalizeAction::NarrowScalar,\n                    scalarWiderThan(TypeIdx, Ty.getSizeInBits()),\n                    changeTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Conditionally limit the maximum size of the scalar.\n  /// For example, when the maximum size of one type depends on the size of\n  /// another such as extracting N bits from an M bit container.\n  LegalizeRuleSet &maxScalarIf(LegalityPredicate Predicate, unsigned TypeIdx,\n                               const LLT Ty) {\n    using namespace LegalityPredicates;\n    using namespace LegalizeMutations;\n    return actionIf(\n        LegalizeAction::NarrowScalar,\n        [=](const LegalityQuery &Query) {\n          const LLT QueryTy = Query.Types[TypeIdx];\n          return QueryTy.isScalar() &&\n                 QueryTy.getSizeInBits() > Ty.getSizeInBits() &&\n                 Predicate(Query);\n        },\n        changeElementTo(typeIdx(TypeIdx), Ty));\n  }\n\n  /// Limit the range of scalar sizes to MinTy and MaxTy.\n  LegalizeRuleSet &clampScalar(unsigned TypeIdx, const LLT MinTy,\n                               const LLT MaxTy) {\n    assert(MinTy.isScalar() && MaxTy.isScalar() && \"Expected scalar types\");\n    return minScalar(TypeIdx, MinTy).maxScalar(TypeIdx, MaxTy);\n  }\n\n  /// Limit the range of scalar sizes to MinTy and MaxTy.\n  LegalizeRuleSet &clampScalarOrElt(unsigned TypeIdx, const LLT MinTy,\n                                    const LLT MaxTy) {\n    return minScalarOrElt(TypeIdx, MinTy).maxScalarOrElt(TypeIdx, MaxTy);\n  }\n\n  /// Widen the scalar to match the size of another.\n  LegalizeRuleSet &minScalarSameAs(unsigned TypeIdx, unsigned LargeTypeIdx) {\n    typeIdx(TypeIdx);\n    return widenScalarIf(\n        [=](const LegalityQuery &Query) {\n          return Query.Types[LargeTypeIdx].getScalarSizeInBits() >\n                 Query.Types[TypeIdx].getSizeInBits();\n        },\n        LegalizeMutations::changeElementSizeTo(TypeIdx, LargeTypeIdx));\n  }\n\n  /// Narrow the scalar to match the size of another.\n  LegalizeRuleSet &maxScalarSameAs(unsigned TypeIdx, unsigned NarrowTypeIdx) {\n    typeIdx(TypeIdx);\n    return narrowScalarIf(\n        [=](const LegalityQuery &Query) {\n          return Query.Types[NarrowTypeIdx].getScalarSizeInBits() <\n                 Query.Types[TypeIdx].getSizeInBits();\n        },\n        LegalizeMutations::changeElementSizeTo(TypeIdx, NarrowTypeIdx));\n  }\n\n  /// Change the type \\p TypeIdx to have the same scalar size as type \\p\n  /// SameSizeIdx.\n  LegalizeRuleSet &scalarSameSizeAs(unsigned TypeIdx, unsigned SameSizeIdx) {\n    return minScalarSameAs(TypeIdx, SameSizeIdx)\n          .maxScalarSameAs(TypeIdx, SameSizeIdx);\n  }\n\n  /// Conditionally widen the scalar or elt to match the size of another.\n  LegalizeRuleSet &minScalarEltSameAsIf(LegalityPredicate Predicate,\n                                   unsigned TypeIdx, unsigned LargeTypeIdx) {\n    typeIdx(TypeIdx);\n    return widenScalarIf(\n        [=](const LegalityQuery &Query) {\n          return Query.Types[LargeTypeIdx].getScalarSizeInBits() >\n                     Query.Types[TypeIdx].getScalarSizeInBits() &&\n                 Predicate(Query);\n        },\n        [=](const LegalityQuery &Query) {\n          LLT T = Query.Types[LargeTypeIdx];\n          return std::make_pair(TypeIdx, T);\n        });\n  }\n\n  /// Add more elements to the vector to reach the next power of two.\n  /// No effect if the type is not a vector or the element count is a power of\n  /// two.\n  LegalizeRuleSet &moreElementsToNextPow2(unsigned TypeIdx) {\n    using namespace LegalityPredicates;\n    return actionIf(LegalizeAction::MoreElements,\n                    numElementsNotPow2(typeIdx(TypeIdx)),\n                    LegalizeMutations::moreElementsToNextPow2(TypeIdx));\n  }\n\n  /// Limit the number of elements in EltTy vectors to at least MinElements.\n  LegalizeRuleSet &clampMinNumElements(unsigned TypeIdx, const LLT EltTy,\n                                       unsigned MinElements) {\n    // Mark the type index as covered:\n    typeIdx(TypeIdx);\n    return actionIf(\n        LegalizeAction::MoreElements,\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          return VecTy.isVector() && VecTy.getElementType() == EltTy &&\n                 VecTy.getNumElements() < MinElements;\n        },\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          return std::make_pair(\n              TypeIdx, LLT::vector(MinElements, VecTy.getElementType()));\n        });\n  }\n  /// Limit the number of elements in EltTy vectors to at most MaxElements.\n  LegalizeRuleSet &clampMaxNumElements(unsigned TypeIdx, const LLT EltTy,\n                                       unsigned MaxElements) {\n    // Mark the type index as covered:\n    typeIdx(TypeIdx);\n    return actionIf(\n        LegalizeAction::FewerElements,\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          return VecTy.isVector() && VecTy.getElementType() == EltTy &&\n                 VecTy.getNumElements() > MaxElements;\n        },\n        [=](const LegalityQuery &Query) {\n          LLT VecTy = Query.Types[TypeIdx];\n          LLT NewTy = LLT::scalarOrVector(MaxElements, VecTy.getElementType());\n          return std::make_pair(TypeIdx, NewTy);\n        });\n  }\n  /// Limit the number of elements for the given vectors to at least MinTy's\n  /// number of elements and at most MaxTy's number of elements.\n  ///\n  /// No effect if the type is not a vector or does not have the same element\n  /// type as the constraints.\n  /// The element type of MinTy and MaxTy must match.\n  LegalizeRuleSet &clampNumElements(unsigned TypeIdx, const LLT MinTy,\n                                    const LLT MaxTy) {\n    assert(MinTy.getElementType() == MaxTy.getElementType() &&\n           \"Expected element types to agree\");\n\n    const LLT EltTy = MinTy.getElementType();\n    return clampMinNumElements(TypeIdx, EltTy, MinTy.getNumElements())\n        .clampMaxNumElements(TypeIdx, EltTy, MaxTy.getNumElements());\n  }\n\n  /// Fallback on the previous implementation. This should only be used while\n  /// porting a rule.\n  LegalizeRuleSet &fallback() {\n    add({always, LegalizeAction::UseLegacyRules});\n    return *this;\n  }\n\n  /// Check if there is no type index which is obviously not handled by the\n  /// LegalizeRuleSet in any way at all.\n  /// \\pre Type indices of the opcode form a dense [0, \\p NumTypeIdxs) set.\n  bool verifyTypeIdxsCoverage(unsigned NumTypeIdxs) const;\n  /// Check if there is no imm index which is obviously not handled by the\n  /// LegalizeRuleSet in any way at all.\n  /// \\pre Type indices of the opcode form a dense [0, \\p NumTypeIdxs) set.\n  bool verifyImmIdxsCoverage(unsigned NumImmIdxs) const;\n\n  /// Apply the ruleset to the given LegalityQuery.\n  LegalizeActionStep apply(const LegalityQuery &Query) const;\n};\n\nclass LegalizerInfo {\npublic:\n  LegalizerInfo();\n  virtual ~LegalizerInfo() = default;\n\n  unsigned getOpcodeIdxForOpcode(unsigned Opcode) const;\n  unsigned getActionDefinitionsIdx(unsigned Opcode) const;\n\n  /// Compute any ancillary tables needed to quickly decide how an operation\n  /// should be handled. This must be called after all \"set*Action\"methods but\n  /// before any query is made or incorrect results may be returned.\n  void computeTables();\n\n  /// Perform simple self-diagnostic and assert if there is anything obviously\n  /// wrong with the actions set up.\n  void verify(const MCInstrInfo &MII) const;\n\n  static bool needsLegalizingToDifferentSize(const LegalizeAction Action) {\n    using namespace LegalizeActions;\n    switch (Action) {\n    case NarrowScalar:\n    case WidenScalar:\n    case FewerElements:\n    case MoreElements:\n    case Unsupported:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  using SizeAndAction = std::pair<uint16_t, LegalizeAction>;\n  using SizeAndActionsVec = std::vector<SizeAndAction>;\n  using SizeChangeStrategy =\n      std::function<SizeAndActionsVec(const SizeAndActionsVec &v)>;\n\n  /// More friendly way to set an action for common types that have an LLT\n  /// representation.\n  /// The LegalizeAction must be one for which NeedsLegalizingToDifferentSize\n  /// returns false.\n  void setAction(const InstrAspect &Aspect, LegalizeAction Action) {\n    assert(!needsLegalizingToDifferentSize(Action));\n    TablesInitialized = false;\n    const unsigned OpcodeIdx = Aspect.Opcode - FirstOp;\n    if (SpecifiedActions[OpcodeIdx].size() <= Aspect.Idx)\n      SpecifiedActions[OpcodeIdx].resize(Aspect.Idx + 1);\n    SpecifiedActions[OpcodeIdx][Aspect.Idx][Aspect.Type] = Action;\n  }\n\n  /// The setAction calls record the non-size-changing legalization actions\n  /// to take on specificly-sized types. The SizeChangeStrategy defines what\n  /// to do when the size of the type needs to be changed to reach a legally\n  /// sized type (i.e., one that was defined through a setAction call).\n  /// e.g.\n  /// setAction ({G_ADD, 0, LLT::scalar(32)}, Legal);\n  /// setLegalizeScalarToDifferentSizeStrategy(\n  ///   G_ADD, 0, widenToLargerTypesAndNarrowToLargest);\n  /// will end up defining getAction({G_ADD, 0, T}) to return the following\n  /// actions for different scalar types T:\n  ///  LLT::scalar(1)..LLT::scalar(31): {WidenScalar, 0, LLT::scalar(32)}\n  ///  LLT::scalar(32):                 {Legal, 0, LLT::scalar(32)}\n  ///  LLT::scalar(33)..:               {NarrowScalar, 0, LLT::scalar(32)}\n  ///\n  /// If no SizeChangeAction gets defined, through this function,\n  /// the default is unsupportedForDifferentSizes.\n  void setLegalizeScalarToDifferentSizeStrategy(const unsigned Opcode,\n                                                const unsigned TypeIdx,\n                                                SizeChangeStrategy S) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (ScalarSizeChangeStrategies[OpcodeIdx].size() <= TypeIdx)\n      ScalarSizeChangeStrategies[OpcodeIdx].resize(TypeIdx + 1);\n    ScalarSizeChangeStrategies[OpcodeIdx][TypeIdx] = S;\n  }\n\n  /// See also setLegalizeScalarToDifferentSizeStrategy.\n  /// This function allows to set the SizeChangeStrategy for vector elements.\n  void setLegalizeVectorElementToDifferentSizeStrategy(const unsigned Opcode,\n                                                       const unsigned TypeIdx,\n                                                       SizeChangeStrategy S) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (VectorElementSizeChangeStrategies[OpcodeIdx].size() <= TypeIdx)\n      VectorElementSizeChangeStrategies[OpcodeIdx].resize(TypeIdx + 1);\n    VectorElementSizeChangeStrategies[OpcodeIdx][TypeIdx] = S;\n  }\n\n  /// A SizeChangeStrategy for the common case where legalization for a\n  /// particular operation consists of only supporting a specific set of type\n  /// sizes. E.g.\n  ///   setAction ({G_DIV, 0, LLT::scalar(32)}, Legal);\n  ///   setAction ({G_DIV, 0, LLT::scalar(64)}, Legal);\n  ///   setLegalizeScalarToDifferentSizeStrategy(\n  ///     G_DIV, 0, unsupportedForDifferentSizes);\n  /// will result in getAction({G_DIV, 0, T}) to return Legal for s32 and s64,\n  /// and Unsupported for all other scalar types T.\n  static SizeAndActionsVec\n  unsupportedForDifferentSizes(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return increaseToLargerTypesAndDecreaseToLargest(v, Unsupported,\n                                                     Unsupported);\n  }\n\n  /// A SizeChangeStrategy for the common case where legalization for a\n  /// particular operation consists of widening the type to a large legal type,\n  /// unless there is no such type and then instead it should be narrowed to the\n  /// largest legal type.\n  static SizeAndActionsVec\n  widenToLargerTypesAndNarrowToLargest(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    assert(v.size() > 0 &&\n           \"At least one size that can be legalized towards is needed\"\n           \" for this SizeChangeStrategy\");\n    return increaseToLargerTypesAndDecreaseToLargest(v, WidenScalar,\n                                                     NarrowScalar);\n  }\n\n  static SizeAndActionsVec\n  widenToLargerTypesUnsupportedOtherwise(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return increaseToLargerTypesAndDecreaseToLargest(v, WidenScalar,\n                                                     Unsupported);\n  }\n\n  static SizeAndActionsVec\n  narrowToSmallerAndUnsupportedIfTooSmall(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return decreaseToSmallerTypesAndIncreaseToSmallest(v, NarrowScalar,\n                                                       Unsupported);\n  }\n\n  static SizeAndActionsVec\n  narrowToSmallerAndWidenToSmallest(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    assert(v.size() > 0 &&\n           \"At least one size that can be legalized towards is needed\"\n           \" for this SizeChangeStrategy\");\n    return decreaseToSmallerTypesAndIncreaseToSmallest(v, NarrowScalar,\n                                                       WidenScalar);\n  }\n\n  /// A SizeChangeStrategy for the common case where legalization for a\n  /// particular vector operation consists of having more elements in the\n  /// vector, to a type that is legal. Unless there is no such type and then\n  /// instead it should be legalized towards the widest vector that's still\n  /// legal. E.g.\n  ///   setAction({G_ADD, LLT::vector(8, 8)}, Legal);\n  ///   setAction({G_ADD, LLT::vector(16, 8)}, Legal);\n  ///   setAction({G_ADD, LLT::vector(2, 32)}, Legal);\n  ///   setAction({G_ADD, LLT::vector(4, 32)}, Legal);\n  ///   setLegalizeVectorElementToDifferentSizeStrategy(\n  ///     G_ADD, 0, moreToWiderTypesAndLessToWidest);\n  /// will result in the following getAction results:\n  ///   * getAction({G_ADD, LLT::vector(8,8)}) returns\n  ///       (Legal, vector(8,8)).\n  ///   * getAction({G_ADD, LLT::vector(9,8)}) returns\n  ///       (MoreElements, vector(16,8)).\n  ///   * getAction({G_ADD, LLT::vector(8,32)}) returns\n  ///       (FewerElements, vector(4,32)).\n  static SizeAndActionsVec\n  moreToWiderTypesAndLessToWidest(const SizeAndActionsVec &v) {\n    using namespace LegalizeActions;\n    return increaseToLargerTypesAndDecreaseToLargest(v, MoreElements,\n                                                     FewerElements);\n  }\n\n  /// Helper function to implement many typical SizeChangeStrategy functions.\n  static SizeAndActionsVec\n  increaseToLargerTypesAndDecreaseToLargest(const SizeAndActionsVec &v,\n                                            LegalizeAction IncreaseAction,\n                                            LegalizeAction DecreaseAction);\n  /// Helper function to implement many typical SizeChangeStrategy functions.\n  static SizeAndActionsVec\n  decreaseToSmallerTypesAndIncreaseToSmallest(const SizeAndActionsVec &v,\n                                              LegalizeAction DecreaseAction,\n                                              LegalizeAction IncreaseAction);\n\n  /// Get the action definitions for the given opcode. Use this to run a\n  /// LegalityQuery through the definitions.\n  const LegalizeRuleSet &getActionDefinitions(unsigned Opcode) const;\n\n  /// Get the action definition builder for the given opcode. Use this to define\n  /// the action definitions.\n  ///\n  /// It is an error to request an opcode that has already been requested by the\n  /// multiple-opcode variant.\n  LegalizeRuleSet &getActionDefinitionsBuilder(unsigned Opcode);\n\n  /// Get the action definition builder for the given set of opcodes. Use this\n  /// to define the action definitions for multiple opcodes at once. The first\n  /// opcode given will be considered the representative opcode and will hold\n  /// the definitions whereas the other opcodes will be configured to refer to\n  /// the representative opcode. This lowers memory requirements and very\n  /// slightly improves performance.\n  ///\n  /// It would be very easy to introduce unexpected side-effects as a result of\n  /// this aliasing if it were permitted to request different but intersecting\n  /// sets of opcodes but that is difficult to keep track of. It is therefore an\n  /// error to request the same opcode twice using this API, to request an\n  /// opcode that already has definitions, or to use the single-opcode API on an\n  /// opcode that has already been requested by this API.\n  LegalizeRuleSet &\n  getActionDefinitionsBuilder(std::initializer_list<unsigned> Opcodes);\n  void aliasActionDefinitions(unsigned OpcodeTo, unsigned OpcodeFrom);\n\n  /// Determine what action should be taken to legalize the described\n  /// instruction. Requires computeTables to have been called.\n  ///\n  /// \\returns a description of the next legalization step to perform.\n  LegalizeActionStep getAction(const LegalityQuery &Query) const;\n\n  /// Determine what action should be taken to legalize the given generic\n  /// instruction.\n  ///\n  /// \\returns a description of the next legalization step to perform.\n  LegalizeActionStep getAction(const MachineInstr &MI,\n                               const MachineRegisterInfo &MRI) const;\n\n  bool isLegal(const LegalityQuery &Query) const {\n    return getAction(Query).Action == LegalizeAction::Legal;\n  }\n\n  bool isLegalOrCustom(const LegalityQuery &Query) const {\n    auto Action = getAction(Query).Action;\n    return Action == LegalizeAction::Legal || Action == LegalizeAction::Custom;\n  }\n\n  bool isLegal(const MachineInstr &MI, const MachineRegisterInfo &MRI) const;\n  bool isLegalOrCustom(const MachineInstr &MI,\n                       const MachineRegisterInfo &MRI) const;\n\n  /// Called for instructions with the Custom LegalizationAction.\n  virtual bool legalizeCustom(LegalizerHelper &Helper,\n                              MachineInstr &MI) const {\n    llvm_unreachable(\"must implement this if custom action is used\");\n  }\n\n  /// \\returns true if MI is either legal or has been legalized and false if not\n  /// legal.\n  /// Return true if MI is either legal or has been legalized and false\n  /// if not legal.\n  virtual bool legalizeIntrinsic(LegalizerHelper &Helper,\n                                 MachineInstr &MI) const {\n    return true;\n  }\n\n  /// Return the opcode (SEXT/ZEXT/ANYEXT) that should be performed while\n  /// widening a constant of type SmallTy which targets can override.\n  /// For eg, the DAG does (SmallTy.isByteSized() ? G_SEXT : G_ZEXT) which\n  /// will be the default.\n  virtual unsigned getExtOpcodeForWideningConstant(LLT SmallTy) const;\n\nprivate:\n  /// Determine what action should be taken to legalize the given generic\n  /// instruction opcode, type-index and type. Requires computeTables to have\n  /// been called.\n  ///\n  /// \\returns a pair consisting of the kind of legalization that should be\n  /// performed and the destination type.\n  std::pair<LegalizeAction, LLT>\n  getAspectAction(const InstrAspect &Aspect) const;\n\n  /// The SizeAndActionsVec is a representation mapping between all natural\n  /// numbers and an Action. The natural number represents the bit size of\n  /// the InstrAspect. For example, for a target with native support for 32-bit\n  /// and 64-bit additions, you'd express that as:\n  /// setScalarAction(G_ADD, 0,\n  ///           {{1, WidenScalar},  // bit sizes [ 1, 31[\n  ///            {32, Legal},       // bit sizes [32, 33[\n  ///            {33, WidenScalar}, // bit sizes [33, 64[\n  ///            {64, Legal},       // bit sizes [64, 65[\n  ///            {65, NarrowScalar} // bit sizes [65, +inf[\n  ///           });\n  /// It may be that only 64-bit pointers are supported on your target:\n  /// setPointerAction(G_PTR_ADD, 0, LLT:pointer(1),\n  ///           {{1, Unsupported},  // bit sizes [ 1, 63[\n  ///            {64, Legal},       // bit sizes [64, 65[\n  ///            {65, Unsupported}, // bit sizes [65, +inf[\n  ///           });\n  void setScalarAction(const unsigned Opcode, const unsigned TypeIndex,\n                       const SizeAndActionsVec &SizeAndActions) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    SmallVector<SizeAndActionsVec, 1> &Actions = ScalarActions[OpcodeIdx];\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n  void setPointerAction(const unsigned Opcode, const unsigned TypeIndex,\n                        const unsigned AddressSpace,\n                        const SizeAndActionsVec &SizeAndActions) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (AddrSpace2PointerActions[OpcodeIdx].find(AddressSpace) ==\n        AddrSpace2PointerActions[OpcodeIdx].end())\n      AddrSpace2PointerActions[OpcodeIdx][AddressSpace] = {{}};\n    SmallVector<SizeAndActionsVec, 1> &Actions =\n        AddrSpace2PointerActions[OpcodeIdx].find(AddressSpace)->second;\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n\n  /// If an operation on a given vector type (say <M x iN>) isn't explicitly\n  /// specified, we proceed in 2 stages. First we legalize the underlying scalar\n  /// (so that there's at least one legal vector with that scalar), then we\n  /// adjust the number of elements in the vector so that it is legal. The\n  /// desired action in the first step is controlled by this function.\n  void setScalarInVectorAction(const unsigned Opcode, const unsigned TypeIndex,\n                               const SizeAndActionsVec &SizeAndActions) {\n    unsigned OpcodeIdx = Opcode - FirstOp;\n    SmallVector<SizeAndActionsVec, 1> &Actions =\n        ScalarInVectorActions[OpcodeIdx];\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n\n  /// See also setScalarInVectorAction.\n  /// This function let's you specify the number of elements in a vector that\n  /// are legal for a legal element size.\n  void setVectorNumElementAction(const unsigned Opcode,\n                                 const unsigned TypeIndex,\n                                 const unsigned ElementSize,\n                                 const SizeAndActionsVec &SizeAndActions) {\n    const unsigned OpcodeIdx = Opcode - FirstOp;\n    if (NumElements2Actions[OpcodeIdx].find(ElementSize) ==\n        NumElements2Actions[OpcodeIdx].end())\n      NumElements2Actions[OpcodeIdx][ElementSize] = {{}};\n    SmallVector<SizeAndActionsVec, 1> &Actions =\n        NumElements2Actions[OpcodeIdx].find(ElementSize)->second;\n    setActions(TypeIndex, Actions, SizeAndActions);\n  }\n\n  /// A partial SizeAndActionsVec potentially doesn't cover all bit sizes,\n  /// i.e. it's OK if it doesn't start from size 1.\n  static void checkPartialSizeAndActionsVector(const SizeAndActionsVec& v) {\n    using namespace LegalizeActions;\n#ifndef NDEBUG\n    // The sizes should be in increasing order\n    int prev_size = -1;\n    for(auto SizeAndAction: v) {\n      assert(SizeAndAction.first > prev_size);\n      prev_size = SizeAndAction.first;\n    }\n    // - for every Widen action, there should be a larger bitsize that\n    //   can be legalized towards (e.g. Legal, Lower, Libcall or Custom\n    //   action).\n    // - for every Narrow action, there should be a smaller bitsize that\n    //   can be legalized towards.\n    int SmallestNarrowIdx = -1;\n    int LargestWidenIdx = -1;\n    int SmallestLegalizableToSameSizeIdx = -1;\n    int LargestLegalizableToSameSizeIdx = -1;\n    for(size_t i=0; i<v.size(); ++i) {\n      switch (v[i].second) {\n        case FewerElements:\n        case NarrowScalar:\n          if (SmallestNarrowIdx == -1)\n            SmallestNarrowIdx = i;\n          break;\n        case WidenScalar:\n        case MoreElements:\n          LargestWidenIdx = i;\n          break;\n        case Unsupported:\n          break;\n        default:\n          if (SmallestLegalizableToSameSizeIdx == -1)\n            SmallestLegalizableToSameSizeIdx = i;\n          LargestLegalizableToSameSizeIdx = i;\n      }\n    }\n    if (SmallestNarrowIdx != -1) {\n      assert(SmallestLegalizableToSameSizeIdx != -1);\n      assert(SmallestNarrowIdx > SmallestLegalizableToSameSizeIdx);\n    }\n    if (LargestWidenIdx != -1)\n      assert(LargestWidenIdx < LargestLegalizableToSameSizeIdx);\n#endif\n  }\n\n  /// A full SizeAndActionsVec must cover all bit sizes, i.e. must start with\n  /// from size 1.\n  static void checkFullSizeAndActionsVector(const SizeAndActionsVec& v) {\n#ifndef NDEBUG\n    // Data structure invariant: The first bit size must be size 1.\n    assert(v.size() >= 1);\n    assert(v[0].first == 1);\n    checkPartialSizeAndActionsVector(v);\n#endif\n  }\n\n  /// Sets actions for all bit sizes on a particular generic opcode, type\n  /// index and scalar or pointer type.\n  void setActions(unsigned TypeIndex,\n                  SmallVector<SizeAndActionsVec, 1> &Actions,\n                  const SizeAndActionsVec &SizeAndActions) {\n    checkFullSizeAndActionsVector(SizeAndActions);\n    if (Actions.size() <= TypeIndex)\n      Actions.resize(TypeIndex + 1);\n    Actions[TypeIndex] = SizeAndActions;\n  }\n\n  static SizeAndAction findAction(const SizeAndActionsVec &Vec,\n                                  const uint32_t Size);\n\n  /// Returns the next action needed to get the scalar or pointer type closer\n  /// to being legal\n  /// E.g. findLegalAction({G_REM, 13}) should return\n  /// (WidenScalar, 32). After that, findLegalAction({G_REM, 32}) will\n  /// probably be called, which should return (Lower, 32).\n  /// This is assuming the setScalarAction on G_REM was something like:\n  /// setScalarAction(G_REM, 0,\n  ///           {{1, WidenScalar},  // bit sizes [ 1, 31[\n  ///            {32, Lower},       // bit sizes [32, 33[\n  ///            {33, NarrowScalar} // bit sizes [65, +inf[\n  ///           });\n  std::pair<LegalizeAction, LLT>\n  findScalarLegalAction(const InstrAspect &Aspect) const;\n\n  /// Returns the next action needed towards legalizing the vector type.\n  std::pair<LegalizeAction, LLT>\n  findVectorLegalAction(const InstrAspect &Aspect) const;\n\n  static const int FirstOp = TargetOpcode::PRE_ISEL_GENERIC_OPCODE_START;\n  static const int LastOp = TargetOpcode::PRE_ISEL_GENERIC_OPCODE_END;\n\n  // Data structures used temporarily during construction of legality data:\n  using TypeMap = DenseMap<LLT, LegalizeAction>;\n  SmallVector<TypeMap, 1> SpecifiedActions[LastOp - FirstOp + 1];\n  SmallVector<SizeChangeStrategy, 1>\n      ScalarSizeChangeStrategies[LastOp - FirstOp + 1];\n  SmallVector<SizeChangeStrategy, 1>\n      VectorElementSizeChangeStrategies[LastOp - FirstOp + 1];\n  bool TablesInitialized;\n\n  // Data structures used by getAction:\n  SmallVector<SizeAndActionsVec, 1> ScalarActions[LastOp - FirstOp + 1];\n  SmallVector<SizeAndActionsVec, 1> ScalarInVectorActions[LastOp - FirstOp + 1];\n  std::unordered_map<uint16_t, SmallVector<SizeAndActionsVec, 1>>\n      AddrSpace2PointerActions[LastOp - FirstOp + 1];\n  std::unordered_map<uint16_t, SmallVector<SizeAndActionsVec, 1>>\n      NumElements2Actions[LastOp - FirstOp + 1];\n\n  LegalizeRuleSet RulesForOpcode[LastOp - FirstOp + 1];\n};\n\n#ifndef NDEBUG\n/// Checks that MIR is fully legal, returns an illegal instruction if it's not,\n/// nullptr otherwise\nconst MachineInstr *machineFunctionIsIllegal(const MachineFunction &MF);\n#endif\n\n} // end namespace llvm.\n\n#endif // LLVM_CODEGEN_GLOBALISEL_LEGALIZERINFO_H\n"}, "37": {"id": 37, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "content": "//===-- llvm/CodeGen/GlobalISel/MachineIRBuilder.h - MIBuilder --*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n/// \\file\n/// This file declares the MachineIRBuilder class.\n/// This is a helper class to build MachineInstr.\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_MACHINEIRBUILDER_H\n#define LLVM_CODEGEN_GLOBALISEL_MACHINEIRBUILDER_H\n\n#include \"llvm/CodeGen/GlobalISel/CSEInfo.h\"\n#include \"llvm/CodeGen/LowLevelType.h\"\n#include \"llvm/CodeGen/MachineBasicBlock.h\"\n#include \"llvm/CodeGen/MachineInstrBuilder.h\"\n#include \"llvm/CodeGen/MachineRegisterInfo.h\"\n#include \"llvm/CodeGen/TargetOpcodes.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/Module.h\"\n\nnamespace llvm {\n\n// Forward declarations.\nclass MachineFunction;\nclass MachineInstr;\nclass TargetInstrInfo;\nclass GISelChangeObserver;\n\n/// Class which stores all the state required in a MachineIRBuilder.\n/// Since MachineIRBuilders will only store state in this object, it allows\n/// to transfer BuilderState between different kinds of MachineIRBuilders.\nstruct MachineIRBuilderState {\n  /// MachineFunction under construction.\n  MachineFunction *MF = nullptr;\n  /// Information used to access the description of the opcodes.\n  const TargetInstrInfo *TII = nullptr;\n  /// Information used to verify types are consistent and to create virtual registers.\n  MachineRegisterInfo *MRI = nullptr;\n  /// Debug location to be set to any instruction we create.\n  DebugLoc DL;\n\n  /// \\name Fields describing the insertion point.\n  /// @{\n  MachineBasicBlock *MBB = nullptr;\n  MachineBasicBlock::iterator II;\n  /// @}\n\n  GISelChangeObserver *Observer = nullptr;\n\n  GISelCSEInfo *CSEInfo = nullptr;\n};\n\nclass DstOp {\n  union {\n    LLT LLTTy;\n    Register Reg;\n    const TargetRegisterClass *RC;\n  };\n\npublic:\n  enum class DstType { Ty_LLT, Ty_Reg, Ty_RC };\n  DstOp(unsigned R) : Reg(R), Ty(DstType::Ty_Reg) {}\n  DstOp(Register R) : Reg(R), Ty(DstType::Ty_Reg) {}\n  DstOp(const MachineOperand &Op) : Reg(Op.getReg()), Ty(DstType::Ty_Reg) {}\n  DstOp(const LLT T) : LLTTy(T), Ty(DstType::Ty_LLT) {}\n  DstOp(const TargetRegisterClass *TRC) : RC(TRC), Ty(DstType::Ty_RC) {}\n\n  void addDefToMIB(MachineRegisterInfo &MRI, MachineInstrBuilder &MIB) const {\n    switch (Ty) {\n    case DstType::Ty_Reg:\n      MIB.addDef(Reg);\n      break;\n    case DstType::Ty_LLT:\n      MIB.addDef(MRI.createGenericVirtualRegister(LLTTy));\n      break;\n    case DstType::Ty_RC:\n      MIB.addDef(MRI.createVirtualRegister(RC));\n      break;\n    }\n  }\n\n  LLT getLLTTy(const MachineRegisterInfo &MRI) const {\n    switch (Ty) {\n    case DstType::Ty_RC:\n      return LLT{};\n    case DstType::Ty_LLT:\n      return LLTTy;\n    case DstType::Ty_Reg:\n      return MRI.getType(Reg);\n    }\n    llvm_unreachable(\"Unrecognised DstOp::DstType enum\");\n  }\n\n  Register getReg() const {\n    assert(Ty == DstType::Ty_Reg && \"Not a register\");\n    return Reg;\n  }\n\n  const TargetRegisterClass *getRegClass() const {\n    switch (Ty) {\n    case DstType::Ty_RC:\n      return RC;\n    default:\n      llvm_unreachable(\"Not a RC Operand\");\n    }\n  }\n\n  DstType getDstOpKind() const { return Ty; }\n\nprivate:\n  DstType Ty;\n};\n\nclass SrcOp {\n  union {\n    MachineInstrBuilder SrcMIB;\n    Register Reg;\n    CmpInst::Predicate Pred;\n    int64_t Imm;\n  };\n\npublic:\n  enum class SrcType { Ty_Reg, Ty_MIB, Ty_Predicate, Ty_Imm };\n  SrcOp(Register R) : Reg(R), Ty(SrcType::Ty_Reg) {}\n  SrcOp(const MachineOperand &Op) : Reg(Op.getReg()), Ty(SrcType::Ty_Reg) {}\n  SrcOp(const MachineInstrBuilder &MIB) : SrcMIB(MIB), Ty(SrcType::Ty_MIB) {}\n  SrcOp(const CmpInst::Predicate P) : Pred(P), Ty(SrcType::Ty_Predicate) {}\n  /// Use of registers held in unsigned integer variables (or more rarely signed\n  /// integers) is no longer permitted to avoid ambiguity with upcoming support\n  /// for immediates.\n  SrcOp(unsigned) = delete;\n  SrcOp(int) = delete;\n  SrcOp(uint64_t V) : Imm(V), Ty(SrcType::Ty_Imm) {}\n  SrcOp(int64_t V) : Imm(V), Ty(SrcType::Ty_Imm) {}\n\n  void addSrcToMIB(MachineInstrBuilder &MIB) const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n      MIB.addPredicate(Pred);\n      break;\n    case SrcType::Ty_Reg:\n      MIB.addUse(Reg);\n      break;\n    case SrcType::Ty_MIB:\n      MIB.addUse(SrcMIB->getOperand(0).getReg());\n      break;\n    case SrcType::Ty_Imm:\n      MIB.addImm(Imm);\n      break;\n    }\n  }\n\n  LLT getLLTTy(const MachineRegisterInfo &MRI) const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n    case SrcType::Ty_Imm:\n      llvm_unreachable(\"Not a register operand\");\n    case SrcType::Ty_Reg:\n      return MRI.getType(Reg);\n    case SrcType::Ty_MIB:\n      return MRI.getType(SrcMIB->getOperand(0).getReg());\n    }\n    llvm_unreachable(\"Unrecognised SrcOp::SrcType enum\");\n  }\n\n  Register getReg() const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n    case SrcType::Ty_Imm:\n      llvm_unreachable(\"Not a register operand\");\n    case SrcType::Ty_Reg:\n      return Reg;\n    case SrcType::Ty_MIB:\n      return SrcMIB->getOperand(0).getReg();\n    }\n    llvm_unreachable(\"Unrecognised SrcOp::SrcType enum\");\n  }\n\n  CmpInst::Predicate getPredicate() const {\n    switch (Ty) {\n    case SrcType::Ty_Predicate:\n      return Pred;\n    default:\n      llvm_unreachable(\"Not a register operand\");\n    }\n  }\n\n  int64_t getImm() const {\n    switch (Ty) {\n    case SrcType::Ty_Imm:\n      return Imm;\n    default:\n      llvm_unreachable(\"Not an immediate\");\n    }\n  }\n\n  SrcType getSrcOpKind() const { return Ty; }\n\nprivate:\n  SrcType Ty;\n};\n\nclass FlagsOp {\n  Optional<unsigned> Flags;\n\npublic:\n  explicit FlagsOp(unsigned F) : Flags(F) {}\n  FlagsOp() : Flags(None) {}\n  Optional<unsigned> getFlags() const { return Flags; }\n};\n/// Helper class to build MachineInstr.\n/// It keeps internally the insertion point and debug location for all\n/// the new instructions we want to create.\n/// This information can be modify via the related setters.\nclass MachineIRBuilder {\n\n  MachineIRBuilderState State;\n\nprotected:\n  void validateTruncExt(const LLT Dst, const LLT Src, bool IsExtend);\n\n  void validateUnaryOp(const LLT Res, const LLT Op0);\n  void validateBinaryOp(const LLT Res, const LLT Op0, const LLT Op1);\n  void validateShiftOp(const LLT Res, const LLT Op0, const LLT Op1);\n\n  void validateSelectOp(const LLT ResTy, const LLT TstTy, const LLT Op0Ty,\n                        const LLT Op1Ty);\n\n  void recordInsertion(MachineInstr *InsertedInstr) const {\n    if (State.Observer)\n      State.Observer->createdInstr(*InsertedInstr);\n  }\n\npublic:\n  /// Some constructors for easy use.\n  MachineIRBuilder() = default;\n  MachineIRBuilder(MachineFunction &MF) { setMF(MF); }\n\n  MachineIRBuilder(MachineBasicBlock &MBB, MachineBasicBlock::iterator InsPt) {\n    setMF(*MBB.getParent());\n    setInsertPt(MBB, InsPt);\n  }\n\n  MachineIRBuilder(MachineInstr &MI) :\n    MachineIRBuilder(*MI.getParent(), MI.getIterator()) {\n    setInstr(MI);\n    setDebugLoc(MI.getDebugLoc());\n  }\n\n  MachineIRBuilder(MachineInstr &MI, GISelChangeObserver &Observer) :\n    MachineIRBuilder(MI) {\n    setChangeObserver(Observer);\n  }\n\n  virtual ~MachineIRBuilder() = default;\n\n  MachineIRBuilder(const MachineIRBuilderState &BState) : State(BState) {}\n\n  const TargetInstrInfo &getTII() {\n    assert(State.TII && \"TargetInstrInfo is not set\");\n    return *State.TII;\n  }\n\n  /// Getter for the function we currently build.\n  MachineFunction &getMF() {\n    assert(State.MF && \"MachineFunction is not set\");\n    return *State.MF;\n  }\n\n  const MachineFunction &getMF() const {\n    assert(State.MF && \"MachineFunction is not set\");\n    return *State.MF;\n  }\n\n  const DataLayout &getDataLayout() const {\n    return getMF().getFunction().getParent()->getDataLayout();\n  }\n\n  /// Getter for DebugLoc\n  const DebugLoc &getDL() { return State.DL; }\n\n  /// Getter for MRI\n  MachineRegisterInfo *getMRI() { return State.MRI; }\n  const MachineRegisterInfo *getMRI() const { return State.MRI; }\n\n  /// Getter for the State\n  MachineIRBuilderState &getState() { return State; }\n\n  /// Getter for the basic block we currently build.\n  const MachineBasicBlock &getMBB() const {\n    assert(State.MBB && \"MachineBasicBlock is not set\");\n    return *State.MBB;\n  }\n\n  MachineBasicBlock &getMBB() {\n    return const_cast<MachineBasicBlock &>(\n        const_cast<const MachineIRBuilder *>(this)->getMBB());\n  }\n\n  GISelCSEInfo *getCSEInfo() { return State.CSEInfo; }\n  const GISelCSEInfo *getCSEInfo() const { return State.CSEInfo; }\n\n  /// Current insertion point for new instructions.\n  MachineBasicBlock::iterator getInsertPt() { return State.II; }\n\n  /// Set the insertion point before the specified position.\n  /// \\pre MBB must be in getMF().\n  /// \\pre II must be a valid iterator in MBB.\n  void setInsertPt(MachineBasicBlock &MBB, MachineBasicBlock::iterator II) {\n    assert(MBB.getParent() == &getMF() &&\n           \"Basic block is in a different function\");\n    State.MBB = &MBB;\n    State.II = II;\n  }\n\n  /// @}\n\n  void setCSEInfo(GISelCSEInfo *Info) { State.CSEInfo = Info; }\n\n  /// \\name Setters for the insertion point.\n  /// @{\n  /// Set the MachineFunction where to build instructions.\n  void setMF(MachineFunction &MF);\n\n  /// Set the insertion point to the  end of \\p MBB.\n  /// \\pre \\p MBB must be contained by getMF().\n  void setMBB(MachineBasicBlock &MBB) {\n    State.MBB = &MBB;\n    State.II = MBB.end();\n    assert(&getMF() == MBB.getParent() &&\n           \"Basic block is in a different function\");\n  }\n\n  /// Set the insertion point to before MI.\n  /// \\pre MI must be in getMF().\n  void setInstr(MachineInstr &MI) {\n    assert(MI.getParent() && \"Instruction is not part of a basic block\");\n    setMBB(*MI.getParent());\n    State.II = MI.getIterator();\n  }\n  /// @}\n\n  /// Set the insertion point to before MI, and set the debug loc to MI's loc.\n  /// \\pre MI must be in getMF().\n  void setInstrAndDebugLoc(MachineInstr &MI) {\n    setInstr(MI);\n    setDebugLoc(MI.getDebugLoc());\n  }\n\n  void setChangeObserver(GISelChangeObserver &Observer) {\n    State.Observer = &Observer;\n  }\n\n  void stopObservingChanges() { State.Observer = nullptr; }\n  /// @}\n\n  /// Set the debug location to \\p DL for all the next build instructions.\n  void setDebugLoc(const DebugLoc &DL) { this->State.DL = DL; }\n\n  /// Get the current instruction's debug location.\n  DebugLoc getDebugLoc() { return State.DL; }\n\n  /// Build and insert <empty> = \\p Opcode <empty>.\n  /// The insertion point is the one set by the last call of either\n  /// setBasicBlock or setMI.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildInstr(unsigned Opcode) {\n    return insertInstr(buildInstrNoInsert(Opcode));\n  }\n\n  /// Build but don't insert <empty> = \\p Opcode <empty>.\n  ///\n  /// \\pre setMF, setBasicBlock or setMI  must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildInstrNoInsert(unsigned Opcode);\n\n  /// Insert an existing instruction at the insertion point.\n  MachineInstrBuilder insertInstr(MachineInstrBuilder MIB);\n\n  /// Build and insert a DBG_VALUE instruction expressing the fact that the\n  /// associated \\p Variable lives in \\p Reg (suitably modified by \\p Expr).\n  MachineInstrBuilder buildDirectDbgValue(Register Reg, const MDNode *Variable,\n                                          const MDNode *Expr);\n\n  /// Build and insert a DBG_VALUE instruction expressing the fact that the\n  /// associated \\p Variable lives in memory at \\p Reg (suitably modified by \\p\n  /// Expr).\n  MachineInstrBuilder buildIndirectDbgValue(Register Reg,\n                                            const MDNode *Variable,\n                                            const MDNode *Expr);\n\n  /// Build and insert a DBG_VALUE instruction expressing the fact that the\n  /// associated \\p Variable lives in the stack slot specified by \\p FI\n  /// (suitably modified by \\p Expr).\n  MachineInstrBuilder buildFIDbgValue(int FI, const MDNode *Variable,\n                                      const MDNode *Expr);\n\n  /// Build and insert a DBG_VALUE instructions specifying that \\p Variable is\n  /// given by \\p C (suitably modified by \\p Expr).\n  MachineInstrBuilder buildConstDbgValue(const Constant &C,\n                                         const MDNode *Variable,\n                                         const MDNode *Expr);\n\n  /// Build and insert a DBG_LABEL instructions specifying that \\p Label is\n  /// given. Convert \"llvm.dbg.label Label\" to \"DBG_LABEL Label\".\n  MachineInstrBuilder buildDbgLabel(const MDNode *Label);\n\n  /// Build and insert \\p Res = G_DYN_STACKALLOC \\p Size, \\p Align\n  ///\n  /// G_DYN_STACKALLOC does a dynamic stack allocation and writes the address of\n  /// the allocated memory into \\p Res.\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildDynStackAlloc(const DstOp &Res, const SrcOp &Size,\n                                         Align Alignment);\n\n  /// Build and insert \\p Res = G_FRAME_INDEX \\p Idx\n  ///\n  /// G_FRAME_INDEX materializes the address of an alloca value or other\n  /// stack-based object.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildFrameIndex(const DstOp &Res, int Idx);\n\n  /// Build and insert \\p Res = G_GLOBAL_VALUE \\p GV\n  ///\n  /// G_GLOBAL_VALUE materializes the address of the specified global\n  /// into \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with pointer type\n  ///      in the same address space as \\p GV.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildGlobalValue(const DstOp &Res, const GlobalValue *GV);\n\n  /// Build and insert \\p Res = G_PTR_ADD \\p Op0, \\p Op1\n  ///\n  /// G_PTR_ADD adds \\p Op1 addressible units to the pointer specified by \\p Op0,\n  /// storing the resulting pointer in \\p Res. Addressible units are typically\n  /// bytes but this can vary between targets.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Op0 must be generic virtual registers with pointer\n  ///      type.\n  /// \\pre \\p Op1 must be a generic virtual register with scalar type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildPtrAdd(const DstOp &Res, const SrcOp &Op0,\n                                  const SrcOp &Op1);\n\n  /// Materialize and insert \\p Res = G_PTR_ADD \\p Op0, (G_CONSTANT \\p Value)\n  ///\n  /// G_PTR_ADD adds \\p Value bytes to the pointer specified by \\p Op0,\n  /// storing the resulting pointer in \\p Res. If \\p Value is zero then no\n  /// G_PTR_ADD or G_CONSTANT will be created and \\pre Op0 will be assigned to\n  /// \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Op0 must be a generic virtual register with pointer type.\n  /// \\pre \\p ValueTy must be a scalar type.\n  /// \\pre \\p Res must be 0. This is to detect confusion between\n  ///      materializePtrAdd() and buildPtrAdd().\n  /// \\post \\p Res will either be a new generic virtual register of the same\n  ///       type as \\p Op0 or \\p Op0 itself.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  Optional<MachineInstrBuilder> materializePtrAdd(Register &Res, Register Op0,\n                                                  const LLT ValueTy,\n                                                  uint64_t Value);\n\n  /// Build and insert \\p Res = G_PTRMASK \\p Op0, \\p Op1\n  MachineInstrBuilder buildPtrMask(const DstOp &Res, const SrcOp &Op0,\n                                   const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_PTRMASK, {Res}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res = G_PTRMASK \\p Op0, \\p G_CONSTANT (1 << NumBits) - 1\n  ///\n  /// This clears the low bits of a pointer operand without destroying its\n  /// pointer properties. This has the effect of rounding the address *down* to\n  /// a specified alignment in bits.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Op0 must be generic virtual registers with pointer\n  ///      type.\n  /// \\pre \\p NumBits must be an integer representing the number of low bits to\n  ///      be cleared in \\p Op0.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildMaskLowPtrBits(const DstOp &Res, const SrcOp &Op0,\n                                          uint32_t NumBits);\n\n  /// Build and insert \\p Res, \\p CarryOut = G_UADDO \\p Op0, \\p Op1\n  ///\n  /// G_UADDO sets \\p Res to \\p Op0 + \\p Op1 (truncated to the bit width) and\n  /// sets \\p CarryOut to 1 if the result overflowed in unsigned arithmetic.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers with the\n  /// same scalar type.\n  ////\\pre \\p CarryOut must be generic virtual register with scalar type\n  ///(typically s1)\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildUAddo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_UADDO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_USUBO \\p Op0, \\p Op1\n  MachineInstrBuilder buildUSubo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_USUBO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SADDO \\p Op0, \\p Op1\n  MachineInstrBuilder buildSAddo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_SADDO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SUBO \\p Op0, \\p Op1\n  MachineInstrBuilder buildSSubo(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1) {\n    return buildInstr(TargetOpcode::G_SSUBO, {Res, CarryOut}, {Op0, Op1});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_UADDE \\p Op0,\n  /// \\p Op1, \\p CarryIn\n  ///\n  /// G_UADDE sets \\p Res to \\p Op0 + \\p Op1 + \\p CarryIn (truncated to the bit\n  /// width) and sets \\p CarryOut to 1 if the result overflowed in unsigned\n  /// arithmetic.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same scalar type.\n  /// \\pre \\p CarryOut and \\p CarryIn must be generic virtual\n  ///      registers with the same scalar type (typically s1)\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildUAdde(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_UADDE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_USUBE \\p Op0, \\p Op1, \\p CarryInp\n  MachineInstrBuilder buildUSube(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_USUBE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SADDE \\p Op0, \\p Op1, \\p CarryInp\n  MachineInstrBuilder buildSAdde(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_SADDE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res, \\p CarryOut = G_SSUBE \\p Op0, \\p Op1, \\p CarryInp\n  MachineInstrBuilder buildSSube(const DstOp &Res, const DstOp &CarryOut,\n                                 const SrcOp &Op0, const SrcOp &Op1,\n                                 const SrcOp &CarryIn) {\n    return buildInstr(TargetOpcode::G_SSUBE, {Res, CarryOut},\n                                             {Op0, Op1, CarryIn});\n  }\n\n  /// Build and insert \\p Res = G_ANYEXT \\p Op0\n  ///\n  /// G_ANYEXT produces a register of the specified width, with bits 0 to\n  /// sizeof(\\p Ty) * 8 set to \\p Op. The remaining bits are unspecified\n  /// (i.e. this is neither zero nor sign-extension). For a vector register,\n  /// each element is extended individually.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be smaller than \\p Res\n  ///\n  /// \\return The newly created instruction.\n\n  MachineInstrBuilder buildAnyExt(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_SEXT \\p Op\n  ///\n  /// G_SEXT produces a register of the specified width, with bits 0 to\n  /// sizeof(\\p Ty) * 8 set to \\p Op. The remaining bits are duplicated from the\n  /// high bit of \\p Op (i.e. 2s-complement sign extended).\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be smaller than \\p Res\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildSExt(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_SEXT_INREG \\p Op, ImmOp\n  MachineInstrBuilder buildSExtInReg(const DstOp &Res, const SrcOp &Op, int64_t ImmOp) {\n    return buildInstr(TargetOpcode::G_SEXT_INREG, {Res}, {Op, SrcOp(ImmOp)});\n  }\n\n  /// Build and insert \\p Res = G_FPEXT \\p Op\n  MachineInstrBuilder buildFPExt(const DstOp &Res, const SrcOp &Op,\n                                 Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FPEXT, {Res}, {Op}, Flags);\n  }\n\n\n  /// Build and insert a G_PTRTOINT instruction.\n  MachineInstrBuilder buildPtrToInt(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_PTRTOINT, {Dst}, {Src});\n  }\n\n  /// Build and insert a G_INTTOPTR instruction.\n  MachineInstrBuilder buildIntToPtr(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_INTTOPTR, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Dst = G_BITCAST \\p Src\n  MachineInstrBuilder buildBitcast(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_BITCAST, {Dst}, {Src});\n  }\n\n    /// Build and insert \\p Dst = G_ADDRSPACE_CAST \\p Src\n  MachineInstrBuilder buildAddrSpaceCast(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_ADDRSPACE_CAST, {Dst}, {Src});\n  }\n\n  /// \\return The opcode of the extension the target wants to use for boolean\n  /// values.\n  unsigned getBoolExtOp(bool IsVec, bool IsFP) const;\n\n  // Build and insert \\p Res = G_ANYEXT \\p Op, \\p Res = G_SEXT \\p Op, or \\p Res\n  // = G_ZEXT \\p Op depending on how the target wants to extend boolean values.\n  MachineInstrBuilder buildBoolExt(const DstOp &Res, const SrcOp &Op,\n                                   bool IsFP);\n\n  /// Build and insert \\p Res = G_ZEXT \\p Op\n  ///\n  /// G_ZEXT produces a register of the specified width, with bits 0 to\n  /// sizeof(\\p Ty) * 8 set to \\p Op. The remaining bits are 0. For a vector\n  /// register, each element is extended individually.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be smaller than \\p Res\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildZExt(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_SEXT \\p Op, \\p Res = G_TRUNC \\p Op, or\n  /// \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildSExtOrTrunc(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_ZEXT \\p Op, \\p Res = G_TRUNC \\p Op, or\n  /// \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildZExtOrTrunc(const DstOp &Res, const SrcOp &Op);\n\n  // Build and insert \\p Res = G_ANYEXT \\p Op, \\p Res = G_TRUNC \\p Op, or\n  /// \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildAnyExtOrTrunc(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = \\p ExtOpc, \\p Res = G_TRUNC \\p\n  /// Op, or \\p Res = COPY \\p Op depending on the differing sizes of \\p Res and\n  /// \\p Op.\n  ///  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildExtOrTrunc(unsigned ExtOpc, const DstOp &Res,\n                                      const SrcOp &Op);\n\n  /// Build and insert an appropriate cast between two registers of equal size.\n  MachineInstrBuilder buildCast(const DstOp &Dst, const SrcOp &Src);\n\n  /// Build and insert G_BR \\p Dest\n  ///\n  /// G_BR is an unconditional branch to \\p Dest.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBr(MachineBasicBlock &Dest);\n\n  /// Build and insert G_BRCOND \\p Tst, \\p Dest\n  ///\n  /// G_BRCOND is a conditional branch to \\p Dest.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Tst must be a generic virtual register with scalar\n  ///      type. At the beginning of legalization, this will be a single\n  ///      bit (s1). Targets with interesting flags registers may change\n  ///      this. For a wider type, whether the branch is taken must only\n  ///      depend on bit 0 (for now).\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildBrCond(const SrcOp &Tst, MachineBasicBlock &Dest);\n\n  /// Build and insert G_BRINDIRECT \\p Tgt\n  ///\n  /// G_BRINDIRECT is an indirect branch to \\p Tgt.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Tgt must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBrIndirect(Register Tgt);\n\n  /// Build and insert G_BRJT \\p TablePtr, \\p JTI, \\p IndexReg\n  ///\n  /// G_BRJT is a jump table branch using a table base pointer \\p TablePtr,\n  /// jump table index \\p JTI and index \\p IndexReg\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p TablePtr must be a generic virtual register with pointer type.\n  /// \\pre \\p JTI must be be a jump table index.\n  /// \\pre \\p IndexReg must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBrJT(Register TablePtr, unsigned JTI,\n                                Register IndexReg);\n\n  /// Build and insert \\p Res = G_CONSTANT \\p Val\n  ///\n  /// G_CONSTANT is an integer constant with the specified size and value. \\p\n  /// Val will be extended or truncated to the size of \\p Reg.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or pointer\n  ///      type.\n  ///\n  /// \\return The newly created instruction.\n  virtual MachineInstrBuilder buildConstant(const DstOp &Res,\n                                            const ConstantInt &Val);\n\n  /// Build and insert \\p Res = G_CONSTANT \\p Val\n  ///\n  /// G_CONSTANT is an integer constant with the specified size and value.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildConstant(const DstOp &Res, int64_t Val);\n  MachineInstrBuilder buildConstant(const DstOp &Res, const APInt &Val);\n\n  /// Build and insert \\p Res = G_FCONSTANT \\p Val\n  ///\n  /// G_FCONSTANT is a floating-point constant with the specified size and\n  /// value.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  virtual MachineInstrBuilder buildFConstant(const DstOp &Res,\n                                             const ConstantFP &Val);\n\n  MachineInstrBuilder buildFConstant(const DstOp &Res, double Val);\n  MachineInstrBuilder buildFConstant(const DstOp &Res, const APFloat &Val);\n\n  /// Build and insert \\p Res = COPY Op\n  ///\n  /// Register-to-register COPY sets \\p Res to \\p Op.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildCopy(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_ASSERT_ZEXT Op, Size\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAssertZExt(const DstOp &Res, const SrcOp &Op,\n                                      unsigned Size);\n\n  /// Build and insert \\p Res = G_ASSERT_SEXT Op, Size\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAssertSExt(const DstOp &Res, const SrcOp &Op,\n                                      unsigned Size);\n\n  /// Build and insert `Res = G_LOAD Addr, MMO`.\n  ///\n  /// Loads the value stored at \\p Addr. Puts the result in \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildLoad(const DstOp &Res, const SrcOp &Addr,\n                                MachineMemOperand &MMO) {\n    return buildLoadInstr(TargetOpcode::G_LOAD, Res, Addr, MMO);\n  }\n\n  /// Build and insert a G_LOAD instruction, while constructing the\n  /// MachineMemOperand.\n  MachineInstrBuilder\n  buildLoad(const DstOp &Res, const SrcOp &Addr, MachinePointerInfo PtrInfo,\n            Align Alignment,\n            MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n            const AAMDNodes &AAInfo = AAMDNodes());\n\n  /// Build and insert `Res = <opcode> Addr, MMO`.\n  ///\n  /// Loads the value stored at \\p Addr. Puts the result in \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildLoadInstr(unsigned Opcode, const DstOp &Res,\n                                     const SrcOp &Addr, MachineMemOperand &MMO);\n\n  /// Helper to create a load from a constant offset given a base address. Load\n  /// the type of \\p Dst from \\p Offset from the given base address and memory\n  /// operand.\n  MachineInstrBuilder buildLoadFromOffset(const DstOp &Dst,\n                                          const SrcOp &BasePtr,\n                                          MachineMemOperand &BaseMMO,\n                                          int64_t Offset);\n\n  /// Build and insert `G_STORE Val, Addr, MMO`.\n  ///\n  /// Stores the value \\p Val to \\p Addr.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Val must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildStore(const SrcOp &Val, const SrcOp &Addr,\n                                 MachineMemOperand &MMO);\n\n  /// Build and insert a G_STORE instruction, while constructing the\n  /// MachineMemOperand.\n  MachineInstrBuilder\n  buildStore(const SrcOp &Val, const SrcOp &Addr, MachinePointerInfo PtrInfo,\n             Align Alignment,\n             MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n             const AAMDNodes &AAInfo = AAMDNodes());\n\n  /// Build and insert `Res0, ... = G_EXTRACT Src, Idx0`.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Src must be generic virtual registers.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildExtract(const DstOp &Res, const SrcOp &Src, uint64_t Index);\n\n  /// Build and insert \\p Res = IMPLICIT_DEF.\n  MachineInstrBuilder buildUndef(const DstOp &Res);\n\n  /// Build and insert instructions to put \\p Ops together at the specified p\n  /// Indices to form a larger register.\n  ///\n  /// If the types of the input registers are uniform and cover the entirity of\n  /// \\p Res then a G_MERGE_VALUES will be produced. Otherwise an IMPLICIT_DEF\n  /// followed by a sequence of G_INSERT instructions.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The final element of the sequence must not extend past the end of the\n  ///      destination register.\n  /// \\pre The bits defined by each Op (derived from index and scalar size) must\n  ///      not overlap.\n  /// \\pre \\p Indices must be in ascending order of bit position.\n  void buildSequence(Register Res, ArrayRef<Register> Ops,\n                     ArrayRef<uint64_t> Indices);\n\n  /// Build and insert \\p Res = G_MERGE_VALUES \\p Op0, ...\n  ///\n  /// G_MERGE_VALUES combines the input elements contiguously into a larger\n  /// register.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the input\n  ///      registers.\n  /// \\pre The type of all \\p Ops registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildMerge(const DstOp &Res, ArrayRef<Register> Ops);\n  MachineInstrBuilder buildMerge(const DstOp &Res,\n                                 std::initializer_list<SrcOp> Ops);\n\n  /// Build and insert \\p Res0, ... = G_UNMERGE_VALUES \\p Op\n  ///\n  /// G_UNMERGE_VALUES splits contiguous bits of the input into multiple\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the input\n  ///      registers.\n  /// \\pre The type of all \\p Res registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildUnmerge(ArrayRef<LLT> Res, const SrcOp &Op);\n  MachineInstrBuilder buildUnmerge(ArrayRef<Register> Res, const SrcOp &Op);\n\n  /// Build and insert an unmerge of \\p Res sized pieces to cover \\p Op\n  MachineInstrBuilder buildUnmerge(LLT Res, const SrcOp &Op);\n\n  /// Build and insert \\p Res = G_BUILD_VECTOR \\p Op0, ...\n  ///\n  /// G_BUILD_VECTOR creates a vector value from multiple scalar registers.\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the\n  ///      input scalar registers.\n  /// \\pre The type of all \\p Ops registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBuildVector(const DstOp &Res,\n                                       ArrayRef<Register> Ops);\n\n  /// Build and insert \\p Res = G_BUILD_VECTOR with \\p Src replicated to fill\n  /// the number of elements\n  MachineInstrBuilder buildSplatVector(const DstOp &Res,\n                                       const SrcOp &Src);\n\n  /// Build and insert \\p Res = G_BUILD_VECTOR_TRUNC \\p Op0, ...\n  ///\n  /// G_BUILD_VECTOR_TRUNC creates a vector value from multiple scalar registers\n  /// which have types larger than the destination vector element type, and\n  /// truncates the values to fit.\n  ///\n  /// If the operands given are already the same size as the vector elt type,\n  /// then this method will instead create a G_BUILD_VECTOR instruction.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The type of all \\p Ops registers must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildBuildVectorTrunc(const DstOp &Res,\n                                            ArrayRef<Register> Ops);\n\n  /// Build and insert a vector splat of a scalar \\p Src using a\n  /// G_INSERT_VECTOR_ELT and G_SHUFFLE_VECTOR idiom.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Src must have the same type as the element type of \\p Dst\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildShuffleSplat(const DstOp &Res, const SrcOp &Src);\n\n  /// Build and insert \\p Res = G_SHUFFLE_VECTOR \\p Src1, \\p Src2, \\p Mask\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildShuffleVector(const DstOp &Res, const SrcOp &Src1,\n                                         const SrcOp &Src2, ArrayRef<int> Mask);\n\n  /// Build and insert \\p Res = G_CONCAT_VECTORS \\p Op0, ...\n  ///\n  /// G_CONCAT_VECTORS creates a vector from the concatenation of 2 or more\n  /// vectors.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre The entire register \\p Res (and no more) must be covered by the input\n  ///      registers.\n  /// \\pre The type of all source operands must be identical.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildConcatVectors(const DstOp &Res,\n                                         ArrayRef<Register> Ops);\n\n  MachineInstrBuilder buildInsert(const DstOp &Res, const SrcOp &Src,\n                                  const SrcOp &Op, unsigned Index);\n\n  /// Build and insert either a G_INTRINSIC (if \\p HasSideEffects is false) or\n  /// G_INTRINSIC_W_SIDE_EFFECTS instruction. Its first operand will be the\n  /// result register definition unless \\p Reg is NoReg (== 0). The second\n  /// operand will be the intrinsic's ID.\n  ///\n  /// Callers are expected to add the required definitions and uses afterwards.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildIntrinsic(Intrinsic::ID ID, ArrayRef<Register> Res,\n                                     bool HasSideEffects);\n  MachineInstrBuilder buildIntrinsic(Intrinsic::ID ID, ArrayRef<DstOp> Res,\n                                     bool HasSideEffects);\n\n  /// Build and insert \\p Res = G_FPTRUNC \\p Op\n  ///\n  /// G_FPTRUNC converts a floating-point value into one with a smaller type.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Res must be smaller than \\p Op\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildFPTrunc(const DstOp &Res, const SrcOp &Op,\n                                   Optional<unsigned> Flags = None);\n\n  /// Build and insert \\p Res = G_TRUNC \\p Op\n  ///\n  /// G_TRUNC extracts the low bits of a type. For a vector type each element is\n  /// truncated independently before being packed into the destination.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Op must be a generic virtual register with scalar or vector type.\n  /// \\pre \\p Res must be smaller than \\p Op\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildTrunc(const DstOp &Res, const SrcOp &Op);\n\n  /// Build and insert a \\p Res = G_ICMP \\p Pred, \\p Op0, \\p Op1\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n\n  /// \\pre \\p Res must be a generic virtual register with scalar or\n  ///      vector type. Typically this starts as s1 or <N x s1>.\n  /// \\pre \\p Op0 and Op1 must be generic virtual registers with the\n  ///      same number of elements as \\p Res. If \\p Res is a scalar,\n  ///      \\p Op0 must be either a scalar or pointer.\n  /// \\pre \\p Pred must be an integer predicate.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildICmp(CmpInst::Predicate Pred, const DstOp &Res,\n                                const SrcOp &Op0, const SrcOp &Op1);\n\n  /// Build and insert a \\p Res = G_FCMP \\p Pred\\p Op0, \\p Op1\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n\n  /// \\pre \\p Res must be a generic virtual register with scalar or\n  ///      vector type. Typically this starts as s1 or <N x s1>.\n  /// \\pre \\p Op0 and Op1 must be generic virtual registers with the\n  ///      same number of elements as \\p Res (or scalar, if \\p Res is\n  ///      scalar).\n  /// \\pre \\p Pred must be a floating-point predicate.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildFCmp(CmpInst::Predicate Pred, const DstOp &Res,\n                                const SrcOp &Op0, const SrcOp &Op1,\n                                Optional<unsigned> Flags = None);\n\n  /// Build and insert a \\p Res = G_SELECT \\p Tst, \\p Op0, \\p Op1\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same type.\n  /// \\pre \\p Tst must be a generic virtual register with scalar, pointer or\n  ///      vector type. If vector then it must have the same number of\n  ///      elements as the other parameters.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildSelect(const DstOp &Res, const SrcOp &Tst,\n                                  const SrcOp &Op0, const SrcOp &Op1,\n                                  Optional<unsigned> Flags = None);\n\n  /// Build and insert \\p Res = G_INSERT_VECTOR_ELT \\p Val,\n  /// \\p Elt, \\p Idx\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res and \\p Val must be a generic virtual register\n  //       with the same vector type.\n  /// \\pre \\p Elt and \\p Idx must be a generic virtual register\n  ///      with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildInsertVectorElement(const DstOp &Res,\n                                               const SrcOp &Val,\n                                               const SrcOp &Elt,\n                                               const SrcOp &Idx);\n\n  /// Build and insert \\p Res = G_EXTRACT_VECTOR_ELT \\p Val, \\p Idx\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register with scalar type.\n  /// \\pre \\p Val must be a generic virtual register with vector type.\n  /// \\pre \\p Idx must be a generic virtual register with scalar type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildExtractVectorElement(const DstOp &Res,\n                                                const SrcOp &Val,\n                                                const SrcOp &Idx);\n\n  /// Build and insert `OldValRes<def>, SuccessRes<def> =\n  /// G_ATOMIC_CMPXCHG_WITH_SUCCESS Addr, CmpVal, NewVal, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with \\p NewVal if it is currently\n  /// \\p CmpVal otherwise leaves it unchanged. Puts the original value from \\p\n  /// Addr in \\p Res, along with an s1 indicating whether it was replaced.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register of scalar type.\n  /// \\pre \\p SuccessRes must be a generic virtual register of scalar type. It\n  ///      will be assigned 0 on failure and 1 on success.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, \\p CmpVal, and \\p NewVal must be generic virtual\n  ///      registers of the same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder\n  buildAtomicCmpXchgWithSuccess(Register OldValRes, Register SuccessRes,\n                                Register Addr, Register CmpVal, Register NewVal,\n                                MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMIC_CMPXCHG Addr, CmpVal, NewVal,\n  /// MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with \\p NewVal if it is currently\n  /// \\p CmpVal otherwise leaves it unchanged. Puts the original value from \\p\n  /// Addr in \\p Res.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register of scalar type.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, \\p CmpVal, and \\p NewVal must be generic virtual\n  ///      registers of the same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicCmpXchg(Register OldValRes, Register Addr,\n                                         Register CmpVal, Register NewVal,\n                                         MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_<Opcode> Addr, Val, MMO`.\n  ///\n  /// Atomically read-modify-update the value at \\p Addr with \\p Val. Puts the\n  /// original value from \\p Addr in \\p OldValRes. The modification is\n  /// determined by the opcode.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMW(unsigned Opcode, const DstOp &OldValRes,\n                                     const SrcOp &Addr, const SrcOp &Val,\n                                     MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_XCHG Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with \\p Val. Puts the original\n  /// value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWXchg(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_ADD Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the addition of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWAdd(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_SUB Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the subtraction of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWSub(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_AND Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise and of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWAnd(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_NAND Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise nand of \\p Val\n  /// and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWNand(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_OR Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise or of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWOr(Register OldValRes, Register Addr,\n                                       Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_XOR Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the bitwise xor of \\p Val and\n  /// the original value. Puts the original value from \\p Addr in \\p OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWXor(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_MAX Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the signed maximum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWMax(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_MIN Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the signed minimum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWMin(Register OldValRes, Register Addr,\n                                        Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_UMAX Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the unsigned maximum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWUmax(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_UMIN Addr, Val, MMO`.\n  ///\n  /// Atomically replace the value at \\p Addr with the unsigned minimum of \\p\n  /// Val and the original value. Puts the original value from \\p Addr in \\p\n  /// OldValRes.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p OldValRes must be a generic virtual register.\n  /// \\pre \\p Addr must be a generic virtual register with pointer type.\n  /// \\pre \\p OldValRes, and \\p Val must be generic virtual registers of the\n  ///      same type.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildAtomicRMWUmin(Register OldValRes, Register Addr,\n                                         Register Val, MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_FADD Addr, Val, MMO`.\n  MachineInstrBuilder buildAtomicRMWFAdd(\n    const DstOp &OldValRes, const SrcOp &Addr, const SrcOp &Val,\n    MachineMemOperand &MMO);\n\n  /// Build and insert `OldValRes<def> = G_ATOMICRMW_FSUB Addr, Val, MMO`.\n  MachineInstrBuilder buildAtomicRMWFSub(\n        const DstOp &OldValRes, const SrcOp &Addr, const SrcOp &Val,\n        MachineMemOperand &MMO);\n\n  /// Build and insert `G_FENCE Ordering, Scope`.\n  MachineInstrBuilder buildFence(unsigned Ordering, unsigned Scope);\n\n  /// Build and insert \\p Dst = G_FREEZE \\p Src\n  MachineInstrBuilder buildFreeze(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_FREEZE, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_BLOCK_ADDR \\p BA\n  ///\n  /// G_BLOCK_ADDR computes the address of a basic block.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res must be a generic virtual register of a pointer type.\n  ///\n  /// \\return The newly created instruction.\n  MachineInstrBuilder buildBlockAddress(Register Res, const BlockAddress *BA);\n\n  /// Build and insert \\p Res = G_ADD \\p Op0, \\p Op1\n  ///\n  /// G_ADD sets \\p Res to the sum of integer parameters \\p Op0 and \\p Op1,\n  /// truncated to their width.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n\n  MachineInstrBuilder buildAdd(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_ADD, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_SUB \\p Op0, \\p Op1\n  ///\n  /// G_SUB sets \\p Res to the sum of integer parameters \\p Op0 and \\p Op1,\n  /// truncated to their width.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n\n  MachineInstrBuilder buildSub(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_SUB, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_MUL \\p Op0, \\p Op1\n  ///\n  /// G_MUL sets \\p Res to the sum of integer parameters \\p Op0 and \\p Op1,\n  /// truncated to their width.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildMul(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_MUL, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildUMulH(const DstOp &Dst, const SrcOp &Src0,\n                                 const SrcOp &Src1,\n                                 Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_UMULH, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildSMulH(const DstOp &Dst, const SrcOp &Src0,\n                                 const SrcOp &Src1,\n                                 Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_SMULH, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMul(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMUL, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMinNum(const DstOp &Dst, const SrcOp &Src0,\n                                   const SrcOp &Src1,\n                                   Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMINNUM, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMaxNum(const DstOp &Dst, const SrcOp &Src0,\n                                   const SrcOp &Src1,\n                                   Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMAXNUM, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMinNumIEEE(const DstOp &Dst, const SrcOp &Src0,\n                                       const SrcOp &Src1,\n                                       Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMINNUM_IEEE, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildFMaxNumIEEE(const DstOp &Dst, const SrcOp &Src0,\n                                       const SrcOp &Src1,\n                                       Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMAXNUM_IEEE, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildShl(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_SHL, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildLShr(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_LSHR, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  MachineInstrBuilder buildAShr(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_ASHR, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_AND \\p Op0, \\p Op1\n  ///\n  /// G_AND sets \\p Res to the bitwise and of integer parameters \\p Op0 and \\p\n  /// Op1.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n\n  MachineInstrBuilder buildAnd(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_AND, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_OR \\p Op0, \\p Op1\n  ///\n  /// G_OR sets \\p Res to the bitwise or of integer parameters \\p Op0 and \\p\n  /// Op1.\n  ///\n  /// \\pre setBasicBlock or setMI must have been called.\n  /// \\pre \\p Res, \\p Op0 and \\p Op1 must be generic virtual registers\n  ///      with the same (scalar or vector) type).\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildOr(const DstOp &Dst, const SrcOp &Src0,\n                              const SrcOp &Src1,\n                              Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_OR, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_XOR \\p Op0, \\p Op1\n  MachineInstrBuilder buildXor(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_XOR, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert a bitwise not,\n  /// \\p NegOne = G_CONSTANT -1\n  /// \\p Res = G_OR \\p Op0, NegOne\n  MachineInstrBuilder buildNot(const DstOp &Dst, const SrcOp &Src0) {\n    auto NegOne = buildConstant(Dst.getLLTTy(*getMRI()), -1);\n    return buildInstr(TargetOpcode::G_XOR, {Dst}, {Src0, NegOne});\n  }\n\n  /// Build and insert \\p Res = G_CTPOP \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTPOP(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTPOP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTLZ \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTLZ(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTLZ, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTLZ_ZERO_UNDEF \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTLZ_ZERO_UNDEF(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTLZ_ZERO_UNDEF, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTTZ \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTTZ(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTTZ, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_CTTZ_ZERO_UNDEF \\p Op0, \\p Src0\n  MachineInstrBuilder buildCTTZ_ZERO_UNDEF(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_CTTZ_ZERO_UNDEF, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Dst = G_BSWAP \\p Src0\n  MachineInstrBuilder buildBSwap(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_BSWAP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_FADD \\p Op0, \\p Op1\n  MachineInstrBuilder buildFAdd(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FADD, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FSUB \\p Op0, \\p Op1\n  MachineInstrBuilder buildFSub(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FSUB, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FDIV \\p Op0, \\p Op1\n  MachineInstrBuilder buildFDiv(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FDIV, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FMA \\p Op0, \\p Op1, \\p Op2\n  MachineInstrBuilder buildFMA(const DstOp &Dst, const SrcOp &Src0,\n                               const SrcOp &Src1, const SrcOp &Src2,\n                               Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMA, {Dst}, {Src0, Src1, Src2}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FMAD \\p Op0, \\p Op1, \\p Op2\n  MachineInstrBuilder buildFMAD(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1, const SrcOp &Src2,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FMAD, {Dst}, {Src0, Src1, Src2}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FNEG \\p Op0\n  MachineInstrBuilder buildFNeg(const DstOp &Dst, const SrcOp &Src0,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FNEG, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FABS \\p Op0\n  MachineInstrBuilder buildFAbs(const DstOp &Dst, const SrcOp &Src0,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FABS, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FCANONICALIZE \\p Src0\n  MachineInstrBuilder buildFCanonicalize(const DstOp &Dst, const SrcOp &Src0,\n                                         Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FCANONICALIZE, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_INTRINSIC_TRUNC \\p Src0\n  MachineInstrBuilder buildIntrinsicTrunc(const DstOp &Dst, const SrcOp &Src0,\n                                         Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_INTRINSIC_TRUNC, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Res = GFFLOOR \\p Op0, \\p Op1\n  MachineInstrBuilder buildFFloor(const DstOp &Dst, const SrcOp &Src0,\n                                          Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FFLOOR, {Dst}, {Src0}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FLOG \\p Src\n  MachineInstrBuilder buildFLog(const DstOp &Dst, const SrcOp &Src,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FLOG, {Dst}, {Src}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FLOG2 \\p Src\n  MachineInstrBuilder buildFLog2(const DstOp &Dst, const SrcOp &Src,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FLOG2, {Dst}, {Src}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FEXP2 \\p Src\n  MachineInstrBuilder buildFExp2(const DstOp &Dst, const SrcOp &Src,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FEXP2, {Dst}, {Src}, Flags);\n  }\n\n  /// Build and insert \\p Dst = G_FPOW \\p Src0, \\p Src1\n  MachineInstrBuilder buildFPow(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1,\n                                Optional<unsigned> Flags = None) {\n    return buildInstr(TargetOpcode::G_FPOW, {Dst}, {Src0, Src1}, Flags);\n  }\n\n  /// Build and insert \\p Res = G_FCOPYSIGN \\p Op0, \\p Op1\n  MachineInstrBuilder buildFCopysign(const DstOp &Dst, const SrcOp &Src0,\n                                     const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_FCOPYSIGN, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_UITOFP \\p Src0\n  MachineInstrBuilder buildUITOFP(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_UITOFP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_SITOFP \\p Src0\n  MachineInstrBuilder buildSITOFP(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_SITOFP, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_FPTOUI \\p Src0\n  MachineInstrBuilder buildFPTOUI(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_FPTOUI, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_FPTOSI \\p Src0\n  MachineInstrBuilder buildFPTOSI(const DstOp &Dst, const SrcOp &Src0) {\n    return buildInstr(TargetOpcode::G_FPTOSI, {Dst}, {Src0});\n  }\n\n  /// Build and insert \\p Res = G_SMIN \\p Op0, \\p Op1\n  MachineInstrBuilder buildSMin(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_SMIN, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_SMAX \\p Op0, \\p Op1\n  MachineInstrBuilder buildSMax(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_SMAX, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_UMIN \\p Op0, \\p Op1\n  MachineInstrBuilder buildUMin(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_UMIN, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Res = G_UMAX \\p Op0, \\p Op1\n  MachineInstrBuilder buildUMax(const DstOp &Dst, const SrcOp &Src0,\n                                const SrcOp &Src1) {\n    return buildInstr(TargetOpcode::G_UMAX, {Dst}, {Src0, Src1});\n  }\n\n  /// Build and insert \\p Dst = G_ABS \\p Src\n  MachineInstrBuilder buildAbs(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_ABS, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_JUMP_TABLE \\p JTI\n  ///\n  /// G_JUMP_TABLE sets \\p Res to the address of the jump table specified by\n  /// the jump table index \\p JTI.\n  ///\n  /// \\return a MachineInstrBuilder for the newly created instruction.\n  MachineInstrBuilder buildJumpTable(const LLT PtrTy, unsigned JTI);\n\n  /// Build and insert \\p Res = G_VECREDUCE_SEQ_FADD \\p ScalarIn, \\p VecIn\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to start the sequential\n  /// reduction operation of \\p VecIn.\n  MachineInstrBuilder buildVecReduceSeqFAdd(const DstOp &Dst,\n                                            const SrcOp &ScalarIn,\n                                            const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SEQ_FADD, {Dst},\n                      {ScalarIn, {VecIn}});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_SEQ_FMUL \\p ScalarIn, \\p VecIn\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to start the sequential\n  /// reduction operation of \\p VecIn.\n  MachineInstrBuilder buildVecReduceSeqFMul(const DstOp &Dst,\n                                            const SrcOp &ScalarIn,\n                                            const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SEQ_FMUL, {Dst},\n                      {ScalarIn, {VecIn}});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FADD \\p Src\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to the reduction operation of\n  /// \\p VecIn.\n  MachineInstrBuilder buildVecReduceFAdd(const DstOp &Dst,\n                                         const SrcOp &ScalarIn,\n                                         const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FADD, {Dst}, {ScalarIn, VecIn});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FMUL \\p Src\n  ///\n  /// \\p ScalarIn is the scalar accumulator input to the reduction operation of\n  /// \\p VecIn.\n  MachineInstrBuilder buildVecReduceFMul(const DstOp &Dst,\n                                         const SrcOp &ScalarIn,\n                                         const SrcOp &VecIn) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FMUL, {Dst}, {ScalarIn, VecIn});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FMAX \\p Src\n  MachineInstrBuilder buildVecReduceFMax(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FMAX, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_FMIN \\p Src\n  MachineInstrBuilder buildVecReduceFMin(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_FMIN, {Dst}, {Src});\n  }\n  /// Build and insert \\p Res = G_VECREDUCE_ADD \\p Src\n  MachineInstrBuilder buildVecReduceAdd(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_ADD, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_MUL \\p Src\n  MachineInstrBuilder buildVecReduceMul(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_MUL, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_AND \\p Src\n  MachineInstrBuilder buildVecReduceAnd(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_AND, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_OR \\p Src\n  MachineInstrBuilder buildVecReduceOr(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_OR, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_XOR \\p Src\n  MachineInstrBuilder buildVecReduceXor(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_XOR, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_SMAX \\p Src\n  MachineInstrBuilder buildVecReduceSMax(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SMAX, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_SMIN \\p Src\n  MachineInstrBuilder buildVecReduceSMin(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_SMIN, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_UMAX \\p Src\n  MachineInstrBuilder buildVecReduceUMax(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_UMAX, {Dst}, {Src});\n  }\n\n  /// Build and insert \\p Res = G_VECREDUCE_UMIN \\p Src\n  MachineInstrBuilder buildVecReduceUMin(const DstOp &Dst, const SrcOp &Src) {\n    return buildInstr(TargetOpcode::G_VECREDUCE_UMIN, {Dst}, {Src});\n  }\n  virtual MachineInstrBuilder buildInstr(unsigned Opc, ArrayRef<DstOp> DstOps,\n                                         ArrayRef<SrcOp> SrcOps,\n                                         Optional<unsigned> Flags = None);\n};\n\n} // End namespace llvm.\n#endif // LLVM_CODEGEN_GLOBALISEL_MACHINEIRBUILDER_H\n"}, "38": {"id": 38, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/RegisterBank.h", "content": "//==-- llvm/CodeGen/GlobalISel/RegisterBank.h - Register Bank ----*- C++ -*-==//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file This file declares the API of register banks.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_REGISTERBANK_H\n#define LLVM_CODEGEN_GLOBALISEL_REGISTERBANK_H\n\n#include \"llvm/ADT/BitVector.h\"\n\nnamespace llvm {\n// Forward declarations.\nclass RegisterBankInfo;\nclass raw_ostream;\nclass TargetRegisterClass;\nclass TargetRegisterInfo;\n\n/// This class implements the register bank concept.\n/// Two instances of RegisterBank must have different ID.\n/// This property is enforced by the RegisterBankInfo class.\nclass RegisterBank {\nprivate:\n  unsigned ID;\n  const char *Name;\n  unsigned Size;\n  BitVector ContainedRegClasses;\n\n  /// Sentinel value used to recognize register bank not properly\n  /// initialized yet.\n  static const unsigned InvalidID;\n\n  /// Only the RegisterBankInfo can initialize RegisterBank properly.\n  friend RegisterBankInfo;\n\npublic:\n  RegisterBank(unsigned ID, const char *Name, unsigned Size,\n               const uint32_t *CoveredClasses, unsigned NumRegClasses);\n\n  /// Get the identifier of this register bank.\n  unsigned getID() const { return ID; }\n\n  /// Get a user friendly name of this register bank.\n  /// Should be used only for debugging purposes.\n  const char *getName() const { return Name; }\n\n  /// Get the maximal size in bits that fits in this register bank.\n  unsigned getSize() const { return Size; }\n\n  /// Check whether this instance is ready to be used.\n  bool isValid() const;\n\n  /// Check if this register bank is valid. In other words,\n  /// if it has been properly constructed.\n  ///\n  /// \\note This method does not check anything when assertions are disabled.\n  ///\n  /// \\return True is the check was successful.\n  bool verify(const TargetRegisterInfo &TRI) const;\n\n  /// Check whether this register bank covers \\p RC.\n  /// In other words, check if this register bank fully covers\n  /// the registers that \\p RC contains.\n  /// \\pre isValid()\n  bool covers(const TargetRegisterClass &RC) const;\n\n  /// Check whether \\p OtherRB is the same as this.\n  bool operator==(const RegisterBank &OtherRB) const;\n  bool operator!=(const RegisterBank &OtherRB) const {\n    return !this->operator==(OtherRB);\n  }\n\n  /// Dump the register mask on dbgs() stream.\n  /// The dump is verbose.\n  void dump(const TargetRegisterInfo *TRI = nullptr) const;\n\n  /// Print the register mask on OS.\n  /// If IsForDebug is false, then only the name of the register bank\n  /// is printed. Otherwise, all the fields are printing.\n  /// TRI is then used to print the name of the register classes that\n  /// this register bank covers.\n  void print(raw_ostream &OS, bool IsForDebug = false,\n             const TargetRegisterInfo *TRI = nullptr) const;\n};\n\ninline raw_ostream &operator<<(raw_ostream &OS, const RegisterBank &RegBank) {\n  RegBank.print(OS);\n  return OS;\n}\n} // End namespace llvm.\n\n#endif\n"}, "39": {"id": 39, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/Utils.h", "content": "//==-- llvm/CodeGen/GlobalISel/Utils.h ---------------------------*- C++ -*-==//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file This file declares the API of helper functions used throughout the\n/// GlobalISel pipeline.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_GLOBALISEL_UTILS_H\n#define LLVM_CODEGEN_GLOBALISEL_UTILS_H\n\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/CodeGen/MachineBasicBlock.h\"\n#include \"llvm/CodeGen/Register.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/LowLevelTypeImpl.h\"\n#include <cstdint>\n\nnamespace llvm {\n\nclass AnalysisUsage;\nclass BlockFrequencyInfo;\nclass GISelKnownBits;\nclass MachineFunction;\nclass MachineInstr;\nclass MachineOperand;\nclass MachineOptimizationRemarkEmitter;\nclass MachineOptimizationRemarkMissed;\nstruct MachinePointerInfo;\nclass MachineRegisterInfo;\nclass MCInstrDesc;\nclass ProfileSummaryInfo;\nclass RegisterBankInfo;\nclass TargetInstrInfo;\nclass TargetLowering;\nclass TargetPassConfig;\nclass TargetRegisterInfo;\nclass TargetRegisterClass;\nclass ConstantFP;\nclass APFloat;\n\n/// Try to constrain Reg to the specified register class. If this fails,\n/// create a new virtual register in the correct class.\n///\n/// \\return The virtual register constrained to the right register class.\nRegister constrainRegToClass(MachineRegisterInfo &MRI,\n                             const TargetInstrInfo &TII,\n                             const RegisterBankInfo &RBI, Register Reg,\n                             const TargetRegisterClass &RegClass);\n\n/// Constrain the Register operand OpIdx, so that it is now constrained to the\n/// TargetRegisterClass passed as an argument (RegClass).\n/// If this fails, create a new virtual register in the correct class and insert\n/// a COPY before \\p InsertPt if it is a use or after if it is a definition.\n/// In both cases, the function also updates the register of RegMo. The debug\n/// location of \\p InsertPt is used for the new copy.\n///\n/// \\return The virtual register constrained to the right register class.\nRegister constrainOperandRegClass(const MachineFunction &MF,\n                                  const TargetRegisterInfo &TRI,\n                                  MachineRegisterInfo &MRI,\n                                  const TargetInstrInfo &TII,\n                                  const RegisterBankInfo &RBI,\n                                  MachineInstr &InsertPt,\n                                  const TargetRegisterClass &RegClass,\n                                  MachineOperand &RegMO);\n\n/// Try to constrain Reg so that it is usable by argument OpIdx of the provided\n/// MCInstrDesc \\p II. If this fails, create a new virtual register in the\n/// correct class and insert a COPY before \\p InsertPt if it is a use or after\n/// if it is a definition. In both cases, the function also updates the register\n/// of RegMo.\n/// This is equivalent to constrainOperandRegClass(..., RegClass, ...)\n/// with RegClass obtained from the MCInstrDesc. The debug location of \\p\n/// InsertPt is used for the new copy.\n///\n/// \\return The virtual register constrained to the right register class.\nRegister constrainOperandRegClass(const MachineFunction &MF,\n                                  const TargetRegisterInfo &TRI,\n                                  MachineRegisterInfo &MRI,\n                                  const TargetInstrInfo &TII,\n                                  const RegisterBankInfo &RBI,\n                                  MachineInstr &InsertPt, const MCInstrDesc &II,\n                                  MachineOperand &RegMO, unsigned OpIdx);\n\n/// Mutate the newly-selected instruction \\p I to constrain its (possibly\n/// generic) virtual register operands to the instruction's register class.\n/// This could involve inserting COPYs before (for uses) or after (for defs).\n/// This requires the number of operands to match the instruction description.\n/// \\returns whether operand regclass constraining succeeded.\n///\n// FIXME: Not all instructions have the same number of operands. We should\n// probably expose a constrain helper per operand and let the target selector\n// constrain individual registers, like fast-isel.\nbool constrainSelectedInstRegOperands(MachineInstr &I,\n                                      const TargetInstrInfo &TII,\n                                      const TargetRegisterInfo &TRI,\n                                      const RegisterBankInfo &RBI);\n\n/// Check if DstReg can be replaced with SrcReg depending on the register\n/// constraints.\nbool canReplaceReg(Register DstReg, Register SrcReg, MachineRegisterInfo &MRI);\n\n/// Check whether an instruction \\p MI is dead: it only defines dead virtual\n/// registers, and doesn't have other side effects.\nbool isTriviallyDead(const MachineInstr &MI, const MachineRegisterInfo &MRI);\n\n/// Report an ISel error as a missed optimization remark to the LLVMContext's\n/// diagnostic stream.  Set the FailedISel MachineFunction property.\nvoid reportGISelFailure(MachineFunction &MF, const TargetPassConfig &TPC,\n                        MachineOptimizationRemarkEmitter &MORE,\n                        MachineOptimizationRemarkMissed &R);\n\nvoid reportGISelFailure(MachineFunction &MF, const TargetPassConfig &TPC,\n                        MachineOptimizationRemarkEmitter &MORE,\n                        const char *PassName, StringRef Msg,\n                        const MachineInstr &MI);\n\n/// Report an ISel warning as a missed optimization remark to the LLVMContext's\n/// diagnostic stream.\nvoid reportGISelWarning(MachineFunction &MF, const TargetPassConfig &TPC,\n                        MachineOptimizationRemarkEmitter &MORE,\n                        MachineOptimizationRemarkMissed &R);\n\n/// If \\p VReg is defined by a G_CONSTANT, return the corresponding value.\nOptional<APInt> getConstantVRegVal(Register VReg,\n                                   const MachineRegisterInfo &MRI);\n\n/// If \\p VReg is defined by a G_CONSTANT fits in int64_t\n/// returns it.\nOptional<int64_t> getConstantVRegSExtVal(Register VReg,\n                                         const MachineRegisterInfo &MRI);\n\n/// Simple struct used to hold a constant integer value and a virtual\n/// register.\nstruct ValueAndVReg {\n  APInt Value;\n  Register VReg;\n};\n/// If \\p VReg is defined by a statically evaluable chain of\n/// instructions rooted on a G_F/CONSTANT (\\p LookThroughInstrs == true)\n/// and that constant fits in int64_t, returns its value as well as the\n/// virtual register defined by this G_F/CONSTANT.\n/// When \\p LookThroughInstrs == false this function behaves like\n/// getConstantVRegVal.\n/// When \\p HandleFConstants == false the function bails on G_FCONSTANTs.\n/// When \\p LookThroughAnyExt == true the function treats G_ANYEXT same as\n/// G_SEXT.\nOptional<ValueAndVReg>\ngetConstantVRegValWithLookThrough(Register VReg, const MachineRegisterInfo &MRI,\n                                  bool LookThroughInstrs = true,\n                                  bool HandleFConstants = true,\n                                  bool LookThroughAnyExt = false);\nconst ConstantFP* getConstantFPVRegVal(Register VReg,\n                                       const MachineRegisterInfo &MRI);\n\n/// See if Reg is defined by an single def instruction that is\n/// Opcode. Also try to do trivial folding if it's a COPY with\n/// same types. Returns null otherwise.\nMachineInstr *getOpcodeDef(unsigned Opcode, Register Reg,\n                           const MachineRegisterInfo &MRI);\n\n/// Simple struct used to hold a Register value and the instruction which\n/// defines it.\nstruct DefinitionAndSourceRegister {\n  MachineInstr *MI;\n  Register Reg;\n};\n\n/// Find the def instruction for \\p Reg, and underlying value Register folding\n/// away any copies.\n///\n/// Also walks through hints such as G_ASSERT_ZEXT.\nOptional<DefinitionAndSourceRegister>\ngetDefSrcRegIgnoringCopies(Register Reg, const MachineRegisterInfo &MRI);\n\n/// Find the def instruction for \\p Reg, folding away any trivial copies. May\n/// return nullptr if \\p Reg is not a generic virtual register.\n///\n/// Also walks through hints such as G_ASSERT_ZEXT.\nMachineInstr *getDefIgnoringCopies(Register Reg,\n                                   const MachineRegisterInfo &MRI);\n\n/// Find the source register for \\p Reg, folding away any trivial copies. It\n/// will be an output register of the instruction that getDefIgnoringCopies\n/// returns. May return an invalid register if \\p Reg is not a generic virtual\n/// register.\n///\n/// Also walks through hints such as G_ASSERT_ZEXT.\nRegister getSrcRegIgnoringCopies(Register Reg, const MachineRegisterInfo &MRI);\n\n/// Returns an APFloat from Val converted to the appropriate size.\nAPFloat getAPFloatFromSize(double Val, unsigned Size);\n\n/// Modify analysis usage so it preserves passes required for the SelectionDAG\n/// fallback.\nvoid getSelectionDAGFallbackAnalysisUsage(AnalysisUsage &AU);\n\nOptional<APInt> ConstantFoldBinOp(unsigned Opcode, const Register Op1,\n                                  const Register Op2,\n                                  const MachineRegisterInfo &MRI);\n\nOptional<APInt> ConstantFoldExtOp(unsigned Opcode, const Register Op1,\n                                  uint64_t Imm, const MachineRegisterInfo &MRI);\n\n/// Test if the given value is known to have exactly one bit set. This differs\n/// from computeKnownBits in that it doesn't necessarily determine which bit is\n/// set.\nbool isKnownToBeAPowerOfTwo(Register Val, const MachineRegisterInfo &MRI,\n                            GISelKnownBits *KnownBits = nullptr);\n\n/// Returns true if \\p Val can be assumed to never be a NaN. If \\p SNaN is true,\n/// this returns if \\p Val can be assumed to never be a signaling NaN.\nbool isKnownNeverNaN(Register Val, const MachineRegisterInfo &MRI,\n                     bool SNaN = false);\n\n/// Returns true if \\p Val can be assumed to never be a signaling NaN.\ninline bool isKnownNeverSNaN(Register Val, const MachineRegisterInfo &MRI) {\n  return isKnownNeverNaN(Val, MRI, true);\n}\n\nAlign inferAlignFromPtrInfo(MachineFunction &MF, const MachinePointerInfo &MPO);\n\n/// Return a virtual register corresponding to the incoming argument register \\p\n/// PhysReg. This register is expected to have class \\p RC, and optional type \\p\n/// RegTy. This assumes all references to the register will use the same type.\n///\n/// If there is an existing live-in argument register, it will be returned.\n/// This will also ensure there is a valid copy\nRegister getFunctionLiveInPhysReg(MachineFunction &MF, const TargetInstrInfo &TII,\n                                  MCRegister PhysReg,\n                                  const TargetRegisterClass &RC,\n                                  LLT RegTy = LLT());\n\n/// Return the least common multiple type of \\p OrigTy and \\p TargetTy, by changing the\n/// number of vector elements or scalar bitwidth. The intent is a\n/// G_MERGE_VALUES, G_BUILD_VECTOR, or G_CONCAT_VECTORS can be constructed from\n/// \\p OrigTy elements, and unmerged into \\p TargetTy\nLLVM_READNONE\nLLT getLCMType(LLT OrigTy, LLT TargetTy);\n\n/// Return a type where the total size is the greatest common divisor of \\p\n/// OrigTy and \\p TargetTy. This will try to either change the number of vector\n/// elements, or bitwidth of scalars. The intent is the result type can be used\n/// as the result of a G_UNMERGE_VALUES from \\p OrigTy, and then some\n/// combination of G_MERGE_VALUES, G_BUILD_VECTOR and G_CONCAT_VECTORS (possibly\n/// with intermediate casts) can re-form \\p TargetTy.\n///\n/// If these are vectors with different element types, this will try to produce\n/// a vector with a compatible total size, but the element type of \\p OrigTy. If\n/// this can't be satisfied, this will produce a scalar smaller than the\n/// original vector elements.\n///\n/// In the worst case, this returns LLT::scalar(1)\nLLVM_READNONE\nLLT getGCDType(LLT OrigTy, LLT TargetTy);\n\n/// Represents a value which can be a Register or a constant.\n///\n/// This is useful in situations where an instruction may have an interesting\n/// register operand or interesting constant operand. For a concrete example,\n/// \\see getVectorSplat.\nclass RegOrConstant {\n  int64_t Cst;\n  Register Reg;\n  bool IsReg;\n\npublic:\n  explicit RegOrConstant(Register Reg) : Reg(Reg), IsReg(true) {}\n  explicit RegOrConstant(int64_t Cst) : Cst(Cst), IsReg(false) {}\n  bool isReg() const { return IsReg; }\n  bool isCst() const { return !IsReg; }\n  Register getReg() const {\n    assert(isReg() && \"Expected a register!\");\n    return Reg;\n  }\n  int64_t getCst() const {\n    assert(isCst() && \"Expected a constant!\");\n    return Cst;\n  }\n};\n\n/// \\returns The splat index of a G_SHUFFLE_VECTOR \\p MI when \\p MI is a splat.\n/// If \\p MI is not a splat, returns None.\nOptional<int> getSplatIndex(MachineInstr &MI);\n\n/// Returns a scalar constant of a G_BUILD_VECTOR splat if it exists.\nOptional<int64_t> getBuildVectorConstantSplat(const MachineInstr &MI,\n                                              const MachineRegisterInfo &MRI);\n\n/// Return true if the specified instruction is a G_BUILD_VECTOR or\n/// G_BUILD_VECTOR_TRUNC where all of the elements are 0 or undef.\nbool isBuildVectorAllZeros(const MachineInstr &MI,\n                           const MachineRegisterInfo &MRI);\n\n/// Return true if the specified instruction is a G_BUILD_VECTOR or\n/// G_BUILD_VECTOR_TRUNC where all of the elements are ~0 or undef.\nbool isBuildVectorAllOnes(const MachineInstr &MI,\n                          const MachineRegisterInfo &MRI);\n\n/// \\returns a value when \\p MI is a vector splat. The splat can be either a\n/// Register or a constant.\n///\n/// Examples:\n///\n/// \\code\n///   %reg = COPY $physreg\n///   %reg_splat = G_BUILD_VECTOR %reg, %reg, ..., %reg\n/// \\endcode\n///\n/// If called on the G_BUILD_VECTOR above, this will return a RegOrConstant\n/// containing %reg.\n///\n/// \\code\n///   %cst = G_CONSTANT iN 4\n///   %constant_splat = G_BUILD_VECTOR %cst, %cst, ..., %cst\n/// \\endcode\n///\n/// In the above case, this will return a RegOrConstant containing 4.\nOptional<RegOrConstant> getVectorSplat(const MachineInstr &MI,\n                                       const MachineRegisterInfo &MRI);\n\n/// Returns true if given the TargetLowering's boolean contents information,\n/// the value \\p Val contains a true value.\nbool isConstTrueVal(const TargetLowering &TLI, int64_t Val, bool IsVector,\n                    bool IsFP);\n\n/// Returns an integer representing true, as defined by the\n/// TargetBooleanContents.\nint64_t getICmpTrueVal(const TargetLowering &TLI, bool IsVector, bool IsFP);\n\n/// Returns true if the given block should be optimized for size.\nbool shouldOptForSize(const MachineBasicBlock &MBB, ProfileSummaryInfo *PSI,\n                      BlockFrequencyInfo *BFI);\n} // End namespace llvm.\n#endif\n"}, "40": {"id": 40, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/LiveRegUnits.h", "content": "//===- llvm/CodeGen/LiveRegUnits.h - Register Unit Set ----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n/// \\file\n/// A set of register units. It is intended for register liveness tracking.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_LIVEREGUNITS_H\n#define LLVM_CODEGEN_LIVEREGUNITS_H\n\n#include \"llvm/ADT/BitVector.h\"\n#include \"llvm/CodeGen/MachineInstrBundle.h\"\n#include \"llvm/CodeGen/TargetRegisterInfo.h\"\n#include \"llvm/MC/LaneBitmask.h\"\n#include \"llvm/MC/MCRegisterInfo.h\"\n#include <cstdint>\n\nnamespace llvm {\n\nclass MachineInstr;\nclass MachineBasicBlock;\n\n/// A set of register units used to track register liveness.\nclass LiveRegUnits {\n  const TargetRegisterInfo *TRI = nullptr;\n  BitVector Units;\n\npublic:\n  /// Constructs a new empty LiveRegUnits set.\n  LiveRegUnits() = default;\n\n  /// Constructs and initialize an empty LiveRegUnits set.\n  LiveRegUnits(const TargetRegisterInfo &TRI) {\n    init(TRI);\n  }\n\n  /// For a machine instruction \\p MI, adds all register units used in\n  /// \\p UsedRegUnits and defined or clobbered in \\p ModifiedRegUnits. This is\n  /// useful when walking over a range of instructions to track registers\n  /// used or defined seperately.\n  static void accumulateUsedDefed(const MachineInstr &MI,\n                                  LiveRegUnits &ModifiedRegUnits,\n                                  LiveRegUnits &UsedRegUnits,\n                                  const TargetRegisterInfo *TRI) {\n    for (ConstMIBundleOperands O(MI); O.isValid(); ++O) {\n      if (O->isRegMask())\n        ModifiedRegUnits.addRegsInMask(O->getRegMask());\n      if (!O->isReg())\n        continue;\n      Register Reg = O->getReg();\n      if (!Reg.isPhysical())\n        continue;\n      if (O->isDef()) {\n        // Some architectures (e.g. AArch64 XZR/WZR) have registers that are\n        // constant and may be used as destinations to indicate the generated\n        // value is discarded. No need to track such case as a def.\n        if (!TRI->isConstantPhysReg(Reg))\n          ModifiedRegUnits.addReg(Reg);\n      } else {\n        assert(O->isUse() && \"Reg operand not a def and not a use\");\n        UsedRegUnits.addReg(Reg);\n      }\n    }\n  }\n\n  /// Initialize and clear the set.\n  void init(const TargetRegisterInfo &TRI) {\n    this->TRI = &TRI;\n    Units.reset();\n    Units.resize(TRI.getNumRegUnits());\n  }\n\n  /// Clears the set.\n  void clear() { Units.reset(); }\n\n  /// Returns true if the set is empty.\n  bool empty() const { return Units.none(); }\n\n  /// Adds register units covered by physical register \\p Reg.\n  void addReg(MCPhysReg Reg) {\n    for (MCRegUnitIterator Unit(Reg, TRI); Unit.isValid(); ++Unit)\n      Units.set(*Unit);\n  }\n\n  /// Adds register units covered by physical register \\p Reg that are\n  /// part of the lanemask \\p Mask.\n  void addRegMasked(MCPhysReg Reg, LaneBitmask Mask) {\n    for (MCRegUnitMaskIterator Unit(Reg, TRI); Unit.isValid(); ++Unit) {\n      LaneBitmask UnitMask = (*Unit).second;\n      if (UnitMask.none() || (UnitMask & Mask).any())\n        Units.set((*Unit).first);\n    }\n  }\n\n  /// Removes all register units covered by physical register \\p Reg.\n  void removeReg(MCPhysReg Reg) {\n    for (MCRegUnitIterator Unit(Reg, TRI); Unit.isValid(); ++Unit)\n      Units.reset(*Unit);\n  }\n\n  /// Removes register units not preserved by the regmask \\p RegMask.\n  /// The regmask has the same format as the one in the RegMask machine operand.\n  void removeRegsNotPreserved(const uint32_t *RegMask);\n\n  /// Adds register units not preserved by the regmask \\p RegMask.\n  /// The regmask has the same format as the one in the RegMask machine operand.\n  void addRegsInMask(const uint32_t *RegMask);\n\n  /// Returns true if no part of physical register \\p Reg is live.\n  bool available(MCPhysReg Reg) const {\n    for (MCRegUnitIterator Unit(Reg, TRI); Unit.isValid(); ++Unit) {\n      if (Units.test(*Unit))\n        return false;\n    }\n    return true;\n  }\n\n  /// Updates liveness when stepping backwards over the instruction \\p MI.\n  /// This removes all register units defined or clobbered in \\p MI and then\n  /// adds the units used (as in use operands) in \\p MI.\n  void stepBackward(const MachineInstr &MI);\n\n  /// Adds all register units used, defined or clobbered in \\p MI.\n  /// This is useful when walking over a range of instruction to find registers\n  /// unused over the whole range.\n  void accumulate(const MachineInstr &MI);\n\n  /// Adds registers living out of block \\p MBB.\n  /// Live out registers are the union of the live-in registers of the successor\n  /// blocks and pristine registers. Live out registers of the end block are the\n  /// callee saved registers.\n  void addLiveOuts(const MachineBasicBlock &MBB);\n\n  /// Adds registers living into block \\p MBB.\n  void addLiveIns(const MachineBasicBlock &MBB);\n\n  /// Adds all register units marked in the bitvector \\p RegUnits.\n  void addUnits(const BitVector &RegUnits) {\n    Units |= RegUnits;\n  }\n  /// Removes all register units marked in the bitvector \\p RegUnits.\n  void removeUnits(const BitVector &RegUnits) {\n    Units.reset(RegUnits);\n  }\n  /// Return the internal bitvector representation of the set.\n  const BitVector &getBitVector() const {\n    return Units;\n  }\n\nprivate:\n  /// Adds pristine registers. Pristine registers are callee saved registers\n  /// that are unused in the function.\n  void addPristines(const MachineFunction &MF);\n};\n\n/// Returns an iterator range over all physical register and mask operands for\n/// \\p MI and bundled instructions. This also skips any debug operands.\ninline iterator_range<filter_iterator<\n    ConstMIBundleOperands, std::function<bool(const MachineOperand &)>>>\nphys_regs_and_masks(const MachineInstr &MI) {\n  std::function<bool(const MachineOperand &)> Pred =\n      [](const MachineOperand &MOP) {\n        return MOP.isRegMask() || (MOP.isReg() && !MOP.isDebug() &&\n                                   Register::isPhysicalRegister(MOP.getReg()));\n      };\n  return make_filter_range(const_mi_bundle_ops(MI), Pred);\n}\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_LIVEREGUNITS_H\n"}, "42": {"id": 42, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineFrameInfo.h", "content": "//===-- CodeGen/MachineFrameInfo.h - Abstract Stack Frame Rep. --*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// The file defines the MachineFrameInfo class.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_MACHINEFRAMEINFO_H\n#define LLVM_CODEGEN_MACHINEFRAMEINFO_H\n\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/Register.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/DataTypes.h\"\n#include <cassert>\n#include <vector>\n\nnamespace llvm {\nclass raw_ostream;\nclass MachineFunction;\nclass MachineBasicBlock;\nclass BitVector;\nclass AllocaInst;\n\n/// The CalleeSavedInfo class tracks the information need to locate where a\n/// callee saved register is in the current frame.\n/// Callee saved reg can also be saved to a different register rather than\n/// on the stack by setting DstReg instead of FrameIdx.\nclass CalleeSavedInfo {\n  Register Reg;\n  union {\n    int FrameIdx;\n    unsigned DstReg;\n  };\n  /// Flag indicating whether the register is actually restored in the epilog.\n  /// In most cases, if a register is saved, it is also restored. There are\n  /// some situations, though, when this is not the case. For example, the\n  /// LR register on ARM is usually saved, but on exit from the function its\n  /// saved value may be loaded directly into PC. Since liveness tracking of\n  /// physical registers treats callee-saved registers are live outside of\n  /// the function, LR would be treated as live-on-exit, even though in these\n  /// scenarios it is not. This flag is added to indicate that the saved\n  /// register described by this object is not restored in the epilog.\n  /// The long-term solution is to model the liveness of callee-saved registers\n  /// by implicit uses on the return instructions, however, the required\n  /// changes in the ARM backend would be quite extensive.\n  bool Restored;\n  /// Flag indicating whether the register is spilled to stack or another\n  /// register.\n  bool SpilledToReg;\n\npublic:\n  explicit CalleeSavedInfo(unsigned R, int FI = 0)\n  : Reg(R), FrameIdx(FI), Restored(true), SpilledToReg(false) {}\n\n  // Accessors.\n  Register getReg()                        const { return Reg; }\n  int getFrameIdx()                        const { return FrameIdx; }\n  unsigned getDstReg()                     const { return DstReg; }\n  void setFrameIdx(int FI) {\n    FrameIdx = FI;\n    SpilledToReg = false;\n  }\n  void setDstReg(Register SpillReg) {\n    DstReg = SpillReg;\n    SpilledToReg = true;\n  }\n  bool isRestored()                        const { return Restored; }\n  void setRestored(bool R)                       { Restored = R; }\n  bool isSpilledToReg()                    const { return SpilledToReg; }\n};\n\n/// The MachineFrameInfo class represents an abstract stack frame until\n/// prolog/epilog code is inserted.  This class is key to allowing stack frame\n/// representation optimizations, such as frame pointer elimination.  It also\n/// allows more mundane (but still important) optimizations, such as reordering\n/// of abstract objects on the stack frame.\n///\n/// To support this, the class assigns unique integer identifiers to stack\n/// objects requested clients.  These identifiers are negative integers for\n/// fixed stack objects (such as arguments passed on the stack) or nonnegative\n/// for objects that may be reordered.  Instructions which refer to stack\n/// objects use a special MO_FrameIndex operand to represent these frame\n/// indexes.\n///\n/// Because this class keeps track of all references to the stack frame, it\n/// knows when a variable sized object is allocated on the stack.  This is the\n/// sole condition which prevents frame pointer elimination, which is an\n/// important optimization on register-poor architectures.  Because original\n/// variable sized alloca's in the source program are the only source of\n/// variable sized stack objects, it is safe to decide whether there will be\n/// any variable sized objects before all stack objects are known (for\n/// example, register allocator spill code never needs variable sized\n/// objects).\n///\n/// When prolog/epilog code emission is performed, the final stack frame is\n/// built and the machine instructions are modified to refer to the actual\n/// stack offsets of the object, eliminating all MO_FrameIndex operands from\n/// the program.\n///\n/// Abstract Stack Frame Information\nclass MachineFrameInfo {\npublic:\n  /// Stack Smashing Protection (SSP) rules require that vulnerable stack\n  /// allocations are located close the stack protector.\n  enum SSPLayoutKind {\n    SSPLK_None,       ///< Did not trigger a stack protector.  No effect on data\n                      ///< layout.\n    SSPLK_LargeArray, ///< Array or nested array >= SSP-buffer-size.  Closest\n                      ///< to the stack protector.\n    SSPLK_SmallArray, ///< Array or nested array < SSP-buffer-size. 2nd closest\n                      ///< to the stack protector.\n    SSPLK_AddrOf      ///< The address of this allocation is exposed and\n                      ///< triggered protection.  3rd closest to the protector.\n  };\n\nprivate:\n  // Represent a single object allocated on the stack.\n  struct StackObject {\n    // The offset of this object from the stack pointer on entry to\n    // the function.  This field has no meaning for a variable sized element.\n    int64_t SPOffset;\n\n    // The size of this object on the stack. 0 means a variable sized object,\n    // ~0ULL means a dead object.\n    uint64_t Size;\n\n    // The required alignment of this stack slot.\n    Align Alignment;\n\n    // If true, the value of the stack object is set before\n    // entering the function and is not modified inside the function. By\n    // default, fixed objects are immutable unless marked otherwise.\n    bool isImmutable;\n\n    // If true the stack object is used as spill slot. It\n    // cannot alias any other memory objects.\n    bool isSpillSlot;\n\n    /// If true, this stack slot is used to spill a value (could be deopt\n    /// and/or GC related) over a statepoint. We know that the address of the\n    /// slot can't alias any LLVM IR value.  This is very similar to a Spill\n    /// Slot, but is created by statepoint lowering is SelectionDAG, not the\n    /// register allocator.\n    bool isStatepointSpillSlot = false;\n\n    /// Identifier for stack memory type analagous to address space. If this is\n    /// non-0, the meaning is target defined. Offsets cannot be directly\n    /// compared between objects with different stack IDs. The object may not\n    /// necessarily reside in the same contiguous memory block as other stack\n    /// objects. Objects with differing stack IDs should not be merged or\n    /// replaced substituted for each other.\n    //\n    /// It is assumed a target uses consecutive, increasing stack IDs starting\n    /// from 1.\n    uint8_t StackID;\n\n    /// If this stack object is originated from an Alloca instruction\n    /// this value saves the original IR allocation. Can be NULL.\n    const AllocaInst *Alloca;\n\n    // If true, the object was mapped into the local frame\n    // block and doesn't need additional handling for allocation beyond that.\n    bool PreAllocated = false;\n\n    // If true, an LLVM IR value might point to this object.\n    // Normally, spill slots and fixed-offset objects don't alias IR-accessible\n    // objects, but there are exceptions (on PowerPC, for example, some byval\n    // arguments have ABI-prescribed offsets).\n    bool isAliased;\n\n    /// If true, the object has been zero-extended.\n    bool isZExt = false;\n\n    /// If true, the object has been zero-extended.\n    bool isSExt = false;\n\n    uint8_t SSPLayout;\n\n    StackObject(uint64_t Size, Align Alignment, int64_t SPOffset,\n                bool IsImmutable, bool IsSpillSlot, const AllocaInst *Alloca,\n                bool IsAliased, uint8_t StackID = 0)\n        : SPOffset(SPOffset), Size(Size), Alignment(Alignment),\n          isImmutable(IsImmutable), isSpillSlot(IsSpillSlot), StackID(StackID),\n          Alloca(Alloca), isAliased(IsAliased), SSPLayout(SSPLK_None) {}\n  };\n\n  /// The alignment of the stack.\n  Align StackAlignment;\n\n  /// Can the stack be realigned. This can be false if the target does not\n  /// support stack realignment, or if the user asks us not to realign the\n  /// stack. In this situation, overaligned allocas are all treated as dynamic\n  /// allocations and the target must handle them as part of DYNAMIC_STACKALLOC\n  /// lowering. All non-alloca stack objects have their alignment clamped to the\n  /// base ABI stack alignment.\n  /// FIXME: There is room for improvement in this case, in terms of\n  /// grouping overaligned allocas into a \"secondary stack frame\" and\n  /// then only use a single alloca to allocate this frame and only a\n  /// single virtual register to access it. Currently, without such an\n  /// optimization, each such alloca gets its own dynamic realignment.\n  bool StackRealignable;\n\n  /// Whether the function has the \\c alignstack attribute.\n  bool ForcedRealign;\n\n  /// The list of stack objects allocated.\n  std::vector<StackObject> Objects;\n\n  /// This contains the number of fixed objects contained on\n  /// the stack.  Because fixed objects are stored at a negative index in the\n  /// Objects list, this is also the index to the 0th object in the list.\n  unsigned NumFixedObjects = 0;\n\n  /// This boolean keeps track of whether any variable\n  /// sized objects have been allocated yet.\n  bool HasVarSizedObjects = false;\n\n  /// This boolean keeps track of whether there is a call\n  /// to builtin \\@llvm.frameaddress.\n  bool FrameAddressTaken = false;\n\n  /// This boolean keeps track of whether there is a call\n  /// to builtin \\@llvm.returnaddress.\n  bool ReturnAddressTaken = false;\n\n  /// This boolean keeps track of whether there is a call\n  /// to builtin \\@llvm.experimental.stackmap.\n  bool HasStackMap = false;\n\n  /// This boolean keeps track of whether there is a call\n  /// to builtin \\@llvm.experimental.patchpoint.\n  bool HasPatchPoint = false;\n\n  /// The prolog/epilog code inserter calculates the final stack\n  /// offsets for all of the fixed size objects, updating the Objects list\n  /// above.  It then updates StackSize to contain the number of bytes that need\n  /// to be allocated on entry to the function.\n  uint64_t StackSize = 0;\n\n  /// The amount that a frame offset needs to be adjusted to\n  /// have the actual offset from the stack/frame pointer.  The exact usage of\n  /// this is target-dependent, but it is typically used to adjust between\n  /// SP-relative and FP-relative offsets.  E.G., if objects are accessed via\n  /// SP then OffsetAdjustment is zero; if FP is used, OffsetAdjustment is set\n  /// to the distance between the initial SP and the value in FP.  For many\n  /// targets, this value is only used when generating debug info (via\n  /// TargetRegisterInfo::getFrameIndexReference); when generating code, the\n  /// corresponding adjustments are performed directly.\n  int OffsetAdjustment = 0;\n\n  /// The prolog/epilog code inserter may process objects that require greater\n  /// alignment than the default alignment the target provides.\n  /// To handle this, MaxAlignment is set to the maximum alignment\n  /// needed by the objects on the current frame.  If this is greater than the\n  /// native alignment maintained by the compiler, dynamic alignment code will\n  /// be needed.\n  ///\n  Align MaxAlignment;\n\n  /// Set to true if this function adjusts the stack -- e.g.,\n  /// when calling another function. This is only valid during and after\n  /// prolog/epilog code insertion.\n  bool AdjustsStack = false;\n\n  /// Set to true if this function has any function calls.\n  bool HasCalls = false;\n\n  /// The frame index for the stack protector.\n  int StackProtectorIdx = -1;\n\n  /// The frame index for the function context. Used for SjLj exceptions.\n  int FunctionContextIdx = -1;\n\n  /// This contains the size of the largest call frame if the target uses frame\n  /// setup/destroy pseudo instructions (as defined in the TargetFrameInfo\n  /// class).  This information is important for frame pointer elimination.\n  /// It is only valid during and after prolog/epilog code insertion.\n  unsigned MaxCallFrameSize = ~0u;\n\n  /// The number of bytes of callee saved registers that the target wants to\n  /// report for the current function in the CodeView S_FRAMEPROC record.\n  unsigned CVBytesOfCalleeSavedRegisters = 0;\n\n  /// The prolog/epilog code inserter fills in this vector with each\n  /// callee saved register saved in either the frame or a different\n  /// register.  Beyond its use by the prolog/ epilog code inserter,\n  /// this data is used for debug info and exception handling.\n  std::vector<CalleeSavedInfo> CSInfo;\n\n  /// Has CSInfo been set yet?\n  bool CSIValid = false;\n\n  /// References to frame indices which are mapped\n  /// into the local frame allocation block. <FrameIdx, LocalOffset>\n  SmallVector<std::pair<int, int64_t>, 32> LocalFrameObjects;\n\n  /// Size of the pre-allocated local frame block.\n  int64_t LocalFrameSize = 0;\n\n  /// Required alignment of the local object blob, which is the strictest\n  /// alignment of any object in it.\n  Align LocalFrameMaxAlign;\n\n  /// Whether the local object blob needs to be allocated together. If not,\n  /// PEI should ignore the isPreAllocated flags on the stack objects and\n  /// just allocate them normally.\n  bool UseLocalStackAllocationBlock = false;\n\n  /// True if the function dynamically adjusts the stack pointer through some\n  /// opaque mechanism like inline assembly or Win32 EH.\n  bool HasOpaqueSPAdjustment = false;\n\n  /// True if the function contains operations which will lower down to\n  /// instructions which manipulate the stack pointer.\n  bool HasCopyImplyingStackAdjustment = false;\n\n  /// True if the function contains a call to the llvm.vastart intrinsic.\n  bool HasVAStart = false;\n\n  /// True if this is a varargs function that contains a musttail call.\n  bool HasMustTailInVarArgFunc = false;\n\n  /// True if this function contains a tail call. If so immutable objects like\n  /// function arguments are no longer so. A tail call *can* override fixed\n  /// stack objects like arguments so we can't treat them as immutable.\n  bool HasTailCall = false;\n\n  /// Not null, if shrink-wrapping found a better place for the prologue.\n  MachineBasicBlock *Save = nullptr;\n  /// Not null, if shrink-wrapping found a better place for the epilogue.\n  MachineBasicBlock *Restore = nullptr;\n\npublic:\n  explicit MachineFrameInfo(unsigned StackAlignment, bool StackRealignable,\n                            bool ForcedRealign)\n      : StackAlignment(assumeAligned(StackAlignment)),\n        StackRealignable(StackRealignable), ForcedRealign(ForcedRealign) {}\n\n  /// Return true if there are any stack objects in this function.\n  bool hasStackObjects() const { return !Objects.empty(); }\n\n  /// This method may be called any time after instruction\n  /// selection is complete to determine if the stack frame for this function\n  /// contains any variable sized objects.\n  bool hasVarSizedObjects() const { return HasVarSizedObjects; }\n\n  /// Return the index for the stack protector object.\n  int getStackProtectorIndex() const { return StackProtectorIdx; }\n  void setStackProtectorIndex(int I) { StackProtectorIdx = I; }\n  bool hasStackProtectorIndex() const { return StackProtectorIdx != -1; }\n\n  /// Return the index for the function context object.\n  /// This object is used for SjLj exceptions.\n  int getFunctionContextIndex() const { return FunctionContextIdx; }\n  void setFunctionContextIndex(int I) { FunctionContextIdx = I; }\n\n  /// This method may be called any time after instruction\n  /// selection is complete to determine if there is a call to\n  /// \\@llvm.frameaddress in this function.\n  bool isFrameAddressTaken() const { return FrameAddressTaken; }\n  void setFrameAddressIsTaken(bool T) { FrameAddressTaken = T; }\n\n  /// This method may be called any time after\n  /// instruction selection is complete to determine if there is a call to\n  /// \\@llvm.returnaddress in this function.\n  bool isReturnAddressTaken() const { return ReturnAddressTaken; }\n  void setReturnAddressIsTaken(bool s) { ReturnAddressTaken = s; }\n\n  /// This method may be called any time after instruction\n  /// selection is complete to determine if there is a call to builtin\n  /// \\@llvm.experimental.stackmap.\n  bool hasStackMap() const { return HasStackMap; }\n  void setHasStackMap(bool s = true) { HasStackMap = s; }\n\n  /// This method may be called any time after instruction\n  /// selection is complete to determine if there is a call to builtin\n  /// \\@llvm.experimental.patchpoint.\n  bool hasPatchPoint() const { return HasPatchPoint; }\n  void setHasPatchPoint(bool s = true) { HasPatchPoint = s; }\n\n  /// Return the minimum frame object index.\n  int getObjectIndexBegin() const { return -NumFixedObjects; }\n\n  /// Return one past the maximum frame object index.\n  int getObjectIndexEnd() const { return (int)Objects.size()-NumFixedObjects; }\n\n  /// Return the number of fixed objects.\n  unsigned getNumFixedObjects() const { return NumFixedObjects; }\n\n  /// Return the number of objects.\n  unsigned getNumObjects() const { return Objects.size(); }\n\n  /// Map a frame index into the local object block\n  void mapLocalFrameObject(int ObjectIndex, int64_t Offset) {\n    LocalFrameObjects.push_back(std::pair<int, int64_t>(ObjectIndex, Offset));\n    Objects[ObjectIndex + NumFixedObjects].PreAllocated = true;\n  }\n\n  /// Get the local offset mapping for a for an object.\n  std::pair<int, int64_t> getLocalFrameObjectMap(int i) const {\n    assert (i >= 0 && (unsigned)i < LocalFrameObjects.size() &&\n            \"Invalid local object reference!\");\n    return LocalFrameObjects[i];\n  }\n\n  /// Return the number of objects allocated into the local object block.\n  int64_t getLocalFrameObjectCount() const { return LocalFrameObjects.size(); }\n\n  /// Set the size of the local object blob.\n  void setLocalFrameSize(int64_t sz) { LocalFrameSize = sz; }\n\n  /// Get the size of the local object blob.\n  int64_t getLocalFrameSize() const { return LocalFrameSize; }\n\n  /// Required alignment of the local object blob,\n  /// which is the strictest alignment of any object in it.\n  void setLocalFrameMaxAlign(Align Alignment) {\n    LocalFrameMaxAlign = Alignment;\n  }\n\n  /// Return the required alignment of the local object blob.\n  Align getLocalFrameMaxAlign() const { return LocalFrameMaxAlign; }\n\n  /// Get whether the local allocation blob should be allocated together or\n  /// let PEI allocate the locals in it directly.\n  bool getUseLocalStackAllocationBlock() const {\n    return UseLocalStackAllocationBlock;\n  }\n\n  /// setUseLocalStackAllocationBlock - Set whether the local allocation blob\n  /// should be allocated together or let PEI allocate the locals in it\n  /// directly.\n  void setUseLocalStackAllocationBlock(bool v) {\n    UseLocalStackAllocationBlock = v;\n  }\n\n  /// Return true if the object was pre-allocated into the local block.\n  bool isObjectPreAllocated(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].PreAllocated;\n  }\n\n  /// Return the size of the specified object.\n  int64_t getObjectSize(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].Size;\n  }\n\n  /// Change the size of the specified stack object.\n  void setObjectSize(int ObjectIdx, int64_t Size) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    Objects[ObjectIdx+NumFixedObjects].Size = Size;\n  }\n\n  LLVM_ATTRIBUTE_DEPRECATED(inline unsigned getObjectAlignment(int ObjectIdx)\n                                const,\n                            \"Use getObjectAlign instead\") {\n    return getObjectAlign(ObjectIdx).value();\n  }\n\n  /// Return the alignment of the specified stack object.\n  Align getObjectAlign(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx + NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx + NumFixedObjects].Alignment;\n  }\n\n  /// setObjectAlignment - Change the alignment of the specified stack object.\n  void setObjectAlignment(int ObjectIdx, Align Alignment) {\n    assert(unsigned(ObjectIdx + NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    Objects[ObjectIdx + NumFixedObjects].Alignment = Alignment;\n\n    // Only ensure max alignment for the default stack.\n    if (getStackID(ObjectIdx) == 0)\n      ensureMaxAlignment(Alignment);\n  }\n\n  LLVM_ATTRIBUTE_DEPRECATED(inline void setObjectAlignment(int ObjectIdx,\n                                                           unsigned Align),\n                            \"Use the version that takes Align instead\") {\n    setObjectAlignment(ObjectIdx, assumeAligned(Align));\n  }\n\n  /// Return the underlying Alloca of the specified\n  /// stack object if it exists. Returns 0 if none exists.\n  const AllocaInst* getObjectAllocation(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].Alloca;\n  }\n\n  /// Return the assigned stack offset of the specified object\n  /// from the incoming stack pointer.\n  int64_t getObjectOffset(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    assert(!isDeadObjectIndex(ObjectIdx) &&\n           \"Getting frame offset for a dead object?\");\n    return Objects[ObjectIdx+NumFixedObjects].SPOffset;\n  }\n\n  bool isObjectZExt(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].isZExt;\n  }\n\n  void setObjectZExt(int ObjectIdx, bool IsZExt) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    Objects[ObjectIdx+NumFixedObjects].isZExt = IsZExt;\n  }\n\n  bool isObjectSExt(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].isSExt;\n  }\n\n  void setObjectSExt(int ObjectIdx, bool IsSExt) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    Objects[ObjectIdx+NumFixedObjects].isSExt = IsSExt;\n  }\n\n  /// Set the stack frame offset of the specified object. The\n  /// offset is relative to the stack pointer on entry to the function.\n  void setObjectOffset(int ObjectIdx, int64_t SPOffset) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    assert(!isDeadObjectIndex(ObjectIdx) &&\n           \"Setting frame offset for a dead object?\");\n    Objects[ObjectIdx+NumFixedObjects].SPOffset = SPOffset;\n  }\n\n  SSPLayoutKind getObjectSSPLayout(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return (SSPLayoutKind)Objects[ObjectIdx+NumFixedObjects].SSPLayout;\n  }\n\n  void setObjectSSPLayout(int ObjectIdx, SSPLayoutKind Kind) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    assert(!isDeadObjectIndex(ObjectIdx) &&\n           \"Setting SSP layout for a dead object?\");\n    Objects[ObjectIdx+NumFixedObjects].SSPLayout = Kind;\n  }\n\n  /// Return the number of bytes that must be allocated to hold\n  /// all of the fixed size frame objects.  This is only valid after\n  /// Prolog/Epilog code insertion has finalized the stack frame layout.\n  uint64_t getStackSize() const { return StackSize; }\n\n  /// Set the size of the stack.\n  void setStackSize(uint64_t Size) { StackSize = Size; }\n\n  /// Estimate and return the size of the stack frame.\n  uint64_t estimateStackSize(const MachineFunction &MF) const;\n\n  /// Return the correction for frame offsets.\n  int getOffsetAdjustment() const { return OffsetAdjustment; }\n\n  /// Set the correction for frame offsets.\n  void setOffsetAdjustment(int Adj) { OffsetAdjustment = Adj; }\n\n  /// Return the alignment in bytes that this function must be aligned to,\n  /// which is greater than the default stack alignment provided by the target.\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned getMaxAlignment() const,\n                            \"Use getMaxAlign instead\") {\n    return MaxAlignment.value();\n  }\n  /// Return the alignment in bytes that this function must be aligned to,\n  /// which is greater than the default stack alignment provided by the target.\n  Align getMaxAlign() const { return MaxAlignment; }\n\n  /// Make sure the function is at least Align bytes aligned.\n  void ensureMaxAlignment(Align Alignment);\n\n  LLVM_ATTRIBUTE_DEPRECATED(inline void ensureMaxAlignment(unsigned Align),\n                            \"Use the version that uses Align instead\") {\n    ensureMaxAlignment(assumeAligned(Align));\n  }\n\n  /// Return true if this function adjusts the stack -- e.g.,\n  /// when calling another function. This is only valid during and after\n  /// prolog/epilog code insertion.\n  bool adjustsStack() const { return AdjustsStack; }\n  void setAdjustsStack(bool V) { AdjustsStack = V; }\n\n  /// Return true if the current function has any function calls.\n  bool hasCalls() const { return HasCalls; }\n  void setHasCalls(bool V) { HasCalls = V; }\n\n  /// Returns true if the function contains opaque dynamic stack adjustments.\n  bool hasOpaqueSPAdjustment() const { return HasOpaqueSPAdjustment; }\n  void setHasOpaqueSPAdjustment(bool B) { HasOpaqueSPAdjustment = B; }\n\n  /// Returns true if the function contains operations which will lower down to\n  /// instructions which manipulate the stack pointer.\n  bool hasCopyImplyingStackAdjustment() const {\n    return HasCopyImplyingStackAdjustment;\n  }\n  void setHasCopyImplyingStackAdjustment(bool B) {\n    HasCopyImplyingStackAdjustment = B;\n  }\n\n  /// Returns true if the function calls the llvm.va_start intrinsic.\n  bool hasVAStart() const { return HasVAStart; }\n  void setHasVAStart(bool B) { HasVAStart = B; }\n\n  /// Returns true if the function is variadic and contains a musttail call.\n  bool hasMustTailInVarArgFunc() const { return HasMustTailInVarArgFunc; }\n  void setHasMustTailInVarArgFunc(bool B) { HasMustTailInVarArgFunc = B; }\n\n  /// Returns true if the function contains a tail call.\n  bool hasTailCall() const { return HasTailCall; }\n  void setHasTailCall() { HasTailCall = true; }\n\n  /// Computes the maximum size of a callframe and the AdjustsStack property.\n  /// This only works for targets defining\n  /// TargetInstrInfo::getCallFrameSetupOpcode(), getCallFrameDestroyOpcode(),\n  /// and getFrameSize().\n  /// This is usually computed by the prologue epilogue inserter but some\n  /// targets may call this to compute it earlier.\n  void computeMaxCallFrameSize(const MachineFunction &MF);\n\n  /// Return the maximum size of a call frame that must be\n  /// allocated for an outgoing function call.  This is only available if\n  /// CallFrameSetup/Destroy pseudo instructions are used by the target, and\n  /// then only during or after prolog/epilog code insertion.\n  ///\n  unsigned getMaxCallFrameSize() const {\n    // TODO: Enable this assert when targets are fixed.\n    //assert(isMaxCallFrameSizeComputed() && \"MaxCallFrameSize not computed yet\");\n    if (!isMaxCallFrameSizeComputed())\n      return 0;\n    return MaxCallFrameSize;\n  }\n  bool isMaxCallFrameSizeComputed() const {\n    return MaxCallFrameSize != ~0u;\n  }\n  void setMaxCallFrameSize(unsigned S) { MaxCallFrameSize = S; }\n\n  /// Returns how many bytes of callee-saved registers the target pushed in the\n  /// prologue. Only used for debug info.\n  unsigned getCVBytesOfCalleeSavedRegisters() const {\n    return CVBytesOfCalleeSavedRegisters;\n  }\n  void setCVBytesOfCalleeSavedRegisters(unsigned S) {\n    CVBytesOfCalleeSavedRegisters = S;\n  }\n\n  /// Create a new object at a fixed location on the stack.\n  /// All fixed objects should be created before other objects are created for\n  /// efficiency. By default, fixed objects are not pointed to by LLVM IR\n  /// values. This returns an index with a negative value.\n  int CreateFixedObject(uint64_t Size, int64_t SPOffset, bool IsImmutable,\n                        bool isAliased = false);\n\n  /// Create a spill slot at a fixed location on the stack.\n  /// Returns an index with a negative value.\n  int CreateFixedSpillStackObject(uint64_t Size, int64_t SPOffset,\n                                  bool IsImmutable = false);\n\n  /// Returns true if the specified index corresponds to a fixed stack object.\n  bool isFixedObjectIndex(int ObjectIdx) const {\n    return ObjectIdx < 0 && (ObjectIdx >= -(int)NumFixedObjects);\n  }\n\n  /// Returns true if the specified index corresponds\n  /// to an object that might be pointed to by an LLVM IR value.\n  bool isAliasedObjectIndex(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].isAliased;\n  }\n\n  /// Returns true if the specified index corresponds to an immutable object.\n  bool isImmutableObjectIndex(int ObjectIdx) const {\n    // Tail calling functions can clobber their function arguments.\n    if (HasTailCall)\n      return false;\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].isImmutable;\n  }\n\n  /// Marks the immutability of an object.\n  void setIsImmutableObjectIndex(int ObjectIdx, bool IsImmutable) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    Objects[ObjectIdx+NumFixedObjects].isImmutable = IsImmutable;\n  }\n\n  /// Returns true if the specified index corresponds to a spill slot.\n  bool isSpillSlotObjectIndex(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].isSpillSlot;\n  }\n\n  bool isStatepointSpillSlotObjectIndex(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].isStatepointSpillSlot;\n  }\n\n  /// \\see StackID\n  uint8_t getStackID(int ObjectIdx) const {\n    return Objects[ObjectIdx+NumFixedObjects].StackID;\n  }\n\n  /// \\see StackID\n  void setStackID(int ObjectIdx, uint8_t ID) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    Objects[ObjectIdx+NumFixedObjects].StackID = ID;\n    // If ID > 0, MaxAlignment may now be overly conservative.\n    // If ID == 0, MaxAlignment will need to be updated separately.\n  }\n\n  /// Returns true if the specified index corresponds to a dead object.\n  bool isDeadObjectIndex(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx+NumFixedObjects].Size == ~0ULL;\n  }\n\n  /// Returns true if the specified index corresponds to a variable sized\n  /// object.\n  bool isVariableSizedObjectIndex(int ObjectIdx) const {\n    assert(unsigned(ObjectIdx + NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    return Objects[ObjectIdx + NumFixedObjects].Size == 0;\n  }\n\n  void markAsStatepointSpillSlotObjectIndex(int ObjectIdx) {\n    assert(unsigned(ObjectIdx+NumFixedObjects) < Objects.size() &&\n           \"Invalid Object Idx!\");\n    Objects[ObjectIdx+NumFixedObjects].isStatepointSpillSlot = true;\n    assert(isStatepointSpillSlotObjectIndex(ObjectIdx) && \"inconsistent\");\n  }\n\n  /// Create a new statically sized stack object, returning\n  /// a nonnegative identifier to represent it.\n  int CreateStackObject(uint64_t Size, Align Alignment, bool isSpillSlot,\n                        const AllocaInst *Alloca = nullptr, uint8_t ID = 0);\n  LLVM_ATTRIBUTE_DEPRECATED(\n      inline int CreateStackObject(uint64_t Size, unsigned Alignment,\n                                   bool isSpillSlot,\n                                   const AllocaInst *Alloca = nullptr,\n                                   uint8_t ID = 0),\n      \"Use CreateStackObject that takes an Align instead\") {\n    return CreateStackObject(Size, assumeAligned(Alignment), isSpillSlot,\n                             Alloca, ID);\n  }\n\n  /// Create a new statically sized stack object that represents a spill slot,\n  /// returning a nonnegative identifier to represent it.\n  int CreateSpillStackObject(uint64_t Size, Align Alignment);\n  LLVM_ATTRIBUTE_DEPRECATED(\n      inline int CreateSpillStackObject(uint64_t Size, unsigned Alignment),\n      \"Use CreateSpillStackObject that takes an Align instead\") {\n    return CreateSpillStackObject(Size, assumeAligned(Alignment));\n  }\n\n  /// Remove or mark dead a statically sized stack object.\n  void RemoveStackObject(int ObjectIdx) {\n    // Mark it dead.\n    Objects[ObjectIdx+NumFixedObjects].Size = ~0ULL;\n  }\n\n  /// Notify the MachineFrameInfo object that a variable sized object has been\n  /// created.  This must be created whenever a variable sized object is\n  /// created, whether or not the index returned is actually used.\n  int CreateVariableSizedObject(Align Alignment, const AllocaInst *Alloca);\n  /// FIXME: Remove this function when transition to Align is over.\n  LLVM_ATTRIBUTE_DEPRECATED(int CreateVariableSizedObject(\n                                unsigned Alignment, const AllocaInst *Alloca),\n                            \"Use the version that takes an Align instead\") {\n    return CreateVariableSizedObject(assumeAligned(Alignment), Alloca);\n  }\n\n  /// Returns a reference to call saved info vector for the current function.\n  const std::vector<CalleeSavedInfo> &getCalleeSavedInfo() const {\n    return CSInfo;\n  }\n  /// \\copydoc getCalleeSavedInfo()\n  std::vector<CalleeSavedInfo> &getCalleeSavedInfo() { return CSInfo; }\n\n  /// Used by prolog/epilog inserter to set the function's callee saved\n  /// information.\n  void setCalleeSavedInfo(std::vector<CalleeSavedInfo> CSI) {\n    CSInfo = std::move(CSI);\n  }\n\n  /// Has the callee saved info been calculated yet?\n  bool isCalleeSavedInfoValid() const { return CSIValid; }\n\n  void setCalleeSavedInfoValid(bool v) { CSIValid = v; }\n\n  MachineBasicBlock *getSavePoint() const { return Save; }\n  void setSavePoint(MachineBasicBlock *NewSave) { Save = NewSave; }\n  MachineBasicBlock *getRestorePoint() const { return Restore; }\n  void setRestorePoint(MachineBasicBlock *NewRestore) { Restore = NewRestore; }\n\n  /// Return a set of physical registers that are pristine.\n  ///\n  /// Pristine registers hold a value that is useless to the current function,\n  /// but that must be preserved - they are callee saved registers that are not\n  /// saved.\n  ///\n  /// Before the PrologueEpilogueInserter has placed the CSR spill code, this\n  /// method always returns an empty set.\n  BitVector getPristineRegs(const MachineFunction &MF) const;\n\n  /// Used by the MachineFunction printer to print information about\n  /// stack objects. Implemented in MachineFunction.cpp.\n  void print(const MachineFunction &MF, raw_ostream &OS) const;\n\n  /// dump - Print the function to stderr.\n  void dump(const MachineFunction &MF) const;\n};\n\n} // End llvm namespace\n\n#endif\n"}, "45": {"id": 45, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineInstrBundle.h", "content": "//===- llvm/CodeGen/MachineInstrBundle.h - MI bundle utilities --*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file provide utility functions to manipulate machine instruction\n// bundles.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_MACHINEINSTRBUNDLE_H\n#define LLVM_CODEGEN_MACHINEINSTRBUNDLE_H\n\n#include \"llvm/CodeGen/MachineBasicBlock.h\"\n\nnamespace llvm {\n\n/// finalizeBundle - Finalize a machine instruction bundle which includes\n/// a sequence of instructions starting from FirstMI to LastMI (exclusive).\n/// This routine adds a BUNDLE instruction to represent the bundle, it adds\n/// IsInternalRead markers to MachineOperands which are defined inside the\n/// bundle, and it copies externally visible defs and uses to the BUNDLE\n/// instruction.\nvoid finalizeBundle(MachineBasicBlock &MBB,\n                    MachineBasicBlock::instr_iterator FirstMI,\n                    MachineBasicBlock::instr_iterator LastMI);\n\n/// finalizeBundle - Same functionality as the previous finalizeBundle except\n/// the last instruction in the bundle is not provided as an input. This is\n/// used in cases where bundles are pre-determined by marking instructions\n/// with 'InsideBundle' marker. It returns the MBB instruction iterator that\n/// points to the end of the bundle.\nMachineBasicBlock::instr_iterator finalizeBundle(MachineBasicBlock &MBB,\n                    MachineBasicBlock::instr_iterator FirstMI);\n\n/// finalizeBundles - Finalize instruction bundles in the specified\n/// MachineFunction. Return true if any bundles are finalized.\nbool finalizeBundles(MachineFunction &MF);\n\n/// Returns an iterator to the first instruction in the bundle containing \\p I.\ninline MachineBasicBlock::instr_iterator getBundleStart(\n    MachineBasicBlock::instr_iterator I) {\n  while (I->isBundledWithPred())\n    --I;\n  return I;\n}\n\n/// Returns an iterator to the first instruction in the bundle containing \\p I.\ninline MachineBasicBlock::const_instr_iterator getBundleStart(\n    MachineBasicBlock::const_instr_iterator I) {\n  while (I->isBundledWithPred())\n    --I;\n  return I;\n}\n\n/// Returns an iterator pointing beyond the bundle containing \\p I.\ninline MachineBasicBlock::instr_iterator getBundleEnd(\n    MachineBasicBlock::instr_iterator I) {\n  while (I->isBundledWithSucc())\n    ++I;\n  ++I;\n  return I;\n}\n\n/// Returns an iterator pointing beyond the bundle containing \\p I.\ninline MachineBasicBlock::const_instr_iterator getBundleEnd(\n    MachineBasicBlock::const_instr_iterator I) {\n  while (I->isBundledWithSucc())\n    ++I;\n  ++I;\n  return I;\n}\n\n//===----------------------------------------------------------------------===//\n// MachineBundleOperand iterator\n//\n\n/// MIBundleOperandIteratorBase - Iterator that visits all operands in a bundle\n/// of MachineInstrs. This class is not intended to be used directly, use one\n/// of the sub-classes instead.\n///\n/// Intended use:\n///\n///   for (MIBundleOperands MIO(MI); MIO.isValid(); ++MIO) {\n///     if (!MIO->isReg())\n///       continue;\n///     ...\n///   }\n///\ntemplate <typename ValueT>\nclass MIBundleOperandIteratorBase\n    : public iterator_facade_base<MIBundleOperandIteratorBase<ValueT>,\n                                  std::forward_iterator_tag, ValueT> {\n  MachineBasicBlock::instr_iterator InstrI, InstrE;\n  MachineInstr::mop_iterator OpI, OpE;\n\n  // If the operands on InstrI are exhausted, advance InstrI to the next\n  // bundled instruction with operands.\n  void advance() {\n    while (OpI == OpE) {\n      // Don't advance off the basic block, or into a new bundle.\n      if (++InstrI == InstrE || !InstrI->isInsideBundle()) {\n        InstrI = InstrE;\n        break;\n      }\n      OpI = InstrI->operands_begin();\n      OpE = InstrI->operands_end();\n    }\n  }\n\nprotected:\n  /// MIBundleOperandIteratorBase - Create an iterator that visits all operands\n  /// on MI, or all operands on every instruction in the bundle containing MI.\n  ///\n  /// @param MI The instruction to examine.\n  ///\n  explicit MIBundleOperandIteratorBase(MachineInstr &MI) {\n    InstrI = getBundleStart(MI.getIterator());\n    InstrE = MI.getParent()->instr_end();\n    OpI = InstrI->operands_begin();\n    OpE = InstrI->operands_end();\n    advance();\n  }\n\n  /// Constructor for an iterator past the last iteration: both instruction\n  /// iterators point to the end of the BB and OpI == OpE.\n  explicit MIBundleOperandIteratorBase(MachineBasicBlock::instr_iterator InstrE,\n                                       MachineInstr::mop_iterator OpE)\n      : InstrI(InstrE), InstrE(InstrE), OpI(OpE), OpE(OpE) {}\n\npublic:\n  /// isValid - Returns true until all the operands have been visited.\n  bool isValid() const { return OpI != OpE; }\n\n  /// Preincrement.  Move to the next operand.\n  void operator++() {\n    assert(isValid() && \"Cannot advance MIOperands beyond the last operand\");\n    ++OpI;\n    advance();\n  }\n\n  ValueT &operator*() const { return *OpI; }\n  ValueT *operator->() const { return &*OpI; }\n\n  bool operator==(const MIBundleOperandIteratorBase &Arg) const {\n    // Iterators are equal, if InstrI matches and either OpIs match or OpI ==\n    // OpE match for both. The second condition allows us to construct an 'end'\n    // iterator, without finding the last instruction in a bundle up-front.\n    return InstrI == Arg.InstrI &&\n           (OpI == Arg.OpI || (OpI == OpE && Arg.OpI == Arg.OpE));\n  }\n  /// getOperandNo - Returns the number of the current operand relative to its\n  /// instruction.\n  ///\n  unsigned getOperandNo() const {\n    return OpI - InstrI->operands_begin();\n  }\n};\n\n/// MIBundleOperands - Iterate over all operands in a bundle of machine\n/// instructions.\n///\nclass MIBundleOperands : public MIBundleOperandIteratorBase<MachineOperand> {\n  /// Constructor for an iterator past the last iteration.\n  MIBundleOperands(MachineBasicBlock::instr_iterator InstrE,\n                   MachineInstr::mop_iterator OpE)\n      : MIBundleOperandIteratorBase(InstrE, OpE) {}\n\npublic:\n  MIBundleOperands(MachineInstr &MI) : MIBundleOperandIteratorBase(MI) {}\n\n  /// Returns an iterator past the last iteration.\n  static MIBundleOperands end(const MachineBasicBlock &MBB) {\n    return {const_cast<MachineBasicBlock &>(MBB).instr_end(),\n            const_cast<MachineBasicBlock &>(MBB).instr_begin()->operands_end()};\n  }\n};\n\n/// ConstMIBundleOperands - Iterate over all operands in a const bundle of\n/// machine instructions.\n///\nclass ConstMIBundleOperands\n    : public MIBundleOperandIteratorBase<const MachineOperand> {\n\n  /// Constructor for an iterator past the last iteration.\n  ConstMIBundleOperands(MachineBasicBlock::instr_iterator InstrE,\n                        MachineInstr::mop_iterator OpE)\n      : MIBundleOperandIteratorBase(InstrE, OpE) {}\n\npublic:\n  ConstMIBundleOperands(const MachineInstr &MI)\n      : MIBundleOperandIteratorBase(const_cast<MachineInstr &>(MI)) {}\n\n  /// Returns an iterator past the last iteration.\n  static ConstMIBundleOperands end(const MachineBasicBlock &MBB) {\n    return {const_cast<MachineBasicBlock &>(MBB).instr_end(),\n            const_cast<MachineBasicBlock &>(MBB).instr_begin()->operands_end()};\n  }\n};\n\ninline iterator_range<ConstMIBundleOperands>\nconst_mi_bundle_ops(const MachineInstr &MI) {\n  return make_range(ConstMIBundleOperands(MI),\n                    ConstMIBundleOperands::end(*MI.getParent()));\n}\n\ninline iterator_range<MIBundleOperands> mi_bundle_ops(MachineInstr &MI) {\n  return make_range(MIBundleOperands(MI),\n                    MIBundleOperands::end(*MI.getParent()));\n}\n\n/// VirtRegInfo - Information about a virtual register used by a set of\n/// operands.\n///\nstruct VirtRegInfo {\n  /// Reads - One of the operands read the virtual register.  This does not\n  /// include undef or internal use operands, see MO::readsReg().\n  bool Reads;\n\n  /// Writes - One of the operands writes the virtual register.\n  bool Writes;\n\n  /// Tied - Uses and defs must use the same register. This can be because of\n  /// a two-address constraint, or there may be a partial redefinition of a\n  /// sub-register.\n  bool Tied;\n};\n\n/// AnalyzeVirtRegInBundle - Analyze how the current instruction or bundle uses\n/// a virtual register.  This function should not be called after operator++(),\n/// it expects a fresh iterator.\n///\n/// @param Reg The virtual register to analyze.\n/// @param Ops When set, this vector will receive an (MI, OpNum) entry for\n///            each operand referring to Reg.\n/// @returns A filled-in RegInfo struct.\nVirtRegInfo AnalyzeVirtRegInBundle(\n    MachineInstr &MI, Register Reg,\n    SmallVectorImpl<std::pair<MachineInstr *, unsigned>> *Ops = nullptr);\n\n/// Information about how a physical register Reg is used by a set of\n/// operands.\nstruct PhysRegInfo {\n  /// There is a regmask operand indicating Reg is clobbered.\n  /// \\see MachineOperand::CreateRegMask().\n  bool Clobbered;\n\n  /// Reg or one of its aliases is defined. The definition may only cover\n  /// parts of the register.\n  bool Defined;\n  /// Reg or a super-register is defined. The definition covers the full\n  /// register.\n  bool FullyDefined;\n\n  /// Reg or one of its aliases is read. The register may only be read\n  /// partially.\n  bool Read;\n  /// Reg or a super-register is read. The full register is read.\n  bool FullyRead;\n\n  /// Either:\n  /// - Reg is FullyDefined and all defs of reg or an overlapping\n  ///   register are dead, or\n  /// - Reg is completely dead because \"defined\" by a clobber.\n  bool DeadDef;\n\n  /// Reg is Defined and all defs of reg or an overlapping register are\n  /// dead.\n  bool PartialDeadDef;\n\n  /// There is a use operand of reg or a super-register with kill flag set.\n  bool Killed;\n};\n\n/// AnalyzePhysRegInBundle - Analyze how the current instruction or bundle uses\n/// a physical register.  This function should not be called after operator++(),\n/// it expects a fresh iterator.\n///\n/// @param Reg The physical register to analyze.\n/// @returns A filled-in PhysRegInfo struct.\nPhysRegInfo AnalyzePhysRegInBundle(const MachineInstr &MI, Register Reg,\n                                   const TargetRegisterInfo *TRI);\n\n} // End llvm namespace\n\n#endif\n"}, "47": {"id": 47, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "content": "//===-- llvm/CodeGen/MachineModuleInfo.h ------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// Collect meta information for a module.  This information should be in a\n// neutral form that can be used by different debugging and exception handling\n// schemes.\n//\n// The organization of information is primarily clustered around the source\n// compile units.  The main exception is source line correspondence where\n// inlining may interleave code from various compile units.\n//\n// The following information can be retrieved from the MachineModuleInfo.\n//\n//  -- Source directories - Directories are uniqued based on their canonical\n//     string and assigned a sequential numeric ID (base 1.)\n//  -- Source files - Files are also uniqued based on their name and directory\n//     ID.  A file ID is sequential number (base 1.)\n//  -- Source line correspondence - A vector of file ID, line#, column# triples.\n//     A DEBUG_LOCATION instruction is generated  by the DAG Legalizer\n//     corresponding to each entry in the source line list.  This allows a debug\n//     emitter to generate labels referenced by debug information tables.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_MACHINEMODULEINFO_H\n#define LLVM_CODEGEN_MACHINEMODULEINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/PointerIntPair.h\"\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/MC/MCContext.h\"\n#include \"llvm/MC/MCSymbol.h\"\n#include \"llvm/Pass.h\"\n#include <memory>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass BasicBlock;\nclass CallInst;\nclass Function;\nclass LLVMTargetMachine;\nclass MMIAddrLabelMap;\nclass MachineFunction;\nclass Module;\n\n//===----------------------------------------------------------------------===//\n/// This class can be derived from and used by targets to hold private\n/// target-specific information for each Module.  Objects of type are\n/// accessed/created with MachineModuleInfo::getObjFileInfo and destroyed when\n/// the MachineModuleInfo is destroyed.\n///\nclass MachineModuleInfoImpl {\npublic:\n  using StubValueTy = PointerIntPair<MCSymbol *, 1, bool>;\n  using SymbolListTy = std::vector<std::pair<MCSymbol *, StubValueTy>>;\n\n  virtual ~MachineModuleInfoImpl();\n\nprotected:\n  /// Return the entries from a DenseMap in a deterministic sorted orer.\n  /// Clears the map.\n  static SymbolListTy getSortedStubs(DenseMap<MCSymbol*, StubValueTy>&);\n};\n\n//===----------------------------------------------------------------------===//\n/// This class contains meta information specific to a module.  Queries can be\n/// made by different debugging and exception handling schemes and reformated\n/// for specific use.\n///\nclass MachineModuleInfo {\n  friend class MachineModuleInfoWrapperPass;\n  friend class MachineModuleAnalysis;\n\n  const LLVMTargetMachine &TM;\n\n  /// This is the MCContext used for the entire code generator.\n  MCContext Context;\n  // This is an external context, that if assigned, will be used instead of the\n  // internal context.\n  MCContext *ExternalContext = nullptr;\n\n  /// This is the LLVM Module being worked on.\n  const Module *TheModule;\n\n  /// This is the object-file-format-specific implementation of\n  /// MachineModuleInfoImpl, which lets targets accumulate whatever info they\n  /// want.\n  MachineModuleInfoImpl *ObjFileMMI;\n\n  /// \\name Exception Handling\n  /// \\{\n\n  /// Vector of all personality functions ever seen. Used to emit common EH\n  /// frames.\n  std::vector<const Function *> Personalities;\n\n  /// The current call site index being processed, if any. 0 if none.\n  unsigned CurCallSite;\n\n  /// \\}\n\n  /// This map keeps track of which symbol is being used for the specified\n  /// basic block's address of label.\n  MMIAddrLabelMap *AddrLabelSymbols;\n\n  // TODO: Ideally, what we'd like is to have a switch that allows emitting\n  // synchronous (precise at call-sites only) CFA into .eh_frame. However,\n  // even under this switch, we'd like .debug_frame to be precise when using\n  // -g. At this moment, there's no way to specify that some CFI directives\n  // go into .eh_frame only, while others go into .debug_frame only.\n\n  /// True if debugging information is available in this module.\n  bool DbgInfoAvailable;\n\n  /// True if this module is being built for windows/msvc, and uses floating\n  /// point.  This is used to emit an undefined reference to _fltused.\n  bool UsesMSVCFloatingPoint;\n\n  /// True if the module calls the __morestack function indirectly, as is\n  /// required under the large code model on x86. This is used to emit\n  /// a definition of a symbol, __morestack_addr, containing the address. See\n  /// comments in lib/Target/X86/X86FrameLowering.cpp for more details.\n  bool UsesMorestackAddr;\n\n  /// True if the module contains split-stack functions. This is used to\n  /// emit .note.GNU-split-stack section as required by the linker for\n  /// special handling split-stack function calling no-split-stack function.\n  bool HasSplitStack;\n\n  /// True if the module contains no-split-stack functions. This is used to\n  /// emit .note.GNU-no-split-stack section when it also contains split-stack\n  /// functions.\n  bool HasNosplitStack;\n\n  /// Maps IR Functions to their corresponding MachineFunctions.\n  DenseMap<const Function*, std::unique_ptr<MachineFunction>> MachineFunctions;\n  /// Next unique number available for a MachineFunction.\n  unsigned NextFnNum = 0;\n  const Function *LastRequest = nullptr; ///< Used for shortcut/cache.\n  MachineFunction *LastResult = nullptr; ///< Used for shortcut/cache.\n\n  MachineModuleInfo &operator=(MachineModuleInfo &&MMII) = delete;\n\npublic:\n  explicit MachineModuleInfo(const LLVMTargetMachine *TM = nullptr);\n\n  explicit MachineModuleInfo(const LLVMTargetMachine *TM,\n                             MCContext *ExtContext);\n\n  MachineModuleInfo(MachineModuleInfo &&MMII);\n\n  ~MachineModuleInfo();\n\n  void initialize();\n  void finalize();\n\n  const LLVMTargetMachine &getTarget() const { return TM; }\n\n  const MCContext &getContext() const {\n    return ExternalContext ? *ExternalContext : Context;\n  }\n  MCContext &getContext() {\n    return ExternalContext ? *ExternalContext : Context;\n  }\n\n  const Module *getModule() const { return TheModule; }\n\n  /// Returns the MachineFunction constructed for the IR function \\p F.\n  /// Creates a new MachineFunction if none exists yet.\n  MachineFunction &getOrCreateMachineFunction(Function &F);\n\n  /// \\brief Returns the MachineFunction associated to IR function \\p F if there\n  /// is one, otherwise nullptr.\n  MachineFunction *getMachineFunction(const Function &F) const;\n\n  /// Delete the MachineFunction \\p MF and reset the link in the IR Function to\n  /// Machine Function map.\n  void deleteMachineFunctionFor(Function &F);\n\n  /// Keep track of various per-function pieces of information for backends\n  /// that would like to do so.\n  template<typename Ty>\n  Ty &getObjFileInfo() {\n    if (ObjFileMMI == nullptr)\n      ObjFileMMI = new Ty(*this);\n    return *static_cast<Ty*>(ObjFileMMI);\n  }\n\n  template<typename Ty>\n  const Ty &getObjFileInfo() const {\n    return const_cast<MachineModuleInfo*>(this)->getObjFileInfo<Ty>();\n  }\n\n  /// Returns true if valid debug info is present.\n  bool hasDebugInfo() const { return DbgInfoAvailable; }\n  void setDebugInfoAvailability(bool avail) { DbgInfoAvailable = avail; }\n\n  bool usesMSVCFloatingPoint() const { return UsesMSVCFloatingPoint; }\n\n  void setUsesMSVCFloatingPoint(bool b) { UsesMSVCFloatingPoint = b; }\n\n  bool usesMorestackAddr() const {\n    return UsesMorestackAddr;\n  }\n\n  void setUsesMorestackAddr(bool b) {\n    UsesMorestackAddr = b;\n  }\n\n  bool hasSplitStack() const {\n    return HasSplitStack;\n  }\n\n  void setHasSplitStack(bool b) {\n    HasSplitStack = b;\n  }\n\n  bool hasNosplitStack() const {\n    return HasNosplitStack;\n  }\n\n  void setHasNosplitStack(bool b) {\n    HasNosplitStack = b;\n  }\n\n  /// Return the symbol to be used for the specified basic block when its\n  /// address is taken.  This cannot be its normal LBB label because the block\n  /// may be accessed outside its containing function.\n  MCSymbol *getAddrLabelSymbol(const BasicBlock *BB) {\n    return getAddrLabelSymbolToEmit(BB).front();\n  }\n\n  /// Return the symbol to be used for the specified basic block when its\n  /// address is taken.  If other blocks were RAUW'd to this one, we may have\n  /// to emit them as well, return the whole set.\n  ArrayRef<MCSymbol *> getAddrLabelSymbolToEmit(const BasicBlock *BB);\n\n  /// \\name Exception Handling\n  /// \\{\n\n  /// Set the call site currently being processed.\n  void setCurrentCallSite(unsigned Site) { CurCallSite = Site; }\n\n  /// Get the call site currently being processed, if any.  return zero if\n  /// none.\n  unsigned getCurrentCallSite() { return CurCallSite; }\n\n  /// Provide the personality function for the exception information.\n  void addPersonality(const Function *Personality);\n\n  /// Return array of personality functions ever seen.\n  const std::vector<const Function *>& getPersonalities() const {\n    return Personalities;\n  }\n  /// \\}\n\n  // MMI owes MCContext. It should never be invalidated.\n  bool invalidate(Module &, const PreservedAnalyses &,\n                  ModuleAnalysisManager::Invalidator &) {\n    return false;\n  }\n}; // End class MachineModuleInfo\n\nclass MachineModuleInfoWrapperPass : public ImmutablePass {\n  MachineModuleInfo MMI;\n\npublic:\n  static char ID; // Pass identification, replacement for typeid\n  explicit MachineModuleInfoWrapperPass(const LLVMTargetMachine *TM = nullptr);\n\n  explicit MachineModuleInfoWrapperPass(const LLVMTargetMachine *TM,\n                                        MCContext *ExtContext);\n\n  // Initialization and Finalization\n  bool doInitialization(Module &) override;\n  bool doFinalization(Module &) override;\n\n  MachineModuleInfo &getMMI() { return MMI; }\n  const MachineModuleInfo &getMMI() const { return MMI; }\n};\n\n/// An analysis that produces \\c MachineInfo for a module.\nclass MachineModuleAnalysis : public AnalysisInfoMixin<MachineModuleAnalysis> {\n  friend AnalysisInfoMixin<MachineModuleAnalysis>;\n  static AnalysisKey Key;\n\n  const LLVMTargetMachine *TM;\n\npublic:\n  /// Provide the result type for this analysis pass.\n  using Result = MachineModuleInfo;\n\n  MachineModuleAnalysis(const LLVMTargetMachine *TM) : TM(TM) {}\n\n  /// Run the analysis pass and produce machine module information.\n  MachineModuleInfo run(Module &M, ModuleAnalysisManager &);\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_MACHINEMODULEINFO_H\n"}, "50": {"id": 50, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/RegisterClassInfo.h", "content": "//===- RegisterClassInfo.h - Dynamic Register Class Info --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file implements the RegisterClassInfo class which provides dynamic\n// information about target register classes. Callee saved and reserved\n// registers depends on calling conventions and other dynamic information, so\n// some things cannot be determined statically.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_REGISTERCLASSINFO_H\n#define LLVM_CODEGEN_REGISTERCLASSINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/BitVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/CodeGen/TargetRegisterInfo.h\"\n#include \"llvm/MC/MCRegisterInfo.h\"\n#include <cassert>\n#include <cstdint>\n#include <memory>\n\nnamespace llvm {\n\nclass RegisterClassInfo {\n  struct RCInfo {\n    unsigned Tag = 0;\n    unsigned NumRegs = 0;\n    bool ProperSubClass = false;\n    uint8_t MinCost = 0;\n    uint16_t LastCostChange = 0;\n    std::unique_ptr<MCPhysReg[]> Order;\n\n    RCInfo() = default;\n\n    operator ArrayRef<MCPhysReg>() const {\n      return makeArrayRef(Order.get(), NumRegs);\n    }\n  };\n\n  // Brief cached information for each register class.\n  std::unique_ptr<RCInfo[]> RegClass;\n\n  // Tag changes whenever cached information needs to be recomputed. An RCInfo\n  // entry is valid when its tag matches.\n  unsigned Tag = 0;\n\n  const MachineFunction *MF = nullptr;\n  const TargetRegisterInfo *TRI = nullptr;\n\n  // Callee saved registers of last MF. Assumed to be valid until the next\n  // runOnFunction() call.\n  // Used only to determine if an update was made to CalleeSavedAliases.\n  const MCPhysReg *CalleeSavedRegs = nullptr;\n\n  // Map register alias to the callee saved Register.\n  SmallVector<MCPhysReg, 4> CalleeSavedAliases;\n\n  // Reserved registers in the current MF.\n  BitVector Reserved;\n\n  std::unique_ptr<unsigned[]> PSetLimits;\n\n  // The register cost values.\n  ArrayRef<uint8_t> RegCosts;\n\n  // Compute all information about RC.\n  void compute(const TargetRegisterClass *RC) const;\n\n  // Return an up-to-date RCInfo for RC.\n  const RCInfo &get(const TargetRegisterClass *RC) const {\n    const RCInfo &RCI = RegClass[RC->getID()];\n    if (Tag != RCI.Tag)\n      compute(RC);\n    return RCI;\n  }\n\npublic:\n  RegisterClassInfo();\n\n  /// runOnFunction - Prepare to answer questions about MF. This must be called\n  /// before any other methods are used.\n  void runOnMachineFunction(const MachineFunction &MF);\n\n  /// getNumAllocatableRegs - Returns the number of actually allocatable\n  /// registers in RC in the current function.\n  unsigned getNumAllocatableRegs(const TargetRegisterClass *RC) const {\n    return get(RC).NumRegs;\n  }\n\n  /// getOrder - Returns the preferred allocation order for RC. The order\n  /// contains no reserved registers, and registers that alias callee saved\n  /// registers come last.\n  ArrayRef<MCPhysReg> getOrder(const TargetRegisterClass *RC) const {\n    return get(RC);\n  }\n\n  /// isProperSubClass - Returns true if RC has a legal super-class with more\n  /// allocatable registers.\n  ///\n  /// Register classes like GR32_NOSP are not proper sub-classes because %esp\n  /// is not allocatable.  Similarly, tGPR is not a proper sub-class in Thumb\n  /// mode because the GPR super-class is not legal.\n  bool isProperSubClass(const TargetRegisterClass *RC) const {\n    return get(RC).ProperSubClass;\n  }\n\n  /// getLastCalleeSavedAlias - Returns the last callee saved register that\n  /// overlaps PhysReg, or NoRegister if Reg doesn't overlap a\n  /// CalleeSavedAliases.\n  MCRegister getLastCalleeSavedAlias(MCRegister PhysReg) const {\n    if (PhysReg.id() < CalleeSavedAliases.size())\n      return CalleeSavedAliases[PhysReg];\n    return MCRegister::NoRegister;\n  }\n\n  /// Get the minimum register cost in RC's allocation order.\n  /// This is the smallest value in RegCosts[Reg] for all\n  /// the registers in getOrder(RC).\n  uint8_t getMinCost(const TargetRegisterClass *RC) const {\n    return get(RC).MinCost;\n  }\n\n  /// Get the position of the last cost change in getOrder(RC).\n  ///\n  /// All registers in getOrder(RC).slice(getLastCostChange(RC)) will have the\n  /// same cost according to RegCosts[Reg].\n  unsigned getLastCostChange(const TargetRegisterClass *RC) const {\n    return get(RC).LastCostChange;\n  }\n\n  /// Get the register unit limit for the given pressure set index.\n  ///\n  /// RegisterClassInfo adjusts this limit for reserved registers.\n  unsigned getRegPressureSetLimit(unsigned Idx) const {\n    if (!PSetLimits[Idx])\n      PSetLimits[Idx] = computePSetLimit(Idx);\n    return PSetLimits[Idx];\n  }\n\nprotected:\n  unsigned computePSetLimit(unsigned Idx) const;\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_REGISTERCLASSINFO_H\n"}, "51": {"id": 51, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAG.h", "content": "//===- llvm/CodeGen/SelectionDAG.h - InstSelection DAG ----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file declares the SelectionDAG class, and transitively defines the\n// SDNode class and subclasses.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_SELECTIONDAG_H\n#define LLVM_CODEGEN_SELECTIONDAG_H\n\n#include \"llvm/ADT/APFloat.h\"\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DenseSet.h\"\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/ADT/SetVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringMap.h\"\n#include \"llvm/ADT/ilist.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/CodeGen/DAGCombine.h\"\n#include \"llvm/CodeGen/ISDOpcodes.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineMemOperand.h\"\n#include \"llvm/CodeGen/SelectionDAGNodes.h\"\n#include \"llvm/CodeGen/ValueTypes.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/Metadata.h\"\n#include \"llvm/Support/Allocator.h\"\n#include \"llvm/Support/ArrayRecycler.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/CodeGen.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include \"llvm/Support/RecyclingAllocator.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstdint>\n#include <functional>\n#include <map>\n#include <string>\n#include <tuple>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass AAResults;\nclass BlockAddress;\nclass BlockFrequencyInfo;\nclass Constant;\nclass ConstantFP;\nclass ConstantInt;\nclass DataLayout;\nstruct fltSemantics;\nclass FunctionLoweringInfo;\nclass GlobalValue;\nstruct KnownBits;\nclass LegacyDivergenceAnalysis;\nclass LLVMContext;\nclass MachineBasicBlock;\nclass MachineConstantPoolValue;\nclass MCSymbol;\nclass OptimizationRemarkEmitter;\nclass ProfileSummaryInfo;\nclass SDDbgValue;\nclass SDDbgOperand;\nclass SDDbgLabel;\nclass SelectionDAG;\nclass SelectionDAGTargetInfo;\nclass TargetLibraryInfo;\nclass TargetLowering;\nclass TargetMachine;\nclass TargetSubtargetInfo;\nclass Value;\n\nclass SDVTListNode : public FoldingSetNode {\n  friend struct FoldingSetTrait<SDVTListNode>;\n\n  /// A reference to an Interned FoldingSetNodeID for this node.\n  /// The Allocator in SelectionDAG holds the data.\n  /// SDVTList contains all types which are frequently accessed in SelectionDAG.\n  /// The size of this list is not expected to be big so it won't introduce\n  /// a memory penalty.\n  FoldingSetNodeIDRef FastID;\n  const EVT *VTs;\n  unsigned int NumVTs;\n  /// The hash value for SDVTList is fixed, so cache it to avoid\n  /// hash calculation.\n  unsigned HashValue;\n\npublic:\n  SDVTListNode(const FoldingSetNodeIDRef ID, const EVT *VT, unsigned int Num) :\n      FastID(ID), VTs(VT), NumVTs(Num) {\n    HashValue = ID.ComputeHash();\n  }\n\n  SDVTList getSDVTList() {\n    SDVTList result = {VTs, NumVTs};\n    return result;\n  }\n};\n\n/// Specialize FoldingSetTrait for SDVTListNode\n/// to avoid computing temp FoldingSetNodeID and hash value.\ntemplate<> struct FoldingSetTrait<SDVTListNode> : DefaultFoldingSetTrait<SDVTListNode> {\n  static void Profile(const SDVTListNode &X, FoldingSetNodeID& ID) {\n    ID = X.FastID;\n  }\n\n  static bool Equals(const SDVTListNode &X, const FoldingSetNodeID &ID,\n                     unsigned IDHash, FoldingSetNodeID &TempID) {\n    if (X.HashValue != IDHash)\n      return false;\n    return ID == X.FastID;\n  }\n\n  static unsigned ComputeHash(const SDVTListNode &X, FoldingSetNodeID &TempID) {\n    return X.HashValue;\n  }\n};\n\ntemplate <> struct ilist_alloc_traits<SDNode> {\n  static void deleteNode(SDNode *) {\n    llvm_unreachable(\"ilist_traits<SDNode> shouldn't see a deleteNode call!\");\n  }\n};\n\n/// Keeps track of dbg_value information through SDISel.  We do\n/// not build SDNodes for these so as not to perturb the generated code;\n/// instead the info is kept off to the side in this structure. Each SDNode may\n/// have one or more associated dbg_value entries. This information is kept in\n/// DbgValMap.\n/// Byval parameters are handled separately because they don't use alloca's,\n/// which busts the normal mechanism.  There is good reason for handling all\n/// parameters separately:  they may not have code generated for them, they\n/// should always go at the beginning of the function regardless of other code\n/// motion, and debug info for them is potentially useful even if the parameter\n/// is unused.  Right now only byval parameters are handled separately.\nclass SDDbgInfo {\n  BumpPtrAllocator Alloc;\n  SmallVector<SDDbgValue*, 32> DbgValues;\n  SmallVector<SDDbgValue*, 32> ByvalParmDbgValues;\n  SmallVector<SDDbgLabel*, 4> DbgLabels;\n  using DbgValMapType = DenseMap<const SDNode *, SmallVector<SDDbgValue *, 2>>;\n  DbgValMapType DbgValMap;\n\npublic:\n  SDDbgInfo() = default;\n  SDDbgInfo(const SDDbgInfo &) = delete;\n  SDDbgInfo &operator=(const SDDbgInfo &) = delete;\n\n  void add(SDDbgValue *V, bool isParameter);\n\n  void add(SDDbgLabel *L) { DbgLabels.push_back(L); }\n\n  /// Invalidate all DbgValues attached to the node and remove\n  /// it from the Node-to-DbgValues map.\n  void erase(const SDNode *Node);\n\n  void clear() {\n    DbgValMap.clear();\n    DbgValues.clear();\n    ByvalParmDbgValues.clear();\n    DbgLabels.clear();\n    Alloc.Reset();\n  }\n\n  BumpPtrAllocator &getAlloc() { return Alloc; }\n\n  bool empty() const {\n    return DbgValues.empty() && ByvalParmDbgValues.empty() && DbgLabels.empty();\n  }\n\n  ArrayRef<SDDbgValue*> getSDDbgValues(const SDNode *Node) const {\n    auto I = DbgValMap.find(Node);\n    if (I != DbgValMap.end())\n      return I->second;\n    return ArrayRef<SDDbgValue*>();\n  }\n\n  using DbgIterator = SmallVectorImpl<SDDbgValue*>::iterator;\n  using DbgLabelIterator = SmallVectorImpl<SDDbgLabel*>::iterator;\n\n  DbgIterator DbgBegin() { return DbgValues.begin(); }\n  DbgIterator DbgEnd()   { return DbgValues.end(); }\n  DbgIterator ByvalParmDbgBegin() { return ByvalParmDbgValues.begin(); }\n  DbgIterator ByvalParmDbgEnd()   { return ByvalParmDbgValues.end(); }\n  DbgLabelIterator DbgLabelBegin() { return DbgLabels.begin(); }\n  DbgLabelIterator DbgLabelEnd()   { return DbgLabels.end(); }\n};\n\nvoid checkForCycles(const SelectionDAG *DAG, bool force = false);\n\n/// This is used to represent a portion of an LLVM function in a low-level\n/// Data Dependence DAG representation suitable for instruction selection.\n/// This DAG is constructed as the first step of instruction selection in order\n/// to allow implementation of machine specific optimizations\n/// and code simplifications.\n///\n/// The representation used by the SelectionDAG is a target-independent\n/// representation, which has some similarities to the GCC RTL representation,\n/// but is significantly more simple, powerful, and is a graph form instead of a\n/// linear form.\n///\nclass SelectionDAG {\n  const TargetMachine &TM;\n  const SelectionDAGTargetInfo *TSI = nullptr;\n  const TargetLowering *TLI = nullptr;\n  const TargetLibraryInfo *LibInfo = nullptr;\n  MachineFunction *MF;\n  Pass *SDAGISelPass = nullptr;\n  LLVMContext *Context;\n  CodeGenOpt::Level OptLevel;\n\n  LegacyDivergenceAnalysis * DA = nullptr;\n  FunctionLoweringInfo * FLI = nullptr;\n\n  /// The function-level optimization remark emitter.  Used to emit remarks\n  /// whenever manipulating the DAG.\n  OptimizationRemarkEmitter *ORE;\n\n  ProfileSummaryInfo *PSI = nullptr;\n  BlockFrequencyInfo *BFI = nullptr;\n\n  /// The starting token.\n  SDNode EntryNode;\n\n  /// The root of the entire DAG.\n  SDValue Root;\n\n  /// A linked list of nodes in the current DAG.\n  ilist<SDNode> AllNodes;\n\n  /// The AllocatorType for allocating SDNodes. We use\n  /// pool allocation with recycling.\n  using NodeAllocatorType = RecyclingAllocator<BumpPtrAllocator, SDNode,\n                                               sizeof(LargestSDNode),\n                                               alignof(MostAlignedSDNode)>;\n\n  /// Pool allocation for nodes.\n  NodeAllocatorType NodeAllocator;\n\n  /// This structure is used to memoize nodes, automatically performing\n  /// CSE with existing nodes when a duplicate is requested.\n  FoldingSet<SDNode> CSEMap;\n\n  /// Pool allocation for machine-opcode SDNode operands.\n  BumpPtrAllocator OperandAllocator;\n  ArrayRecycler<SDUse> OperandRecycler;\n\n  /// Pool allocation for misc. objects that are created once per SelectionDAG.\n  BumpPtrAllocator Allocator;\n\n  /// Tracks dbg_value and dbg_label information through SDISel.\n  SDDbgInfo *DbgInfo;\n\n  using CallSiteInfo = MachineFunction::CallSiteInfo;\n  using CallSiteInfoImpl = MachineFunction::CallSiteInfoImpl;\n\n  struct CallSiteDbgInfo {\n    CallSiteInfo CSInfo;\n    MDNode *HeapAllocSite = nullptr;\n    bool NoMerge = false;\n  };\n\n  DenseMap<const SDNode *, CallSiteDbgInfo> SDCallSiteDbgInfo;\n\n  uint16_t NextPersistentId = 0;\n\npublic:\n  /// Clients of various APIs that cause global effects on\n  /// the DAG can optionally implement this interface.  This allows the clients\n  /// to handle the various sorts of updates that happen.\n  ///\n  /// A DAGUpdateListener automatically registers itself with DAG when it is\n  /// constructed, and removes itself when destroyed in RAII fashion.\n  struct DAGUpdateListener {\n    DAGUpdateListener *const Next;\n    SelectionDAG &DAG;\n\n    explicit DAGUpdateListener(SelectionDAG &D)\n      : Next(D.UpdateListeners), DAG(D) {\n      DAG.UpdateListeners = this;\n    }\n\n    virtual ~DAGUpdateListener() {\n      assert(DAG.UpdateListeners == this &&\n             \"DAGUpdateListeners must be destroyed in LIFO order\");\n      DAG.UpdateListeners = Next;\n    }\n\n    /// The node N that was deleted and, if E is not null, an\n    /// equivalent node E that replaced it.\n    virtual void NodeDeleted(SDNode *N, SDNode *E);\n\n    /// The node N that was updated.\n    virtual void NodeUpdated(SDNode *N);\n\n    /// The node N that was inserted.\n    virtual void NodeInserted(SDNode *N);\n  };\n\n  struct DAGNodeDeletedListener : public DAGUpdateListener {\n    std::function<void(SDNode *, SDNode *)> Callback;\n\n    DAGNodeDeletedListener(SelectionDAG &DAG,\n                           std::function<void(SDNode *, SDNode *)> Callback)\n        : DAGUpdateListener(DAG), Callback(std::move(Callback)) {}\n\n    void NodeDeleted(SDNode *N, SDNode *E) override { Callback(N, E); }\n\n   private:\n    virtual void anchor();\n  };\n\n  /// Help to insert SDNodeFlags automatically in transforming. Use\n  /// RAII to save and resume flags in current scope.\n  class FlagInserter {\n    SelectionDAG &DAG;\n    SDNodeFlags Flags;\n    FlagInserter *LastInserter;\n\n  public:\n    FlagInserter(SelectionDAG &SDAG, SDNodeFlags Flags)\n        : DAG(SDAG), Flags(Flags),\n          LastInserter(SDAG.getFlagInserter()) {\n      SDAG.setFlagInserter(this);\n    }\n    FlagInserter(SelectionDAG &SDAG, SDNode *N)\n        : FlagInserter(SDAG, N->getFlags()) {}\n\n    FlagInserter(const FlagInserter &) = delete;\n    FlagInserter &operator=(const FlagInserter &) = delete;\n    ~FlagInserter() { DAG.setFlagInserter(LastInserter); }\n\n    SDNodeFlags getFlags() const { return Flags; }\n  };\n\n  /// When true, additional steps are taken to\n  /// ensure that getConstant() and similar functions return DAG nodes that\n  /// have legal types. This is important after type legalization since\n  /// any illegally typed nodes generated after this point will not experience\n  /// type legalization.\n  bool NewNodesMustHaveLegalTypes = false;\n\nprivate:\n  /// DAGUpdateListener is a friend so it can manipulate the listener stack.\n  friend struct DAGUpdateListener;\n\n  /// Linked list of registered DAGUpdateListener instances.\n  /// This stack is maintained by DAGUpdateListener RAII.\n  DAGUpdateListener *UpdateListeners = nullptr;\n\n  /// Implementation of setSubgraphColor.\n  /// Return whether we had to truncate the search.\n  bool setSubgraphColorHelper(SDNode *N, const char *Color,\n                              DenseSet<SDNode *> &visited,\n                              int level, bool &printed);\n\n  template <typename SDNodeT, typename... ArgTypes>\n  SDNodeT *newSDNode(ArgTypes &&... Args) {\n    return new (NodeAllocator.template Allocate<SDNodeT>())\n        SDNodeT(std::forward<ArgTypes>(Args)...);\n  }\n\n  /// Build a synthetic SDNodeT with the given args and extract its subclass\n  /// data as an integer (e.g. for use in a folding set).\n  ///\n  /// The args to this function are the same as the args to SDNodeT's\n  /// constructor, except the second arg (assumed to be a const DebugLoc&) is\n  /// omitted.\n  template <typename SDNodeT, typename... ArgTypes>\n  static uint16_t getSyntheticNodeSubclassData(unsigned IROrder,\n                                               ArgTypes &&... Args) {\n    // The compiler can reduce this expression to a constant iff we pass an\n    // empty DebugLoc.  Thankfully, the debug location doesn't have any bearing\n    // on the subclass data.\n    return SDNodeT(IROrder, DebugLoc(), std::forward<ArgTypes>(Args)...)\n        .getRawSubclassData();\n  }\n\n  template <typename SDNodeTy>\n  static uint16_t getSyntheticNodeSubclassData(unsigned Opc, unsigned Order,\n                                                SDVTList VTs, EVT MemoryVT,\n                                                MachineMemOperand *MMO) {\n    return SDNodeTy(Opc, Order, DebugLoc(), VTs, MemoryVT, MMO)\n         .getRawSubclassData();\n  }\n\n  void createOperands(SDNode *Node, ArrayRef<SDValue> Vals);\n\n  void removeOperands(SDNode *Node) {\n    if (!Node->OperandList)\n      return;\n    OperandRecycler.deallocate(\n        ArrayRecycler<SDUse>::Capacity::get(Node->NumOperands),\n        Node->OperandList);\n    Node->NumOperands = 0;\n    Node->OperandList = nullptr;\n  }\n  void CreateTopologicalOrder(std::vector<SDNode*>& Order);\n\npublic:\n  // Maximum depth for recursive analysis such as computeKnownBits, etc.\n  static constexpr unsigned MaxRecursionDepth = 6;\n\n  explicit SelectionDAG(const TargetMachine &TM, CodeGenOpt::Level);\n  SelectionDAG(const SelectionDAG &) = delete;\n  SelectionDAG &operator=(const SelectionDAG &) = delete;\n  ~SelectionDAG();\n\n  /// Prepare this SelectionDAG to process code in the given MachineFunction.\n  void init(MachineFunction &NewMF, OptimizationRemarkEmitter &NewORE,\n            Pass *PassPtr, const TargetLibraryInfo *LibraryInfo,\n            LegacyDivergenceAnalysis * Divergence,\n            ProfileSummaryInfo *PSIin, BlockFrequencyInfo *BFIin);\n\n  void setFunctionLoweringInfo(FunctionLoweringInfo * FuncInfo) {\n    FLI = FuncInfo;\n  }\n\n  /// Clear state and free memory necessary to make this\n  /// SelectionDAG ready to process a new block.\n  void clear();\n\n  MachineFunction &getMachineFunction() const { return *MF; }\n  const Pass *getPass() const { return SDAGISelPass; }\n\n  const DataLayout &getDataLayout() const { return MF->getDataLayout(); }\n  const TargetMachine &getTarget() const { return TM; }\n  const TargetSubtargetInfo &getSubtarget() const { return MF->getSubtarget(); }\n  const TargetLowering &getTargetLoweringInfo() const { return *TLI; }\n  const TargetLibraryInfo &getLibInfo() const { return *LibInfo; }\n  const SelectionDAGTargetInfo &getSelectionDAGInfo() const { return *TSI; }\n  const LegacyDivergenceAnalysis *getDivergenceAnalysis() const { return DA; }\n  LLVMContext *getContext() const { return Context; }\n  OptimizationRemarkEmitter &getORE() const { return *ORE; }\n  ProfileSummaryInfo *getPSI() const { return PSI; }\n  BlockFrequencyInfo *getBFI() const { return BFI; }\n\n  FlagInserter *getFlagInserter() { return Inserter; }\n  void setFlagInserter(FlagInserter *FI) { Inserter = FI; }\n\n  /// Just dump dot graph to a user-provided path and title.\n  /// This doesn't open the dot viewer program and\n  /// helps visualization when outside debugging session.\n  /// FileName expects absolute path. If provided\n  /// without any path separators then the file\n  /// will be created in the current directory.\n  /// Error will be emitted if the path is insane.\n#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)\n  LLVM_DUMP_METHOD void dumpDotGraph(const Twine &FileName, const Twine &Title);\n#endif\n\n  /// Pop up a GraphViz/gv window with the DAG rendered using 'dot'.\n  void viewGraph(const std::string &Title);\n  void viewGraph();\n\n#ifndef NDEBUG\n  std::map<const SDNode *, std::string> NodeGraphAttrs;\n#endif\n\n  /// Clear all previously defined node graph attributes.\n  /// Intended to be used from a debugging tool (eg. gdb).\n  void clearGraphAttrs();\n\n  /// Set graph attributes for a node. (eg. \"color=red\".)\n  void setGraphAttrs(const SDNode *N, const char *Attrs);\n\n  /// Get graph attributes for a node. (eg. \"color=red\".)\n  /// Used from getNodeAttributes.\n  std::string getGraphAttrs(const SDNode *N) const;\n\n  /// Convenience for setting node color attribute.\n  void setGraphColor(const SDNode *N, const char *Color);\n\n  /// Convenience for setting subgraph color attribute.\n  void setSubgraphColor(SDNode *N, const char *Color);\n\n  using allnodes_const_iterator = ilist<SDNode>::const_iterator;\n\n  allnodes_const_iterator allnodes_begin() const { return AllNodes.begin(); }\n  allnodes_const_iterator allnodes_end() const { return AllNodes.end(); }\n\n  using allnodes_iterator = ilist<SDNode>::iterator;\n\n  allnodes_iterator allnodes_begin() { return AllNodes.begin(); }\n  allnodes_iterator allnodes_end() { return AllNodes.end(); }\n\n  ilist<SDNode>::size_type allnodes_size() const {\n    return AllNodes.size();\n  }\n\n  iterator_range<allnodes_iterator> allnodes() {\n    return make_range(allnodes_begin(), allnodes_end());\n  }\n  iterator_range<allnodes_const_iterator> allnodes() const {\n    return make_range(allnodes_begin(), allnodes_end());\n  }\n\n  /// Return the root tag of the SelectionDAG.\n  const SDValue &getRoot() const { return Root; }\n\n  /// Return the token chain corresponding to the entry of the function.\n  SDValue getEntryNode() const {\n    return SDValue(const_cast<SDNode *>(&EntryNode), 0);\n  }\n\n  /// Set the current root tag of the SelectionDAG.\n  ///\n  const SDValue &setRoot(SDValue N) {\n    assert((!N.getNode() || N.getValueType() == MVT::Other) &&\n           \"DAG root value is not a chain!\");\n    if (N.getNode())\n      checkForCycles(N.getNode(), this);\n    Root = N;\n    if (N.getNode())\n      checkForCycles(this);\n    return Root;\n  }\n\n#ifndef NDEBUG\n  void VerifyDAGDiverence();\n#endif\n\n  /// This iterates over the nodes in the SelectionDAG, folding\n  /// certain types of nodes together, or eliminating superfluous nodes.  The\n  /// Level argument controls whether Combine is allowed to produce nodes and\n  /// types that are illegal on the target.\n  void Combine(CombineLevel Level, AAResults *AA,\n               CodeGenOpt::Level OptLevel);\n\n  /// This transforms the SelectionDAG into a SelectionDAG that\n  /// only uses types natively supported by the target.\n  /// Returns \"true\" if it made any changes.\n  ///\n  /// Note that this is an involved process that may invalidate pointers into\n  /// the graph.\n  bool LegalizeTypes();\n\n  /// This transforms the SelectionDAG into a SelectionDAG that is\n  /// compatible with the target instruction selector, as indicated by the\n  /// TargetLowering object.\n  ///\n  /// Note that this is an involved process that may invalidate pointers into\n  /// the graph.\n  void Legalize();\n\n  /// Transforms a SelectionDAG node and any operands to it into a node\n  /// that is compatible with the target instruction selector, as indicated by\n  /// the TargetLowering object.\n  ///\n  /// \\returns true if \\c N is a valid, legal node after calling this.\n  ///\n  /// This essentially runs a single recursive walk of the \\c Legalize process\n  /// over the given node (and its operands). This can be used to incrementally\n  /// legalize the DAG. All of the nodes which are directly replaced,\n  /// potentially including N, are added to the output parameter \\c\n  /// UpdatedNodes so that the delta to the DAG can be understood by the\n  /// caller.\n  ///\n  /// When this returns false, N has been legalized in a way that make the\n  /// pointer passed in no longer valid. It may have even been deleted from the\n  /// DAG, and so it shouldn't be used further. When this returns true, the\n  /// N passed in is a legal node, and can be immediately processed as such.\n  /// This may still have done some work on the DAG, and will still populate\n  /// UpdatedNodes with any new nodes replacing those originally in the DAG.\n  bool LegalizeOp(SDNode *N, SmallSetVector<SDNode *, 16> &UpdatedNodes);\n\n  /// This transforms the SelectionDAG into a SelectionDAG\n  /// that only uses vector math operations supported by the target.  This is\n  /// necessary as a separate step from Legalize because unrolling a vector\n  /// operation can introduce illegal types, which requires running\n  /// LegalizeTypes again.\n  ///\n  /// This returns true if it made any changes; in that case, LegalizeTypes\n  /// is called again before Legalize.\n  ///\n  /// Note that this is an involved process that may invalidate pointers into\n  /// the graph.\n  bool LegalizeVectors();\n\n  /// This method deletes all unreachable nodes in the SelectionDAG.\n  void RemoveDeadNodes();\n\n  /// Remove the specified node from the system.  This node must\n  /// have no referrers.\n  void DeleteNode(SDNode *N);\n\n  /// Return an SDVTList that represents the list of values specified.\n  SDVTList getVTList(EVT VT);\n  SDVTList getVTList(EVT VT1, EVT VT2);\n  SDVTList getVTList(EVT VT1, EVT VT2, EVT VT3);\n  SDVTList getVTList(EVT VT1, EVT VT2, EVT VT3, EVT VT4);\n  SDVTList getVTList(ArrayRef<EVT> VTs);\n\n  //===--------------------------------------------------------------------===//\n  // Node creation methods.\n\n  /// Create a ConstantSDNode wrapping a constant value.\n  /// If VT is a vector type, the constant is splatted into a BUILD_VECTOR.\n  ///\n  /// If only legal types can be produced, this does the necessary\n  /// transformations (e.g., if the vector element type is illegal).\n  /// @{\n  SDValue getConstant(uint64_t Val, const SDLoc &DL, EVT VT,\n                      bool isTarget = false, bool isOpaque = false);\n  SDValue getConstant(const APInt &Val, const SDLoc &DL, EVT VT,\n                      bool isTarget = false, bool isOpaque = false);\n\n  SDValue getAllOnesConstant(const SDLoc &DL, EVT VT, bool IsTarget = false,\n                             bool IsOpaque = false) {\n    return getConstant(APInt::getAllOnesValue(VT.getScalarSizeInBits()), DL,\n                       VT, IsTarget, IsOpaque);\n  }\n\n  SDValue getConstant(const ConstantInt &Val, const SDLoc &DL, EVT VT,\n                      bool isTarget = false, bool isOpaque = false);\n  SDValue getIntPtrConstant(uint64_t Val, const SDLoc &DL,\n                            bool isTarget = false);\n  SDValue getShiftAmountConstant(uint64_t Val, EVT VT, const SDLoc &DL,\n                                 bool LegalTypes = true);\n  SDValue getVectorIdxConstant(uint64_t Val, const SDLoc &DL,\n                               bool isTarget = false);\n\n  SDValue getTargetConstant(uint64_t Val, const SDLoc &DL, EVT VT,\n                            bool isOpaque = false) {\n    return getConstant(Val, DL, VT, true, isOpaque);\n  }\n  SDValue getTargetConstant(const APInt &Val, const SDLoc &DL, EVT VT,\n                            bool isOpaque = false) {\n    return getConstant(Val, DL, VT, true, isOpaque);\n  }\n  SDValue getTargetConstant(const ConstantInt &Val, const SDLoc &DL, EVT VT,\n                            bool isOpaque = false) {\n    return getConstant(Val, DL, VT, true, isOpaque);\n  }\n\n  /// Create a true or false constant of type \\p VT using the target's\n  /// BooleanContent for type \\p OpVT.\n  SDValue getBoolConstant(bool V, const SDLoc &DL, EVT VT, EVT OpVT);\n  /// @}\n\n  /// Create a ConstantFPSDNode wrapping a constant value.\n  /// If VT is a vector type, the constant is splatted into a BUILD_VECTOR.\n  ///\n  /// If only legal types can be produced, this does the necessary\n  /// transformations (e.g., if the vector element type is illegal).\n  /// The forms that take a double should only be used for simple constants\n  /// that can be exactly represented in VT.  No checks are made.\n  /// @{\n  SDValue getConstantFP(double Val, const SDLoc &DL, EVT VT,\n                        bool isTarget = false);\n  SDValue getConstantFP(const APFloat &Val, const SDLoc &DL, EVT VT,\n                        bool isTarget = false);\n  SDValue getConstantFP(const ConstantFP &V, const SDLoc &DL, EVT VT,\n                        bool isTarget = false);\n  SDValue getTargetConstantFP(double Val, const SDLoc &DL, EVT VT) {\n    return getConstantFP(Val, DL, VT, true);\n  }\n  SDValue getTargetConstantFP(const APFloat &Val, const SDLoc &DL, EVT VT) {\n    return getConstantFP(Val, DL, VT, true);\n  }\n  SDValue getTargetConstantFP(const ConstantFP &Val, const SDLoc &DL, EVT VT) {\n    return getConstantFP(Val, DL, VT, true);\n  }\n  /// @}\n\n  SDValue getGlobalAddress(const GlobalValue *GV, const SDLoc &DL, EVT VT,\n                           int64_t offset = 0, bool isTargetGA = false,\n                           unsigned TargetFlags = 0);\n  SDValue getTargetGlobalAddress(const GlobalValue *GV, const SDLoc &DL, EVT VT,\n                                 int64_t offset = 0, unsigned TargetFlags = 0) {\n    return getGlobalAddress(GV, DL, VT, offset, true, TargetFlags);\n  }\n  SDValue getFrameIndex(int FI, EVT VT, bool isTarget = false);\n  SDValue getTargetFrameIndex(int FI, EVT VT) {\n    return getFrameIndex(FI, VT, true);\n  }\n  SDValue getJumpTable(int JTI, EVT VT, bool isTarget = false,\n                       unsigned TargetFlags = 0);\n  SDValue getTargetJumpTable(int JTI, EVT VT, unsigned TargetFlags = 0) {\n    return getJumpTable(JTI, VT, true, TargetFlags);\n  }\n  SDValue getConstantPool(const Constant *C, EVT VT, MaybeAlign Align = None,\n                          int Offs = 0, bool isT = false,\n                          unsigned TargetFlags = 0);\n  SDValue getTargetConstantPool(const Constant *C, EVT VT,\n                                MaybeAlign Align = None, int Offset = 0,\n                                unsigned TargetFlags = 0) {\n    return getConstantPool(C, VT, Align, Offset, true, TargetFlags);\n  }\n  SDValue getConstantPool(MachineConstantPoolValue *C, EVT VT,\n                          MaybeAlign Align = None, int Offs = 0,\n                          bool isT = false, unsigned TargetFlags = 0);\n  SDValue getTargetConstantPool(MachineConstantPoolValue *C, EVT VT,\n                                MaybeAlign Align = None, int Offset = 0,\n                                unsigned TargetFlags = 0) {\n    return getConstantPool(C, VT, Align, Offset, true, TargetFlags);\n  }\n  SDValue getTargetIndex(int Index, EVT VT, int64_t Offset = 0,\n                         unsigned TargetFlags = 0);\n  // When generating a branch to a BB, we don't in general know enough\n  // to provide debug info for the BB at that time, so keep this one around.\n  SDValue getBasicBlock(MachineBasicBlock *MBB);\n  SDValue getExternalSymbol(const char *Sym, EVT VT);\n  SDValue getTargetExternalSymbol(const char *Sym, EVT VT,\n                                  unsigned TargetFlags = 0);\n  SDValue getMCSymbol(MCSymbol *Sym, EVT VT);\n\n  SDValue getValueType(EVT);\n  SDValue getRegister(unsigned Reg, EVT VT);\n  SDValue getRegisterMask(const uint32_t *RegMask);\n  SDValue getEHLabel(const SDLoc &dl, SDValue Root, MCSymbol *Label);\n  SDValue getLabelNode(unsigned Opcode, const SDLoc &dl, SDValue Root,\n                       MCSymbol *Label);\n  SDValue getBlockAddress(const BlockAddress *BA, EVT VT, int64_t Offset = 0,\n                          bool isTarget = false, unsigned TargetFlags = 0);\n  SDValue getTargetBlockAddress(const BlockAddress *BA, EVT VT,\n                                int64_t Offset = 0, unsigned TargetFlags = 0) {\n    return getBlockAddress(BA, VT, Offset, true, TargetFlags);\n  }\n\n  SDValue getCopyToReg(SDValue Chain, const SDLoc &dl, unsigned Reg,\n                       SDValue N) {\n    return getNode(ISD::CopyToReg, dl, MVT::Other, Chain,\n                   getRegister(Reg, N.getValueType()), N);\n  }\n\n  // This version of the getCopyToReg method takes an extra operand, which\n  // indicates that there is potentially an incoming glue value (if Glue is not\n  // null) and that there should be a glue result.\n  SDValue getCopyToReg(SDValue Chain, const SDLoc &dl, unsigned Reg, SDValue N,\n                       SDValue Glue) {\n    SDVTList VTs = getVTList(MVT::Other, MVT::Glue);\n    SDValue Ops[] = { Chain, getRegister(Reg, N.getValueType()), N, Glue };\n    return getNode(ISD::CopyToReg, dl, VTs,\n                   makeArrayRef(Ops, Glue.getNode() ? 4 : 3));\n  }\n\n  // Similar to last getCopyToReg() except parameter Reg is a SDValue\n  SDValue getCopyToReg(SDValue Chain, const SDLoc &dl, SDValue Reg, SDValue N,\n                       SDValue Glue) {\n    SDVTList VTs = getVTList(MVT::Other, MVT::Glue);\n    SDValue Ops[] = { Chain, Reg, N, Glue };\n    return getNode(ISD::CopyToReg, dl, VTs,\n                   makeArrayRef(Ops, Glue.getNode() ? 4 : 3));\n  }\n\n  SDValue getCopyFromReg(SDValue Chain, const SDLoc &dl, unsigned Reg, EVT VT) {\n    SDVTList VTs = getVTList(VT, MVT::Other);\n    SDValue Ops[] = { Chain, getRegister(Reg, VT) };\n    return getNode(ISD::CopyFromReg, dl, VTs, Ops);\n  }\n\n  // This version of the getCopyFromReg method takes an extra operand, which\n  // indicates that there is potentially an incoming glue value (if Glue is not\n  // null) and that there should be a glue result.\n  SDValue getCopyFromReg(SDValue Chain, const SDLoc &dl, unsigned Reg, EVT VT,\n                         SDValue Glue) {\n    SDVTList VTs = getVTList(VT, MVT::Other, MVT::Glue);\n    SDValue Ops[] = { Chain, getRegister(Reg, VT), Glue };\n    return getNode(ISD::CopyFromReg, dl, VTs,\n                   makeArrayRef(Ops, Glue.getNode() ? 3 : 2));\n  }\n\n  SDValue getCondCode(ISD::CondCode Cond);\n\n  /// Return an ISD::VECTOR_SHUFFLE node. The number of elements in VT,\n  /// which must be a vector type, must match the number of mask elements\n  /// NumElts. An integer mask element equal to -1 is treated as undefined.\n  SDValue getVectorShuffle(EVT VT, const SDLoc &dl, SDValue N1, SDValue N2,\n                           ArrayRef<int> Mask);\n\n  /// Return an ISD::BUILD_VECTOR node. The number of elements in VT,\n  /// which must be a vector type, must match the number of operands in Ops.\n  /// The operands must have the same type as (or, for integers, a type wider\n  /// than) VT's element type.\n  SDValue getBuildVector(EVT VT, const SDLoc &DL, ArrayRef<SDValue> Ops) {\n    // VerifySDNode (via InsertNode) checks BUILD_VECTOR later.\n    return getNode(ISD::BUILD_VECTOR, DL, VT, Ops);\n  }\n\n  /// Return an ISD::BUILD_VECTOR node. The number of elements in VT,\n  /// which must be a vector type, must match the number of operands in Ops.\n  /// The operands must have the same type as (or, for integers, a type wider\n  /// than) VT's element type.\n  SDValue getBuildVector(EVT VT, const SDLoc &DL, ArrayRef<SDUse> Ops) {\n    // VerifySDNode (via InsertNode) checks BUILD_VECTOR later.\n    return getNode(ISD::BUILD_VECTOR, DL, VT, Ops);\n  }\n\n  /// Return a splat ISD::BUILD_VECTOR node, consisting of Op splatted to all\n  /// elements. VT must be a vector type. Op's type must be the same as (or,\n  /// for integers, a type wider than) VT's element type.\n  SDValue getSplatBuildVector(EVT VT, const SDLoc &DL, SDValue Op) {\n    // VerifySDNode (via InsertNode) checks BUILD_VECTOR later.\n    if (Op.getOpcode() == ISD::UNDEF) {\n      assert((VT.getVectorElementType() == Op.getValueType() ||\n              (VT.isInteger() &&\n               VT.getVectorElementType().bitsLE(Op.getValueType()))) &&\n             \"A splatted value must have a width equal or (for integers) \"\n             \"greater than the vector element type!\");\n      return getNode(ISD::UNDEF, SDLoc(), VT);\n    }\n\n    SmallVector<SDValue, 16> Ops(VT.getVectorNumElements(), Op);\n    return getNode(ISD::BUILD_VECTOR, DL, VT, Ops);\n  }\n\n  // Return a splat ISD::SPLAT_VECTOR node, consisting of Op splatted to all\n  // elements.\n  SDValue getSplatVector(EVT VT, const SDLoc &DL, SDValue Op) {\n    if (Op.getOpcode() == ISD::UNDEF) {\n      assert((VT.getVectorElementType() == Op.getValueType() ||\n              (VT.isInteger() &&\n               VT.getVectorElementType().bitsLE(Op.getValueType()))) &&\n             \"A splatted value must have a width equal or (for integers) \"\n             \"greater than the vector element type!\");\n      return getNode(ISD::UNDEF, SDLoc(), VT);\n    }\n    return getNode(ISD::SPLAT_VECTOR, DL, VT, Op);\n  }\n\n  /// Returns an ISD::VECTOR_SHUFFLE node semantically equivalent to\n  /// the shuffle node in input but with swapped operands.\n  ///\n  /// Example: shuffle A, B, <0,5,2,7> -> shuffle B, A, <4,1,6,3>\n  SDValue getCommutedVectorShuffle(const ShuffleVectorSDNode &SV);\n\n  /// Convert Op, which must be of float type, to the\n  /// float type VT, by either extending or rounding (by truncation).\n  SDValue getFPExtendOrRound(SDValue Op, const SDLoc &DL, EVT VT);\n\n  /// Convert Op, which must be a STRICT operation of float type, to the\n  /// float type VT, by either extending or rounding (by truncation).\n  std::pair<SDValue, SDValue>\n  getStrictFPExtendOrRound(SDValue Op, SDValue Chain, const SDLoc &DL, EVT VT);\n\n  /// Convert Op, which must be of integer type, to the\n  /// integer type VT, by either any-extending or truncating it.\n  SDValue getAnyExtOrTrunc(SDValue Op, const SDLoc &DL, EVT VT);\n\n  /// Convert Op, which must be of integer type, to the\n  /// integer type VT, by either sign-extending or truncating it.\n  SDValue getSExtOrTrunc(SDValue Op, const SDLoc &DL, EVT VT);\n\n  /// Convert Op, which must be of integer type, to the\n  /// integer type VT, by either zero-extending or truncating it.\n  SDValue getZExtOrTrunc(SDValue Op, const SDLoc &DL, EVT VT);\n\n  /// Return the expression required to zero extend the Op\n  /// value assuming it was the smaller SrcTy value.\n  SDValue getZeroExtendInReg(SDValue Op, const SDLoc &DL, EVT VT);\n\n  /// Convert Op, which must be of integer type, to the integer type VT, by\n  /// either truncating it or performing either zero or sign extension as\n  /// appropriate extension for the pointer's semantics.\n  SDValue getPtrExtOrTrunc(SDValue Op, const SDLoc &DL, EVT VT);\n\n  /// Return the expression required to extend the Op as a pointer value\n  /// assuming it was the smaller SrcTy value. This may be either a zero extend\n  /// or a sign extend.\n  SDValue getPtrExtendInReg(SDValue Op, const SDLoc &DL, EVT VT);\n\n  /// Convert Op, which must be of integer type, to the integer type VT,\n  /// by using an extension appropriate for the target's\n  /// BooleanContent for type OpVT or truncating it.\n  SDValue getBoolExtOrTrunc(SDValue Op, const SDLoc &SL, EVT VT, EVT OpVT);\n\n  /// Create a bitwise NOT operation as (XOR Val, -1).\n  SDValue getNOT(const SDLoc &DL, SDValue Val, EVT VT);\n\n  /// Create a logical NOT operation as (XOR Val, BooleanOne).\n  SDValue getLogicalNOT(const SDLoc &DL, SDValue Val, EVT VT);\n\n  /// Returns sum of the base pointer and offset.\n  /// Unlike getObjectPtrOffset this does not set NoUnsignedWrap by default.\n  SDValue getMemBasePlusOffset(SDValue Base, TypeSize Offset, const SDLoc &DL,\n                               const SDNodeFlags Flags = SDNodeFlags());\n  SDValue getMemBasePlusOffset(SDValue Base, SDValue Offset, const SDLoc &DL,\n                               const SDNodeFlags Flags = SDNodeFlags());\n\n  /// Create an add instruction with appropriate flags when used for\n  /// addressing some offset of an object. i.e. if a load is split into multiple\n  /// components, create an add nuw from the base pointer to the offset.\n  SDValue getObjectPtrOffset(const SDLoc &SL, SDValue Ptr, TypeSize Offset) {\n    SDNodeFlags Flags;\n    Flags.setNoUnsignedWrap(true);\n    return getMemBasePlusOffset(Ptr, Offset, SL, Flags);\n  }\n\n  SDValue getObjectPtrOffset(const SDLoc &SL, SDValue Ptr, SDValue Offset) {\n    // The object itself can't wrap around the address space, so it shouldn't be\n    // possible for the adds of the offsets to the split parts to overflow.\n    SDNodeFlags Flags;\n    Flags.setNoUnsignedWrap(true);\n    return getMemBasePlusOffset(Ptr, Offset, SL, Flags);\n  }\n\n  /// Return a new CALLSEQ_START node, that starts new call frame, in which\n  /// InSize bytes are set up inside CALLSEQ_START..CALLSEQ_END sequence and\n  /// OutSize specifies part of the frame set up prior to the sequence.\n  SDValue getCALLSEQ_START(SDValue Chain, uint64_t InSize, uint64_t OutSize,\n                           const SDLoc &DL) {\n    SDVTList VTs = getVTList(MVT::Other, MVT::Glue);\n    SDValue Ops[] = { Chain,\n                      getIntPtrConstant(InSize, DL, true),\n                      getIntPtrConstant(OutSize, DL, true) };\n    return getNode(ISD::CALLSEQ_START, DL, VTs, Ops);\n  }\n\n  /// Return a new CALLSEQ_END node, which always must have a\n  /// glue result (to ensure it's not CSE'd).\n  /// CALLSEQ_END does not have a useful SDLoc.\n  SDValue getCALLSEQ_END(SDValue Chain, SDValue Op1, SDValue Op2,\n                         SDValue InGlue, const SDLoc &DL) {\n    SDVTList NodeTys = getVTList(MVT::Other, MVT::Glue);\n    SmallVector<SDValue, 4> Ops;\n    Ops.push_back(Chain);\n    Ops.push_back(Op1);\n    Ops.push_back(Op2);\n    if (InGlue.getNode())\n      Ops.push_back(InGlue);\n    return getNode(ISD::CALLSEQ_END, DL, NodeTys, Ops);\n  }\n\n  /// Return true if the result of this operation is always undefined.\n  bool isUndef(unsigned Opcode, ArrayRef<SDValue> Ops);\n\n  /// Return an UNDEF node. UNDEF does not have a useful SDLoc.\n  SDValue getUNDEF(EVT VT) {\n    return getNode(ISD::UNDEF, SDLoc(), VT);\n  }\n\n  /// Return a node that represents the runtime scaling 'MulImm * RuntimeVL'.\n  SDValue getVScale(const SDLoc &DL, EVT VT, APInt MulImm) {\n    assert(MulImm.getMinSignedBits() <= VT.getSizeInBits() &&\n           \"Immediate does not fit VT\");\n    return getNode(ISD::VSCALE, DL, VT,\n                   getConstant(MulImm.sextOrTrunc(VT.getSizeInBits()), DL, VT));\n  }\n\n  /// Return a GLOBAL_OFFSET_TABLE node. This does not have a useful SDLoc.\n  SDValue getGLOBAL_OFFSET_TABLE(EVT VT) {\n    return getNode(ISD::GLOBAL_OFFSET_TABLE, SDLoc(), VT);\n  }\n\n  /// Gets or creates the specified node.\n  ///\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT,\n                  ArrayRef<SDUse> Ops);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT,\n                  ArrayRef<SDValue> Ops, const SDNodeFlags Flags);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, ArrayRef<EVT> ResultTys,\n                  ArrayRef<SDValue> Ops);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList,\n                  ArrayRef<SDValue> Ops, const SDNodeFlags Flags);\n\n  // Use flags from current flag inserter.\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT,\n                  ArrayRef<SDValue> Ops);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList,\n                  ArrayRef<SDValue> Ops);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue Operand);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue N1,\n                  SDValue N2);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue N1,\n                  SDValue N2, SDValue N3);\n\n  // Specialize based on number of operands.\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue Operand,\n                  const SDNodeFlags Flags);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue N1,\n                  SDValue N2, const SDNodeFlags Flags);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue N1,\n                  SDValue N2, SDValue N3, const SDNodeFlags Flags);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue N1,\n                  SDValue N2, SDValue N3, SDValue N4);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, EVT VT, SDValue N1,\n                  SDValue N2, SDValue N3, SDValue N4, SDValue N5);\n\n  // Specialize again based on number of operands for nodes with a VTList\n  // rather than a single VT.\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList, SDValue N);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList, SDValue N1,\n                  SDValue N2);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList, SDValue N1,\n                  SDValue N2, SDValue N3);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList, SDValue N1,\n                  SDValue N2, SDValue N3, SDValue N4);\n  SDValue getNode(unsigned Opcode, const SDLoc &DL, SDVTList VTList, SDValue N1,\n                  SDValue N2, SDValue N3, SDValue N4, SDValue N5);\n\n  /// Compute a TokenFactor to force all the incoming stack arguments to be\n  /// loaded from the stack. This is used in tail call lowering to protect\n  /// stack arguments from being clobbered.\n  SDValue getStackArgumentTokenFactor(SDValue Chain);\n\n  LLVM_ATTRIBUTE_DEPRECATED(SDValue getMemcpy(SDValue Chain, const SDLoc &dl,\n                                              SDValue Dst, SDValue Src,\n                                              SDValue Size, unsigned Align,\n                                              bool isVol, bool AlwaysInline,\n                                              bool isTailCall,\n                                              MachinePointerInfo DstPtrInfo,\n                                              MachinePointerInfo SrcPtrInfo),\n                            \"Use the version that takes Align instead\") {\n    return getMemcpy(Chain, dl, Dst, Src, Size, llvm::Align(Align), isVol,\n                     AlwaysInline, isTailCall, DstPtrInfo, SrcPtrInfo);\n  }\n\n  SDValue getMemcpy(SDValue Chain, const SDLoc &dl, SDValue Dst, SDValue Src,\n                    SDValue Size, Align Alignment, bool isVol,\n                    bool AlwaysInline, bool isTailCall,\n                    MachinePointerInfo DstPtrInfo,\n                    MachinePointerInfo SrcPtrInfo);\n\n  LLVM_ATTRIBUTE_DEPRECATED(SDValue getMemmove(SDValue Chain, const SDLoc &dl,\n                                               SDValue Dst, SDValue Src,\n                                               SDValue Size, unsigned Align,\n                                               bool isVol, bool isTailCall,\n                                               MachinePointerInfo DstPtrInfo,\n                                               MachinePointerInfo SrcPtrInfo),\n                            \"Use the version that takes Align instead\") {\n    return getMemmove(Chain, dl, Dst, Src, Size, llvm::Align(Align), isVol,\n                      isTailCall, DstPtrInfo, SrcPtrInfo);\n  }\n  SDValue getMemmove(SDValue Chain, const SDLoc &dl, SDValue Dst, SDValue Src,\n                     SDValue Size, Align Alignment, bool isVol, bool isTailCall,\n                     MachinePointerInfo DstPtrInfo,\n                     MachinePointerInfo SrcPtrInfo);\n\n  LLVM_ATTRIBUTE_DEPRECATED(SDValue getMemset(SDValue Chain, const SDLoc &dl,\n                                              SDValue Dst, SDValue Src,\n                                              SDValue Size, unsigned Align,\n                                              bool isVol, bool isTailCall,\n                                              MachinePointerInfo DstPtrInfo),\n                            \"Use the version that takes Align instead\") {\n    return getMemset(Chain, dl, Dst, Src, Size, llvm::Align(Align), isVol,\n                     isTailCall, DstPtrInfo);\n  }\n  SDValue getMemset(SDValue Chain, const SDLoc &dl, SDValue Dst, SDValue Src,\n                    SDValue Size, Align Alignment, bool isVol, bool isTailCall,\n                    MachinePointerInfo DstPtrInfo);\n\n  SDValue getAtomicMemcpy(SDValue Chain, const SDLoc &dl, SDValue Dst,\n                          unsigned DstAlign, SDValue Src, unsigned SrcAlign,\n                          SDValue Size, Type *SizeTy, unsigned ElemSz,\n                          bool isTailCall, MachinePointerInfo DstPtrInfo,\n                          MachinePointerInfo SrcPtrInfo);\n\n  SDValue getAtomicMemmove(SDValue Chain, const SDLoc &dl, SDValue Dst,\n                           unsigned DstAlign, SDValue Src, unsigned SrcAlign,\n                           SDValue Size, Type *SizeTy, unsigned ElemSz,\n                           bool isTailCall, MachinePointerInfo DstPtrInfo,\n                           MachinePointerInfo SrcPtrInfo);\n\n  SDValue getAtomicMemset(SDValue Chain, const SDLoc &dl, SDValue Dst,\n                          unsigned DstAlign, SDValue Value, SDValue Size,\n                          Type *SizeTy, unsigned ElemSz, bool isTailCall,\n                          MachinePointerInfo DstPtrInfo);\n\n  /// Helper function to make it easier to build SetCC's if you just have an\n  /// ISD::CondCode instead of an SDValue.\n  SDValue getSetCC(const SDLoc &DL, EVT VT, SDValue LHS, SDValue RHS,\n                   ISD::CondCode Cond, SDValue Chain = SDValue(),\n                   bool IsSignaling = false) {\n    assert(LHS.getValueType().isVector() == RHS.getValueType().isVector() &&\n           \"Cannot compare scalars to vectors\");\n    assert(LHS.getValueType().isVector() == VT.isVector() &&\n           \"Cannot compare scalars to vectors\");\n    assert(Cond != ISD::SETCC_INVALID &&\n           \"Cannot create a setCC of an invalid node.\");\n    if (Chain)\n      return getNode(IsSignaling ? ISD::STRICT_FSETCCS : ISD::STRICT_FSETCC, DL,\n                     {VT, MVT::Other}, {Chain, LHS, RHS, getCondCode(Cond)});\n    return getNode(ISD::SETCC, DL, VT, LHS, RHS, getCondCode(Cond));\n  }\n\n  /// Helper function to make it easier to build Select's if you just have\n  /// operands and don't want to check for vector.\n  SDValue getSelect(const SDLoc &DL, EVT VT, SDValue Cond, SDValue LHS,\n                    SDValue RHS) {\n    assert(LHS.getValueType() == RHS.getValueType() &&\n           \"Cannot use select on differing types\");\n    assert(VT.isVector() == LHS.getValueType().isVector() &&\n           \"Cannot mix vectors and scalars\");\n    auto Opcode = Cond.getValueType().isVector() ? ISD::VSELECT : ISD::SELECT;\n    return getNode(Opcode, DL, VT, Cond, LHS, RHS);\n  }\n\n  /// Helper function to make it easier to build SelectCC's if you just have an\n  /// ISD::CondCode instead of an SDValue.\n  SDValue getSelectCC(const SDLoc &DL, SDValue LHS, SDValue RHS, SDValue True,\n                      SDValue False, ISD::CondCode Cond) {\n    return getNode(ISD::SELECT_CC, DL, True.getValueType(), LHS, RHS, True,\n                   False, getCondCode(Cond));\n  }\n\n  /// Try to simplify a select/vselect into 1 of its operands or a constant.\n  SDValue simplifySelect(SDValue Cond, SDValue TVal, SDValue FVal);\n\n  /// Try to simplify a shift into 1 of its operands or a constant.\n  SDValue simplifyShift(SDValue X, SDValue Y);\n\n  /// Try to simplify a floating-point binary operation into 1 of its operands\n  /// or a constant.\n  SDValue simplifyFPBinop(unsigned Opcode, SDValue X, SDValue Y,\n                          SDNodeFlags Flags);\n\n  /// VAArg produces a result and token chain, and takes a pointer\n  /// and a source value as input.\n  SDValue getVAArg(EVT VT, const SDLoc &dl, SDValue Chain, SDValue Ptr,\n                   SDValue SV, unsigned Align);\n\n  /// Gets a node for an atomic cmpxchg op. There are two\n  /// valid Opcodes. ISD::ATOMIC_CMO_SWAP produces the value loaded and a\n  /// chain result. ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS produces the value loaded,\n  /// a success flag (initially i1), and a chain.\n  SDValue getAtomicCmpSwap(unsigned Opcode, const SDLoc &dl, EVT MemVT,\n                           SDVTList VTs, SDValue Chain, SDValue Ptr,\n                           SDValue Cmp, SDValue Swp, MachineMemOperand *MMO);\n\n  /// Gets a node for an atomic op, produces result (if relevant)\n  /// and chain and takes 2 operands.\n  SDValue getAtomic(unsigned Opcode, const SDLoc &dl, EVT MemVT, SDValue Chain,\n                    SDValue Ptr, SDValue Val, MachineMemOperand *MMO);\n\n  /// Gets a node for an atomic op, produces result and chain and\n  /// takes 1 operand.\n  SDValue getAtomic(unsigned Opcode, const SDLoc &dl, EVT MemVT, EVT VT,\n                    SDValue Chain, SDValue Ptr, MachineMemOperand *MMO);\n\n  /// Gets a node for an atomic op, produces result and chain and takes N\n  /// operands.\n  SDValue getAtomic(unsigned Opcode, const SDLoc &dl, EVT MemVT,\n                    SDVTList VTList, ArrayRef<SDValue> Ops,\n                    MachineMemOperand *MMO);\n\n  /// Creates a MemIntrinsicNode that may produce a\n  /// result and takes a list of operands. Opcode may be INTRINSIC_VOID,\n  /// INTRINSIC_W_CHAIN, or a target-specific opcode with a value not\n  /// less than FIRST_TARGET_MEMORY_OPCODE.\n  SDValue getMemIntrinsicNode(\n      unsigned Opcode, const SDLoc &dl, SDVTList VTList, ArrayRef<SDValue> Ops,\n      EVT MemVT, MachinePointerInfo PtrInfo, Align Alignment,\n      MachineMemOperand::Flags Flags = MachineMemOperand::MOLoad |\n                                       MachineMemOperand::MOStore,\n      uint64_t Size = 0, const AAMDNodes &AAInfo = AAMDNodes());\n\n  inline SDValue getMemIntrinsicNode(\n      unsigned Opcode, const SDLoc &dl, SDVTList VTList, ArrayRef<SDValue> Ops,\n      EVT MemVT, MachinePointerInfo PtrInfo, MaybeAlign Alignment = None,\n      MachineMemOperand::Flags Flags = MachineMemOperand::MOLoad |\n                                       MachineMemOperand::MOStore,\n      uint64_t Size = 0, const AAMDNodes &AAInfo = AAMDNodes()) {\n    // Ensure that codegen never sees alignment 0\n    return getMemIntrinsicNode(Opcode, dl, VTList, Ops, MemVT, PtrInfo,\n                               Alignment.getValueOr(getEVTAlign(MemVT)), Flags,\n                               Size, AAInfo);\n  }\n\n  LLVM_ATTRIBUTE_DEPRECATED(\n      inline SDValue getMemIntrinsicNode(\n          unsigned Opcode, const SDLoc &dl, SDVTList VTList,\n          ArrayRef<SDValue> Ops, EVT MemVT, MachinePointerInfo PtrInfo,\n          unsigned Alignment,\n          MachineMemOperand::Flags Flags = MachineMemOperand::MOLoad |\n                                           MachineMemOperand::MOStore,\n          uint64_t Size = 0, const AAMDNodes &AAInfo = AAMDNodes()),\n      \"\") {\n    return getMemIntrinsicNode(Opcode, dl, VTList, Ops, MemVT, PtrInfo,\n                               MaybeAlign(Alignment), Flags, Size, AAInfo);\n  }\n\n  SDValue getMemIntrinsicNode(unsigned Opcode, const SDLoc &dl, SDVTList VTList,\n                              ArrayRef<SDValue> Ops, EVT MemVT,\n                              MachineMemOperand *MMO);\n\n  /// Creates a LifetimeSDNode that starts (`IsStart==true`) or ends\n  /// (`IsStart==false`) the lifetime of the portion of `FrameIndex` between\n  /// offsets `Offset` and `Offset + Size`.\n  SDValue getLifetimeNode(bool IsStart, const SDLoc &dl, SDValue Chain,\n                          int FrameIndex, int64_t Size, int64_t Offset = -1);\n\n  /// Creates a PseudoProbeSDNode with function GUID `Guid` and\n  /// the index of the block `Index` it is probing, as well as the attributes\n  /// `attr` of the probe.\n  SDValue getPseudoProbeNode(const SDLoc &Dl, SDValue Chain, uint64_t Guid,\n                             uint64_t Index, uint32_t Attr);\n\n  /// Create a MERGE_VALUES node from the given operands.\n  SDValue getMergeValues(ArrayRef<SDValue> Ops, const SDLoc &dl);\n\n  /// Loads are not normal binary operators: their result type is not\n  /// determined by their operands, and they produce a value AND a token chain.\n  ///\n  /// This function will set the MOLoad flag on MMOFlags, but you can set it if\n  /// you want.  The MOStore flag must not be set.\n  SDValue getLoad(EVT VT, const SDLoc &dl, SDValue Chain, SDValue Ptr,\n                  MachinePointerInfo PtrInfo,\n                  MaybeAlign Alignment = MaybeAlign(),\n                  MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n                  const AAMDNodes &AAInfo = AAMDNodes(),\n                  const MDNode *Ranges = nullptr);\n  /// FIXME: Remove once transition to Align is over.\n  inline SDValue\n  getLoad(EVT VT, const SDLoc &dl, SDValue Chain, SDValue Ptr,\n          MachinePointerInfo PtrInfo, unsigned Alignment,\n          MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n          const AAMDNodes &AAInfo = AAMDNodes(),\n          const MDNode *Ranges = nullptr) {\n    return getLoad(VT, dl, Chain, Ptr, PtrInfo, MaybeAlign(Alignment), MMOFlags,\n                   AAInfo, Ranges);\n  }\n  SDValue getLoad(EVT VT, const SDLoc &dl, SDValue Chain, SDValue Ptr,\n                  MachineMemOperand *MMO);\n  SDValue\n  getExtLoad(ISD::LoadExtType ExtType, const SDLoc &dl, EVT VT, SDValue Chain,\n             SDValue Ptr, MachinePointerInfo PtrInfo, EVT MemVT,\n             MaybeAlign Alignment = MaybeAlign(),\n             MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n             const AAMDNodes &AAInfo = AAMDNodes());\n  /// FIXME: Remove once transition to Align is over.\n  inline SDValue\n  getExtLoad(ISD::LoadExtType ExtType, const SDLoc &dl, EVT VT, SDValue Chain,\n             SDValue Ptr, MachinePointerInfo PtrInfo, EVT MemVT,\n             unsigned Alignment,\n             MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n             const AAMDNodes &AAInfo = AAMDNodes()) {\n    return getExtLoad(ExtType, dl, VT, Chain, Ptr, PtrInfo, MemVT,\n                      MaybeAlign(Alignment), MMOFlags, AAInfo);\n  }\n  SDValue getExtLoad(ISD::LoadExtType ExtType, const SDLoc &dl, EVT VT,\n                     SDValue Chain, SDValue Ptr, EVT MemVT,\n                     MachineMemOperand *MMO);\n  SDValue getIndexedLoad(SDValue OrigLoad, const SDLoc &dl, SDValue Base,\n                         SDValue Offset, ISD::MemIndexedMode AM);\n  SDValue getLoad(ISD::MemIndexedMode AM, ISD::LoadExtType ExtType, EVT VT,\n                  const SDLoc &dl, SDValue Chain, SDValue Ptr, SDValue Offset,\n                  MachinePointerInfo PtrInfo, EVT MemVT, Align Alignment,\n                  MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n                  const AAMDNodes &AAInfo = AAMDNodes(),\n                  const MDNode *Ranges = nullptr);\n  inline SDValue getLoad(\n      ISD::MemIndexedMode AM, ISD::LoadExtType ExtType, EVT VT, const SDLoc &dl,\n      SDValue Chain, SDValue Ptr, SDValue Offset, MachinePointerInfo PtrInfo,\n      EVT MemVT, MaybeAlign Alignment = MaybeAlign(),\n      MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n      const AAMDNodes &AAInfo = AAMDNodes(), const MDNode *Ranges = nullptr) {\n    // Ensures that codegen never sees a None Alignment.\n    return getLoad(AM, ExtType, VT, dl, Chain, Ptr, Offset, PtrInfo, MemVT,\n                   Alignment.getValueOr(getEVTAlign(MemVT)), MMOFlags, AAInfo,\n                   Ranges);\n  }\n  /// FIXME: Remove once transition to Align is over.\n  inline SDValue\n  getLoad(ISD::MemIndexedMode AM, ISD::LoadExtType ExtType, EVT VT,\n          const SDLoc &dl, SDValue Chain, SDValue Ptr, SDValue Offset,\n          MachinePointerInfo PtrInfo, EVT MemVT, unsigned Alignment,\n          MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n          const AAMDNodes &AAInfo = AAMDNodes(),\n          const MDNode *Ranges = nullptr) {\n    return getLoad(AM, ExtType, VT, dl, Chain, Ptr, Offset, PtrInfo, MemVT,\n                   MaybeAlign(Alignment), MMOFlags, AAInfo, Ranges);\n  }\n  SDValue getLoad(ISD::MemIndexedMode AM, ISD::LoadExtType ExtType, EVT VT,\n                  const SDLoc &dl, SDValue Chain, SDValue Ptr, SDValue Offset,\n                  EVT MemVT, MachineMemOperand *MMO);\n\n  /// Helper function to build ISD::STORE nodes.\n  ///\n  /// This function will set the MOStore flag on MMOFlags, but you can set it if\n  /// you want.  The MOLoad and MOInvariant flags must not be set.\n\n  SDValue\n  getStore(SDValue Chain, const SDLoc &dl, SDValue Val, SDValue Ptr,\n           MachinePointerInfo PtrInfo, Align Alignment,\n           MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n           const AAMDNodes &AAInfo = AAMDNodes());\n  inline SDValue\n  getStore(SDValue Chain, const SDLoc &dl, SDValue Val, SDValue Ptr,\n           MachinePointerInfo PtrInfo, MaybeAlign Alignment = MaybeAlign(),\n           MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n           const AAMDNodes &AAInfo = AAMDNodes()) {\n    return getStore(Chain, dl, Val, Ptr, PtrInfo,\n                    Alignment.getValueOr(getEVTAlign(Val.getValueType())),\n                    MMOFlags, AAInfo);\n  }\n  /// FIXME: Remove once transition to Align is over.\n  inline SDValue\n  getStore(SDValue Chain, const SDLoc &dl, SDValue Val, SDValue Ptr,\n           MachinePointerInfo PtrInfo, unsigned Alignment,\n           MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n           const AAMDNodes &AAInfo = AAMDNodes()) {\n    return getStore(Chain, dl, Val, Ptr, PtrInfo, MaybeAlign(Alignment),\n                    MMOFlags, AAInfo);\n  }\n  SDValue getStore(SDValue Chain, const SDLoc &dl, SDValue Val, SDValue Ptr,\n                   MachineMemOperand *MMO);\n  SDValue\n  getTruncStore(SDValue Chain, const SDLoc &dl, SDValue Val, SDValue Ptr,\n                MachinePointerInfo PtrInfo, EVT SVT, Align Alignment,\n                MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n                const AAMDNodes &AAInfo = AAMDNodes());\n  inline SDValue\n  getTruncStore(SDValue Chain, const SDLoc &dl, SDValue Val, SDValue Ptr,\n                MachinePointerInfo PtrInfo, EVT SVT,\n                MaybeAlign Alignment = MaybeAlign(),\n                MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n                const AAMDNodes &AAInfo = AAMDNodes()) {\n    return getTruncStore(Chain, dl, Val, Ptr, PtrInfo, SVT,\n                         Alignment.getValueOr(getEVTAlign(SVT)), MMOFlags,\n                         AAInfo);\n  }\n  /// FIXME: Remove once transition to Align is over.\n  inline SDValue\n  getTruncStore(SDValue Chain, const SDLoc &dl, SDValue Val, SDValue Ptr,\n                MachinePointerInfo PtrInfo, EVT SVT, unsigned Alignment,\n                MachineMemOperand::Flags MMOFlags = MachineMemOperand::MONone,\n                const AAMDNodes &AAInfo = AAMDNodes()) {\n    return getTruncStore(Chain, dl, Val, Ptr, PtrInfo, SVT,\n                         MaybeAlign(Alignment), MMOFlags, AAInfo);\n  }\n  SDValue getTruncStore(SDValue Chain, const SDLoc &dl, SDValue Val,\n                        SDValue Ptr, EVT SVT, MachineMemOperand *MMO);\n  SDValue getIndexedStore(SDValue OrigStore, const SDLoc &dl, SDValue Base,\n                          SDValue Offset, ISD::MemIndexedMode AM);\n\n  SDValue getMaskedLoad(EVT VT, const SDLoc &dl, SDValue Chain, SDValue Base,\n                        SDValue Offset, SDValue Mask, SDValue Src0, EVT MemVT,\n                        MachineMemOperand *MMO, ISD::MemIndexedMode AM,\n                        ISD::LoadExtType, bool IsExpanding = false);\n  SDValue getIndexedMaskedLoad(SDValue OrigLoad, const SDLoc &dl, SDValue Base,\n                               SDValue Offset, ISD::MemIndexedMode AM);\n  SDValue getMaskedStore(SDValue Chain, const SDLoc &dl, SDValue Val,\n                         SDValue Base, SDValue Offset, SDValue Mask, EVT MemVT,\n                         MachineMemOperand *MMO, ISD::MemIndexedMode AM,\n                         bool IsTruncating = false, bool IsCompressing = false);\n  SDValue getIndexedMaskedStore(SDValue OrigStore, const SDLoc &dl,\n                                SDValue Base, SDValue Offset,\n                                ISD::MemIndexedMode AM);\n  SDValue getMaskedGather(SDVTList VTs, EVT VT, const SDLoc &dl,\n                          ArrayRef<SDValue> Ops, MachineMemOperand *MMO,\n                          ISD::MemIndexType IndexType, ISD::LoadExtType ExtTy);\n  SDValue getMaskedScatter(SDVTList VTs, EVT VT, const SDLoc &dl,\n                           ArrayRef<SDValue> Ops, MachineMemOperand *MMO,\n                           ISD::MemIndexType IndexType,\n                           bool IsTruncating = false);\n\n  /// Construct a node to track a Value* through the backend.\n  SDValue getSrcValue(const Value *v);\n\n  /// Return an MDNodeSDNode which holds an MDNode.\n  SDValue getMDNode(const MDNode *MD);\n\n  /// Return a bitcast using the SDLoc of the value operand, and casting to the\n  /// provided type. Use getNode to set a custom SDLoc.\n  SDValue getBitcast(EVT VT, SDValue V);\n\n  /// Return an AddrSpaceCastSDNode.\n  SDValue getAddrSpaceCast(const SDLoc &dl, EVT VT, SDValue Ptr, unsigned SrcAS,\n                           unsigned DestAS);\n\n  /// Return a freeze using the SDLoc of the value operand.\n  SDValue getFreeze(SDValue V);\n\n  /// Return an AssertAlignSDNode.\n  SDValue getAssertAlign(const SDLoc &DL, SDValue V, Align A);\n\n  /// Return the specified value casted to\n  /// the target's desired shift amount type.\n  SDValue getShiftAmountOperand(EVT LHSTy, SDValue Op);\n\n  /// Expand the specified \\c ISD::VAARG node as the Legalize pass would.\n  SDValue expandVAArg(SDNode *Node);\n\n  /// Expand the specified \\c ISD::VACOPY node as the Legalize pass would.\n  SDValue expandVACopy(SDNode *Node);\n\n  /// Returs an GlobalAddress of the function from the current module with\n  /// name matching the given ExternalSymbol. Additionally can provide the\n  /// matched function.\n  /// Panics the function doesn't exists.\n  SDValue getSymbolFunctionGlobalAddress(SDValue Op,\n                                         Function **TargetFunction = nullptr);\n\n  /// *Mutate* the specified node in-place to have the\n  /// specified operands.  If the resultant node already exists in the DAG,\n  /// this does not modify the specified node, instead it returns the node that\n  /// already exists.  If the resultant node does not exist in the DAG, the\n  /// input node is returned.  As a degenerate case, if you specify the same\n  /// input operands as the node already has, the input node is returned.\n  SDNode *UpdateNodeOperands(SDNode *N, SDValue Op);\n  SDNode *UpdateNodeOperands(SDNode *N, SDValue Op1, SDValue Op2);\n  SDNode *UpdateNodeOperands(SDNode *N, SDValue Op1, SDValue Op2,\n                               SDValue Op3);\n  SDNode *UpdateNodeOperands(SDNode *N, SDValue Op1, SDValue Op2,\n                               SDValue Op3, SDValue Op4);\n  SDNode *UpdateNodeOperands(SDNode *N, SDValue Op1, SDValue Op2,\n                               SDValue Op3, SDValue Op4, SDValue Op5);\n  SDNode *UpdateNodeOperands(SDNode *N, ArrayRef<SDValue> Ops);\n\n  /// Creates a new TokenFactor containing \\p Vals. If \\p Vals contains 64k\n  /// values or more, move values into new TokenFactors in 64k-1 blocks, until\n  /// the final TokenFactor has less than 64k operands.\n  SDValue getTokenFactor(const SDLoc &DL, SmallVectorImpl<SDValue> &Vals);\n\n  /// *Mutate* the specified machine node's memory references to the provided\n  /// list.\n  void setNodeMemRefs(MachineSDNode *N,\n                      ArrayRef<MachineMemOperand *> NewMemRefs);\n\n  // Calculate divergence of node \\p N based on its operands.\n  bool calculateDivergence(SDNode *N);\n\n  // Propagates the change in divergence to users\n  void updateDivergence(SDNode * N);\n\n  /// These are used for target selectors to *mutate* the\n  /// specified node to have the specified return type, Target opcode, and\n  /// operands.  Note that target opcodes are stored as\n  /// ~TargetOpcode in the node opcode field.  The resultant node is returned.\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT, SDValue Op1);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT,\n                       SDValue Op1, SDValue Op2);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT,\n                       SDValue Op1, SDValue Op2, SDValue Op3);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT,\n                       ArrayRef<SDValue> Ops);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT1, EVT VT2);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT1,\n                       EVT VT2, ArrayRef<SDValue> Ops);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT1,\n                       EVT VT2, EVT VT3, ArrayRef<SDValue> Ops);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, EVT VT1,\n                       EVT VT2, SDValue Op1, SDValue Op2);\n  SDNode *SelectNodeTo(SDNode *N, unsigned MachineOpc, SDVTList VTs,\n                       ArrayRef<SDValue> Ops);\n\n  /// This *mutates* the specified node to have the specified\n  /// return type, opcode, and operands.\n  SDNode *MorphNodeTo(SDNode *N, unsigned Opc, SDVTList VTs,\n                      ArrayRef<SDValue> Ops);\n\n  /// Mutate the specified strict FP node to its non-strict equivalent,\n  /// unlinking the node from its chain and dropping the metadata arguments.\n  /// The node must be a strict FP node.\n  SDNode *mutateStrictFPToFP(SDNode *Node);\n\n  /// These are used for target selectors to create a new node\n  /// with specified return type(s), MachineInstr opcode, and operands.\n  ///\n  /// Note that getMachineNode returns the resultant node.  If there is already\n  /// a node of the specified opcode and operands, it returns that node instead\n  /// of the current one.\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT,\n                                SDValue Op1);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT,\n                                SDValue Op1, SDValue Op2);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT,\n                                SDValue Op1, SDValue Op2, SDValue Op3);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT,\n                                ArrayRef<SDValue> Ops);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT1,\n                                EVT VT2, SDValue Op1, SDValue Op2);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT1,\n                                EVT VT2, SDValue Op1, SDValue Op2, SDValue Op3);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT1,\n                                EVT VT2, ArrayRef<SDValue> Ops);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT1,\n                                EVT VT2, EVT VT3, SDValue Op1, SDValue Op2);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT1,\n                                EVT VT2, EVT VT3, SDValue Op1, SDValue Op2,\n                                SDValue Op3);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, EVT VT1,\n                                EVT VT2, EVT VT3, ArrayRef<SDValue> Ops);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl,\n                                ArrayRef<EVT> ResultTys, ArrayRef<SDValue> Ops);\n  MachineSDNode *getMachineNode(unsigned Opcode, const SDLoc &dl, SDVTList VTs,\n                                ArrayRef<SDValue> Ops);\n\n  /// A convenience function for creating TargetInstrInfo::EXTRACT_SUBREG nodes.\n  SDValue getTargetExtractSubreg(int SRIdx, const SDLoc &DL, EVT VT,\n                                 SDValue Operand);\n\n  /// A convenience function for creating TargetInstrInfo::INSERT_SUBREG nodes.\n  SDValue getTargetInsertSubreg(int SRIdx, const SDLoc &DL, EVT VT,\n                                SDValue Operand, SDValue Subreg);\n\n  /// Get the specified node if it's already available, or else return NULL.\n  SDNode *getNodeIfExists(unsigned Opcode, SDVTList VTList,\n                          ArrayRef<SDValue> Ops, const SDNodeFlags Flags);\n  SDNode *getNodeIfExists(unsigned Opcode, SDVTList VTList,\n                          ArrayRef<SDValue> Ops);\n\n  /// Check if a node exists without modifying its flags.\n  bool doesNodeExist(unsigned Opcode, SDVTList VTList, ArrayRef<SDValue> Ops);\n\n  /// Creates a SDDbgValue node.\n  SDDbgValue *getDbgValue(DIVariable *Var, DIExpression *Expr, SDNode *N,\n                          unsigned R, bool IsIndirect, const DebugLoc &DL,\n                          unsigned O);\n\n  /// Creates a constant SDDbgValue node.\n  SDDbgValue *getConstantDbgValue(DIVariable *Var, DIExpression *Expr,\n                                  const Value *C, const DebugLoc &DL,\n                                  unsigned O);\n\n  /// Creates a FrameIndex SDDbgValue node.\n  SDDbgValue *getFrameIndexDbgValue(DIVariable *Var, DIExpression *Expr,\n                                    unsigned FI, bool IsIndirect,\n                                    const DebugLoc &DL, unsigned O);\n\n  /// Creates a FrameIndex SDDbgValue node.\n  SDDbgValue *getFrameIndexDbgValue(DIVariable *Var, DIExpression *Expr,\n                                    unsigned FI,\n                                    ArrayRef<SDNode *> Dependencies,\n                                    bool IsIndirect, const DebugLoc &DL,\n                                    unsigned O);\n\n  /// Creates a VReg SDDbgValue node.\n  SDDbgValue *getVRegDbgValue(DIVariable *Var, DIExpression *Expr,\n                              unsigned VReg, bool IsIndirect,\n                              const DebugLoc &DL, unsigned O);\n\n  /// Creates a SDDbgValue node from a list of locations.\n  SDDbgValue *getDbgValueList(DIVariable *Var, DIExpression *Expr,\n                              ArrayRef<SDDbgOperand> Locs,\n                              ArrayRef<SDNode *> Dependencies, bool IsIndirect,\n                              const DebugLoc &DL, unsigned O, bool IsVariadic);\n\n  /// Creates a SDDbgLabel node.\n  SDDbgLabel *getDbgLabel(DILabel *Label, const DebugLoc &DL, unsigned O);\n\n  /// Transfer debug values from one node to another, while optionally\n  /// generating fragment expressions for split-up values. If \\p InvalidateDbg\n  /// is set, debug values are invalidated after they are transferred.\n  void transferDbgValues(SDValue From, SDValue To, unsigned OffsetInBits = 0,\n                         unsigned SizeInBits = 0, bool InvalidateDbg = true);\n\n  /// Remove the specified node from the system. If any of its\n  /// operands then becomes dead, remove them as well. Inform UpdateListener\n  /// for each node deleted.\n  void RemoveDeadNode(SDNode *N);\n\n  /// This method deletes the unreachable nodes in the\n  /// given list, and any nodes that become unreachable as a result.\n  void RemoveDeadNodes(SmallVectorImpl<SDNode *> &DeadNodes);\n\n  /// Modify anything using 'From' to use 'To' instead.\n  /// This can cause recursive merging of nodes in the DAG.  Use the first\n  /// version if 'From' is known to have a single result, use the second\n  /// if you have two nodes with identical results (or if 'To' has a superset\n  /// of the results of 'From'), use the third otherwise.\n  ///\n  /// These methods all take an optional UpdateListener, which (if not null) is\n  /// informed about nodes that are deleted and modified due to recursive\n  /// changes in the dag.\n  ///\n  /// These functions only replace all existing uses. It's possible that as\n  /// these replacements are being performed, CSE may cause the From node\n  /// to be given new uses. These new uses of From are left in place, and\n  /// not automatically transferred to To.\n  ///\n  void ReplaceAllUsesWith(SDValue From, SDValue To);\n  void ReplaceAllUsesWith(SDNode *From, SDNode *To);\n  void ReplaceAllUsesWith(SDNode *From, const SDValue *To);\n\n  /// Replace any uses of From with To, leaving\n  /// uses of other values produced by From.getNode() alone.\n  void ReplaceAllUsesOfValueWith(SDValue From, SDValue To);\n\n  /// Like ReplaceAllUsesOfValueWith, but for multiple values at once.\n  /// This correctly handles the case where\n  /// there is an overlap between the From values and the To values.\n  void ReplaceAllUsesOfValuesWith(const SDValue *From, const SDValue *To,\n                                  unsigned Num);\n\n  /// If an existing load has uses of its chain, create a token factor node with\n  /// that chain and the new memory node's chain and update users of the old\n  /// chain to the token factor. This ensures that the new memory node will have\n  /// the same relative memory dependency position as the old load. Returns the\n  /// new merged load chain.\n  SDValue makeEquivalentMemoryOrdering(SDValue OldChain, SDValue NewMemOpChain);\n\n  /// If an existing load has uses of its chain, create a token factor node with\n  /// that chain and the new memory node's chain and update users of the old\n  /// chain to the token factor. This ensures that the new memory node will have\n  /// the same relative memory dependency position as the old load. Returns the\n  /// new merged load chain.\n  SDValue makeEquivalentMemoryOrdering(LoadSDNode *OldLoad, SDValue NewMemOp);\n\n  /// Topological-sort the AllNodes list and a\n  /// assign a unique node id for each node in the DAG based on their\n  /// topological order. Returns the number of nodes.\n  unsigned AssignTopologicalOrder();\n\n  /// Move node N in the AllNodes list to be immediately\n  /// before the given iterator Position. This may be used to update the\n  /// topological ordering when the list of nodes is modified.\n  void RepositionNode(allnodes_iterator Position, SDNode *N) {\n    AllNodes.insert(Position, AllNodes.remove(N));\n  }\n\n  /// Returns an APFloat semantics tag appropriate for the given type. If VT is\n  /// a vector type, the element semantics are returned.\n  static const fltSemantics &EVTToAPFloatSemantics(EVT VT) {\n    switch (VT.getScalarType().getSimpleVT().SimpleTy) {\n    default: llvm_unreachable(\"Unknown FP format\");\n    case MVT::f16:     return APFloat::IEEEhalf();\n    case MVT::bf16:    return APFloat::BFloat();\n    case MVT::f32:     return APFloat::IEEEsingle();\n    case MVT::f64:     return APFloat::IEEEdouble();\n    case MVT::f80:     return APFloat::x87DoubleExtended();\n    case MVT::f128:    return APFloat::IEEEquad();\n    case MVT::ppcf128: return APFloat::PPCDoubleDouble();\n    }\n  }\n\n  /// Add a dbg_value SDNode. If SD is non-null that means the\n  /// value is produced by SD.\n  void AddDbgValue(SDDbgValue *DB, bool isParameter);\n\n  /// Add a dbg_label SDNode.\n  void AddDbgLabel(SDDbgLabel *DB);\n\n  /// Get the debug values which reference the given SDNode.\n  ArrayRef<SDDbgValue*> GetDbgValues(const SDNode* SD) const {\n    return DbgInfo->getSDDbgValues(SD);\n  }\n\npublic:\n  /// Return true if there are any SDDbgValue nodes associated\n  /// with this SelectionDAG.\n  bool hasDebugValues() const { return !DbgInfo->empty(); }\n\n  SDDbgInfo::DbgIterator DbgBegin() const { return DbgInfo->DbgBegin(); }\n  SDDbgInfo::DbgIterator DbgEnd() const  { return DbgInfo->DbgEnd(); }\n\n  SDDbgInfo::DbgIterator ByvalParmDbgBegin() const {\n    return DbgInfo->ByvalParmDbgBegin();\n  }\n  SDDbgInfo::DbgIterator ByvalParmDbgEnd() const {\n    return DbgInfo->ByvalParmDbgEnd();\n  }\n\n  SDDbgInfo::DbgLabelIterator DbgLabelBegin() const {\n    return DbgInfo->DbgLabelBegin();\n  }\n  SDDbgInfo::DbgLabelIterator DbgLabelEnd() const {\n    return DbgInfo->DbgLabelEnd();\n  }\n\n  /// To be invoked on an SDNode that is slated to be erased. This\n  /// function mirrors \\c llvm::salvageDebugInfo.\n  void salvageDebugInfo(SDNode &N);\n\n  void dump() const;\n\n  /// In most cases this function returns the ABI alignment for a given type,\n  /// except for illegal vector types where the alignment exceeds that of the\n  /// stack. In such cases we attempt to break the vector down to a legal type\n  /// and return the ABI alignment for that instead.\n  Align getReducedAlign(EVT VT, bool UseABI);\n\n  /// Create a stack temporary based on the size in bytes and the alignment\n  SDValue CreateStackTemporary(TypeSize Bytes, Align Alignment);\n\n  /// Create a stack temporary, suitable for holding the specified value type.\n  /// If minAlign is specified, the slot size will have at least that alignment.\n  SDValue CreateStackTemporary(EVT VT, unsigned minAlign = 1);\n\n  /// Create a stack temporary suitable for holding either of the specified\n  /// value types.\n  SDValue CreateStackTemporary(EVT VT1, EVT VT2);\n\n  SDValue FoldSymbolOffset(unsigned Opcode, EVT VT,\n                           const GlobalAddressSDNode *GA,\n                           const SDNode *N2);\n\n  SDValue FoldConstantArithmetic(unsigned Opcode, const SDLoc &DL, EVT VT,\n                                 ArrayRef<SDValue> Ops);\n\n  SDValue FoldConstantVectorArithmetic(unsigned Opcode, const SDLoc &DL, EVT VT,\n                                       ArrayRef<SDValue> Ops,\n                                       const SDNodeFlags Flags = SDNodeFlags());\n\n  /// Fold floating-point operations with 2 operands when both operands are\n  /// constants and/or undefined.\n  SDValue foldConstantFPMath(unsigned Opcode, const SDLoc &DL, EVT VT,\n                             SDValue N1, SDValue N2);\n\n  /// Constant fold a setcc to true or false.\n  SDValue FoldSetCC(EVT VT, SDValue N1, SDValue N2, ISD::CondCode Cond,\n                    const SDLoc &dl);\n\n  /// See if the specified operand can be simplified with the knowledge that\n  /// only the bits specified by DemandedBits are used.  If so, return the\n  /// simpler operand, otherwise return a null SDValue.\n  ///\n  /// (This exists alongside SimplifyDemandedBits because GetDemandedBits can\n  /// simplify nodes with multiple uses more aggressively.)\n  SDValue GetDemandedBits(SDValue V, const APInt &DemandedBits);\n\n  /// See if the specified operand can be simplified with the knowledge that\n  /// only the bits specified by DemandedBits are used in the elements specified\n  /// by DemandedElts.  If so, return the simpler operand, otherwise return a\n  /// null SDValue.\n  ///\n  /// (This exists alongside SimplifyDemandedBits because GetDemandedBits can\n  /// simplify nodes with multiple uses more aggressively.)\n  SDValue GetDemandedBits(SDValue V, const APInt &DemandedBits,\n                          const APInt &DemandedElts);\n\n  /// Return true if the sign bit of Op is known to be zero.\n  /// We use this predicate to simplify operations downstream.\n  bool SignBitIsZero(SDValue Op, unsigned Depth = 0) const;\n\n  /// Return true if 'Op & Mask' is known to be zero.  We\n  /// use this predicate to simplify operations downstream.  Op and Mask are\n  /// known to be the same type.\n  bool MaskedValueIsZero(SDValue Op, const APInt &Mask,\n                         unsigned Depth = 0) const;\n\n  /// Return true if 'Op & Mask' is known to be zero in DemandedElts.  We\n  /// use this predicate to simplify operations downstream.  Op and Mask are\n  /// known to be the same type.\n  bool MaskedValueIsZero(SDValue Op, const APInt &Mask,\n                         const APInt &DemandedElts, unsigned Depth = 0) const;\n\n  /// Return true if '(Op & Mask) == Mask'.\n  /// Op and Mask are known to be the same type.\n  bool MaskedValueIsAllOnes(SDValue Op, const APInt &Mask,\n                            unsigned Depth = 0) const;\n\n  /// Determine which bits of Op are known to be either zero or one and return\n  /// them in Known. For vectors, the known bits are those that are shared by\n  /// every vector element.\n  /// Targets can implement the computeKnownBitsForTargetNode method in the\n  /// TargetLowering class to allow target nodes to be understood.\n  KnownBits computeKnownBits(SDValue Op, unsigned Depth = 0) const;\n\n  /// Determine which bits of Op are known to be either zero or one and return\n  /// them in Known. The DemandedElts argument allows us to only collect the\n  /// known bits that are shared by the requested vector elements.\n  /// Targets can implement the computeKnownBitsForTargetNode method in the\n  /// TargetLowering class to allow target nodes to be understood.\n  KnownBits computeKnownBits(SDValue Op, const APInt &DemandedElts,\n                             unsigned Depth = 0) const;\n\n  /// Used to represent the possible overflow behavior of an operation.\n  /// Never: the operation cannot overflow.\n  /// Always: the operation will always overflow.\n  /// Sometime: the operation may or may not overflow.\n  enum OverflowKind {\n    OFK_Never,\n    OFK_Sometime,\n    OFK_Always,\n  };\n\n  /// Determine if the result of the addition of 2 node can overflow.\n  OverflowKind computeOverflowKind(SDValue N0, SDValue N1) const;\n\n  /// Test if the given value is known to have exactly one bit set. This differs\n  /// from computeKnownBits in that it doesn't necessarily determine which bit\n  /// is set.\n  bool isKnownToBeAPowerOfTwo(SDValue Val) const;\n\n  /// Return the number of times the sign bit of the register is replicated into\n  /// the other bits. We know that at least 1 bit is always equal to the sign\n  /// bit (itself), but other cases can give us information. For example,\n  /// immediately after an \"SRA X, 2\", we know that the top 3 bits are all equal\n  /// to each other, so we return 3. Targets can implement the\n  /// ComputeNumSignBitsForTarget method in the TargetLowering class to allow\n  /// target nodes to be understood.\n  unsigned ComputeNumSignBits(SDValue Op, unsigned Depth = 0) const;\n\n  /// Return the number of times the sign bit of the register is replicated into\n  /// the other bits. We know that at least 1 bit is always equal to the sign\n  /// bit (itself), but other cases can give us information. For example,\n  /// immediately after an \"SRA X, 2\", we know that the top 3 bits are all equal\n  /// to each other, so we return 3. The DemandedElts argument allows\n  /// us to only collect the minimum sign bits of the requested vector elements.\n  /// Targets can implement the ComputeNumSignBitsForTarget method in the\n  /// TargetLowering class to allow target nodes to be understood.\n  unsigned ComputeNumSignBits(SDValue Op, const APInt &DemandedElts,\n                              unsigned Depth = 0) const;\n\n  /// Return true if the specified operand is an ISD::ADD with a ConstantSDNode\n  /// on the right-hand side, or if it is an ISD::OR with a ConstantSDNode that\n  /// is guaranteed to have the same semantics as an ADD. This handles the\n  /// equivalence:\n  ///     X|Cst == X+Cst iff X&Cst = 0.\n  bool isBaseWithConstantOffset(SDValue Op) const;\n\n  /// Test whether the given SDValue is known to never be NaN. If \\p SNaN is\n  /// true, returns if \\p Op is known to never be a signaling NaN (it may still\n  /// be a qNaN).\n  bool isKnownNeverNaN(SDValue Op, bool SNaN = false, unsigned Depth = 0) const;\n\n  /// \\returns true if \\p Op is known to never be a signaling NaN.\n  bool isKnownNeverSNaN(SDValue Op, unsigned Depth = 0) const {\n    return isKnownNeverNaN(Op, true, Depth);\n  }\n\n  /// Test whether the given floating point SDValue is known to never be\n  /// positive or negative zero.\n  bool isKnownNeverZeroFloat(SDValue Op) const;\n\n  /// Test whether the given SDValue is known to contain non-zero value(s).\n  bool isKnownNeverZero(SDValue Op) const;\n\n  /// Test whether two SDValues are known to compare equal. This\n  /// is true if they are the same value, or if one is negative zero and the\n  /// other positive zero.\n  bool isEqualTo(SDValue A, SDValue B) const;\n\n  /// Return true if A and B have no common bits set. As an example, this can\n  /// allow an 'add' to be transformed into an 'or'.\n  bool haveNoCommonBitsSet(SDValue A, SDValue B) const;\n\n  /// Test whether \\p V has a splatted value for all the demanded elements.\n  ///\n  /// On success \\p UndefElts will indicate the elements that have UNDEF\n  /// values instead of the splat value, this is only guaranteed to be correct\n  /// for \\p DemandedElts.\n  ///\n  /// NOTE: The function will return true for a demanded splat of UNDEF values.\n  bool isSplatValue(SDValue V, const APInt &DemandedElts, APInt &UndefElts,\n                    unsigned Depth = 0);\n\n  /// Test whether \\p V has a splatted value.\n  bool isSplatValue(SDValue V, bool AllowUndefs = false);\n\n  /// If V is a splatted value, return the source vector and its splat index.\n  SDValue getSplatSourceVector(SDValue V, int &SplatIndex);\n\n  /// If V is a splat vector, return its scalar source operand by extracting\n  /// that element from the source vector.\n  SDValue getSplatValue(SDValue V);\n\n  /// If a SHL/SRA/SRL node \\p V has a constant or splat constant shift amount\n  /// that is less than the element bit-width of the shift node, return it.\n  const APInt *getValidShiftAmountConstant(SDValue V,\n                                           const APInt &DemandedElts) const;\n\n  /// If a SHL/SRA/SRL node \\p V has constant shift amounts that are all less\n  /// than the element bit-width of the shift node, return the minimum value.\n  const APInt *\n  getValidMinimumShiftAmountConstant(SDValue V,\n                                     const APInt &DemandedElts) const;\n\n  /// If a SHL/SRA/SRL node \\p V has constant shift amounts that are all less\n  /// than the element bit-width of the shift node, return the maximum value.\n  const APInt *\n  getValidMaximumShiftAmountConstant(SDValue V,\n                                     const APInt &DemandedElts) const;\n\n  /// Match a binop + shuffle pyramid that represents a horizontal reduction\n  /// over the elements of a vector starting from the EXTRACT_VECTOR_ELT node /p\n  /// Extract. The reduction must use one of the opcodes listed in /p\n  /// CandidateBinOps and on success /p BinOp will contain the matching opcode.\n  /// Returns the vector that is being reduced on, or SDValue() if a reduction\n  /// was not matched. If \\p AllowPartials is set then in the case of a\n  /// reduction pattern that only matches the first few stages, the extracted\n  /// subvector of the start of the reduction is returned.\n  SDValue matchBinOpReduction(SDNode *Extract, ISD::NodeType &BinOp,\n                              ArrayRef<ISD::NodeType> CandidateBinOps,\n                              bool AllowPartials = false);\n\n  /// Utility function used by legalize and lowering to\n  /// \"unroll\" a vector operation by splitting out the scalars and operating\n  /// on each element individually.  If the ResNE is 0, fully unroll the vector\n  /// op. If ResNE is less than the width of the vector op, unroll up to ResNE.\n  /// If the  ResNE is greater than the width of the vector op, unroll the\n  /// vector op and fill the end of the resulting vector with UNDEFS.\n  SDValue UnrollVectorOp(SDNode *N, unsigned ResNE = 0);\n\n  /// Like UnrollVectorOp(), but for the [US](ADD|SUB|MUL)O family of opcodes.\n  /// This is a separate function because those opcodes have two results.\n  std::pair<SDValue, SDValue> UnrollVectorOverflowOp(SDNode *N,\n                                                     unsigned ResNE = 0);\n\n  /// Return true if loads are next to each other and can be\n  /// merged. Check that both are nonvolatile and if LD is loading\n  /// 'Bytes' bytes from a location that is 'Dist' units away from the\n  /// location that the 'Base' load is loading from.\n  bool areNonVolatileConsecutiveLoads(LoadSDNode *LD, LoadSDNode *Base,\n                                      unsigned Bytes, int Dist) const;\n\n  /// Infer alignment of a load / store address. Return None if it cannot be\n  /// inferred.\n  MaybeAlign InferPtrAlign(SDValue Ptr) const;\n\n  LLVM_ATTRIBUTE_DEPRECATED(inline unsigned InferPtrAlignment(SDValue Ptr)\n                                const,\n                            \"Use InferPtrAlign instead\") {\n    if (auto A = InferPtrAlign(Ptr))\n      return A->value();\n    return 0;\n  }\n\n  /// Compute the VTs needed for the low/hi parts of a type\n  /// which is split (or expanded) into two not necessarily identical pieces.\n  std::pair<EVT, EVT> GetSplitDestVTs(const EVT &VT) const;\n\n  /// Compute the VTs needed for the low/hi parts of a type, dependent on an\n  /// enveloping VT that has been split into two identical pieces. Sets the\n  /// HisIsEmpty flag when hi type has zero storage size.\n  std::pair<EVT, EVT> GetDependentSplitDestVTs(const EVT &VT, const EVT &EnvVT,\n                                               bool *HiIsEmpty) const;\n\n  /// Split the vector with EXTRACT_SUBVECTOR using the provides\n  /// VTs and return the low/high part.\n  std::pair<SDValue, SDValue> SplitVector(const SDValue &N, const SDLoc &DL,\n                                          const EVT &LoVT, const EVT &HiVT);\n\n  /// Split the vector with EXTRACT_SUBVECTOR and return the low/high part.\n  std::pair<SDValue, SDValue> SplitVector(const SDValue &N, const SDLoc &DL) {\n    EVT LoVT, HiVT;\n    std::tie(LoVT, HiVT) = GetSplitDestVTs(N.getValueType());\n    return SplitVector(N, DL, LoVT, HiVT);\n  }\n\n  /// Split the node's operand with EXTRACT_SUBVECTOR and\n  /// return the low/high part.\n  std::pair<SDValue, SDValue> SplitVectorOperand(const SDNode *N, unsigned OpNo)\n  {\n    return SplitVector(N->getOperand(OpNo), SDLoc(N));\n  }\n\n  /// Widen the vector up to the next power of two using INSERT_SUBVECTOR.\n  SDValue WidenVector(const SDValue &N, const SDLoc &DL);\n\n  /// Append the extracted elements from Start to Count out of the vector Op in\n  /// Args. If Count is 0, all of the elements will be extracted. The extracted\n  /// elements will have type EVT if it is provided, and otherwise their type\n  /// will be Op's element type.\n  void ExtractVectorElements(SDValue Op, SmallVectorImpl<SDValue> &Args,\n                             unsigned Start = 0, unsigned Count = 0,\n                             EVT EltVT = EVT());\n\n  /// Compute the default alignment value for the given type.\n  Align getEVTAlign(EVT MemoryVT) const;\n  /// Compute the default alignment value for the given type.\n  /// FIXME: Remove once transition to Align is over.\n  inline unsigned getEVTAlignment(EVT MemoryVT) const {\n    return getEVTAlign(MemoryVT).value();\n  }\n\n  /// Test whether the given value is a constant int or similar node.\n  SDNode *isConstantIntBuildVectorOrConstantInt(SDValue N) const;\n\n  /// Test whether the given value is a constant FP or similar node.\n  SDNode *isConstantFPBuildVectorOrConstantFP(SDValue N) const ;\n\n  /// \\returns true if \\p N is any kind of constant or build_vector of\n  /// constants, int or float. If a vector, it may not necessarily be a splat.\n  inline bool isConstantValueOfAnyType(SDValue N) const {\n    return isConstantIntBuildVectorOrConstantInt(N) ||\n           isConstantFPBuildVectorOrConstantFP(N);\n  }\n\n  void addCallSiteInfo(const SDNode *CallNode, CallSiteInfoImpl &&CallInfo) {\n    SDCallSiteDbgInfo[CallNode].CSInfo = std::move(CallInfo);\n  }\n\n  CallSiteInfo getSDCallSiteInfo(const SDNode *CallNode) {\n    auto I = SDCallSiteDbgInfo.find(CallNode);\n    if (I != SDCallSiteDbgInfo.end())\n      return std::move(I->second).CSInfo;\n    return CallSiteInfo();\n  }\n\n  void addHeapAllocSite(const SDNode *Node, MDNode *MD) {\n    SDCallSiteDbgInfo[Node].HeapAllocSite = MD;\n  }\n\n  /// Return the HeapAllocSite type associated with the SDNode, if it exists.\n  MDNode *getHeapAllocSite(const SDNode *Node) {\n    auto It = SDCallSiteDbgInfo.find(Node);\n    if (It == SDCallSiteDbgInfo.end())\n      return nullptr;\n    return It->second.HeapAllocSite;\n  }\n\n  void addNoMergeSiteInfo(const SDNode *Node, bool NoMerge) {\n    if (NoMerge)\n      SDCallSiteDbgInfo[Node].NoMerge = NoMerge;\n  }\n\n  bool getNoMergeSiteInfo(const SDNode *Node) {\n    auto I = SDCallSiteDbgInfo.find(Node);\n    if (I == SDCallSiteDbgInfo.end())\n      return false;\n    return I->second.NoMerge;\n  }\n\n  /// Return the current function's default denormal handling kind for the given\n  /// floating point type.\n  DenormalMode getDenormalMode(EVT VT) const {\n    return MF->getDenormalMode(EVTToAPFloatSemantics(VT));\n  }\n\n  bool shouldOptForSize() const;\n\n  /// Get the (commutative) neutral element for the given opcode, if it exists.\n  SDValue getNeutralElement(unsigned Opcode, const SDLoc &DL, EVT VT,\n                            SDNodeFlags Flags);\n\nprivate:\n  void InsertNode(SDNode *N);\n  bool RemoveNodeFromCSEMaps(SDNode *N);\n  void AddModifiedNodeToCSEMaps(SDNode *N);\n  SDNode *FindModifiedNodeSlot(SDNode *N, SDValue Op, void *&InsertPos);\n  SDNode *FindModifiedNodeSlot(SDNode *N, SDValue Op1, SDValue Op2,\n                               void *&InsertPos);\n  SDNode *FindModifiedNodeSlot(SDNode *N, ArrayRef<SDValue> Ops,\n                               void *&InsertPos);\n  SDNode *UpdateSDLocOnMergeSDNode(SDNode *N, const SDLoc &loc);\n\n  void DeleteNodeNotInCSEMaps(SDNode *N);\n  void DeallocateNode(SDNode *N);\n\n  void allnodes_clear();\n\n  /// Look up the node specified by ID in CSEMap.  If it exists, return it.  If\n  /// not, return the insertion token that will make insertion faster.  This\n  /// overload is for nodes other than Constant or ConstantFP, use the other one\n  /// for those.\n  SDNode *FindNodeOrInsertPos(const FoldingSetNodeID &ID, void *&InsertPos);\n\n  /// Look up the node specified by ID in CSEMap.  If it exists, return it.  If\n  /// not, return the insertion token that will make insertion faster.  Performs\n  /// additional processing for constant nodes.\n  SDNode *FindNodeOrInsertPos(const FoldingSetNodeID &ID, const SDLoc &DL,\n                              void *&InsertPos);\n\n  /// List of non-single value types.\n  FoldingSet<SDVTListNode> VTListMap;\n\n  /// Maps to auto-CSE operations.\n  std::vector<CondCodeSDNode*> CondCodeNodes;\n\n  std::vector<SDNode*> ValueTypeNodes;\n  std::map<EVT, SDNode*, EVT::compareRawBits> ExtendedValueTypeNodes;\n  StringMap<SDNode*> ExternalSymbols;\n\n  std::map<std::pair<std::string, unsigned>, SDNode *> TargetExternalSymbols;\n  DenseMap<MCSymbol *, SDNode *> MCSymbols;\n\n  FlagInserter *Inserter = nullptr;\n};\n\ntemplate <> struct GraphTraits<SelectionDAG*> : public GraphTraits<SDNode*> {\n  using nodes_iterator = pointer_iterator<SelectionDAG::allnodes_iterator>;\n\n  static nodes_iterator nodes_begin(SelectionDAG *G) {\n    return nodes_iterator(G->allnodes_begin());\n  }\n\n  static nodes_iterator nodes_end(SelectionDAG *G) {\n    return nodes_iterator(G->allnodes_end());\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_SELECTIONDAG_H\n"}, "52": {"id": 52, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "content": "//===- llvm/CodeGen/SelectionDAGNodes.h - SelectionDAG Nodes ----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file declares the SDNode class and derived classes, which are used to\n// represent the nodes and operations present in a SelectionDAG.  These nodes\n// and operations are machine code level operations, with some similarities to\n// the GCC RTL representation.\n//\n// Clients should include the SelectionDAG.h file instead of this file directly.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_SELECTIONDAGNODES_H\n#define LLVM_CODEGEN_SELECTIONDAGNODES_H\n\n#include \"llvm/ADT/APFloat.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/BitVector.h\"\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/ADT/GraphTraits.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/ilist_node.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/CodeGen/ISDOpcodes.h\"\n#include \"llvm/CodeGen/MachineMemOperand.h\"\n#include \"llvm/CodeGen/Register.h\"\n#include \"llvm/CodeGen/ValueTypes.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/Metadata.h\"\n#include \"llvm/IR/Operator.h\"\n#include \"llvm/Support/AlignOf.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include \"llvm/Support/TypeSize.h\"\n#include <algorithm>\n#include <cassert>\n#include <climits>\n#include <cstddef>\n#include <cstdint>\n#include <cstring>\n#include <iterator>\n#include <string>\n#include <tuple>\n\nnamespace llvm {\n\nclass APInt;\nclass Constant;\ntemplate <typename T> struct DenseMapInfo;\nclass GlobalValue;\nclass MachineBasicBlock;\nclass MachineConstantPoolValue;\nclass MCSymbol;\nclass raw_ostream;\nclass SDNode;\nclass SelectionDAG;\nclass Type;\nclass Value;\n\nvoid checkForCycles(const SDNode *N, const SelectionDAG *DAG = nullptr,\n                    bool force = false);\n\n/// This represents a list of ValueType's that has been intern'd by\n/// a SelectionDAG.  Instances of this simple value class are returned by\n/// SelectionDAG::getVTList(...).\n///\nstruct SDVTList {\n  const EVT *VTs;\n  unsigned int NumVTs;\n};\n\nnamespace ISD {\n\n  /// Node predicates\n\n/// If N is a BUILD_VECTOR or SPLAT_VECTOR node whose elements are all the\n/// same constant or undefined, return true and return the constant value in\n/// \\p SplatValue.\nbool isConstantSplatVector(const SDNode *N, APInt &SplatValue);\n\n/// Return true if the specified node is a BUILD_VECTOR or SPLAT_VECTOR where\n/// all of the elements are ~0 or undef. If \\p BuildVectorOnly is set to\n/// true, it only checks BUILD_VECTOR.\nbool isConstantSplatVectorAllOnes(const SDNode *N,\n                                  bool BuildVectorOnly = false);\n\n/// Return true if the specified node is a BUILD_VECTOR or SPLAT_VECTOR where\n/// all of the elements are 0 or undef. If \\p BuildVectorOnly is set to true, it\n/// only checks BUILD_VECTOR.\nbool isConstantSplatVectorAllZeros(const SDNode *N,\n                                   bool BuildVectorOnly = false);\n\n/// Return true if the specified node is a BUILD_VECTOR where all of the\n/// elements are ~0 or undef.\nbool isBuildVectorAllOnes(const SDNode *N);\n\n/// Return true if the specified node is a BUILD_VECTOR where all of the\n/// elements are 0 or undef.\nbool isBuildVectorAllZeros(const SDNode *N);\n\n/// Return true if the specified node is a BUILD_VECTOR node of all\n/// ConstantSDNode or undef.\nbool isBuildVectorOfConstantSDNodes(const SDNode *N);\n\n/// Return true if the specified node is a BUILD_VECTOR node of all\n/// ConstantFPSDNode or undef.\nbool isBuildVectorOfConstantFPSDNodes(const SDNode *N);\n\n/// Return true if the node has at least one operand and all operands of the\n/// specified node are ISD::UNDEF.\nbool allOperandsUndef(const SDNode *N);\n\n} // end namespace ISD\n\n//===----------------------------------------------------------------------===//\n/// Unlike LLVM values, Selection DAG nodes may return multiple\n/// values as the result of a computation.  Many nodes return multiple values,\n/// from loads (which define a token and a return value) to ADDC (which returns\n/// a result and a carry value), to calls (which may return an arbitrary number\n/// of values).\n///\n/// As such, each use of a SelectionDAG computation must indicate the node that\n/// computes it as well as which return value to use from that node.  This pair\n/// of information is represented with the SDValue value type.\n///\nclass SDValue {\n  friend struct DenseMapInfo<SDValue>;\n\n  SDNode *Node = nullptr; // The node defining the value we are using.\n  unsigned ResNo = 0;     // Which return value of the node we are using.\n\npublic:\n  SDValue() = default;\n  SDValue(SDNode *node, unsigned resno);\n\n  /// get the index which selects a specific result in the SDNode\n  unsigned getResNo() const { return ResNo; }\n\n  /// get the SDNode which holds the desired result\n  SDNode *getNode() const { return Node; }\n\n  /// set the SDNode\n  void setNode(SDNode *N) { Node = N; }\n\n  inline SDNode *operator->() const { return Node; }\n\n  bool operator==(const SDValue &O) const {\n    return Node == O.Node && ResNo == O.ResNo;\n  }\n  bool operator!=(const SDValue &O) const {\n    return !operator==(O);\n  }\n  bool operator<(const SDValue &O) const {\n    return std::tie(Node, ResNo) < std::tie(O.Node, O.ResNo);\n  }\n  explicit operator bool() const {\n    return Node != nullptr;\n  }\n\n  SDValue getValue(unsigned R) const {\n    return SDValue(Node, R);\n  }\n\n  /// Return true if this node is an operand of N.\n  bool isOperandOf(const SDNode *N) const;\n\n  /// Return the ValueType of the referenced return value.\n  inline EVT getValueType() const;\n\n  /// Return the simple ValueType of the referenced return value.\n  MVT getSimpleValueType() const {\n    return getValueType().getSimpleVT();\n  }\n\n  /// Returns the size of the value in bits.\n  ///\n  /// If the value type is a scalable vector type, the scalable property will\n  /// be set and the runtime size will be a positive integer multiple of the\n  /// base size.\n  TypeSize getValueSizeInBits() const {\n    return getValueType().getSizeInBits();\n  }\n\n  uint64_t getScalarValueSizeInBits() const {\n    return getValueType().getScalarType().getFixedSizeInBits();\n  }\n\n  // Forwarding methods - These forward to the corresponding methods in SDNode.\n  inline unsigned getOpcode() const;\n  inline unsigned getNumOperands() const;\n  inline const SDValue &getOperand(unsigned i) const;\n  inline uint64_t getConstantOperandVal(unsigned i) const;\n  inline const APInt &getConstantOperandAPInt(unsigned i) const;\n  inline bool isTargetMemoryOpcode() const;\n  inline bool isTargetOpcode() const;\n  inline bool isMachineOpcode() const;\n  inline bool isUndef() const;\n  inline unsigned getMachineOpcode() const;\n  inline const DebugLoc &getDebugLoc() const;\n  inline void dump() const;\n  inline void dump(const SelectionDAG *G) const;\n  inline void dumpr() const;\n  inline void dumpr(const SelectionDAG *G) const;\n\n  /// Return true if this operand (which must be a chain) reaches the\n  /// specified operand without crossing any side-effecting instructions.\n  /// In practice, this looks through token factors and non-volatile loads.\n  /// In order to remain efficient, this only\n  /// looks a couple of nodes in, it does not do an exhaustive search.\n  bool reachesChainWithoutSideEffects(SDValue Dest,\n                                      unsigned Depth = 2) const;\n\n  /// Return true if there are no nodes using value ResNo of Node.\n  inline bool use_empty() const;\n\n  /// Return true if there is exactly one node using value ResNo of Node.\n  inline bool hasOneUse() const;\n};\n\ntemplate<> struct DenseMapInfo<SDValue> {\n  static inline SDValue getEmptyKey() {\n    SDValue V;\n    V.ResNo = -1U;\n    return V;\n  }\n\n  static inline SDValue getTombstoneKey() {\n    SDValue V;\n    V.ResNo = -2U;\n    return V;\n  }\n\n  static unsigned getHashValue(const SDValue &Val) {\n    return ((unsigned)((uintptr_t)Val.getNode() >> 4) ^\n            (unsigned)((uintptr_t)Val.getNode() >> 9)) + Val.getResNo();\n  }\n\n  static bool isEqual(const SDValue &LHS, const SDValue &RHS) {\n    return LHS == RHS;\n  }\n};\n\n/// Allow casting operators to work directly on\n/// SDValues as if they were SDNode*'s.\ntemplate<> struct simplify_type<SDValue> {\n  using SimpleType = SDNode *;\n\n  static SimpleType getSimplifiedValue(SDValue &Val) {\n    return Val.getNode();\n  }\n};\ntemplate<> struct simplify_type<const SDValue> {\n  using SimpleType = /*const*/ SDNode *;\n\n  static SimpleType getSimplifiedValue(const SDValue &Val) {\n    return Val.getNode();\n  }\n};\n\n/// Represents a use of a SDNode. This class holds an SDValue,\n/// which records the SDNode being used and the result number, a\n/// pointer to the SDNode using the value, and Next and Prev pointers,\n/// which link together all the uses of an SDNode.\n///\nclass SDUse {\n  /// Val - The value being used.\n  SDValue Val;\n  /// User - The user of this value.\n  SDNode *User = nullptr;\n  /// Prev, Next - Pointers to the uses list of the SDNode referred by\n  /// this operand.\n  SDUse **Prev = nullptr;\n  SDUse *Next = nullptr;\n\npublic:\n  SDUse() = default;\n  SDUse(const SDUse &U) = delete;\n  SDUse &operator=(const SDUse &) = delete;\n\n  /// Normally SDUse will just implicitly convert to an SDValue that it holds.\n  operator const SDValue&() const { return Val; }\n\n  /// If implicit conversion to SDValue doesn't work, the get() method returns\n  /// the SDValue.\n  const SDValue &get() const { return Val; }\n\n  /// This returns the SDNode that contains this Use.\n  SDNode *getUser() { return User; }\n\n  /// Get the next SDUse in the use list.\n  SDUse *getNext() const { return Next; }\n\n  /// Convenience function for get().getNode().\n  SDNode *getNode() const { return Val.getNode(); }\n  /// Convenience function for get().getResNo().\n  unsigned getResNo() const { return Val.getResNo(); }\n  /// Convenience function for get().getValueType().\n  EVT getValueType() const { return Val.getValueType(); }\n\n  /// Convenience function for get().operator==\n  bool operator==(const SDValue &V) const {\n    return Val == V;\n  }\n\n  /// Convenience function for get().operator!=\n  bool operator!=(const SDValue &V) const {\n    return Val != V;\n  }\n\n  /// Convenience function for get().operator<\n  bool operator<(const SDValue &V) const {\n    return Val < V;\n  }\n\nprivate:\n  friend class SelectionDAG;\n  friend class SDNode;\n  // TODO: unfriend HandleSDNode once we fix its operand handling.\n  friend class HandleSDNode;\n\n  void setUser(SDNode *p) { User = p; }\n\n  /// Remove this use from its existing use list, assign it the\n  /// given value, and add it to the new value's node's use list.\n  inline void set(const SDValue &V);\n  /// Like set, but only supports initializing a newly-allocated\n  /// SDUse with a non-null value.\n  inline void setInitial(const SDValue &V);\n  /// Like set, but only sets the Node portion of the value,\n  /// leaving the ResNo portion unmodified.\n  inline void setNode(SDNode *N);\n\n  void addToList(SDUse **List) {\n    Next = *List;\n    if (Next) Next->Prev = &Next;\n    Prev = List;\n    *List = this;\n  }\n\n  void removeFromList() {\n    *Prev = Next;\n    if (Next) Next->Prev = Prev;\n  }\n};\n\n/// simplify_type specializations - Allow casting operators to work directly on\n/// SDValues as if they were SDNode*'s.\ntemplate<> struct simplify_type<SDUse> {\n  using SimpleType = SDNode *;\n\n  static SimpleType getSimplifiedValue(SDUse &Val) {\n    return Val.getNode();\n  }\n};\n\n/// These are IR-level optimization flags that may be propagated to SDNodes.\n/// TODO: This data structure should be shared by the IR optimizer and the\n/// the backend.\nstruct SDNodeFlags {\nprivate:\n  bool NoUnsignedWrap : 1;\n  bool NoSignedWrap : 1;\n  bool Exact : 1;\n  bool NoNaNs : 1;\n  bool NoInfs : 1;\n  bool NoSignedZeros : 1;\n  bool AllowReciprocal : 1;\n  bool AllowContract : 1;\n  bool ApproximateFuncs : 1;\n  bool AllowReassociation : 1;\n\n  // We assume instructions do not raise floating-point exceptions by default,\n  // and only those marked explicitly may do so.  We could choose to represent\n  // this via a positive \"FPExcept\" flags like on the MI level, but having a\n  // negative \"NoFPExcept\" flag here (that defaults to true) makes the flag\n  // intersection logic more straightforward.\n  bool NoFPExcept : 1;\n\npublic:\n  /// Default constructor turns off all optimization flags.\n  SDNodeFlags()\n      : NoUnsignedWrap(false), NoSignedWrap(false), Exact(false), NoNaNs(false),\n        NoInfs(false), NoSignedZeros(false), AllowReciprocal(false),\n        AllowContract(false), ApproximateFuncs(false),\n        AllowReassociation(false), NoFPExcept(false) {}\n\n  /// Propagate the fast-math-flags from an IR FPMathOperator.\n  void copyFMF(const FPMathOperator &FPMO) {\n    setNoNaNs(FPMO.hasNoNaNs());\n    setNoInfs(FPMO.hasNoInfs());\n    setNoSignedZeros(FPMO.hasNoSignedZeros());\n    setAllowReciprocal(FPMO.hasAllowReciprocal());\n    setAllowContract(FPMO.hasAllowContract());\n    setApproximateFuncs(FPMO.hasApproxFunc());\n    setAllowReassociation(FPMO.hasAllowReassoc());\n  }\n\n  // These are mutators for each flag.\n  void setNoUnsignedWrap(bool b) { NoUnsignedWrap = b; }\n  void setNoSignedWrap(bool b) { NoSignedWrap = b; }\n  void setExact(bool b) { Exact = b; }\n  void setNoNaNs(bool b) { NoNaNs = b; }\n  void setNoInfs(bool b) { NoInfs = b; }\n  void setNoSignedZeros(bool b) { NoSignedZeros = b; }\n  void setAllowReciprocal(bool b) { AllowReciprocal = b; }\n  void setAllowContract(bool b) { AllowContract = b; }\n  void setApproximateFuncs(bool b) { ApproximateFuncs = b; }\n  void setAllowReassociation(bool b) { AllowReassociation = b; }\n  void setNoFPExcept(bool b) { NoFPExcept = b; }\n\n  // These are accessors for each flag.\n  bool hasNoUnsignedWrap() const { return NoUnsignedWrap; }\n  bool hasNoSignedWrap() const { return NoSignedWrap; }\n  bool hasExact() const { return Exact; }\n  bool hasNoNaNs() const { return NoNaNs; }\n  bool hasNoInfs() const { return NoInfs; }\n  bool hasNoSignedZeros() const { return NoSignedZeros; }\n  bool hasAllowReciprocal() const { return AllowReciprocal; }\n  bool hasAllowContract() const { return AllowContract; }\n  bool hasApproximateFuncs() const { return ApproximateFuncs; }\n  bool hasAllowReassociation() const { return AllowReassociation; }\n  bool hasNoFPExcept() const { return NoFPExcept; }\n\n  /// Clear any flags in this flag set that aren't also set in Flags. All\n  /// flags will be cleared if Flags are undefined.\n  void intersectWith(const SDNodeFlags Flags) {\n    NoUnsignedWrap &= Flags.NoUnsignedWrap;\n    NoSignedWrap &= Flags.NoSignedWrap;\n    Exact &= Flags.Exact;\n    NoNaNs &= Flags.NoNaNs;\n    NoInfs &= Flags.NoInfs;\n    NoSignedZeros &= Flags.NoSignedZeros;\n    AllowReciprocal &= Flags.AllowReciprocal;\n    AllowContract &= Flags.AllowContract;\n    ApproximateFuncs &= Flags.ApproximateFuncs;\n    AllowReassociation &= Flags.AllowReassociation;\n    NoFPExcept &= Flags.NoFPExcept;\n  }\n};\n\n/// Represents one node in the SelectionDAG.\n///\nclass SDNode : public FoldingSetNode, public ilist_node<SDNode> {\nprivate:\n  /// The operation that this node performs.\n  int16_t NodeType;\n\nprotected:\n  // We define a set of mini-helper classes to help us interpret the bits in our\n  // SubclassData.  These are designed to fit within a uint16_t so they pack\n  // with NodeType.\n\n#if defined(_AIX) && (!defined(__GNUC__) || defined(__ibmxl__))\n// Except for GCC; by default, AIX compilers store bit-fields in 4-byte words\n// and give the `pack` pragma push semantics.\n#define BEGIN_TWO_BYTE_PACK() _Pragma(\"pack(2)\")\n#define END_TWO_BYTE_PACK() _Pragma(\"pack(pop)\")\n#else\n#define BEGIN_TWO_BYTE_PACK()\n#define END_TWO_BYTE_PACK()\n#endif\n\nBEGIN_TWO_BYTE_PACK()\n  class SDNodeBitfields {\n    friend class SDNode;\n    friend class MemIntrinsicSDNode;\n    friend class MemSDNode;\n    friend class SelectionDAG;\n\n    uint16_t HasDebugValue : 1;\n    uint16_t IsMemIntrinsic : 1;\n    uint16_t IsDivergent : 1;\n  };\n  enum { NumSDNodeBits = 3 };\n\n  class ConstantSDNodeBitfields {\n    friend class ConstantSDNode;\n\n    uint16_t : NumSDNodeBits;\n\n    uint16_t IsOpaque : 1;\n  };\n\n  class MemSDNodeBitfields {\n    friend class MemSDNode;\n    friend class MemIntrinsicSDNode;\n    friend class AtomicSDNode;\n\n    uint16_t : NumSDNodeBits;\n\n    uint16_t IsVolatile : 1;\n    uint16_t IsNonTemporal : 1;\n    uint16_t IsDereferenceable : 1;\n    uint16_t IsInvariant : 1;\n  };\n  enum { NumMemSDNodeBits = NumSDNodeBits + 4 };\n\n  class LSBaseSDNodeBitfields {\n    friend class LSBaseSDNode;\n    friend class MaskedLoadStoreSDNode;\n    friend class MaskedGatherScatterSDNode;\n\n    uint16_t : NumMemSDNodeBits;\n\n    // This storage is shared between disparate class hierarchies to hold an\n    // enumeration specific to the class hierarchy in use.\n    //   LSBaseSDNode => enum ISD::MemIndexedMode\n    //   MaskedLoadStoreBaseSDNode => enum ISD::MemIndexedMode\n    //   MaskedGatherScatterSDNode => enum ISD::MemIndexType\n    uint16_t AddressingMode : 3;\n  };\n  enum { NumLSBaseSDNodeBits = NumMemSDNodeBits + 3 };\n\n  class LoadSDNodeBitfields {\n    friend class LoadSDNode;\n    friend class MaskedLoadSDNode;\n    friend class MaskedGatherSDNode;\n\n    uint16_t : NumLSBaseSDNodeBits;\n\n    uint16_t ExtTy : 2; // enum ISD::LoadExtType\n    uint16_t IsExpanding : 1;\n  };\n\n  class StoreSDNodeBitfields {\n    friend class StoreSDNode;\n    friend class MaskedStoreSDNode;\n    friend class MaskedScatterSDNode;\n\n    uint16_t : NumLSBaseSDNodeBits;\n\n    uint16_t IsTruncating : 1;\n    uint16_t IsCompressing : 1;\n  };\n\n  union {\n    char RawSDNodeBits[sizeof(uint16_t)];\n    SDNodeBitfields SDNodeBits;\n    ConstantSDNodeBitfields ConstantSDNodeBits;\n    MemSDNodeBitfields MemSDNodeBits;\n    LSBaseSDNodeBitfields LSBaseSDNodeBits;\n    LoadSDNodeBitfields LoadSDNodeBits;\n    StoreSDNodeBitfields StoreSDNodeBits;\n  };\nEND_TWO_BYTE_PACK()\n#undef BEGIN_TWO_BYTE_PACK\n#undef END_TWO_BYTE_PACK\n\n  // RawSDNodeBits must cover the entirety of the union.  This means that all of\n  // the union's members must have size <= RawSDNodeBits.  We write the RHS as\n  // \"2\" instead of sizeof(RawSDNodeBits) because MSVC can't handle the latter.\n  static_assert(sizeof(SDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(ConstantSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(MemSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(LSBaseSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(LoadSDNodeBitfields) <= 2, \"field too wide\");\n  static_assert(sizeof(StoreSDNodeBitfields) <= 2, \"field too wide\");\n\nprivate:\n  friend class SelectionDAG;\n  // TODO: unfriend HandleSDNode once we fix its operand handling.\n  friend class HandleSDNode;\n\n  /// Unique id per SDNode in the DAG.\n  int NodeId = -1;\n\n  /// The values that are used by this operation.\n  SDUse *OperandList = nullptr;\n\n  /// The types of the values this node defines.  SDNode's may\n  /// define multiple values simultaneously.\n  const EVT *ValueList;\n\n  /// List of uses for this SDNode.\n  SDUse *UseList = nullptr;\n\n  /// The number of entries in the Operand/Value list.\n  unsigned short NumOperands = 0;\n  unsigned short NumValues;\n\n  // The ordering of the SDNodes. It roughly corresponds to the ordering of the\n  // original LLVM instructions.\n  // This is used for turning off scheduling, because we'll forgo\n  // the normal scheduling algorithms and output the instructions according to\n  // this ordering.\n  unsigned IROrder;\n\n  /// Source line information.\n  DebugLoc debugLoc;\n\n  /// Return a pointer to the specified value type.\n  static const EVT *getValueTypeList(EVT VT);\n\n  SDNodeFlags Flags;\n\npublic:\n  /// Unique and persistent id per SDNode in the DAG.\n  /// Used for debug printing.\n  uint16_t PersistentId;\n\n  //===--------------------------------------------------------------------===//\n  //  Accessors\n  //\n\n  /// Return the SelectionDAG opcode value for this node. For\n  /// pre-isel nodes (those for which isMachineOpcode returns false), these\n  /// are the opcode values in the ISD and <target>ISD namespaces. For\n  /// post-isel opcodes, see getMachineOpcode.\n  unsigned getOpcode()  const { return (unsigned short)NodeType; }\n\n  /// Test if this node has a target-specific opcode (in the\n  /// \\<target\\>ISD namespace).\n  bool isTargetOpcode() const { return NodeType >= ISD::BUILTIN_OP_END; }\n\n  /// Test if this node has a target-specific opcode that may raise\n  /// FP exceptions (in the \\<target\\>ISD namespace and greater than\n  /// FIRST_TARGET_STRICTFP_OPCODE).  Note that all target memory\n  /// opcode are currently automatically considered to possibly raise\n  /// FP exceptions as well.\n  bool isTargetStrictFPOpcode() const {\n    return NodeType >= ISD::FIRST_TARGET_STRICTFP_OPCODE;\n  }\n\n  /// Test if this node has a target-specific\n  /// memory-referencing opcode (in the \\<target\\>ISD namespace and\n  /// greater than FIRST_TARGET_MEMORY_OPCODE).\n  bool isTargetMemoryOpcode() const {\n    return NodeType >= ISD::FIRST_TARGET_MEMORY_OPCODE;\n  }\n\n  /// Return true if the type of the node type undefined.\n  bool isUndef() const { return NodeType == ISD::UNDEF; }\n\n  /// Test if this node is a memory intrinsic (with valid pointer information).\n  /// INTRINSIC_W_CHAIN and INTRINSIC_VOID nodes are sometimes created for\n  /// non-memory intrinsics (with chains) that are not really instances of\n  /// MemSDNode. For such nodes, we need some extra state to determine the\n  /// proper classof relationship.\n  bool isMemIntrinsic() const {\n    return (NodeType == ISD::INTRINSIC_W_CHAIN ||\n            NodeType == ISD::INTRINSIC_VOID) &&\n           SDNodeBits.IsMemIntrinsic;\n  }\n\n  /// Test if this node is a strict floating point pseudo-op.\n  bool isStrictFPOpcode() {\n    switch (NodeType) {\n      default:\n        return false;\n      case ISD::STRICT_FP16_TO_FP:\n      case ISD::STRICT_FP_TO_FP16:\n#define DAG_INSTRUCTION(NAME, NARG, ROUND_MODE, INTRINSIC, DAGN)               \\\n      case ISD::STRICT_##DAGN:\n#include \"llvm/IR/ConstrainedOps.def\"\n        return true;\n    }\n  }\n\n  /// Test if this node has a post-isel opcode, directly\n  /// corresponding to a MachineInstr opcode.\n  bool isMachineOpcode() const { return NodeType < 0; }\n\n  /// This may only be called if isMachineOpcode returns\n  /// true. It returns the MachineInstr opcode value that the node's opcode\n  /// corresponds to.\n  unsigned getMachineOpcode() const {\n    assert(isMachineOpcode() && \"Not a MachineInstr opcode!\");\n    return ~NodeType;\n  }\n\n  bool getHasDebugValue() const { return SDNodeBits.HasDebugValue; }\n  void setHasDebugValue(bool b) { SDNodeBits.HasDebugValue = b; }\n\n  bool isDivergent() const { return SDNodeBits.IsDivergent; }\n\n  /// Return true if there are no uses of this node.\n  bool use_empty() const { return UseList == nullptr; }\n\n  /// Return true if there is exactly one use of this node.\n  bool hasOneUse() const { return hasSingleElement(uses()); }\n\n  /// Return the number of uses of this node. This method takes\n  /// time proportional to the number of uses.\n  size_t use_size() const { return std::distance(use_begin(), use_end()); }\n\n  /// Return the unique node id.\n  int getNodeId() const { return NodeId; }\n\n  /// Set unique node id.\n  void setNodeId(int Id) { NodeId = Id; }\n\n  /// Return the node ordering.\n  unsigned getIROrder() const { return IROrder; }\n\n  /// Set the node ordering.\n  void setIROrder(unsigned Order) { IROrder = Order; }\n\n  /// Return the source location info.\n  const DebugLoc &getDebugLoc() const { return debugLoc; }\n\n  /// Set source location info.  Try to avoid this, putting\n  /// it in the constructor is preferable.\n  void setDebugLoc(DebugLoc dl) { debugLoc = std::move(dl); }\n\n  /// This class provides iterator support for SDUse\n  /// operands that use a specific SDNode.\n  class use_iterator\n    : public std::iterator<std::forward_iterator_tag, SDUse, ptrdiff_t> {\n    friend class SDNode;\n\n    SDUse *Op = nullptr;\n\n    explicit use_iterator(SDUse *op) : Op(op) {}\n\n  public:\n    using reference = std::iterator<std::forward_iterator_tag,\n                                    SDUse, ptrdiff_t>::reference;\n    using pointer = std::iterator<std::forward_iterator_tag,\n                                  SDUse, ptrdiff_t>::pointer;\n\n    use_iterator() = default;\n    use_iterator(const use_iterator &I) : Op(I.Op) {}\n\n    bool operator==(const use_iterator &x) const {\n      return Op == x.Op;\n    }\n    bool operator!=(const use_iterator &x) const {\n      return !operator==(x);\n    }\n\n    /// Return true if this iterator is at the end of uses list.\n    bool atEnd() const { return Op == nullptr; }\n\n    // Iterator traversal: forward iteration only.\n    use_iterator &operator++() {          // Preincrement\n      assert(Op && \"Cannot increment end iterator!\");\n      Op = Op->getNext();\n      return *this;\n    }\n\n    use_iterator operator++(int) {        // Postincrement\n      use_iterator tmp = *this; ++*this; return tmp;\n    }\n\n    /// Retrieve a pointer to the current user node.\n    SDNode *operator*() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      return Op->getUser();\n    }\n\n    SDNode *operator->() const { return operator*(); }\n\n    SDUse &getUse() const { return *Op; }\n\n    /// Retrieve the operand # of this use in its user.\n    unsigned getOperandNo() const {\n      assert(Op && \"Cannot dereference end iterator!\");\n      return (unsigned)(Op - Op->getUser()->OperandList);\n    }\n  };\n\n  /// Provide iteration support to walk over all uses of an SDNode.\n  use_iterator use_begin() const {\n    return use_iterator(UseList);\n  }\n\n  static use_iterator use_end() { return use_iterator(nullptr); }\n\n  inline iterator_range<use_iterator> uses() {\n    return make_range(use_begin(), use_end());\n  }\n  inline iterator_range<use_iterator> uses() const {\n    return make_range(use_begin(), use_end());\n  }\n\n  /// Return true if there are exactly NUSES uses of the indicated value.\n  /// This method ignores uses of other values defined by this operation.\n  bool hasNUsesOfValue(unsigned NUses, unsigned Value) const;\n\n  /// Return true if there are any use of the indicated value.\n  /// This method ignores uses of other values defined by this operation.\n  bool hasAnyUseOfValue(unsigned Value) const;\n\n  /// Return true if this node is the only use of N.\n  bool isOnlyUserOf(const SDNode *N) const;\n\n  /// Return true if this node is an operand of N.\n  bool isOperandOf(const SDNode *N) const;\n\n  /// Return true if this node is a predecessor of N.\n  /// NOTE: Implemented on top of hasPredecessor and every bit as\n  /// expensive. Use carefully.\n  bool isPredecessorOf(const SDNode *N) const {\n    return N->hasPredecessor(this);\n  }\n\n  /// Return true if N is a predecessor of this node.\n  /// N is either an operand of this node, or can be reached by recursively\n  /// traversing up the operands.\n  /// NOTE: This is an expensive method. Use it carefully.\n  bool hasPredecessor(const SDNode *N) const;\n\n  /// Returns true if N is a predecessor of any node in Worklist. This\n  /// helper keeps Visited and Worklist sets externally to allow unions\n  /// searches to be performed in parallel, caching of results across\n  /// queries and incremental addition to Worklist. Stops early if N is\n  /// found but will resume. Remember to clear Visited and Worklists\n  /// if DAG changes. MaxSteps gives a maximum number of nodes to visit before\n  /// giving up. The TopologicalPrune flag signals that positive NodeIds are\n  /// topologically ordered (Operands have strictly smaller node id) and search\n  /// can be pruned leveraging this.\n  static bool hasPredecessorHelper(const SDNode *N,\n                                   SmallPtrSetImpl<const SDNode *> &Visited,\n                                   SmallVectorImpl<const SDNode *> &Worklist,\n                                   unsigned int MaxSteps = 0,\n                                   bool TopologicalPrune = false) {\n    SmallVector<const SDNode *, 8> DeferredNodes;\n    if (Visited.count(N))\n      return true;\n\n    // Node Id's are assigned in three places: As a topological\n    // ordering (> 0), during legalization (results in values set to\n    // 0), new nodes (set to -1). If N has a topolgical id then we\n    // know that all nodes with ids smaller than it cannot be\n    // successors and we need not check them. Filter out all node\n    // that can't be matches. We add them to the worklist before exit\n    // in case of multiple calls. Note that during selection the topological id\n    // may be violated if a node's predecessor is selected before it. We mark\n    // this at selection negating the id of unselected successors and\n    // restricting topological pruning to positive ids.\n\n    int NId = N->getNodeId();\n    // If we Invalidated the Id, reconstruct original NId.\n    if (NId < -1)\n      NId = -(NId + 1);\n\n    bool Found = false;\n    while (!Worklist.empty()) {\n      const SDNode *M = Worklist.pop_back_val();\n      int MId = M->getNodeId();\n      if (TopologicalPrune && M->getOpcode() != ISD::TokenFactor && (NId > 0) &&\n          (MId > 0) && (MId < NId)) {\n        DeferredNodes.push_back(M);\n        continue;\n      }\n      for (const SDValue &OpV : M->op_values()) {\n        SDNode *Op = OpV.getNode();\n        if (Visited.insert(Op).second)\n          Worklist.push_back(Op);\n        if (Op == N)\n          Found = true;\n      }\n      if (Found)\n        break;\n      if (MaxSteps != 0 && Visited.size() >= MaxSteps)\n        break;\n    }\n    // Push deferred nodes back on worklist.\n    Worklist.append(DeferredNodes.begin(), DeferredNodes.end());\n    // If we bailed early, conservatively return found.\n    if (MaxSteps != 0 && Visited.size() >= MaxSteps)\n      return true;\n    return Found;\n  }\n\n  /// Return true if all the users of N are contained in Nodes.\n  /// NOTE: Requires at least one match, but doesn't require them all.\n  static bool areOnlyUsersOf(ArrayRef<const SDNode *> Nodes, const SDNode *N);\n\n  /// Return the number of values used by this operation.\n  unsigned getNumOperands() const { return NumOperands; }\n\n  /// Return the maximum number of operands that a SDNode can hold.\n  static constexpr size_t getMaxNumOperands() {\n    return std::numeric_limits<decltype(SDNode::NumOperands)>::max();\n  }\n\n  /// Helper method returns the integer value of a ConstantSDNode operand.\n  inline uint64_t getConstantOperandVal(unsigned Num) const;\n\n  /// Helper method returns the APInt of a ConstantSDNode operand.\n  inline const APInt &getConstantOperandAPInt(unsigned Num) const;\n\n  const SDValue &getOperand(unsigned Num) const {\n    assert(Num < NumOperands && \"Invalid child # of SDNode!\");\n    return OperandList[Num];\n  }\n\n  using op_iterator = SDUse *;\n\n  op_iterator op_begin() const { return OperandList; }\n  op_iterator op_end() const { return OperandList+NumOperands; }\n  ArrayRef<SDUse> ops() const { return makeArrayRef(op_begin(), op_end()); }\n\n  /// Iterator for directly iterating over the operand SDValue's.\n  struct value_op_iterator\n      : iterator_adaptor_base<value_op_iterator, op_iterator,\n                              std::random_access_iterator_tag, SDValue,\n                              ptrdiff_t, value_op_iterator *,\n                              value_op_iterator *> {\n    explicit value_op_iterator(SDUse *U = nullptr)\n      : iterator_adaptor_base(U) {}\n\n    const SDValue &operator*() const { return I->get(); }\n  };\n\n  iterator_range<value_op_iterator> op_values() const {\n    return make_range(value_op_iterator(op_begin()),\n                      value_op_iterator(op_end()));\n  }\n\n  SDVTList getVTList() const {\n    SDVTList X = { ValueList, NumValues };\n    return X;\n  }\n\n  /// If this node has a glue operand, return the node\n  /// to which the glue operand points. Otherwise return NULL.\n  SDNode *getGluedNode() const {\n    if (getNumOperands() != 0 &&\n        getOperand(getNumOperands()-1).getValueType() == MVT::Glue)\n      return getOperand(getNumOperands()-1).getNode();\n    return nullptr;\n  }\n\n  /// If this node has a glue value with a user, return\n  /// the user (there is at most one). Otherwise return NULL.\n  SDNode *getGluedUser() const {\n    for (use_iterator UI = use_begin(), UE = use_end(); UI != UE; ++UI)\n      if (UI.getUse().get().getValueType() == MVT::Glue)\n        return *UI;\n    return nullptr;\n  }\n\n  SDNodeFlags getFlags() const { return Flags; }\n  void setFlags(SDNodeFlags NewFlags) { Flags = NewFlags; }\n\n  /// Clear any flags in this node that aren't also set in Flags.\n  /// If Flags is not in a defined state then this has no effect.\n  void intersectFlagsWith(const SDNodeFlags Flags);\n\n  /// Return the number of values defined/returned by this operator.\n  unsigned getNumValues() const { return NumValues; }\n\n  /// Return the type of a specified result.\n  EVT getValueType(unsigned ResNo) const {\n    assert(ResNo < NumValues && \"Illegal result number!\");\n    return ValueList[ResNo];\n  }\n\n  /// Return the type of a specified result as a simple type.\n  MVT getSimpleValueType(unsigned ResNo) const {\n    return getValueType(ResNo).getSimpleVT();\n  }\n\n  /// Returns MVT::getSizeInBits(getValueType(ResNo)).\n  ///\n  /// If the value type is a scalable vector type, the scalable property will\n  /// be set and the runtime size will be a positive integer multiple of the\n  /// base size.\n  TypeSize getValueSizeInBits(unsigned ResNo) const {\n    return getValueType(ResNo).getSizeInBits();\n  }\n\n  using value_iterator = const EVT *;\n\n  value_iterator value_begin() const { return ValueList; }\n  value_iterator value_end() const { return ValueList+NumValues; }\n  iterator_range<value_iterator> values() const {\n    return llvm::make_range(value_begin(), value_end());\n  }\n\n  /// Return the opcode of this operation for printing.\n  std::string getOperationName(const SelectionDAG *G = nullptr) const;\n  static const char* getIndexedModeName(ISD::MemIndexedMode AM);\n  void print_types(raw_ostream &OS, const SelectionDAG *G) const;\n  void print_details(raw_ostream &OS, const SelectionDAG *G) const;\n  void print(raw_ostream &OS, const SelectionDAG *G = nullptr) const;\n  void printr(raw_ostream &OS, const SelectionDAG *G = nullptr) const;\n\n  /// Print a SelectionDAG node and all children down to\n  /// the leaves.  The given SelectionDAG allows target-specific nodes\n  /// to be printed in human-readable form.  Unlike printr, this will\n  /// print the whole DAG, including children that appear multiple\n  /// times.\n  ///\n  void printrFull(raw_ostream &O, const SelectionDAG *G = nullptr) const;\n\n  /// Print a SelectionDAG node and children up to\n  /// depth \"depth.\"  The given SelectionDAG allows target-specific\n  /// nodes to be printed in human-readable form.  Unlike printr, this\n  /// will print children that appear multiple times wherever they are\n  /// used.\n  ///\n  void printrWithDepth(raw_ostream &O, const SelectionDAG *G = nullptr,\n                       unsigned depth = 100) const;\n\n  /// Dump this node, for debugging.\n  void dump() const;\n\n  /// Dump (recursively) this node and its use-def subgraph.\n  void dumpr() const;\n\n  /// Dump this node, for debugging.\n  /// The given SelectionDAG allows target-specific nodes to be printed\n  /// in human-readable form.\n  void dump(const SelectionDAG *G) const;\n\n  /// Dump (recursively) this node and its use-def subgraph.\n  /// The given SelectionDAG allows target-specific nodes to be printed\n  /// in human-readable form.\n  void dumpr(const SelectionDAG *G) const;\n\n  /// printrFull to dbgs().  The given SelectionDAG allows\n  /// target-specific nodes to be printed in human-readable form.\n  /// Unlike dumpr, this will print the whole DAG, including children\n  /// that appear multiple times.\n  void dumprFull(const SelectionDAG *G = nullptr) const;\n\n  /// printrWithDepth to dbgs().  The given\n  /// SelectionDAG allows target-specific nodes to be printed in\n  /// human-readable form.  Unlike dumpr, this will print children\n  /// that appear multiple times wherever they are used.\n  ///\n  void dumprWithDepth(const SelectionDAG *G = nullptr,\n                      unsigned depth = 100) const;\n\n  /// Gather unique data for the node.\n  void Profile(FoldingSetNodeID &ID) const;\n\n  /// This method should only be used by the SDUse class.\n  void addUse(SDUse &U) { U.addToList(&UseList); }\n\nprotected:\n  static SDVTList getSDVTList(EVT VT) {\n    SDVTList Ret = { getValueTypeList(VT), 1 };\n    return Ret;\n  }\n\n  /// Create an SDNode.\n  ///\n  /// SDNodes are created without any operands, and never own the operand\n  /// storage. To add operands, see SelectionDAG::createOperands.\n  SDNode(unsigned Opc, unsigned Order, DebugLoc dl, SDVTList VTs)\n      : NodeType(Opc), ValueList(VTs.VTs), NumValues(VTs.NumVTs),\n        IROrder(Order), debugLoc(std::move(dl)) {\n    memset(&RawSDNodeBits, 0, sizeof(RawSDNodeBits));\n    assert(debugLoc.hasTrivialDestructor() && \"Expected trivial destructor\");\n    assert(NumValues == VTs.NumVTs &&\n           \"NumValues wasn't wide enough for its operands!\");\n  }\n\n  /// Release the operands and set this node to have zero operands.\n  void DropOperands();\n};\n\n/// Wrapper class for IR location info (IR ordering and DebugLoc) to be passed\n/// into SDNode creation functions.\n/// When an SDNode is created from the DAGBuilder, the DebugLoc is extracted\n/// from the original Instruction, and IROrder is the ordinal position of\n/// the instruction.\n/// When an SDNode is created after the DAG is being built, both DebugLoc and\n/// the IROrder are propagated from the original SDNode.\n/// So SDLoc class provides two constructors besides the default one, one to\n/// be used by the DAGBuilder, the other to be used by others.\nclass SDLoc {\nprivate:\n  DebugLoc DL;\n  int IROrder = 0;\n\npublic:\n  SDLoc() = default;\n  SDLoc(const SDNode *N) : DL(N->getDebugLoc()), IROrder(N->getIROrder()) {}\n  SDLoc(const SDValue V) : SDLoc(V.getNode()) {}\n  SDLoc(const Instruction *I, int Order) : IROrder(Order) {\n    assert(Order >= 0 && \"bad IROrder\");\n    if (I)\n      DL = I->getDebugLoc();\n  }\n\n  unsigned getIROrder() const { return IROrder; }\n  const DebugLoc &getDebugLoc() const { return DL; }\n};\n\n// Define inline functions from the SDValue class.\n\ninline SDValue::SDValue(SDNode *node, unsigned resno)\n    : Node(node), ResNo(resno) {\n  // Explicitly check for !ResNo to avoid use-after-free, because there are\n  // callers that use SDValue(N, 0) with a deleted N to indicate successful\n  // combines.\n  assert((!Node || !ResNo || ResNo < Node->getNumValues()) &&\n         \"Invalid result number for the given node!\");\n  assert(ResNo < -2U && \"Cannot use result numbers reserved for DenseMaps.\");\n}\n\ninline unsigned SDValue::getOpcode() const {\n  return Node->getOpcode();\n}\n\ninline EVT SDValue::getValueType() const {\n  return Node->getValueType(ResNo);\n}\n\ninline unsigned SDValue::getNumOperands() const {\n  return Node->getNumOperands();\n}\n\ninline const SDValue &SDValue::getOperand(unsigned i) const {\n  return Node->getOperand(i);\n}\n\ninline uint64_t SDValue::getConstantOperandVal(unsigned i) const {\n  return Node->getConstantOperandVal(i);\n}\n\ninline const APInt &SDValue::getConstantOperandAPInt(unsigned i) const {\n  return Node->getConstantOperandAPInt(i);\n}\n\ninline bool SDValue::isTargetOpcode() const {\n  return Node->isTargetOpcode();\n}\n\ninline bool SDValue::isTargetMemoryOpcode() const {\n  return Node->isTargetMemoryOpcode();\n}\n\ninline bool SDValue::isMachineOpcode() const {\n  return Node->isMachineOpcode();\n}\n\ninline unsigned SDValue::getMachineOpcode() const {\n  return Node->getMachineOpcode();\n}\n\ninline bool SDValue::isUndef() const {\n  return Node->isUndef();\n}\n\ninline bool SDValue::use_empty() const {\n  return !Node->hasAnyUseOfValue(ResNo);\n}\n\ninline bool SDValue::hasOneUse() const {\n  return Node->hasNUsesOfValue(1, ResNo);\n}\n\ninline const DebugLoc &SDValue::getDebugLoc() const {\n  return Node->getDebugLoc();\n}\n\ninline void SDValue::dump() const {\n  return Node->dump();\n}\n\ninline void SDValue::dump(const SelectionDAG *G) const {\n  return Node->dump(G);\n}\n\ninline void SDValue::dumpr() const {\n  return Node->dumpr();\n}\n\ninline void SDValue::dumpr(const SelectionDAG *G) const {\n  return Node->dumpr(G);\n}\n\n// Define inline functions from the SDUse class.\n\ninline void SDUse::set(const SDValue &V) {\n  if (Val.getNode()) removeFromList();\n  Val = V;\n  if (V.getNode()) V.getNode()->addUse(*this);\n}\n\ninline void SDUse::setInitial(const SDValue &V) {\n  Val = V;\n  V.getNode()->addUse(*this);\n}\n\ninline void SDUse::setNode(SDNode *N) {\n  if (Val.getNode()) removeFromList();\n  Val.setNode(N);\n  if (N) N->addUse(*this);\n}\n\n/// This class is used to form a handle around another node that\n/// is persistent and is updated across invocations of replaceAllUsesWith on its\n/// operand.  This node should be directly created by end-users and not added to\n/// the AllNodes list.\nclass HandleSDNode : public SDNode {\n  SDUse Op;\n\npublic:\n  explicit HandleSDNode(SDValue X)\n    : SDNode(ISD::HANDLENODE, 0, DebugLoc(), getSDVTList(MVT::Other)) {\n    // HandleSDNodes are never inserted into the DAG, so they won't be\n    // auto-numbered. Use ID 65535 as a sentinel.\n    PersistentId = 0xffff;\n\n    // Manually set up the operand list. This node type is special in that it's\n    // always stack allocated and SelectionDAG does not manage its operands.\n    // TODO: This should either (a) not be in the SDNode hierarchy, or (b) not\n    // be so special.\n    Op.setUser(this);\n    Op.setInitial(X);\n    NumOperands = 1;\n    OperandList = &Op;\n  }\n  ~HandleSDNode();\n\n  const SDValue &getValue() const { return Op; }\n};\n\nclass AddrSpaceCastSDNode : public SDNode {\nprivate:\n  unsigned SrcAddrSpace;\n  unsigned DestAddrSpace;\n\npublic:\n  AddrSpaceCastSDNode(unsigned Order, const DebugLoc &dl, EVT VT,\n                      unsigned SrcAS, unsigned DestAS);\n\n  unsigned getSrcAddressSpace() const { return SrcAddrSpace; }\n  unsigned getDestAddressSpace() const { return DestAddrSpace; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ADDRSPACECAST;\n  }\n};\n\n/// This is an abstract virtual class for memory operations.\nclass MemSDNode : public SDNode {\nprivate:\n  // VT of in-memory value.\n  EVT MemoryVT;\n\nprotected:\n  /// Memory reference information.\n  MachineMemOperand *MMO;\n\npublic:\n  MemSDNode(unsigned Opc, unsigned Order, const DebugLoc &dl, SDVTList VTs,\n            EVT memvt, MachineMemOperand *MMO);\n\n  bool readMem() const { return MMO->isLoad(); }\n  bool writeMem() const { return MMO->isStore(); }\n\n  /// Returns alignment and volatility of the memory access\n  Align getOriginalAlign() const { return MMO->getBaseAlign(); }\n  Align getAlign() const { return MMO->getAlign(); }\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned getOriginalAlignment() const,\n                            \"Use getOriginalAlign() instead\") {\n    return MMO->getBaseAlign().value();\n  }\n  // FIXME: Remove once transition to getAlign is over.\n  unsigned getAlignment() const { return MMO->getAlign().value(); }\n\n  /// Return the SubclassData value, without HasDebugValue. This contains an\n  /// encoding of the volatile flag, as well as bits used by subclasses. This\n  /// function should only be used to compute a FoldingSetNodeID value.\n  /// The HasDebugValue bit is masked out because CSE map needs to match\n  /// nodes with debug info with nodes without debug info. Same is about\n  /// isDivergent bit.\n  unsigned getRawSubclassData() const {\n    uint16_t Data;\n    union {\n      char RawSDNodeBits[sizeof(uint16_t)];\n      SDNodeBitfields SDNodeBits;\n    };\n    memcpy(&RawSDNodeBits, &this->RawSDNodeBits, sizeof(this->RawSDNodeBits));\n    SDNodeBits.HasDebugValue = 0;\n    SDNodeBits.IsDivergent = false;\n    memcpy(&Data, &RawSDNodeBits, sizeof(RawSDNodeBits));\n    return Data;\n  }\n\n  bool isVolatile() const { return MemSDNodeBits.IsVolatile; }\n  bool isNonTemporal() const { return MemSDNodeBits.IsNonTemporal; }\n  bool isDereferenceable() const { return MemSDNodeBits.IsDereferenceable; }\n  bool isInvariant() const { return MemSDNodeBits.IsInvariant; }\n\n  // Returns the offset from the location of the access.\n  int64_t getSrcValueOffset() const { return MMO->getOffset(); }\n\n  /// Returns the AA info that describes the dereference.\n  AAMDNodes getAAInfo() const { return MMO->getAAInfo(); }\n\n  /// Returns the Ranges that describes the dereference.\n  const MDNode *getRanges() const { return MMO->getRanges(); }\n\n  /// Returns the synchronization scope ID for this memory operation.\n  SyncScope::ID getSyncScopeID() const { return MMO->getSyncScopeID(); }\n\n  /// Return the atomic ordering requirements for this memory operation. For\n  /// cmpxchg atomic operations, return the atomic ordering requirements when\n  /// store occurs.\n  AtomicOrdering getOrdering() const { return MMO->getOrdering(); }\n\n  /// Return true if the memory operation ordering is Unordered or higher.\n  bool isAtomic() const { return MMO->isAtomic(); }\n\n  /// Returns true if the memory operation doesn't imply any ordering\n  /// constraints on surrounding memory operations beyond the normal memory\n  /// aliasing rules.\n  bool isUnordered() const { return MMO->isUnordered(); }\n\n  /// Returns true if the memory operation is neither atomic or volatile.\n  bool isSimple() const { return !isAtomic() && !isVolatile(); }\n\n  /// Return the type of the in-memory value.\n  EVT getMemoryVT() const { return MemoryVT; }\n\n  /// Return a MachineMemOperand object describing the memory\n  /// reference performed by operation.\n  MachineMemOperand *getMemOperand() const { return MMO; }\n\n  const MachinePointerInfo &getPointerInfo() const {\n    return MMO->getPointerInfo();\n  }\n\n  /// Return the address space for the associated pointer\n  unsigned getAddressSpace() const {\n    return getPointerInfo().getAddrSpace();\n  }\n\n  /// Update this MemSDNode's MachineMemOperand information\n  /// to reflect the alignment of NewMMO, if it has a greater alignment.\n  /// This must only be used when the new alignment applies to all users of\n  /// this MachineMemOperand.\n  void refineAlignment(const MachineMemOperand *NewMMO) {\n    MMO->refineAlignment(NewMMO);\n  }\n\n  const SDValue &getChain() const { return getOperand(0); }\n\n  const SDValue &getBasePtr() const {\n    switch (getOpcode()) {\n    case ISD::STORE:\n    case ISD::MSTORE:\n      return getOperand(2);\n    case ISD::MGATHER:\n    case ISD::MSCATTER:\n      return getOperand(3);\n    default:\n      return getOperand(1);\n    }\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    // For some targets, we lower some target intrinsics to a MemIntrinsicNode\n    // with either an intrinsic or a target opcode.\n    return N->getOpcode() == ISD::LOAD                ||\n           N->getOpcode() == ISD::STORE               ||\n           N->getOpcode() == ISD::PREFETCH            ||\n           N->getOpcode() == ISD::ATOMIC_CMP_SWAP     ||\n           N->getOpcode() == ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS ||\n           N->getOpcode() == ISD::ATOMIC_SWAP         ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_ADD     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_SUB     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_AND     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_CLR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_OR      ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_XOR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_NAND    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MIN     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MAX     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMIN    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMAX    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FADD    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FSUB    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD         ||\n           N->getOpcode() == ISD::ATOMIC_STORE        ||\n           N->getOpcode() == ISD::MLOAD               ||\n           N->getOpcode() == ISD::MSTORE              ||\n           N->getOpcode() == ISD::MGATHER             ||\n           N->getOpcode() == ISD::MSCATTER            ||\n           N->isMemIntrinsic()                        ||\n           N->isTargetMemoryOpcode();\n  }\n};\n\n/// This is an SDNode representing atomic operations.\nclass AtomicSDNode : public MemSDNode {\npublic:\n  AtomicSDNode(unsigned Opc, unsigned Order, const DebugLoc &dl, SDVTList VTL,\n               EVT MemVT, MachineMemOperand *MMO)\n    : MemSDNode(Opc, Order, dl, VTL, MemVT, MMO) {\n    assert(((Opc != ISD::ATOMIC_LOAD && Opc != ISD::ATOMIC_STORE) ||\n            MMO->isAtomic()) && \"then why are we using an AtomicSDNode?\");\n  }\n\n  const SDValue &getBasePtr() const { return getOperand(1); }\n  const SDValue &getVal() const { return getOperand(2); }\n\n  /// Returns true if this SDNode represents cmpxchg atomic operation, false\n  /// otherwise.\n  bool isCompareAndSwap() const {\n    unsigned Op = getOpcode();\n    return Op == ISD::ATOMIC_CMP_SWAP ||\n           Op == ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS;\n  }\n\n  /// For cmpxchg atomic operations, return the atomic ordering requirements\n  /// when store does not occur.\n  AtomicOrdering getFailureOrdering() const {\n    assert(isCompareAndSwap() && \"Must be cmpxchg operation\");\n    return MMO->getFailureOrdering();\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ATOMIC_CMP_SWAP     ||\n           N->getOpcode() == ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS ||\n           N->getOpcode() == ISD::ATOMIC_SWAP         ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_ADD     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_SUB     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_AND     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_CLR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_OR      ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_XOR     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_NAND    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MIN     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_MAX     ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMIN    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_UMAX    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FADD    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD_FSUB    ||\n           N->getOpcode() == ISD::ATOMIC_LOAD         ||\n           N->getOpcode() == ISD::ATOMIC_STORE;\n  }\n};\n\n/// This SDNode is used for target intrinsics that touch\n/// memory and need an associated MachineMemOperand. Its opcode may be\n/// INTRINSIC_VOID, INTRINSIC_W_CHAIN, PREFETCH, or a target-specific opcode\n/// with a value not less than FIRST_TARGET_MEMORY_OPCODE.\nclass MemIntrinsicSDNode : public MemSDNode {\npublic:\n  MemIntrinsicSDNode(unsigned Opc, unsigned Order, const DebugLoc &dl,\n                     SDVTList VTs, EVT MemoryVT, MachineMemOperand *MMO)\n      : MemSDNode(Opc, Order, dl, VTs, MemoryVT, MMO) {\n    SDNodeBits.IsMemIntrinsic = true;\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    // We lower some target intrinsics to their target opcode\n    // early a node with a target opcode can be of this class\n    return N->isMemIntrinsic()             ||\n           N->getOpcode() == ISD::PREFETCH ||\n           N->isTargetMemoryOpcode();\n  }\n};\n\n/// This SDNode is used to implement the code generator\n/// support for the llvm IR shufflevector instruction.  It combines elements\n/// from two input vectors into a new input vector, with the selection and\n/// ordering of elements determined by an array of integers, referred to as\n/// the shuffle mask.  For input vectors of width N, mask indices of 0..N-1\n/// refer to elements from the LHS input, and indices from N to 2N-1 the RHS.\n/// An index of -1 is treated as undef, such that the code generator may put\n/// any value in the corresponding element of the result.\nclass ShuffleVectorSDNode : public SDNode {\n  // The memory for Mask is owned by the SelectionDAG's OperandAllocator, and\n  // is freed when the SelectionDAG object is destroyed.\n  const int *Mask;\n\nprotected:\n  friend class SelectionDAG;\n\n  ShuffleVectorSDNode(EVT VT, unsigned Order, const DebugLoc &dl, const int *M)\n      : SDNode(ISD::VECTOR_SHUFFLE, Order, dl, getSDVTList(VT)), Mask(M) {}\n\npublic:\n  ArrayRef<int> getMask() const {\n    EVT VT = getValueType(0);\n    return makeArrayRef(Mask, VT.getVectorNumElements());\n  }\n\n  int getMaskElt(unsigned Idx) const {\n    assert(Idx < getValueType(0).getVectorNumElements() && \"Idx out of range!\");\n    return Mask[Idx];\n  }\n\n  bool isSplat() const { return isSplatMask(Mask, getValueType(0)); }\n\n  int getSplatIndex() const {\n    assert(isSplat() && \"Cannot get splat index for non-splat!\");\n    EVT VT = getValueType(0);\n    for (unsigned i = 0, e = VT.getVectorNumElements(); i != e; ++i)\n      if (Mask[i] >= 0)\n        return Mask[i];\n\n    // We can choose any index value here and be correct because all elements\n    // are undefined. Return 0 for better potential for callers to simplify.\n    return 0;\n  }\n\n  static bool isSplatMask(const int *Mask, EVT VT);\n\n  /// Change values in a shuffle permute mask assuming\n  /// the two vector operands have swapped position.\n  static void commuteMask(MutableArrayRef<int> Mask) {\n    unsigned NumElems = Mask.size();\n    for (unsigned i = 0; i != NumElems; ++i) {\n      int idx = Mask[i];\n      if (idx < 0)\n        continue;\n      else if (idx < (int)NumElems)\n        Mask[i] = idx + NumElems;\n      else\n        Mask[i] = idx - NumElems;\n    }\n  }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::VECTOR_SHUFFLE;\n  }\n};\n\nclass ConstantSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const ConstantInt *Value;\n\n  ConstantSDNode(bool isTarget, bool isOpaque, const ConstantInt *val, EVT VT)\n      : SDNode(isTarget ? ISD::TargetConstant : ISD::Constant, 0, DebugLoc(),\n               getSDVTList(VT)),\n        Value(val) {\n    ConstantSDNodeBits.IsOpaque = isOpaque;\n  }\n\npublic:\n  const ConstantInt *getConstantIntValue() const { return Value; }\n  const APInt &getAPIntValue() const { return Value->getValue(); }\n  uint64_t getZExtValue() const { return Value->getZExtValue(); }\n  int64_t getSExtValue() const { return Value->getSExtValue(); }\n  uint64_t getLimitedValue(uint64_t Limit = UINT64_MAX) {\n    return Value->getLimitedValue(Limit);\n  }\n  MaybeAlign getMaybeAlignValue() const { return Value->getMaybeAlignValue(); }\n  Align getAlignValue() const { return Value->getAlignValue(); }\n\n  bool isOne() const { return Value->isOne(); }\n  bool isNullValue() const { return Value->isZero(); }\n  bool isAllOnesValue() const { return Value->isMinusOne(); }\n\n  bool isOpaque() const { return ConstantSDNodeBits.IsOpaque; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::Constant ||\n           N->getOpcode() == ISD::TargetConstant;\n  }\n};\n\nuint64_t SDNode::getConstantOperandVal(unsigned Num) const {\n  return cast<ConstantSDNode>(getOperand(Num))->getZExtValue();\n}\n\nconst APInt &SDNode::getConstantOperandAPInt(unsigned Num) const {\n  return cast<ConstantSDNode>(getOperand(Num))->getAPIntValue();\n}\n\nclass ConstantFPSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const ConstantFP *Value;\n\n  ConstantFPSDNode(bool isTarget, const ConstantFP *val, EVT VT)\n      : SDNode(isTarget ? ISD::TargetConstantFP : ISD::ConstantFP, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Value(val) {}\n\npublic:\n  const APFloat& getValueAPF() const { return Value->getValueAPF(); }\n  const ConstantFP *getConstantFPValue() const { return Value; }\n\n  /// Return true if the value is positive or negative zero.\n  bool isZero() const { return Value->isZero(); }\n\n  /// Return true if the value is a NaN.\n  bool isNaN() const { return Value->isNaN(); }\n\n  /// Return true if the value is an infinity\n  bool isInfinity() const { return Value->isInfinity(); }\n\n  /// Return true if the value is negative.\n  bool isNegative() const { return Value->isNegative(); }\n\n  /// We don't rely on operator== working on double values, as\n  /// it returns true for things that are clearly not equal, like -0.0 and 0.0.\n  /// As such, this method can be used to do an exact bit-for-bit comparison of\n  /// two floating point values.\n\n  /// We leave the version with the double argument here because it's just so\n  /// convenient to write \"2.0\" and the like.  Without this function we'd\n  /// have to duplicate its logic everywhere it's called.\n  bool isExactlyValue(double V) const {\n    return Value->getValueAPF().isExactlyValue(V);\n  }\n  bool isExactlyValue(const APFloat& V) const;\n\n  static bool isValueValidForType(EVT VT, const APFloat& Val);\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ConstantFP ||\n           N->getOpcode() == ISD::TargetConstantFP;\n  }\n};\n\n/// Returns true if \\p V is a constant integer zero.\nbool isNullConstant(SDValue V);\n\n/// Returns true if \\p V is an FP constant with a value of positive zero.\nbool isNullFPConstant(SDValue V);\n\n/// Returns true if \\p V is an integer constant with all bits set.\nbool isAllOnesConstant(SDValue V);\n\n/// Returns true if \\p V is a constant integer one.\nbool isOneConstant(SDValue V);\n\n/// Return the non-bitcasted source operand of \\p V if it exists.\n/// If \\p V is not a bitcasted value, it is returned as-is.\nSDValue peekThroughBitcasts(SDValue V);\n\n/// Return the non-bitcasted and one-use source operand of \\p V if it exists.\n/// If \\p V is not a bitcasted one-use value, it is returned as-is.\nSDValue peekThroughOneUseBitcasts(SDValue V);\n\n/// Return the non-extracted vector source operand of \\p V if it exists.\n/// If \\p V is not an extracted subvector, it is returned as-is.\nSDValue peekThroughExtractSubvectors(SDValue V);\n\n/// Returns true if \\p V is a bitwise not operation. Assumes that an all ones\n/// constant is canonicalized to be operand 1.\nbool isBitwiseNot(SDValue V, bool AllowUndefs = false);\n\n/// Returns the SDNode if it is a constant splat BuildVector or constant int.\nConstantSDNode *isConstOrConstSplat(SDValue N, bool AllowUndefs = false,\n                                    bool AllowTruncation = false);\n\n/// Returns the SDNode if it is a demanded constant splat BuildVector or\n/// constant int.\nConstantSDNode *isConstOrConstSplat(SDValue N, const APInt &DemandedElts,\n                                    bool AllowUndefs = false,\n                                    bool AllowTruncation = false);\n\n/// Returns the SDNode if it is a constant splat BuildVector or constant float.\nConstantFPSDNode *isConstOrConstSplatFP(SDValue N, bool AllowUndefs = false);\n\n/// Returns the SDNode if it is a demanded constant splat BuildVector or\n/// constant float.\nConstantFPSDNode *isConstOrConstSplatFP(SDValue N, const APInt &DemandedElts,\n                                        bool AllowUndefs = false);\n\n/// Return true if the value is a constant 0 integer or a splatted vector of\n/// a constant 0 integer (with no undefs by default).\n/// Build vector implicit truncation is not an issue for null values.\nbool isNullOrNullSplat(SDValue V, bool AllowUndefs = false);\n\n/// Return true if the value is a constant 1 integer or a splatted vector of a\n/// constant 1 integer (with no undefs).\n/// Does not permit build vector implicit truncation.\nbool isOneOrOneSplat(SDValue V, bool AllowUndefs = false);\n\n/// Return true if the value is a constant -1 integer or a splatted vector of a\n/// constant -1 integer (with no undefs).\n/// Does not permit build vector implicit truncation.\nbool isAllOnesOrAllOnesSplat(SDValue V, bool AllowUndefs = false);\n\nclass GlobalAddressSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const GlobalValue *TheGlobal;\n  int64_t Offset;\n  unsigned TargetFlags;\n\n  GlobalAddressSDNode(unsigned Opc, unsigned Order, const DebugLoc &DL,\n                      const GlobalValue *GA, EVT VT, int64_t o,\n                      unsigned TF);\n\npublic:\n  const GlobalValue *getGlobal() const { return TheGlobal; }\n  int64_t getOffset() const { return Offset; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n  // Return the address space this GlobalAddress belongs to.\n  unsigned getAddressSpace() const;\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::GlobalAddress ||\n           N->getOpcode() == ISD::TargetGlobalAddress ||\n           N->getOpcode() == ISD::GlobalTLSAddress ||\n           N->getOpcode() == ISD::TargetGlobalTLSAddress;\n  }\n};\n\nclass FrameIndexSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  int FI;\n\n  FrameIndexSDNode(int fi, EVT VT, bool isTarg)\n    : SDNode(isTarg ? ISD::TargetFrameIndex : ISD::FrameIndex,\n      0, DebugLoc(), getSDVTList(VT)), FI(fi) {\n  }\n\npublic:\n  int getIndex() const { return FI; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::FrameIndex ||\n           N->getOpcode() == ISD::TargetFrameIndex;\n  }\n};\n\n/// This SDNode is used for LIFETIME_START/LIFETIME_END values, which indicate\n/// the offet and size that are started/ended in the underlying FrameIndex.\nclass LifetimeSDNode : public SDNode {\n  friend class SelectionDAG;\n  int64_t Size;\n  int64_t Offset; // -1 if offset is unknown.\n\n  LifetimeSDNode(unsigned Opcode, unsigned Order, const DebugLoc &dl,\n                 SDVTList VTs, int64_t Size, int64_t Offset)\n      : SDNode(Opcode, Order, dl, VTs), Size(Size), Offset(Offset) {}\npublic:\n  int64_t getFrameIndex() const {\n    return cast<FrameIndexSDNode>(getOperand(1))->getIndex();\n  }\n\n  bool hasOffset() const { return Offset >= 0; }\n  int64_t getOffset() const {\n    assert(hasOffset() && \"offset is unknown\");\n    return Offset;\n  }\n  int64_t getSize() const {\n    assert(hasOffset() && \"offset is unknown\");\n    return Size;\n  }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::LIFETIME_START ||\n           N->getOpcode() == ISD::LIFETIME_END;\n  }\n};\n\n/// This SDNode is used for PSEUDO_PROBE values, which are the function guid and\n/// the index of the basic block being probed. A pseudo probe serves as a place\n/// holder and will be removed at the end of compilation. It does not have any\n/// operand because we do not want the instruction selection to deal with any.\nclass PseudoProbeSDNode : public SDNode {\n  friend class SelectionDAG;\n  uint64_t Guid;\n  uint64_t Index;\n  uint32_t Attributes;\n\n  PseudoProbeSDNode(unsigned Opcode, unsigned Order, const DebugLoc &Dl,\n                    SDVTList VTs, uint64_t Guid, uint64_t Index, uint32_t Attr)\n      : SDNode(Opcode, Order, Dl, VTs), Guid(Guid), Index(Index),\n        Attributes(Attr) {}\n\npublic:\n  uint64_t getGuid() const { return Guid; }\n  uint64_t getIndex() const { return Index; }\n  uint32_t getAttributes() const { return Attributes; }\n\n  // Methods to support isa and dyn_cast\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::PSEUDO_PROBE;\n  }\n};\n\nclass JumpTableSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  int JTI;\n  unsigned TargetFlags;\n\n  JumpTableSDNode(int jti, EVT VT, bool isTarg, unsigned TF)\n    : SDNode(isTarg ? ISD::TargetJumpTable : ISD::JumpTable,\n      0, DebugLoc(), getSDVTList(VT)), JTI(jti), TargetFlags(TF) {\n  }\n\npublic:\n  int getIndex() const { return JTI; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::JumpTable ||\n           N->getOpcode() == ISD::TargetJumpTable;\n  }\n};\n\nclass ConstantPoolSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  union {\n    const Constant *ConstVal;\n    MachineConstantPoolValue *MachineCPVal;\n  } Val;\n  int Offset;  // It's a MachineConstantPoolValue if top bit is set.\n  Align Alignment; // Minimum alignment requirement of CP.\n  unsigned TargetFlags;\n\n  ConstantPoolSDNode(bool isTarget, const Constant *c, EVT VT, int o,\n                     Align Alignment, unsigned TF)\n      : SDNode(isTarget ? ISD::TargetConstantPool : ISD::ConstantPool, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Offset(o), Alignment(Alignment), TargetFlags(TF) {\n    assert(Offset >= 0 && \"Offset is too large\");\n    Val.ConstVal = c;\n  }\n\n  ConstantPoolSDNode(bool isTarget, MachineConstantPoolValue *v, EVT VT, int o,\n                     Align Alignment, unsigned TF)\n      : SDNode(isTarget ? ISD::TargetConstantPool : ISD::ConstantPool, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Offset(o), Alignment(Alignment), TargetFlags(TF) {\n    assert(Offset >= 0 && \"Offset is too large\");\n    Val.MachineCPVal = v;\n    Offset |= 1 << (sizeof(unsigned)*CHAR_BIT-1);\n  }\n\npublic:\n  bool isMachineConstantPoolEntry() const {\n    return Offset < 0;\n  }\n\n  const Constant *getConstVal() const {\n    assert(!isMachineConstantPoolEntry() && \"Wrong constantpool type\");\n    return Val.ConstVal;\n  }\n\n  MachineConstantPoolValue *getMachineCPVal() const {\n    assert(isMachineConstantPoolEntry() && \"Wrong constantpool type\");\n    return Val.MachineCPVal;\n  }\n\n  int getOffset() const {\n    return Offset & ~(1 << (sizeof(unsigned)*CHAR_BIT-1));\n  }\n\n  // Return the alignment of this constant pool object, which is either 0 (for\n  // default alignment) or the desired value.\n  Align getAlign() const { return Alignment; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  Type *getType() const;\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ConstantPool ||\n           N->getOpcode() == ISD::TargetConstantPool;\n  }\n};\n\n/// Completely target-dependent object reference.\nclass TargetIndexSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  unsigned TargetFlags;\n  int Index;\n  int64_t Offset;\n\npublic:\n  TargetIndexSDNode(int Idx, EVT VT, int64_t Ofs, unsigned TF)\n      : SDNode(ISD::TargetIndex, 0, DebugLoc(), getSDVTList(VT)),\n        TargetFlags(TF), Index(Idx), Offset(Ofs) {}\n\n  unsigned getTargetFlags() const { return TargetFlags; }\n  int getIndex() const { return Index; }\n  int64_t getOffset() const { return Offset; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::TargetIndex;\n  }\n};\n\nclass BasicBlockSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  MachineBasicBlock *MBB;\n\n  /// Debug info is meaningful and potentially useful here, but we create\n  /// blocks out of order when they're jumped to, which makes it a bit\n  /// harder.  Let's see if we need it first.\n  explicit BasicBlockSDNode(MachineBasicBlock *mbb)\n    : SDNode(ISD::BasicBlock, 0, DebugLoc(), getSDVTList(MVT::Other)), MBB(mbb)\n  {}\n\npublic:\n  MachineBasicBlock *getBasicBlock() const { return MBB; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::BasicBlock;\n  }\n};\n\n/// A \"pseudo-class\" with methods for operating on BUILD_VECTORs.\nclass BuildVectorSDNode : public SDNode {\npublic:\n  // These are constructed as SDNodes and then cast to BuildVectorSDNodes.\n  explicit BuildVectorSDNode() = delete;\n\n  /// Check if this is a constant splat, and if so, find the\n  /// smallest element size that splats the vector.  If MinSplatBits is\n  /// nonzero, the element size must be at least that large.  Note that the\n  /// splat element may be the entire vector (i.e., a one element vector).\n  /// Returns the splat element value in SplatValue.  Any undefined bits in\n  /// that value are zero, and the corresponding bits in the SplatUndef mask\n  /// are set.  The SplatBitSize value is set to the splat element size in\n  /// bits.  HasAnyUndefs is set to true if any bits in the vector are\n  /// undefined.  isBigEndian describes the endianness of the target.\n  bool isConstantSplat(APInt &SplatValue, APInt &SplatUndef,\n                       unsigned &SplatBitSize, bool &HasAnyUndefs,\n                       unsigned MinSplatBits = 0,\n                       bool isBigEndian = false) const;\n\n  /// Returns the demanded splatted value or a null value if this is not a\n  /// splat.\n  ///\n  /// The DemandedElts mask indicates the elements that must be in the splat.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  SDValue getSplatValue(const APInt &DemandedElts,\n                        BitVector *UndefElements = nullptr) const;\n\n  /// Returns the splatted value or a null value if this is not a splat.\n  ///\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  SDValue getSplatValue(BitVector *UndefElements = nullptr) const;\n\n  /// Find the shortest repeating sequence of values in the build vector.\n  ///\n  /// e.g. { u, X, u, X, u, u, X, u } -> { X }\n  ///      { X, Y, u, Y, u, u, X, u } -> { X, Y }\n  ///\n  /// Currently this must be a power-of-2 build vector.\n  /// The DemandedElts mask indicates the elements that must be present,\n  /// undemanded elements in Sequence may be null (SDValue()). If passed a\n  /// non-null UndefElements bitvector, it will resize it to match the original\n  /// vector width and set the bits where elements are undef. If result is\n  /// false, Sequence will be empty.\n  bool getRepeatedSequence(const APInt &DemandedElts,\n                           SmallVectorImpl<SDValue> &Sequence,\n                           BitVector *UndefElements = nullptr) const;\n\n  /// Find the shortest repeating sequence of values in the build vector.\n  ///\n  /// e.g. { u, X, u, X, u, u, X, u } -> { X }\n  ///      { X, Y, u, Y, u, u, X, u } -> { X, Y }\n  ///\n  /// Currently this must be a power-of-2 build vector.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the original vector width and set the bits where elements are undef.\n  /// If result is false, Sequence will be empty.\n  bool getRepeatedSequence(SmallVectorImpl<SDValue> &Sequence,\n                           BitVector *UndefElements = nullptr) const;\n\n  /// Returns the demanded splatted constant or null if this is not a constant\n  /// splat.\n  ///\n  /// The DemandedElts mask indicates the elements that must be in the splat.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantSDNode *\n  getConstantSplatNode(const APInt &DemandedElts,\n                       BitVector *UndefElements = nullptr) const;\n\n  /// Returns the splatted constant or null if this is not a constant\n  /// splat.\n  ///\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantSDNode *\n  getConstantSplatNode(BitVector *UndefElements = nullptr) const;\n\n  /// Returns the demanded splatted constant FP or null if this is not a\n  /// constant FP splat.\n  ///\n  /// The DemandedElts mask indicates the elements that must be in the splat.\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantFPSDNode *\n  getConstantFPSplatNode(const APInt &DemandedElts,\n                         BitVector *UndefElements = nullptr) const;\n\n  /// Returns the splatted constant FP or null if this is not a constant\n  /// FP splat.\n  ///\n  /// If passed a non-null UndefElements bitvector, it will resize it to match\n  /// the vector width and set the bits where elements are undef.\n  ConstantFPSDNode *\n  getConstantFPSplatNode(BitVector *UndefElements = nullptr) const;\n\n  /// If this is a constant FP splat and the splatted constant FP is an\n  /// exact power or 2, return the log base 2 integer value.  Otherwise,\n  /// return -1.\n  ///\n  /// The BitWidth specifies the necessary bit precision.\n  int32_t getConstantFPSplatPow2ToLog2Int(BitVector *UndefElements,\n                                          uint32_t BitWidth) const;\n\n  bool isConstant() const;\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::BUILD_VECTOR;\n  }\n};\n\n/// An SDNode that holds an arbitrary LLVM IR Value. This is\n/// used when the SelectionDAG needs to make a simple reference to something\n/// in the LLVM IR representation.\n///\nclass SrcValueSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const Value *V;\n\n  /// Create a SrcValue for a general value.\n  explicit SrcValueSDNode(const Value *v)\n    : SDNode(ISD::SRCVALUE, 0, DebugLoc(), getSDVTList(MVT::Other)), V(v) {}\n\npublic:\n  /// Return the contained Value.\n  const Value *getValue() const { return V; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::SRCVALUE;\n  }\n};\n\nclass MDNodeSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const MDNode *MD;\n\n  explicit MDNodeSDNode(const MDNode *md)\n  : SDNode(ISD::MDNODE_SDNODE, 0, DebugLoc(), getSDVTList(MVT::Other)), MD(md)\n  {}\n\npublic:\n  const MDNode *getMD() const { return MD; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MDNODE_SDNODE;\n  }\n};\n\nclass RegisterSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  Register Reg;\n\n  RegisterSDNode(Register reg, EVT VT)\n    : SDNode(ISD::Register, 0, DebugLoc(), getSDVTList(VT)), Reg(reg) {}\n\npublic:\n  Register getReg() const { return Reg; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::Register;\n  }\n};\n\nclass RegisterMaskSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  // The memory for RegMask is not owned by the node.\n  const uint32_t *RegMask;\n\n  RegisterMaskSDNode(const uint32_t *mask)\n    : SDNode(ISD::RegisterMask, 0, DebugLoc(), getSDVTList(MVT::Untyped)),\n      RegMask(mask) {}\n\npublic:\n  const uint32_t *getRegMask() const { return RegMask; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::RegisterMask;\n  }\n};\n\nclass BlockAddressSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const BlockAddress *BA;\n  int64_t Offset;\n  unsigned TargetFlags;\n\n  BlockAddressSDNode(unsigned NodeTy, EVT VT, const BlockAddress *ba,\n                     int64_t o, unsigned Flags)\n    : SDNode(NodeTy, 0, DebugLoc(), getSDVTList(VT)),\n             BA(ba), Offset(o), TargetFlags(Flags) {}\n\npublic:\n  const BlockAddress *getBlockAddress() const { return BA; }\n  int64_t getOffset() const { return Offset; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::BlockAddress ||\n           N->getOpcode() == ISD::TargetBlockAddress;\n  }\n};\n\nclass LabelSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  MCSymbol *Label;\n\n  LabelSDNode(unsigned Opcode, unsigned Order, const DebugLoc &dl, MCSymbol *L)\n      : SDNode(Opcode, Order, dl, getSDVTList(MVT::Other)), Label(L) {\n    assert(LabelSDNode::classof(this) && \"not a label opcode\");\n  }\n\npublic:\n  MCSymbol *getLabel() const { return Label; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::EH_LABEL ||\n           N->getOpcode() == ISD::ANNOTATION_LABEL;\n  }\n};\n\nclass ExternalSymbolSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  const char *Symbol;\n  unsigned TargetFlags;\n\n  ExternalSymbolSDNode(bool isTarget, const char *Sym, unsigned TF, EVT VT)\n      : SDNode(isTarget ? ISD::TargetExternalSymbol : ISD::ExternalSymbol, 0,\n               DebugLoc(), getSDVTList(VT)),\n        Symbol(Sym), TargetFlags(TF) {}\n\npublic:\n  const char *getSymbol() const { return Symbol; }\n  unsigned getTargetFlags() const { return TargetFlags; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::ExternalSymbol ||\n           N->getOpcode() == ISD::TargetExternalSymbol;\n  }\n};\n\nclass MCSymbolSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  MCSymbol *Symbol;\n\n  MCSymbolSDNode(MCSymbol *Symbol, EVT VT)\n      : SDNode(ISD::MCSymbol, 0, DebugLoc(), getSDVTList(VT)), Symbol(Symbol) {}\n\npublic:\n  MCSymbol *getMCSymbol() const { return Symbol; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MCSymbol;\n  }\n};\n\nclass CondCodeSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  ISD::CondCode Condition;\n\n  explicit CondCodeSDNode(ISD::CondCode Cond)\n    : SDNode(ISD::CONDCODE, 0, DebugLoc(), getSDVTList(MVT::Other)),\n      Condition(Cond) {}\n\npublic:\n  ISD::CondCode get() const { return Condition; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::CONDCODE;\n  }\n};\n\n/// This class is used to represent EVT's, which are used\n/// to parameterize some operations.\nclass VTSDNode : public SDNode {\n  friend class SelectionDAG;\n\n  EVT ValueType;\n\n  explicit VTSDNode(EVT VT)\n    : SDNode(ISD::VALUETYPE, 0, DebugLoc(), getSDVTList(MVT::Other)),\n      ValueType(VT) {}\n\npublic:\n  EVT getVT() const { return ValueType; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::VALUETYPE;\n  }\n};\n\n/// Base class for LoadSDNode and StoreSDNode\nclass LSBaseSDNode : public MemSDNode {\npublic:\n  LSBaseSDNode(ISD::NodeType NodeTy, unsigned Order, const DebugLoc &dl,\n               SDVTList VTs, ISD::MemIndexedMode AM, EVT MemVT,\n               MachineMemOperand *MMO)\n      : MemSDNode(NodeTy, Order, dl, VTs, MemVT, MMO) {\n    LSBaseSDNodeBits.AddressingMode = AM;\n    assert(getAddressingMode() == AM && \"Value truncated\");\n  }\n\n  const SDValue &getOffset() const {\n    return getOperand(getOpcode() == ISD::LOAD ? 2 : 3);\n  }\n\n  /// Return the addressing mode for this load or store:\n  /// unindexed, pre-inc, pre-dec, post-inc, or post-dec.\n  ISD::MemIndexedMode getAddressingMode() const {\n    return static_cast<ISD::MemIndexedMode>(LSBaseSDNodeBits.AddressingMode);\n  }\n\n  /// Return true if this is a pre/post inc/dec load/store.\n  bool isIndexed() const { return getAddressingMode() != ISD::UNINDEXED; }\n\n  /// Return true if this is NOT a pre/post inc/dec load/store.\n  bool isUnindexed() const { return getAddressingMode() == ISD::UNINDEXED; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::LOAD ||\n           N->getOpcode() == ISD::STORE;\n  }\n};\n\n/// This class is used to represent ISD::LOAD nodes.\nclass LoadSDNode : public LSBaseSDNode {\n  friend class SelectionDAG;\n\n  LoadSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n             ISD::MemIndexedMode AM, ISD::LoadExtType ETy, EVT MemVT,\n             MachineMemOperand *MMO)\n      : LSBaseSDNode(ISD::LOAD, Order, dl, VTs, AM, MemVT, MMO) {\n    LoadSDNodeBits.ExtTy = ETy;\n    assert(readMem() && \"Load MachineMemOperand is not a load!\");\n    assert(!writeMem() && \"Load MachineMemOperand is a store!\");\n  }\n\npublic:\n  /// Return whether this is a plain node,\n  /// or one of the varieties of value-extending loads.\n  ISD::LoadExtType getExtensionType() const {\n    return static_cast<ISD::LoadExtType>(LoadSDNodeBits.ExtTy);\n  }\n\n  const SDValue &getBasePtr() const { return getOperand(1); }\n  const SDValue &getOffset() const { return getOperand(2); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::LOAD;\n  }\n};\n\n/// This class is used to represent ISD::STORE nodes.\nclass StoreSDNode : public LSBaseSDNode {\n  friend class SelectionDAG;\n\n  StoreSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n              ISD::MemIndexedMode AM, bool isTrunc, EVT MemVT,\n              MachineMemOperand *MMO)\n      : LSBaseSDNode(ISD::STORE, Order, dl, VTs, AM, MemVT, MMO) {\n    StoreSDNodeBits.IsTruncating = isTrunc;\n    assert(!readMem() && \"Store MachineMemOperand is a load!\");\n    assert(writeMem() && \"Store MachineMemOperand is not a store!\");\n  }\n\npublic:\n  /// Return true if the op does a truncation before store.\n  /// For integers this is the same as doing a TRUNCATE and storing the result.\n  /// For floats, it is the same as doing an FP_ROUND and storing the result.\n  bool isTruncatingStore() const { return StoreSDNodeBits.IsTruncating; }\n  void setTruncatingStore(bool Truncating) {\n    StoreSDNodeBits.IsTruncating = Truncating;\n  }\n\n  const SDValue &getValue() const { return getOperand(1); }\n  const SDValue &getBasePtr() const { return getOperand(2); }\n  const SDValue &getOffset() const { return getOperand(3); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::STORE;\n  }\n};\n\n/// This base class is used to represent MLOAD and MSTORE nodes\nclass MaskedLoadStoreSDNode : public MemSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedLoadStoreSDNode(ISD::NodeType NodeTy, unsigned Order,\n                        const DebugLoc &dl, SDVTList VTs,\n                        ISD::MemIndexedMode AM, EVT MemVT,\n                        MachineMemOperand *MMO)\n      : MemSDNode(NodeTy, Order, dl, VTs, MemVT, MMO) {\n    LSBaseSDNodeBits.AddressingMode = AM;\n    assert(getAddressingMode() == AM && \"Value truncated\");\n  }\n\n  // MaskedLoadSDNode (Chain, ptr, offset, mask, passthru)\n  // MaskedStoreSDNode (Chain, data, ptr, offset, mask)\n  // Mask is a vector of i1 elements\n  const SDValue &getOffset() const {\n    return getOperand(getOpcode() == ISD::MLOAD ? 2 : 3);\n  }\n  const SDValue &getMask() const {\n    return getOperand(getOpcode() == ISD::MLOAD ? 3 : 4);\n  }\n\n  /// Return the addressing mode for this load or store:\n  /// unindexed, pre-inc, pre-dec, post-inc, or post-dec.\n  ISD::MemIndexedMode getAddressingMode() const {\n    return static_cast<ISD::MemIndexedMode>(LSBaseSDNodeBits.AddressingMode);\n  }\n\n  /// Return true if this is a pre/post inc/dec load/store.\n  bool isIndexed() const { return getAddressingMode() != ISD::UNINDEXED; }\n\n  /// Return true if this is NOT a pre/post inc/dec load/store.\n  bool isUnindexed() const { return getAddressingMode() == ISD::UNINDEXED; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MLOAD ||\n           N->getOpcode() == ISD::MSTORE;\n  }\n};\n\n/// This class is used to represent an MLOAD node\nclass MaskedLoadSDNode : public MaskedLoadStoreSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedLoadSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                   ISD::MemIndexedMode AM, ISD::LoadExtType ETy,\n                   bool IsExpanding, EVT MemVT, MachineMemOperand *MMO)\n      : MaskedLoadStoreSDNode(ISD::MLOAD, Order, dl, VTs, AM, MemVT, MMO) {\n    LoadSDNodeBits.ExtTy = ETy;\n    LoadSDNodeBits.IsExpanding = IsExpanding;\n  }\n\n  ISD::LoadExtType getExtensionType() const {\n    return static_cast<ISD::LoadExtType>(LoadSDNodeBits.ExtTy);\n  }\n\n  const SDValue &getBasePtr() const { return getOperand(1); }\n  const SDValue &getOffset() const { return getOperand(2); }\n  const SDValue &getMask() const { return getOperand(3); }\n  const SDValue &getPassThru() const { return getOperand(4); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MLOAD;\n  }\n\n  bool isExpandingLoad() const { return LoadSDNodeBits.IsExpanding; }\n};\n\n/// This class is used to represent an MSTORE node\nclass MaskedStoreSDNode : public MaskedLoadStoreSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedStoreSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                    ISD::MemIndexedMode AM, bool isTrunc, bool isCompressing,\n                    EVT MemVT, MachineMemOperand *MMO)\n      : MaskedLoadStoreSDNode(ISD::MSTORE, Order, dl, VTs, AM, MemVT, MMO) {\n    StoreSDNodeBits.IsTruncating = isTrunc;\n    StoreSDNodeBits.IsCompressing = isCompressing;\n  }\n\n  /// Return true if the op does a truncation before store.\n  /// For integers this is the same as doing a TRUNCATE and storing the result.\n  /// For floats, it is the same as doing an FP_ROUND and storing the result.\n  bool isTruncatingStore() const { return StoreSDNodeBits.IsTruncating; }\n\n  /// Returns true if the op does a compression to the vector before storing.\n  /// The node contiguously stores the active elements (integers or floats)\n  /// in src (those with their respective bit set in writemask k) to unaligned\n  /// memory at base_addr.\n  bool isCompressingStore() const { return StoreSDNodeBits.IsCompressing; }\n\n  const SDValue &getValue() const { return getOperand(1); }\n  const SDValue &getBasePtr() const { return getOperand(2); }\n  const SDValue &getOffset() const { return getOperand(3); }\n  const SDValue &getMask() const { return getOperand(4); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MSTORE;\n  }\n};\n\n/// This is a base class used to represent\n/// MGATHER and MSCATTER nodes\n///\nclass MaskedGatherScatterSDNode : public MemSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedGatherScatterSDNode(ISD::NodeType NodeTy, unsigned Order,\n                            const DebugLoc &dl, SDVTList VTs, EVT MemVT,\n                            MachineMemOperand *MMO, ISD::MemIndexType IndexType)\n      : MemSDNode(NodeTy, Order, dl, VTs, MemVT, MMO) {\n    LSBaseSDNodeBits.AddressingMode = IndexType;\n    assert(getIndexType() == IndexType && \"Value truncated\");\n  }\n\n  /// How is Index applied to BasePtr when computing addresses.\n  ISD::MemIndexType getIndexType() const {\n    return static_cast<ISD::MemIndexType>(LSBaseSDNodeBits.AddressingMode);\n  }\n  void setIndexType(ISD::MemIndexType IndexType) {\n    LSBaseSDNodeBits.AddressingMode = IndexType;\n  }\n  bool isIndexScaled() const {\n    return (getIndexType() == ISD::SIGNED_SCALED) ||\n           (getIndexType() == ISD::UNSIGNED_SCALED);\n  }\n  bool isIndexSigned() const {\n    return (getIndexType() == ISD::SIGNED_SCALED) ||\n           (getIndexType() == ISD::SIGNED_UNSCALED);\n  }\n\n  // In the both nodes address is Op1, mask is Op2:\n  // MaskedGatherSDNode  (Chain, passthru, mask, base, index, scale)\n  // MaskedScatterSDNode (Chain, value, mask, base, index, scale)\n  // Mask is a vector of i1 elements\n  const SDValue &getBasePtr() const { return getOperand(3); }\n  const SDValue &getIndex()   const { return getOperand(4); }\n  const SDValue &getMask()    const { return getOperand(2); }\n  const SDValue &getScale()   const { return getOperand(5); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MGATHER ||\n           N->getOpcode() == ISD::MSCATTER;\n  }\n};\n\n/// This class is used to represent an MGATHER node\n///\nclass MaskedGatherSDNode : public MaskedGatherScatterSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedGatherSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                     EVT MemVT, MachineMemOperand *MMO,\n                     ISD::MemIndexType IndexType, ISD::LoadExtType ETy)\n      : MaskedGatherScatterSDNode(ISD::MGATHER, Order, dl, VTs, MemVT, MMO,\n                                  IndexType) {\n    LoadSDNodeBits.ExtTy = ETy;\n  }\n\n  const SDValue &getPassThru() const { return getOperand(1); }\n\n  ISD::LoadExtType getExtensionType() const {\n    return ISD::LoadExtType(LoadSDNodeBits.ExtTy);\n  }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MGATHER;\n  }\n};\n\n/// This class is used to represent an MSCATTER node\n///\nclass MaskedScatterSDNode : public MaskedGatherScatterSDNode {\npublic:\n  friend class SelectionDAG;\n\n  MaskedScatterSDNode(unsigned Order, const DebugLoc &dl, SDVTList VTs,\n                      EVT MemVT, MachineMemOperand *MMO,\n                      ISD::MemIndexType IndexType, bool IsTrunc)\n      : MaskedGatherScatterSDNode(ISD::MSCATTER, Order, dl, VTs, MemVT, MMO,\n                                  IndexType) {\n    StoreSDNodeBits.IsTruncating = IsTrunc;\n  }\n\n  /// Return true if the op does a truncation before store.\n  /// For integers this is the same as doing a TRUNCATE and storing the result.\n  /// For floats, it is the same as doing an FP_ROUND and storing the result.\n  bool isTruncatingStore() const { return StoreSDNodeBits.IsTruncating; }\n\n  const SDValue &getValue() const { return getOperand(1); }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::MSCATTER;\n  }\n};\n\n/// An SDNode that represents everything that will be needed\n/// to construct a MachineInstr. These nodes are created during the\n/// instruction selection proper phase.\n///\n/// Note that the only supported way to set the `memoperands` is by calling the\n/// `SelectionDAG::setNodeMemRefs` function as the memory management happens\n/// inside the DAG rather than in the node.\nclass MachineSDNode : public SDNode {\nprivate:\n  friend class SelectionDAG;\n\n  MachineSDNode(unsigned Opc, unsigned Order, const DebugLoc &DL, SDVTList VTs)\n      : SDNode(Opc, Order, DL, VTs) {}\n\n  // We use a pointer union between a single `MachineMemOperand` pointer and\n  // a pointer to an array of `MachineMemOperand` pointers. This is null when\n  // the number of these is zero, the single pointer variant used when the\n  // number is one, and the array is used for larger numbers.\n  //\n  // The array is allocated via the `SelectionDAG`'s allocator and so will\n  // always live until the DAG is cleaned up and doesn't require ownership here.\n  //\n  // We can't use something simpler like `TinyPtrVector` here because `SDNode`\n  // subclasses aren't managed in a conforming C++ manner. See the comments on\n  // `SelectionDAG::MorphNodeTo` which details what all goes on, but the\n  // constraint here is that these don't manage memory with their constructor or\n  // destructor and can be initialized to a good state even if they start off\n  // uninitialized.\n  PointerUnion<MachineMemOperand *, MachineMemOperand **> MemRefs = {};\n\n  // Note that this could be folded into the above `MemRefs` member if doing so\n  // is advantageous at some point. We don't need to store this in most cases.\n  // However, at the moment this doesn't appear to make the allocation any\n  // smaller and makes the code somewhat simpler to read.\n  int NumMemRefs = 0;\n\npublic:\n  using mmo_iterator = ArrayRef<MachineMemOperand *>::const_iterator;\n\n  ArrayRef<MachineMemOperand *> memoperands() const {\n    // Special case the common cases.\n    if (NumMemRefs == 0)\n      return {};\n    if (NumMemRefs == 1)\n      return makeArrayRef(MemRefs.getAddrOfPtr1(), 1);\n\n    // Otherwise we have an actual array.\n    return makeArrayRef(MemRefs.get<MachineMemOperand **>(), NumMemRefs);\n  }\n  mmo_iterator memoperands_begin() const { return memoperands().begin(); }\n  mmo_iterator memoperands_end() const { return memoperands().end(); }\n  bool memoperands_empty() const { return memoperands().empty(); }\n\n  /// Clear out the memory reference descriptor list.\n  void clearMemRefs() {\n    MemRefs = nullptr;\n    NumMemRefs = 0;\n  }\n\n  static bool classof(const SDNode *N) {\n    return N->isMachineOpcode();\n  }\n};\n\n/// An SDNode that records if a register contains a value that is guaranteed to\n/// be aligned accordingly.\nclass AssertAlignSDNode : public SDNode {\n  Align Alignment;\n\npublic:\n  AssertAlignSDNode(unsigned Order, const DebugLoc &DL, EVT VT, Align A)\n      : SDNode(ISD::AssertAlign, Order, DL, getSDVTList(VT)), Alignment(A) {}\n\n  Align getAlign() const { return Alignment; }\n\n  static bool classof(const SDNode *N) {\n    return N->getOpcode() == ISD::AssertAlign;\n  }\n};\n\nclass SDNodeIterator : public std::iterator<std::forward_iterator_tag,\n                                            SDNode, ptrdiff_t> {\n  const SDNode *Node;\n  unsigned Operand;\n\n  SDNodeIterator(const SDNode *N, unsigned Op) : Node(N), Operand(Op) {}\n\npublic:\n  bool operator==(const SDNodeIterator& x) const {\n    return Operand == x.Operand;\n  }\n  bool operator!=(const SDNodeIterator& x) const { return !operator==(x); }\n\n  pointer operator*() const {\n    return Node->getOperand(Operand).getNode();\n  }\n  pointer operator->() const { return operator*(); }\n\n  SDNodeIterator& operator++() {                // Preincrement\n    ++Operand;\n    return *this;\n  }\n  SDNodeIterator operator++(int) { // Postincrement\n    SDNodeIterator tmp = *this; ++*this; return tmp;\n  }\n  size_t operator-(SDNodeIterator Other) const {\n    assert(Node == Other.Node &&\n           \"Cannot compare iterators of two different nodes!\");\n    return Operand - Other.Operand;\n  }\n\n  static SDNodeIterator begin(const SDNode *N) { return SDNodeIterator(N, 0); }\n  static SDNodeIterator end  (const SDNode *N) {\n    return SDNodeIterator(N, N->getNumOperands());\n  }\n\n  unsigned getOperand() const { return Operand; }\n  const SDNode *getNode() const { return Node; }\n};\n\ntemplate <> struct GraphTraits<SDNode*> {\n  using NodeRef = SDNode *;\n  using ChildIteratorType = SDNodeIterator;\n\n  static NodeRef getEntryNode(SDNode *N) { return N; }\n\n  static ChildIteratorType child_begin(NodeRef N) {\n    return SDNodeIterator::begin(N);\n  }\n\n  static ChildIteratorType child_end(NodeRef N) {\n    return SDNodeIterator::end(N);\n  }\n};\n\n/// A representation of the largest SDNode, for use in sizeof().\n///\n/// This needs to be a union because the largest node differs on 32 bit systems\n/// with 4 and 8 byte pointer alignment, respectively.\nusing LargestSDNode = AlignedCharArrayUnion<AtomicSDNode, TargetIndexSDNode,\n                                            BlockAddressSDNode,\n                                            GlobalAddressSDNode,\n                                            PseudoProbeSDNode>;\n\n/// The SDNode class with the greatest alignment requirement.\nusing MostAlignedSDNode = GlobalAddressSDNode;\n\nnamespace ISD {\n\n  /// Returns true if the specified node is a non-extending and unindexed load.\n  inline bool isNormalLoad(const SDNode *N) {\n    const LoadSDNode *Ld = dyn_cast<LoadSDNode>(N);\n    return Ld && Ld->getExtensionType() == ISD::NON_EXTLOAD &&\n      Ld->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Returns true if the specified node is a non-extending load.\n  inline bool isNON_EXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::NON_EXTLOAD;\n  }\n\n  /// Returns true if the specified node is a EXTLOAD.\n  inline bool isEXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::EXTLOAD;\n  }\n\n  /// Returns true if the specified node is a SEXTLOAD.\n  inline bool isSEXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::SEXTLOAD;\n  }\n\n  /// Returns true if the specified node is a ZEXTLOAD.\n  inline bool isZEXTLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getExtensionType() == ISD::ZEXTLOAD;\n  }\n\n  /// Returns true if the specified node is an unindexed load.\n  inline bool isUNINDEXEDLoad(const SDNode *N) {\n    return isa<LoadSDNode>(N) &&\n      cast<LoadSDNode>(N)->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Returns true if the specified node is a non-truncating\n  /// and unindexed store.\n  inline bool isNormalStore(const SDNode *N) {\n    const StoreSDNode *St = dyn_cast<StoreSDNode>(N);\n    return St && !St->isTruncatingStore() &&\n      St->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Returns true if the specified node is a non-truncating store.\n  inline bool isNON_TRUNCStore(const SDNode *N) {\n    return isa<StoreSDNode>(N) && !cast<StoreSDNode>(N)->isTruncatingStore();\n  }\n\n  /// Returns true if the specified node is a truncating store.\n  inline bool isTRUNCStore(const SDNode *N) {\n    return isa<StoreSDNode>(N) && cast<StoreSDNode>(N)->isTruncatingStore();\n  }\n\n  /// Returns true if the specified node is an unindexed store.\n  inline bool isUNINDEXEDStore(const SDNode *N) {\n    return isa<StoreSDNode>(N) &&\n      cast<StoreSDNode>(N)->getAddressingMode() == ISD::UNINDEXED;\n  }\n\n  /// Attempt to match a unary predicate against a scalar/splat constant or\n  /// every element of a constant BUILD_VECTOR.\n  /// If AllowUndef is true, then UNDEF elements will pass nullptr to Match.\n  bool matchUnaryPredicate(SDValue Op,\n                           std::function<bool(ConstantSDNode *)> Match,\n                           bool AllowUndefs = false);\n\n  /// Attempt to match a binary predicate against a pair of scalar/splat\n  /// constants or every element of a pair of constant BUILD_VECTORs.\n  /// If AllowUndef is true, then UNDEF elements will pass nullptr to Match.\n  /// If AllowTypeMismatch is true then RetType + ArgTypes don't need to match.\n  bool matchBinaryPredicate(\n      SDValue LHS, SDValue RHS,\n      std::function<bool(ConstantSDNode *, ConstantSDNode *)> Match,\n      bool AllowUndefs = false, bool AllowTypeMismatch = false);\n\n  /// Returns true if the specified value is the overflow result from one\n  /// of the overflow intrinsic nodes.\n  inline bool isOverflowIntrOpRes(SDValue Op) {\n    unsigned Opc = Op.getOpcode();\n    return (Op.getResNo() == 1 &&\n            (Opc == ISD::SADDO || Opc == ISD::UADDO || Opc == ISD::SSUBO ||\n             Opc == ISD::USUBO || Opc == ISD::SMULO || Opc == ISD::UMULO));\n  }\n\n} // end namespace ISD\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_SELECTIONDAGNODES_H\n"}, "53": {"id": 53, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetCallingConv.h", "content": "//===-- llvm/CodeGen/TargetCallingConv.h - Calling Convention ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines types for working with calling-convention information.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_TARGETCALLINGCONV_H\n#define LLVM_CODEGEN_TARGETCALLINGCONV_H\n\n#include \"llvm/CodeGen/ValueTypes.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include \"llvm/Support/MathExtras.h\"\n#include <cassert>\n#include <climits>\n#include <cstdint>\n\nnamespace llvm {\nnamespace ISD {\n\n  struct ArgFlagsTy {\n  private:\n    unsigned IsZExt : 1;     ///< Zero extended\n    unsigned IsSExt : 1;     ///< Sign extended\n    unsigned IsInReg : 1;    ///< Passed in register\n    unsigned IsSRet : 1;     ///< Hidden struct-ret ptr\n    unsigned IsByVal : 1;    ///< Struct passed by value\n    unsigned IsByRef : 1;    ///< Passed in memory\n    unsigned IsNest : 1;     ///< Nested fn static chain\n    unsigned IsReturned : 1; ///< Always returned\n    unsigned IsSplit : 1;\n    unsigned IsInAlloca : 1;   ///< Passed with inalloca\n    unsigned IsPreallocated : 1; ///< ByVal without the copy\n    unsigned IsSplitEnd : 1;   ///< Last part of a split\n    unsigned IsSwiftSelf : 1;  ///< Swift self parameter\n    unsigned IsSwiftError : 1; ///< Swift error parameter\n    unsigned IsCFGuardTarget : 1; ///< Control Flow Guard target\n    unsigned IsHva : 1;        ///< HVA field for\n    unsigned IsHvaStart : 1;   ///< HVA structure start\n    unsigned IsSecArgPass : 1; ///< Second argument\n    unsigned ByValOrByRefAlign : 4; ///< Log 2 of byval/byref alignment\n    unsigned OrigAlign : 5;    ///< Log 2 of original alignment\n    unsigned IsInConsecutiveRegsLast : 1;\n    unsigned IsInConsecutiveRegs : 1;\n    unsigned IsCopyElisionCandidate : 1; ///< Argument copy elision candidate\n    unsigned IsPointer : 1;\n\n    unsigned ByValOrByRefSize; ///< Byval or byref struct size\n\n    unsigned PointerAddrSpace; ///< Address space of pointer argument\n\n    /// Set the alignment used by byref or byval parameters.\n    void setAlignImpl(Align A) {\n      ByValOrByRefAlign = encode(A);\n      assert(getNonZeroByValAlign() == A && \"bitfield overflow\");\n    }\n\n  public:\n    ArgFlagsTy()\n      : IsZExt(0), IsSExt(0), IsInReg(0), IsSRet(0), IsByVal(0), IsByRef(0),\n          IsNest(0), IsReturned(0), IsSplit(0), IsInAlloca(0), IsPreallocated(0),\n          IsSplitEnd(0), IsSwiftSelf(0), IsSwiftError(0), IsCFGuardTarget(0),\n          IsHva(0), IsHvaStart(0), IsSecArgPass(0), ByValOrByRefAlign(0),\n          OrigAlign(0), IsInConsecutiveRegsLast(0), IsInConsecutiveRegs(0),\n          IsCopyElisionCandidate(0), IsPointer(0), ByValOrByRefSize(0),\n          PointerAddrSpace(0) {\n      static_assert(sizeof(*this) == 3 * sizeof(unsigned), \"flags are too big\");\n    }\n\n    bool isZExt() const { return IsZExt; }\n    void setZExt() { IsZExt = 1; }\n\n    bool isSExt() const { return IsSExt; }\n    void setSExt() { IsSExt = 1; }\n\n    bool isInReg() const { return IsInReg; }\n    void setInReg() { IsInReg = 1; }\n\n    bool isSRet() const { return IsSRet; }\n    void setSRet() { IsSRet = 1; }\n\n    bool isByVal() const { return IsByVal; }\n    void setByVal() { IsByVal = 1; }\n\n    bool isByRef() const { return IsByRef; }\n    void setByRef() { IsByRef = 1; }\n\n    bool isInAlloca() const { return IsInAlloca; }\n    void setInAlloca() { IsInAlloca = 1; }\n\n    bool isPreallocated() const { return IsPreallocated; }\n    void setPreallocated() { IsPreallocated = 1; }\n\n    bool isSwiftSelf() const { return IsSwiftSelf; }\n    void setSwiftSelf() { IsSwiftSelf = 1; }\n\n    bool isSwiftError() const { return IsSwiftError; }\n    void setSwiftError() { IsSwiftError = 1; }\n\n    bool isCFGuardTarget() const { return IsCFGuardTarget; }\n    void setCFGuardTarget() { IsCFGuardTarget = 1; }\n\n    bool isHva() const { return IsHva; }\n    void setHva() { IsHva = 1; }\n\n    bool isHvaStart() const { return IsHvaStart; }\n    void setHvaStart() { IsHvaStart = 1; }\n\n    bool isSecArgPass() const { return IsSecArgPass; }\n    void setSecArgPass() { IsSecArgPass = 1; }\n\n    bool isNest() const { return IsNest; }\n    void setNest() { IsNest = 1; }\n\n    bool isReturned() const { return IsReturned; }\n    void setReturned(bool V = true) { IsReturned = V; }\n\n    bool isInConsecutiveRegs()  const { return IsInConsecutiveRegs; }\n    void setInConsecutiveRegs(bool Flag = true) { IsInConsecutiveRegs = Flag; }\n\n    bool isInConsecutiveRegsLast() const { return IsInConsecutiveRegsLast; }\n    void setInConsecutiveRegsLast(bool Flag = true) {\n      IsInConsecutiveRegsLast = Flag;\n    }\n\n    bool isSplit()   const { return IsSplit; }\n    void setSplit()  { IsSplit = 1; }\n\n    bool isSplitEnd()   const { return IsSplitEnd; }\n    void setSplitEnd()  { IsSplitEnd = 1; }\n\n    bool isCopyElisionCandidate()  const { return IsCopyElisionCandidate; }\n    void setCopyElisionCandidate() { IsCopyElisionCandidate = 1; }\n\n    bool isPointer()  const { return IsPointer; }\n    void setPointer() { IsPointer = 1; }\n\n    LLVM_ATTRIBUTE_DEPRECATED(unsigned getByValAlign() const,\n                              \"Use getNonZeroByValAlign() instead\") {\n      MaybeAlign A = decodeMaybeAlign(ByValOrByRefAlign);\n      return A ? A->value() : 0;\n    }\n    Align getNonZeroByValAlign() const {\n      MaybeAlign A = decodeMaybeAlign(ByValOrByRefAlign);\n      assert(A && \"ByValAlign must be defined\");\n      return *A;\n    }\n    void setByValAlign(Align A) {\n      assert(isByVal() && !isByRef());\n      setAlignImpl(A);\n    }\n\n    void setByRefAlign(Align A) {\n      assert(!isByVal() && isByRef());\n      setAlignImpl(A);\n    }\n\n    LLVM_ATTRIBUTE_DEPRECATED(unsigned getOrigAlign() const,\n                              \"Use getNonZeroOrigAlign() instead\") {\n      MaybeAlign A = decodeMaybeAlign(OrigAlign);\n      return A ? A->value() : 0;\n    }\n    Align getNonZeroOrigAlign() const {\n      return decodeMaybeAlign(OrigAlign).valueOrOne();\n    }\n    void setOrigAlign(Align A) {\n      OrigAlign = encode(A);\n      assert(getNonZeroOrigAlign() == A && \"bitfield overflow\");\n    }\n\n    unsigned getByValSize() const {\n      assert(isByVal() && !isByRef());\n      return ByValOrByRefSize;\n    }\n    void setByValSize(unsigned S) {\n      assert(isByVal() && !isByRef());\n      ByValOrByRefSize = S;\n    }\n\n    unsigned getByRefSize() const {\n      assert(!isByVal() && isByRef());\n      return ByValOrByRefSize;\n    }\n    void setByRefSize(unsigned S) {\n      assert(!isByVal() && isByRef());\n      ByValOrByRefSize = S;\n    }\n\n    unsigned getPointerAddrSpace() const { return PointerAddrSpace; }\n    void setPointerAddrSpace(unsigned AS) { PointerAddrSpace = AS; }\n};\n\n  /// InputArg - This struct carries flags and type information about a\n  /// single incoming (formal) argument or incoming (from the perspective\n  /// of the caller) return value virtual register.\n  ///\n  struct InputArg {\n    ArgFlagsTy Flags;\n    MVT VT = MVT::Other;\n    EVT ArgVT;\n    bool Used = false;\n\n    /// Index original Function's argument.\n    unsigned OrigArgIndex;\n    /// Sentinel value for implicit machine-level input arguments.\n    static const unsigned NoArgIndex = UINT_MAX;\n\n    /// Offset in bytes of current input value relative to the beginning of\n    /// original argument. E.g. if argument was splitted into four 32 bit\n    /// registers, we got 4 InputArgs with PartOffsets 0, 4, 8 and 12.\n    unsigned PartOffset;\n\n    InputArg() = default;\n    InputArg(ArgFlagsTy flags, EVT vt, EVT argvt, bool used,\n             unsigned origIdx, unsigned partOffs)\n      : Flags(flags), Used(used), OrigArgIndex(origIdx), PartOffset(partOffs) {\n      VT = vt.getSimpleVT();\n      ArgVT = argvt;\n    }\n\n    bool isOrigArg() const {\n      return OrigArgIndex != NoArgIndex;\n    }\n\n    unsigned getOrigArgIndex() const {\n      assert(OrigArgIndex != NoArgIndex && \"Implicit machine-level argument\");\n      return OrigArgIndex;\n    }\n  };\n\n  /// OutputArg - This struct carries flags and a value for a\n  /// single outgoing (actual) argument or outgoing (from the perspective\n  /// of the caller) return value virtual register.\n  ///\n  struct OutputArg {\n    ArgFlagsTy Flags;\n    MVT VT;\n    EVT ArgVT;\n\n    /// IsFixed - Is this a \"fixed\" value, ie not passed through a vararg \"...\".\n    bool IsFixed = false;\n\n    /// Index original Function's argument.\n    unsigned OrigArgIndex;\n\n    /// Offset in bytes of current output value relative to the beginning of\n    /// original argument. E.g. if argument was splitted into four 32 bit\n    /// registers, we got 4 OutputArgs with PartOffsets 0, 4, 8 and 12.\n    unsigned PartOffset;\n\n    OutputArg() = default;\n    OutputArg(ArgFlagsTy flags, EVT vt, EVT argvt, bool isfixed,\n              unsigned origIdx, unsigned partOffs)\n      : Flags(flags), IsFixed(isfixed), OrigArgIndex(origIdx),\n        PartOffset(partOffs) {\n      VT = vt.getSimpleVT();\n      ArgVT = argvt;\n    }\n  };\n\n} // end namespace ISD\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_TARGETCALLINGCONV_H\n"}, "54": {"id": 54, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetInstrInfo.h", "content": "//===- llvm/CodeGen/TargetInstrInfo.h - Instruction Info --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file describes the target machine instruction set to the code generator.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_TARGETINSTRINFO_H\n#define LLVM_CODEGEN_TARGETINSTRINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DenseMapInfo.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/CodeGen/MIRFormatter.h\"\n#include \"llvm/CodeGen/MachineBasicBlock.h\"\n#include \"llvm/CodeGen/MachineCombinerPattern.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineInstr.h\"\n#include \"llvm/CodeGen/MachineInstrBuilder.h\"\n#include \"llvm/CodeGen/MachineOperand.h\"\n#include \"llvm/CodeGen/MachineOutliner.h\"\n#include \"llvm/CodeGen/RegisterClassInfo.h\"\n#include \"llvm/CodeGen/VirtRegMap.h\"\n#include \"llvm/MC/MCInstrInfo.h\"\n#include \"llvm/Support/BranchProbability.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass AAResults;\nclass DFAPacketizer;\nclass InstrItineraryData;\nclass LiveIntervals;\nclass LiveVariables;\nclass MachineLoop;\nclass MachineMemOperand;\nclass MachineRegisterInfo;\nclass MCAsmInfo;\nclass MCInst;\nstruct MCSchedModel;\nclass Module;\nclass ScheduleDAG;\nclass ScheduleDAGMI;\nclass ScheduleHazardRecognizer;\nclass SDNode;\nclass SelectionDAG;\nclass RegScavenger;\nclass TargetRegisterClass;\nclass TargetRegisterInfo;\nclass TargetSchedModel;\nclass TargetSubtargetInfo;\n\ntemplate <class T> class SmallVectorImpl;\n\nusing ParamLoadedValue = std::pair<MachineOperand, DIExpression*>;\n\nstruct DestSourcePair {\n  const MachineOperand *Destination;\n  const MachineOperand *Source;\n\n  DestSourcePair(const MachineOperand &Dest, const MachineOperand &Src)\n      : Destination(&Dest), Source(&Src) {}\n};\n\n/// Used to describe a register and immediate addition.\nstruct RegImmPair {\n  Register Reg;\n  int64_t Imm;\n\n  RegImmPair(Register Reg, int64_t Imm) : Reg(Reg), Imm(Imm) {}\n};\n\n/// Used to describe addressing mode similar to ExtAddrMode in CodeGenPrepare.\n/// It holds the register values, the scale value and the displacement.\nstruct ExtAddrMode {\n  Register BaseReg;\n  Register ScaledReg;\n  int64_t Scale;\n  int64_t Displacement;\n};\n\n//---------------------------------------------------------------------------\n///\n/// TargetInstrInfo - Interface to description of machine instruction set\n///\nclass TargetInstrInfo : public MCInstrInfo {\npublic:\n  TargetInstrInfo(unsigned CFSetupOpcode = ~0u, unsigned CFDestroyOpcode = ~0u,\n                  unsigned CatchRetOpcode = ~0u, unsigned ReturnOpcode = ~0u)\n      : CallFrameSetupOpcode(CFSetupOpcode),\n        CallFrameDestroyOpcode(CFDestroyOpcode), CatchRetOpcode(CatchRetOpcode),\n        ReturnOpcode(ReturnOpcode) {}\n  TargetInstrInfo(const TargetInstrInfo &) = delete;\n  TargetInstrInfo &operator=(const TargetInstrInfo &) = delete;\n  virtual ~TargetInstrInfo();\n\n  static bool isGenericOpcode(unsigned Opc) {\n    return Opc <= TargetOpcode::GENERIC_OP_END;\n  }\n\n  /// Given a machine instruction descriptor, returns the register\n  /// class constraint for OpNum, or NULL.\n  virtual\n  const TargetRegisterClass *getRegClass(const MCInstrDesc &MCID, unsigned OpNum,\n                                         const TargetRegisterInfo *TRI,\n                                         const MachineFunction &MF) const;\n\n  /// Return true if the instruction is trivially rematerializable, meaning it\n  /// has no side effects and requires no operands that aren't always available.\n  /// This means the only allowed uses are constants and unallocatable physical\n  /// registers so that the instructions result is independent of the place\n  /// in the function.\n  bool isTriviallyReMaterializable(const MachineInstr &MI,\n                                   AAResults *AA = nullptr) const {\n    return MI.getOpcode() == TargetOpcode::IMPLICIT_DEF ||\n           (MI.getDesc().isRematerializable() &&\n            (isReallyTriviallyReMaterializable(MI, AA) ||\n             isReallyTriviallyReMaterializableGeneric(MI, AA)));\n  }\n\nprotected:\n  /// For instructions with opcodes for which the M_REMATERIALIZABLE flag is\n  /// set, this hook lets the target specify whether the instruction is actually\n  /// trivially rematerializable, taking into consideration its operands. This\n  /// predicate must return false if the instruction has any side effects other\n  /// than producing a value, or if it requres any address registers that are\n  /// not always available.\n  /// Requirements must be check as stated in isTriviallyReMaterializable() .\n  virtual bool isReallyTriviallyReMaterializable(const MachineInstr &MI,\n                                                 AAResults *AA) const {\n    return false;\n  }\n\n  /// This method commutes the operands of the given machine instruction MI.\n  /// The operands to be commuted are specified by their indices OpIdx1 and\n  /// OpIdx2.\n  ///\n  /// If a target has any instructions that are commutable but require\n  /// converting to different instructions or making non-trivial changes\n  /// to commute them, this method can be overloaded to do that.\n  /// The default implementation simply swaps the commutable operands.\n  ///\n  /// If NewMI is false, MI is modified in place and returned; otherwise, a\n  /// new machine instruction is created and returned.\n  ///\n  /// Do not call this method for a non-commutable instruction.\n  /// Even though the instruction is commutable, the method may still\n  /// fail to commute the operands, null pointer is returned in such cases.\n  virtual MachineInstr *commuteInstructionImpl(MachineInstr &MI, bool NewMI,\n                                               unsigned OpIdx1,\n                                               unsigned OpIdx2) const;\n\n  /// Assigns the (CommutableOpIdx1, CommutableOpIdx2) pair of commutable\n  /// operand indices to (ResultIdx1, ResultIdx2).\n  /// One or both input values of the pair: (ResultIdx1, ResultIdx2) may be\n  /// predefined to some indices or be undefined (designated by the special\n  /// value 'CommuteAnyOperandIndex').\n  /// The predefined result indices cannot be re-defined.\n  /// The function returns true iff after the result pair redefinition\n  /// the fixed result pair is equal to or equivalent to the source pair of\n  /// indices: (CommutableOpIdx1, CommutableOpIdx2). It is assumed here that\n  /// the pairs (x,y) and (y,x) are equivalent.\n  static bool fixCommutedOpIndices(unsigned &ResultIdx1, unsigned &ResultIdx2,\n                                   unsigned CommutableOpIdx1,\n                                   unsigned CommutableOpIdx2);\n\nprivate:\n  /// For instructions with opcodes for which the M_REMATERIALIZABLE flag is\n  /// set and the target hook isReallyTriviallyReMaterializable returns false,\n  /// this function does target-independent tests to determine if the\n  /// instruction is really trivially rematerializable.\n  bool isReallyTriviallyReMaterializableGeneric(const MachineInstr &MI,\n                                                AAResults *AA) const;\n\npublic:\n  /// These methods return the opcode of the frame setup/destroy instructions\n  /// if they exist (-1 otherwise).  Some targets use pseudo instructions in\n  /// order to abstract away the difference between operating with a frame\n  /// pointer and operating without, through the use of these two instructions.\n  ///\n  unsigned getCallFrameSetupOpcode() const { return CallFrameSetupOpcode; }\n  unsigned getCallFrameDestroyOpcode() const { return CallFrameDestroyOpcode; }\n\n  /// Returns true if the argument is a frame pseudo instruction.\n  bool isFrameInstr(const MachineInstr &I) const {\n    return I.getOpcode() == getCallFrameSetupOpcode() ||\n           I.getOpcode() == getCallFrameDestroyOpcode();\n  }\n\n  /// Returns true if the argument is a frame setup pseudo instruction.\n  bool isFrameSetup(const MachineInstr &I) const {\n    return I.getOpcode() == getCallFrameSetupOpcode();\n  }\n\n  /// Returns size of the frame associated with the given frame instruction.\n  /// For frame setup instruction this is frame that is set up space set up\n  /// after the instruction. For frame destroy instruction this is the frame\n  /// freed by the caller.\n  /// Note, in some cases a call frame (or a part of it) may be prepared prior\n  /// to the frame setup instruction. It occurs in the calls that involve\n  /// inalloca arguments. This function reports only the size of the frame part\n  /// that is set up between the frame setup and destroy pseudo instructions.\n  int64_t getFrameSize(const MachineInstr &I) const {\n    assert(isFrameInstr(I) && \"Not a frame instruction\");\n    assert(I.getOperand(0).getImm() >= 0);\n    return I.getOperand(0).getImm();\n  }\n\n  /// Returns the total frame size, which is made up of the space set up inside\n  /// the pair of frame start-stop instructions and the space that is set up\n  /// prior to the pair.\n  int64_t getFrameTotalSize(const MachineInstr &I) const {\n    if (isFrameSetup(I)) {\n      assert(I.getOperand(1).getImm() >= 0 &&\n             \"Frame size must not be negative\");\n      return getFrameSize(I) + I.getOperand(1).getImm();\n    }\n    return getFrameSize(I);\n  }\n\n  unsigned getCatchReturnOpcode() const { return CatchRetOpcode; }\n  unsigned getReturnOpcode() const { return ReturnOpcode; }\n\n  /// Returns the actual stack pointer adjustment made by an instruction\n  /// as part of a call sequence. By default, only call frame setup/destroy\n  /// instructions adjust the stack, but targets may want to override this\n  /// to enable more fine-grained adjustment, or adjust by a different value.\n  virtual int getSPAdjust(const MachineInstr &MI) const;\n\n  /// Return true if the instruction is a \"coalescable\" extension instruction.\n  /// That is, it's like a copy where it's legal for the source to overlap the\n  /// destination. e.g. X86::MOVSX64rr32. If this returns true, then it's\n  /// expected the pre-extension value is available as a subreg of the result\n  /// register. This also returns the sub-register index in SubIdx.\n  virtual bool isCoalescableExtInstr(const MachineInstr &MI, Register &SrcReg,\n                                     Register &DstReg, unsigned &SubIdx) const {\n    return false;\n  }\n\n  /// If the specified machine instruction is a direct\n  /// load from a stack slot, return the virtual or physical register number of\n  /// the destination along with the FrameIndex of the loaded stack slot.  If\n  /// not, return 0.  This predicate must return 0 if the instruction has\n  /// any side effects other than loading from the stack slot.\n  virtual unsigned isLoadFromStackSlot(const MachineInstr &MI,\n                                       int &FrameIndex) const {\n    return 0;\n  }\n\n  /// Optional extension of isLoadFromStackSlot that returns the number of\n  /// bytes loaded from the stack. This must be implemented if a backend\n  /// supports partial stack slot spills/loads to further disambiguate\n  /// what the load does.\n  virtual unsigned isLoadFromStackSlot(const MachineInstr &MI,\n                                       int &FrameIndex,\n                                       unsigned &MemBytes) const {\n    MemBytes = 0;\n    return isLoadFromStackSlot(MI, FrameIndex);\n  }\n\n  /// Check for post-frame ptr elimination stack locations as well.\n  /// This uses a heuristic so it isn't reliable for correctness.\n  virtual unsigned isLoadFromStackSlotPostFE(const MachineInstr &MI,\n                                             int &FrameIndex) const {\n    return 0;\n  }\n\n  /// If the specified machine instruction has a load from a stack slot,\n  /// return true along with the FrameIndices of the loaded stack slot and the\n  /// machine mem operands containing the reference.\n  /// If not, return false.  Unlike isLoadFromStackSlot, this returns true for\n  /// any instructions that loads from the stack.  This is just a hint, as some\n  /// cases may be missed.\n  virtual bool hasLoadFromStackSlot(\n      const MachineInstr &MI,\n      SmallVectorImpl<const MachineMemOperand *> &Accesses) const;\n\n  /// If the specified machine instruction is a direct\n  /// store to a stack slot, return the virtual or physical register number of\n  /// the source reg along with the FrameIndex of the loaded stack slot.  If\n  /// not, return 0.  This predicate must return 0 if the instruction has\n  /// any side effects other than storing to the stack slot.\n  virtual unsigned isStoreToStackSlot(const MachineInstr &MI,\n                                      int &FrameIndex) const {\n    return 0;\n  }\n\n  /// Optional extension of isStoreToStackSlot that returns the number of\n  /// bytes stored to the stack. This must be implemented if a backend\n  /// supports partial stack slot spills/loads to further disambiguate\n  /// what the store does.\n  virtual unsigned isStoreToStackSlot(const MachineInstr &MI,\n                                      int &FrameIndex,\n                                      unsigned &MemBytes) const {\n    MemBytes = 0;\n    return isStoreToStackSlot(MI, FrameIndex);\n  }\n\n  /// Check for post-frame ptr elimination stack locations as well.\n  /// This uses a heuristic, so it isn't reliable for correctness.\n  virtual unsigned isStoreToStackSlotPostFE(const MachineInstr &MI,\n                                            int &FrameIndex) const {\n    return 0;\n  }\n\n  /// If the specified machine instruction has a store to a stack slot,\n  /// return true along with the FrameIndices of the loaded stack slot and the\n  /// machine mem operands containing the reference.\n  /// If not, return false.  Unlike isStoreToStackSlot,\n  /// this returns true for any instructions that stores to the\n  /// stack.  This is just a hint, as some cases may be missed.\n  virtual bool hasStoreToStackSlot(\n      const MachineInstr &MI,\n      SmallVectorImpl<const MachineMemOperand *> &Accesses) const;\n\n  /// Return true if the specified machine instruction\n  /// is a copy of one stack slot to another and has no other effect.\n  /// Provide the identity of the two frame indices.\n  virtual bool isStackSlotCopy(const MachineInstr &MI, int &DestFrameIndex,\n                               int &SrcFrameIndex) const {\n    return false;\n  }\n\n  /// Compute the size in bytes and offset within a stack slot of a spilled\n  /// register or subregister.\n  ///\n  /// \\param [out] Size in bytes of the spilled value.\n  /// \\param [out] Offset in bytes within the stack slot.\n  /// \\returns true if both Size and Offset are successfully computed.\n  ///\n  /// Not all subregisters have computable spill slots. For example,\n  /// subregisters registers may not be byte-sized, and a pair of discontiguous\n  /// subregisters has no single offset.\n  ///\n  /// Targets with nontrivial bigendian implementations may need to override\n  /// this, particularly to support spilled vector registers.\n  virtual bool getStackSlotRange(const TargetRegisterClass *RC, unsigned SubIdx,\n                                 unsigned &Size, unsigned &Offset,\n                                 const MachineFunction &MF) const;\n\n  /// Return true if the given instruction is terminator that is unspillable,\n  /// according to isUnspillableTerminatorImpl.\n  bool isUnspillableTerminator(const MachineInstr *MI) const {\n    return MI->isTerminator() && isUnspillableTerminatorImpl(MI);\n  }\n\n  /// Returns the size in bytes of the specified MachineInstr, or ~0U\n  /// when this function is not implemented by a target.\n  virtual unsigned getInstSizeInBytes(const MachineInstr &MI) const {\n    return ~0U;\n  }\n\n  /// Return true if the instruction is as cheap as a move instruction.\n  ///\n  /// Targets for different archs need to override this, and different\n  /// micro-architectures can also be finely tuned inside.\n  virtual bool isAsCheapAsAMove(const MachineInstr &MI) const {\n    return MI.isAsCheapAsAMove();\n  }\n\n  /// Return true if the instruction should be sunk by MachineSink.\n  ///\n  /// MachineSink determines on its own whether the instruction is safe to sink;\n  /// this gives the target a hook to override the default behavior with regards\n  /// to which instructions should be sunk.\n  virtual bool shouldSink(const MachineInstr &MI) const { return true; }\n\n  /// Re-issue the specified 'original' instruction at the\n  /// specific location targeting a new destination register.\n  /// The register in Orig->getOperand(0).getReg() will be substituted by\n  /// DestReg:SubIdx. Any existing subreg index is preserved or composed with\n  /// SubIdx.\n  virtual void reMaterialize(MachineBasicBlock &MBB,\n                             MachineBasicBlock::iterator MI, Register DestReg,\n                             unsigned SubIdx, const MachineInstr &Orig,\n                             const TargetRegisterInfo &TRI) const;\n\n  /// Clones instruction or the whole instruction bundle \\p Orig and\n  /// insert into \\p MBB before \\p InsertBefore. The target may update operands\n  /// that are required to be unique.\n  ///\n  /// \\p Orig must not return true for MachineInstr::isNotDuplicable().\n  virtual MachineInstr &duplicate(MachineBasicBlock &MBB,\n                                  MachineBasicBlock::iterator InsertBefore,\n                                  const MachineInstr &Orig) const;\n\n  /// This method must be implemented by targets that\n  /// set the M_CONVERTIBLE_TO_3_ADDR flag.  When this flag is set, the target\n  /// may be able to convert a two-address instruction into one or more true\n  /// three-address instructions on demand.  This allows the X86 target (for\n  /// example) to convert ADD and SHL instructions into LEA instructions if they\n  /// would require register copies due to two-addressness.\n  ///\n  /// This method returns a null pointer if the transformation cannot be\n  /// performed, otherwise it returns the last new instruction.\n  ///\n  virtual MachineInstr *convertToThreeAddress(MachineFunction::iterator &MFI,\n                                              MachineInstr &MI,\n                                              LiveVariables *LV) const {\n    return nullptr;\n  }\n\n  // This constant can be used as an input value of operand index passed to\n  // the method findCommutedOpIndices() to tell the method that the\n  // corresponding operand index is not pre-defined and that the method\n  // can pick any commutable operand.\n  static const unsigned CommuteAnyOperandIndex = ~0U;\n\n  /// This method commutes the operands of the given machine instruction MI.\n  ///\n  /// The operands to be commuted are specified by their indices OpIdx1 and\n  /// OpIdx2. OpIdx1 and OpIdx2 arguments may be set to a special value\n  /// 'CommuteAnyOperandIndex', which means that the method is free to choose\n  /// any arbitrarily chosen commutable operand. If both arguments are set to\n  /// 'CommuteAnyOperandIndex' then the method looks for 2 different commutable\n  /// operands; then commutes them if such operands could be found.\n  ///\n  /// If NewMI is false, MI is modified in place and returned; otherwise, a\n  /// new machine instruction is created and returned.\n  ///\n  /// Do not call this method for a non-commutable instruction or\n  /// for non-commuable operands.\n  /// Even though the instruction is commutable, the method may still\n  /// fail to commute the operands, null pointer is returned in such cases.\n  MachineInstr *\n  commuteInstruction(MachineInstr &MI, bool NewMI = false,\n                     unsigned OpIdx1 = CommuteAnyOperandIndex,\n                     unsigned OpIdx2 = CommuteAnyOperandIndex) const;\n\n  /// Returns true iff the routine could find two commutable operands in the\n  /// given machine instruction.\n  /// The 'SrcOpIdx1' and 'SrcOpIdx2' are INPUT and OUTPUT arguments.\n  /// If any of the INPUT values is set to the special value\n  /// 'CommuteAnyOperandIndex' then the method arbitrarily picks a commutable\n  /// operand, then returns its index in the corresponding argument.\n  /// If both of INPUT values are set to 'CommuteAnyOperandIndex' then method\n  /// looks for 2 commutable operands.\n  /// If INPUT values refer to some operands of MI, then the method simply\n  /// returns true if the corresponding operands are commutable and returns\n  /// false otherwise.\n  ///\n  /// For example, calling this method this way:\n  ///     unsigned Op1 = 1, Op2 = CommuteAnyOperandIndex;\n  ///     findCommutedOpIndices(MI, Op1, Op2);\n  /// can be interpreted as a query asking to find an operand that would be\n  /// commutable with the operand#1.\n  virtual bool findCommutedOpIndices(const MachineInstr &MI,\n                                     unsigned &SrcOpIdx1,\n                                     unsigned &SrcOpIdx2) const;\n\n  /// A pair composed of a register and a sub-register index.\n  /// Used to give some type checking when modeling Reg:SubReg.\n  struct RegSubRegPair {\n    Register Reg;\n    unsigned SubReg;\n\n    RegSubRegPair(Register Reg = Register(), unsigned SubReg = 0)\n        : Reg(Reg), SubReg(SubReg) {}\n\n    bool operator==(const RegSubRegPair& P) const {\n      return Reg == P.Reg && SubReg == P.SubReg;\n    }\n    bool operator!=(const RegSubRegPair& P) const {\n      return !(*this == P);\n    }\n  };\n\n  /// A pair composed of a pair of a register and a sub-register index,\n  /// and another sub-register index.\n  /// Used to give some type checking when modeling Reg:SubReg1, SubReg2.\n  struct RegSubRegPairAndIdx : RegSubRegPair {\n    unsigned SubIdx;\n\n    RegSubRegPairAndIdx(Register Reg = Register(), unsigned SubReg = 0,\n                        unsigned SubIdx = 0)\n        : RegSubRegPair(Reg, SubReg), SubIdx(SubIdx) {}\n  };\n\n  /// Build the equivalent inputs of a REG_SEQUENCE for the given \\p MI\n  /// and \\p DefIdx.\n  /// \\p [out] InputRegs of the equivalent REG_SEQUENCE. Each element of\n  /// the list is modeled as <Reg:SubReg, SubIdx>. Operands with the undef\n  /// flag are not added to this list.\n  /// E.g., REG_SEQUENCE %1:sub1, sub0, %2, sub1 would produce\n  /// two elements:\n  /// - %1:sub1, sub0\n  /// - %2<:0>, sub1\n  ///\n  /// \\returns true if it is possible to build such an input sequence\n  /// with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isRegSequence() or MI.isRegSequenceLike().\n  ///\n  /// \\note The generic implementation does not provide any support for\n  /// MI.isRegSequenceLike(). In other words, one has to override\n  /// getRegSequenceLikeInputs for target specific instructions.\n  bool\n  getRegSequenceInputs(const MachineInstr &MI, unsigned DefIdx,\n                       SmallVectorImpl<RegSubRegPairAndIdx> &InputRegs) const;\n\n  /// Build the equivalent inputs of a EXTRACT_SUBREG for the given \\p MI\n  /// and \\p DefIdx.\n  /// \\p [out] InputReg of the equivalent EXTRACT_SUBREG.\n  /// E.g., EXTRACT_SUBREG %1:sub1, sub0, sub1 would produce:\n  /// - %1:sub1, sub0\n  ///\n  /// \\returns true if it is possible to build such an input sequence\n  /// with the pair \\p MI, \\p DefIdx and the operand has no undef flag set.\n  /// False otherwise.\n  ///\n  /// \\pre MI.isExtractSubreg() or MI.isExtractSubregLike().\n  ///\n  /// \\note The generic implementation does not provide any support for\n  /// MI.isExtractSubregLike(). In other words, one has to override\n  /// getExtractSubregLikeInputs for target specific instructions.\n  bool getExtractSubregInputs(const MachineInstr &MI, unsigned DefIdx,\n                              RegSubRegPairAndIdx &InputReg) const;\n\n  /// Build the equivalent inputs of a INSERT_SUBREG for the given \\p MI\n  /// and \\p DefIdx.\n  /// \\p [out] BaseReg and \\p [out] InsertedReg contain\n  /// the equivalent inputs of INSERT_SUBREG.\n  /// E.g., INSERT_SUBREG %0:sub0, %1:sub1, sub3 would produce:\n  /// - BaseReg: %0:sub0\n  /// - InsertedReg: %1:sub1, sub3\n  ///\n  /// \\returns true if it is possible to build such an input sequence\n  /// with the pair \\p MI, \\p DefIdx and the operand has no undef flag set.\n  /// False otherwise.\n  ///\n  /// \\pre MI.isInsertSubreg() or MI.isInsertSubregLike().\n  ///\n  /// \\note The generic implementation does not provide any support for\n  /// MI.isInsertSubregLike(). In other words, one has to override\n  /// getInsertSubregLikeInputs for target specific instructions.\n  bool getInsertSubregInputs(const MachineInstr &MI, unsigned DefIdx,\n                             RegSubRegPair &BaseReg,\n                             RegSubRegPairAndIdx &InsertedReg) const;\n\n  /// Return true if two machine instructions would produce identical values.\n  /// By default, this is only true when the two instructions\n  /// are deemed identical except for defs. If this function is called when the\n  /// IR is still in SSA form, the caller can pass the MachineRegisterInfo for\n  /// aggressive checks.\n  virtual bool produceSameValue(const MachineInstr &MI0,\n                                const MachineInstr &MI1,\n                                const MachineRegisterInfo *MRI = nullptr) const;\n\n  /// \\returns true if a branch from an instruction with opcode \\p BranchOpc\n  ///  bytes is capable of jumping to a position \\p BrOffset bytes away.\n  virtual bool isBranchOffsetInRange(unsigned BranchOpc,\n                                     int64_t BrOffset) const {\n    llvm_unreachable(\"target did not implement\");\n  }\n\n  /// \\returns The block that branch instruction \\p MI jumps to.\n  virtual MachineBasicBlock *getBranchDestBlock(const MachineInstr &MI) const {\n    llvm_unreachable(\"target did not implement\");\n  }\n\n  /// Insert an unconditional indirect branch at the end of \\p MBB to \\p\n  /// NewDestBB.  \\p BrOffset indicates the offset of \\p NewDestBB relative to\n  /// the offset of the position to insert the new branch.\n  ///\n  /// \\returns The number of bytes added to the block.\n  virtual unsigned insertIndirectBranch(MachineBasicBlock &MBB,\n                                        MachineBasicBlock &NewDestBB,\n                                        const DebugLoc &DL,\n                                        int64_t BrOffset = 0,\n                                        RegScavenger *RS = nullptr) const {\n    llvm_unreachable(\"target did not implement\");\n  }\n\n  /// Analyze the branching code at the end of MBB, returning\n  /// true if it cannot be understood (e.g. it's a switch dispatch or isn't\n  /// implemented for a target).  Upon success, this returns false and returns\n  /// with the following information in various cases:\n  ///\n  /// 1. If this block ends with no branches (it just falls through to its succ)\n  ///    just return false, leaving TBB/FBB null.\n  /// 2. If this block ends with only an unconditional branch, it sets TBB to be\n  ///    the destination block.\n  /// 3. If this block ends with a conditional branch and it falls through to a\n  ///    successor block, it sets TBB to be the branch destination block and a\n  ///    list of operands that evaluate the condition. These operands can be\n  ///    passed to other TargetInstrInfo methods to create new branches.\n  /// 4. If this block ends with a conditional branch followed by an\n  ///    unconditional branch, it returns the 'true' destination in TBB, the\n  ///    'false' destination in FBB, and a list of operands that evaluate the\n  ///    condition.  These operands can be passed to other TargetInstrInfo\n  ///    methods to create new branches.\n  ///\n  /// Note that removeBranch and insertBranch must be implemented to support\n  /// cases where this method returns success.\n  ///\n  /// If AllowModify is true, then this routine is allowed to modify the basic\n  /// block (e.g. delete instructions after the unconditional branch).\n  ///\n  /// The CFG information in MBB.Predecessors and MBB.Successors must be valid\n  /// before calling this function.\n  virtual bool analyzeBranch(MachineBasicBlock &MBB, MachineBasicBlock *&TBB,\n                             MachineBasicBlock *&FBB,\n                             SmallVectorImpl<MachineOperand> &Cond,\n                             bool AllowModify = false) const {\n    return true;\n  }\n\n  /// Represents a predicate at the MachineFunction level.  The control flow a\n  /// MachineBranchPredicate represents is:\n  ///\n  ///  Reg = LHS `Predicate` RHS         == ConditionDef\n  ///  if Reg then goto TrueDest else goto FalseDest\n  ///\n  struct MachineBranchPredicate {\n    enum ComparePredicate {\n      PRED_EQ,     // True if two values are equal\n      PRED_NE,     // True if two values are not equal\n      PRED_INVALID // Sentinel value\n    };\n\n    ComparePredicate Predicate = PRED_INVALID;\n    MachineOperand LHS = MachineOperand::CreateImm(0);\n    MachineOperand RHS = MachineOperand::CreateImm(0);\n    MachineBasicBlock *TrueDest = nullptr;\n    MachineBasicBlock *FalseDest = nullptr;\n    MachineInstr *ConditionDef = nullptr;\n\n    /// SingleUseCondition is true if ConditionDef is dead except for the\n    /// branch(es) at the end of the basic block.\n    ///\n    bool SingleUseCondition = false;\n\n    explicit MachineBranchPredicate() = default;\n  };\n\n  /// Analyze the branching code at the end of MBB and parse it into the\n  /// MachineBranchPredicate structure if possible.  Returns false on success\n  /// and true on failure.\n  ///\n  /// If AllowModify is true, then this routine is allowed to modify the basic\n  /// block (e.g. delete instructions after the unconditional branch).\n  ///\n  virtual bool analyzeBranchPredicate(MachineBasicBlock &MBB,\n                                      MachineBranchPredicate &MBP,\n                                      bool AllowModify = false) const {\n    return true;\n  }\n\n  /// Remove the branching code at the end of the specific MBB.\n  /// This is only invoked in cases where analyzeBranch returns success. It\n  /// returns the number of instructions that were removed.\n  /// If \\p BytesRemoved is non-null, report the change in code size from the\n  /// removed instructions.\n  virtual unsigned removeBranch(MachineBasicBlock &MBB,\n                                int *BytesRemoved = nullptr) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::removeBranch!\");\n  }\n\n  /// Insert branch code into the end of the specified MachineBasicBlock. The\n  /// operands to this method are the same as those returned by analyzeBranch.\n  /// This is only invoked in cases where analyzeBranch returns success. It\n  /// returns the number of instructions inserted. If \\p BytesAdded is non-null,\n  /// report the change in code size from the added instructions.\n  ///\n  /// It is also invoked by tail merging to add unconditional branches in\n  /// cases where analyzeBranch doesn't apply because there was no original\n  /// branch to analyze.  At least this much must be implemented, else tail\n  /// merging needs to be disabled.\n  ///\n  /// The CFG information in MBB.Predecessors and MBB.Successors must be valid\n  /// before calling this function.\n  virtual unsigned insertBranch(MachineBasicBlock &MBB, MachineBasicBlock *TBB,\n                                MachineBasicBlock *FBB,\n                                ArrayRef<MachineOperand> Cond,\n                                const DebugLoc &DL,\n                                int *BytesAdded = nullptr) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::insertBranch!\");\n  }\n\n  unsigned insertUnconditionalBranch(MachineBasicBlock &MBB,\n                                     MachineBasicBlock *DestBB,\n                                     const DebugLoc &DL,\n                                     int *BytesAdded = nullptr) const {\n    return insertBranch(MBB, DestBB, nullptr, ArrayRef<MachineOperand>(), DL,\n                        BytesAdded);\n  }\n\n  /// Object returned by analyzeLoopForPipelining. Allows software pipelining\n  /// implementations to query attributes of the loop being pipelined and to\n  /// apply target-specific updates to the loop once pipelining is complete.\n  class PipelinerLoopInfo {\n  public:\n    virtual ~PipelinerLoopInfo();\n    /// Return true if the given instruction should not be pipelined and should\n    /// be ignored. An example could be a loop comparison, or induction variable\n    /// update with no users being pipelined.\n    virtual bool shouldIgnoreForPipelining(const MachineInstr *MI) const = 0;\n\n    /// Create a condition to determine if the trip count of the loop is greater\n    /// than TC.\n    ///\n    /// If the trip count is statically known to be greater than TC, return\n    /// true. If the trip count is statically known to be not greater than TC,\n    /// return false. Otherwise return nullopt and fill out Cond with the test\n    /// condition.\n    virtual Optional<bool>\n    createTripCountGreaterCondition(int TC, MachineBasicBlock &MBB,\n                                    SmallVectorImpl<MachineOperand> &Cond) = 0;\n\n    /// Modify the loop such that the trip count is\n    /// OriginalTC + TripCountAdjust.\n    virtual void adjustTripCount(int TripCountAdjust) = 0;\n\n    /// Called when the loop's preheader has been modified to NewPreheader.\n    virtual void setPreheader(MachineBasicBlock *NewPreheader) = 0;\n\n    /// Called when the loop is being removed. Any instructions in the preheader\n    /// should be removed.\n    ///\n    /// Once this function is called, no other functions on this object are\n    /// valid; the loop has been removed.\n    virtual void disposed() = 0;\n  };\n\n  /// Analyze loop L, which must be a single-basic-block loop, and if the\n  /// conditions can be understood enough produce a PipelinerLoopInfo object.\n  virtual std::unique_ptr<PipelinerLoopInfo>\n  analyzeLoopForPipelining(MachineBasicBlock *LoopBB) const {\n    return nullptr;\n  }\n\n  /// Analyze the loop code, return true if it cannot be understood. Upon\n  /// success, this function returns false and returns information about the\n  /// induction variable and compare instruction used at the end.\n  virtual bool analyzeLoop(MachineLoop &L, MachineInstr *&IndVarInst,\n                           MachineInstr *&CmpInst) const {\n    return true;\n  }\n\n  /// Generate code to reduce the loop iteration by one and check if the loop\n  /// is finished.  Return the value/register of the new loop count.  We need\n  /// this function when peeling off one or more iterations of a loop. This\n  /// function assumes the nth iteration is peeled first.\n  virtual unsigned reduceLoopCount(MachineBasicBlock &MBB,\n                                   MachineBasicBlock &PreHeader,\n                                   MachineInstr *IndVar, MachineInstr &Cmp,\n                                   SmallVectorImpl<MachineOperand> &Cond,\n                                   SmallVectorImpl<MachineInstr *> &PrevInsts,\n                                   unsigned Iter, unsigned MaxIter) const {\n    llvm_unreachable(\"Target didn't implement ReduceLoopCount\");\n  }\n\n  /// Delete the instruction OldInst and everything after it, replacing it with\n  /// an unconditional branch to NewDest. This is used by the tail merging pass.\n  virtual void ReplaceTailWithBranchTo(MachineBasicBlock::iterator Tail,\n                                       MachineBasicBlock *NewDest) const;\n\n  /// Return true if it's legal to split the given basic\n  /// block at the specified instruction (i.e. instruction would be the start\n  /// of a new basic block).\n  virtual bool isLegalToSplitMBBAt(MachineBasicBlock &MBB,\n                                   MachineBasicBlock::iterator MBBI) const {\n    return true;\n  }\n\n  /// Return true if it's profitable to predicate\n  /// instructions with accumulated instruction latency of \"NumCycles\"\n  /// of the specified basic block, where the probability of the instructions\n  /// being executed is given by Probability, and Confidence is a measure\n  /// of our confidence that it will be properly predicted.\n  virtual bool isProfitableToIfCvt(MachineBasicBlock &MBB, unsigned NumCycles,\n                                   unsigned ExtraPredCycles,\n                                   BranchProbability Probability) const {\n    return false;\n  }\n\n  /// Second variant of isProfitableToIfCvt. This one\n  /// checks for the case where two basic blocks from true and false path\n  /// of a if-then-else (diamond) are predicated on mutually exclusive\n  /// predicates, where the probability of the true path being taken is given\n  /// by Probability, and Confidence is a measure of our confidence that it\n  /// will be properly predicted.\n  virtual bool isProfitableToIfCvt(MachineBasicBlock &TMBB, unsigned NumTCycles,\n                                   unsigned ExtraTCycles,\n                                   MachineBasicBlock &FMBB, unsigned NumFCycles,\n                                   unsigned ExtraFCycles,\n                                   BranchProbability Probability) const {\n    return false;\n  }\n\n  /// Return true if it's profitable for if-converter to duplicate instructions\n  /// of specified accumulated instruction latencies in the specified MBB to\n  /// enable if-conversion.\n  /// The probability of the instructions being executed is given by\n  /// Probability, and Confidence is a measure of our confidence that it\n  /// will be properly predicted.\n  virtual bool isProfitableToDupForIfCvt(MachineBasicBlock &MBB,\n                                         unsigned NumCycles,\n                                         BranchProbability Probability) const {\n    return false;\n  }\n\n  /// Return the increase in code size needed to predicate a contiguous run of\n  /// NumInsts instructions.\n  virtual unsigned extraSizeToPredicateInstructions(const MachineFunction &MF,\n                                                    unsigned NumInsts) const {\n    return 0;\n  }\n\n  /// Return an estimate for the code size reduction (in bytes) which will be\n  /// caused by removing the given branch instruction during if-conversion.\n  virtual unsigned predictBranchSizeForIfCvt(MachineInstr &MI) const {\n    return getInstSizeInBytes(MI);\n  }\n\n  /// Return true if it's profitable to unpredicate\n  /// one side of a 'diamond', i.e. two sides of if-else predicated on mutually\n  /// exclusive predicates.\n  /// e.g.\n  ///   subeq  r0, r1, #1\n  ///   addne  r0, r1, #1\n  /// =>\n  ///   sub    r0, r1, #1\n  ///   addne  r0, r1, #1\n  ///\n  /// This may be profitable is conditional instructions are always executed.\n  virtual bool isProfitableToUnpredicate(MachineBasicBlock &TMBB,\n                                         MachineBasicBlock &FMBB) const {\n    return false;\n  }\n\n  /// Return true if it is possible to insert a select\n  /// instruction that chooses between TrueReg and FalseReg based on the\n  /// condition code in Cond.\n  ///\n  /// When successful, also return the latency in cycles from TrueReg,\n  /// FalseReg, and Cond to the destination register. In most cases, a select\n  /// instruction will be 1 cycle, so CondCycles = TrueCycles = FalseCycles = 1\n  ///\n  /// Some x86 implementations have 2-cycle cmov instructions.\n  ///\n  /// @param MBB         Block where select instruction would be inserted.\n  /// @param Cond        Condition returned by analyzeBranch.\n  /// @param DstReg      Virtual dest register that the result should write to.\n  /// @param TrueReg     Virtual register to select when Cond is true.\n  /// @param FalseReg    Virtual register to select when Cond is false.\n  /// @param CondCycles  Latency from Cond+Branch to select output.\n  /// @param TrueCycles  Latency from TrueReg to select output.\n  /// @param FalseCycles Latency from FalseReg to select output.\n  virtual bool canInsertSelect(const MachineBasicBlock &MBB,\n                               ArrayRef<MachineOperand> Cond, Register DstReg,\n                               Register TrueReg, Register FalseReg,\n                               int &CondCycles, int &TrueCycles,\n                               int &FalseCycles) const {\n    return false;\n  }\n\n  /// Insert a select instruction into MBB before I that will copy TrueReg to\n  /// DstReg when Cond is true, and FalseReg to DstReg when Cond is false.\n  ///\n  /// This function can only be called after canInsertSelect() returned true.\n  /// The condition in Cond comes from analyzeBranch, and it can be assumed\n  /// that the same flags or registers required by Cond are available at the\n  /// insertion point.\n  ///\n  /// @param MBB      Block where select instruction should be inserted.\n  /// @param I        Insertion point.\n  /// @param DL       Source location for debugging.\n  /// @param DstReg   Virtual register to be defined by select instruction.\n  /// @param Cond     Condition as computed by analyzeBranch.\n  /// @param TrueReg  Virtual register to copy when Cond is true.\n  /// @param FalseReg Virtual register to copy when Cons is false.\n  virtual void insertSelect(MachineBasicBlock &MBB,\n                            MachineBasicBlock::iterator I, const DebugLoc &DL,\n                            Register DstReg, ArrayRef<MachineOperand> Cond,\n                            Register TrueReg, Register FalseReg) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::insertSelect!\");\n  }\n\n  /// Analyze the given select instruction, returning true if\n  /// it cannot be understood. It is assumed that MI->isSelect() is true.\n  ///\n  /// When successful, return the controlling condition and the operands that\n  /// determine the true and false result values.\n  ///\n  ///   Result = SELECT Cond, TrueOp, FalseOp\n  ///\n  /// Some targets can optimize select instructions, for example by predicating\n  /// the instruction defining one of the operands. Such targets should set\n  /// Optimizable.\n  ///\n  /// @param         MI Select instruction to analyze.\n  /// @param Cond    Condition controlling the select.\n  /// @param TrueOp  Operand number of the value selected when Cond is true.\n  /// @param FalseOp Operand number of the value selected when Cond is false.\n  /// @param Optimizable Returned as true if MI is optimizable.\n  /// @returns False on success.\n  virtual bool analyzeSelect(const MachineInstr &MI,\n                             SmallVectorImpl<MachineOperand> &Cond,\n                             unsigned &TrueOp, unsigned &FalseOp,\n                             bool &Optimizable) const {\n    assert(MI.getDesc().isSelect() && \"MI must be a select instruction\");\n    return true;\n  }\n\n  /// Given a select instruction that was understood by\n  /// analyzeSelect and returned Optimizable = true, attempt to optimize MI by\n  /// merging it with one of its operands. Returns NULL on failure.\n  ///\n  /// When successful, returns the new select instruction. The client is\n  /// responsible for deleting MI.\n  ///\n  /// If both sides of the select can be optimized, PreferFalse is used to pick\n  /// a side.\n  ///\n  /// @param MI          Optimizable select instruction.\n  /// @param NewMIs     Set that record all MIs in the basic block up to \\p\n  /// MI. Has to be updated with any newly created MI or deleted ones.\n  /// @param PreferFalse Try to optimize FalseOp instead of TrueOp.\n  /// @returns Optimized instruction or NULL.\n  virtual MachineInstr *optimizeSelect(MachineInstr &MI,\n                                       SmallPtrSetImpl<MachineInstr *> &NewMIs,\n                                       bool PreferFalse = false) const {\n    // This function must be implemented if Optimizable is ever set.\n    llvm_unreachable(\"Target must implement TargetInstrInfo::optimizeSelect!\");\n  }\n\n  /// Emit instructions to copy a pair of physical registers.\n  ///\n  /// This function should support copies within any legal register class as\n  /// well as any cross-class copies created during instruction selection.\n  ///\n  /// The source and destination registers may overlap, which may require a\n  /// careful implementation when multiple copy instructions are required for\n  /// large registers. See for example the ARM target.\n  virtual void copyPhysReg(MachineBasicBlock &MBB,\n                           MachineBasicBlock::iterator MI, const DebugLoc &DL,\n                           MCRegister DestReg, MCRegister SrcReg,\n                           bool KillSrc) const {\n    llvm_unreachable(\"Target didn't implement TargetInstrInfo::copyPhysReg!\");\n  }\n\n  /// Allow targets to tell MachineVerifier whether a specific register\n  /// MachineOperand can be used as part of PC-relative addressing.\n  /// PC-relative addressing modes in many CISC architectures contain\n  /// (non-PC) registers as offsets or scaling values, which inherently\n  /// tags the corresponding MachineOperand with OPERAND_PCREL.\n  ///\n  /// @param MO The MachineOperand in question. MO.isReg() should always\n  /// be true.\n  /// @return Whether this operand is allowed to be used PC-relatively.\n  virtual bool isPCRelRegisterOperandLegal(const MachineOperand &MO) const {\n    return false;\n  }\n\nprotected:\n  /// Target-dependent implementation for IsCopyInstr.\n  /// If the specific machine instruction is a instruction that moves/copies\n  /// value from one register to another register return destination and source\n  /// registers as machine operands.\n  virtual Optional<DestSourcePair>\n  isCopyInstrImpl(const MachineInstr &MI) const {\n    return None;\n  }\n\n  /// Return true if the given terminator MI is not expected to spill. This\n  /// sets the live interval as not spillable and adjusts phi node lowering to\n  /// not introduce copies after the terminator. Use with care, these are\n  /// currently used for hardware loop intrinsics in very controlled situations,\n  /// created prior to registry allocation in loops that only have single phi\n  /// users for the terminators value. They may run out of registers if not used\n  /// carefully.\n  virtual bool isUnspillableTerminatorImpl(const MachineInstr *MI) const {\n    return false;\n  }\n\npublic:\n  /// If the specific machine instruction is a instruction that moves/copies\n  /// value from one register to another register return destination and source\n  /// registers as machine operands.\n  /// For COPY-instruction the method naturally returns destination and source\n  /// registers as machine operands, for all other instructions the method calls\n  /// target-dependent implementation.\n  Optional<DestSourcePair> isCopyInstr(const MachineInstr &MI) const {\n    if (MI.isCopy()) {\n      return DestSourcePair{MI.getOperand(0), MI.getOperand(1)};\n    }\n    return isCopyInstrImpl(MI);\n  }\n\n  /// If the specific machine instruction is an instruction that adds an\n  /// immediate value and a physical register, and stores the result in\n  /// the given physical register \\c Reg, return a pair of the source\n  /// register and the offset which has been added.\n  virtual Optional<RegImmPair> isAddImmediate(const MachineInstr &MI,\n                                              Register Reg) const {\n    return None;\n  }\n\n  /// Returns true if MI is an instruction that defines Reg to have a constant\n  /// value and the value is recorded in ImmVal. The ImmVal is a result that\n  /// should be interpreted as modulo size of Reg.\n  virtual bool getConstValDefinedInReg(const MachineInstr &MI,\n                                       const Register Reg,\n                                       int64_t &ImmVal) const {\n    return false;\n  }\n\n  /// Store the specified register of the given register class to the specified\n  /// stack frame index. The store instruction is to be added to the given\n  /// machine basic block before the specified machine instruction. If isKill\n  /// is true, the register operand is the last use and must be marked kill.\n  virtual void storeRegToStackSlot(MachineBasicBlock &MBB,\n                                   MachineBasicBlock::iterator MI,\n                                   Register SrcReg, bool isKill, int FrameIndex,\n                                   const TargetRegisterClass *RC,\n                                   const TargetRegisterInfo *TRI) const {\n    llvm_unreachable(\"Target didn't implement \"\n                     \"TargetInstrInfo::storeRegToStackSlot!\");\n  }\n\n  /// Load the specified register of the given register class from the specified\n  /// stack frame index. The load instruction is to be added to the given\n  /// machine basic block before the specified machine instruction.\n  virtual void loadRegFromStackSlot(MachineBasicBlock &MBB,\n                                    MachineBasicBlock::iterator MI,\n                                    Register DestReg, int FrameIndex,\n                                    const TargetRegisterClass *RC,\n                                    const TargetRegisterInfo *TRI) const {\n    llvm_unreachable(\"Target didn't implement \"\n                     \"TargetInstrInfo::loadRegFromStackSlot!\");\n  }\n\n  /// This function is called for all pseudo instructions\n  /// that remain after register allocation. Many pseudo instructions are\n  /// created to help register allocation. This is the place to convert them\n  /// into real instructions. The target can edit MI in place, or it can insert\n  /// new instructions and erase MI. The function should return true if\n  /// anything was changed.\n  virtual bool expandPostRAPseudo(MachineInstr &MI) const { return false; }\n\n  /// Check whether the target can fold a load that feeds a subreg operand\n  /// (or a subreg operand that feeds a store).\n  /// For example, X86 may want to return true if it can fold\n  /// movl (%esp), %eax\n  /// subb, %al, ...\n  /// Into:\n  /// subb (%esp), ...\n  ///\n  /// Ideally, we'd like the target implementation of foldMemoryOperand() to\n  /// reject subregs - but since this behavior used to be enforced in the\n  /// target-independent code, moving this responsibility to the targets\n  /// has the potential of causing nasty silent breakage in out-of-tree targets.\n  virtual bool isSubregFoldable() const { return false; }\n\n  /// Attempt to fold a load or store of the specified stack\n  /// slot into the specified machine instruction for the specified operand(s).\n  /// If this is possible, a new instruction is returned with the specified\n  /// operand folded, otherwise NULL is returned.\n  /// The new instruction is inserted before MI, and the client is responsible\n  /// for removing the old instruction.\n  /// If VRM is passed, the assigned physregs can be inspected by target to\n  /// decide on using an opcode (note that those assignments can still change).\n  MachineInstr *foldMemoryOperand(MachineInstr &MI, ArrayRef<unsigned> Ops,\n                                  int FI,\n                                  LiveIntervals *LIS = nullptr,\n                                  VirtRegMap *VRM = nullptr) const;\n\n  /// Same as the previous version except it allows folding of any load and\n  /// store from / to any address, not just from a specific stack slot.\n  MachineInstr *foldMemoryOperand(MachineInstr &MI, ArrayRef<unsigned> Ops,\n                                  MachineInstr &LoadMI,\n                                  LiveIntervals *LIS = nullptr) const;\n\n  /// Return true when there is potentially a faster code sequence\n  /// for an instruction chain ending in \\p Root. All potential patterns are\n  /// returned in the \\p Pattern vector. Pattern should be sorted in priority\n  /// order since the pattern evaluator stops checking as soon as it finds a\n  /// faster sequence.\n  /// \\param Root - Instruction that could be combined with one of its operands\n  /// \\param Patterns - Vector of possible combination patterns\n  virtual bool\n  getMachineCombinerPatterns(MachineInstr &Root,\n                             SmallVectorImpl<MachineCombinerPattern> &Patterns,\n                             bool DoRegPressureReduce) const;\n\n  /// Return true if target supports reassociation of instructions in machine\n  /// combiner pass to reduce register pressure for a given BB.\n  virtual bool\n  shouldReduceRegisterPressure(MachineBasicBlock *MBB,\n                               RegisterClassInfo *RegClassInfo) const {\n    return false;\n  }\n\n  /// Fix up the placeholder we may add in genAlternativeCodeSequence().\n  virtual void\n  finalizeInsInstrs(MachineInstr &Root, MachineCombinerPattern &P,\n                    SmallVectorImpl<MachineInstr *> &InsInstrs) const {}\n\n  /// Return true when a code sequence can improve throughput. It\n  /// should be called only for instructions in loops.\n  /// \\param Pattern - combiner pattern\n  virtual bool isThroughputPattern(MachineCombinerPattern Pattern) const;\n\n  /// Return true if the input \\P Inst is part of a chain of dependent ops\n  /// that are suitable for reassociation, otherwise return false.\n  /// If the instruction's operands must be commuted to have a previous\n  /// instruction of the same type define the first source operand, \\P Commuted\n  /// will be set to true.\n  bool isReassociationCandidate(const MachineInstr &Inst, bool &Commuted) const;\n\n  /// Return true when \\P Inst is both associative and commutative.\n  virtual bool isAssociativeAndCommutative(const MachineInstr &Inst) const {\n    return false;\n  }\n\n  /// Return true when \\P Inst has reassociable operands in the same \\P MBB.\n  virtual bool hasReassociableOperands(const MachineInstr &Inst,\n                                       const MachineBasicBlock *MBB) const;\n\n  /// Return true when \\P Inst has reassociable sibling.\n  bool hasReassociableSibling(const MachineInstr &Inst, bool &Commuted) const;\n\n  /// When getMachineCombinerPatterns() finds patterns, this function generates\n  /// the instructions that could replace the original code sequence. The client\n  /// has to decide whether the actual replacement is beneficial or not.\n  /// \\param Root - Instruction that could be combined with one of its operands\n  /// \\param Pattern - Combination pattern for Root\n  /// \\param InsInstrs - Vector of new instructions that implement P\n  /// \\param DelInstrs - Old instructions, including Root, that could be\n  /// replaced by InsInstr\n  /// \\param InstIdxForVirtReg - map of virtual register to instruction in\n  /// InsInstr that defines it\n  virtual void genAlternativeCodeSequence(\n      MachineInstr &Root, MachineCombinerPattern Pattern,\n      SmallVectorImpl<MachineInstr *> &InsInstrs,\n      SmallVectorImpl<MachineInstr *> &DelInstrs,\n      DenseMap<unsigned, unsigned> &InstIdxForVirtReg) const;\n\n  /// Attempt to reassociate \\P Root and \\P Prev according to \\P Pattern to\n  /// reduce critical path length.\n  void reassociateOps(MachineInstr &Root, MachineInstr &Prev,\n                      MachineCombinerPattern Pattern,\n                      SmallVectorImpl<MachineInstr *> &InsInstrs,\n                      SmallVectorImpl<MachineInstr *> &DelInstrs,\n                      DenseMap<unsigned, unsigned> &InstrIdxForVirtReg) const;\n\n  /// The limit on resource length extension we accept in MachineCombiner Pass.\n  virtual int getExtendResourceLenLimit() const { return 0; }\n\n  /// This is an architecture-specific helper function of reassociateOps.\n  /// Set special operand attributes for new instructions after reassociation.\n  virtual void setSpecialOperandAttr(MachineInstr &OldMI1, MachineInstr &OldMI2,\n                                     MachineInstr &NewMI1,\n                                     MachineInstr &NewMI2) const {}\n\n  virtual void setSpecialOperandAttr(MachineInstr &MI, uint16_t Flags) const {}\n\n  /// Return true when a target supports MachineCombiner.\n  virtual bool useMachineCombiner() const { return false; }\n\n  /// Return true if the given SDNode can be copied during scheduling\n  /// even if it has glue.\n  virtual bool canCopyGluedNodeDuringSchedule(SDNode *N) const { return false; }\n\nprotected:\n  /// Target-dependent implementation for foldMemoryOperand.\n  /// Target-independent code in foldMemoryOperand will\n  /// take care of adding a MachineMemOperand to the newly created instruction.\n  /// The instruction and any auxiliary instructions necessary will be inserted\n  /// at InsertPt.\n  virtual MachineInstr *\n  foldMemoryOperandImpl(MachineFunction &MF, MachineInstr &MI,\n                        ArrayRef<unsigned> Ops,\n                        MachineBasicBlock::iterator InsertPt, int FrameIndex,\n                        LiveIntervals *LIS = nullptr,\n                        VirtRegMap *VRM = nullptr) const {\n    return nullptr;\n  }\n\n  /// Target-dependent implementation for foldMemoryOperand.\n  /// Target-independent code in foldMemoryOperand will\n  /// take care of adding a MachineMemOperand to the newly created instruction.\n  /// The instruction and any auxiliary instructions necessary will be inserted\n  /// at InsertPt.\n  virtual MachineInstr *foldMemoryOperandImpl(\n      MachineFunction &MF, MachineInstr &MI, ArrayRef<unsigned> Ops,\n      MachineBasicBlock::iterator InsertPt, MachineInstr &LoadMI,\n      LiveIntervals *LIS = nullptr) const {\n    return nullptr;\n  }\n\n  /// Target-dependent implementation of getRegSequenceInputs.\n  ///\n  /// \\returns true if it is possible to build the equivalent\n  /// REG_SEQUENCE inputs with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isRegSequenceLike().\n  ///\n  /// \\see TargetInstrInfo::getRegSequenceInputs.\n  virtual bool getRegSequenceLikeInputs(\n      const MachineInstr &MI, unsigned DefIdx,\n      SmallVectorImpl<RegSubRegPairAndIdx> &InputRegs) const {\n    return false;\n  }\n\n  /// Target-dependent implementation of getExtractSubregInputs.\n  ///\n  /// \\returns true if it is possible to build the equivalent\n  /// EXTRACT_SUBREG inputs with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isExtractSubregLike().\n  ///\n  /// \\see TargetInstrInfo::getExtractSubregInputs.\n  virtual bool getExtractSubregLikeInputs(const MachineInstr &MI,\n                                          unsigned DefIdx,\n                                          RegSubRegPairAndIdx &InputReg) const {\n    return false;\n  }\n\n  /// Target-dependent implementation of getInsertSubregInputs.\n  ///\n  /// \\returns true if it is possible to build the equivalent\n  /// INSERT_SUBREG inputs with the pair \\p MI, \\p DefIdx. False otherwise.\n  ///\n  /// \\pre MI.isInsertSubregLike().\n  ///\n  /// \\see TargetInstrInfo::getInsertSubregInputs.\n  virtual bool\n  getInsertSubregLikeInputs(const MachineInstr &MI, unsigned DefIdx,\n                            RegSubRegPair &BaseReg,\n                            RegSubRegPairAndIdx &InsertedReg) const {\n    return false;\n  }\n\npublic:\n  /// getAddressSpaceForPseudoSourceKind - Given the kind of memory\n  /// (e.g. stack) the target returns the corresponding address space.\n  virtual unsigned\n  getAddressSpaceForPseudoSourceKind(unsigned Kind) const {\n    return 0;\n  }\n\n  /// unfoldMemoryOperand - Separate a single instruction which folded a load or\n  /// a store or a load and a store into two or more instruction. If this is\n  /// possible, returns true as well as the new instructions by reference.\n  virtual bool\n  unfoldMemoryOperand(MachineFunction &MF, MachineInstr &MI, unsigned Reg,\n                      bool UnfoldLoad, bool UnfoldStore,\n                      SmallVectorImpl<MachineInstr *> &NewMIs) const {\n    return false;\n  }\n\n  virtual bool unfoldMemoryOperand(SelectionDAG &DAG, SDNode *N,\n                                   SmallVectorImpl<SDNode *> &NewNodes) const {\n    return false;\n  }\n\n  /// Returns the opcode of the would be new\n  /// instruction after load / store are unfolded from an instruction of the\n  /// specified opcode. It returns zero if the specified unfolding is not\n  /// possible. If LoadRegIndex is non-null, it is filled in with the operand\n  /// index of the operand which will hold the register holding the loaded\n  /// value.\n  virtual unsigned\n  getOpcodeAfterMemoryUnfold(unsigned Opc, bool UnfoldLoad, bool UnfoldStore,\n                             unsigned *LoadRegIndex = nullptr) const {\n    return 0;\n  }\n\n  /// This is used by the pre-regalloc scheduler to determine if two loads are\n  /// loading from the same base address. It should only return true if the base\n  /// pointers are the same and the only differences between the two addresses\n  /// are the offset. It also returns the offsets by reference.\n  virtual bool areLoadsFromSameBasePtr(SDNode *Load1, SDNode *Load2,\n                                       int64_t &Offset1,\n                                       int64_t &Offset2) const {\n    return false;\n  }\n\n  /// This is a used by the pre-regalloc scheduler to determine (in conjunction\n  /// with areLoadsFromSameBasePtr) if two loads should be scheduled together.\n  /// On some targets if two loads are loading from\n  /// addresses in the same cache line, it's better if they are scheduled\n  /// together. This function takes two integers that represent the load offsets\n  /// from the common base address. It returns true if it decides it's desirable\n  /// to schedule the two loads together. \"NumLoads\" is the number of loads that\n  /// have already been scheduled after Load1.\n  virtual bool shouldScheduleLoadsNear(SDNode *Load1, SDNode *Load2,\n                                       int64_t Offset1, int64_t Offset2,\n                                       unsigned NumLoads) const {\n    return false;\n  }\n\n  /// Get the base operand and byte offset of an instruction that reads/writes\n  /// memory. This is a convenience function for callers that are only prepared\n  /// to handle a single base operand.\n  bool getMemOperandWithOffset(const MachineInstr &MI,\n                               const MachineOperand *&BaseOp, int64_t &Offset,\n                               bool &OffsetIsScalable,\n                               const TargetRegisterInfo *TRI) const;\n\n  /// Get zero or more base operands and the byte offset of an instruction that\n  /// reads/writes memory. Note that there may be zero base operands if the\n  /// instruction accesses a constant address.\n  /// It returns false if MI does not read/write memory.\n  /// It returns false if base operands and offset could not be determined.\n  /// It is not guaranteed to always recognize base operands and offsets in all\n  /// cases.\n  virtual bool getMemOperandsWithOffsetWidth(\n      const MachineInstr &MI, SmallVectorImpl<const MachineOperand *> &BaseOps,\n      int64_t &Offset, bool &OffsetIsScalable, unsigned &Width,\n      const TargetRegisterInfo *TRI) const {\n    return false;\n  }\n\n  /// Return true if the instruction contains a base register and offset. If\n  /// true, the function also sets the operand position in the instruction\n  /// for the base register and offset.\n  virtual bool getBaseAndOffsetPosition(const MachineInstr &MI,\n                                        unsigned &BasePos,\n                                        unsigned &OffsetPos) const {\n    return false;\n  }\n\n  /// Target dependent implementation to get the values constituting the address\n  /// MachineInstr that is accessing memory. These values are returned as a\n  /// struct ExtAddrMode which contains all relevant information to make up the\n  /// address.\n  virtual Optional<ExtAddrMode>\n  getAddrModeFromMemoryOp(const MachineInstr &MemI,\n                          const TargetRegisterInfo *TRI) const {\n    return None;\n  }\n\n  /// Returns true if MI's Def is NullValueReg, and the MI\n  /// does not change the Zero value. i.e. cases such as rax = shr rax, X where\n  /// NullValueReg = rax. Note that if the NullValueReg is non-zero, this\n  /// function can return true even if becomes zero. Specifically cases such as\n  /// NullValueReg = shl NullValueReg, 63.\n  virtual bool preservesZeroValueInReg(const MachineInstr *MI,\n                                       const Register NullValueReg,\n                                       const TargetRegisterInfo *TRI) const {\n    return false;\n  }\n\n  /// If the instruction is an increment of a constant value, return the amount.\n  virtual bool getIncrementValue(const MachineInstr &MI, int &Value) const {\n    return false;\n  }\n\n  /// Returns true if the two given memory operations should be scheduled\n  /// adjacent. Note that you have to add:\n  ///   DAG->addMutation(createLoadClusterDAGMutation(DAG->TII, DAG->TRI));\n  /// or\n  ///   DAG->addMutation(createStoreClusterDAGMutation(DAG->TII, DAG->TRI));\n  /// to TargetPassConfig::createMachineScheduler() to have an effect.\n  ///\n  /// \\p BaseOps1 and \\p BaseOps2 are memory operands of two memory operations.\n  /// \\p NumLoads is the number of loads that will be in the cluster if this\n  /// hook returns true.\n  /// \\p NumBytes is the number of bytes that will be loaded from all the\n  /// clustered loads if this hook returns true.\n  virtual bool shouldClusterMemOps(ArrayRef<const MachineOperand *> BaseOps1,\n                                   ArrayRef<const MachineOperand *> BaseOps2,\n                                   unsigned NumLoads, unsigned NumBytes) const {\n    llvm_unreachable(\"target did not implement shouldClusterMemOps()\");\n  }\n\n  /// Reverses the branch condition of the specified condition list,\n  /// returning false on success and true if it cannot be reversed.\n  virtual bool\n  reverseBranchCondition(SmallVectorImpl<MachineOperand> &Cond) const {\n    return true;\n  }\n\n  /// Insert a noop into the instruction stream at the specified point.\n  virtual void insertNoop(MachineBasicBlock &MBB,\n                          MachineBasicBlock::iterator MI) const;\n\n  /// Insert noops into the instruction stream at the specified point.\n  virtual void insertNoops(MachineBasicBlock &MBB,\n                           MachineBasicBlock::iterator MI,\n                           unsigned Quantity) const;\n\n  /// Return the noop instruction to use for a noop.\n  virtual void getNoop(MCInst &NopInst) const;\n\n  /// Return true for post-incremented instructions.\n  virtual bool isPostIncrement(const MachineInstr &MI) const { return false; }\n\n  /// Returns true if the instruction is already predicated.\n  virtual bool isPredicated(const MachineInstr &MI) const { return false; }\n\n  // Returns a MIRPrinter comment for this machine operand.\n  virtual std::string\n  createMIROperandComment(const MachineInstr &MI, const MachineOperand &Op,\n                          unsigned OpIdx, const TargetRegisterInfo *TRI) const;\n\n  /// Returns true if the instruction is a\n  /// terminator instruction that has not been predicated.\n  bool isUnpredicatedTerminator(const MachineInstr &MI) const;\n\n  /// Returns true if MI is an unconditional tail call.\n  virtual bool isUnconditionalTailCall(const MachineInstr &MI) const {\n    return false;\n  }\n\n  /// Returns true if the tail call can be made conditional on BranchCond.\n  virtual bool canMakeTailCallConditional(SmallVectorImpl<MachineOperand> &Cond,\n                                          const MachineInstr &TailCall) const {\n    return false;\n  }\n\n  /// Replace the conditional branch in MBB with a conditional tail call.\n  virtual void replaceBranchWithTailCall(MachineBasicBlock &MBB,\n                                         SmallVectorImpl<MachineOperand> &Cond,\n                                         const MachineInstr &TailCall) const {\n    llvm_unreachable(\"Target didn't implement replaceBranchWithTailCall!\");\n  }\n\n  /// Convert the instruction into a predicated instruction.\n  /// It returns true if the operation was successful.\n  virtual bool PredicateInstruction(MachineInstr &MI,\n                                    ArrayRef<MachineOperand> Pred) const;\n\n  /// Returns true if the first specified predicate\n  /// subsumes the second, e.g. GE subsumes GT.\n  virtual bool SubsumesPredicate(ArrayRef<MachineOperand> Pred1,\n                                 ArrayRef<MachineOperand> Pred2) const {\n    return false;\n  }\n\n  /// If the specified instruction defines any predicate\n  /// or condition code register(s) used for predication, returns true as well\n  /// as the definition predicate(s) by reference.\n  /// SkipDead should be set to false at any point that dead\n  /// predicate instructions should be considered as being defined.\n  /// A dead predicate instruction is one that is guaranteed to be removed\n  /// after a call to PredicateInstruction.\n  virtual bool ClobbersPredicate(MachineInstr &MI,\n                                 std::vector<MachineOperand> &Pred,\n                                 bool SkipDead) const {\n    return false;\n  }\n\n  /// Return true if the specified instruction can be predicated.\n  /// By default, this returns true for every instruction with a\n  /// PredicateOperand.\n  virtual bool isPredicable(const MachineInstr &MI) const {\n    return MI.getDesc().isPredicable();\n  }\n\n  /// Return true if it's safe to move a machine\n  /// instruction that defines the specified register class.\n  virtual bool isSafeToMoveRegClassDefs(const TargetRegisterClass *RC) const {\n    return true;\n  }\n\n  /// Test if the given instruction should be considered a scheduling boundary.\n  /// This primarily includes labels and terminators.\n  virtual bool isSchedulingBoundary(const MachineInstr &MI,\n                                    const MachineBasicBlock *MBB,\n                                    const MachineFunction &MF) const;\n\n  /// Measure the specified inline asm to determine an approximation of its\n  /// length.\n  virtual unsigned getInlineAsmLength(\n    const char *Str, const MCAsmInfo &MAI,\n    const TargetSubtargetInfo *STI = nullptr) const;\n\n  /// Allocate and return a hazard recognizer to use for this target when\n  /// scheduling the machine instructions before register allocation.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetHazardRecognizer(const TargetSubtargetInfo *STI,\n                               const ScheduleDAG *DAG) const;\n\n  /// Allocate and return a hazard recognizer to use for this target when\n  /// scheduling the machine instructions before register allocation.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetMIHazardRecognizer(const InstrItineraryData *,\n                                 const ScheduleDAGMI *DAG) const;\n\n  /// Allocate and return a hazard recognizer to use for this target when\n  /// scheduling the machine instructions after register allocation.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetPostRAHazardRecognizer(const InstrItineraryData *,\n                                     const ScheduleDAG *DAG) const;\n\n  /// Allocate and return a hazard recognizer to use for by non-scheduling\n  /// passes.\n  virtual ScheduleHazardRecognizer *\n  CreateTargetPostRAHazardRecognizer(const MachineFunction &MF) const {\n    return nullptr;\n  }\n\n  /// Provide a global flag for disabling the PreRA hazard recognizer that\n  /// targets may choose to honor.\n  bool usePreRAHazardRecognizer() const;\n\n  /// For a comparison instruction, return the source registers\n  /// in SrcReg and SrcReg2 if having two register operands, and the value it\n  /// compares against in CmpValue. Return true if the comparison instruction\n  /// can be analyzed.\n  virtual bool analyzeCompare(const MachineInstr &MI, Register &SrcReg,\n                              Register &SrcReg2, int &Mask, int &Value) const {\n    return false;\n  }\n\n  /// See if the comparison instruction can be converted\n  /// into something more efficient. E.g., on ARM most instructions can set the\n  /// flags register, obviating the need for a separate CMP.\n  virtual bool optimizeCompareInstr(MachineInstr &CmpInstr, Register SrcReg,\n                                    Register SrcReg2, int Mask, int Value,\n                                    const MachineRegisterInfo *MRI) const {\n    return false;\n  }\n  virtual bool optimizeCondBranch(MachineInstr &MI) const { return false; }\n\n  /// Try to remove the load by folding it to a register operand at the use.\n  /// We fold the load instructions if and only if the\n  /// def and use are in the same BB. We only look at one load and see\n  /// whether it can be folded into MI. FoldAsLoadDefReg is the virtual register\n  /// defined by the load we are trying to fold. DefMI returns the machine\n  /// instruction that defines FoldAsLoadDefReg, and the function returns\n  /// the machine instruction generated due to folding.\n  virtual MachineInstr *optimizeLoadInstr(MachineInstr &MI,\n                                          const MachineRegisterInfo *MRI,\n                                          Register &FoldAsLoadDefReg,\n                                          MachineInstr *&DefMI) const {\n    return nullptr;\n  }\n\n  /// 'Reg' is known to be defined by a move immediate instruction,\n  /// try to fold the immediate into the use instruction.\n  /// If MRI->hasOneNonDBGUse(Reg) is true, and this function returns true,\n  /// then the caller may assume that DefMI has been erased from its parent\n  /// block. The caller may assume that it will not be erased by this\n  /// function otherwise.\n  virtual bool FoldImmediate(MachineInstr &UseMI, MachineInstr &DefMI,\n                             Register Reg, MachineRegisterInfo *MRI) const {\n    return false;\n  }\n\n  /// Return the number of u-operations the given machine\n  /// instruction will be decoded to on the target cpu. The itinerary's\n  /// IssueWidth is the number of microops that can be dispatched each\n  /// cycle. An instruction with zero microops takes no dispatch resources.\n  virtual unsigned getNumMicroOps(const InstrItineraryData *ItinData,\n                                  const MachineInstr &MI) const;\n\n  /// Return true for pseudo instructions that don't consume any\n  /// machine resources in their current form. These are common cases that the\n  /// scheduler should consider free, rather than conservatively handling them\n  /// as instructions with no itinerary.\n  bool isZeroCost(unsigned Opcode) const {\n    return Opcode <= TargetOpcode::COPY;\n  }\n\n  virtual int getOperandLatency(const InstrItineraryData *ItinData,\n                                SDNode *DefNode, unsigned DefIdx,\n                                SDNode *UseNode, unsigned UseIdx) const;\n\n  /// Compute and return the use operand latency of a given pair of def and use.\n  /// In most cases, the static scheduling itinerary was enough to determine the\n  /// operand latency. But it may not be possible for instructions with variable\n  /// number of defs / uses.\n  ///\n  /// This is a raw interface to the itinerary that may be directly overridden\n  /// by a target. Use computeOperandLatency to get the best estimate of\n  /// latency.\n  virtual int getOperandLatency(const InstrItineraryData *ItinData,\n                                const MachineInstr &DefMI, unsigned DefIdx,\n                                const MachineInstr &UseMI,\n                                unsigned UseIdx) const;\n\n  /// Compute the instruction latency of a given instruction.\n  /// If the instruction has higher cost when predicated, it's returned via\n  /// PredCost.\n  virtual unsigned getInstrLatency(const InstrItineraryData *ItinData,\n                                   const MachineInstr &MI,\n                                   unsigned *PredCost = nullptr) const;\n\n  virtual unsigned getPredicationCost(const MachineInstr &MI) const;\n\n  virtual int getInstrLatency(const InstrItineraryData *ItinData,\n                              SDNode *Node) const;\n\n  /// Return the default expected latency for a def based on its opcode.\n  unsigned defaultDefLatency(const MCSchedModel &SchedModel,\n                             const MachineInstr &DefMI) const;\n\n  int computeDefOperandLatency(const InstrItineraryData *ItinData,\n                               const MachineInstr &DefMI) const;\n\n  /// Return true if this opcode has high latency to its result.\n  virtual bool isHighLatencyDef(int opc) const { return false; }\n\n  /// Compute operand latency between a def of 'Reg'\n  /// and a use in the current loop. Return true if the target considered\n  /// it 'high'. This is used by optimization passes such as machine LICM to\n  /// determine whether it makes sense to hoist an instruction out even in a\n  /// high register pressure situation.\n  virtual bool hasHighOperandLatency(const TargetSchedModel &SchedModel,\n                                     const MachineRegisterInfo *MRI,\n                                     const MachineInstr &DefMI, unsigned DefIdx,\n                                     const MachineInstr &UseMI,\n                                     unsigned UseIdx) const {\n    return false;\n  }\n\n  /// Compute operand latency of a def of 'Reg'. Return true\n  /// if the target considered it 'low'.\n  virtual bool hasLowDefLatency(const TargetSchedModel &SchedModel,\n                                const MachineInstr &DefMI,\n                                unsigned DefIdx) const;\n\n  /// Perform target-specific instruction verification.\n  virtual bool verifyInstruction(const MachineInstr &MI,\n                                 StringRef &ErrInfo) const {\n    return true;\n  }\n\n  /// Return the current execution domain and bit mask of\n  /// possible domains for instruction.\n  ///\n  /// Some micro-architectures have multiple execution domains, and multiple\n  /// opcodes that perform the same operation in different domains.  For\n  /// example, the x86 architecture provides the por, orps, and orpd\n  /// instructions that all do the same thing.  There is a latency penalty if a\n  /// register is written in one domain and read in another.\n  ///\n  /// This function returns a pair (domain, mask) containing the execution\n  /// domain of MI, and a bit mask of possible domains.  The setExecutionDomain\n  /// function can be used to change the opcode to one of the domains in the\n  /// bit mask.  Instructions whose execution domain can't be changed should\n  /// return a 0 mask.\n  ///\n  /// The execution domain numbers don't have any special meaning except domain\n  /// 0 is used for instructions that are not associated with any interesting\n  /// execution domain.\n  ///\n  virtual std::pair<uint16_t, uint16_t>\n  getExecutionDomain(const MachineInstr &MI) const {\n    return std::make_pair(0, 0);\n  }\n\n  /// Change the opcode of MI to execute in Domain.\n  ///\n  /// The bit (1 << Domain) must be set in the mask returned from\n  /// getExecutionDomain(MI).\n  virtual void setExecutionDomain(MachineInstr &MI, unsigned Domain) const {}\n\n  /// Returns the preferred minimum clearance\n  /// before an instruction with an unwanted partial register update.\n  ///\n  /// Some instructions only write part of a register, and implicitly need to\n  /// read the other parts of the register.  This may cause unwanted stalls\n  /// preventing otherwise unrelated instructions from executing in parallel in\n  /// an out-of-order CPU.\n  ///\n  /// For example, the x86 instruction cvtsi2ss writes its result to bits\n  /// [31:0] of the destination xmm register. Bits [127:32] are unaffected, so\n  /// the instruction needs to wait for the old value of the register to become\n  /// available:\n  ///\n  ///   addps %xmm1, %xmm0\n  ///   movaps %xmm0, (%rax)\n  ///   cvtsi2ss %rbx, %xmm0\n  ///\n  /// In the code above, the cvtsi2ss instruction needs to wait for the addps\n  /// instruction before it can issue, even though the high bits of %xmm0\n  /// probably aren't needed.\n  ///\n  /// This hook returns the preferred clearance before MI, measured in\n  /// instructions.  Other defs of MI's operand OpNum are avoided in the last N\n  /// instructions before MI.  It should only return a positive value for\n  /// unwanted dependencies.  If the old bits of the defined register have\n  /// useful values, or if MI is determined to otherwise read the dependency,\n  /// the hook should return 0.\n  ///\n  /// The unwanted dependency may be handled by:\n  ///\n  /// 1. Allocating the same register for an MI def and use.  That makes the\n  ///    unwanted dependency identical to a required dependency.\n  ///\n  /// 2. Allocating a register for the def that has no defs in the previous N\n  ///    instructions.\n  ///\n  /// 3. Calling breakPartialRegDependency() with the same arguments.  This\n  ///    allows the target to insert a dependency breaking instruction.\n  ///\n  virtual unsigned\n  getPartialRegUpdateClearance(const MachineInstr &MI, unsigned OpNum,\n                               const TargetRegisterInfo *TRI) const {\n    // The default implementation returns 0 for no partial register dependency.\n    return 0;\n  }\n\n  /// Return the minimum clearance before an instruction that reads an\n  /// unused register.\n  ///\n  /// For example, AVX instructions may copy part of a register operand into\n  /// the unused high bits of the destination register.\n  ///\n  /// vcvtsi2sdq %rax, undef %xmm0, %xmm14\n  ///\n  /// In the code above, vcvtsi2sdq copies %xmm0[127:64] into %xmm14 creating a\n  /// false dependence on any previous write to %xmm0.\n  ///\n  /// This hook works similarly to getPartialRegUpdateClearance, except that it\n  /// does not take an operand index. Instead sets \\p OpNum to the index of the\n  /// unused register.\n  virtual unsigned getUndefRegClearance(const MachineInstr &MI, unsigned OpNum,\n                                        const TargetRegisterInfo *TRI) const {\n    // The default implementation returns 0 for no undef register dependency.\n    return 0;\n  }\n\n  /// Insert a dependency-breaking instruction\n  /// before MI to eliminate an unwanted dependency on OpNum.\n  ///\n  /// If it wasn't possible to avoid a def in the last N instructions before MI\n  /// (see getPartialRegUpdateClearance), this hook will be called to break the\n  /// unwanted dependency.\n  ///\n  /// On x86, an xorps instruction can be used as a dependency breaker:\n  ///\n  ///   addps %xmm1, %xmm0\n  ///   movaps %xmm0, (%rax)\n  ///   xorps %xmm0, %xmm0\n  ///   cvtsi2ss %rbx, %xmm0\n  ///\n  /// An <imp-kill> operand should be added to MI if an instruction was\n  /// inserted.  This ties the instructions together in the post-ra scheduler.\n  ///\n  virtual void breakPartialRegDependency(MachineInstr &MI, unsigned OpNum,\n                                         const TargetRegisterInfo *TRI) const {}\n\n  /// Create machine specific model for scheduling.\n  virtual DFAPacketizer *\n  CreateTargetScheduleState(const TargetSubtargetInfo &) const {\n    return nullptr;\n  }\n\n  /// Sometimes, it is possible for the target\n  /// to tell, even without aliasing information, that two MIs access different\n  /// memory addresses. This function returns true if two MIs access different\n  /// memory addresses and false otherwise.\n  ///\n  /// Assumes any physical registers used to compute addresses have the same\n  /// value for both instructions. (This is the most useful assumption for\n  /// post-RA scheduling.)\n  ///\n  /// See also MachineInstr::mayAlias, which is implemented on top of this\n  /// function.\n  virtual bool\n  areMemAccessesTriviallyDisjoint(const MachineInstr &MIa,\n                                  const MachineInstr &MIb) const {\n    assert(MIa.mayLoadOrStore() &&\n           \"MIa must load from or modify a memory location\");\n    assert(MIb.mayLoadOrStore() &&\n           \"MIb must load from or modify a memory location\");\n    return false;\n  }\n\n  /// Return the value to use for the MachineCSE's LookAheadLimit,\n  /// which is a heuristic used for CSE'ing phys reg defs.\n  virtual unsigned getMachineCSELookAheadLimit() const {\n    // The default lookahead is small to prevent unprofitable quadratic\n    // behavior.\n    return 5;\n  }\n\n  /// Return the maximal number of alias checks on memory operands. For\n  /// instructions with more than one memory operands, the alias check on a\n  /// single MachineInstr pair has quadratic overhead and results in\n  /// unacceptable performance in the worst case. The limit here is to clamp\n  /// that maximal checks performed. Usually, that's the product of memory\n  /// operand numbers from that pair of MachineInstr to be checked. For\n  /// instance, with two MachineInstrs with 4 and 5 memory operands\n  /// correspondingly, a total of 20 checks are required. With this limit set to\n  /// 16, their alias check is skipped. We choose to limit the product instead\n  /// of the individual instruction as targets may have special MachineInstrs\n  /// with a considerably high number of memory operands, such as `ldm` in ARM.\n  /// Setting this limit per MachineInstr would result in either too high\n  /// overhead or too rigid restriction.\n  virtual unsigned getMemOperandAACheckLimit() const { return 16; }\n\n  /// Return an array that contains the ids of the target indices (used for the\n  /// TargetIndex machine operand) and their names.\n  ///\n  /// MIR Serialization is able to serialize only the target indices that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<int, const char *>>\n  getSerializableTargetIndices() const {\n    return None;\n  }\n\n  /// Decompose the machine operand's target flags into two values - the direct\n  /// target flag value and any of bit flags that are applied.\n  virtual std::pair<unsigned, unsigned>\n  decomposeMachineOperandsTargetFlags(unsigned /*TF*/) const {\n    return std::make_pair(0u, 0u);\n  }\n\n  /// Return an array that contains the direct target flag values and their\n  /// names.\n  ///\n  /// MIR Serialization is able to serialize only the target flags that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<unsigned, const char *>>\n  getSerializableDirectMachineOperandTargetFlags() const {\n    return None;\n  }\n\n  /// Return an array that contains the bitmask target flag values and their\n  /// names.\n  ///\n  /// MIR Serialization is able to serialize only the target flags that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<unsigned, const char *>>\n  getSerializableBitmaskMachineOperandTargetFlags() const {\n    return None;\n  }\n\n  /// Return an array that contains the MMO target flag values and their\n  /// names.\n  ///\n  /// MIR Serialization is able to serialize only the MMO target flags that are\n  /// defined by this method.\n  virtual ArrayRef<std::pair<MachineMemOperand::Flags, const char *>>\n  getSerializableMachineMemOperandTargetFlags() const {\n    return None;\n  }\n\n  /// Determines whether \\p Inst is a tail call instruction. Override this\n  /// method on targets that do not properly set MCID::Return and MCID::Call on\n  /// tail call instructions.\"\n  virtual bool isTailCall(const MachineInstr &Inst) const {\n    return Inst.isReturn() && Inst.isCall();\n  }\n\n  /// True if the instruction is bound to the top of its basic block and no\n  /// other instructions shall be inserted before it. This can be implemented\n  /// to prevent register allocator to insert spills before such instructions.\n  virtual bool isBasicBlockPrologue(const MachineInstr &MI) const {\n    return false;\n  }\n\n  /// During PHI eleimination lets target to make necessary checks and\n  /// insert the copy to the PHI destination register in a target specific\n  /// manner.\n  virtual MachineInstr *createPHIDestinationCopy(\n      MachineBasicBlock &MBB, MachineBasicBlock::iterator InsPt,\n      const DebugLoc &DL, Register Src, Register Dst) const {\n    return BuildMI(MBB, InsPt, DL, get(TargetOpcode::COPY), Dst)\n        .addReg(Src);\n  }\n\n  /// During PHI eleimination lets target to make necessary checks and\n  /// insert the copy to the PHI destination register in a target specific\n  /// manner.\n  virtual MachineInstr *createPHISourceCopy(MachineBasicBlock &MBB,\n                                            MachineBasicBlock::iterator InsPt,\n                                            const DebugLoc &DL, Register Src,\n                                            unsigned SrcSubReg,\n                                            Register Dst) const {\n    return BuildMI(MBB, InsPt, DL, get(TargetOpcode::COPY), Dst)\n        .addReg(Src, 0, SrcSubReg);\n  }\n\n  /// Returns a \\p outliner::OutlinedFunction struct containing target-specific\n  /// information for a set of outlining candidates.\n  virtual outliner::OutlinedFunction getOutliningCandidateInfo(\n      std::vector<outliner::Candidate> &RepeatedSequenceLocs) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::getOutliningCandidateInfo!\");\n  }\n\n  /// Returns how or if \\p MI should be outlined.\n  virtual outliner::InstrType\n  getOutliningType(MachineBasicBlock::iterator &MIT, unsigned Flags) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::getOutliningType!\");\n  }\n\n  /// Optional target hook that returns true if \\p MBB is safe to outline from,\n  /// and returns any target-specific information in \\p Flags.\n  virtual bool isMBBSafeToOutlineFrom(MachineBasicBlock &MBB,\n                                      unsigned &Flags) const {\n    return true;\n  }\n\n  /// Insert a custom frame for outlined functions.\n  virtual void buildOutlinedFrame(MachineBasicBlock &MBB, MachineFunction &MF,\n                                  const outliner::OutlinedFunction &OF) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::buildOutlinedFrame!\");\n  }\n\n  /// Insert a call to an outlined function into the program.\n  /// Returns an iterator to the spot where we inserted the call. This must be\n  /// implemented by the target.\n  virtual MachineBasicBlock::iterator\n  insertOutlinedCall(Module &M, MachineBasicBlock &MBB,\n                     MachineBasicBlock::iterator &It, MachineFunction &MF,\n                     const outliner::Candidate &C) const {\n    llvm_unreachable(\n        \"Target didn't implement TargetInstrInfo::insertOutlinedCall!\");\n  }\n\n  /// Return true if the function can safely be outlined from.\n  /// A function \\p MF is considered safe for outlining if an outlined function\n  /// produced from instructions in F will produce a program which produces the\n  /// same output for any set of given inputs.\n  virtual bool isFunctionSafeToOutlineFrom(MachineFunction &MF,\n                                           bool OutlineFromLinkOnceODRs) const {\n    llvm_unreachable(\"Target didn't implement \"\n                     \"TargetInstrInfo::isFunctionSafeToOutlineFrom!\");\n  }\n\n  /// Return true if the function should be outlined from by default.\n  virtual bool shouldOutlineFromFunctionByDefault(MachineFunction &MF) const {\n    return false;\n  }\n\n  /// Produce the expression describing the \\p MI loading a value into\n  /// the physical register \\p Reg. This hook should only be used with\n  /// \\p MIs belonging to VReg-less functions.\n  virtual Optional<ParamLoadedValue> describeLoadedValue(const MachineInstr &MI,\n                                                         Register Reg) const;\n\n  /// Given the generic extension instruction \\p ExtMI, returns true if this\n  /// extension is a likely candidate for being folded into an another\n  /// instruction.\n  virtual bool isExtendLikelyToBeFolded(MachineInstr &ExtMI,\n                                        MachineRegisterInfo &MRI) const {\n    return false;\n  }\n\n  /// Return MIR formatter to format/parse MIR operands.  Target can override\n  /// this virtual function and return target specific MIR formatter.\n  virtual const MIRFormatter *getMIRFormatter() const {\n    if (!Formatter.get())\n      Formatter = std::make_unique<MIRFormatter>();\n    return Formatter.get();\n  }\n\n  /// Returns the target-specific default value for tail duplication.\n  /// This value will be used if the tail-dup-placement-threshold argument is\n  /// not provided.\n  virtual unsigned getTailDuplicateSize(CodeGenOpt::Level OptLevel) const {\n    return OptLevel >= CodeGenOpt::Aggressive ? 4 : 2;\n  }\n\nprivate:\n  mutable std::unique_ptr<MIRFormatter> Formatter;\n  unsigned CallFrameSetupOpcode, CallFrameDestroyOpcode;\n  unsigned CatchRetOpcode;\n  unsigned ReturnOpcode;\n};\n\n/// Provide DenseMapInfo for TargetInstrInfo::RegSubRegPair.\ntemplate <> struct DenseMapInfo<TargetInstrInfo::RegSubRegPair> {\n  using RegInfo = DenseMapInfo<unsigned>;\n\n  static inline TargetInstrInfo::RegSubRegPair getEmptyKey() {\n    return TargetInstrInfo::RegSubRegPair(RegInfo::getEmptyKey(),\n                                          RegInfo::getEmptyKey());\n  }\n\n  static inline TargetInstrInfo::RegSubRegPair getTombstoneKey() {\n    return TargetInstrInfo::RegSubRegPair(RegInfo::getTombstoneKey(),\n                                          RegInfo::getTombstoneKey());\n  }\n\n  /// Reuse getHashValue implementation from\n  /// std::pair<unsigned, unsigned>.\n  static unsigned getHashValue(const TargetInstrInfo::RegSubRegPair &Val) {\n    std::pair<unsigned, unsigned> PairVal = std::make_pair(Val.Reg, Val.SubReg);\n    return DenseMapInfo<std::pair<unsigned, unsigned>>::getHashValue(PairVal);\n  }\n\n  static bool isEqual(const TargetInstrInfo::RegSubRegPair &LHS,\n                      const TargetInstrInfo::RegSubRegPair &RHS) {\n    return RegInfo::isEqual(LHS.Reg, RHS.Reg) &&\n           RegInfo::isEqual(LHS.SubReg, RHS.SubReg);\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_TARGETINSTRINFO_H\n"}, "55": {"id": 55, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "content": "//===- llvm/CodeGen/TargetLowering.h - Target Lowering Info -----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n///\n/// \\file\n/// This file describes how to lower LLVM code to machine code.  This has two\n/// main components:\n///\n///  1. Which ValueTypes are natively supported by the target.\n///  2. Which operations are supported for supported ValueTypes.\n///  3. Cost thresholds for alternative implementations of certain operations.\n///\n/// In addition it has a few other components, like information about FP\n/// immediates.\n///\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CODEGEN_TARGETLOWERING_H\n#define LLVM_CODEGEN_TARGETLOWERING_H\n\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/CodeGen/DAGCombine.h\"\n#include \"llvm/CodeGen/ISDOpcodes.h\"\n#include \"llvm/CodeGen/RuntimeLibcalls.h\"\n#include \"llvm/CodeGen/SelectionDAG.h\"\n#include \"llvm/CodeGen/SelectionDAGNodes.h\"\n#include \"llvm/CodeGen/TargetCallingConv.h\"\n#include \"llvm/CodeGen/ValueTypes.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include \"llvm/IR/InlineAsm.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/MachineValueType.h\"\n#include <algorithm>\n#include <cassert>\n#include <climits>\n#include <cstdint>\n#include <iterator>\n#include <map>\n#include <string>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass BranchProbability;\nclass CCState;\nclass CCValAssign;\nclass Constant;\nclass FastISel;\nclass FunctionLoweringInfo;\nclass GlobalValue;\nclass GISelKnownBits;\nclass IntrinsicInst;\nstruct KnownBits;\nclass LegacyDivergenceAnalysis;\nclass LLVMContext;\nclass MachineBasicBlock;\nclass MachineFunction;\nclass MachineInstr;\nclass MachineJumpTableInfo;\nclass MachineLoop;\nclass MachineRegisterInfo;\nclass MCContext;\nclass MCExpr;\nclass Module;\nclass ProfileSummaryInfo;\nclass TargetLibraryInfo;\nclass TargetMachine;\nclass TargetRegisterClass;\nclass TargetRegisterInfo;\nclass TargetTransformInfo;\nclass Value;\n\nnamespace Sched {\n\n  enum Preference {\n    None,             // No preference\n    Source,           // Follow source order.\n    RegPressure,      // Scheduling for lowest register pressure.\n    Hybrid,           // Scheduling for both latency and register pressure.\n    ILP,              // Scheduling for ILP in low register pressure mode.\n    VLIW              // Scheduling for VLIW targets.\n  };\n\n} // end namespace Sched\n\n// MemOp models a memory operation, either memset or memcpy/memmove.\nstruct MemOp {\nprivate:\n  // Shared\n  uint64_t Size;\n  bool DstAlignCanChange; // true if destination alignment can satisfy any\n                          // constraint.\n  Align DstAlign;         // Specified alignment of the memory operation.\n\n  bool AllowOverlap;\n  // memset only\n  bool IsMemset;   // If setthis memory operation is a memset.\n  bool ZeroMemset; // If set clears out memory with zeros.\n  // memcpy only\n  bool MemcpyStrSrc; // Indicates whether the memcpy source is an in-register\n                     // constant so it does not need to be loaded.\n  Align SrcAlign;    // Inferred alignment of the source or default value if the\n                     // memory operation does not need to load the value.\npublic:\n  static MemOp Copy(uint64_t Size, bool DstAlignCanChange, Align DstAlign,\n                    Align SrcAlign, bool IsVolatile,\n                    bool MemcpyStrSrc = false) {\n    MemOp Op;\n    Op.Size = Size;\n    Op.DstAlignCanChange = DstAlignCanChange;\n    Op.DstAlign = DstAlign;\n    Op.AllowOverlap = !IsVolatile;\n    Op.IsMemset = false;\n    Op.ZeroMemset = false;\n    Op.MemcpyStrSrc = MemcpyStrSrc;\n    Op.SrcAlign = SrcAlign;\n    return Op;\n  }\n\n  static MemOp Set(uint64_t Size, bool DstAlignCanChange, Align DstAlign,\n                   bool IsZeroMemset, bool IsVolatile) {\n    MemOp Op;\n    Op.Size = Size;\n    Op.DstAlignCanChange = DstAlignCanChange;\n    Op.DstAlign = DstAlign;\n    Op.AllowOverlap = !IsVolatile;\n    Op.IsMemset = true;\n    Op.ZeroMemset = IsZeroMemset;\n    Op.MemcpyStrSrc = false;\n    return Op;\n  }\n\n  uint64_t size() const { return Size; }\n  Align getDstAlign() const {\n    assert(!DstAlignCanChange);\n    return DstAlign;\n  }\n  bool isFixedDstAlign() const { return !DstAlignCanChange; }\n  bool allowOverlap() const { return AllowOverlap; }\n  bool isMemset() const { return IsMemset; }\n  bool isMemcpy() const { return !IsMemset; }\n  bool isMemcpyWithFixedDstAlign() const {\n    return isMemcpy() && !DstAlignCanChange;\n  }\n  bool isZeroMemset() const { return isMemset() && ZeroMemset; }\n  bool isMemcpyStrSrc() const {\n    assert(isMemcpy() && \"Must be a memcpy\");\n    return MemcpyStrSrc;\n  }\n  Align getSrcAlign() const {\n    assert(isMemcpy() && \"Must be a memcpy\");\n    return SrcAlign;\n  }\n  bool isSrcAligned(Align AlignCheck) const {\n    return isMemset() || llvm::isAligned(AlignCheck, SrcAlign.value());\n  }\n  bool isDstAligned(Align AlignCheck) const {\n    return DstAlignCanChange || llvm::isAligned(AlignCheck, DstAlign.value());\n  }\n  bool isAligned(Align AlignCheck) const {\n    return isSrcAligned(AlignCheck) && isDstAligned(AlignCheck);\n  }\n};\n\n/// This base class for TargetLowering contains the SelectionDAG-independent\n/// parts that can be used from the rest of CodeGen.\nclass TargetLoweringBase {\npublic:\n  /// This enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum LegalizeAction : uint8_t {\n    Legal,      // The target natively supports this operation.\n    Promote,    // This operation should be executed in a larger type.\n    Expand,     // Try to expand this to other ops, otherwise use a libcall.\n    LibCall,    // Don't try to expand this to other ops, always use a libcall.\n    Custom      // Use the LowerOperation hook to implement custom lowering.\n  };\n\n  /// This enum indicates whether a types are legal for a target, and if not,\n  /// what action should be used to make them valid.\n  enum LegalizeTypeAction : uint8_t {\n    TypeLegal,           // The target natively supports this type.\n    TypePromoteInteger,  // Replace this integer with a larger one.\n    TypeExpandInteger,   // Split this integer into two of half the size.\n    TypeSoftenFloat,     // Convert this float to a same size integer type.\n    TypeExpandFloat,     // Split this float into two of half the size.\n    TypeScalarizeVector, // Replace this one-element vector with its element.\n    TypeSplitVector,     // Split this vector into two of half the size.\n    TypeWidenVector,     // This vector should be widened into a larger vector.\n    TypePromoteFloat,    // Replace this float with a larger one.\n    TypeSoftPromoteHalf, // Soften half to i16 and use float to do arithmetic.\n    TypeScalarizeScalableVector, // This action is explicitly left unimplemented.\n                                 // While it is theoretically possible to\n                                 // legalize operations on scalable types with a\n                                 // loop that handles the vscale * #lanes of the\n                                 // vector, this is non-trivial at SelectionDAG\n                                 // level and these types are better to be\n                                 // widened or promoted.\n  };\n\n  /// LegalizeKind holds the legalization kind that needs to happen to EVT\n  /// in order to type-legalize it.\n  using LegalizeKind = std::pair<LegalizeTypeAction, EVT>;\n\n  /// Enum that describes how the target represents true/false values.\n  enum BooleanContent {\n    UndefinedBooleanContent,    // Only bit 0 counts, the rest can hold garbage.\n    ZeroOrOneBooleanContent,        // All bits zero except for bit 0.\n    ZeroOrNegativeOneBooleanContent // All bits equal to bit 0.\n  };\n\n  /// Enum that describes what type of support for selects the target has.\n  enum SelectSupportKind {\n    ScalarValSelect,      // The target supports scalar selects (ex: cmov).\n    ScalarCondVectorVal,  // The target supports selects with a scalar condition\n                          // and vector values (ex: cmov).\n    VectorMaskSelect      // The target supports vector selects with a vector\n                          // mask (ex: x86 blends).\n  };\n\n  /// Enum that specifies what an atomic load/AtomicRMWInst is expanded\n  /// to, if at all. Exists because different targets have different levels of\n  /// support for these atomic instructions, and also have different options\n  /// w.r.t. what they should expand to.\n  enum class AtomicExpansionKind {\n    None,    // Don't expand the instruction.\n    LLSC,    // Expand the instruction into loadlinked/storeconditional; used\n             // by ARM/AArch64.\n    LLOnly,  // Expand the (load) instruction into just a load-linked, which has\n             // greater atomic guarantees than a normal load.\n    CmpXChg, // Expand the instruction into cmpxchg; used by at least X86.\n    MaskedIntrinsic, // Use a target-specific intrinsic for the LL/SC loop.\n  };\n\n  /// Enum that specifies when a multiplication should be expanded.\n  enum class MulExpansionKind {\n    Always,            // Always expand the instruction.\n    OnlyLegalOrCustom, // Only expand when the resulting instructions are legal\n                       // or custom.\n  };\n\n  /// Enum that specifies when a float negation is beneficial.\n  enum class NegatibleCost {\n    Cheaper = 0,    // Negated expression is cheaper.\n    Neutral = 1,    // Negated expression has the same cost.\n    Expensive = 2   // Negated expression is more expensive.\n  };\n\n  class ArgListEntry {\n  public:\n    Value *Val = nullptr;\n    SDValue Node = SDValue();\n    Type *Ty = nullptr;\n    bool IsSExt : 1;\n    bool IsZExt : 1;\n    bool IsInReg : 1;\n    bool IsSRet : 1;\n    bool IsNest : 1;\n    bool IsByVal : 1;\n    bool IsByRef : 1;\n    bool IsInAlloca : 1;\n    bool IsPreallocated : 1;\n    bool IsReturned : 1;\n    bool IsSwiftSelf : 1;\n    bool IsSwiftError : 1;\n    bool IsCFGuardTarget : 1;\n    MaybeAlign Alignment = None;\n    Type *ByValType = nullptr;\n    Type *PreallocatedType = nullptr;\n\n    ArgListEntry()\n        : IsSExt(false), IsZExt(false), IsInReg(false), IsSRet(false),\n          IsNest(false), IsByVal(false), IsByRef(false), IsInAlloca(false),\n          IsPreallocated(false), IsReturned(false), IsSwiftSelf(false),\n          IsSwiftError(false), IsCFGuardTarget(false) {}\n\n    void setAttributes(const CallBase *Call, unsigned ArgIdx);\n  };\n  using ArgListTy = std::vector<ArgListEntry>;\n\n  virtual void markLibCallAttributes(MachineFunction *MF, unsigned CC,\n                                     ArgListTy &Args) const {};\n\n  static ISD::NodeType getExtendForContent(BooleanContent Content) {\n    switch (Content) {\n    case UndefinedBooleanContent:\n      // Extend by adding rubbish bits.\n      return ISD::ANY_EXTEND;\n    case ZeroOrOneBooleanContent:\n      // Extend by adding zero bits.\n      return ISD::ZERO_EXTEND;\n    case ZeroOrNegativeOneBooleanContent:\n      // Extend by copying the sign bit.\n      return ISD::SIGN_EXTEND;\n    }\n    llvm_unreachable(\"Invalid content kind\");\n  }\n\n  explicit TargetLoweringBase(const TargetMachine &TM);\n  TargetLoweringBase(const TargetLoweringBase &) = delete;\n  TargetLoweringBase &operator=(const TargetLoweringBase &) = delete;\n  virtual ~TargetLoweringBase() = default;\n\n  /// Return true if the target support strict float operation\n  bool isStrictFPEnabled() const {\n    return IsStrictFPEnabled;\n  }\n\nprotected:\n  /// Initialize all of the actions to default values.\n  void initActions();\n\npublic:\n  const TargetMachine &getTargetMachine() const { return TM; }\n\n  virtual bool useSoftFloat() const { return false; }\n\n  /// Return the pointer type for the given address space, defaults to\n  /// the pointer type from the data layout.\n  /// FIXME: The default needs to be removed once all the code is updated.\n  virtual MVT getPointerTy(const DataLayout &DL, uint32_t AS = 0) const {\n    return MVT::getIntegerVT(DL.getPointerSizeInBits(AS));\n  }\n\n  /// Return the in-memory pointer type for the given address space, defaults to\n  /// the pointer type from the data layout.  FIXME: The default needs to be\n  /// removed once all the code is updated.\n  MVT getPointerMemTy(const DataLayout &DL, uint32_t AS = 0) const {\n    return MVT::getIntegerVT(DL.getPointerSizeInBits(AS));\n  }\n\n  /// Return the type for frame index, which is determined by\n  /// the alloca address space specified through the data layout.\n  MVT getFrameIndexTy(const DataLayout &DL) const {\n    return getPointerTy(DL, DL.getAllocaAddrSpace());\n  }\n\n  /// Return the type for code pointers, which is determined by the program\n  /// address space specified through the data layout.\n  MVT getProgramPointerTy(const DataLayout &DL) const {\n    return getPointerTy(DL, DL.getProgramAddressSpace());\n  }\n\n  /// Return the type for operands of fence.\n  /// TODO: Let fence operands be of i32 type and remove this.\n  virtual MVT getFenceOperandTy(const DataLayout &DL) const {\n    return getPointerTy(DL);\n  }\n\n  /// EVT is not used in-tree, but is used by out-of-tree target.\n  /// A documentation for this function would be nice...\n  virtual MVT getScalarShiftAmountTy(const DataLayout &, EVT) const;\n\n  EVT getShiftAmountTy(EVT LHSTy, const DataLayout &DL,\n                       bool LegalTypes = true) const;\n\n  /// Return the preferred type to use for a shift opcode, given the shifted\n  /// amount type is \\p ShiftValueTy.\n  LLVM_READONLY\n  virtual LLT getPreferredShiftAmountTy(LLT ShiftValueTy) const {\n    return ShiftValueTy;\n  }\n\n  /// Returns the type to be used for the index operand of:\n  /// ISD::INSERT_VECTOR_ELT, ISD::EXTRACT_VECTOR_ELT,\n  /// ISD::INSERT_SUBVECTOR, and ISD::EXTRACT_SUBVECTOR\n  virtual MVT getVectorIdxTy(const DataLayout &DL) const {\n    return getPointerTy(DL);\n  }\n\n  /// This callback is used to inspect load/store instructions and add\n  /// target-specific MachineMemOperand flags to them.  The default\n  /// implementation does nothing.\n  virtual MachineMemOperand::Flags getTargetMMOFlags(const Instruction &I) const {\n    return MachineMemOperand::MONone;\n  }\n\n  MachineMemOperand::Flags getLoadMemOperandFlags(const LoadInst &LI,\n                                                  const DataLayout &DL) const;\n  MachineMemOperand::Flags getStoreMemOperandFlags(const StoreInst &SI,\n                                                   const DataLayout &DL) const;\n  MachineMemOperand::Flags getAtomicMemOperandFlags(const Instruction &AI,\n                                                    const DataLayout &DL) const;\n\n  virtual bool isSelectSupported(SelectSupportKind /*kind*/) const {\n    return true;\n  }\n\n  /// Return true if it is profitable to convert a select of FP constants into\n  /// a constant pool load whose address depends on the select condition. The\n  /// parameter may be used to differentiate a select with FP compare from\n  /// integer compare.\n  virtual bool reduceSelectOfFPConstantLoads(EVT CmpOpVT) const {\n    return true;\n  }\n\n  /// Return true if multiple condition registers are available.\n  bool hasMultipleConditionRegisters() const {\n    return HasMultipleConditionRegisters;\n  }\n\n  /// Return true if the target has BitExtract instructions.\n  bool hasExtractBitsInsn() const { return HasExtractBitsInsn; }\n\n  /// Return the preferred vector type legalization action.\n  virtual TargetLoweringBase::LegalizeTypeAction\n  getPreferredVectorAction(MVT VT) const {\n    // The default action for one element vectors is to scalarize\n    if (VT.getVectorElementCount().isScalar())\n      return TypeScalarizeVector;\n    // The default action for an odd-width vector is to widen.\n    if (!VT.isPow2VectorType())\n      return TypeWidenVector;\n    // The default action for other vectors is to promote\n    return TypePromoteInteger;\n  }\n\n  // Return true if the half type should be passed around as i16, but promoted\n  // to float around arithmetic. The default behavior is to pass around as\n  // float and convert around loads/stores/bitcasts and other places where\n  // the size matters.\n  virtual bool softPromoteHalfType() const { return false; }\n\n  // There are two general methods for expanding a BUILD_VECTOR node:\n  //  1. Use SCALAR_TO_VECTOR on the defined scalar values and then shuffle\n  //     them together.\n  //  2. Build the vector on the stack and then load it.\n  // If this function returns true, then method (1) will be used, subject to\n  // the constraint that all of the necessary shuffles are legal (as determined\n  // by isShuffleMaskLegal). If this function returns false, then method (2) is\n  // always used. The vector type, and the number of defined values, are\n  // provided.\n  virtual bool\n  shouldExpandBuildVectorWithShuffles(EVT /* VT */,\n                                      unsigned DefinedValues) const {\n    return DefinedValues < 3;\n  }\n\n  /// Return true if integer divide is usually cheaper than a sequence of\n  /// several shifts, adds, and multiplies for this target.\n  /// The definition of \"cheaper\" may depend on whether we're optimizing\n  /// for speed or for size.\n  virtual bool isIntDivCheap(EVT VT, AttributeList Attr) const { return false; }\n\n  /// Return true if the target can handle a standalone remainder operation.\n  virtual bool hasStandaloneRem(EVT VT) const {\n    return true;\n  }\n\n  /// Return true if SQRT(X) shouldn't be replaced with X*RSQRT(X).\n  virtual bool isFsqrtCheap(SDValue X, SelectionDAG &DAG) const {\n    // Default behavior is to replace SQRT(X) with X*RSQRT(X).\n    return false;\n  }\n\n  /// Reciprocal estimate status values used by the functions below.\n  enum ReciprocalEstimate : int {\n    Unspecified = -1,\n    Disabled = 0,\n    Enabled = 1\n  };\n\n  /// Return a ReciprocalEstimate enum value for a square root of the given type\n  /// based on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getRecipEstimateSqrtEnabled(EVT VT, MachineFunction &MF) const;\n\n  /// Return a ReciprocalEstimate enum value for a division of the given type\n  /// based on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getRecipEstimateDivEnabled(EVT VT, MachineFunction &MF) const;\n\n  /// Return the refinement step count for a square root of the given type based\n  /// on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getSqrtRefinementSteps(EVT VT, MachineFunction &MF) const;\n\n  /// Return the refinement step count for a division of the given type based\n  /// on the function's attributes. If the operation is not overridden by\n  /// the function's attributes, \"Unspecified\" is returned and target defaults\n  /// are expected to be used for instruction selection.\n  int getDivRefinementSteps(EVT VT, MachineFunction &MF) const;\n\n  /// Returns true if target has indicated at least one type should be bypassed.\n  bool isSlowDivBypassed() const { return !BypassSlowDivWidths.empty(); }\n\n  /// Returns map of slow types for division or remainder with corresponding\n  /// fast types\n  const DenseMap<unsigned int, unsigned int> &getBypassSlowDivWidths() const {\n    return BypassSlowDivWidths;\n  }\n\n  /// Return true if Flow Control is an expensive operation that should be\n  /// avoided.\n  bool isJumpExpensive() const { return JumpIsExpensive; }\n\n  /// Return true if selects are only cheaper than branches if the branch is\n  /// unlikely to be predicted right.\n  bool isPredictableSelectExpensive() const {\n    return PredictableSelectIsExpensive;\n  }\n\n  virtual bool fallBackToDAGISel(const Instruction &Inst) const {\n    return false;\n  }\n\n  /// If a branch or a select condition is skewed in one direction by more than\n  /// this factor, it is very likely to be predicted correctly.\n  virtual BranchProbability getPredictableBranchThreshold() const;\n\n  /// Return true if the following transform is beneficial:\n  /// fold (conv (load x)) -> (load (conv*)x)\n  /// On architectures that don't natively support some vector loads\n  /// efficiently, casting the load to a smaller vector of larger types and\n  /// loading is more efficient, however, this can be undone by optimizations in\n  /// dag combiner.\n  virtual bool isLoadBitCastBeneficial(EVT LoadVT, EVT BitcastVT,\n                                       const SelectionDAG &DAG,\n                                       const MachineMemOperand &MMO) const {\n    // Don't do if we could do an indexed load on the original type, but not on\n    // the new one.\n    if (!LoadVT.isSimple() || !BitcastVT.isSimple())\n      return true;\n\n    MVT LoadMVT = LoadVT.getSimpleVT();\n\n    // Don't bother doing this if it's just going to be promoted again later, as\n    // doing so might interfere with other combines.\n    if (getOperationAction(ISD::LOAD, LoadMVT) == Promote &&\n        getTypeToPromoteTo(ISD::LOAD, LoadMVT) == BitcastVT.getSimpleVT())\n      return false;\n\n    bool Fast = false;\n    return allowsMemoryAccess(*DAG.getContext(), DAG.getDataLayout(), BitcastVT,\n                              MMO, &Fast) && Fast;\n  }\n\n  /// Return true if the following transform is beneficial:\n  /// (store (y (conv x)), y*)) -> (store x, (x*))\n  virtual bool isStoreBitCastBeneficial(EVT StoreVT, EVT BitcastVT,\n                                        const SelectionDAG &DAG,\n                                        const MachineMemOperand &MMO) const {\n    // Default to the same logic as loads.\n    return isLoadBitCastBeneficial(StoreVT, BitcastVT, DAG, MMO);\n  }\n\n  /// Return true if it is expected to be cheaper to do a store of a non-zero\n  /// vector constant with the given size and type for the address space than to\n  /// store the individual scalar element constants.\n  virtual bool storeOfVectorConstantIsCheap(EVT MemVT,\n                                            unsigned NumElem,\n                                            unsigned AddrSpace) const {\n    return false;\n  }\n\n  /// Allow store merging for the specified type after legalization in addition\n  /// to before legalization. This may transform stores that do not exist\n  /// earlier (for example, stores created from intrinsics).\n  virtual bool mergeStoresAfterLegalization(EVT MemVT) const {\n    return true;\n  }\n\n  /// Returns if it's reasonable to merge stores to MemVT size.\n  virtual bool canMergeStoresTo(unsigned AS, EVT MemVT,\n                                const SelectionDAG &DAG) const {\n    return true;\n  }\n\n  /// Return true if it is cheap to speculate a call to intrinsic cttz.\n  virtual bool isCheapToSpeculateCttz() const {\n    return false;\n  }\n\n  /// Return true if it is cheap to speculate a call to intrinsic ctlz.\n  virtual bool isCheapToSpeculateCtlz() const {\n    return false;\n  }\n\n  /// Return true if ctlz instruction is fast.\n  virtual bool isCtlzFast() const {\n    return false;\n  }\n\n  /// Return the maximum number of \"x & (x - 1)\" operations that can be done\n  /// instead of deferring to a custom CTPOP.\n  virtual unsigned getCustomCtpopCost(EVT VT, ISD::CondCode Cond) const {\n    return 1;\n  }\n\n  /// Return true if instruction generated for equality comparison is folded\n  /// with instruction generated for signed comparison.\n  virtual bool isEqualityCmpFoldedWithSignedCmp() const { return true; }\n\n  /// Return true if it is safe to transform an integer-domain bitwise operation\n  /// into the equivalent floating-point operation. This should be set to true\n  /// if the target has IEEE-754-compliant fabs/fneg operations for the input\n  /// type.\n  virtual bool hasBitPreservingFPLogic(EVT VT) const {\n    return false;\n  }\n\n  /// Return true if it is cheaper to split the store of a merged int val\n  /// from a pair of smaller values into multiple stores.\n  virtual bool isMultiStoresCheaperThanBitsMerge(EVT LTy, EVT HTy) const {\n    return false;\n  }\n\n  /// Return if the target supports combining a\n  /// chain like:\n  /// \\code\n  ///   %andResult = and %val1, #mask\n  ///   %icmpResult = icmp %andResult, 0\n  /// \\endcode\n  /// into a single machine instruction of a form like:\n  /// \\code\n  ///   cc = test %register, #mask\n  /// \\endcode\n  virtual bool isMaskAndCmp0FoldingBeneficial(const Instruction &AndI) const {\n    return false;\n  }\n\n  /// Use bitwise logic to make pairs of compares more efficient. For example:\n  /// and (seteq A, B), (seteq C, D) --> seteq (or (xor A, B), (xor C, D)), 0\n  /// This should be true when it takes more than one instruction to lower\n  /// setcc (cmp+set on x86 scalar), when bitwise ops are faster than logic on\n  /// condition bits (crand on PowerPC), and/or when reducing cmp+br is a win.\n  virtual bool convertSetCCLogicToBitwiseLogic(EVT VT) const {\n    return false;\n  }\n\n  /// Return the preferred operand type if the target has a quick way to compare\n  /// integer values of the given size. Assume that any legal integer type can\n  /// be compared efficiently. Targets may override this to allow illegal wide\n  /// types to return a vector type if there is support to compare that type.\n  virtual MVT hasFastEqualityCompare(unsigned NumBits) const {\n    MVT VT = MVT::getIntegerVT(NumBits);\n    return isTypeLegal(VT) ? VT : MVT::INVALID_SIMPLE_VALUE_TYPE;\n  }\n\n  /// Return true if the target should transform:\n  /// (X & Y) == Y ---> (~X & Y) == 0\n  /// (X & Y) != Y ---> (~X & Y) != 0\n  ///\n  /// This may be profitable if the target has a bitwise and-not operation that\n  /// sets comparison flags. A target may want to limit the transformation based\n  /// on the type of Y or if Y is a constant.\n  ///\n  /// Note that the transform will not occur if Y is known to be a power-of-2\n  /// because a mask and compare of a single bit can be handled by inverting the\n  /// predicate, for example:\n  /// (X & 8) == 8 ---> (X & 8) != 0\n  virtual bool hasAndNotCompare(SDValue Y) const {\n    return false;\n  }\n\n  /// Return true if the target has a bitwise and-not operation:\n  /// X = ~A & B\n  /// This can be used to simplify select or other instructions.\n  virtual bool hasAndNot(SDValue X) const {\n    // If the target has the more complex version of this operation, assume that\n    // it has this operation too.\n    return hasAndNotCompare(X);\n  }\n\n  /// Return true if the target has a bit-test instruction:\n  ///   (X & (1 << Y)) ==/!= 0\n  /// This knowledge can be used to prevent breaking the pattern,\n  /// or creating it if it could be recognized.\n  virtual bool hasBitTest(SDValue X, SDValue Y) const { return false; }\n\n  /// There are two ways to clear extreme bits (either low or high):\n  /// Mask:    x &  (-1 << y)  (the instcombine canonical form)\n  /// Shifts:  x >> y << y\n  /// Return true if the variant with 2 variable shifts is preferred.\n  /// Return false if there is no preference.\n  virtual bool shouldFoldMaskToVariableShiftPair(SDValue X) const {\n    // By default, let's assume that no one prefers shifts.\n    return false;\n  }\n\n  /// Return true if it is profitable to fold a pair of shifts into a mask.\n  /// This is usually true on most targets. But some targets, like Thumb1,\n  /// have immediate shift instructions, but no immediate \"and\" instruction;\n  /// this makes the fold unprofitable.\n  virtual bool shouldFoldConstantShiftPairToMask(const SDNode *N,\n                                                 CombineLevel Level) const {\n    return true;\n  }\n\n  /// Should we tranform the IR-optimal check for whether given truncation\n  /// down into KeptBits would be truncating or not:\n  ///   (add %x, (1 << (KeptBits-1))) srccond (1 << KeptBits)\n  /// Into it's more traditional form:\n  ///   ((%x << C) a>> C) dstcond %x\n  /// Return true if we should transform.\n  /// Return false if there is no preference.\n  virtual bool shouldTransformSignedTruncationCheck(EVT XVT,\n                                                    unsigned KeptBits) const {\n    // By default, let's assume that no one prefers shifts.\n    return false;\n  }\n\n  /// Given the pattern\n  ///   (X & (C l>>/<< Y)) ==/!= 0\n  /// return true if it should be transformed into:\n  ///   ((X <</l>> Y) & C) ==/!= 0\n  /// WARNING: if 'X' is a constant, the fold may deadlock!\n  /// FIXME: we could avoid passing XC, but we can't use isConstOrConstSplat()\n  ///        here because it can end up being not linked in.\n  virtual bool shouldProduceAndByConstByHoistingConstFromShiftsLHSOfAnd(\n      SDValue X, ConstantSDNode *XC, ConstantSDNode *CC, SDValue Y,\n      unsigned OldShiftOpcode, unsigned NewShiftOpcode,\n      SelectionDAG &DAG) const {\n    if (hasBitTest(X, Y)) {\n      // One interesting pattern that we'd want to form is 'bit test':\n      //   ((1 << Y) & C) ==/!= 0\n      // But we also need to be careful not to try to reverse that fold.\n\n      // Is this '1 << Y' ?\n      if (OldShiftOpcode == ISD::SHL && CC->isOne())\n        return false; // Keep the 'bit test' pattern.\n\n      // Will it be '1 << Y' after the transform ?\n      if (XC && NewShiftOpcode == ISD::SHL && XC->isOne())\n        return true; // Do form the 'bit test' pattern.\n    }\n\n    // If 'X' is a constant, and we transform, then we will immediately\n    // try to undo the fold, thus causing endless combine loop.\n    // So by default, let's assume everyone prefers the fold\n    // iff 'X' is not a constant.\n    return !XC;\n  }\n\n  /// These two forms are equivalent:\n  ///   sub %y, (xor %x, -1)\n  ///   add (add %x, 1), %y\n  /// The variant with two add's is IR-canonical.\n  /// Some targets may prefer one to the other.\n  virtual bool preferIncOfAddToSubOfNot(EVT VT) const {\n    // By default, let's assume that everyone prefers the form with two add's.\n    return true;\n  }\n\n  /// Return true if the target wants to use the optimization that\n  /// turns ext(promotableInst1(...(promotableInstN(load)))) into\n  /// promotedInst1(...(promotedInstN(ext(load)))).\n  bool enableExtLdPromotion() const { return EnableExtLdPromotion; }\n\n  /// Return true if the target can combine store(extractelement VectorTy,\n  /// Idx).\n  /// \\p Cost[out] gives the cost of that transformation when this is true.\n  virtual bool canCombineStoreAndExtract(Type *VectorTy, Value *Idx,\n                                         unsigned &Cost) const {\n    return false;\n  }\n\n  /// Return true if inserting a scalar into a variable element of an undef\n  /// vector is more efficiently handled by splatting the scalar instead.\n  virtual bool shouldSplatInsEltVarIndex(EVT) const {\n    return false;\n  }\n\n  /// Return true if target always beneficiates from combining into FMA for a\n  /// given value type. This must typically return false on targets where FMA\n  /// takes more cycles to execute than FADD.\n  virtual bool enableAggressiveFMAFusion(EVT VT) const {\n    return false;\n  }\n\n  /// Return the ValueType of the result of SETCC operations.\n  virtual EVT getSetCCResultType(const DataLayout &DL, LLVMContext &Context,\n                                 EVT VT) const;\n\n  /// Return the ValueType for comparison libcalls. Comparions libcalls include\n  /// floating point comparion calls, and Ordered/Unordered check calls on\n  /// floating point numbers.\n  virtual\n  MVT::SimpleValueType getCmpLibcallReturnType() const;\n\n  /// For targets without i1 registers, this gives the nature of the high-bits\n  /// of boolean values held in types wider than i1.\n  ///\n  /// \"Boolean values\" are special true/false values produced by nodes like\n  /// SETCC and consumed (as the condition) by nodes like SELECT and BRCOND.\n  /// Not to be confused with general values promoted from i1.  Some cpus\n  /// distinguish between vectors of boolean and scalars; the isVec parameter\n  /// selects between the two kinds.  For example on X86 a scalar boolean should\n  /// be zero extended from i1, while the elements of a vector of booleans\n  /// should be sign extended from i1.\n  ///\n  /// Some cpus also treat floating point types the same way as they treat\n  /// vectors instead of the way they treat scalars.\n  BooleanContent getBooleanContents(bool isVec, bool isFloat) const {\n    if (isVec)\n      return BooleanVectorContents;\n    return isFloat ? BooleanFloatContents : BooleanContents;\n  }\n\n  BooleanContent getBooleanContents(EVT Type) const {\n    return getBooleanContents(Type.isVector(), Type.isFloatingPoint());\n  }\n\n  /// Return target scheduling preference.\n  Sched::Preference getSchedulingPreference() const {\n    return SchedPreferenceInfo;\n  }\n\n  /// Some scheduler, e.g. hybrid, can switch to different scheduling heuristics\n  /// for different nodes. This function returns the preference (or none) for\n  /// the given node.\n  virtual Sched::Preference getSchedulingPreference(SDNode *) const {\n    return Sched::None;\n  }\n\n  /// Return the register class that should be used for the specified value\n  /// type.\n  virtual const TargetRegisterClass *getRegClassFor(MVT VT, bool isDivergent = false) const {\n    (void)isDivergent;\n    const TargetRegisterClass *RC = RegClassForVT[VT.SimpleTy];\n    assert(RC && \"This value type is not natively supported!\");\n    return RC;\n  }\n\n  /// Allows target to decide about the register class of the\n  /// specific value that is live outside the defining block.\n  /// Returns true if the value needs uniform register class.\n  virtual bool requiresUniformRegister(MachineFunction &MF,\n                                       const Value *) const {\n    return false;\n  }\n\n  /// Return the 'representative' register class for the specified value\n  /// type.\n  ///\n  /// The 'representative' register class is the largest legal super-reg\n  /// register class for the register class of the value type.  For example, on\n  /// i386 the rep register class for i8, i16, and i32 are GR32; while the rep\n  /// register class is GR64 on x86_64.\n  virtual const TargetRegisterClass *getRepRegClassFor(MVT VT) const {\n    const TargetRegisterClass *RC = RepRegClassForVT[VT.SimpleTy];\n    return RC;\n  }\n\n  /// Return the cost of the 'representative' register class for the specified\n  /// value type.\n  virtual uint8_t getRepRegClassCostFor(MVT VT) const {\n    return RepRegClassCostForVT[VT.SimpleTy];\n  }\n\n  /// Return true if SHIFT instructions should be expanded to SHIFT_PARTS\n  /// instructions, and false if a library call is preferred (e.g for code-size\n  /// reasons).\n  virtual bool shouldExpandShift(SelectionDAG &DAG, SDNode *N) const {\n    return true;\n  }\n\n  /// Return true if the target has native support for the specified value type.\n  /// This means that it has a register that directly holds it without\n  /// promotions or expansions.\n  bool isTypeLegal(EVT VT) const {\n    assert(!VT.isSimple() ||\n           (unsigned)VT.getSimpleVT().SimpleTy < array_lengthof(RegClassForVT));\n    return VT.isSimple() && RegClassForVT[VT.getSimpleVT().SimpleTy] != nullptr;\n  }\n\n  class ValueTypeActionImpl {\n    /// ValueTypeActions - For each value type, keep a LegalizeTypeAction enum\n    /// that indicates how instruction selection should deal with the type.\n    LegalizeTypeAction ValueTypeActions[MVT::LAST_VALUETYPE];\n\n  public:\n    ValueTypeActionImpl() {\n      std::fill(std::begin(ValueTypeActions), std::end(ValueTypeActions),\n                TypeLegal);\n    }\n\n    LegalizeTypeAction getTypeAction(MVT VT) const {\n      return ValueTypeActions[VT.SimpleTy];\n    }\n\n    void setTypeAction(MVT VT, LegalizeTypeAction Action) {\n      ValueTypeActions[VT.SimpleTy] = Action;\n    }\n  };\n\n  const ValueTypeActionImpl &getValueTypeActions() const {\n    return ValueTypeActions;\n  }\n\n  /// Return how we should legalize values of this type, either it is already\n  /// legal (return 'Legal') or we need to promote it to a larger type (return\n  /// 'Promote'), or we need to expand it into multiple registers of smaller\n  /// integer type (return 'Expand').  'Custom' is not an option.\n  LegalizeTypeAction getTypeAction(LLVMContext &Context, EVT VT) const {\n    return getTypeConversion(Context, VT).first;\n  }\n  LegalizeTypeAction getTypeAction(MVT VT) const {\n    return ValueTypeActions.getTypeAction(VT);\n  }\n\n  /// For types supported by the target, this is an identity function.  For\n  /// types that must be promoted to larger types, this returns the larger type\n  /// to promote to.  For integer types that are larger than the largest integer\n  /// register, this contains one step in the expansion to get to the smaller\n  /// register. For illegal floating point types, this returns the integer type\n  /// to transform to.\n  EVT getTypeToTransformTo(LLVMContext &Context, EVT VT) const {\n    return getTypeConversion(Context, VT).second;\n  }\n\n  /// For types supported by the target, this is an identity function.  For\n  /// types that must be expanded (i.e. integer types that are larger than the\n  /// largest integer register or illegal floating point types), this returns\n  /// the largest legal type it will be expanded to.\n  EVT getTypeToExpandTo(LLVMContext &Context, EVT VT) const {\n    assert(!VT.isVector());\n    while (true) {\n      switch (getTypeAction(Context, VT)) {\n      case TypeLegal:\n        return VT;\n      case TypeExpandInteger:\n        VT = getTypeToTransformTo(Context, VT);\n        break;\n      default:\n        llvm_unreachable(\"Type is not legal nor is it to be expanded!\");\n      }\n    }\n  }\n\n  /// Vector types are broken down into some number of legal first class types.\n  /// For example, EVT::v8f32 maps to 2 EVT::v4f32 with Altivec or SSE1, or 8\n  /// promoted EVT::f64 values with the X86 FP stack.  Similarly, EVT::v2i64\n  /// turns into 4 EVT::i32 values with both PPC and X86.\n  ///\n  /// This method returns the number of registers needed, and the VT for each\n  /// register.  It also returns the VT and quantity of the intermediate values\n  /// before they are promoted/expanded.\n  unsigned getVectorTypeBreakdown(LLVMContext &Context, EVT VT,\n                                  EVT &IntermediateVT,\n                                  unsigned &NumIntermediates,\n                                  MVT &RegisterVT) const;\n\n  /// Certain targets such as MIPS require that some types such as vectors are\n  /// always broken down into scalars in some contexts. This occurs even if the\n  /// vector type is legal.\n  virtual unsigned getVectorTypeBreakdownForCallingConv(\n      LLVMContext &Context, CallingConv::ID CC, EVT VT, EVT &IntermediateVT,\n      unsigned &NumIntermediates, MVT &RegisterVT) const {\n    return getVectorTypeBreakdown(Context, VT, IntermediateVT, NumIntermediates,\n                                  RegisterVT);\n  }\n\n  struct IntrinsicInfo {\n    unsigned     opc = 0;          // target opcode\n    EVT          memVT;            // memory VT\n\n    // value representing memory location\n    PointerUnion<const Value *, const PseudoSourceValue *> ptrVal;\n\n    int          offset = 0;       // offset off of ptrVal\n    uint64_t     size = 0;         // the size of the memory location\n                                   // (taken from memVT if zero)\n    MaybeAlign align = Align(1);   // alignment\n\n    MachineMemOperand::Flags flags = MachineMemOperand::MONone;\n    IntrinsicInfo() = default;\n  };\n\n  /// Given an intrinsic, checks if on the target the intrinsic will need to map\n  /// to a MemIntrinsicNode (touches memory). If this is the case, it returns\n  /// true and store the intrinsic information into the IntrinsicInfo that was\n  /// passed to the function.\n  virtual bool getTgtMemIntrinsic(IntrinsicInfo &, const CallInst &,\n                                  MachineFunction &,\n                                  unsigned /*Intrinsic*/) const {\n    return false;\n  }\n\n  /// Returns true if the target can instruction select the specified FP\n  /// immediate natively. If false, the legalizer will materialize the FP\n  /// immediate as a load from a constant pool.\n  virtual bool isFPImmLegal(const APFloat & /*Imm*/, EVT /*VT*/,\n                            bool ForCodeSize = false) const {\n    return false;\n  }\n\n  /// Targets can use this to indicate that they only support *some*\n  /// VECTOR_SHUFFLE operations, those with specific masks.  By default, if a\n  /// target supports the VECTOR_SHUFFLE node, all mask values are assumed to be\n  /// legal.\n  virtual bool isShuffleMaskLegal(ArrayRef<int> /*Mask*/, EVT /*VT*/) const {\n    return true;\n  }\n\n  /// Returns true if the operation can trap for the value type.\n  ///\n  /// VT must be a legal type. By default, we optimistically assume most\n  /// operations don't trap except for integer divide and remainder.\n  virtual bool canOpTrap(unsigned Op, EVT VT) const;\n\n  /// Similar to isShuffleMaskLegal. Targets can use this to indicate if there\n  /// is a suitable VECTOR_SHUFFLE that can be used to replace a VAND with a\n  /// constant pool entry.\n  virtual bool isVectorClearMaskLegal(ArrayRef<int> /*Mask*/,\n                                      EVT /*VT*/) const {\n    return false;\n  }\n\n  /// Return how this operation should be treated: either it is legal, needs to\n  /// be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getOperationAction(unsigned Op, EVT VT) const {\n    if (VT.isExtended()) return Expand;\n    // If a target-specific SDNode requires legalization, require the target\n    // to provide custom legalization for it.\n    if (Op >= array_lengthof(OpActions[0])) return Custom;\n    return OpActions[(unsigned)VT.getSimpleVT().SimpleTy][Op];\n  }\n\n  /// Custom method defined by each target to indicate if an operation which\n  /// may require a scale is supported natively by the target.\n  /// If not, the operation is illegal.\n  virtual bool isSupportedFixedPointOperation(unsigned Op, EVT VT,\n                                              unsigned Scale) const {\n    return false;\n  }\n\n  /// Some fixed point operations may be natively supported by the target but\n  /// only for specific scales. This method allows for checking\n  /// if the width is supported by the target for a given operation that may\n  /// depend on scale.\n  LegalizeAction getFixedPointOperationAction(unsigned Op, EVT VT,\n                                              unsigned Scale) const {\n    auto Action = getOperationAction(Op, VT);\n    if (Action != Legal)\n      return Action;\n\n    // This operation is supported in this type but may only work on specific\n    // scales.\n    bool Supported;\n    switch (Op) {\n    default:\n      llvm_unreachable(\"Unexpected fixed point operation.\");\n    case ISD::SMULFIX:\n    case ISD::SMULFIXSAT:\n    case ISD::UMULFIX:\n    case ISD::UMULFIXSAT:\n    case ISD::SDIVFIX:\n    case ISD::SDIVFIXSAT:\n    case ISD::UDIVFIX:\n    case ISD::UDIVFIXSAT:\n      Supported = isSupportedFixedPointOperation(Op, VT, Scale);\n      break;\n    }\n\n    return Supported ? Action : Expand;\n  }\n\n  // If Op is a strict floating-point operation, return the result\n  // of getOperationAction for the equivalent non-strict operation.\n  LegalizeAction getStrictFPOperationAction(unsigned Op, EVT VT) const {\n    unsigned EqOpc;\n    switch (Op) {\n      default: llvm_unreachable(\"Unexpected FP pseudo-opcode\");\n#define DAG_INSTRUCTION(NAME, NARG, ROUND_MODE, INTRINSIC, DAGN)               \\\n      case ISD::STRICT_##DAGN: EqOpc = ISD::DAGN; break;\n#define CMP_INSTRUCTION(NAME, NARG, ROUND_MODE, INTRINSIC, DAGN)               \\\n      case ISD::STRICT_##DAGN: EqOpc = ISD::SETCC; break;\n#include \"llvm/IR/ConstrainedOps.def\"\n    }\n\n    return getOperationAction(EqOpc, VT);\n  }\n\n  /// Return true if the specified operation is legal on this target or can be\n  /// made legal with custom lowering. This is used to help guide high-level\n  /// lowering decisions. LegalOnly is an optional convenience for code paths\n  /// traversed pre and post legalisation.\n  bool isOperationLegalOrCustom(unsigned Op, EVT VT,\n                                bool LegalOnly = false) const {\n    if (LegalOnly)\n      return isOperationLegal(Op, VT);\n\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n      (getOperationAction(Op, VT) == Legal ||\n       getOperationAction(Op, VT) == Custom);\n  }\n\n  /// Return true if the specified operation is legal on this target or can be\n  /// made legal using promotion. This is used to help guide high-level lowering\n  /// decisions. LegalOnly is an optional convenience for code paths traversed\n  /// pre and post legalisation.\n  bool isOperationLegalOrPromote(unsigned Op, EVT VT,\n                                 bool LegalOnly = false) const {\n    if (LegalOnly)\n      return isOperationLegal(Op, VT);\n\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n      (getOperationAction(Op, VT) == Legal ||\n       getOperationAction(Op, VT) == Promote);\n  }\n\n  /// Return true if the specified operation is legal on this target or can be\n  /// made legal with custom lowering or using promotion. This is used to help\n  /// guide high-level lowering decisions. LegalOnly is an optional convenience\n  /// for code paths traversed pre and post legalisation.\n  bool isOperationLegalOrCustomOrPromote(unsigned Op, EVT VT,\n                                         bool LegalOnly = false) const {\n    if (LegalOnly)\n      return isOperationLegal(Op, VT);\n\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n      (getOperationAction(Op, VT) == Legal ||\n       getOperationAction(Op, VT) == Custom ||\n       getOperationAction(Op, VT) == Promote);\n  }\n\n  /// Return true if the operation uses custom lowering, regardless of whether\n  /// the type is legal or not.\n  bool isOperationCustom(unsigned Op, EVT VT) const {\n    return getOperationAction(Op, VT) == Custom;\n  }\n\n  /// Return true if lowering to a jump table is allowed.\n  virtual bool areJTsAllowed(const Function *Fn) const {\n    if (Fn->getFnAttribute(\"no-jump-tables\").getValueAsString() == \"true\")\n      return false;\n\n    return isOperationLegalOrCustom(ISD::BR_JT, MVT::Other) ||\n           isOperationLegalOrCustom(ISD::BRIND, MVT::Other);\n  }\n\n  /// Check whether the range [Low,High] fits in a machine word.\n  bool rangeFitsInWord(const APInt &Low, const APInt &High,\n                       const DataLayout &DL) const {\n    // FIXME: Using the pointer type doesn't seem ideal.\n    uint64_t BW = DL.getIndexSizeInBits(0u);\n    uint64_t Range = (High - Low).getLimitedValue(UINT64_MAX - 1) + 1;\n    return Range <= BW;\n  }\n\n  /// Return true if lowering to a jump table is suitable for a set of case\n  /// clusters which may contain \\p NumCases cases, \\p Range range of values.\n  virtual bool isSuitableForJumpTable(const SwitchInst *SI, uint64_t NumCases,\n                                      uint64_t Range, ProfileSummaryInfo *PSI,\n                                      BlockFrequencyInfo *BFI) const;\n\n  /// Return true if lowering to a bit test is suitable for a set of case\n  /// clusters which contains \\p NumDests unique destinations, \\p Low and\n  /// \\p High as its lowest and highest case values, and expects \\p NumCmps\n  /// case value comparisons. Check if the number of destinations, comparison\n  /// metric, and range are all suitable.\n  bool isSuitableForBitTests(unsigned NumDests, unsigned NumCmps,\n                             const APInt &Low, const APInt &High,\n                             const DataLayout &DL) const {\n    // FIXME: I don't think NumCmps is the correct metric: a single case and a\n    // range of cases both require only one branch to lower. Just looking at the\n    // number of clusters and destinations should be enough to decide whether to\n    // build bit tests.\n\n    // To lower a range with bit tests, the range must fit the bitwidth of a\n    // machine word.\n    if (!rangeFitsInWord(Low, High, DL))\n      return false;\n\n    // Decide whether it's profitable to lower this range with bit tests. Each\n    // destination requires a bit test and branch, and there is an overall range\n    // check branch. For a small number of clusters, separate comparisons might\n    // be cheaper, and for many destinations, splitting the range might be\n    // better.\n    return (NumDests == 1 && NumCmps >= 3) || (NumDests == 2 && NumCmps >= 5) ||\n           (NumDests == 3 && NumCmps >= 6);\n  }\n\n  /// Return true if the specified operation is illegal on this target or\n  /// unlikely to be made legal with custom lowering. This is used to help guide\n  /// high-level lowering decisions.\n  bool isOperationExpand(unsigned Op, EVT VT) const {\n    return (!isTypeLegal(VT) || getOperationAction(Op, VT) == Expand);\n  }\n\n  /// Return true if the specified operation is legal on this target.\n  bool isOperationLegal(unsigned Op, EVT VT) const {\n    return (VT == MVT::Other || isTypeLegal(VT)) &&\n           getOperationAction(Op, VT) == Legal;\n  }\n\n  /// Return how this load with extension should be treated: either it is legal,\n  /// needs to be promoted to a larger size, needs to be expanded to some other\n  /// code sequence, or the target has a custom expander for it.\n  LegalizeAction getLoadExtAction(unsigned ExtType, EVT ValVT,\n                                  EVT MemVT) const {\n    if (ValVT.isExtended() || MemVT.isExtended()) return Expand;\n    unsigned ValI = (unsigned) ValVT.getSimpleVT().SimpleTy;\n    unsigned MemI = (unsigned) MemVT.getSimpleVT().SimpleTy;\n    assert(ExtType < ISD::LAST_LOADEXT_TYPE && ValI < MVT::LAST_VALUETYPE &&\n           MemI < MVT::LAST_VALUETYPE && \"Table isn't big enough!\");\n    unsigned Shift = 4 * ExtType;\n    return (LegalizeAction)((LoadExtActions[ValI][MemI] >> Shift) & 0xf);\n  }\n\n  /// Return true if the specified load with extension is legal on this target.\n  bool isLoadExtLegal(unsigned ExtType, EVT ValVT, EVT MemVT) const {\n    return getLoadExtAction(ExtType, ValVT, MemVT) == Legal;\n  }\n\n  /// Return true if the specified load with extension is legal or custom\n  /// on this target.\n  bool isLoadExtLegalOrCustom(unsigned ExtType, EVT ValVT, EVT MemVT) const {\n    return getLoadExtAction(ExtType, ValVT, MemVT) == Legal ||\n           getLoadExtAction(ExtType, ValVT, MemVT) == Custom;\n  }\n\n  /// Return how this store with truncation should be treated: either it is\n  /// legal, needs to be promoted to a larger size, needs to be expanded to some\n  /// other code sequence, or the target has a custom expander for it.\n  LegalizeAction getTruncStoreAction(EVT ValVT, EVT MemVT) const {\n    if (ValVT.isExtended() || MemVT.isExtended()) return Expand;\n    unsigned ValI = (unsigned) ValVT.getSimpleVT().SimpleTy;\n    unsigned MemI = (unsigned) MemVT.getSimpleVT().SimpleTy;\n    assert(ValI < MVT::LAST_VALUETYPE && MemI < MVT::LAST_VALUETYPE &&\n           \"Table isn't big enough!\");\n    return TruncStoreActions[ValI][MemI];\n  }\n\n  /// Return true if the specified store with truncation is legal on this\n  /// target.\n  bool isTruncStoreLegal(EVT ValVT, EVT MemVT) const {\n    return isTypeLegal(ValVT) && getTruncStoreAction(ValVT, MemVT) == Legal;\n  }\n\n  /// Return true if the specified store with truncation has solution on this\n  /// target.\n  bool isTruncStoreLegalOrCustom(EVT ValVT, EVT MemVT) const {\n    return isTypeLegal(ValVT) &&\n      (getTruncStoreAction(ValVT, MemVT) == Legal ||\n       getTruncStoreAction(ValVT, MemVT) == Custom);\n  }\n\n  /// Return how the indexed load should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedLoadAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_Load);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedLoadLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n      (getIndexedLoadAction(IdxMode, VT.getSimpleVT()) == Legal ||\n       getIndexedLoadAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Return how the indexed store should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedStoreAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_Store);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedStoreLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n      (getIndexedStoreAction(IdxMode, VT.getSimpleVT()) == Legal ||\n       getIndexedStoreAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Return how the indexed load should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedMaskedLoadAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_MaskedLoad);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedMaskedLoadLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n           (getIndexedMaskedLoadAction(IdxMode, VT.getSimpleVT()) == Legal ||\n            getIndexedMaskedLoadAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Return how the indexed store should be treated: either it is legal, needs\n  /// to be promoted to a larger size, needs to be expanded to some other code\n  /// sequence, or the target has a custom expander for it.\n  LegalizeAction getIndexedMaskedStoreAction(unsigned IdxMode, MVT VT) const {\n    return getIndexedModeAction(IdxMode, VT, IMAB_MaskedStore);\n  }\n\n  /// Return true if the specified indexed load is legal on this target.\n  bool isIndexedMaskedStoreLegal(unsigned IdxMode, EVT VT) const {\n    return VT.isSimple() &&\n           (getIndexedMaskedStoreAction(IdxMode, VT.getSimpleVT()) == Legal ||\n            getIndexedMaskedStoreAction(IdxMode, VT.getSimpleVT()) == Custom);\n  }\n\n  /// Returns true if the index type for a masked gather/scatter requires\n  /// extending\n  virtual bool shouldExtendGSIndex(EVT VT, EVT &EltTy) const { return false; }\n\n  // Returns true if VT is a legal index type for masked gathers/scatters\n  // on this target\n  virtual bool shouldRemoveExtendFromGSIndex(EVT VT) const { return false; }\n\n  /// Return how the condition code should be treated: either it is legal, needs\n  /// to be expanded to some other code sequence, or the target has a custom\n  /// expander for it.\n  LegalizeAction\n  getCondCodeAction(ISD::CondCode CC, MVT VT) const {\n    assert((unsigned)CC < array_lengthof(CondCodeActions) &&\n           ((unsigned)VT.SimpleTy >> 3) < array_lengthof(CondCodeActions[0]) &&\n           \"Table isn't big enough!\");\n    // See setCondCodeAction for how this is encoded.\n    uint32_t Shift = 4 * (VT.SimpleTy & 0x7);\n    uint32_t Value = CondCodeActions[CC][VT.SimpleTy >> 3];\n    LegalizeAction Action = (LegalizeAction) ((Value >> Shift) & 0xF);\n    assert(Action != Promote && \"Can't promote condition code!\");\n    return Action;\n  }\n\n  /// Return true if the specified condition code is legal on this target.\n  bool isCondCodeLegal(ISD::CondCode CC, MVT VT) const {\n    return getCondCodeAction(CC, VT) == Legal;\n  }\n\n  /// Return true if the specified condition code is legal or custom on this\n  /// target.\n  bool isCondCodeLegalOrCustom(ISD::CondCode CC, MVT VT) const {\n    return getCondCodeAction(CC, VT) == Legal ||\n           getCondCodeAction(CC, VT) == Custom;\n  }\n\n  /// If the action for this operation is to promote, this method returns the\n  /// ValueType to promote to.\n  MVT getTypeToPromoteTo(unsigned Op, MVT VT) const {\n    assert(getOperationAction(Op, VT) == Promote &&\n           \"This operation isn't promoted!\");\n\n    // See if this has an explicit type specified.\n    std::map<std::pair<unsigned, MVT::SimpleValueType>,\n             MVT::SimpleValueType>::const_iterator PTTI =\n      PromoteToType.find(std::make_pair(Op, VT.SimpleTy));\n    if (PTTI != PromoteToType.end()) return PTTI->second;\n\n    assert((VT.isInteger() || VT.isFloatingPoint()) &&\n           \"Cannot autopromote this type, add it with AddPromotedToType.\");\n\n    MVT NVT = VT;\n    do {\n      NVT = (MVT::SimpleValueType)(NVT.SimpleTy+1);\n      assert(NVT.isInteger() == VT.isInteger() && NVT != MVT::isVoid &&\n             \"Didn't find type to promote to!\");\n    } while (!isTypeLegal(NVT) ||\n              getOperationAction(Op, NVT) == Promote);\n    return NVT;\n  }\n\n  /// Return the EVT corresponding to this LLVM type.  This is fixed by the LLVM\n  /// operations except for the pointer size.  If AllowUnknown is true, this\n  /// will return MVT::Other for types with no EVT counterpart (e.g. structs),\n  /// otherwise it will assert.\n  EVT getValueType(const DataLayout &DL, Type *Ty,\n                   bool AllowUnknown = false) const {\n    // Lower scalar pointers to native pointer types.\n    if (auto *PTy = dyn_cast<PointerType>(Ty))\n      return getPointerTy(DL, PTy->getAddressSpace());\n\n    if (auto *VTy = dyn_cast<VectorType>(Ty)) {\n      Type *EltTy = VTy->getElementType();\n      // Lower vectors of pointers to native pointer types.\n      if (auto *PTy = dyn_cast<PointerType>(EltTy)) {\n        EVT PointerTy(getPointerTy(DL, PTy->getAddressSpace()));\n        EltTy = PointerTy.getTypeForEVT(Ty->getContext());\n      }\n      return EVT::getVectorVT(Ty->getContext(), EVT::getEVT(EltTy, false),\n                              VTy->getElementCount());\n    }\n\n    return EVT::getEVT(Ty, AllowUnknown);\n  }\n\n  EVT getMemValueType(const DataLayout &DL, Type *Ty,\n                      bool AllowUnknown = false) const {\n    // Lower scalar pointers to native pointer types.\n    if (PointerType *PTy = dyn_cast<PointerType>(Ty))\n      return getPointerMemTy(DL, PTy->getAddressSpace());\n    else if (VectorType *VTy = dyn_cast<VectorType>(Ty)) {\n      Type *Elm = VTy->getElementType();\n      if (PointerType *PT = dyn_cast<PointerType>(Elm)) {\n        EVT PointerTy(getPointerMemTy(DL, PT->getAddressSpace()));\n        Elm = PointerTy.getTypeForEVT(Ty->getContext());\n      }\n      return EVT::getVectorVT(Ty->getContext(), EVT::getEVT(Elm, false),\n                              VTy->getElementCount());\n    }\n\n    return getValueType(DL, Ty, AllowUnknown);\n  }\n\n\n  /// Return the MVT corresponding to this LLVM type. See getValueType.\n  MVT getSimpleValueType(const DataLayout &DL, Type *Ty,\n                         bool AllowUnknown = false) const {\n    return getValueType(DL, Ty, AllowUnknown).getSimpleVT();\n  }\n\n  /// Return the desired alignment for ByVal or InAlloca aggregate function\n  /// arguments in the caller parameter area.  This is the actual alignment, not\n  /// its logarithm.\n  virtual unsigned getByValTypeAlignment(Type *Ty, const DataLayout &DL) const;\n\n  /// Return the type of registers that this ValueType will eventually require.\n  MVT getRegisterType(MVT VT) const {\n    assert((unsigned)VT.SimpleTy < array_lengthof(RegisterTypeForVT));\n    return RegisterTypeForVT[VT.SimpleTy];\n  }\n\n  /// Return the type of registers that this ValueType will eventually require.\n  MVT getRegisterType(LLVMContext &Context, EVT VT) const {\n    if (VT.isSimple()) {\n      assert((unsigned)VT.getSimpleVT().SimpleTy <\n                array_lengthof(RegisterTypeForVT));\n      return RegisterTypeForVT[VT.getSimpleVT().SimpleTy];\n    }\n    if (VT.isVector()) {\n      EVT VT1;\n      MVT RegisterVT;\n      unsigned NumIntermediates;\n      (void)getVectorTypeBreakdown(Context, VT, VT1,\n                                   NumIntermediates, RegisterVT);\n      return RegisterVT;\n    }\n    if (VT.isInteger()) {\n      return getRegisterType(Context, getTypeToTransformTo(Context, VT));\n    }\n    llvm_unreachable(\"Unsupported extended type!\");\n  }\n\n  /// Return the number of registers that this ValueType will eventually\n  /// require.\n  ///\n  /// This is one for any types promoted to live in larger registers, but may be\n  /// more than one for types (like i64) that are split into pieces.  For types\n  /// like i140, which are first promoted then expanded, it is the number of\n  /// registers needed to hold all the bits of the original type.  For an i140\n  /// on a 32 bit machine this means 5 registers.\n  unsigned getNumRegisters(LLVMContext &Context, EVT VT) const {\n    if (VT.isSimple()) {\n      assert((unsigned)VT.getSimpleVT().SimpleTy <\n                array_lengthof(NumRegistersForVT));\n      return NumRegistersForVT[VT.getSimpleVT().SimpleTy];\n    }\n    if (VT.isVector()) {\n      EVT VT1;\n      MVT VT2;\n      unsigned NumIntermediates;\n      return getVectorTypeBreakdown(Context, VT, VT1, NumIntermediates, VT2);\n    }\n    if (VT.isInteger()) {\n      unsigned BitWidth = VT.getSizeInBits();\n      unsigned RegWidth = getRegisterType(Context, VT).getSizeInBits();\n      return (BitWidth + RegWidth - 1) / RegWidth;\n    }\n    llvm_unreachable(\"Unsupported extended type!\");\n  }\n\n  /// Certain combinations of ABIs, Targets and features require that types\n  /// are legal for some operations and not for other operations.\n  /// For MIPS all vector types must be passed through the integer register set.\n  virtual MVT getRegisterTypeForCallingConv(LLVMContext &Context,\n                                            CallingConv::ID CC, EVT VT) const {\n    return getRegisterType(Context, VT);\n  }\n\n  /// Certain targets require unusual breakdowns of certain types. For MIPS,\n  /// this occurs when a vector type is used, as vector are passed through the\n  /// integer register set.\n  virtual unsigned getNumRegistersForCallingConv(LLVMContext &Context,\n                                                 CallingConv::ID CC,\n                                                 EVT VT) const {\n    return getNumRegisters(Context, VT);\n  }\n\n  /// Certain targets have context senstive alignment requirements, where one\n  /// type has the alignment requirement of another type.\n  virtual Align getABIAlignmentForCallingConv(Type *ArgTy,\n                                              DataLayout DL) const {\n    return DL.getABITypeAlign(ArgTy);\n  }\n\n  /// If true, then instruction selection should seek to shrink the FP constant\n  /// of the specified type to a smaller type in order to save space and / or\n  /// reduce runtime.\n  virtual bool ShouldShrinkFPConstant(EVT) const { return true; }\n\n  /// Return true if it is profitable to reduce a load to a smaller type.\n  /// Example: (i16 (trunc (i32 (load x))) -> i16 load x\n  virtual bool shouldReduceLoadWidth(SDNode *Load, ISD::LoadExtType ExtTy,\n                                     EVT NewVT) const {\n    // By default, assume that it is cheaper to extract a subvector from a wide\n    // vector load rather than creating multiple narrow vector loads.\n    if (NewVT.isVector() && !Load->hasOneUse())\n      return false;\n\n    return true;\n  }\n\n  /// When splitting a value of the specified type into parts, does the Lo\n  /// or Hi part come first?  This usually follows the endianness, except\n  /// for ppcf128, where the Hi part always comes first.\n  bool hasBigEndianPartOrdering(EVT VT, const DataLayout &DL) const {\n    return DL.isBigEndian() || VT == MVT::ppcf128;\n  }\n\n  /// If true, the target has custom DAG combine transformations that it can\n  /// perform for the specified node.\n  bool hasTargetDAGCombine(ISD::NodeType NT) const {\n    assert(unsigned(NT >> 3) < array_lengthof(TargetDAGCombineArray));\n    return TargetDAGCombineArray[NT >> 3] & (1 << (NT&7));\n  }\n\n  unsigned getGatherAllAliasesMaxDepth() const {\n    return GatherAllAliasesMaxDepth;\n  }\n\n  /// Returns the size of the platform's va_list object.\n  virtual unsigned getVaListSizeInBits(const DataLayout &DL) const {\n    return getPointerTy(DL).getSizeInBits();\n  }\n\n  /// Get maximum # of store operations permitted for llvm.memset\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to replace a call to llvm.memset. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxStoresPerMemset(bool OptSize) const {\n    return OptSize ? MaxStoresPerMemsetOptSize : MaxStoresPerMemset;\n  }\n\n  /// Get maximum # of store operations permitted for llvm.memcpy\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to replace a call to llvm.memcpy. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxStoresPerMemcpy(bool OptSize) const {\n    return OptSize ? MaxStoresPerMemcpyOptSize : MaxStoresPerMemcpy;\n  }\n\n  /// \\brief Get maximum # of store operations to be glued together\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to glue together during lowering of llvm.memcpy. The value is set by\n  //  the target at the performance threshold for such a replacement.\n  virtual unsigned getMaxGluedStoresPerMemcpy() const {\n    return MaxGluedStoresPerMemcpy;\n  }\n\n  /// Get maximum # of load operations permitted for memcmp\n  ///\n  /// This function returns the maximum number of load operations permitted\n  /// to replace a call to memcmp. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxExpandSizeMemcmp(bool OptSize) const {\n    return OptSize ? MaxLoadsPerMemcmpOptSize : MaxLoadsPerMemcmp;\n  }\n\n  /// Get maximum # of store operations permitted for llvm.memmove\n  ///\n  /// This function returns the maximum number of store operations permitted\n  /// to replace a call to llvm.memmove. The value is set by the target at the\n  /// performance threshold for such a replacement. If OptSize is true,\n  /// return the limit for functions that have OptSize attribute.\n  unsigned getMaxStoresPerMemmove(bool OptSize) const {\n    return OptSize ? MaxStoresPerMemmoveOptSize : MaxStoresPerMemmove;\n  }\n\n  /// Determine if the target supports unaligned memory accesses.\n  ///\n  /// This function returns true if the target allows unaligned memory accesses\n  /// of the specified type in the given address space. If true, it also returns\n  /// whether the unaligned memory access is \"fast\" in the last argument by\n  /// reference. This is used, for example, in situations where an array\n  /// copy/move/set is converted to a sequence of store operations. Its use\n  /// helps to ensure that such replacements don't generate code that causes an\n  /// alignment error (trap) on the target machine.\n  virtual bool allowsMisalignedMemoryAccesses(\n      EVT, unsigned AddrSpace = 0, Align Alignment = Align(1),\n      MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n      bool * /*Fast*/ = nullptr) const {\n    return false;\n  }\n\n  /// LLT handling variant.\n  virtual bool allowsMisalignedMemoryAccesses(\n      LLT, unsigned AddrSpace = 0, Align Alignment = Align(1),\n      MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n      bool * /*Fast*/ = nullptr) const {\n    return false;\n  }\n\n  /// This function returns true if the memory access is aligned or if the\n  /// target allows this specific unaligned memory access. If the access is\n  /// allowed, the optional final parameter returns if the access is also fast\n  /// (as defined by the target).\n  bool allowsMemoryAccessForAlignment(\n      LLVMContext &Context, const DataLayout &DL, EVT VT,\n      unsigned AddrSpace = 0, Align Alignment = Align(1),\n      MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n      bool *Fast = nullptr) const;\n\n  /// Return true if the memory access of this type is aligned or if the target\n  /// allows this specific unaligned access for the given MachineMemOperand.\n  /// If the access is allowed, the optional final parameter returns if the\n  /// access is also fast (as defined by the target).\n  bool allowsMemoryAccessForAlignment(LLVMContext &Context,\n                                      const DataLayout &DL, EVT VT,\n                                      const MachineMemOperand &MMO,\n                                      bool *Fast = nullptr) const;\n\n  /// Return true if the target supports a memory access of this type for the\n  /// given address space and alignment. If the access is allowed, the optional\n  /// final parameter returns if the access is also fast (as defined by the\n  /// target).\n  virtual bool\n  allowsMemoryAccess(LLVMContext &Context, const DataLayout &DL, EVT VT,\n                     unsigned AddrSpace = 0, Align Alignment = Align(1),\n                     MachineMemOperand::Flags Flags = MachineMemOperand::MONone,\n                     bool *Fast = nullptr) const;\n\n  /// Return true if the target supports a memory access of this type for the\n  /// given MachineMemOperand. If the access is allowed, the optional\n  /// final parameter returns if the access is also fast (as defined by the\n  /// target).\n  bool allowsMemoryAccess(LLVMContext &Context, const DataLayout &DL, EVT VT,\n                          const MachineMemOperand &MMO,\n                          bool *Fast = nullptr) const;\n\n  /// LLT handling variant.\n  bool allowsMemoryAccess(LLVMContext &Context, const DataLayout &DL, LLT Ty,\n                          const MachineMemOperand &MMO,\n                          bool *Fast = nullptr) const;\n\n  /// Returns the target specific optimal type for load and store operations as\n  /// a result of memset, memcpy, and memmove lowering.\n  /// It returns EVT::Other if the type should be determined using generic\n  /// target-independent logic.\n  virtual EVT\n  getOptimalMemOpType(const MemOp &Op,\n                      const AttributeList & /*FuncAttributes*/) const {\n    return MVT::Other;\n  }\n\n  /// LLT returning variant.\n  virtual LLT\n  getOptimalMemOpLLT(const MemOp &Op,\n                     const AttributeList & /*FuncAttributes*/) const {\n    return LLT();\n  }\n\n  /// Returns true if it's safe to use load / store of the specified type to\n  /// expand memcpy / memset inline.\n  ///\n  /// This is mostly true for all types except for some special cases. For\n  /// example, on X86 targets without SSE2 f64 load / store are done with fldl /\n  /// fstpl which also does type conversion. Note the specified type doesn't\n  /// have to be legal as the hook is used before type legalization.\n  virtual bool isSafeMemOpType(MVT /*VT*/) const { return true; }\n\n  /// Return lower limit for number of blocks in a jump table.\n  virtual unsigned getMinimumJumpTableEntries() const;\n\n  /// Return lower limit of the density in a jump table.\n  unsigned getMinimumJumpTableDensity(bool OptForSize) const;\n\n  /// Return upper limit for number of entries in a jump table.\n  /// Zero if no limit.\n  unsigned getMaximumJumpTableSize() const;\n\n  virtual bool isJumpTableRelative() const;\n\n  /// If a physical register, this specifies the register that\n  /// llvm.savestack/llvm.restorestack should save and restore.\n  Register getStackPointerRegisterToSaveRestore() const {\n    return StackPointerRegisterToSaveRestore;\n  }\n\n  /// If a physical register, this returns the register that receives the\n  /// exception address on entry to an EH pad.\n  virtual Register\n  getExceptionPointerRegister(const Constant *PersonalityFn) const {\n    return Register();\n  }\n\n  /// If a physical register, this returns the register that receives the\n  /// exception typeid on entry to a landing pad.\n  virtual Register\n  getExceptionSelectorRegister(const Constant *PersonalityFn) const {\n    return Register();\n  }\n\n  virtual bool needsFixedCatchObjects() const {\n    report_fatal_error(\"Funclet EH is not implemented for this target\");\n  }\n\n  /// Return the minimum stack alignment of an argument.\n  Align getMinStackArgumentAlignment() const {\n    return MinStackArgumentAlignment;\n  }\n\n  /// Return the minimum function alignment.\n  Align getMinFunctionAlignment() const { return MinFunctionAlignment; }\n\n  /// Return the preferred function alignment.\n  Align getPrefFunctionAlignment() const { return PrefFunctionAlignment; }\n\n  /// Return the preferred loop alignment.\n  virtual Align getPrefLoopAlignment(MachineLoop *ML = nullptr) const {\n    return PrefLoopAlignment;\n  }\n\n  /// Should loops be aligned even when the function is marked OptSize (but not\n  /// MinSize).\n  virtual bool alignLoopsWithOptSize() const {\n    return false;\n  }\n\n  /// If the target has a standard location for the stack protector guard,\n  /// returns the address of that location. Otherwise, returns nullptr.\n  /// DEPRECATED: please override useLoadStackGuardNode and customize\n  ///             LOAD_STACK_GUARD, or customize \\@llvm.stackguard().\n  virtual Value *getIRStackGuard(IRBuilder<> &IRB) const;\n\n  /// Inserts necessary declarations for SSP (stack protection) purpose.\n  /// Should be used only when getIRStackGuard returns nullptr.\n  virtual void insertSSPDeclarations(Module &M) const;\n\n  /// Return the variable that's previously inserted by insertSSPDeclarations,\n  /// if any, otherwise return nullptr. Should be used only when\n  /// getIRStackGuard returns nullptr.\n  virtual Value *getSDagStackGuard(const Module &M) const;\n\n  /// If this function returns true, stack protection checks should XOR the\n  /// frame pointer (or whichever pointer is used to address locals) into the\n  /// stack guard value before checking it. getIRStackGuard must return nullptr\n  /// if this returns true.\n  virtual bool useStackGuardXorFP() const { return false; }\n\n  /// If the target has a standard stack protection check function that\n  /// performs validation and error handling, returns the function. Otherwise,\n  /// returns nullptr. Must be previously inserted by insertSSPDeclarations.\n  /// Should be used only when getIRStackGuard returns nullptr.\n  virtual Function *getSSPStackGuardCheck(const Module &M) const;\n\nprotected:\n  Value *getDefaultSafeStackPointerLocation(IRBuilder<> &IRB,\n                                            bool UseTLS) const;\n\npublic:\n  /// Returns the target-specific address of the unsafe stack pointer.\n  virtual Value *getSafeStackPointerLocation(IRBuilder<> &IRB) const;\n\n  /// Returns the name of the symbol used to emit stack probes or the empty\n  /// string if not applicable.\n  virtual bool hasStackProbeSymbol(MachineFunction &MF) const { return false; }\n\n  virtual bool hasInlineStackProbe(MachineFunction &MF) const { return false; }\n\n  virtual StringRef getStackProbeSymbolName(MachineFunction &MF) const {\n    return \"\";\n  }\n\n  /// Returns true if a cast from SrcAS to DestAS is \"cheap\", such that e.g. we\n  /// are happy to sink it into basic blocks. A cast may be free, but not\n  /// necessarily a no-op. e.g. a free truncate from a 64-bit to 32-bit pointer.\n  virtual bool isFreeAddrSpaceCast(unsigned SrcAS, unsigned DestAS) const;\n\n  /// Return true if the pointer arguments to CI should be aligned by aligning\n  /// the object whose address is being passed. If so then MinSize is set to the\n  /// minimum size the object must be to be aligned and PrefAlign is set to the\n  /// preferred alignment.\n  virtual bool shouldAlignPointerArgs(CallInst * /*CI*/, unsigned & /*MinSize*/,\n                                      unsigned & /*PrefAlign*/) const {\n    return false;\n  }\n\n  //===--------------------------------------------------------------------===//\n  /// \\name Helpers for TargetTransformInfo implementations\n  /// @{\n\n  /// Get the ISD node that corresponds to the Instruction class opcode.\n  int InstructionOpcodeToISD(unsigned Opcode) const;\n\n  /// Estimate the cost of type-legalization and the legalized type.\n  std::pair<int, MVT> getTypeLegalizationCost(const DataLayout &DL,\n                                              Type *Ty) const;\n\n  /// @}\n\n  //===--------------------------------------------------------------------===//\n  /// \\name Helpers for atomic expansion.\n  /// @{\n\n  /// Returns the maximum atomic operation size (in bits) supported by\n  /// the backend. Atomic operations greater than this size (as well\n  /// as ones that are not naturally aligned), will be expanded by\n  /// AtomicExpandPass into an __atomic_* library call.\n  unsigned getMaxAtomicSizeInBitsSupported() const {\n    return MaxAtomicSizeInBitsSupported;\n  }\n\n  /// Returns the size of the smallest cmpxchg or ll/sc instruction\n  /// the backend supports.  Any smaller operations are widened in\n  /// AtomicExpandPass.\n  ///\n  /// Note that *unlike* operations above the maximum size, atomic ops\n  /// are still natively supported below the minimum; they just\n  /// require a more complex expansion.\n  unsigned getMinCmpXchgSizeInBits() const { return MinCmpXchgSizeInBits; }\n\n  /// Whether the target supports unaligned atomic operations.\n  bool supportsUnalignedAtomics() const { return SupportsUnalignedAtomics; }\n\n  /// Whether AtomicExpandPass should automatically insert fences and reduce\n  /// ordering for this atomic. This should be true for most architectures with\n  /// weak memory ordering. Defaults to false.\n  virtual bool shouldInsertFencesForAtomic(const Instruction *I) const {\n    return false;\n  }\n\n  /// Perform a load-linked operation on Addr, returning a \"Value *\" with the\n  /// corresponding pointee type. This may entail some non-trivial operations to\n  /// truncate or reconstruct types that will be illegal in the backend. See\n  /// ARMISelLowering for an example implementation.\n  virtual Value *emitLoadLinked(IRBuilder<> &Builder, Value *Addr,\n                                AtomicOrdering Ord) const {\n    llvm_unreachable(\"Load linked unimplemented on this target\");\n  }\n\n  /// Perform a store-conditional operation to Addr. Return the status of the\n  /// store. This should be 0 if the store succeeded, non-zero otherwise.\n  virtual Value *emitStoreConditional(IRBuilder<> &Builder, Value *Val,\n                                      Value *Addr, AtomicOrdering Ord) const {\n    llvm_unreachable(\"Store conditional unimplemented on this target\");\n  }\n\n  /// Perform a masked atomicrmw using a target-specific intrinsic. This\n  /// represents the core LL/SC loop which will be lowered at a late stage by\n  /// the backend.\n  virtual Value *emitMaskedAtomicRMWIntrinsic(IRBuilder<> &Builder,\n                                              AtomicRMWInst *AI,\n                                              Value *AlignedAddr, Value *Incr,\n                                              Value *Mask, Value *ShiftAmt,\n                                              AtomicOrdering Ord) const {\n    llvm_unreachable(\"Masked atomicrmw expansion unimplemented on this target\");\n  }\n\n  /// Perform a masked cmpxchg using a target-specific intrinsic. This\n  /// represents the core LL/SC loop which will be lowered at a late stage by\n  /// the backend.\n  virtual Value *emitMaskedAtomicCmpXchgIntrinsic(\n      IRBuilder<> &Builder, AtomicCmpXchgInst *CI, Value *AlignedAddr,\n      Value *CmpVal, Value *NewVal, Value *Mask, AtomicOrdering Ord) const {\n    llvm_unreachable(\"Masked cmpxchg expansion unimplemented on this target\");\n  }\n\n  /// Inserts in the IR a target-specific intrinsic specifying a fence.\n  /// It is called by AtomicExpandPass before expanding an\n  ///   AtomicRMW/AtomicCmpXchg/AtomicStore/AtomicLoad\n  ///   if shouldInsertFencesForAtomic returns true.\n  ///\n  /// Inst is the original atomic instruction, prior to other expansions that\n  /// may be performed.\n  ///\n  /// This function should either return a nullptr, or a pointer to an IR-level\n  ///   Instruction*. Even complex fence sequences can be represented by a\n  ///   single Instruction* through an intrinsic to be lowered later.\n  /// Backends should override this method to produce target-specific intrinsic\n  ///   for their fences.\n  /// FIXME: Please note that the default implementation here in terms of\n  ///   IR-level fences exists for historical/compatibility reasons and is\n  ///   *unsound* ! Fences cannot, in general, be used to restore sequential\n  ///   consistency. For example, consider the following example:\n  /// atomic<int> x = y = 0;\n  /// int r1, r2, r3, r4;\n  /// Thread 0:\n  ///   x.store(1);\n  /// Thread 1:\n  ///   y.store(1);\n  /// Thread 2:\n  ///   r1 = x.load();\n  ///   r2 = y.load();\n  /// Thread 3:\n  ///   r3 = y.load();\n  ///   r4 = x.load();\n  ///  r1 = r3 = 1 and r2 = r4 = 0 is impossible as long as the accesses are all\n  ///  seq_cst. But if they are lowered to monotonic accesses, no amount of\n  ///  IR-level fences can prevent it.\n  /// @{\n  virtual Instruction *emitLeadingFence(IRBuilder<> &Builder, Instruction *Inst,\n                                        AtomicOrdering Ord) const {\n    if (isReleaseOrStronger(Ord) && Inst->hasAtomicStore())\n      return Builder.CreateFence(Ord);\n    else\n      return nullptr;\n  }\n\n  virtual Instruction *emitTrailingFence(IRBuilder<> &Builder,\n                                         Instruction *Inst,\n                                         AtomicOrdering Ord) const {\n    if (isAcquireOrStronger(Ord))\n      return Builder.CreateFence(Ord);\n    else\n      return nullptr;\n  }\n  /// @}\n\n  // Emits code that executes when the comparison result in the ll/sc\n  // expansion of a cmpxchg instruction is such that the store-conditional will\n  // not execute.  This makes it possible to balance out the load-linked with\n  // a dedicated instruction, if desired.\n  // E.g., on ARM, if ldrex isn't followed by strex, the exclusive monitor would\n  // be unnecessarily held, except if clrex, inserted by this hook, is executed.\n  virtual void emitAtomicCmpXchgNoStoreLLBalance(IRBuilder<> &Builder) const {}\n\n  /// Returns true if the given (atomic) store should be expanded by the\n  /// IR-level AtomicExpand pass into an \"atomic xchg\" which ignores its input.\n  virtual bool shouldExpandAtomicStoreInIR(StoreInst *SI) const {\n    return false;\n  }\n\n  /// Returns true if arguments should be sign-extended in lib calls.\n  virtual bool shouldSignExtendTypeInLibCall(EVT Type, bool IsSigned) const {\n    return IsSigned;\n  }\n\n  /// Returns true if arguments should be extended in lib calls.\n  virtual bool shouldExtendTypeInLibCall(EVT Type) const {\n    return true;\n  }\n\n  /// Returns how the given (atomic) load should be expanded by the\n  /// IR-level AtomicExpand pass.\n  virtual AtomicExpansionKind shouldExpandAtomicLoadInIR(LoadInst *LI) const {\n    return AtomicExpansionKind::None;\n  }\n\n  /// Returns how the given atomic cmpxchg should be expanded by the IR-level\n  /// AtomicExpand pass.\n  virtual AtomicExpansionKind\n  shouldExpandAtomicCmpXchgInIR(AtomicCmpXchgInst *AI) const {\n    return AtomicExpansionKind::None;\n  }\n\n  /// Returns how the IR-level AtomicExpand pass should expand the given\n  /// AtomicRMW, if at all. Default is to never expand.\n  virtual AtomicExpansionKind shouldExpandAtomicRMWInIR(AtomicRMWInst *RMW) const {\n    return RMW->isFloatingPointOperation() ?\n      AtomicExpansionKind::CmpXChg : AtomicExpansionKind::None;\n  }\n\n  /// On some platforms, an AtomicRMW that never actually modifies the value\n  /// (such as fetch_add of 0) can be turned into a fence followed by an\n  /// atomic load. This may sound useless, but it makes it possible for the\n  /// processor to keep the cacheline shared, dramatically improving\n  /// performance. And such idempotent RMWs are useful for implementing some\n  /// kinds of locks, see for example (justification + benchmarks):\n  /// http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf\n  /// This method tries doing that transformation, returning the atomic load if\n  /// it succeeds, and nullptr otherwise.\n  /// If shouldExpandAtomicLoadInIR returns true on that load, it will undergo\n  /// another round of expansion.\n  virtual LoadInst *\n  lowerIdempotentRMWIntoFencedLoad(AtomicRMWInst *RMWI) const {\n    return nullptr;\n  }\n\n  /// Returns how the platform's atomic operations are extended (ZERO_EXTEND,\n  /// SIGN_EXTEND, or ANY_EXTEND).\n  virtual ISD::NodeType getExtendForAtomicOps() const {\n    return ISD::ZERO_EXTEND;\n  }\n\n  /// Returns how the platform's atomic compare and swap expects its comparison\n  /// value to be extended (ZERO_EXTEND, SIGN_EXTEND, or ANY_EXTEND). This is\n  /// separate from getExtendForAtomicOps, which is concerned with the\n  /// sign-extension of the instruction's output, whereas here we are concerned\n  /// with the sign-extension of the input. For targets with compare-and-swap\n  /// instructions (or sub-word comparisons in their LL/SC loop expansions),\n  /// the input can be ANY_EXTEND, but the output will still have a specific\n  /// extension.\n  virtual ISD::NodeType getExtendForAtomicCmpSwapArg() const {\n    return ISD::ANY_EXTEND;\n  }\n\n  /// @}\n\n  /// Returns true if we should normalize\n  /// select(N0&N1, X, Y) => select(N0, select(N1, X, Y), Y) and\n  /// select(N0|N1, X, Y) => select(N0, select(N1, X, Y, Y)) if it is likely\n  /// that it saves us from materializing N0 and N1 in an integer register.\n  /// Targets that are able to perform and/or on flags should return false here.\n  virtual bool shouldNormalizeToSelectSequence(LLVMContext &Context,\n                                               EVT VT) const {\n    // If a target has multiple condition registers, then it likely has logical\n    // operations on those registers.\n    if (hasMultipleConditionRegisters())\n      return false;\n    // Only do the transform if the value won't be split into multiple\n    // registers.\n    LegalizeTypeAction Action = getTypeAction(Context, VT);\n    return Action != TypeExpandInteger && Action != TypeExpandFloat &&\n      Action != TypeSplitVector;\n  }\n\n  virtual bool isProfitableToCombineMinNumMaxNum(EVT VT) const { return true; }\n\n  /// Return true if a select of constants (select Cond, C1, C2) should be\n  /// transformed into simple math ops with the condition value. For example:\n  /// select Cond, C1, C1-1 --> add (zext Cond), C1-1\n  virtual bool convertSelectOfConstantsToMath(EVT VT) const {\n    return false;\n  }\n\n  /// Return true if it is profitable to transform an integer\n  /// multiplication-by-constant into simpler operations like shifts and adds.\n  /// This may be true if the target does not directly support the\n  /// multiplication operation for the specified type or the sequence of simpler\n  /// ops is faster than the multiply.\n  virtual bool decomposeMulByConstant(LLVMContext &Context,\n                                      EVT VT, SDValue C) const {\n    return false;\n  }\n\n  /// Return true if it is more correct/profitable to use strict FP_TO_INT\n  /// conversion operations - canonicalizing the FP source value instead of\n  /// converting all cases and then selecting based on value.\n  /// This may be true if the target throws exceptions for out of bounds\n  /// conversions or has fast FP CMOV.\n  virtual bool shouldUseStrictFP_TO_INT(EVT FpVT, EVT IntVT,\n                                        bool IsSigned) const {\n    return false;\n  }\n\n  //===--------------------------------------------------------------------===//\n  // TargetLowering Configuration Methods - These methods should be invoked by\n  // the derived class constructor to configure this object for the target.\n  //\nprotected:\n  /// Specify how the target extends the result of integer and floating point\n  /// boolean values from i1 to a wider type.  See getBooleanContents.\n  void setBooleanContents(BooleanContent Ty) {\n    BooleanContents = Ty;\n    BooleanFloatContents = Ty;\n  }\n\n  /// Specify how the target extends the result of integer and floating point\n  /// boolean values from i1 to a wider type.  See getBooleanContents.\n  void setBooleanContents(BooleanContent IntTy, BooleanContent FloatTy) {\n    BooleanContents = IntTy;\n    BooleanFloatContents = FloatTy;\n  }\n\n  /// Specify how the target extends the result of a vector boolean value from a\n  /// vector of i1 to a wider type.  See getBooleanContents.\n  void setBooleanVectorContents(BooleanContent Ty) {\n    BooleanVectorContents = Ty;\n  }\n\n  /// Specify the target scheduling preference.\n  void setSchedulingPreference(Sched::Preference Pref) {\n    SchedPreferenceInfo = Pref;\n  }\n\n  /// Indicate the minimum number of blocks to generate jump tables.\n  void setMinimumJumpTableEntries(unsigned Val);\n\n  /// Indicate the maximum number of entries in jump tables.\n  /// Set to zero to generate unlimited jump tables.\n  void setMaximumJumpTableSize(unsigned);\n\n  /// If set to a physical register, this specifies the register that\n  /// llvm.savestack/llvm.restorestack should save and restore.\n  void setStackPointerRegisterToSaveRestore(Register R) {\n    StackPointerRegisterToSaveRestore = R;\n  }\n\n  /// Tells the code generator that the target has multiple (allocatable)\n  /// condition registers that can be used to store the results of comparisons\n  /// for use by selects and conditional branches. With multiple condition\n  /// registers, the code generator will not aggressively sink comparisons into\n  /// the blocks of their users.\n  void setHasMultipleConditionRegisters(bool hasManyRegs = true) {\n    HasMultipleConditionRegisters = hasManyRegs;\n  }\n\n  /// Tells the code generator that the target has BitExtract instructions.\n  /// The code generator will aggressively sink \"shift\"s into the blocks of\n  /// their users if the users will generate \"and\" instructions which can be\n  /// combined with \"shift\" to BitExtract instructions.\n  void setHasExtractBitsInsn(bool hasExtractInsn = true) {\n    HasExtractBitsInsn = hasExtractInsn;\n  }\n\n  /// Tells the code generator not to expand logic operations on comparison\n  /// predicates into separate sequences that increase the amount of flow\n  /// control.\n  void setJumpIsExpensive(bool isExpensive = true);\n\n  /// Tells the code generator which bitwidths to bypass.\n  void addBypassSlowDiv(unsigned int SlowBitWidth, unsigned int FastBitWidth) {\n    BypassSlowDivWidths[SlowBitWidth] = FastBitWidth;\n  }\n\n  /// Add the specified register class as an available regclass for the\n  /// specified value type. This indicates the selector can handle values of\n  /// that class natively.\n  void addRegisterClass(MVT VT, const TargetRegisterClass *RC) {\n    assert((unsigned)VT.SimpleTy < array_lengthof(RegClassForVT));\n    RegClassForVT[VT.SimpleTy] = RC;\n  }\n\n  /// Return the largest legal super-reg register class of the register class\n  /// for the specified type and its associated \"cost\".\n  virtual std::pair<const TargetRegisterClass *, uint8_t>\n  findRepresentativeClass(const TargetRegisterInfo *TRI, MVT VT) const;\n\n  /// Once all of the register classes are added, this allows us to compute\n  /// derived properties we expose.\n  void computeRegisterProperties(const TargetRegisterInfo *TRI);\n\n  /// Indicate that the specified operation does not work with the specified\n  /// type and indicate what to do about it. Note that VT may refer to either\n  /// the type of a result or that of an operand of Op.\n  void setOperationAction(unsigned Op, MVT VT,\n                          LegalizeAction Action) {\n    assert(Op < array_lengthof(OpActions[0]) && \"Table isn't big enough!\");\n    OpActions[(unsigned)VT.SimpleTy][Op] = Action;\n  }\n\n  /// Indicate that the specified load with extension does not work with the\n  /// specified type and indicate what to do about it.\n  void setLoadExtAction(unsigned ExtType, MVT ValVT, MVT MemVT,\n                        LegalizeAction Action) {\n    assert(ExtType < ISD::LAST_LOADEXT_TYPE && ValVT.isValid() &&\n           MemVT.isValid() && \"Table isn't big enough!\");\n    assert((unsigned)Action < 0x10 && \"too many bits for bitfield array\");\n    unsigned Shift = 4 * ExtType;\n    LoadExtActions[ValVT.SimpleTy][MemVT.SimpleTy] &= ~((uint16_t)0xF << Shift);\n    LoadExtActions[ValVT.SimpleTy][MemVT.SimpleTy] |= (uint16_t)Action << Shift;\n  }\n\n  /// Indicate that the specified truncating store does not work with the\n  /// specified type and indicate what to do about it.\n  void setTruncStoreAction(MVT ValVT, MVT MemVT,\n                           LegalizeAction Action) {\n    assert(ValVT.isValid() && MemVT.isValid() && \"Table isn't big enough!\");\n    TruncStoreActions[(unsigned)ValVT.SimpleTy][MemVT.SimpleTy] = Action;\n  }\n\n  /// Indicate that the specified indexed load does or does not work with the\n  /// specified type and indicate what to do abort it.\n  ///\n  /// NOTE: All indexed mode loads are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedLoadAction(unsigned IdxMode, MVT VT, LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_Load, Action);\n  }\n\n  /// Indicate that the specified indexed store does or does not work with the\n  /// specified type and indicate what to do about it.\n  ///\n  /// NOTE: All indexed mode stores are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedStoreAction(unsigned IdxMode, MVT VT, LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_Store, Action);\n  }\n\n  /// Indicate that the specified indexed masked load does or does not work with\n  /// the specified type and indicate what to do about it.\n  ///\n  /// NOTE: All indexed mode masked loads are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedMaskedLoadAction(unsigned IdxMode, MVT VT,\n                                  LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_MaskedLoad, Action);\n  }\n\n  /// Indicate that the specified indexed masked store does or does not work\n  /// with the specified type and indicate what to do about it.\n  ///\n  /// NOTE: All indexed mode masked stores are initialized to Expand in\n  /// TargetLowering.cpp\n  void setIndexedMaskedStoreAction(unsigned IdxMode, MVT VT,\n                                   LegalizeAction Action) {\n    setIndexedModeAction(IdxMode, VT, IMAB_MaskedStore, Action);\n  }\n\n  /// Indicate that the specified condition code is or isn't supported on the\n  /// target and indicate what to do about it.\n  void setCondCodeAction(ISD::CondCode CC, MVT VT,\n                         LegalizeAction Action) {\n    assert(VT.isValid() && (unsigned)CC < array_lengthof(CondCodeActions) &&\n           \"Table isn't big enough!\");\n    assert((unsigned)Action < 0x10 && \"too many bits for bitfield array\");\n    /// The lower 3 bits of the SimpleTy index into Nth 4bit set from the 32-bit\n    /// value and the upper 29 bits index into the second dimension of the array\n    /// to select what 32-bit value to use.\n    uint32_t Shift = 4 * (VT.SimpleTy & 0x7);\n    CondCodeActions[CC][VT.SimpleTy >> 3] &= ~((uint32_t)0xF << Shift);\n    CondCodeActions[CC][VT.SimpleTy >> 3] |= (uint32_t)Action << Shift;\n  }\n\n  /// If Opc/OrigVT is specified as being promoted, the promotion code defaults\n  /// to trying a larger integer/fp until it can find one that works. If that\n  /// default is insufficient, this method can be used by the target to override\n  /// the default.\n  void AddPromotedToType(unsigned Opc, MVT OrigVT, MVT DestVT) {\n    PromoteToType[std::make_pair(Opc, OrigVT.SimpleTy)] = DestVT.SimpleTy;\n  }\n\n  /// Convenience method to set an operation to Promote and specify the type\n  /// in a single call.\n  void setOperationPromotedToType(unsigned Opc, MVT OrigVT, MVT DestVT) {\n    setOperationAction(Opc, OrigVT, Promote);\n    AddPromotedToType(Opc, OrigVT, DestVT);\n  }\n\n  /// Targets should invoke this method for each target independent node that\n  /// they want to provide a custom DAG combiner for by implementing the\n  /// PerformDAGCombine virtual method.\n  void setTargetDAGCombine(ISD::NodeType NT) {\n    assert(unsigned(NT >> 3) < array_lengthof(TargetDAGCombineArray));\n    TargetDAGCombineArray[NT >> 3] |= 1 << (NT&7);\n  }\n\n  /// Set the target's minimum function alignment.\n  void setMinFunctionAlignment(Align Alignment) {\n    MinFunctionAlignment = Alignment;\n  }\n\n  /// Set the target's preferred function alignment.  This should be set if\n  /// there is a performance benefit to higher-than-minimum alignment\n  void setPrefFunctionAlignment(Align Alignment) {\n    PrefFunctionAlignment = Alignment;\n  }\n\n  /// Set the target's preferred loop alignment. Default alignment is one, it\n  /// means the target does not care about loop alignment. The target may also\n  /// override getPrefLoopAlignment to provide per-loop values.\n  void setPrefLoopAlignment(Align Alignment) { PrefLoopAlignment = Alignment; }\n\n  /// Set the minimum stack alignment of an argument.\n  void setMinStackArgumentAlignment(Align Alignment) {\n    MinStackArgumentAlignment = Alignment;\n  }\n\n  /// Set the maximum atomic operation size supported by the\n  /// backend. Atomic operations greater than this size (as well as\n  /// ones that are not naturally aligned), will be expanded by\n  /// AtomicExpandPass into an __atomic_* library call.\n  void setMaxAtomicSizeInBitsSupported(unsigned SizeInBits) {\n    MaxAtomicSizeInBitsSupported = SizeInBits;\n  }\n\n  /// Sets the minimum cmpxchg or ll/sc size supported by the backend.\n  void setMinCmpXchgSizeInBits(unsigned SizeInBits) {\n    MinCmpXchgSizeInBits = SizeInBits;\n  }\n\n  /// Sets whether unaligned atomic operations are supported.\n  void setSupportsUnalignedAtomics(bool UnalignedSupported) {\n    SupportsUnalignedAtomics = UnalignedSupported;\n  }\n\npublic:\n  //===--------------------------------------------------------------------===//\n  // Addressing mode description hooks (used by LSR etc).\n  //\n\n  /// CodeGenPrepare sinks address calculations into the same BB as Load/Store\n  /// instructions reading the address. This allows as much computation as\n  /// possible to be done in the address mode for that operand. This hook lets\n  /// targets also pass back when this should be done on intrinsics which\n  /// load/store.\n  virtual bool getAddrModeArguments(IntrinsicInst * /*I*/,\n                                    SmallVectorImpl<Value*> &/*Ops*/,\n                                    Type *&/*AccessTy*/) const {\n    return false;\n  }\n\n  /// This represents an addressing mode of:\n  ///    BaseGV + BaseOffs + BaseReg + Scale*ScaleReg\n  /// If BaseGV is null,  there is no BaseGV.\n  /// If BaseOffs is zero, there is no base offset.\n  /// If HasBaseReg is false, there is no base register.\n  /// If Scale is zero, there is no ScaleReg.  Scale of 1 indicates a reg with\n  /// no scale.\n  struct AddrMode {\n    GlobalValue *BaseGV = nullptr;\n    int64_t      BaseOffs = 0;\n    bool         HasBaseReg = false;\n    int64_t      Scale = 0;\n    AddrMode() = default;\n  };\n\n  /// Return true if the addressing mode represented by AM is legal for this\n  /// target, for a load/store of the specified type.\n  ///\n  /// The type may be VoidTy, in which case only return true if the addressing\n  /// mode is legal for a load/store of any legal type.  TODO: Handle\n  /// pre/postinc as well.\n  ///\n  /// If the address space cannot be determined, it will be -1.\n  ///\n  /// TODO: Remove default argument\n  virtual bool isLegalAddressingMode(const DataLayout &DL, const AddrMode &AM,\n                                     Type *Ty, unsigned AddrSpace,\n                                     Instruction *I = nullptr) const;\n\n  /// Return the cost of the scaling factor used in the addressing mode\n  /// represented by AM for this target, for a load/store of the specified type.\n  ///\n  /// If the AM is supported, the return value must be >= 0.\n  /// If the AM is not supported, it returns a negative value.\n  /// TODO: Handle pre/postinc as well.\n  /// TODO: Remove default argument\n  virtual int getScalingFactorCost(const DataLayout &DL, const AddrMode &AM,\n                                   Type *Ty, unsigned AS = 0) const {\n    // Default: assume that any scaling factor used in a legal AM is free.\n    if (isLegalAddressingMode(DL, AM, Ty, AS))\n      return 0;\n    return -1;\n  }\n\n  /// Return true if the specified immediate is legal icmp immediate, that is\n  /// the target has icmp instructions which can compare a register against the\n  /// immediate without having to materialize the immediate into a register.\n  virtual bool isLegalICmpImmediate(int64_t) const {\n    return true;\n  }\n\n  /// Return true if the specified immediate is legal add immediate, that is the\n  /// target has add instructions which can add a register with the immediate\n  /// without having to materialize the immediate into a register.\n  virtual bool isLegalAddImmediate(int64_t) const {\n    return true;\n  }\n\n  /// Return true if the specified immediate is legal for the value input of a\n  /// store instruction.\n  virtual bool isLegalStoreImmediate(int64_t Value) const {\n    // Default implementation assumes that at least 0 works since it is likely\n    // that a zero register exists or a zero immediate is allowed.\n    return Value == 0;\n  }\n\n  /// Return true if it's significantly cheaper to shift a vector by a uniform\n  /// scalar than by an amount which will vary across each lane. On x86 before\n  /// AVX2 for example, there is a \"psllw\" instruction for the former case, but\n  /// no simple instruction for a general \"a << b\" operation on vectors.\n  /// This should also apply to lowering for vector funnel shifts (rotates).\n  virtual bool isVectorShiftByScalarCheap(Type *Ty) const {\n    return false;\n  }\n\n  /// Given a shuffle vector SVI representing a vector splat, return a new\n  /// scalar type of size equal to SVI's scalar type if the new type is more\n  /// profitable. Returns nullptr otherwise. For example under MVE float splats\n  /// are converted to integer to prevent the need to move from SPR to GPR\n  /// registers.\n  virtual Type* shouldConvertSplatType(ShuffleVectorInst* SVI) const {\n    return nullptr;\n  }\n\n  /// Given a set in interconnected phis of type 'From' that are loaded/stored\n  /// or bitcast to type 'To', return true if the set should be converted to\n  /// 'To'.\n  virtual bool shouldConvertPhiType(Type *From, Type *To) const {\n    return (From->isIntegerTy() || From->isFloatingPointTy()) &&\n           (To->isIntegerTy() || To->isFloatingPointTy());\n  }\n\n  /// Returns true if the opcode is a commutative binary operation.\n  virtual bool isCommutativeBinOp(unsigned Opcode) const {\n    // FIXME: This should get its info from the td file.\n    switch (Opcode) {\n    case ISD::ADD:\n    case ISD::SMIN:\n    case ISD::SMAX:\n    case ISD::UMIN:\n    case ISD::UMAX:\n    case ISD::MUL:\n    case ISD::MULHU:\n    case ISD::MULHS:\n    case ISD::SMUL_LOHI:\n    case ISD::UMUL_LOHI:\n    case ISD::FADD:\n    case ISD::FMUL:\n    case ISD::AND:\n    case ISD::OR:\n    case ISD::XOR:\n    case ISD::SADDO:\n    case ISD::UADDO:\n    case ISD::ADDC:\n    case ISD::ADDE:\n    case ISD::SADDSAT:\n    case ISD::UADDSAT:\n    case ISD::FMINNUM:\n    case ISD::FMAXNUM:\n    case ISD::FMINNUM_IEEE:\n    case ISD::FMAXNUM_IEEE:\n    case ISD::FMINIMUM:\n    case ISD::FMAXIMUM:\n      return true;\n    default: return false;\n    }\n  }\n\n  /// Return true if the node is a math/logic binary operator.\n  virtual bool isBinOp(unsigned Opcode) const {\n    // A commutative binop must be a binop.\n    if (isCommutativeBinOp(Opcode))\n      return true;\n    // These are non-commutative binops.\n    switch (Opcode) {\n    case ISD::SUB:\n    case ISD::SHL:\n    case ISD::SRL:\n    case ISD::SRA:\n    case ISD::SDIV:\n    case ISD::UDIV:\n    case ISD::SREM:\n    case ISD::UREM:\n    case ISD::FSUB:\n    case ISD::FDIV:\n    case ISD::FREM:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  /// Return true if it's free to truncate a value of type FromTy to type\n  /// ToTy. e.g. On x86 it's free to truncate a i32 value in register EAX to i16\n  /// by referencing its sub-register AX.\n  /// Targets must return false when FromTy <= ToTy.\n  virtual bool isTruncateFree(Type *FromTy, Type *ToTy) const {\n    return false;\n  }\n\n  /// Return true if a truncation from FromTy to ToTy is permitted when deciding\n  /// whether a call is in tail position. Typically this means that both results\n  /// would be assigned to the same register or stack slot, but it could mean\n  /// the target performs adequate checks of its own before proceeding with the\n  /// tail call.  Targets must return false when FromTy <= ToTy.\n  virtual bool allowTruncateForTailCall(Type *FromTy, Type *ToTy) const {\n    return false;\n  }\n\n  virtual bool isTruncateFree(EVT FromVT, EVT ToVT) const {\n    return false;\n  }\n\n  virtual bool isProfitableToHoist(Instruction *I) const { return true; }\n\n  /// Return true if the extension represented by \\p I is free.\n  /// Unlikely the is[Z|FP]ExtFree family which is based on types,\n  /// this method can use the context provided by \\p I to decide\n  /// whether or not \\p I is free.\n  /// This method extends the behavior of the is[Z|FP]ExtFree family.\n  /// In other words, if is[Z|FP]Free returns true, then this method\n  /// returns true as well. The converse is not true.\n  /// The target can perform the adequate checks by overriding isExtFreeImpl.\n  /// \\pre \\p I must be a sign, zero, or fp extension.\n  bool isExtFree(const Instruction *I) const {\n    switch (I->getOpcode()) {\n    case Instruction::FPExt:\n      if (isFPExtFree(EVT::getEVT(I->getType()),\n                      EVT::getEVT(I->getOperand(0)->getType())))\n        return true;\n      break;\n    case Instruction::ZExt:\n      if (isZExtFree(I->getOperand(0)->getType(), I->getType()))\n        return true;\n      break;\n    case Instruction::SExt:\n      break;\n    default:\n      llvm_unreachable(\"Instruction is not an extension\");\n    }\n    return isExtFreeImpl(I);\n  }\n\n  /// Return true if \\p Load and \\p Ext can form an ExtLoad.\n  /// For example, in AArch64\n  ///   %L = load i8, i8* %ptr\n  ///   %E = zext i8 %L to i32\n  /// can be lowered into one load instruction\n  ///   ldrb w0, [x0]\n  bool isExtLoad(const LoadInst *Load, const Instruction *Ext,\n                 const DataLayout &DL) const {\n    EVT VT = getValueType(DL, Ext->getType());\n    EVT LoadVT = getValueType(DL, Load->getType());\n\n    // If the load has other users and the truncate is not free, the ext\n    // probably isn't free.\n    if (!Load->hasOneUse() && (isTypeLegal(LoadVT) || !isTypeLegal(VT)) &&\n        !isTruncateFree(Ext->getType(), Load->getType()))\n      return false;\n\n    // Check whether the target supports casts folded into loads.\n    unsigned LType;\n    if (isa<ZExtInst>(Ext))\n      LType = ISD::ZEXTLOAD;\n    else {\n      assert(isa<SExtInst>(Ext) && \"Unexpected ext type!\");\n      LType = ISD::SEXTLOAD;\n    }\n\n    return isLoadExtLegal(LType, VT, LoadVT);\n  }\n\n  /// Return true if any actual instruction that defines a value of type FromTy\n  /// implicitly zero-extends the value to ToTy in the result register.\n  ///\n  /// The function should return true when it is likely that the truncate can\n  /// be freely folded with an instruction defining a value of FromTy. If\n  /// the defining instruction is unknown (because you're looking at a\n  /// function argument, PHI, etc.) then the target may require an\n  /// explicit truncate, which is not necessarily free, but this function\n  /// does not deal with those cases.\n  /// Targets must return false when FromTy >= ToTy.\n  virtual bool isZExtFree(Type *FromTy, Type *ToTy) const {\n    return false;\n  }\n\n  virtual bool isZExtFree(EVT FromTy, EVT ToTy) const {\n    return false;\n  }\n\n  /// Return true if sign-extension from FromTy to ToTy is cheaper than\n  /// zero-extension.\n  virtual bool isSExtCheaperThanZExt(EVT FromTy, EVT ToTy) const {\n    return false;\n  }\n\n  /// Return true if sinking I's operands to the same basic block as I is\n  /// profitable, e.g. because the operands can be folded into a target\n  /// instruction during instruction selection. After calling the function\n  /// \\p Ops contains the Uses to sink ordered by dominance (dominating users\n  /// come first).\n  virtual bool shouldSinkOperands(Instruction *I,\n                                  SmallVectorImpl<Use *> &Ops) const {\n    return false;\n  }\n\n  /// Return true if the target supplies and combines to a paired load\n  /// two loaded values of type LoadedType next to each other in memory.\n  /// RequiredAlignment gives the minimal alignment constraints that must be met\n  /// to be able to select this paired load.\n  ///\n  /// This information is *not* used to generate actual paired loads, but it is\n  /// used to generate a sequence of loads that is easier to combine into a\n  /// paired load.\n  /// For instance, something like this:\n  /// a = load i64* addr\n  /// b = trunc i64 a to i32\n  /// c = lshr i64 a, 32\n  /// d = trunc i64 c to i32\n  /// will be optimized into:\n  /// b = load i32* addr1\n  /// d = load i32* addr2\n  /// Where addr1 = addr2 +/- sizeof(i32).\n  ///\n  /// In other words, unless the target performs a post-isel load combining,\n  /// this information should not be provided because it will generate more\n  /// loads.\n  virtual bool hasPairedLoad(EVT /*LoadedType*/,\n                             Align & /*RequiredAlignment*/) const {\n    return false;\n  }\n\n  /// Return true if the target has a vector blend instruction.\n  virtual bool hasVectorBlend() const { return false; }\n\n  /// Get the maximum supported factor for interleaved memory accesses.\n  /// Default to be the minimum interleave factor: 2.\n  virtual unsigned getMaxSupportedInterleaveFactor() const { return 2; }\n\n  /// Lower an interleaved load to target specific intrinsics. Return\n  /// true on success.\n  ///\n  /// \\p LI is the vector load instruction.\n  /// \\p Shuffles is the shufflevector list to DE-interleave the loaded vector.\n  /// \\p Indices is the corresponding indices for each shufflevector.\n  /// \\p Factor is the interleave factor.\n  virtual bool lowerInterleavedLoad(LoadInst *LI,\n                                    ArrayRef<ShuffleVectorInst *> Shuffles,\n                                    ArrayRef<unsigned> Indices,\n                                    unsigned Factor) const {\n    return false;\n  }\n\n  /// Lower an interleaved store to target specific intrinsics. Return\n  /// true on success.\n  ///\n  /// \\p SI is the vector store instruction.\n  /// \\p SVI is the shufflevector to RE-interleave the stored vector.\n  /// \\p Factor is the interleave factor.\n  virtual bool lowerInterleavedStore(StoreInst *SI, ShuffleVectorInst *SVI,\n                                     unsigned Factor) const {\n    return false;\n  }\n\n  /// Return true if zero-extending the specific node Val to type VT2 is free\n  /// (either because it's implicitly zero-extended such as ARM ldrb / ldrh or\n  /// because it's folded such as X86 zero-extending loads).\n  virtual bool isZExtFree(SDValue Val, EVT VT2) const {\n    return isZExtFree(Val.getValueType(), VT2);\n  }\n\n  /// Return true if an fpext operation is free (for instance, because\n  /// single-precision floating-point numbers are implicitly extended to\n  /// double-precision).\n  virtual bool isFPExtFree(EVT DestVT, EVT SrcVT) const {\n    assert(SrcVT.isFloatingPoint() && DestVT.isFloatingPoint() &&\n           \"invalid fpext types\");\n    return false;\n  }\n\n  /// Return true if an fpext operation input to an \\p Opcode operation is free\n  /// (for instance, because half-precision floating-point numbers are\n  /// implicitly extended to float-precision) for an FMA instruction.\n  virtual bool isFPExtFoldable(const SelectionDAG &DAG, unsigned Opcode,\n                               EVT DestVT, EVT SrcVT) const {\n    assert(DestVT.isFloatingPoint() && SrcVT.isFloatingPoint() &&\n           \"invalid fpext types\");\n    return isFPExtFree(DestVT, SrcVT);\n  }\n\n  /// Return true if folding a vector load into ExtVal (a sign, zero, or any\n  /// extend node) is profitable.\n  virtual bool isVectorLoadExtDesirable(SDValue ExtVal) const { return false; }\n\n  /// Return true if an fneg operation is free to the point where it is never\n  /// worthwhile to replace it with a bitwise operation.\n  virtual bool isFNegFree(EVT VT) const {\n    assert(VT.isFloatingPoint());\n    return false;\n  }\n\n  /// Return true if an fabs operation is free to the point where it is never\n  /// worthwhile to replace it with a bitwise operation.\n  virtual bool isFAbsFree(EVT VT) const {\n    assert(VT.isFloatingPoint());\n    return false;\n  }\n\n  /// Return true if an FMA operation is faster than a pair of fmul and fadd\n  /// instructions. fmuladd intrinsics will be expanded to FMAs when this method\n  /// returns true, otherwise fmuladd is expanded to fmul + fadd.\n  ///\n  /// NOTE: This may be called before legalization on types for which FMAs are\n  /// not legal, but should return true if those types will eventually legalize\n  /// to types that support FMAs. After legalization, it will only be called on\n  /// types that support FMAs (via Legal or Custom actions)\n  virtual bool isFMAFasterThanFMulAndFAdd(const MachineFunction &MF,\n                                          EVT) const {\n    return false;\n  }\n\n  /// IR version\n  virtual bool isFMAFasterThanFMulAndFAdd(const Function &F, Type *) const {\n    return false;\n  }\n\n  /// Returns true if be combined with to form an ISD::FMAD. \\p N may be an\n  /// ISD::FADD, ISD::FSUB, or an ISD::FMUL which will be distributed into an\n  /// fadd/fsub.\n  virtual bool isFMADLegal(const SelectionDAG &DAG, const SDNode *N) const {\n    assert((N->getOpcode() == ISD::FADD || N->getOpcode() == ISD::FSUB ||\n            N->getOpcode() == ISD::FMUL) &&\n           \"unexpected node in FMAD forming combine\");\n    return isOperationLegal(ISD::FMAD, N->getValueType(0));\n  }\n\n  // Return true when the decision to generate FMA's (or FMS, FMLA etc) rather\n  // than FMUL and ADD is delegated to the machine combiner.\n  virtual bool generateFMAsInMachineCombiner(EVT VT,\n                                             CodeGenOpt::Level OptLevel) const {\n    return false;\n  }\n\n  /// Return true if it's profitable to narrow operations of type VT1 to\n  /// VT2. e.g. on x86, it's profitable to narrow from i32 to i8 but not from\n  /// i32 to i16.\n  virtual bool isNarrowingProfitable(EVT /*VT1*/, EVT /*VT2*/) const {\n    return false;\n  }\n\n  /// Return true if it is beneficial to convert a load of a constant to\n  /// just the constant itself.\n  /// On some targets it might be more efficient to use a combination of\n  /// arithmetic instructions to materialize the constant instead of loading it\n  /// from a constant pool.\n  virtual bool shouldConvertConstantLoadToIntImm(const APInt &Imm,\n                                                 Type *Ty) const {\n    return false;\n  }\n\n  /// Return true if EXTRACT_SUBVECTOR is cheap for extracting this result type\n  /// from this source type with this index. This is needed because\n  /// EXTRACT_SUBVECTOR usually has custom lowering that depends on the index of\n  /// the first element, and only the target knows which lowering is cheap.\n  virtual bool isExtractSubvectorCheap(EVT ResVT, EVT SrcVT,\n                                       unsigned Index) const {\n    return false;\n  }\n\n  /// Try to convert an extract element of a vector binary operation into an\n  /// extract element followed by a scalar operation.\n  virtual bool shouldScalarizeBinop(SDValue VecOp) const {\n    return false;\n  }\n\n  /// Return true if extraction of a scalar element from the given vector type\n  /// at the given index is cheap. For example, if scalar operations occur on\n  /// the same register file as vector operations, then an extract element may\n  /// be a sub-register rename rather than an actual instruction.\n  virtual bool isExtractVecEltCheap(EVT VT, unsigned Index) const {\n    return false;\n  }\n\n  /// Try to convert math with an overflow comparison into the corresponding DAG\n  /// node operation. Targets may want to override this independently of whether\n  /// the operation is legal/custom for the given type because it may obscure\n  /// matching of other patterns.\n  virtual bool shouldFormOverflowOp(unsigned Opcode, EVT VT,\n                                    bool MathUsed) const {\n    // TODO: The default logic is inherited from code in CodeGenPrepare.\n    // The opcode should not make a difference by default?\n    if (Opcode != ISD::UADDO)\n      return false;\n\n    // Allow the transform as long as we have an integer type that is not\n    // obviously illegal and unsupported and if the math result is used\n    // besides the overflow check. On some targets (e.g. SPARC), it is\n    // not profitable to form on overflow op if the math result has no\n    // concrete users.\n    if (VT.isVector())\n      return false;\n    return MathUsed && (VT.isSimple() || !isOperationExpand(Opcode, VT));\n  }\n\n  // Return true if it is profitable to use a scalar input to a BUILD_VECTOR\n  // even if the vector itself has multiple uses.\n  virtual bool aggressivelyPreferBuildVectorSources(EVT VecVT) const {\n    return false;\n  }\n\n  // Return true if CodeGenPrepare should consider splitting large offset of a\n  // GEP to make the GEP fit into the addressing mode and can be sunk into the\n  // same blocks of its users.\n  virtual bool shouldConsiderGEPOffsetSplit() const { return false; }\n\n  /// Return true if creating a shift of the type by the given\n  /// amount is not profitable.\n  virtual bool shouldAvoidTransformToShift(EVT VT, unsigned Amount) const {\n    return false;\n  }\n\n  /// Does this target require the clearing of high-order bits in a register\n  /// passed to the fp16 to fp conversion library function.\n  virtual bool shouldKeepZExtForFP16Conv() const { return false; }\n\n  //===--------------------------------------------------------------------===//\n  // Runtime Library hooks\n  //\n\n  /// Rename the default libcall routine name for the specified libcall.\n  void setLibcallName(RTLIB::Libcall Call, const char *Name) {\n    LibcallRoutineNames[Call] = Name;\n  }\n\n  /// Get the libcall routine name for the specified libcall.\n  const char *getLibcallName(RTLIB::Libcall Call) const {\n    return LibcallRoutineNames[Call];\n  }\n\n  /// Override the default CondCode to be used to test the result of the\n  /// comparison libcall against zero.\n  void setCmpLibcallCC(RTLIB::Libcall Call, ISD::CondCode CC) {\n    CmpLibcallCCs[Call] = CC;\n  }\n\n  /// Get the CondCode that's to be used to test the result of the comparison\n  /// libcall against zero.\n  ISD::CondCode getCmpLibcallCC(RTLIB::Libcall Call) const {\n    return CmpLibcallCCs[Call];\n  }\n\n  /// Set the CallingConv that should be used for the specified libcall.\n  void setLibcallCallingConv(RTLIB::Libcall Call, CallingConv::ID CC) {\n    LibcallCallingConvs[Call] = CC;\n  }\n\n  /// Get the CallingConv that should be used for the specified libcall.\n  CallingConv::ID getLibcallCallingConv(RTLIB::Libcall Call) const {\n    return LibcallCallingConvs[Call];\n  }\n\n  /// Execute target specific actions to finalize target lowering.\n  /// This is used to set extra flags in MachineFrameInformation and freezing\n  /// the set of reserved registers.\n  /// The default implementation just freezes the set of reserved registers.\n  virtual void finalizeLowering(MachineFunction &MF) const;\n\n  //===----------------------------------------------------------------------===//\n  //  GlobalISel Hooks\n  //===----------------------------------------------------------------------===//\n  /// Check whether or not \\p MI needs to be moved close to its uses.\n  virtual bool shouldLocalize(const MachineInstr &MI, const TargetTransformInfo *TTI) const;\n\n\nprivate:\n  const TargetMachine &TM;\n\n  /// Tells the code generator that the target has multiple (allocatable)\n  /// condition registers that can be used to store the results of comparisons\n  /// for use by selects and conditional branches. With multiple condition\n  /// registers, the code generator will not aggressively sink comparisons into\n  /// the blocks of their users.\n  bool HasMultipleConditionRegisters;\n\n  /// Tells the code generator that the target has BitExtract instructions.\n  /// The code generator will aggressively sink \"shift\"s into the blocks of\n  /// their users if the users will generate \"and\" instructions which can be\n  /// combined with \"shift\" to BitExtract instructions.\n  bool HasExtractBitsInsn;\n\n  /// Tells the code generator to bypass slow divide or remainder\n  /// instructions. For example, BypassSlowDivWidths[32,8] tells the code\n  /// generator to bypass 32-bit integer div/rem with an 8-bit unsigned integer\n  /// div/rem when the operands are positive and less than 256.\n  DenseMap <unsigned int, unsigned int> BypassSlowDivWidths;\n\n  /// Tells the code generator that it shouldn't generate extra flow control\n  /// instructions and should attempt to combine flow control instructions via\n  /// predication.\n  bool JumpIsExpensive;\n\n  /// Information about the contents of the high-bits in boolean values held in\n  /// a type wider than i1. See getBooleanContents.\n  BooleanContent BooleanContents;\n\n  /// Information about the contents of the high-bits in boolean values held in\n  /// a type wider than i1. See getBooleanContents.\n  BooleanContent BooleanFloatContents;\n\n  /// Information about the contents of the high-bits in boolean vector values\n  /// when the element type is wider than i1. See getBooleanContents.\n  BooleanContent BooleanVectorContents;\n\n  /// The target scheduling preference: shortest possible total cycles or lowest\n  /// register usage.\n  Sched::Preference SchedPreferenceInfo;\n\n  /// The minimum alignment that any argument on the stack needs to have.\n  Align MinStackArgumentAlignment;\n\n  /// The minimum function alignment (used when optimizing for size, and to\n  /// prevent explicitly provided alignment from leading to incorrect code).\n  Align MinFunctionAlignment;\n\n  /// The preferred function alignment (used when alignment unspecified and\n  /// optimizing for speed).\n  Align PrefFunctionAlignment;\n\n  /// The preferred loop alignment (in log2 bot in bytes).\n  Align PrefLoopAlignment;\n\n  /// Size in bits of the maximum atomics size the backend supports.\n  /// Accesses larger than this will be expanded by AtomicExpandPass.\n  unsigned MaxAtomicSizeInBitsSupported;\n\n  /// Size in bits of the minimum cmpxchg or ll/sc operation the\n  /// backend supports.\n  unsigned MinCmpXchgSizeInBits;\n\n  /// This indicates if the target supports unaligned atomic operations.\n  bool SupportsUnalignedAtomics;\n\n  /// If set to a physical register, this specifies the register that\n  /// llvm.savestack/llvm.restorestack should save and restore.\n  Register StackPointerRegisterToSaveRestore;\n\n  /// This indicates the default register class to use for each ValueType the\n  /// target supports natively.\n  const TargetRegisterClass *RegClassForVT[MVT::LAST_VALUETYPE];\n  uint16_t NumRegistersForVT[MVT::LAST_VALUETYPE];\n  MVT RegisterTypeForVT[MVT::LAST_VALUETYPE];\n\n  /// This indicates the \"representative\" register class to use for each\n  /// ValueType the target supports natively. This information is used by the\n  /// scheduler to track register pressure. By default, the representative\n  /// register class is the largest legal super-reg register class of the\n  /// register class of the specified type. e.g. On x86, i8, i16, and i32's\n  /// representative class would be GR32.\n  const TargetRegisterClass *RepRegClassForVT[MVT::LAST_VALUETYPE];\n\n  /// This indicates the \"cost\" of the \"representative\" register class for each\n  /// ValueType. The cost is used by the scheduler to approximate register\n  /// pressure.\n  uint8_t RepRegClassCostForVT[MVT::LAST_VALUETYPE];\n\n  /// For any value types we are promoting or expanding, this contains the value\n  /// type that we are changing to.  For Expanded types, this contains one step\n  /// of the expand (e.g. i64 -> i32), even if there are multiple steps required\n  /// (e.g. i64 -> i16).  For types natively supported by the system, this holds\n  /// the same type (e.g. i32 -> i32).\n  MVT TransformToType[MVT::LAST_VALUETYPE];\n\n  /// For each operation and each value type, keep a LegalizeAction that\n  /// indicates how instruction selection should deal with the operation.  Most\n  /// operations are Legal (aka, supported natively by the target), but\n  /// operations that are not should be described.  Note that operations on\n  /// non-legal value types are not described here.\n  LegalizeAction OpActions[MVT::LAST_VALUETYPE][ISD::BUILTIN_OP_END];\n\n  /// For each load extension type and each value type, keep a LegalizeAction\n  /// that indicates how instruction selection should deal with a load of a\n  /// specific value type and extension type. Uses 4-bits to store the action\n  /// for each of the 4 load ext types.\n  uint16_t LoadExtActions[MVT::LAST_VALUETYPE][MVT::LAST_VALUETYPE];\n\n  /// For each value type pair keep a LegalizeAction that indicates whether a\n  /// truncating store of a specific value type and truncating type is legal.\n  LegalizeAction TruncStoreActions[MVT::LAST_VALUETYPE][MVT::LAST_VALUETYPE];\n\n  /// For each indexed mode and each value type, keep a quad of LegalizeAction\n  /// that indicates how instruction selection should deal with the load /\n  /// store / maskedload / maskedstore.\n  ///\n  /// The first dimension is the value_type for the reference. The second\n  /// dimension represents the various modes for load store.\n  uint16_t IndexedModeActions[MVT::LAST_VALUETYPE][ISD::LAST_INDEXED_MODE];\n\n  /// For each condition code (ISD::CondCode) keep a LegalizeAction that\n  /// indicates how instruction selection should deal with the condition code.\n  ///\n  /// Because each CC action takes up 4 bits, we need to have the array size be\n  /// large enough to fit all of the value types. This can be done by rounding\n  /// up the MVT::LAST_VALUETYPE value to the next multiple of 8.\n  uint32_t CondCodeActions[ISD::SETCC_INVALID][(MVT::LAST_VALUETYPE + 7) / 8];\n\n  ValueTypeActionImpl ValueTypeActions;\n\nprivate:\n  LegalizeKind getTypeConversion(LLVMContext &Context, EVT VT) const;\n\n  /// Targets can specify ISD nodes that they would like PerformDAGCombine\n  /// callbacks for by calling setTargetDAGCombine(), which sets a bit in this\n  /// array.\n  unsigned char\n  TargetDAGCombineArray[(ISD::BUILTIN_OP_END+CHAR_BIT-1)/CHAR_BIT];\n\n  /// For operations that must be promoted to a specific type, this holds the\n  /// destination type.  This map should be sparse, so don't hold it as an\n  /// array.\n  ///\n  /// Targets add entries to this map with AddPromotedToType(..), clients access\n  /// this with getTypeToPromoteTo(..).\n  std::map<std::pair<unsigned, MVT::SimpleValueType>, MVT::SimpleValueType>\n    PromoteToType;\n\n  /// Stores the name each libcall.\n  const char *LibcallRoutineNames[RTLIB::UNKNOWN_LIBCALL + 1];\n\n  /// The ISD::CondCode that should be used to test the result of each of the\n  /// comparison libcall against zero.\n  ISD::CondCode CmpLibcallCCs[RTLIB::UNKNOWN_LIBCALL];\n\n  /// Stores the CallingConv that should be used for each libcall.\n  CallingConv::ID LibcallCallingConvs[RTLIB::UNKNOWN_LIBCALL];\n\n  /// Set default libcall names and calling conventions.\n  void InitLibcalls(const Triple &TT);\n\n  /// The bits of IndexedModeActions used to store the legalisation actions\n  /// We store the data as   | ML | MS |  L |  S | each taking 4 bits.\n  enum IndexedModeActionsBits {\n    IMAB_Store = 0,\n    IMAB_Load = 4,\n    IMAB_MaskedStore = 8,\n    IMAB_MaskedLoad = 12\n  };\n\n  void setIndexedModeAction(unsigned IdxMode, MVT VT, unsigned Shift,\n                            LegalizeAction Action) {\n    assert(VT.isValid() && IdxMode < ISD::LAST_INDEXED_MODE &&\n           (unsigned)Action < 0xf && \"Table isn't big enough!\");\n    unsigned Ty = (unsigned)VT.SimpleTy;\n    IndexedModeActions[Ty][IdxMode] &= ~(0xf << Shift);\n    IndexedModeActions[Ty][IdxMode] |= ((uint16_t)Action) << Shift;\n  }\n\n  LegalizeAction getIndexedModeAction(unsigned IdxMode, MVT VT,\n                                      unsigned Shift) const {\n    assert(IdxMode < ISD::LAST_INDEXED_MODE && VT.isValid() &&\n           \"Table isn't big enough!\");\n    unsigned Ty = (unsigned)VT.SimpleTy;\n    return (LegalizeAction)((IndexedModeActions[Ty][IdxMode] >> Shift) & 0xf);\n  }\n\nprotected:\n  /// Return true if the extension represented by \\p I is free.\n  /// \\pre \\p I is a sign, zero, or fp extension and\n  ///      is[Z|FP]ExtFree of the related types is not true.\n  virtual bool isExtFreeImpl(const Instruction *I) const { return false; }\n\n  /// Depth that GatherAllAliases should should continue looking for chain\n  /// dependencies when trying to find a more preferable chain. As an\n  /// approximation, this should be more than the number of consecutive stores\n  /// expected to be merged.\n  unsigned GatherAllAliasesMaxDepth;\n\n  /// \\brief Specify maximum number of store instructions per memset call.\n  ///\n  /// When lowering \\@llvm.memset this field specifies the maximum number of\n  /// store operations that may be substituted for the call to memset. Targets\n  /// must set this value based on the cost threshold for that target. Targets\n  /// should assume that the memset will be done using as many of the largest\n  /// store operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, storing 9 bytes on a 32-bit machine\n  /// with 16-bit alignment would result in four 2-byte stores and one 1-byte\n  /// store.  This only applies to setting a constant array of a constant size.\n  unsigned MaxStoresPerMemset;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxStoresPerMemsetOptSize;\n\n  /// \\brief Specify maximum number of store instructions per memcpy call.\n  ///\n  /// When lowering \\@llvm.memcpy this field specifies the maximum number of\n  /// store operations that may be substituted for a call to memcpy. Targets\n  /// must set this value based on the cost threshold for that target. Targets\n  /// should assume that the memcpy will be done using as many of the largest\n  /// store operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, storing 7 bytes on a 32-bit machine\n  /// with 32-bit alignment would result in one 4-byte store, a one 2-byte store\n  /// and one 1-byte store. This only applies to copying a constant array of\n  /// constant size.\n  unsigned MaxStoresPerMemcpy;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxStoresPerMemcpyOptSize;\n  /// \\brief Specify max number of store instructions to glue in inlined memcpy.\n  ///\n  /// When memcpy is inlined based on MaxStoresPerMemcpy, specify maximum number\n  /// of store instructions to keep together. This helps in pairing and\n  //  vectorization later on.\n  unsigned MaxGluedStoresPerMemcpy = 0;\n\n  /// \\brief Specify maximum number of load instructions per memcmp call.\n  ///\n  /// When lowering \\@llvm.memcmp this field specifies the maximum number of\n  /// pairs of load operations that may be substituted for a call to memcmp.\n  /// Targets must set this value based on the cost threshold for that target.\n  /// Targets should assume that the memcmp will be done using as many of the\n  /// largest load operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, loading 7 bytes on a 32-bit machine\n  /// with 32-bit alignment would result in one 4-byte load, a one 2-byte load\n  /// and one 1-byte load. This only applies to copying a constant array of\n  /// constant size.\n  unsigned MaxLoadsPerMemcmp;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxLoadsPerMemcmpOptSize;\n\n  /// \\brief Specify maximum number of store instructions per memmove call.\n  ///\n  /// When lowering \\@llvm.memmove this field specifies the maximum number of\n  /// store instructions that may be substituted for a call to memmove. Targets\n  /// must set this value based on the cost threshold for that target. Targets\n  /// should assume that the memmove will be done using as many of the largest\n  /// store operations first, followed by smaller ones, if necessary, per\n  /// alignment restrictions. For example, moving 9 bytes on a 32-bit machine\n  /// with 8-bit alignment would result in nine 1-byte stores.  This only\n  /// applies to copying a constant array of constant size.\n  unsigned MaxStoresPerMemmove;\n  /// Likewise for functions with the OptSize attribute.\n  unsigned MaxStoresPerMemmoveOptSize;\n\n  /// Tells the code generator that select is more expensive than a branch if\n  /// the branch is usually predicted right.\n  bool PredictableSelectIsExpensive;\n\n  /// \\see enableExtLdPromotion.\n  bool EnableExtLdPromotion;\n\n  /// Return true if the value types that can be represented by the specified\n  /// register class are all legal.\n  bool isLegalRC(const TargetRegisterInfo &TRI,\n                 const TargetRegisterClass &RC) const;\n\n  /// Replace/modify any TargetFrameIndex operands with a targte-dependent\n  /// sequence of memory operands that is recognized by PrologEpilogInserter.\n  MachineBasicBlock *emitPatchPoint(MachineInstr &MI,\n                                    MachineBasicBlock *MBB) const;\n\n  bool IsStrictFPEnabled;\n};\n\n/// This class defines information used to lower LLVM code to legal SelectionDAG\n/// operators that the target instruction selector can accept natively.\n///\n/// This class also defines callbacks that targets must implement to lower\n/// target-specific constructs to SelectionDAG operators.\nclass TargetLowering : public TargetLoweringBase {\npublic:\n  struct DAGCombinerInfo;\n  struct MakeLibCallOptions;\n\n  TargetLowering(const TargetLowering &) = delete;\n  TargetLowering &operator=(const TargetLowering &) = delete;\n\n  explicit TargetLowering(const TargetMachine &TM);\n\n  bool isPositionIndependent() const;\n\n  virtual bool isSDNodeSourceOfDivergence(const SDNode *N,\n                                          FunctionLoweringInfo *FLI,\n                                          LegacyDivergenceAnalysis *DA) const {\n    return false;\n  }\n\n  virtual bool isSDNodeAlwaysUniform(const SDNode * N) const {\n    return false;\n  }\n\n  /// Returns true by value, base pointer and offset pointer and addressing mode\n  /// by reference if the node's address can be legally represented as\n  /// pre-indexed load / store address.\n  virtual bool getPreIndexedAddressParts(SDNode * /*N*/, SDValue &/*Base*/,\n                                         SDValue &/*Offset*/,\n                                         ISD::MemIndexedMode &/*AM*/,\n                                         SelectionDAG &/*DAG*/) const {\n    return false;\n  }\n\n  /// Returns true by value, base pointer and offset pointer and addressing mode\n  /// by reference if this node can be combined with a load / store to form a\n  /// post-indexed load / store.\n  virtual bool getPostIndexedAddressParts(SDNode * /*N*/, SDNode * /*Op*/,\n                                          SDValue &/*Base*/,\n                                          SDValue &/*Offset*/,\n                                          ISD::MemIndexedMode &/*AM*/,\n                                          SelectionDAG &/*DAG*/) const {\n    return false;\n  }\n\n  /// Returns true if the specified base+offset is a legal indexed addressing\n  /// mode for this target. \\p MI is the load or store instruction that is being\n  /// considered for transformation.\n  virtual bool isIndexingLegal(MachineInstr &MI, Register Base, Register Offset,\n                               bool IsPre, MachineRegisterInfo &MRI) const {\n    return false;\n  }\n\n  /// Return the entry encoding for a jump table in the current function.  The\n  /// returned value is a member of the MachineJumpTableInfo::JTEntryKind enum.\n  virtual unsigned getJumpTableEncoding() const;\n\n  virtual const MCExpr *\n  LowerCustomJumpTableEntry(const MachineJumpTableInfo * /*MJTI*/,\n                            const MachineBasicBlock * /*MBB*/, unsigned /*uid*/,\n                            MCContext &/*Ctx*/) const {\n    llvm_unreachable(\"Need to implement this hook if target has custom JTIs\");\n  }\n\n  /// Returns relocation base for the given PIC jumptable.\n  virtual SDValue getPICJumpTableRelocBase(SDValue Table,\n                                           SelectionDAG &DAG) const;\n\n  /// This returns the relocation base for the given PIC jumptable, the same as\n  /// getPICJumpTableRelocBase, but as an MCExpr.\n  virtual const MCExpr *\n  getPICJumpTableRelocBaseExpr(const MachineFunction *MF,\n                               unsigned JTI, MCContext &Ctx) const;\n\n  /// Return true if folding a constant offset with the given GlobalAddress is\n  /// legal.  It is frequently not legal in PIC relocation models.\n  virtual bool isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const;\n\n  bool isInTailCallPosition(SelectionDAG &DAG, SDNode *Node,\n                            SDValue &Chain) const;\n\n  void softenSetCCOperands(SelectionDAG &DAG, EVT VT, SDValue &NewLHS,\n                           SDValue &NewRHS, ISD::CondCode &CCCode,\n                           const SDLoc &DL, const SDValue OldLHS,\n                           const SDValue OldRHS) const;\n\n  void softenSetCCOperands(SelectionDAG &DAG, EVT VT, SDValue &NewLHS,\n                           SDValue &NewRHS, ISD::CondCode &CCCode,\n                           const SDLoc &DL, const SDValue OldLHS,\n                           const SDValue OldRHS, SDValue &Chain,\n                           bool IsSignaling = false) const;\n\n  /// Returns a pair of (return value, chain).\n  /// It is an error to pass RTLIB::UNKNOWN_LIBCALL as \\p LC.\n  std::pair<SDValue, SDValue> makeLibCall(SelectionDAG &DAG, RTLIB::Libcall LC,\n                                          EVT RetVT, ArrayRef<SDValue> Ops,\n                                          MakeLibCallOptions CallOptions,\n                                          const SDLoc &dl,\n                                          SDValue Chain = SDValue()) const;\n\n  /// Check whether parameters to a call that are passed in callee saved\n  /// registers are the same as from the calling function.  This needs to be\n  /// checked for tail call eligibility.\n  bool parametersInCSRMatch(const MachineRegisterInfo &MRI,\n      const uint32_t *CallerPreservedMask,\n      const SmallVectorImpl<CCValAssign> &ArgLocs,\n      const SmallVectorImpl<SDValue> &OutVals) const;\n\n  //===--------------------------------------------------------------------===//\n  // TargetLowering Optimization Methods\n  //\n\n  /// A convenience struct that encapsulates a DAG, and two SDValues for\n  /// returning information from TargetLowering to its clients that want to\n  /// combine.\n  struct TargetLoweringOpt {\n    SelectionDAG &DAG;\n    bool LegalTys;\n    bool LegalOps;\n    SDValue Old;\n    SDValue New;\n\n    explicit TargetLoweringOpt(SelectionDAG &InDAG,\n                               bool LT, bool LO) :\n      DAG(InDAG), LegalTys(LT), LegalOps(LO) {}\n\n    bool LegalTypes() const { return LegalTys; }\n    bool LegalOperations() const { return LegalOps; }\n\n    bool CombineTo(SDValue O, SDValue N) {\n      Old = O;\n      New = N;\n      return true;\n    }\n  };\n\n  /// Determines the optimal series of memory ops to replace the memset / memcpy.\n  /// Return true if the number of memory ops is below the threshold (Limit).\n  /// It returns the types of the sequence of memory ops to perform\n  /// memset / memcpy by reference.\n  bool findOptimalMemOpLowering(std::vector<EVT> &MemOps, unsigned Limit,\n                                const MemOp &Op, unsigned DstAS, unsigned SrcAS,\n                                const AttributeList &FuncAttributes) const;\n\n  /// Check to see if the specified operand of the specified instruction is a\n  /// constant integer.  If so, check to see if there are any bits set in the\n  /// constant that are not demanded.  If so, shrink the constant and return\n  /// true.\n  bool ShrinkDemandedConstant(SDValue Op, const APInt &DemandedBits,\n                              const APInt &DemandedElts,\n                              TargetLoweringOpt &TLO) const;\n\n  /// Helper wrapper around ShrinkDemandedConstant, demanding all elements.\n  bool ShrinkDemandedConstant(SDValue Op, const APInt &DemandedBits,\n                              TargetLoweringOpt &TLO) const;\n\n  // Target hook to do target-specific const optimization, which is called by\n  // ShrinkDemandedConstant. This function should return true if the target\n  // doesn't want ShrinkDemandedConstant to further optimize the constant.\n  virtual bool targetShrinkDemandedConstant(SDValue Op,\n                                            const APInt &DemandedBits,\n                                            const APInt &DemandedElts,\n                                            TargetLoweringOpt &TLO) const {\n    return false;\n  }\n\n  /// Convert x+y to (VT)((SmallVT)x+(SmallVT)y) if the casts are free.  This\n  /// uses isZExtFree and ZERO_EXTEND for the widening cast, but it could be\n  /// generalized for targets with other types of implicit widening casts.\n  bool ShrinkDemandedOp(SDValue Op, unsigned BitWidth, const APInt &Demanded,\n                        TargetLoweringOpt &TLO) const;\n\n  /// Look at Op.  At this point, we know that only the DemandedBits bits of the\n  /// result of Op are ever used downstream.  If we can use this information to\n  /// simplify Op, create a new simplified DAG node and return true, returning\n  /// the original and new nodes in Old and New.  Otherwise, analyze the\n  /// expression and return a mask of KnownOne and KnownZero bits for the\n  /// expression (used to simplify the caller).  The KnownZero/One bits may only\n  /// be accurate for those bits in the Demanded masks.\n  /// \\p AssumeSingleUse When this parameter is true, this function will\n  ///    attempt to simplify \\p Op even if there are multiple uses.\n  ///    Callers are responsible for correctly updating the DAG based on the\n  ///    results of this function, because simply replacing replacing TLO.Old\n  ///    with TLO.New will be incorrect when this parameter is true and TLO.Old\n  ///    has multiple uses.\n  bool SimplifyDemandedBits(SDValue Op, const APInt &DemandedBits,\n                            const APInt &DemandedElts, KnownBits &Known,\n                            TargetLoweringOpt &TLO, unsigned Depth = 0,\n                            bool AssumeSingleUse = false) const;\n\n  /// Helper wrapper around SimplifyDemandedBits, demanding all elements.\n  /// Adds Op back to the worklist upon success.\n  bool SimplifyDemandedBits(SDValue Op, const APInt &DemandedBits,\n                            KnownBits &Known, TargetLoweringOpt &TLO,\n                            unsigned Depth = 0,\n                            bool AssumeSingleUse = false) const;\n\n  /// Helper wrapper around SimplifyDemandedBits.\n  /// Adds Op back to the worklist upon success.\n  bool SimplifyDemandedBits(SDValue Op, const APInt &DemandedBits,\n                            DAGCombinerInfo &DCI) const;\n\n  /// More limited version of SimplifyDemandedBits that can be used to \"look\n  /// through\" ops that don't contribute to the DemandedBits/DemandedElts -\n  /// bitwise ops etc.\n  SDValue SimplifyMultipleUseDemandedBits(SDValue Op, const APInt &DemandedBits,\n                                          const APInt &DemandedElts,\n                                          SelectionDAG &DAG,\n                                          unsigned Depth) const;\n\n  /// Helper wrapper around SimplifyMultipleUseDemandedBits, demanding all\n  /// elements.\n  SDValue SimplifyMultipleUseDemandedBits(SDValue Op, const APInt &DemandedBits,\n                                          SelectionDAG &DAG,\n                                          unsigned Depth = 0) const;\n\n  /// Helper wrapper around SimplifyMultipleUseDemandedBits, demanding all\n  /// bits from only some vector elements.\n  SDValue SimplifyMultipleUseDemandedVectorElts(SDValue Op,\n                                                const APInt &DemandedElts,\n                                                SelectionDAG &DAG,\n                                                unsigned Depth = 0) const;\n\n  /// Look at Vector Op. At this point, we know that only the DemandedElts\n  /// elements of the result of Op are ever used downstream.  If we can use\n  /// this information to simplify Op, create a new simplified DAG node and\n  /// return true, storing the original and new nodes in TLO.\n  /// Otherwise, analyze the expression and return a mask of KnownUndef and\n  /// KnownZero elements for the expression (used to simplify the caller).\n  /// The KnownUndef/Zero elements may only be accurate for those bits\n  /// in the DemandedMask.\n  /// \\p AssumeSingleUse When this parameter is true, this function will\n  ///    attempt to simplify \\p Op even if there are multiple uses.\n  ///    Callers are responsible for correctly updating the DAG based on the\n  ///    results of this function, because simply replacing replacing TLO.Old\n  ///    with TLO.New will be incorrect when this parameter is true and TLO.Old\n  ///    has multiple uses.\n  bool SimplifyDemandedVectorElts(SDValue Op, const APInt &DemandedEltMask,\n                                  APInt &KnownUndef, APInt &KnownZero,\n                                  TargetLoweringOpt &TLO, unsigned Depth = 0,\n                                  bool AssumeSingleUse = false) const;\n\n  /// Helper wrapper around SimplifyDemandedVectorElts.\n  /// Adds Op back to the worklist upon success.\n  bool SimplifyDemandedVectorElts(SDValue Op, const APInt &DemandedElts,\n                                  APInt &KnownUndef, APInt &KnownZero,\n                                  DAGCombinerInfo &DCI) const;\n\n  /// Determine which of the bits specified in Mask are known to be either zero\n  /// or one and return them in the KnownZero/KnownOne bitsets. The DemandedElts\n  /// argument allows us to only collect the known bits that are shared by the\n  /// requested vector elements.\n  virtual void computeKnownBitsForTargetNode(const SDValue Op,\n                                             KnownBits &Known,\n                                             const APInt &DemandedElts,\n                                             const SelectionDAG &DAG,\n                                             unsigned Depth = 0) const;\n\n  /// Determine which of the bits specified in Mask are known to be either zero\n  /// or one and return them in the KnownZero/KnownOne bitsets. The DemandedElts\n  /// argument allows us to only collect the known bits that are shared by the\n  /// requested vector elements. This is for GISel.\n  virtual void computeKnownBitsForTargetInstr(GISelKnownBits &Analysis,\n                                              Register R, KnownBits &Known,\n                                              const APInt &DemandedElts,\n                                              const MachineRegisterInfo &MRI,\n                                              unsigned Depth = 0) const;\n\n  /// Determine the known alignment for the pointer value \\p R. This is can\n  /// typically be inferred from the number of low known 0 bits. However, for a\n  /// pointer with a non-integral address space, the alignment value may be\n  /// independent from the known low bits.\n  virtual Align computeKnownAlignForTargetInstr(GISelKnownBits &Analysis,\n                                                Register R,\n                                                const MachineRegisterInfo &MRI,\n                                                unsigned Depth = 0) const;\n\n  /// Determine which of the bits of FrameIndex \\p FIOp are known to be 0.\n  /// Default implementation computes low bits based on alignment\n  /// information. This should preserve known bits passed into it.\n  virtual void computeKnownBitsForFrameIndex(int FIOp,\n                                             KnownBits &Known,\n                                             const MachineFunction &MF) const;\n\n  /// This method can be implemented by targets that want to expose additional\n  /// information about sign bits to the DAG Combiner. The DemandedElts\n  /// argument allows us to only collect the minimum sign bits that are shared\n  /// by the requested vector elements.\n  virtual unsigned ComputeNumSignBitsForTargetNode(SDValue Op,\n                                                   const APInt &DemandedElts,\n                                                   const SelectionDAG &DAG,\n                                                   unsigned Depth = 0) const;\n\n  /// This method can be implemented by targets that want to expose additional\n  /// information about sign bits to GlobalISel combiners. The DemandedElts\n  /// argument allows us to only collect the minimum sign bits that are shared\n  /// by the requested vector elements.\n  virtual unsigned computeNumSignBitsForTargetInstr(GISelKnownBits &Analysis,\n                                                    Register R,\n                                                    const APInt &DemandedElts,\n                                                    const MachineRegisterInfo &MRI,\n                                                    unsigned Depth = 0) const;\n\n  /// Attempt to simplify any target nodes based on the demanded vector\n  /// elements, returning true on success. Otherwise, analyze the expression and\n  /// return a mask of KnownUndef and KnownZero elements for the expression\n  /// (used to simplify the caller). The KnownUndef/Zero elements may only be\n  /// accurate for those bits in the DemandedMask.\n  virtual bool SimplifyDemandedVectorEltsForTargetNode(\n      SDValue Op, const APInt &DemandedElts, APInt &KnownUndef,\n      APInt &KnownZero, TargetLoweringOpt &TLO, unsigned Depth = 0) const;\n\n  /// Attempt to simplify any target nodes based on the demanded bits/elts,\n  /// returning true on success. Otherwise, analyze the\n  /// expression and return a mask of KnownOne and KnownZero bits for the\n  /// expression (used to simplify the caller).  The KnownZero/One bits may only\n  /// be accurate for those bits in the Demanded masks.\n  virtual bool SimplifyDemandedBitsForTargetNode(SDValue Op,\n                                                 const APInt &DemandedBits,\n                                                 const APInt &DemandedElts,\n                                                 KnownBits &Known,\n                                                 TargetLoweringOpt &TLO,\n                                                 unsigned Depth = 0) const;\n\n  /// More limited version of SimplifyDemandedBits that can be used to \"look\n  /// through\" ops that don't contribute to the DemandedBits/DemandedElts -\n  /// bitwise ops etc.\n  virtual SDValue SimplifyMultipleUseDemandedBitsForTargetNode(\n      SDValue Op, const APInt &DemandedBits, const APInt &DemandedElts,\n      SelectionDAG &DAG, unsigned Depth) const;\n\n  /// Tries to build a legal vector shuffle using the provided parameters\n  /// or equivalent variations. The Mask argument maybe be modified as the\n  /// function tries different variations.\n  /// Returns an empty SDValue if the operation fails.\n  SDValue buildLegalVectorShuffle(EVT VT, const SDLoc &DL, SDValue N0,\n                                  SDValue N1, MutableArrayRef<int> Mask,\n                                  SelectionDAG &DAG) const;\n\n  /// This method returns the constant pool value that will be loaded by LD.\n  /// NOTE: You must check for implicit extensions of the constant by LD.\n  virtual const Constant *getTargetConstantFromLoad(LoadSDNode *LD) const;\n\n  /// If \\p SNaN is false, \\returns true if \\p Op is known to never be any\n  /// NaN. If \\p sNaN is true, returns if \\p Op is known to never be a signaling\n  /// NaN.\n  virtual bool isKnownNeverNaNForTargetNode(SDValue Op,\n                                            const SelectionDAG &DAG,\n                                            bool SNaN = false,\n                                            unsigned Depth = 0) const;\n  struct DAGCombinerInfo {\n    void *DC;  // The DAG Combiner object.\n    CombineLevel Level;\n    bool CalledByLegalizer;\n\n  public:\n    SelectionDAG &DAG;\n\n    DAGCombinerInfo(SelectionDAG &dag, CombineLevel level,  bool cl, void *dc)\n      : DC(dc), Level(level), CalledByLegalizer(cl), DAG(dag) {}\n\n    bool isBeforeLegalize() const { return Level == BeforeLegalizeTypes; }\n    bool isBeforeLegalizeOps() const { return Level < AfterLegalizeVectorOps; }\n    bool isAfterLegalizeDAG() const { return Level >= AfterLegalizeDAG; }\n    CombineLevel getDAGCombineLevel() { return Level; }\n    bool isCalledByLegalizer() const { return CalledByLegalizer; }\n\n    void AddToWorklist(SDNode *N);\n    SDValue CombineTo(SDNode *N, ArrayRef<SDValue> To, bool AddTo = true);\n    SDValue CombineTo(SDNode *N, SDValue Res, bool AddTo = true);\n    SDValue CombineTo(SDNode *N, SDValue Res0, SDValue Res1, bool AddTo = true);\n\n    bool recursivelyDeleteUnusedNodes(SDNode *N);\n\n    void CommitTargetLoweringOpt(const TargetLoweringOpt &TLO);\n  };\n\n  /// Return if the N is a constant or constant vector equal to the true value\n  /// from getBooleanContents().\n  bool isConstTrueVal(const SDNode *N) const;\n\n  /// Return if the N is a constant or constant vector equal to the false value\n  /// from getBooleanContents().\n  bool isConstFalseVal(const SDNode *N) const;\n\n  /// Return if \\p N is a True value when extended to \\p VT.\n  bool isExtendedTrueVal(const ConstantSDNode *N, EVT VT, bool SExt) const;\n\n  /// Try to simplify a setcc built with the specified operands and cc. If it is\n  /// unable to simplify it, return a null SDValue.\n  SDValue SimplifySetCC(EVT VT, SDValue N0, SDValue N1, ISD::CondCode Cond,\n                        bool foldBooleans, DAGCombinerInfo &DCI,\n                        const SDLoc &dl) const;\n\n  // For targets which wrap address, unwrap for analysis.\n  virtual SDValue unwrapAddress(SDValue N) const { return N; }\n\n  /// Returns true (and the GlobalValue and the offset) if the node is a\n  /// GlobalAddress + offset.\n  virtual bool\n  isGAPlusOffset(SDNode *N, const GlobalValue* &GA, int64_t &Offset) const;\n\n  /// This method will be invoked for all target nodes and for any\n  /// target-independent nodes that the target has registered with invoke it\n  /// for.\n  ///\n  /// The semantics are as follows:\n  /// Return Value:\n  ///   SDValue.Val == 0   - No change was made\n  ///   SDValue.Val == N   - N was replaced, is dead, and is already handled.\n  ///   otherwise          - N should be replaced by the returned Operand.\n  ///\n  /// In addition, methods provided by DAGCombinerInfo may be used to perform\n  /// more complex transformations.\n  ///\n  virtual SDValue PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI) const;\n\n  /// Return true if it is profitable to move this shift by a constant amount\n  /// though its operand, adjusting any immediate operands as necessary to\n  /// preserve semantics. This transformation may not be desirable if it\n  /// disrupts a particularly auspicious target-specific tree (e.g. bitfield\n  /// extraction in AArch64). By default, it returns true.\n  ///\n  /// @param N the shift node\n  /// @param Level the current DAGCombine legalization level.\n  virtual bool isDesirableToCommuteWithShift(const SDNode *N,\n                                             CombineLevel Level) const {\n    return true;\n  }\n\n  /// Return true if the target has native support for the specified value type\n  /// and it is 'desirable' to use the type for the given node type. e.g. On x86\n  /// i16 is legal, but undesirable since i16 instruction encodings are longer\n  /// and some i16 instructions are slow.\n  virtual bool isTypeDesirableForOp(unsigned /*Opc*/, EVT VT) const {\n    // By default, assume all legal types are desirable.\n    return isTypeLegal(VT);\n  }\n\n  /// Return true if it is profitable for dag combiner to transform a floating\n  /// point op of specified opcode to a equivalent op of an integer\n  /// type. e.g. f32 load -> i32 load can be profitable on ARM.\n  virtual bool isDesirableToTransformToIntegerOp(unsigned /*Opc*/,\n                                                 EVT /*VT*/) const {\n    return false;\n  }\n\n  /// This method query the target whether it is beneficial for dag combiner to\n  /// promote the specified node. If true, it should return the desired\n  /// promotion type by reference.\n  virtual bool IsDesirableToPromoteOp(SDValue /*Op*/, EVT &/*PVT*/) const {\n    return false;\n  }\n\n  /// Return true if the target supports swifterror attribute. It optimizes\n  /// loads and stores to reading and writing a specific register.\n  virtual bool supportSwiftError() const {\n    return false;\n  }\n\n  /// Return true if the target supports that a subset of CSRs for the given\n  /// machine function is handled explicitly via copies.\n  virtual bool supportSplitCSR(MachineFunction *MF) const {\n    return false;\n  }\n\n  /// Perform necessary initialization to handle a subset of CSRs explicitly\n  /// via copies. This function is called at the beginning of instruction\n  /// selection.\n  virtual void initializeSplitCSR(MachineBasicBlock *Entry) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Insert explicit copies in entry and exit blocks. We copy a subset of\n  /// CSRs to virtual registers in the entry block, and copy them back to\n  /// physical registers in the exit blocks. This function is called at the end\n  /// of instruction selection.\n  virtual void insertCopiesSplitCSR(\n      MachineBasicBlock *Entry,\n      const SmallVectorImpl<MachineBasicBlock *> &Exits) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Return the newly negated expression if the cost is not expensive and\n  /// set the cost in \\p Cost to indicate that if it is cheaper or neutral to\n  /// do the negation.\n  virtual SDValue getNegatedExpression(SDValue Op, SelectionDAG &DAG,\n                                       bool LegalOps, bool OptForSize,\n                                       NegatibleCost &Cost,\n                                       unsigned Depth = 0) const;\n\n  /// This is the helper function to return the newly negated expression only\n  /// when the cost is cheaper.\n  SDValue getCheaperNegatedExpression(SDValue Op, SelectionDAG &DAG,\n                                      bool LegalOps, bool OptForSize,\n                                      unsigned Depth = 0) const {\n    NegatibleCost Cost = NegatibleCost::Expensive;\n    SDValue Neg =\n        getNegatedExpression(Op, DAG, LegalOps, OptForSize, Cost, Depth);\n    if (Neg && Cost == NegatibleCost::Cheaper)\n      return Neg;\n    // Remove the new created node to avoid the side effect to the DAG.\n    if (Neg && Neg.getNode()->use_empty())\n      DAG.RemoveDeadNode(Neg.getNode());\n    return SDValue();\n  }\n\n  /// This is the helper function to return the newly negated expression if\n  /// the cost is not expensive.\n  SDValue getNegatedExpression(SDValue Op, SelectionDAG &DAG, bool LegalOps,\n                               bool OptForSize, unsigned Depth = 0) const {\n    NegatibleCost Cost = NegatibleCost::Expensive;\n    return getNegatedExpression(Op, DAG, LegalOps, OptForSize, Cost, Depth);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Lowering methods - These methods must be implemented by targets so that\n  // the SelectionDAGBuilder code knows how to lower these.\n  //\n\n  /// Target-specific splitting of values into parts that fit a register\n  /// storing a legal type\n  virtual bool splitValueIntoRegisterParts(SelectionDAG &DAG, const SDLoc &DL,\n                                           SDValue Val, SDValue *Parts,\n                                           unsigned NumParts, MVT PartVT,\n                                           Optional<CallingConv::ID> CC) const {\n    return false;\n  }\n\n  /// Target-specific combining of register parts into its original value\n  virtual SDValue\n  joinRegisterPartsIntoValue(SelectionDAG &DAG, const SDLoc &DL,\n                             const SDValue *Parts, unsigned NumParts,\n                             MVT PartVT, EVT ValueVT,\n                             Optional<CallingConv::ID> CC) const {\n    return SDValue();\n  }\n\n  /// This hook must be implemented to lower the incoming (formal) arguments,\n  /// described by the Ins array, into the specified DAG. The implementation\n  /// should fill in the InVals array with legal-type argument values, and\n  /// return the resulting token chain value.\n  virtual SDValue LowerFormalArguments(\n      SDValue /*Chain*/, CallingConv::ID /*CallConv*/, bool /*isVarArg*/,\n      const SmallVectorImpl<ISD::InputArg> & /*Ins*/, const SDLoc & /*dl*/,\n      SelectionDAG & /*DAG*/, SmallVectorImpl<SDValue> & /*InVals*/) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// This structure contains all information that is necessary for lowering\n  /// calls. It is passed to TLI::LowerCallTo when the SelectionDAG builder\n  /// needs to lower a call, and targets will see this struct in their LowerCall\n  /// implementation.\n  struct CallLoweringInfo {\n    SDValue Chain;\n    Type *RetTy = nullptr;\n    bool RetSExt           : 1;\n    bool RetZExt           : 1;\n    bool IsVarArg          : 1;\n    bool IsInReg           : 1;\n    bool DoesNotReturn     : 1;\n    bool IsReturnValueUsed : 1;\n    bool IsConvergent      : 1;\n    bool IsPatchPoint      : 1;\n    bool IsPreallocated : 1;\n    bool NoMerge           : 1;\n\n    // IsTailCall should be modified by implementations of\n    // TargetLowering::LowerCall that perform tail call conversions.\n    bool IsTailCall = false;\n\n    // Is Call lowering done post SelectionDAG type legalization.\n    bool IsPostTypeLegalization = false;\n\n    unsigned NumFixedArgs = -1;\n    CallingConv::ID CallConv = CallingConv::C;\n    SDValue Callee;\n    ArgListTy Args;\n    SelectionDAG &DAG;\n    SDLoc DL;\n    const CallBase *CB = nullptr;\n    SmallVector<ISD::OutputArg, 32> Outs;\n    SmallVector<SDValue, 32> OutVals;\n    SmallVector<ISD::InputArg, 32> Ins;\n    SmallVector<SDValue, 4> InVals;\n\n    CallLoweringInfo(SelectionDAG &DAG)\n        : RetSExt(false), RetZExt(false), IsVarArg(false), IsInReg(false),\n          DoesNotReturn(false), IsReturnValueUsed(true), IsConvergent(false),\n          IsPatchPoint(false), IsPreallocated(false), NoMerge(false),\n          DAG(DAG) {}\n\n    CallLoweringInfo &setDebugLoc(const SDLoc &dl) {\n      DL = dl;\n      return *this;\n    }\n\n    CallLoweringInfo &setChain(SDValue InChain) {\n      Chain = InChain;\n      return *this;\n    }\n\n    // setCallee with target/module-specific attributes\n    CallLoweringInfo &setLibCallee(CallingConv::ID CC, Type *ResultType,\n                                   SDValue Target, ArgListTy &&ArgsList) {\n      RetTy = ResultType;\n      Callee = Target;\n      CallConv = CC;\n      NumFixedArgs = ArgsList.size();\n      Args = std::move(ArgsList);\n\n      DAG.getTargetLoweringInfo().markLibCallAttributes(\n          &(DAG.getMachineFunction()), CC, Args);\n      return *this;\n    }\n\n    CallLoweringInfo &setCallee(CallingConv::ID CC, Type *ResultType,\n                                SDValue Target, ArgListTy &&ArgsList) {\n      RetTy = ResultType;\n      Callee = Target;\n      CallConv = CC;\n      NumFixedArgs = ArgsList.size();\n      Args = std::move(ArgsList);\n      return *this;\n    }\n\n    CallLoweringInfo &setCallee(Type *ResultType, FunctionType *FTy,\n                                SDValue Target, ArgListTy &&ArgsList,\n                                const CallBase &Call) {\n      RetTy = ResultType;\n\n      IsInReg = Call.hasRetAttr(Attribute::InReg);\n      DoesNotReturn =\n          Call.doesNotReturn() ||\n          (!isa<InvokeInst>(Call) && isa<UnreachableInst>(Call.getNextNode()));\n      IsVarArg = FTy->isVarArg();\n      IsReturnValueUsed = !Call.use_empty();\n      RetSExt = Call.hasRetAttr(Attribute::SExt);\n      RetZExt = Call.hasRetAttr(Attribute::ZExt);\n      NoMerge = Call.hasFnAttr(Attribute::NoMerge);\n      \n      Callee = Target;\n\n      CallConv = Call.getCallingConv();\n      NumFixedArgs = FTy->getNumParams();\n      Args = std::move(ArgsList);\n\n      CB = &Call;\n\n      return *this;\n    }\n\n    CallLoweringInfo &setInRegister(bool Value = true) {\n      IsInReg = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setNoReturn(bool Value = true) {\n      DoesNotReturn = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setVarArg(bool Value = true) {\n      IsVarArg = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setTailCall(bool Value = true) {\n      IsTailCall = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setDiscardResult(bool Value = true) {\n      IsReturnValueUsed = !Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setConvergent(bool Value = true) {\n      IsConvergent = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setSExtResult(bool Value = true) {\n      RetSExt = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setZExtResult(bool Value = true) {\n      RetZExt = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setIsPatchPoint(bool Value = true) {\n      IsPatchPoint = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setIsPreallocated(bool Value = true) {\n      IsPreallocated = Value;\n      return *this;\n    }\n\n    CallLoweringInfo &setIsPostTypeLegalization(bool Value=true) {\n      IsPostTypeLegalization = Value;\n      return *this;\n    }\n\n    ArgListTy &getArgs() {\n      return Args;\n    }\n  };\n\n  /// This structure is used to pass arguments to makeLibCall function.\n  struct MakeLibCallOptions {\n    // By passing type list before soften to makeLibCall, the target hook\n    // shouldExtendTypeInLibCall can get the original type before soften.\n    ArrayRef<EVT> OpsVTBeforeSoften;\n    EVT RetVTBeforeSoften;\n    bool IsSExt : 1;\n    bool DoesNotReturn : 1;\n    bool IsReturnValueUsed : 1;\n    bool IsPostTypeLegalization : 1;\n    bool IsSoften : 1;\n\n    MakeLibCallOptions()\n        : IsSExt(false), DoesNotReturn(false), IsReturnValueUsed(true),\n          IsPostTypeLegalization(false), IsSoften(false) {}\n\n    MakeLibCallOptions &setSExt(bool Value = true) {\n      IsSExt = Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setNoReturn(bool Value = true) {\n      DoesNotReturn = Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setDiscardResult(bool Value = true) {\n      IsReturnValueUsed = !Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setIsPostTypeLegalization(bool Value = true) {\n      IsPostTypeLegalization = Value;\n      return *this;\n    }\n\n    MakeLibCallOptions &setTypeListBeforeSoften(ArrayRef<EVT> OpsVT, EVT RetVT,\n                                                bool Value = true) {\n      OpsVTBeforeSoften = OpsVT;\n      RetVTBeforeSoften = RetVT;\n      IsSoften = Value;\n      return *this;\n    }\n  };\n\n  /// This function lowers an abstract call to a function into an actual call.\n  /// This returns a pair of operands.  The first element is the return value\n  /// for the function (if RetTy is not VoidTy).  The second element is the\n  /// outgoing token chain. It calls LowerCall to do the actual lowering.\n  std::pair<SDValue, SDValue> LowerCallTo(CallLoweringInfo &CLI) const;\n\n  /// This hook must be implemented to lower calls into the specified\n  /// DAG. The outgoing arguments to the call are described by the Outs array,\n  /// and the values to be returned by the call are described by the Ins\n  /// array. The implementation should fill in the InVals array with legal-type\n  /// return values from the call, and return the resulting token chain value.\n  virtual SDValue\n    LowerCall(CallLoweringInfo &/*CLI*/,\n              SmallVectorImpl<SDValue> &/*InVals*/) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Target-specific cleanup for formal ByVal parameters.\n  virtual void HandleByVal(CCState *, unsigned &, Align) const {}\n\n  /// This hook should be implemented to check whether the return values\n  /// described by the Outs array can fit into the return registers.  If false\n  /// is returned, an sret-demotion is performed.\n  virtual bool CanLowerReturn(CallingConv::ID /*CallConv*/,\n                              MachineFunction &/*MF*/, bool /*isVarArg*/,\n               const SmallVectorImpl<ISD::OutputArg> &/*Outs*/,\n               LLVMContext &/*Context*/) const\n  {\n    // Return true by default to get preexisting behavior.\n    return true;\n  }\n\n  /// This hook must be implemented to lower outgoing return values, described\n  /// by the Outs array, into the specified DAG. The implementation should\n  /// return the resulting token chain value.\n  virtual SDValue LowerReturn(SDValue /*Chain*/, CallingConv::ID /*CallConv*/,\n                              bool /*isVarArg*/,\n                              const SmallVectorImpl<ISD::OutputArg> & /*Outs*/,\n                              const SmallVectorImpl<SDValue> & /*OutVals*/,\n                              const SDLoc & /*dl*/,\n                              SelectionDAG & /*DAG*/) const {\n    llvm_unreachable(\"Not Implemented\");\n  }\n\n  /// Return true if result of the specified node is used by a return node\n  /// only. It also compute and return the input chain for the tail call.\n  ///\n  /// This is used to determine whether it is possible to codegen a libcall as\n  /// tail call at legalization time.\n  virtual bool isUsedByReturnOnly(SDNode *, SDValue &/*Chain*/) const {\n    return false;\n  }\n\n  /// Return true if the target may be able emit the call instruction as a tail\n  /// call. This is used by optimization passes to determine if it's profitable\n  /// to duplicate return instructions to enable tailcall optimization.\n  virtual bool mayBeEmittedAsTailCall(const CallInst *) const {\n    return false;\n  }\n\n  /// Return the builtin name for the __builtin___clear_cache intrinsic\n  /// Default is to invoke the clear cache library call\n  virtual const char * getClearCacheBuiltinName() const {\n    return \"__clear_cache\";\n  }\n\n  /// Return the register ID of the name passed in. Used by named register\n  /// global variables extension. There is no target-independent behaviour\n  /// so the default action is to bail.\n  virtual Register getRegisterByName(const char* RegName, LLT Ty,\n                                     const MachineFunction &MF) const {\n    report_fatal_error(\"Named registers not implemented for this target\");\n  }\n\n  /// Return the type that should be used to zero or sign extend a\n  /// zeroext/signext integer return value.  FIXME: Some C calling conventions\n  /// require the return type to be promoted, but this is not true all the time,\n  /// e.g. i1/i8/i16 on x86/x86_64. It is also not necessary for non-C calling\n  /// conventions. The frontend should handle this and include all of the\n  /// necessary information.\n  virtual EVT getTypeForExtReturn(LLVMContext &Context, EVT VT,\n                                       ISD::NodeType /*ExtendKind*/) const {\n    EVT MinVT = getRegisterType(Context, MVT::i32);\n    return VT.bitsLT(MinVT) ? MinVT : VT;\n  }\n\n  /// For some targets, an LLVM struct type must be broken down into multiple\n  /// simple types, but the calling convention specifies that the entire struct\n  /// must be passed in a block of consecutive registers.\n  virtual bool\n  functionArgumentNeedsConsecutiveRegisters(Type *Ty, CallingConv::ID CallConv,\n                                            bool isVarArg) const {\n    return false;\n  }\n\n  /// For most targets, an LLVM type must be broken down into multiple\n  /// smaller types. Usually the halves are ordered according to the endianness\n  /// but for some platform that would break. So this method will default to\n  /// matching the endianness but can be overridden.\n  virtual bool\n  shouldSplitFunctionArgumentsAsLittleEndian(const DataLayout &DL) const {\n    return DL.isLittleEndian();\n  }\n\n  /// Returns a 0 terminated array of registers that can be safely used as\n  /// scratch registers.\n  virtual const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const {\n    return nullptr;\n  }\n\n  /// This callback is used to prepare for a volatile or atomic load.\n  /// It takes a chain node as input and returns the chain for the load itself.\n  ///\n  /// Having a callback like this is necessary for targets like SystemZ,\n  /// which allows a CPU to reuse the result of a previous load indefinitely,\n  /// even if a cache-coherent store is performed by another CPU.  The default\n  /// implementation does nothing.\n  virtual SDValue prepareVolatileOrAtomicLoad(SDValue Chain, const SDLoc &DL,\n                                              SelectionDAG &DAG) const {\n    return Chain;\n  }\n\n  /// Should SelectionDAG lower an atomic store of the given kind as a normal\n  /// StoreSDNode (as opposed to an AtomicSDNode)?  NOTE: The intention is to\n  /// eventually migrate all targets to the using StoreSDNodes, but porting is\n  /// being done target at a time.\n  virtual bool lowerAtomicStoreAsStoreSDNode(const StoreInst &SI) const {\n    assert(SI.isAtomic() && \"violated precondition\");\n    return false;\n  }\n\n  /// Should SelectionDAG lower an atomic load of the given kind as a normal\n  /// LoadSDNode (as opposed to an AtomicSDNode)?  NOTE: The intention is to\n  /// eventually migrate all targets to the using LoadSDNodes, but porting is\n  /// being done target at a time.\n  virtual bool lowerAtomicLoadAsLoadSDNode(const LoadInst &LI) const {\n    assert(LI.isAtomic() && \"violated precondition\");\n    return false;\n  }\n\n\n  /// This callback is invoked by the type legalizer to legalize nodes with an\n  /// illegal operand type but legal result types.  It replaces the\n  /// LowerOperation callback in the type Legalizer.  The reason we can not do\n  /// away with LowerOperation entirely is that LegalizeDAG isn't yet ready to\n  /// use this callback.\n  ///\n  /// TODO: Consider merging with ReplaceNodeResults.\n  ///\n  /// The target places new result values for the node in Results (their number\n  /// and types must exactly match those of the original return values of\n  /// the node), or leaves Results empty, which indicates that the node is not\n  /// to be custom lowered after all.\n  /// The default implementation calls LowerOperation.\n  virtual void LowerOperationWrapper(SDNode *N,\n                                     SmallVectorImpl<SDValue> &Results,\n                                     SelectionDAG &DAG) const;\n\n  /// This callback is invoked for operations that are unsupported by the\n  /// target, which are registered to use 'custom' lowering, and whose defined\n  /// values are all legal.  If the target has no operations that require custom\n  /// lowering, it need not implement this.  The default implementation of this\n  /// aborts.\n  virtual SDValue LowerOperation(SDValue Op, SelectionDAG &DAG) const;\n\n  /// This callback is invoked when a node result type is illegal for the\n  /// target, and the operation was registered to use 'custom' lowering for that\n  /// result type.  The target places new result values for the node in Results\n  /// (their number and types must exactly match those of the original return\n  /// values of the node), or leaves Results empty, which indicates that the\n  /// node is not to be custom lowered after all.\n  ///\n  /// If the target has no operations that require custom lowering, it need not\n  /// implement this.  The default implementation aborts.\n  virtual void ReplaceNodeResults(SDNode * /*N*/,\n                                  SmallVectorImpl<SDValue> &/*Results*/,\n                                  SelectionDAG &/*DAG*/) const {\n    llvm_unreachable(\"ReplaceNodeResults not implemented for this target!\");\n  }\n\n  /// This method returns the name of a target specific DAG node.\n  virtual const char *getTargetNodeName(unsigned Opcode) const;\n\n  /// This method returns a target specific FastISel object, or null if the\n  /// target does not support \"fast\" ISel.\n  virtual FastISel *createFastISel(FunctionLoweringInfo &,\n                                   const TargetLibraryInfo *) const {\n    return nullptr;\n  }\n\n  bool verifyReturnAddressArgumentIsConstant(SDValue Op,\n                                             SelectionDAG &DAG) const;\n\n  //===--------------------------------------------------------------------===//\n  // Inline Asm Support hooks\n  //\n\n  /// This hook allows the target to expand an inline asm call to be explicit\n  /// llvm code if it wants to.  This is useful for turning simple inline asms\n  /// into LLVM intrinsics, which gives the compiler more information about the\n  /// behavior of the code.\n  virtual bool ExpandInlineAsm(CallInst *) const {\n    return false;\n  }\n\n  enum ConstraintType {\n    C_Register,            // Constraint represents specific register(s).\n    C_RegisterClass,       // Constraint represents any of register(s) in class.\n    C_Memory,              // Memory constraint.\n    C_Immediate,           // Requires an immediate.\n    C_Other,               // Something else.\n    C_Unknown              // Unsupported constraint.\n  };\n\n  enum ConstraintWeight {\n    // Generic weights.\n    CW_Invalid  = -1,     // No match.\n    CW_Okay     = 0,      // Acceptable.\n    CW_Good     = 1,      // Good weight.\n    CW_Better   = 2,      // Better weight.\n    CW_Best     = 3,      // Best weight.\n\n    // Well-known weights.\n    CW_SpecificReg  = CW_Okay,    // Specific register operands.\n    CW_Register     = CW_Good,    // Register operands.\n    CW_Memory       = CW_Better,  // Memory operands.\n    CW_Constant     = CW_Best,    // Constant operand.\n    CW_Default      = CW_Okay     // Default or don't know type.\n  };\n\n  /// This contains information for each constraint that we are lowering.\n  struct AsmOperandInfo : public InlineAsm::ConstraintInfo {\n    /// This contains the actual string for the code, like \"m\".  TargetLowering\n    /// picks the 'best' code from ConstraintInfo::Codes that most closely\n    /// matches the operand.\n    std::string ConstraintCode;\n\n    /// Information about the constraint code, e.g. Register, RegisterClass,\n    /// Memory, Other, Unknown.\n    TargetLowering::ConstraintType ConstraintType = TargetLowering::C_Unknown;\n\n    /// If this is the result output operand or a clobber, this is null,\n    /// otherwise it is the incoming operand to the CallInst.  This gets\n    /// modified as the asm is processed.\n    Value *CallOperandVal = nullptr;\n\n    /// The ValueType for the operand value.\n    MVT ConstraintVT = MVT::Other;\n\n    /// Copy constructor for copying from a ConstraintInfo.\n    AsmOperandInfo(InlineAsm::ConstraintInfo Info)\n        : InlineAsm::ConstraintInfo(std::move(Info)) {}\n\n    /// Return true of this is an input operand that is a matching constraint\n    /// like \"4\".\n    bool isMatchingInputConstraint() const;\n\n    /// If this is an input matching constraint, this method returns the output\n    /// operand it matches.\n    unsigned getMatchedOperand() const;\n  };\n\n  using AsmOperandInfoVector = std::vector<AsmOperandInfo>;\n\n  /// Split up the constraint string from the inline assembly value into the\n  /// specific constraints and their prefixes, and also tie in the associated\n  /// operand values.  If this returns an empty vector, and if the constraint\n  /// string itself isn't empty, there was an error parsing.\n  virtual AsmOperandInfoVector ParseConstraints(const DataLayout &DL,\n                                                const TargetRegisterInfo *TRI,\n                                                const CallBase &Call) const;\n\n  /// Examine constraint type and operand type and determine a weight value.\n  /// The operand object must already have been set up with the operand type.\n  virtual ConstraintWeight getMultipleConstraintMatchWeight(\n      AsmOperandInfo &info, int maIndex) const;\n\n  /// Examine constraint string and operand type and determine a weight value.\n  /// The operand object must already have been set up with the operand type.\n  virtual ConstraintWeight getSingleConstraintMatchWeight(\n      AsmOperandInfo &info, const char *constraint) const;\n\n  /// Determines the constraint code and constraint type to use for the specific\n  /// AsmOperandInfo, setting OpInfo.ConstraintCode and OpInfo.ConstraintType.\n  /// If the actual operand being passed in is available, it can be passed in as\n  /// Op, otherwise an empty SDValue can be passed.\n  virtual void ComputeConstraintToUse(AsmOperandInfo &OpInfo,\n                                      SDValue Op,\n                                      SelectionDAG *DAG = nullptr) const;\n\n  /// Given a constraint, return the type of constraint it is for this target.\n  virtual ConstraintType getConstraintType(StringRef Constraint) const;\n\n  /// Given a physical register constraint (e.g.  {edx}), return the register\n  /// number and the register class for the register.\n  ///\n  /// Given a register class constraint, like 'r', if this corresponds directly\n  /// to an LLVM register class, return a register of 0 and the register class\n  /// pointer.\n  ///\n  /// This should only be used for C_Register constraints.  On error, this\n  /// returns a register number of 0 and a null register class pointer.\n  virtual std::pair<unsigned, const TargetRegisterClass *>\n  getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,\n                               StringRef Constraint, MVT VT) const;\n\n  virtual unsigned getInlineAsmMemConstraint(StringRef ConstraintCode) const {\n    if (ConstraintCode == \"m\")\n      return InlineAsm::Constraint_m;\n    return InlineAsm::Constraint_Unknown;\n  }\n\n  /// Try to replace an X constraint, which matches anything, with another that\n  /// has more specific requirements based on the type of the corresponding\n  /// operand.  This returns null if there is no replacement to make.\n  virtual const char *LowerXConstraint(EVT ConstraintVT) const;\n\n  /// Lower the specified operand into the Ops vector.  If it is invalid, don't\n  /// add anything to Ops.\n  virtual void LowerAsmOperandForConstraint(SDValue Op, std::string &Constraint,\n                                            std::vector<SDValue> &Ops,\n                                            SelectionDAG &DAG) const;\n\n  // Lower custom output constraints. If invalid, return SDValue().\n  virtual SDValue LowerAsmOutputForConstraint(SDValue &Chain, SDValue &Flag,\n                                              const SDLoc &DL,\n                                              const AsmOperandInfo &OpInfo,\n                                              SelectionDAG &DAG) const;\n\n  //===--------------------------------------------------------------------===//\n  // Div utility functions\n  //\n  SDValue BuildSDIV(SDNode *N, SelectionDAG &DAG, bool IsAfterLegalization,\n                    SmallVectorImpl<SDNode *> &Created) const;\n  SDValue BuildUDIV(SDNode *N, SelectionDAG &DAG, bool IsAfterLegalization,\n                    SmallVectorImpl<SDNode *> &Created) const;\n\n  /// Targets may override this function to provide custom SDIV lowering for\n  /// power-of-2 denominators.  If the target returns an empty SDValue, LLVM\n  /// assumes SDIV is expensive and replaces it with a series of other integer\n  /// operations.\n  virtual SDValue BuildSDIVPow2(SDNode *N, const APInt &Divisor,\n                                SelectionDAG &DAG,\n                                SmallVectorImpl<SDNode *> &Created) const;\n\n  /// Indicate whether this target prefers to combine FDIVs with the same\n  /// divisor. If the transform should never be done, return zero. If the\n  /// transform should be done, return the minimum number of divisor uses\n  /// that must exist.\n  virtual unsigned combineRepeatedFPDivisors() const {\n    return 0;\n  }\n\n  /// Hooks for building estimates in place of slower divisions and square\n  /// roots.\n\n  /// Return either a square root or its reciprocal estimate value for the input\n  /// operand.\n  /// \\p Enabled is a ReciprocalEstimate enum with value either 'Unspecified' or\n  /// 'Enabled' as set by a potential default override attribute.\n  /// If \\p RefinementSteps is 'Unspecified', the number of Newton-Raphson\n  /// refinement iterations required to generate a sufficient (though not\n  /// necessarily IEEE-754 compliant) estimate is returned in that parameter.\n  /// The boolean UseOneConstNR output is used to select a Newton-Raphson\n  /// algorithm implementation that uses either one or two constants.\n  /// The boolean Reciprocal is used to select whether the estimate is for the\n  /// square root of the input operand or the reciprocal of its square root.\n  /// A target may choose to implement its own refinement within this function.\n  /// If that's true, then return '0' as the number of RefinementSteps to avoid\n  /// any further refinement of the estimate.\n  /// An empty SDValue return means no estimate sequence can be created.\n  virtual SDValue getSqrtEstimate(SDValue Operand, SelectionDAG &DAG,\n                                  int Enabled, int &RefinementSteps,\n                                  bool &UseOneConstNR, bool Reciprocal) const {\n    return SDValue();\n  }\n\n  /// Return a reciprocal estimate value for the input operand.\n  /// \\p Enabled is a ReciprocalEstimate enum with value either 'Unspecified' or\n  /// 'Enabled' as set by a potential default override attribute.\n  /// If \\p RefinementSteps is 'Unspecified', the number of Newton-Raphson\n  /// refinement iterations required to generate a sufficient (though not\n  /// necessarily IEEE-754 compliant) estimate is returned in that parameter.\n  /// A target may choose to implement its own refinement within this function.\n  /// If that's true, then return '0' as the number of RefinementSteps to avoid\n  /// any further refinement of the estimate.\n  /// An empty SDValue return means no estimate sequence can be created.\n  virtual SDValue getRecipEstimate(SDValue Operand, SelectionDAG &DAG,\n                                   int Enabled, int &RefinementSteps) const {\n    return SDValue();\n  }\n\n  /// Return a target-dependent comparison result if the input operand is\n  /// suitable for use with a square root estimate calculation. For example, the\n  /// comparison may check if the operand is NAN, INF, zero, normal, etc. The\n  /// result should be used as the condition operand for a select or branch.\n  virtual SDValue getSqrtInputTest(SDValue Operand, SelectionDAG &DAG,\n                                   const DenormalMode &Mode) const;\n\n  /// Return a target-dependent result if the input operand is not suitable for\n  /// use with a square root estimate calculation.\n  virtual SDValue getSqrtResultForDenormInput(SDValue Operand,\n                                              SelectionDAG &DAG) const {\n    return DAG.getConstantFP(0.0, SDLoc(Operand), Operand.getValueType());\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Legalization utility functions\n  //\n\n  /// Expand a MUL or [US]MUL_LOHI of n-bit values into two or four nodes,\n  /// respectively, each computing an n/2-bit part of the result.\n  /// \\param Result A vector that will be filled with the parts of the result\n  ///        in little-endian order.\n  /// \\param LL Low bits of the LHS of the MUL.  You can use this parameter\n  ///        if you want to control how low bits are extracted from the LHS.\n  /// \\param LH High bits of the LHS of the MUL.  See LL for meaning.\n  /// \\param RL Low bits of the RHS of the MUL.  See LL for meaning\n  /// \\param RH High bits of the RHS of the MUL.  See LL for meaning.\n  /// \\returns true if the node has been expanded, false if it has not\n  bool expandMUL_LOHI(unsigned Opcode, EVT VT, const SDLoc &dl, SDValue LHS,\n                      SDValue RHS, SmallVectorImpl<SDValue> &Result, EVT HiLoVT,\n                      SelectionDAG &DAG, MulExpansionKind Kind,\n                      SDValue LL = SDValue(), SDValue LH = SDValue(),\n                      SDValue RL = SDValue(), SDValue RH = SDValue()) const;\n\n  /// Expand a MUL into two nodes.  One that computes the high bits of\n  /// the result and one that computes the low bits.\n  /// \\param HiLoVT The value type to use for the Lo and Hi nodes.\n  /// \\param LL Low bits of the LHS of the MUL.  You can use this parameter\n  ///        if you want to control how low bits are extracted from the LHS.\n  /// \\param LH High bits of the LHS of the MUL.  See LL for meaning.\n  /// \\param RL Low bits of the RHS of the MUL.  See LL for meaning\n  /// \\param RH High bits of the RHS of the MUL.  See LL for meaning.\n  /// \\returns true if the node has been expanded. false if it has not\n  bool expandMUL(SDNode *N, SDValue &Lo, SDValue &Hi, EVT HiLoVT,\n                 SelectionDAG &DAG, MulExpansionKind Kind,\n                 SDValue LL = SDValue(), SDValue LH = SDValue(),\n                 SDValue RL = SDValue(), SDValue RH = SDValue()) const;\n\n  /// Expand funnel shift.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandFunnelShift(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand rotations.\n  /// \\param N Node to expand\n  /// \\param AllowVectorOps expand vector rotate, this should only be performed\n  ///        if the legalization is happening outside of LegalizeVectorOps\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandROT(SDNode *N, bool AllowVectorOps, SDValue &Result,\n                 SelectionDAG &DAG) const;\n\n  /// Expand float(f32) to SINT(i64) conversion\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandFP_TO_SINT(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand float to UINT conversion\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\param Chain output chain after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandFP_TO_UINT(SDNode *N, SDValue &Result, SDValue &Chain,\n                        SelectionDAG &DAG) const;\n\n  /// Expand UINT(i64) to double(f64) conversion\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\param Chain output chain after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandUINT_TO_FP(SDNode *N, SDValue &Result, SDValue &Chain,\n                        SelectionDAG &DAG) const;\n\n  /// Expand fminnum/fmaxnum into fminnum_ieee/fmaxnum_ieee with quieted inputs.\n  SDValue expandFMINNUM_FMAXNUM(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Expand FP_TO_[US]INT_SAT into FP_TO_[US]INT and selects or min/max.\n  /// \\param N Node to expand\n  /// \\returns The expansion result\n  SDValue expandFP_TO_INT_SAT(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Expand CTPOP nodes. Expands vector/scalar CTPOP nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandCTPOP(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand CTLZ/CTLZ_ZERO_UNDEF nodes. Expands vector/scalar CTLZ nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandCTLZ(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand CTTZ/CTTZ_ZERO_UNDEF nodes. Expands vector/scalar CTTZ nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandCTTZ(SDNode *N, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Expand ABS nodes. Expands vector/scalar ABS nodes,\n  /// vector nodes can only succeed if all operations are legal/custom.\n  /// (ABS x) -> (XOR (ADD x, (SRA x, type_size)), (SRA x, type_size))\n  /// \\param N Node to expand\n  /// \\param Result output after conversion\n  /// \\param IsNegative indicate negated abs\n  /// \\returns True, if the expansion was successful, false otherwise\n  bool expandABS(SDNode *N, SDValue &Result, SelectionDAG &DAG,\n                 bool IsNegative = false) const;\n\n  /// Expand BSWAP nodes. Expands scalar/vector BSWAP nodes with i16/i32/i64\n  /// scalar types. Returns SDValue() if expand fails.\n  /// \\param N Node to expand\n  /// \\returns The expansion result or SDValue() if it fails.\n  SDValue expandBSWAP(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Expand BITREVERSE nodes. Expands scalar/vector BITREVERSE nodes.\n  /// Returns SDValue() if expand fails.\n  /// \\param N Node to expand\n  /// \\returns The expansion result or SDValue() if it fails.\n  SDValue expandBITREVERSE(SDNode *N, SelectionDAG &DAG) const;\n\n  /// Turn load of vector type into a load of the individual elements.\n  /// \\param LD load to expand\n  /// \\returns BUILD_VECTOR and TokenFactor nodes.\n  std::pair<SDValue, SDValue> scalarizeVectorLoad(LoadSDNode *LD,\n                                                  SelectionDAG &DAG) const;\n\n  // Turn a store of a vector type into stores of the individual elements.\n  /// \\param ST Store with a vector value type\n  /// \\returns TokenFactor of the individual store chains.\n  SDValue scalarizeVectorStore(StoreSDNode *ST, SelectionDAG &DAG) const;\n\n  /// Expands an unaligned load to 2 half-size loads for an integer, and\n  /// possibly more for vectors.\n  std::pair<SDValue, SDValue> expandUnalignedLoad(LoadSDNode *LD,\n                                                  SelectionDAG &DAG) const;\n\n  /// Expands an unaligned store to 2 half-size stores for integer values, and\n  /// possibly more for vectors.\n  SDValue expandUnalignedStore(StoreSDNode *ST, SelectionDAG &DAG) const;\n\n  /// Increments memory address \\p Addr according to the type of the value\n  /// \\p DataVT that should be stored. If the data is stored in compressed\n  /// form, the memory address should be incremented according to the number of\n  /// the stored elements. This number is equal to the number of '1's bits\n  /// in the \\p Mask.\n  /// \\p DataVT is a vector type. \\p Mask is a vector value.\n  /// \\p DataVT and \\p Mask have the same number of vector elements.\n  SDValue IncrementMemoryAddress(SDValue Addr, SDValue Mask, const SDLoc &DL,\n                                 EVT DataVT, SelectionDAG &DAG,\n                                 bool IsCompressedMemory) const;\n\n  /// Get a pointer to vector element \\p Idx located in memory for a vector of\n  /// type \\p VecVT starting at a base address of \\p VecPtr. If \\p Idx is out of\n  /// bounds the returned pointer is unspecified, but will be within the vector\n  /// bounds.\n  SDValue getVectorElementPointer(SelectionDAG &DAG, SDValue VecPtr, EVT VecVT,\n                                  SDValue Index) const;\n\n  /// Method for building the DAG expansion of ISD::[US][MIN|MAX]. This\n  /// method accepts integers as its arguments.\n  SDValue expandIntMINMAX(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US][ADD|SUB]SAT. This\n  /// method accepts integers as its arguments.\n  SDValue expandAddSubSat(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US]SHLSAT. This\n  /// method accepts integers as its arguments.\n  SDValue expandShlSat(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[U|S]MULFIX[SAT]. This\n  /// method accepts integers as its arguments.\n  SDValue expandFixedPointMul(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US]DIVFIX[SAT]. This\n  /// method accepts integers as its arguments.\n  /// Note: This method may fail if the division could not be performed\n  /// within the type. Clients must retry with a wider type if this happens.\n  SDValue expandFixedPointDiv(unsigned Opcode, const SDLoc &dl,\n                              SDValue LHS, SDValue RHS,\n                              unsigned Scale, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::U(ADD|SUB)O. Expansion\n  /// always suceeds and populates the Result and Overflow arguments.\n  void expandUADDSUBO(SDNode *Node, SDValue &Result, SDValue &Overflow,\n                      SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::S(ADD|SUB)O. Expansion\n  /// always suceeds and populates the Result and Overflow arguments.\n  void expandSADDSUBO(SDNode *Node, SDValue &Result, SDValue &Overflow,\n                      SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::[US]MULO. Returns whether\n  /// expansion was successful and populates the Result and Overflow arguments.\n  bool expandMULO(SDNode *Node, SDValue &Result, SDValue &Overflow,\n                  SelectionDAG &DAG) const;\n\n  /// Expand a VECREDUCE_* into an explicit calculation. If Count is specified,\n  /// only the first Count elements of the vector are used.\n  SDValue expandVecReduce(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Expand a VECREDUCE_SEQ_* into an explicit ordered calculation.\n  SDValue expandVecReduceSeq(SDNode *Node, SelectionDAG &DAG) const;\n\n  /// Expand an SREM or UREM using SDIV/UDIV or SDIVREM/UDIVREM, if legal.\n  /// Returns true if the expansion was successful.\n  bool expandREM(SDNode *Node, SDValue &Result, SelectionDAG &DAG) const;\n\n  /// Method for building the DAG expansion of ISD::VECTOR_SPLICE. This\n  /// method accepts vectors as its arguments.\n  SDValue expandVectorSplice(SDNode *Node, SelectionDAG &DAG) const;\n\n  //===--------------------------------------------------------------------===//\n  // Instruction Emitting Hooks\n  //\n\n  /// This method should be implemented by targets that mark instructions with\n  /// the 'usesCustomInserter' flag.  These instructions are special in various\n  /// ways, which require special support to insert.  The specified MachineInstr\n  /// is created but not inserted into any basic blocks, and this method is\n  /// called to expand it into a sequence of instructions, potentially also\n  /// creating new basic blocks and control flow.\n  /// As long as the returned basic block is different (i.e., we created a new\n  /// one), the custom inserter is free to modify the rest of \\p MBB.\n  virtual MachineBasicBlock *\n  EmitInstrWithCustomInserter(MachineInstr &MI, MachineBasicBlock *MBB) const;\n\n  /// This method should be implemented by targets that mark instructions with\n  /// the 'hasPostISelHook' flag. These instructions must be adjusted after\n  /// instruction selection by target hooks.  e.g. To fill in optional defs for\n  /// ARM 's' setting instructions.\n  virtual void AdjustInstrPostInstrSelection(MachineInstr &MI,\n                                             SDNode *Node) const;\n\n  /// If this function returns true, SelectionDAGBuilder emits a\n  /// LOAD_STACK_GUARD node when it is lowering Intrinsic::stackprotector.\n  virtual bool useLoadStackGuardNode() const {\n    return false;\n  }\n\n  virtual SDValue emitStackGuardXorFP(SelectionDAG &DAG, SDValue Val,\n                                      const SDLoc &DL) const {\n    llvm_unreachable(\"not implemented for this target\");\n  }\n\n  /// Lower TLS global address SDNode for target independent emulated TLS model.\n  virtual SDValue LowerToTLSEmulatedModel(const GlobalAddressSDNode *GA,\n                                          SelectionDAG &DAG) const;\n\n  /// Expands target specific indirect branch for the case of JumpTable\n  /// expanasion.\n  virtual SDValue expandIndirectJTBranch(const SDLoc& dl, SDValue Value, SDValue Addr,\n                                         SelectionDAG &DAG) const {\n    return DAG.getNode(ISD::BRIND, dl, MVT::Other, Value, Addr);\n  }\n\n  // seteq(x, 0) -> truncate(srl(ctlz(zext(x)), log2(#bits)))\n  // If we're comparing for equality to zero and isCtlzFast is true, expose the\n  // fact that this can be implemented as a ctlz/srl pair, so that the dag\n  // combiner can fold the new nodes.\n  SDValue lowerCmpEqZeroToCtlzSrl(SDValue Op, SelectionDAG &DAG) const;\n\n  /// Give targets the chance to reduce the number of distinct addresing modes.\n  ISD::MemIndexType getCanonicalIndexType(ISD::MemIndexType IndexType,\n                                          EVT MemVT, SDValue Offsets) const;\n\nprivate:\n  SDValue foldSetCCWithAnd(EVT VT, SDValue N0, SDValue N1, ISD::CondCode Cond,\n                           const SDLoc &DL, DAGCombinerInfo &DCI) const;\n  SDValue foldSetCCWithBinOp(EVT VT, SDValue N0, SDValue N1, ISD::CondCode Cond,\n                             const SDLoc &DL, DAGCombinerInfo &DCI) const;\n\n  SDValue optimizeSetCCOfSignedTruncationCheck(EVT SCCVT, SDValue N0,\n                                               SDValue N1, ISD::CondCode Cond,\n                                               DAGCombinerInfo &DCI,\n                                               const SDLoc &DL) const;\n\n  // (X & (C l>>/<< Y)) ==/!= 0  -->  ((X <</l>> Y) & C) ==/!= 0\n  SDValue optimizeSetCCByHoistingAndByConstFromLogicalShift(\n      EVT SCCVT, SDValue N0, SDValue N1C, ISD::CondCode Cond,\n      DAGCombinerInfo &DCI, const SDLoc &DL) const;\n\n  SDValue prepareUREMEqFold(EVT SETCCVT, SDValue REMNode,\n                            SDValue CompTargetNode, ISD::CondCode Cond,\n                            DAGCombinerInfo &DCI, const SDLoc &DL,\n                            SmallVectorImpl<SDNode *> &Created) const;\n  SDValue buildUREMEqFold(EVT SETCCVT, SDValue REMNode, SDValue CompTargetNode,\n                          ISD::CondCode Cond, DAGCombinerInfo &DCI,\n                          const SDLoc &DL) const;\n\n  SDValue prepareSREMEqFold(EVT SETCCVT, SDValue REMNode,\n                            SDValue CompTargetNode, ISD::CondCode Cond,\n                            DAGCombinerInfo &DCI, const SDLoc &DL,\n                            SmallVectorImpl<SDNode *> &Created) const;\n  SDValue buildSREMEqFold(EVT SETCCVT, SDValue REMNode, SDValue CompTargetNode,\n                          ISD::CondCode Cond, DAGCombinerInfo &DCI,\n                          const SDLoc &DL) const;\n};\n\n/// Given an LLVM IR type and return type attributes, compute the return value\n/// EVTs and flags, and optionally also the offsets, if the return value is\n/// being lowered to memory.\nvoid GetReturnInfo(CallingConv::ID CC, Type *ReturnType, AttributeList attr,\n                   SmallVectorImpl<ISD::OutputArg> &Outs,\n                   const TargetLowering &TLI, const DataLayout &DL);\n\n} // end namespace llvm\n\n#endif // LLVM_CODEGEN_TARGETLOWERING_H\n"}, "61": {"id": 61, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/ConstantFolder.h", "content": "//===- ConstantFolder.h - Constant folding helper ---------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the ConstantFolder class, a helper for IRBuilder.\n// It provides IRBuilder with a set of methods for creating constants\n// with minimal folding.  For general constant creation and folding,\n// use ConstantExpr and the routines in llvm/Analysis/ConstantFolding.h.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_CONSTANTFOLDER_H\n#define LLVM_IR_CONSTANTFOLDER_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/IRBuilderFolder.h\"\n\nnamespace llvm {\n\n/// ConstantFolder - Create constants with minimum, target independent, folding.\nclass ConstantFolder final : public IRBuilderFolder {\n  virtual void anchor();\n\npublic:\n  explicit ConstantFolder() = default;\n\n  //===--------------------------------------------------------------------===//\n  // Binary Operators\n  //===--------------------------------------------------------------------===//\n\n  Constant *CreateAdd(Constant *LHS, Constant *RHS,\n                      bool HasNUW = false, bool HasNSW = false) const override {\n    return ConstantExpr::getAdd(LHS, RHS, HasNUW, HasNSW);\n  }\n\n  Constant *CreateFAdd(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getFAdd(LHS, RHS);\n  }\n\n  Constant *CreateSub(Constant *LHS, Constant *RHS,\n                      bool HasNUW = false, bool HasNSW = false) const override {\n    return ConstantExpr::getSub(LHS, RHS, HasNUW, HasNSW);\n  }\n\n  Constant *CreateFSub(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getFSub(LHS, RHS);\n  }\n\n  Constant *CreateMul(Constant *LHS, Constant *RHS,\n                      bool HasNUW = false, bool HasNSW = false) const override {\n    return ConstantExpr::getMul(LHS, RHS, HasNUW, HasNSW);\n  }\n\n  Constant *CreateFMul(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getFMul(LHS, RHS);\n  }\n\n  Constant *CreateUDiv(Constant *LHS, Constant *RHS,\n                               bool isExact = false) const override {\n    return ConstantExpr::getUDiv(LHS, RHS, isExact);\n  }\n\n  Constant *CreateSDiv(Constant *LHS, Constant *RHS,\n                               bool isExact = false) const override {\n    return ConstantExpr::getSDiv(LHS, RHS, isExact);\n  }\n\n  Constant *CreateFDiv(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getFDiv(LHS, RHS);\n  }\n\n  Constant *CreateURem(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getURem(LHS, RHS);\n  }\n\n  Constant *CreateSRem(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getSRem(LHS, RHS);\n  }\n\n  Constant *CreateFRem(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getFRem(LHS, RHS);\n  }\n\n  Constant *CreateShl(Constant *LHS, Constant *RHS,\n                      bool HasNUW = false, bool HasNSW = false) const override {\n    return ConstantExpr::getShl(LHS, RHS, HasNUW, HasNSW);\n  }\n\n  Constant *CreateLShr(Constant *LHS, Constant *RHS,\n                       bool isExact = false) const override {\n    return ConstantExpr::getLShr(LHS, RHS, isExact);\n  }\n\n  Constant *CreateAShr(Constant *LHS, Constant *RHS,\n                       bool isExact = false) const override {\n    return ConstantExpr::getAShr(LHS, RHS, isExact);\n  }\n\n  Constant *CreateAnd(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getAnd(LHS, RHS);\n  }\n\n  Constant *CreateOr(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getOr(LHS, RHS);\n  }\n\n  Constant *CreateXor(Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::getXor(LHS, RHS);\n  }\n\n  Constant *CreateBinOp(Instruction::BinaryOps Opc,\n                        Constant *LHS, Constant *RHS) const override {\n    return ConstantExpr::get(Opc, LHS, RHS);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Unary Operators\n  //===--------------------------------------------------------------------===//\n\n  Constant *CreateNeg(Constant *C,\n                      bool HasNUW = false, bool HasNSW = false) const override {\n    return ConstantExpr::getNeg(C, HasNUW, HasNSW);\n  }\n\n  Constant *CreateFNeg(Constant *C) const override {\n    return ConstantExpr::getFNeg(C);\n  }\n\n  Constant *CreateNot(Constant *C) const override {\n    return ConstantExpr::getNot(C);\n  }\n\n  Constant *CreateUnOp(Instruction::UnaryOps Opc, Constant *C) const override {\n    return ConstantExpr::get(Opc, C);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Memory Instructions\n  //===--------------------------------------------------------------------===//\n\n  Constant *CreateGetElementPtr(Type *Ty, Constant *C,\n                                ArrayRef<Constant *> IdxList) const override {\n    return ConstantExpr::getGetElementPtr(Ty, C, IdxList);\n  }\n\n  Constant *CreateGetElementPtr(Type *Ty, Constant *C,\n                                Constant *Idx) const override {\n    // This form of the function only exists to avoid ambiguous overload\n    // warnings about whether to convert Idx to ArrayRef<Constant *> or\n    // ArrayRef<Value *>.\n    return ConstantExpr::getGetElementPtr(Ty, C, Idx);\n  }\n\n  Constant *CreateGetElementPtr(Type *Ty, Constant *C,\n                                ArrayRef<Value *> IdxList) const override {\n    return ConstantExpr::getGetElementPtr(Ty, C, IdxList);\n  }\n\n  Constant *CreateInBoundsGetElementPtr(\n      Type *Ty, Constant *C, ArrayRef<Constant *> IdxList) const override {\n    return ConstantExpr::getInBoundsGetElementPtr(Ty, C, IdxList);\n  }\n\n  Constant *CreateInBoundsGetElementPtr(Type *Ty, Constant *C,\n                                        Constant *Idx) const override {\n    // This form of the function only exists to avoid ambiguous overload\n    // warnings about whether to convert Idx to ArrayRef<Constant *> or\n    // ArrayRef<Value *>.\n    return ConstantExpr::getInBoundsGetElementPtr(Ty, C, Idx);\n  }\n\n  Constant *CreateInBoundsGetElementPtr(\n      Type *Ty, Constant *C, ArrayRef<Value *> IdxList) const override {\n    return ConstantExpr::getInBoundsGetElementPtr(Ty, C, IdxList);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Cast/Conversion Operators\n  //===--------------------------------------------------------------------===//\n\n  Constant *CreateCast(Instruction::CastOps Op, Constant *C,\n                       Type *DestTy) const override {\n    return ConstantExpr::getCast(Op, C, DestTy);\n  }\n\n  Constant *CreatePointerCast(Constant *C, Type *DestTy) const override {\n    return ConstantExpr::getPointerCast(C, DestTy);\n  }\n\n  Constant *CreatePointerBitCastOrAddrSpaceCast(Constant *C,\n                                                Type *DestTy) const override {\n    return ConstantExpr::getPointerBitCastOrAddrSpaceCast(C, DestTy);\n  }\n\n  Constant *CreateIntCast(Constant *C, Type *DestTy,\n                          bool isSigned) const override {\n    return ConstantExpr::getIntegerCast(C, DestTy, isSigned);\n  }\n\n  Constant *CreateFPCast(Constant *C, Type *DestTy) const override {\n    return ConstantExpr::getFPCast(C, DestTy);\n  }\n\n  Constant *CreateBitCast(Constant *C, Type *DestTy) const override {\n    return CreateCast(Instruction::BitCast, C, DestTy);\n  }\n\n  Constant *CreateIntToPtr(Constant *C, Type *DestTy) const override {\n    return CreateCast(Instruction::IntToPtr, C, DestTy);\n  }\n\n  Constant *CreatePtrToInt(Constant *C, Type *DestTy) const override {\n    return CreateCast(Instruction::PtrToInt, C, DestTy);\n  }\n\n  Constant *CreateZExtOrBitCast(Constant *C, Type *DestTy) const override {\n    return ConstantExpr::getZExtOrBitCast(C, DestTy);\n  }\n\n  Constant *CreateSExtOrBitCast(Constant *C, Type *DestTy) const override {\n    return ConstantExpr::getSExtOrBitCast(C, DestTy);\n  }\n\n  Constant *CreateTruncOrBitCast(Constant *C, Type *DestTy) const override {\n    return ConstantExpr::getTruncOrBitCast(C, DestTy);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Compare Instructions\n  //===--------------------------------------------------------------------===//\n\n  Constant *CreateICmp(CmpInst::Predicate P, Constant *LHS,\n                       Constant *RHS) const override {\n    return ConstantExpr::getCompare(P, LHS, RHS);\n  }\n\n  Constant *CreateFCmp(CmpInst::Predicate P, Constant *LHS,\n                       Constant *RHS) const override {\n    return ConstantExpr::getCompare(P, LHS, RHS);\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Other Instructions\n  //===--------------------------------------------------------------------===//\n\n  Constant *CreateSelect(Constant *C, Constant *True,\n                         Constant *False) const override {\n    return ConstantExpr::getSelect(C, True, False);\n  }\n\n  Constant *CreateExtractElement(Constant *Vec, Constant *Idx) const override {\n    return ConstantExpr::getExtractElement(Vec, Idx);\n  }\n\n  Constant *CreateInsertElement(Constant *Vec, Constant *NewElt,\n                                Constant *Idx) const override {\n    return ConstantExpr::getInsertElement(Vec, NewElt, Idx);\n  }\n\n  Constant *CreateShuffleVector(Constant *V1, Constant *V2,\n                                ArrayRef<int> Mask) const override {\n    return ConstantExpr::getShuffleVector(V1, V2, Mask);\n  }\n\n  Constant *CreateExtractValue(Constant *Agg,\n                               ArrayRef<unsigned> IdxList) const override {\n    return ConstantExpr::getExtractValue(Agg, IdxList);\n  }\n\n  Constant *CreateInsertValue(Constant *Agg, Constant *Val,\n                              ArrayRef<unsigned> IdxList) const override {\n    return ConstantExpr::getInsertValue(Agg, Val, IdxList);\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_CONSTANTFOLDER_H\n"}, "64": {"id": 64, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DebugInfoMetadata.h", "content": "//===- llvm/IR/DebugInfoMetadata.h - Debug info metadata --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// Declarations for metadata specific to debug info.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_DEBUGINFOMETADATA_H\n#define LLVM_IR_DEBUGINFOMETADATA_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/BitmaskEnum.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/PointerUnion.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/BinaryFormat/Dwarf.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/Metadata.h\"\n#include \"llvm/Support/Casting.h\"\n#include <cassert>\n#include <climits>\n#include <cstddef>\n#include <cstdint>\n#include <iterator>\n#include <type_traits>\n#include <vector>\n\n// Helper macros for defining get() overrides.\n#define DEFINE_MDNODE_GET_UNPACK_IMPL(...) __VA_ARGS__\n#define DEFINE_MDNODE_GET_UNPACK(ARGS) DEFINE_MDNODE_GET_UNPACK_IMPL ARGS\n#define DEFINE_MDNODE_GET_DISTINCT_TEMPORARY(CLASS, FORMAL, ARGS)              \\\n  static CLASS *getDistinct(LLVMContext &Context,                              \\\n                            DEFINE_MDNODE_GET_UNPACK(FORMAL)) {                \\\n    return getImpl(Context, DEFINE_MDNODE_GET_UNPACK(ARGS), Distinct);         \\\n  }                                                                            \\\n  static Temp##CLASS getTemporary(LLVMContext &Context,                        \\\n                                  DEFINE_MDNODE_GET_UNPACK(FORMAL)) {          \\\n    return Temp##CLASS(                                                        \\\n        getImpl(Context, DEFINE_MDNODE_GET_UNPACK(ARGS), Temporary));          \\\n  }\n#define DEFINE_MDNODE_GET(CLASS, FORMAL, ARGS)                                 \\\n  static CLASS *get(LLVMContext &Context, DEFINE_MDNODE_GET_UNPACK(FORMAL)) {  \\\n    return getImpl(Context, DEFINE_MDNODE_GET_UNPACK(ARGS), Uniqued);          \\\n  }                                                                            \\\n  static CLASS *getIfExists(LLVMContext &Context,                              \\\n                            DEFINE_MDNODE_GET_UNPACK(FORMAL)) {                \\\n    return getImpl(Context, DEFINE_MDNODE_GET_UNPACK(ARGS), Uniqued,           \\\n                   /* ShouldCreate */ false);                                  \\\n  }                                                                            \\\n  DEFINE_MDNODE_GET_DISTINCT_TEMPORARY(CLASS, FORMAL, ARGS)\n\nnamespace llvm {\n\nclass DITypeRefArray {\n  const MDTuple *N = nullptr;\n\npublic:\n  DITypeRefArray() = default;\n  DITypeRefArray(const MDTuple *N) : N(N) {}\n\n  explicit operator bool() const { return get(); }\n  explicit operator MDTuple *() const { return get(); }\n\n  MDTuple *get() const { return const_cast<MDTuple *>(N); }\n  MDTuple *operator->() const { return get(); }\n  MDTuple &operator*() const { return *get(); }\n\n  // FIXME: Fix callers and remove condition on N.\n  unsigned size() const { return N ? N->getNumOperands() : 0u; }\n  DIType *operator[](unsigned I) const {\n    return cast_or_null<DIType>(N->getOperand(I));\n  }\n\n  class iterator : std::iterator<std::input_iterator_tag, DIType *,\n                                 std::ptrdiff_t, void, DIType *> {\n    MDNode::op_iterator I = nullptr;\n\n  public:\n    iterator() = default;\n    explicit iterator(MDNode::op_iterator I) : I(I) {}\n\n    DIType *operator*() const { return cast_or_null<DIType>(*I); }\n\n    iterator &operator++() {\n      ++I;\n      return *this;\n    }\n\n    iterator operator++(int) {\n      iterator Temp(*this);\n      ++I;\n      return Temp;\n    }\n\n    bool operator==(const iterator &X) const { return I == X.I; }\n    bool operator!=(const iterator &X) const { return I != X.I; }\n  };\n\n  // FIXME: Fix callers and remove condition on N.\n  iterator begin() const { return N ? iterator(N->op_begin()) : iterator(); }\n  iterator end() const { return N ? iterator(N->op_end()) : iterator(); }\n};\n\n/// Tagged DWARF-like metadata node.\n///\n/// A metadata node with a DWARF tag (i.e., a constant named \\c DW_TAG_*,\n/// defined in llvm/BinaryFormat/Dwarf.h).  Called \\a DINode because it's\n/// potentially used for non-DWARF output.\nclass DINode : public MDNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\nprotected:\n  DINode(LLVMContext &C, unsigned ID, StorageType Storage, unsigned Tag,\n         ArrayRef<Metadata *> Ops1, ArrayRef<Metadata *> Ops2 = None)\n      : MDNode(C, ID, Storage, Ops1, Ops2) {\n    assert(Tag < 1u << 16);\n    SubclassData16 = Tag;\n  }\n  ~DINode() = default;\n\n  template <class Ty> Ty *getOperandAs(unsigned I) const {\n    return cast_or_null<Ty>(getOperand(I));\n  }\n\n  StringRef getStringOperand(unsigned I) const {\n    if (auto *S = getOperandAs<MDString>(I))\n      return S->getString();\n    return StringRef();\n  }\n\n  static MDString *getCanonicalMDString(LLVMContext &Context, StringRef S) {\n    if (S.empty())\n      return nullptr;\n    return MDString::get(Context, S);\n  }\n\n  /// Allow subclasses to mutate the tag.\n  void setTag(unsigned Tag) { SubclassData16 = Tag; }\n\npublic:\n  unsigned getTag() const { return SubclassData16; }\n\n  /// Debug info flags.\n  ///\n  /// The three accessibility flags are mutually exclusive and rolled together\n  /// in the first two bits.\n  enum DIFlags : uint32_t {\n#define HANDLE_DI_FLAG(ID, NAME) Flag##NAME = ID,\n#define DI_FLAG_LARGEST_NEEDED\n#include \"llvm/IR/DebugInfoFlags.def\"\n    FlagAccessibility = FlagPrivate | FlagProtected | FlagPublic,\n    FlagPtrToMemberRep = FlagSingleInheritance | FlagMultipleInheritance |\n                         FlagVirtualInheritance,\n    LLVM_MARK_AS_BITMASK_ENUM(FlagLargest)\n  };\n\n  static DIFlags getFlag(StringRef Flag);\n  static StringRef getFlagString(DIFlags Flag);\n\n  /// Split up a flags bitfield.\n  ///\n  /// Split \\c Flags into \\c SplitFlags, a vector of its components.  Returns\n  /// any remaining (unrecognized) bits.\n  static DIFlags splitFlags(DIFlags Flags,\n                            SmallVectorImpl<DIFlags> &SplitFlags);\n\n  static bool classof(const Metadata *MD) {\n    switch (MD->getMetadataID()) {\n    default:\n      return false;\n    case GenericDINodeKind:\n    case DISubrangeKind:\n    case DIEnumeratorKind:\n    case DIBasicTypeKind:\n    case DIStringTypeKind:\n    case DIDerivedTypeKind:\n    case DICompositeTypeKind:\n    case DISubroutineTypeKind:\n    case DIFileKind:\n    case DICompileUnitKind:\n    case DISubprogramKind:\n    case DILexicalBlockKind:\n    case DILexicalBlockFileKind:\n    case DINamespaceKind:\n    case DICommonBlockKind:\n    case DITemplateTypeParameterKind:\n    case DITemplateValueParameterKind:\n    case DIGlobalVariableKind:\n    case DILocalVariableKind:\n    case DILabelKind:\n    case DIObjCPropertyKind:\n    case DIImportedEntityKind:\n    case DIModuleKind:\n    case DIGenericSubrangeKind:\n      return true;\n    }\n  }\n};\n\n/// Generic tagged DWARF-like metadata node.\n///\n/// An un-specialized DWARF-like metadata node.  The first operand is a\n/// (possibly empty) null-separated \\a MDString header that contains arbitrary\n/// fields.  The remaining operands are \\a dwarf_operands(), and are pointers\n/// to other metadata.\nclass GenericDINode : public DINode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  GenericDINode(LLVMContext &C, StorageType Storage, unsigned Hash,\n                unsigned Tag, ArrayRef<Metadata *> Ops1,\n                ArrayRef<Metadata *> Ops2)\n      : DINode(C, GenericDINodeKind, Storage, Tag, Ops1, Ops2) {\n    setHash(Hash);\n  }\n  ~GenericDINode() { dropAllReferences(); }\n\n  void setHash(unsigned Hash) { SubclassData32 = Hash; }\n  void recalculateHash();\n\n  static GenericDINode *getImpl(LLVMContext &Context, unsigned Tag,\n                                StringRef Header, ArrayRef<Metadata *> DwarfOps,\n                                StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Tag, getCanonicalMDString(Context, Header),\n                   DwarfOps, Storage, ShouldCreate);\n  }\n\n  static GenericDINode *getImpl(LLVMContext &Context, unsigned Tag,\n                                MDString *Header, ArrayRef<Metadata *> DwarfOps,\n                                StorageType Storage, bool ShouldCreate = true);\n\n  TempGenericDINode cloneImpl() const {\n    return getTemporary(getContext(), getTag(), getHeader(),\n                        SmallVector<Metadata *, 4>(dwarf_operands()));\n  }\n\npublic:\n  unsigned getHash() const { return SubclassData32; }\n\n  DEFINE_MDNODE_GET(GenericDINode, (unsigned Tag, StringRef Header,\n                                    ArrayRef<Metadata *> DwarfOps),\n                    (Tag, Header, DwarfOps))\n  DEFINE_MDNODE_GET(GenericDINode, (unsigned Tag, MDString *Header,\n                                    ArrayRef<Metadata *> DwarfOps),\n                    (Tag, Header, DwarfOps))\n\n  /// Return a (temporary) clone of this.\n  TempGenericDINode clone() const { return cloneImpl(); }\n\n  unsigned getTag() const { return SubclassData16; }\n  StringRef getHeader() const { return getStringOperand(0); }\n  MDString *getRawHeader() const { return getOperandAs<MDString>(0); }\n\n  op_iterator dwarf_op_begin() const { return op_begin() + 1; }\n  op_iterator dwarf_op_end() const { return op_end(); }\n  op_range dwarf_operands() const {\n    return op_range(dwarf_op_begin(), dwarf_op_end());\n  }\n\n  unsigned getNumDwarfOperands() const { return getNumOperands() - 1; }\n  const MDOperand &getDwarfOperand(unsigned I) const {\n    return getOperand(I + 1);\n  }\n  void replaceDwarfOperandWith(unsigned I, Metadata *New) {\n    replaceOperandWith(I + 1, New);\n  }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == GenericDINodeKind;\n  }\n};\n\n/// Array subrange.\n///\n/// TODO: Merge into node for DW_TAG_array_type, which should have a custom\n/// type.\nclass DISubrange : public DINode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  DISubrange(LLVMContext &C, StorageType Storage, ArrayRef<Metadata *> Ops)\n      : DINode(C, DISubrangeKind, Storage, dwarf::DW_TAG_subrange_type, Ops) {}\n\n  ~DISubrange() = default;\n\n  static DISubrange *getImpl(LLVMContext &Context, int64_t Count,\n                             int64_t LowerBound, StorageType Storage,\n                             bool ShouldCreate = true);\n\n  static DISubrange *getImpl(LLVMContext &Context, Metadata *CountNode,\n                             int64_t LowerBound, StorageType Storage,\n                             bool ShouldCreate = true);\n\n  static DISubrange *getImpl(LLVMContext &Context, Metadata *CountNode,\n                             Metadata *LowerBound, Metadata *UpperBound,\n                             Metadata *Stride, StorageType Storage,\n                             bool ShouldCreate = true);\n\n  TempDISubrange cloneImpl() const {\n    return getTemporary(getContext(), getRawCountNode(), getRawLowerBound(),\n                        getRawUpperBound(), getRawStride());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DISubrange, (int64_t Count, int64_t LowerBound = 0),\n                    (Count, LowerBound))\n\n  DEFINE_MDNODE_GET(DISubrange, (Metadata *CountNode, int64_t LowerBound = 0),\n                    (CountNode, LowerBound))\n\n  DEFINE_MDNODE_GET(DISubrange,\n                    (Metadata * CountNode, Metadata *LowerBound,\n                     Metadata *UpperBound, Metadata *Stride),\n                    (CountNode, LowerBound, UpperBound, Stride))\n\n  TempDISubrange clone() const { return cloneImpl(); }\n\n  Metadata *getRawCountNode() const {\n    return getOperand(0).get();\n  }\n\n  Metadata *getRawLowerBound() const { return getOperand(1).get(); }\n\n  Metadata *getRawUpperBound() const { return getOperand(2).get(); }\n\n  Metadata *getRawStride() const { return getOperand(3).get(); }\n\n  typedef PointerUnion<ConstantInt*, DIVariable*> CountType;\n  typedef PointerUnion<ConstantInt *, DIVariable *, DIExpression *> BoundType;\n\n  CountType getCount() const;\n\n  BoundType getLowerBound() const;\n\n  BoundType getUpperBound() const;\n\n  BoundType getStride() const;\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DISubrangeKind;\n  }\n};\n\nclass DIGenericSubrange : public DINode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  DIGenericSubrange(LLVMContext &C, StorageType Storage,\n                    ArrayRef<Metadata *> Ops)\n      : DINode(C, DIGenericSubrangeKind, Storage,\n               dwarf::DW_TAG_generic_subrange, Ops) {}\n\n  ~DIGenericSubrange() = default;\n\n  static DIGenericSubrange *getImpl(LLVMContext &Context, Metadata *CountNode,\n                                    Metadata *LowerBound, Metadata *UpperBound,\n                                    Metadata *Stride, StorageType Storage,\n                                    bool ShouldCreate = true);\n\n  TempDIGenericSubrange cloneImpl() const {\n    return getTemporary(getContext(), getRawCountNode(), getRawLowerBound(),\n                        getRawUpperBound(), getRawStride());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIGenericSubrange,\n                    (Metadata * CountNode, Metadata *LowerBound,\n                     Metadata *UpperBound, Metadata *Stride),\n                    (CountNode, LowerBound, UpperBound, Stride))\n\n  TempDIGenericSubrange clone() const { return cloneImpl(); }\n\n  Metadata *getRawCountNode() const { return getOperand(0).get(); }\n  Metadata *getRawLowerBound() const { return getOperand(1).get(); }\n  Metadata *getRawUpperBound() const { return getOperand(2).get(); }\n  Metadata *getRawStride() const { return getOperand(3).get(); }\n\n  using BoundType = PointerUnion<DIVariable *, DIExpression *>;\n\n  BoundType getCount() const;\n  BoundType getLowerBound() const;\n  BoundType getUpperBound() const;\n  BoundType getStride() const;\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIGenericSubrangeKind;\n  }\n};\n\n/// Enumeration value.\n///\n/// TODO: Add a pointer to the context (DW_TAG_enumeration_type) once that no\n/// longer creates a type cycle.\nclass DIEnumerator : public DINode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  APInt Value;\n  DIEnumerator(LLVMContext &C, StorageType Storage, const APInt &Value,\n               bool IsUnsigned, ArrayRef<Metadata *> Ops)\n      : DINode(C, DIEnumeratorKind, Storage, dwarf::DW_TAG_enumerator, Ops),\n        Value(Value) {\n    SubclassData32 = IsUnsigned;\n  }\n  DIEnumerator(LLVMContext &C, StorageType Storage, int64_t Value,\n               bool IsUnsigned, ArrayRef<Metadata *> Ops)\n      : DIEnumerator(C, Storage, APInt(64, Value, !IsUnsigned), IsUnsigned,\n                     Ops) {}\n  ~DIEnumerator() = default;\n\n  static DIEnumerator *getImpl(LLVMContext &Context, const APInt &Value,\n                               bool IsUnsigned, StringRef Name,\n                               StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Value, IsUnsigned,\n                   getCanonicalMDString(Context, Name), Storage, ShouldCreate);\n  }\n  static DIEnumerator *getImpl(LLVMContext &Context, const APInt &Value,\n                               bool IsUnsigned, MDString *Name,\n                               StorageType Storage, bool ShouldCreate = true);\n\n  TempDIEnumerator cloneImpl() const {\n    return getTemporary(getContext(), getValue(), isUnsigned(), getName());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIEnumerator,\n                    (int64_t Value, bool IsUnsigned, StringRef Name),\n                    (APInt(64, Value, !IsUnsigned), IsUnsigned, Name))\n  DEFINE_MDNODE_GET(DIEnumerator,\n                    (int64_t Value, bool IsUnsigned, MDString *Name),\n                    (APInt(64, Value, !IsUnsigned), IsUnsigned, Name))\n  DEFINE_MDNODE_GET(DIEnumerator,\n                    (APInt Value, bool IsUnsigned, StringRef Name),\n                    (Value, IsUnsigned, Name))\n  DEFINE_MDNODE_GET(DIEnumerator,\n                    (APInt Value, bool IsUnsigned, MDString *Name),\n                    (Value, IsUnsigned, Name))\n\n  TempDIEnumerator clone() const { return cloneImpl(); }\n\n  const APInt &getValue() const { return Value; }\n  bool isUnsigned() const { return SubclassData32; }\n  StringRef getName() const { return getStringOperand(0); }\n\n  MDString *getRawName() const { return getOperandAs<MDString>(0); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIEnumeratorKind;\n  }\n};\n\n/// Base class for scope-like contexts.\n///\n/// Base class for lexical scopes and types (which are also declaration\n/// contexts).\n///\n/// TODO: Separate the concepts of declaration contexts and lexical scopes.\nclass DIScope : public DINode {\nprotected:\n  DIScope(LLVMContext &C, unsigned ID, StorageType Storage, unsigned Tag,\n          ArrayRef<Metadata *> Ops)\n      : DINode(C, ID, Storage, Tag, Ops) {}\n  ~DIScope() = default;\n\npublic:\n  DIFile *getFile() const { return cast_or_null<DIFile>(getRawFile()); }\n\n  inline StringRef getFilename() const;\n  inline StringRef getDirectory() const;\n  inline Optional<StringRef> getSource() const;\n\n  StringRef getName() const;\n  DIScope *getScope() const;\n\n  /// Return the raw underlying file.\n  ///\n  /// A \\a DIFile is a \\a DIScope, but it doesn't point at a separate file (it\n  /// \\em is the file).  If \\c this is an \\a DIFile, we need to return \\c this.\n  /// Otherwise, return the first operand, which is where all other subclasses\n  /// store their file pointer.\n  Metadata *getRawFile() const {\n    return isa<DIFile>(this) ? const_cast<DIScope *>(this)\n                             : static_cast<Metadata *>(getOperand(0));\n  }\n\n  static bool classof(const Metadata *MD) {\n    switch (MD->getMetadataID()) {\n    default:\n      return false;\n    case DIBasicTypeKind:\n    case DIStringTypeKind:\n    case DIDerivedTypeKind:\n    case DICompositeTypeKind:\n    case DISubroutineTypeKind:\n    case DIFileKind:\n    case DICompileUnitKind:\n    case DISubprogramKind:\n    case DILexicalBlockKind:\n    case DILexicalBlockFileKind:\n    case DINamespaceKind:\n    case DICommonBlockKind:\n    case DIModuleKind:\n      return true;\n    }\n  }\n};\n\n/// File.\n///\n/// TODO: Merge with directory/file node (including users).\n/// TODO: Canonicalize paths on creation.\nclass DIFile : public DIScope {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\npublic:\n  /// Which algorithm (e.g. MD5) a checksum was generated with.\n  ///\n  /// The encoding is explicit because it is used directly in Bitcode. The\n  /// value 0 is reserved to indicate the absence of a checksum in Bitcode.\n  enum ChecksumKind {\n    // The first variant was originally CSK_None, encoded as 0. The new\n    // internal representation removes the need for this by wrapping the\n    // ChecksumInfo in an Optional, but to preserve Bitcode compatibility the 0\n    // encoding is reserved.\n    CSK_MD5 = 1,\n    CSK_SHA1 = 2,\n    CSK_SHA256 = 3,\n    CSK_Last = CSK_SHA256 // Should be last enumeration.\n  };\n\n  /// A single checksum, represented by a \\a Kind and a \\a Value (a string).\n  template <typename T>\n  struct ChecksumInfo {\n    /// The kind of checksum which \\a Value encodes.\n    ChecksumKind Kind;\n    /// The string value of the checksum.\n    T Value;\n\n    ChecksumInfo(ChecksumKind Kind, T Value) : Kind(Kind), Value(Value) { }\n    ~ChecksumInfo() = default;\n    bool operator==(const ChecksumInfo<T> &X) const {\n      return Kind == X.Kind && Value == X.Value;\n    }\n    bool operator!=(const ChecksumInfo<T> &X) const { return !(*this == X); }\n    StringRef getKindAsString() const { return getChecksumKindAsString(Kind); }\n  };\n\nprivate:\n  Optional<ChecksumInfo<MDString *>> Checksum;\n  Optional<MDString *> Source;\n\n  DIFile(LLVMContext &C, StorageType Storage,\n         Optional<ChecksumInfo<MDString *>> CS, Optional<MDString *> Src,\n         ArrayRef<Metadata *> Ops)\n      : DIScope(C, DIFileKind, Storage, dwarf::DW_TAG_file_type, Ops),\n        Checksum(CS), Source(Src) {}\n  ~DIFile() = default;\n\n  static DIFile *getImpl(LLVMContext &Context, StringRef Filename,\n                         StringRef Directory,\n                         Optional<ChecksumInfo<StringRef>> CS,\n                         Optional<StringRef> Source,\n                         StorageType Storage, bool ShouldCreate = true) {\n    Optional<ChecksumInfo<MDString *>> MDChecksum;\n    if (CS)\n      MDChecksum.emplace(CS->Kind, getCanonicalMDString(Context, CS->Value));\n    return getImpl(Context, getCanonicalMDString(Context, Filename),\n                   getCanonicalMDString(Context, Directory), MDChecksum,\n                   Source ? Optional<MDString *>(getCanonicalMDString(Context, *Source)) : None,\n                   Storage, ShouldCreate);\n  }\n  static DIFile *getImpl(LLVMContext &Context, MDString *Filename,\n                         MDString *Directory,\n                         Optional<ChecksumInfo<MDString *>> CS,\n                         Optional<MDString *> Source, StorageType Storage,\n                         bool ShouldCreate = true);\n\n  TempDIFile cloneImpl() const {\n    return getTemporary(getContext(), getFilename(), getDirectory(),\n                        getChecksum(), getSource());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIFile, (StringRef Filename, StringRef Directory,\n                             Optional<ChecksumInfo<StringRef>> CS = None,\n                             Optional<StringRef> Source = None),\n                    (Filename, Directory, CS, Source))\n  DEFINE_MDNODE_GET(DIFile, (MDString * Filename, MDString *Directory,\n                             Optional<ChecksumInfo<MDString *>> CS = None,\n                             Optional<MDString *> Source = None),\n                    (Filename, Directory, CS, Source))\n\n  TempDIFile clone() const { return cloneImpl(); }\n\n  StringRef getFilename() const { return getStringOperand(0); }\n  StringRef getDirectory() const { return getStringOperand(1); }\n  Optional<ChecksumInfo<StringRef>> getChecksum() const {\n    Optional<ChecksumInfo<StringRef>> StringRefChecksum;\n    if (Checksum)\n      StringRefChecksum.emplace(Checksum->Kind, Checksum->Value->getString());\n    return StringRefChecksum;\n  }\n  Optional<StringRef> getSource() const {\n    return Source ? Optional<StringRef>((*Source)->getString()) : None;\n  }\n\n  MDString *getRawFilename() const { return getOperandAs<MDString>(0); }\n  MDString *getRawDirectory() const { return getOperandAs<MDString>(1); }\n  Optional<ChecksumInfo<MDString *>> getRawChecksum() const { return Checksum; }\n  Optional<MDString *> getRawSource() const { return Source; }\n\n  static StringRef getChecksumKindAsString(ChecksumKind CSKind);\n  static Optional<ChecksumKind> getChecksumKind(StringRef CSKindStr);\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIFileKind;\n  }\n};\n\nStringRef DIScope::getFilename() const {\n  if (auto *F = getFile())\n    return F->getFilename();\n  return \"\";\n}\n\nStringRef DIScope::getDirectory() const {\n  if (auto *F = getFile())\n    return F->getDirectory();\n  return \"\";\n}\n\nOptional<StringRef> DIScope::getSource() const {\n  if (auto *F = getFile())\n    return F->getSource();\n  return None;\n}\n\n/// Base class for types.\n///\n/// TODO: Remove the hardcoded name and context, since many types don't use\n/// them.\n/// TODO: Split up flags.\nclass DIType : public DIScope {\n  unsigned Line;\n  DIFlags Flags;\n  uint64_t SizeInBits;\n  uint64_t OffsetInBits;\n  uint32_t AlignInBits;\n\nprotected:\n  DIType(LLVMContext &C, unsigned ID, StorageType Storage, unsigned Tag,\n         unsigned Line, uint64_t SizeInBits, uint32_t AlignInBits,\n         uint64_t OffsetInBits, DIFlags Flags, ArrayRef<Metadata *> Ops)\n      : DIScope(C, ID, Storage, Tag, Ops) {\n    init(Line, SizeInBits, AlignInBits, OffsetInBits, Flags);\n  }\n  ~DIType() = default;\n\n  void init(unsigned Line, uint64_t SizeInBits, uint32_t AlignInBits,\n            uint64_t OffsetInBits, DIFlags Flags) {\n    this->Line = Line;\n    this->Flags = Flags;\n    this->SizeInBits = SizeInBits;\n    this->AlignInBits = AlignInBits;\n    this->OffsetInBits = OffsetInBits;\n  }\n\n  /// Change fields in place.\n  void mutate(unsigned Tag, unsigned Line, uint64_t SizeInBits,\n              uint32_t AlignInBits, uint64_t OffsetInBits, DIFlags Flags) {\n    assert(isDistinct() && \"Only distinct nodes can mutate\");\n    setTag(Tag);\n    init(Line, SizeInBits, AlignInBits, OffsetInBits, Flags);\n  }\n\npublic:\n  TempDIType clone() const {\n    return TempDIType(cast<DIType>(MDNode::clone().release()));\n  }\n\n  unsigned getLine() const { return Line; }\n  uint64_t getSizeInBits() const { return SizeInBits; }\n  uint32_t getAlignInBits() const { return AlignInBits; }\n  uint32_t getAlignInBytes() const { return getAlignInBits() / CHAR_BIT; }\n  uint64_t getOffsetInBits() const { return OffsetInBits; }\n  DIFlags getFlags() const { return Flags; }\n\n  DIScope *getScope() const { return cast_or_null<DIScope>(getRawScope()); }\n  StringRef getName() const { return getStringOperand(2); }\n\n\n  Metadata *getRawScope() const { return getOperand(1); }\n  MDString *getRawName() const { return getOperandAs<MDString>(2); }\n\n  /// Returns a new temporary DIType with updated Flags\n  TempDIType cloneWithFlags(DIFlags NewFlags) const {\n    auto NewTy = clone();\n    NewTy->Flags = NewFlags;\n    return NewTy;\n  }\n\n  bool isPrivate() const {\n    return (getFlags() & FlagAccessibility) == FlagPrivate;\n  }\n  bool isProtected() const {\n    return (getFlags() & FlagAccessibility) == FlagProtected;\n  }\n  bool isPublic() const {\n    return (getFlags() & FlagAccessibility) == FlagPublic;\n  }\n  bool isForwardDecl() const { return getFlags() & FlagFwdDecl; }\n  bool isAppleBlockExtension() const { return getFlags() & FlagAppleBlock; }\n  bool isVirtual() const { return getFlags() & FlagVirtual; }\n  bool isArtificial() const { return getFlags() & FlagArtificial; }\n  bool isObjectPointer() const { return getFlags() & FlagObjectPointer; }\n  bool isObjcClassComplete() const {\n    return getFlags() & FlagObjcClassComplete;\n  }\n  bool isVector() const { return getFlags() & FlagVector; }\n  bool isBitField() const { return getFlags() & FlagBitField; }\n  bool isStaticMember() const { return getFlags() & FlagStaticMember; }\n  bool isLValueReference() const { return getFlags() & FlagLValueReference; }\n  bool isRValueReference() const { return getFlags() & FlagRValueReference; }\n  bool isTypePassByValue() const { return getFlags() & FlagTypePassByValue; }\n  bool isTypePassByReference() const {\n    return getFlags() & FlagTypePassByReference;\n  }\n  bool isBigEndian() const { return getFlags() & FlagBigEndian; }\n  bool isLittleEndian() const { return getFlags() & FlagLittleEndian; }\n  bool getExportSymbols() const { return getFlags() & FlagExportSymbols; }\n\n  static bool classof(const Metadata *MD) {\n    switch (MD->getMetadataID()) {\n    default:\n      return false;\n    case DIBasicTypeKind:\n    case DIStringTypeKind:\n    case DIDerivedTypeKind:\n    case DICompositeTypeKind:\n    case DISubroutineTypeKind:\n      return true;\n    }\n  }\n};\n\n/// Basic type, like 'int' or 'float'.\n///\n/// TODO: Split out DW_TAG_unspecified_type.\n/// TODO: Drop unused accessors.\nclass DIBasicType : public DIType {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Encoding;\n\n  DIBasicType(LLVMContext &C, StorageType Storage, unsigned Tag,\n              uint64_t SizeInBits, uint32_t AlignInBits, unsigned Encoding,\n              DIFlags Flags, ArrayRef<Metadata *> Ops)\n      : DIType(C, DIBasicTypeKind, Storage, Tag, 0, SizeInBits, AlignInBits, 0,\n               Flags, Ops),\n        Encoding(Encoding) {}\n  ~DIBasicType() = default;\n\n  static DIBasicType *getImpl(LLVMContext &Context, unsigned Tag,\n                              StringRef Name, uint64_t SizeInBits,\n                              uint32_t AlignInBits, unsigned Encoding,\n                              DIFlags Flags, StorageType Storage,\n                              bool ShouldCreate = true) {\n    return getImpl(Context, Tag, getCanonicalMDString(Context, Name),\n                   SizeInBits, AlignInBits, Encoding, Flags, Storage,\n                   ShouldCreate);\n  }\n  static DIBasicType *getImpl(LLVMContext &Context, unsigned Tag,\n                              MDString *Name, uint64_t SizeInBits,\n                              uint32_t AlignInBits, unsigned Encoding,\n                              DIFlags Flags, StorageType Storage,\n                              bool ShouldCreate = true);\n\n  TempDIBasicType cloneImpl() const {\n    return getTemporary(getContext(), getTag(), getName(), getSizeInBits(),\n                        getAlignInBits(), getEncoding(), getFlags());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIBasicType, (unsigned Tag, StringRef Name),\n                    (Tag, Name, 0, 0, 0, FlagZero))\n  DEFINE_MDNODE_GET(DIBasicType,\n                    (unsigned Tag, StringRef Name, uint64_t SizeInBits),\n                    (Tag, Name, SizeInBits, 0, 0, FlagZero))\n  DEFINE_MDNODE_GET(DIBasicType,\n                    (unsigned Tag, MDString *Name, uint64_t SizeInBits),\n                    (Tag, Name, SizeInBits, 0, 0, FlagZero))\n  DEFINE_MDNODE_GET(DIBasicType,\n                    (unsigned Tag, StringRef Name, uint64_t SizeInBits,\n                     uint32_t AlignInBits, unsigned Encoding, DIFlags Flags),\n                    (Tag, Name, SizeInBits, AlignInBits, Encoding, Flags))\n  DEFINE_MDNODE_GET(DIBasicType,\n                    (unsigned Tag, MDString *Name, uint64_t SizeInBits,\n                     uint32_t AlignInBits, unsigned Encoding, DIFlags Flags),\n                    (Tag, Name, SizeInBits, AlignInBits, Encoding, Flags))\n\n  TempDIBasicType clone() const { return cloneImpl(); }\n\n  unsigned getEncoding() const { return Encoding; }\n\n  enum class Signedness { Signed, Unsigned };\n\n  /// Return the signedness of this type, or None if this type is neither\n  /// signed nor unsigned.\n  Optional<Signedness> getSignedness() const;\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIBasicTypeKind;\n  }\n};\n\n/// String type, Fortran CHARACTER(n)\nclass DIStringType : public DIType {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Encoding;\n\n  DIStringType(LLVMContext &C, StorageType Storage, unsigned Tag,\n               uint64_t SizeInBits, uint32_t AlignInBits, unsigned Encoding,\n               ArrayRef<Metadata *> Ops)\n      : DIType(C, DIStringTypeKind, Storage, Tag, 0, SizeInBits, AlignInBits, 0,\n               FlagZero, Ops),\n        Encoding(Encoding) {}\n  ~DIStringType() = default;\n\n  static DIStringType *getImpl(LLVMContext &Context, unsigned Tag,\n                               StringRef Name, Metadata *StringLength,\n                               Metadata *StrLenExp, uint64_t SizeInBits,\n                               uint32_t AlignInBits, unsigned Encoding,\n                               StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Tag, getCanonicalMDString(Context, Name),\n                   StringLength, StrLenExp, SizeInBits, AlignInBits, Encoding,\n                   Storage, ShouldCreate);\n  }\n  static DIStringType *getImpl(LLVMContext &Context, unsigned Tag,\n                               MDString *Name, Metadata *StringLength,\n                               Metadata *StrLenExp, uint64_t SizeInBits,\n                               uint32_t AlignInBits, unsigned Encoding,\n                               StorageType Storage, bool ShouldCreate = true);\n\n  TempDIStringType cloneImpl() const {\n    return getTemporary(getContext(), getTag(), getRawName(),\n                        getRawStringLength(), getRawStringLengthExp(),\n                        getSizeInBits(), getAlignInBits(), getEncoding());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIStringType,\n                    (unsigned Tag, StringRef Name, uint64_t SizeInBits,\n                     uint32_t AlignInBits),\n                    (Tag, Name, nullptr, nullptr, SizeInBits, AlignInBits, 0))\n  DEFINE_MDNODE_GET(DIStringType,\n                    (unsigned Tag, MDString *Name, Metadata *StringLength,\n                     Metadata *StringLengthExp, uint64_t SizeInBits,\n                     uint32_t AlignInBits, unsigned Encoding),\n                    (Tag, Name, StringLength, StringLengthExp, SizeInBits,\n                     AlignInBits, Encoding))\n  DEFINE_MDNODE_GET(DIStringType,\n                    (unsigned Tag, StringRef Name, Metadata *StringLength,\n                     Metadata *StringLengthExp, uint64_t SizeInBits,\n                     uint32_t AlignInBits, unsigned Encoding),\n                    (Tag, Name, StringLength, StringLengthExp, SizeInBits,\n                     AlignInBits, Encoding))\n\n  TempDIStringType clone() const { return cloneImpl(); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIStringTypeKind;\n  }\n\n  DIVariable *getStringLength() const {\n    return cast_or_null<DIVariable>(getRawStringLength());\n  }\n\n  DIExpression *getStringLengthExp() const {\n    return cast_or_null<DIExpression>(getRawStringLengthExp());\n  }\n\n  unsigned getEncoding() const { return Encoding; }\n\n  Metadata *getRawStringLength() const { return getOperand(3); }\n\n  Metadata *getRawStringLengthExp() const { return getOperand(4); }\n};\n\n/// Derived types.\n///\n/// This includes qualified types, pointers, references, friends, typedefs, and\n/// class members.\n///\n/// TODO: Split out members (inheritance, fields, methods, etc.).\nclass DIDerivedType : public DIType {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  /// The DWARF address space of the memory pointed to or referenced by a\n  /// pointer or reference type respectively.\n  Optional<unsigned> DWARFAddressSpace;\n\n  DIDerivedType(LLVMContext &C, StorageType Storage, unsigned Tag,\n                unsigned Line, uint64_t SizeInBits, uint32_t AlignInBits,\n                uint64_t OffsetInBits, Optional<unsigned> DWARFAddressSpace,\n                DIFlags Flags, ArrayRef<Metadata *> Ops)\n      : DIType(C, DIDerivedTypeKind, Storage, Tag, Line, SizeInBits,\n               AlignInBits, OffsetInBits, Flags, Ops),\n        DWARFAddressSpace(DWARFAddressSpace) {}\n  ~DIDerivedType() = default;\n\n  static DIDerivedType *\n  getImpl(LLVMContext &Context, unsigned Tag, StringRef Name, DIFile *File,\n          unsigned Line, DIScope *Scope, DIType *BaseType, uint64_t SizeInBits,\n          uint32_t AlignInBits, uint64_t OffsetInBits,\n          Optional<unsigned> DWARFAddressSpace, DIFlags Flags,\n          Metadata *ExtraData, StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Tag, getCanonicalMDString(Context, Name), File,\n                   Line, Scope, BaseType, SizeInBits, AlignInBits, OffsetInBits,\n                   DWARFAddressSpace, Flags, ExtraData, Storage, ShouldCreate);\n  }\n  static DIDerivedType *getImpl(LLVMContext &Context, unsigned Tag,\n                                MDString *Name, Metadata *File, unsigned Line,\n                                Metadata *Scope, Metadata *BaseType,\n                                uint64_t SizeInBits, uint32_t AlignInBits,\n                                uint64_t OffsetInBits,\n                                Optional<unsigned> DWARFAddressSpace,\n                                DIFlags Flags, Metadata *ExtraData,\n                                StorageType Storage, bool ShouldCreate = true);\n\n  TempDIDerivedType cloneImpl() const {\n    return getTemporary(getContext(), getTag(), getName(), getFile(), getLine(),\n                        getScope(), getBaseType(), getSizeInBits(),\n                        getAlignInBits(), getOffsetInBits(),\n                        getDWARFAddressSpace(), getFlags(), getExtraData());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIDerivedType,\n                    (unsigned Tag, MDString *Name, Metadata *File,\n                     unsigned Line, Metadata *Scope, Metadata *BaseType,\n                     uint64_t SizeInBits, uint32_t AlignInBits,\n                     uint64_t OffsetInBits,\n                     Optional<unsigned> DWARFAddressSpace, DIFlags Flags,\n                     Metadata *ExtraData = nullptr),\n                    (Tag, Name, File, Line, Scope, BaseType, SizeInBits,\n                     AlignInBits, OffsetInBits, DWARFAddressSpace, Flags,\n                     ExtraData))\n  DEFINE_MDNODE_GET(DIDerivedType,\n                    (unsigned Tag, StringRef Name, DIFile *File, unsigned Line,\n                     DIScope *Scope, DIType *BaseType, uint64_t SizeInBits,\n                     uint32_t AlignInBits, uint64_t OffsetInBits,\n                     Optional<unsigned> DWARFAddressSpace, DIFlags Flags,\n                     Metadata *ExtraData = nullptr),\n                    (Tag, Name, File, Line, Scope, BaseType, SizeInBits,\n                     AlignInBits, OffsetInBits, DWARFAddressSpace, Flags,\n                     ExtraData))\n\n  TempDIDerivedType clone() const { return cloneImpl(); }\n\n  /// Get the base type this is derived from.\n  DIType *getBaseType() const { return cast_or_null<DIType>(getRawBaseType()); }\n  Metadata *getRawBaseType() const { return getOperand(3); }\n\n  /// \\returns The DWARF address space of the memory pointed to or referenced by\n  /// a pointer or reference type respectively.\n  Optional<unsigned> getDWARFAddressSpace() const { return DWARFAddressSpace; }\n\n  /// Get extra data associated with this derived type.\n  ///\n  /// Class type for pointer-to-members, objective-c property node for ivars,\n  /// global constant wrapper for static members, or virtual base pointer offset\n  /// for inheritance.\n  ///\n  /// TODO: Separate out types that need this extra operand: pointer-to-member\n  /// types and member fields (static members and ivars).\n  Metadata *getExtraData() const { return getRawExtraData(); }\n  Metadata *getRawExtraData() const { return getOperand(4); }\n\n  /// Get casted version of extra data.\n  /// @{\n  DIType *getClassType() const {\n    assert(getTag() == dwarf::DW_TAG_ptr_to_member_type);\n    return cast_or_null<DIType>(getExtraData());\n  }\n\n  DIObjCProperty *getObjCProperty() const {\n    return dyn_cast_or_null<DIObjCProperty>(getExtraData());\n  }\n\n  uint32_t getVBPtrOffset() const {\n    assert(getTag() == dwarf::DW_TAG_inheritance);\n    if (auto *CM = cast_or_null<ConstantAsMetadata>(getExtraData()))\n      if (auto *CI = dyn_cast_or_null<ConstantInt>(CM->getValue()))\n        return static_cast<uint32_t>(CI->getZExtValue());\n    return 0;\n  }\n\n  Constant *getStorageOffsetInBits() const {\n    assert(getTag() == dwarf::DW_TAG_member && isBitField());\n    if (auto *C = cast_or_null<ConstantAsMetadata>(getExtraData()))\n      return C->getValue();\n    return nullptr;\n  }\n\n  Constant *getConstant() const {\n    assert(getTag() == dwarf::DW_TAG_member && isStaticMember());\n    if (auto *C = cast_or_null<ConstantAsMetadata>(getExtraData()))\n      return C->getValue();\n    return nullptr;\n  }\n  Constant *getDiscriminantValue() const {\n    assert(getTag() == dwarf::DW_TAG_member && !isStaticMember());\n    if (auto *C = cast_or_null<ConstantAsMetadata>(getExtraData()))\n      return C->getValue();\n    return nullptr;\n  }\n  /// @}\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIDerivedTypeKind;\n  }\n};\n\n/// Composite types.\n///\n/// TODO: Detach from DerivedTypeBase (split out MDEnumType?).\n/// TODO: Create a custom, unrelated node for DW_TAG_array_type.\nclass DICompositeType : public DIType {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned RuntimeLang;\n\n  DICompositeType(LLVMContext &C, StorageType Storage, unsigned Tag,\n                  unsigned Line, unsigned RuntimeLang, uint64_t SizeInBits,\n                  uint32_t AlignInBits, uint64_t OffsetInBits, DIFlags Flags,\n                  ArrayRef<Metadata *> Ops)\n      : DIType(C, DICompositeTypeKind, Storage, Tag, Line, SizeInBits,\n               AlignInBits, OffsetInBits, Flags, Ops),\n        RuntimeLang(RuntimeLang) {}\n  ~DICompositeType() = default;\n\n  /// Change fields in place.\n  void mutate(unsigned Tag, unsigned Line, unsigned RuntimeLang,\n              uint64_t SizeInBits, uint32_t AlignInBits,\n              uint64_t OffsetInBits, DIFlags Flags) {\n    assert(isDistinct() && \"Only distinct nodes can mutate\");\n    assert(getRawIdentifier() && \"Only ODR-uniqued nodes should mutate\");\n    this->RuntimeLang = RuntimeLang;\n    DIType::mutate(Tag, Line, SizeInBits, AlignInBits, OffsetInBits, Flags);\n  }\n\n  static DICompositeType *\n  getImpl(LLVMContext &Context, unsigned Tag, StringRef Name, Metadata *File,\n          unsigned Line, DIScope *Scope, DIType *BaseType, uint64_t SizeInBits,\n          uint32_t AlignInBits, uint64_t OffsetInBits, DIFlags Flags,\n          DINodeArray Elements, unsigned RuntimeLang, DIType *VTableHolder,\n          DITemplateParameterArray TemplateParams, StringRef Identifier,\n          DIDerivedType *Discriminator, Metadata *DataLocation,\n          Metadata *Associated, Metadata *Allocated, Metadata *Rank,\n          StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(\n        Context, Tag, getCanonicalMDString(Context, Name), File, Line, Scope,\n        BaseType, SizeInBits, AlignInBits, OffsetInBits, Flags, Elements.get(),\n        RuntimeLang, VTableHolder, TemplateParams.get(),\n        getCanonicalMDString(Context, Identifier), Discriminator, DataLocation,\n        Associated, Allocated, Rank, Storage, ShouldCreate);\n  }\n  static DICompositeType *\n  getImpl(LLVMContext &Context, unsigned Tag, MDString *Name, Metadata *File,\n          unsigned Line, Metadata *Scope, Metadata *BaseType,\n          uint64_t SizeInBits, uint32_t AlignInBits, uint64_t OffsetInBits,\n          DIFlags Flags, Metadata *Elements, unsigned RuntimeLang,\n          Metadata *VTableHolder, Metadata *TemplateParams,\n          MDString *Identifier, Metadata *Discriminator, Metadata *DataLocation,\n          Metadata *Associated, Metadata *Allocated, Metadata *Rank,\n          StorageType Storage, bool ShouldCreate = true);\n\n  TempDICompositeType cloneImpl() const {\n    return getTemporary(getContext(), getTag(), getName(), getFile(), getLine(),\n                        getScope(), getBaseType(), getSizeInBits(),\n                        getAlignInBits(), getOffsetInBits(), getFlags(),\n                        getElements(), getRuntimeLang(), getVTableHolder(),\n                        getTemplateParams(), getIdentifier(),\n                        getDiscriminator(), getRawDataLocation(),\n                        getRawAssociated(), getRawAllocated(), getRawRank());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(\n      DICompositeType,\n      (unsigned Tag, StringRef Name, DIFile *File, unsigned Line,\n       DIScope *Scope, DIType *BaseType, uint64_t SizeInBits,\n       uint32_t AlignInBits, uint64_t OffsetInBits, DIFlags Flags,\n       DINodeArray Elements, unsigned RuntimeLang, DIType *VTableHolder,\n       DITemplateParameterArray TemplateParams = nullptr,\n       StringRef Identifier = \"\", DIDerivedType *Discriminator = nullptr,\n       Metadata *DataLocation = nullptr, Metadata *Associated = nullptr,\n       Metadata *Allocated = nullptr, Metadata *Rank = nullptr),\n      (Tag, Name, File, Line, Scope, BaseType, SizeInBits, AlignInBits,\n       OffsetInBits, Flags, Elements, RuntimeLang, VTableHolder, TemplateParams,\n       Identifier, Discriminator, DataLocation, Associated, Allocated, Rank))\n  DEFINE_MDNODE_GET(\n      DICompositeType,\n      (unsigned Tag, MDString *Name, Metadata *File, unsigned Line,\n       Metadata *Scope, Metadata *BaseType, uint64_t SizeInBits,\n       uint32_t AlignInBits, uint64_t OffsetInBits, DIFlags Flags,\n       Metadata *Elements, unsigned RuntimeLang, Metadata *VTableHolder,\n       Metadata *TemplateParams = nullptr, MDString *Identifier = nullptr,\n       Metadata *Discriminator = nullptr, Metadata *DataLocation = nullptr,\n       Metadata *Associated = nullptr, Metadata *Allocated = nullptr,\n       Metadata *Rank = nullptr),\n      (Tag, Name, File, Line, Scope, BaseType, SizeInBits, AlignInBits,\n       OffsetInBits, Flags, Elements, RuntimeLang, VTableHolder, TemplateParams,\n       Identifier, Discriminator, DataLocation, Associated, Allocated, Rank))\n\n  TempDICompositeType clone() const { return cloneImpl(); }\n\n  /// Get a DICompositeType with the given ODR identifier.\n  ///\n  /// If \\a LLVMContext::isODRUniquingDebugTypes(), gets the mapped\n  /// DICompositeType for the given ODR \\c Identifier.  If none exists, creates\n  /// a new node.\n  ///\n  /// Else, returns \\c nullptr.\n  static DICompositeType *\n  getODRType(LLVMContext &Context, MDString &Identifier, unsigned Tag,\n             MDString *Name, Metadata *File, unsigned Line, Metadata *Scope,\n             Metadata *BaseType, uint64_t SizeInBits, uint32_t AlignInBits,\n             uint64_t OffsetInBits, DIFlags Flags, Metadata *Elements,\n             unsigned RuntimeLang, Metadata *VTableHolder,\n             Metadata *TemplateParams, Metadata *Discriminator,\n             Metadata *DataLocation, Metadata *Associated, Metadata *Allocated,\n             Metadata *Rank);\n  static DICompositeType *getODRTypeIfExists(LLVMContext &Context,\n                                             MDString &Identifier);\n\n  /// Build a DICompositeType with the given ODR identifier.\n  ///\n  /// Looks up the mapped DICompositeType for the given ODR \\c Identifier.  If\n  /// it doesn't exist, creates a new one.  If it does exist and \\a\n  /// isForwardDecl(), and the new arguments would be a definition, mutates the\n  /// the type in place.  In either case, returns the type.\n  ///\n  /// If not \\a LLVMContext::isODRUniquingDebugTypes(), this function returns\n  /// nullptr.\n  static DICompositeType *\n  buildODRType(LLVMContext &Context, MDString &Identifier, unsigned Tag,\n               MDString *Name, Metadata *File, unsigned Line, Metadata *Scope,\n               Metadata *BaseType, uint64_t SizeInBits, uint32_t AlignInBits,\n               uint64_t OffsetInBits, DIFlags Flags, Metadata *Elements,\n               unsigned RuntimeLang, Metadata *VTableHolder,\n               Metadata *TemplateParams, Metadata *Discriminator,\n               Metadata *DataLocation, Metadata *Associated,\n               Metadata *Allocated, Metadata *Rank);\n\n  DIType *getBaseType() const { return cast_or_null<DIType>(getRawBaseType()); }\n  DINodeArray getElements() const {\n    return cast_or_null<MDTuple>(getRawElements());\n  }\n  DIType *getVTableHolder() const {\n    return cast_or_null<DIType>(getRawVTableHolder());\n  }\n  DITemplateParameterArray getTemplateParams() const {\n    return cast_or_null<MDTuple>(getRawTemplateParams());\n  }\n  StringRef getIdentifier() const { return getStringOperand(7); }\n  unsigned getRuntimeLang() const { return RuntimeLang; }\n\n  Metadata *getRawBaseType() const { return getOperand(3); }\n  Metadata *getRawElements() const { return getOperand(4); }\n  Metadata *getRawVTableHolder() const { return getOperand(5); }\n  Metadata *getRawTemplateParams() const { return getOperand(6); }\n  MDString *getRawIdentifier() const { return getOperandAs<MDString>(7); }\n  Metadata *getRawDiscriminator() const { return getOperand(8); }\n  DIDerivedType *getDiscriminator() const { return getOperandAs<DIDerivedType>(8); }\n  Metadata *getRawDataLocation() const { return getOperand(9); }\n  DIVariable *getDataLocation() const {\n    return dyn_cast_or_null<DIVariable>(getRawDataLocation());\n  }\n  DIExpression *getDataLocationExp() const {\n    return dyn_cast_or_null<DIExpression>(getRawDataLocation());\n  }\n  Metadata *getRawAssociated() const { return getOperand(10); }\n  DIVariable *getAssociated() const {\n    return dyn_cast_or_null<DIVariable>(getRawAssociated());\n  }\n  DIExpression *getAssociatedExp() const {\n    return dyn_cast_or_null<DIExpression>(getRawAssociated());\n  }\n  Metadata *getRawAllocated() const { return getOperand(11); }\n  DIVariable *getAllocated() const {\n    return dyn_cast_or_null<DIVariable>(getRawAllocated());\n  }\n  DIExpression *getAllocatedExp() const {\n    return dyn_cast_or_null<DIExpression>(getRawAllocated());\n  }\n  Metadata *getRawRank() const { return getOperand(12); }\n  ConstantInt *getRankConst() const {\n    if (auto *MD = dyn_cast_or_null<ConstantAsMetadata>(getRawRank()))\n      return dyn_cast_or_null<ConstantInt>(MD->getValue());\n    return nullptr;\n  }\n  DIExpression *getRankExp() const {\n    return dyn_cast_or_null<DIExpression>(getRawRank());\n  }\n\n  /// Replace operands.\n  ///\n  /// If this \\a isUniqued() and not \\a isResolved(), on a uniquing collision\n  /// this will be RAUW'ed and deleted.  Use a \\a TrackingMDRef to keep track\n  /// of its movement if necessary.\n  /// @{\n  void replaceElements(DINodeArray Elements) {\n#ifndef NDEBUG\n    for (DINode *Op : getElements())\n      assert(is_contained(Elements->operands(), Op) &&\n             \"Lost a member during member list replacement\");\n#endif\n    replaceOperandWith(4, Elements.get());\n  }\n\n  void replaceVTableHolder(DIType *VTableHolder) {\n    replaceOperandWith(5, VTableHolder);\n  }\n\n  void replaceTemplateParams(DITemplateParameterArray TemplateParams) {\n    replaceOperandWith(6, TemplateParams.get());\n  }\n  /// @}\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DICompositeTypeKind;\n  }\n};\n\n/// Type array for a subprogram.\n///\n/// TODO: Fold the array of types in directly as operands.\nclass DISubroutineType : public DIType {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  /// The calling convention used with DW_AT_calling_convention. Actually of\n  /// type dwarf::CallingConvention.\n  uint8_t CC;\n\n  DISubroutineType(LLVMContext &C, StorageType Storage, DIFlags Flags,\n                   uint8_t CC, ArrayRef<Metadata *> Ops)\n      : DIType(C, DISubroutineTypeKind, Storage, dwarf::DW_TAG_subroutine_type,\n               0, 0, 0, 0, Flags, Ops),\n        CC(CC) {}\n  ~DISubroutineType() = default;\n\n  static DISubroutineType *getImpl(LLVMContext &Context, DIFlags Flags,\n                                   uint8_t CC, DITypeRefArray TypeArray,\n                                   StorageType Storage,\n                                   bool ShouldCreate = true) {\n    return getImpl(Context, Flags, CC, TypeArray.get(), Storage, ShouldCreate);\n  }\n  static DISubroutineType *getImpl(LLVMContext &Context, DIFlags Flags,\n                                   uint8_t CC, Metadata *TypeArray,\n                                   StorageType Storage,\n                                   bool ShouldCreate = true);\n\n  TempDISubroutineType cloneImpl() const {\n    return getTemporary(getContext(), getFlags(), getCC(), getTypeArray());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DISubroutineType,\n                    (DIFlags Flags, uint8_t CC, DITypeRefArray TypeArray),\n                    (Flags, CC, TypeArray))\n  DEFINE_MDNODE_GET(DISubroutineType,\n                    (DIFlags Flags, uint8_t CC, Metadata *TypeArray),\n                    (Flags, CC, TypeArray))\n\n  TempDISubroutineType clone() const { return cloneImpl(); }\n\n  uint8_t getCC() const { return CC; }\n\n  DITypeRefArray getTypeArray() const {\n    return cast_or_null<MDTuple>(getRawTypeArray());\n  }\n\n  Metadata *getRawTypeArray() const { return getOperand(3); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DISubroutineTypeKind;\n  }\n};\n\n/// Compile unit.\nclass DICompileUnit : public DIScope {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\npublic:\n  enum DebugEmissionKind : unsigned {\n    NoDebug = 0,\n    FullDebug,\n    LineTablesOnly,\n    DebugDirectivesOnly,\n    LastEmissionKind = DebugDirectivesOnly\n  };\n\n  enum class DebugNameTableKind : unsigned {\n    Default = 0,\n    GNU = 1,\n    None = 2,\n    LastDebugNameTableKind = None\n  };\n\n  static Optional<DebugEmissionKind> getEmissionKind(StringRef Str);\n  static const char *emissionKindString(DebugEmissionKind EK);\n  static Optional<DebugNameTableKind> getNameTableKind(StringRef Str);\n  static const char *nameTableKindString(DebugNameTableKind PK);\n\nprivate:\n  unsigned SourceLanguage;\n  bool IsOptimized;\n  unsigned RuntimeVersion;\n  unsigned EmissionKind;\n  uint64_t DWOId;\n  bool SplitDebugInlining;\n  bool DebugInfoForProfiling;\n  unsigned NameTableKind;\n  bool RangesBaseAddress;\n\n  DICompileUnit(LLVMContext &C, StorageType Storage, unsigned SourceLanguage,\n                bool IsOptimized, unsigned RuntimeVersion,\n                unsigned EmissionKind, uint64_t DWOId, bool SplitDebugInlining,\n                bool DebugInfoForProfiling, unsigned NameTableKind,\n                bool RangesBaseAddress, ArrayRef<Metadata *> Ops)\n      : DIScope(C, DICompileUnitKind, Storage, dwarf::DW_TAG_compile_unit, Ops),\n        SourceLanguage(SourceLanguage), IsOptimized(IsOptimized),\n        RuntimeVersion(RuntimeVersion), EmissionKind(EmissionKind),\n        DWOId(DWOId), SplitDebugInlining(SplitDebugInlining),\n        DebugInfoForProfiling(DebugInfoForProfiling),\n        NameTableKind(NameTableKind), RangesBaseAddress(RangesBaseAddress) {\n    assert(Storage != Uniqued);\n  }\n  ~DICompileUnit() = default;\n\n  static DICompileUnit *\n  getImpl(LLVMContext &Context, unsigned SourceLanguage, DIFile *File,\n          StringRef Producer, bool IsOptimized, StringRef Flags,\n          unsigned RuntimeVersion, StringRef SplitDebugFilename,\n          unsigned EmissionKind, DICompositeTypeArray EnumTypes,\n          DIScopeArray RetainedTypes,\n          DIGlobalVariableExpressionArray GlobalVariables,\n          DIImportedEntityArray ImportedEntities, DIMacroNodeArray Macros,\n          uint64_t DWOId, bool SplitDebugInlining, bool DebugInfoForProfiling,\n          unsigned NameTableKind, bool RangesBaseAddress, StringRef SysRoot,\n          StringRef SDK, StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(\n        Context, SourceLanguage, File, getCanonicalMDString(Context, Producer),\n        IsOptimized, getCanonicalMDString(Context, Flags), RuntimeVersion,\n        getCanonicalMDString(Context, SplitDebugFilename), EmissionKind,\n        EnumTypes.get(), RetainedTypes.get(), GlobalVariables.get(),\n        ImportedEntities.get(), Macros.get(), DWOId, SplitDebugInlining,\n        DebugInfoForProfiling, NameTableKind, RangesBaseAddress,\n        getCanonicalMDString(Context, SysRoot),\n        getCanonicalMDString(Context, SDK), Storage, ShouldCreate);\n  }\n  static DICompileUnit *\n  getImpl(LLVMContext &Context, unsigned SourceLanguage, Metadata *File,\n          MDString *Producer, bool IsOptimized, MDString *Flags,\n          unsigned RuntimeVersion, MDString *SplitDebugFilename,\n          unsigned EmissionKind, Metadata *EnumTypes, Metadata *RetainedTypes,\n          Metadata *GlobalVariables, Metadata *ImportedEntities,\n          Metadata *Macros, uint64_t DWOId, bool SplitDebugInlining,\n          bool DebugInfoForProfiling, unsigned NameTableKind,\n          bool RangesBaseAddress, MDString *SysRoot, MDString *SDK,\n          StorageType Storage, bool ShouldCreate = true);\n\n  TempDICompileUnit cloneImpl() const {\n    return getTemporary(\n        getContext(), getSourceLanguage(), getFile(), getProducer(),\n        isOptimized(), getFlags(), getRuntimeVersion(), getSplitDebugFilename(),\n        getEmissionKind(), getEnumTypes(), getRetainedTypes(),\n        getGlobalVariables(), getImportedEntities(), getMacros(), DWOId,\n        getSplitDebugInlining(), getDebugInfoForProfiling(), getNameTableKind(),\n        getRangesBaseAddress(), getSysRoot(), getSDK());\n  }\n\npublic:\n  static void get() = delete;\n  static void getIfExists() = delete;\n\n  DEFINE_MDNODE_GET_DISTINCT_TEMPORARY(\n      DICompileUnit,\n      (unsigned SourceLanguage, DIFile *File, StringRef Producer,\n       bool IsOptimized, StringRef Flags, unsigned RuntimeVersion,\n       StringRef SplitDebugFilename, DebugEmissionKind EmissionKind,\n       DICompositeTypeArray EnumTypes, DIScopeArray RetainedTypes,\n       DIGlobalVariableExpressionArray GlobalVariables,\n       DIImportedEntityArray ImportedEntities, DIMacroNodeArray Macros,\n       uint64_t DWOId, bool SplitDebugInlining, bool DebugInfoForProfiling,\n       DebugNameTableKind NameTableKind, bool RangesBaseAddress,\n       StringRef SysRoot, StringRef SDK),\n      (SourceLanguage, File, Producer, IsOptimized, Flags, RuntimeVersion,\n       SplitDebugFilename, EmissionKind, EnumTypes, RetainedTypes,\n       GlobalVariables, ImportedEntities, Macros, DWOId, SplitDebugInlining,\n       DebugInfoForProfiling, (unsigned)NameTableKind, RangesBaseAddress,\n       SysRoot, SDK))\n  DEFINE_MDNODE_GET_DISTINCT_TEMPORARY(\n      DICompileUnit,\n      (unsigned SourceLanguage, Metadata *File, MDString *Producer,\n       bool IsOptimized, MDString *Flags, unsigned RuntimeVersion,\n       MDString *SplitDebugFilename, unsigned EmissionKind, Metadata *EnumTypes,\n       Metadata *RetainedTypes, Metadata *GlobalVariables,\n       Metadata *ImportedEntities, Metadata *Macros, uint64_t DWOId,\n       bool SplitDebugInlining, bool DebugInfoForProfiling,\n       unsigned NameTableKind, bool RangesBaseAddress, MDString *SysRoot,\n       MDString *SDK),\n      (SourceLanguage, File, Producer, IsOptimized, Flags, RuntimeVersion,\n       SplitDebugFilename, EmissionKind, EnumTypes, RetainedTypes,\n       GlobalVariables, ImportedEntities, Macros, DWOId, SplitDebugInlining,\n       DebugInfoForProfiling, NameTableKind, RangesBaseAddress, SysRoot, SDK))\n\n  TempDICompileUnit clone() const { return cloneImpl(); }\n\n  unsigned getSourceLanguage() const { return SourceLanguage; }\n  bool isOptimized() const { return IsOptimized; }\n  unsigned getRuntimeVersion() const { return RuntimeVersion; }\n  DebugEmissionKind getEmissionKind() const {\n    return (DebugEmissionKind)EmissionKind;\n  }\n  bool isDebugDirectivesOnly() const {\n    return EmissionKind == DebugDirectivesOnly;\n  }\n  bool getDebugInfoForProfiling() const { return DebugInfoForProfiling; }\n  DebugNameTableKind getNameTableKind() const {\n    return (DebugNameTableKind)NameTableKind;\n  }\n  bool getRangesBaseAddress() const { return RangesBaseAddress; }\n  StringRef getProducer() const { return getStringOperand(1); }\n  StringRef getFlags() const { return getStringOperand(2); }\n  StringRef getSplitDebugFilename() const { return getStringOperand(3); }\n  DICompositeTypeArray getEnumTypes() const {\n    return cast_or_null<MDTuple>(getRawEnumTypes());\n  }\n  DIScopeArray getRetainedTypes() const {\n    return cast_or_null<MDTuple>(getRawRetainedTypes());\n  }\n  DIGlobalVariableExpressionArray getGlobalVariables() const {\n    return cast_or_null<MDTuple>(getRawGlobalVariables());\n  }\n  DIImportedEntityArray getImportedEntities() const {\n    return cast_or_null<MDTuple>(getRawImportedEntities());\n  }\n  DIMacroNodeArray getMacros() const {\n    return cast_or_null<MDTuple>(getRawMacros());\n  }\n  uint64_t getDWOId() const { return DWOId; }\n  void setDWOId(uint64_t DwoId) { DWOId = DwoId; }\n  bool getSplitDebugInlining() const { return SplitDebugInlining; }\n  void setSplitDebugInlining(bool SplitDebugInlining) {\n    this->SplitDebugInlining = SplitDebugInlining;\n  }\n  StringRef getSysRoot() const { return getStringOperand(9); }\n  StringRef getSDK() const { return getStringOperand(10); }\n\n  MDString *getRawProducer() const { return getOperandAs<MDString>(1); }\n  MDString *getRawFlags() const { return getOperandAs<MDString>(2); }\n  MDString *getRawSplitDebugFilename() const {\n    return getOperandAs<MDString>(3);\n  }\n  Metadata *getRawEnumTypes() const { return getOperand(4); }\n  Metadata *getRawRetainedTypes() const { return getOperand(5); }\n  Metadata *getRawGlobalVariables() const { return getOperand(6); }\n  Metadata *getRawImportedEntities() const { return getOperand(7); }\n  Metadata *getRawMacros() const { return getOperand(8); }\n  MDString *getRawSysRoot() const { return getOperandAs<MDString>(9); }\n  MDString *getRawSDK() const { return getOperandAs<MDString>(10); }\n\n  /// Replace arrays.\n  ///\n  /// If this \\a isUniqued() and not \\a isResolved(), it will be RAUW'ed and\n  /// deleted on a uniquing collision.  In practice, uniquing collisions on \\a\n  /// DICompileUnit should be fairly rare.\n  /// @{\n  void replaceEnumTypes(DICompositeTypeArray N) {\n    replaceOperandWith(4, N.get());\n  }\n  void replaceRetainedTypes(DITypeArray N) {\n    replaceOperandWith(5, N.get());\n  }\n  void replaceGlobalVariables(DIGlobalVariableExpressionArray N) {\n    replaceOperandWith(6, N.get());\n  }\n  void replaceImportedEntities(DIImportedEntityArray N) {\n    replaceOperandWith(7, N.get());\n  }\n  void replaceMacros(DIMacroNodeArray N) { replaceOperandWith(8, N.get()); }\n  /// @}\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DICompileUnitKind;\n  }\n};\n\n/// A scope for locals.\n///\n/// A legal scope for lexical blocks, local variables, and debug info\n/// locations.  Subclasses are \\a DISubprogram, \\a DILexicalBlock, and \\a\n/// DILexicalBlockFile.\nclass DILocalScope : public DIScope {\nprotected:\n  DILocalScope(LLVMContext &C, unsigned ID, StorageType Storage, unsigned Tag,\n               ArrayRef<Metadata *> Ops)\n      : DIScope(C, ID, Storage, Tag, Ops) {}\n  ~DILocalScope() = default;\n\npublic:\n  /// Get the subprogram for this scope.\n  ///\n  /// Return this if it's an \\a DISubprogram; otherwise, look up the scope\n  /// chain.\n  DISubprogram *getSubprogram() const;\n\n  /// Get the first non DILexicalBlockFile scope of this scope.\n  ///\n  /// Return this if it's not a \\a DILexicalBlockFIle; otherwise, look up the\n  /// scope chain.\n  DILocalScope *getNonLexicalBlockFileScope() const;\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DISubprogramKind ||\n           MD->getMetadataID() == DILexicalBlockKind ||\n           MD->getMetadataID() == DILexicalBlockFileKind;\n  }\n};\n\n/// Debug location.\n///\n/// A debug location in source code, used for debug info and otherwise.\nclass DILocation : public MDNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  DILocation(LLVMContext &C, StorageType Storage, unsigned Line,\n             unsigned Column, ArrayRef<Metadata *> MDs, bool ImplicitCode);\n  ~DILocation() { dropAllReferences(); }\n\n  static DILocation *getImpl(LLVMContext &Context, unsigned Line,\n                             unsigned Column, Metadata *Scope,\n                             Metadata *InlinedAt, bool ImplicitCode,\n                             StorageType Storage, bool ShouldCreate = true);\n  static DILocation *getImpl(LLVMContext &Context, unsigned Line,\n                             unsigned Column, DILocalScope *Scope,\n                             DILocation *InlinedAt, bool ImplicitCode,\n                             StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Line, Column, static_cast<Metadata *>(Scope),\n                   static_cast<Metadata *>(InlinedAt), ImplicitCode, Storage,\n                   ShouldCreate);\n  }\n\n  /// With a given unsigned int \\p U, use up to 13 bits to represent it.\n  /// old_bit 1~5  --> new_bit 1~5\n  /// old_bit 6~12 --> new_bit 7~13\n  /// new_bit_6 is 0 if higher bits (7~13) are all 0\n  static unsigned getPrefixEncodingFromUnsigned(unsigned U) {\n    U &= 0xfff;\n    return U > 0x1f ? (((U & 0xfe0) << 1) | (U & 0x1f) | 0x20) : U;\n  }\n\n  /// Reverse transformation as getPrefixEncodingFromUnsigned.\n  static unsigned getUnsignedFromPrefixEncoding(unsigned U) {\n    if (U & 1)\n      return 0;\n    U >>= 1;\n    return (U & 0x20) ? (((U >> 1) & 0xfe0) | (U & 0x1f)) : (U & 0x1f);\n  }\n\n  /// Returns the next component stored in discriminator.\n  static unsigned getNextComponentInDiscriminator(unsigned D) {\n    if ((D & 1) == 0)\n      return D >> ((D & 0x40) ? 14 : 7);\n    else\n      return D >> 1;\n  }\n\n  TempDILocation cloneImpl() const {\n    // Get the raw scope/inlinedAt since it is possible to invoke this on\n    // a DILocation containing temporary metadata.\n    return getTemporary(getContext(), getLine(), getColumn(), getRawScope(),\n                        getRawInlinedAt(), isImplicitCode());\n  }\n\n  static unsigned encodeComponent(unsigned C) {\n    return (C == 0) ? 1U : (getPrefixEncodingFromUnsigned(C) << 1);\n  }\n\n  static unsigned encodingBits(unsigned C) {\n    return (C == 0) ? 1 : (C > 0x1f ? 14 : 7);\n  }\n\npublic:\n  // Disallow replacing operands.\n  void replaceOperandWith(unsigned I, Metadata *New) = delete;\n\n  DEFINE_MDNODE_GET(DILocation,\n                    (unsigned Line, unsigned Column, Metadata *Scope,\n                     Metadata *InlinedAt = nullptr, bool ImplicitCode = false),\n                    (Line, Column, Scope, InlinedAt, ImplicitCode))\n  DEFINE_MDNODE_GET(DILocation,\n                    (unsigned Line, unsigned Column, DILocalScope *Scope,\n                     DILocation *InlinedAt = nullptr,\n                     bool ImplicitCode = false),\n                    (Line, Column, Scope, InlinedAt, ImplicitCode))\n\n  /// Return a (temporary) clone of this.\n  TempDILocation clone() const { return cloneImpl(); }\n\n  unsigned getLine() const { return SubclassData32; }\n  unsigned getColumn() const { return SubclassData16; }\n  DILocalScope *getScope() const { return cast<DILocalScope>(getRawScope()); }\n\n  DILocation *getInlinedAt() const {\n    return cast_or_null<DILocation>(getRawInlinedAt());\n  }\n\n  /// Check if the location corresponds to an implicit code.\n  /// When the ImplicitCode flag is true, it means that the Instruction\n  /// with this DILocation has been added by the front-end but it hasn't been\n  /// written explicitly by the user (e.g. cleanup stuff in C++ put on a closing\n  /// bracket). It's useful for code coverage to not show a counter on \"empty\"\n  /// lines.\n  bool isImplicitCode() const { return SubclassData1; }\n  void setImplicitCode(bool ImplicitCode) { SubclassData1 = ImplicitCode; }\n\n  DIFile *getFile() const { return getScope()->getFile(); }\n  StringRef getFilename() const { return getScope()->getFilename(); }\n  StringRef getDirectory() const { return getScope()->getDirectory(); }\n  Optional<StringRef> getSource() const { return getScope()->getSource(); }\n\n  /// Get the scope where this is inlined.\n  ///\n  /// Walk through \\a getInlinedAt() and return \\a getScope() from the deepest\n  /// location.\n  DILocalScope *getInlinedAtScope() const {\n    if (auto *IA = getInlinedAt())\n      return IA->getInlinedAtScope();\n    return getScope();\n  }\n\n  /// Get the DWARF discriminator.\n  ///\n  /// DWARF discriminators distinguish identical file locations between\n  /// instructions that are on different basic blocks.\n  ///\n  /// There are 3 components stored in discriminator, from lower bits:\n  ///\n  /// Base discriminator: assigned by AddDiscriminators pass to identify IRs\n  ///                     that are defined by the same source line, but\n  ///                     different basic blocks.\n  /// Duplication factor: assigned by optimizations that will scale down\n  ///                     the execution frequency of the original IR.\n  /// Copy Identifier: assigned by optimizations that clones the IR.\n  ///                  Each copy of the IR will be assigned an identifier.\n  ///\n  /// Encoding:\n  ///\n  /// The above 3 components are encoded into a 32bit unsigned integer in\n  /// order. If the lowest bit is 1, the current component is empty, and the\n  /// next component will start in the next bit. Otherwise, the current\n  /// component is non-empty, and its content starts in the next bit. The\n  /// value of each components is either 5 bit or 12 bit: if the 7th bit\n  /// is 0, the bit 2~6 (5 bits) are used to represent the component; if the\n  /// 7th bit is 1, the bit 2~6 (5 bits) and 8~14 (7 bits) are combined to\n  /// represent the component. Thus, the number of bits used for a component\n  /// is either 0 (if it and all the next components are empty); 1 - if it is\n  /// empty; 7 - if its value is up to and including 0x1f (lsb and msb are both\n  /// 0); or 14, if its value is up to and including 0x1ff. Note that the last\n  /// component is also capped at 0x1ff, even in the case when both first\n  /// components are 0, and we'd technically have 29 bits available.\n  ///\n  /// For precise control over the data being encoded in the discriminator,\n  /// use encodeDiscriminator/decodeDiscriminator.\n\n  inline unsigned getDiscriminator() const;\n\n  // For the regular discriminator, it stands for all empty components if all\n  // the lowest 3 bits are non-zero and all higher 29 bits are unused(zero by\n  // default). Here we fully leverage the higher 29 bits for pseudo probe use.\n  // This is the format:\n  // [2:0] - 0x7\n  // [31:3] - pseudo probe fields guaranteed to be non-zero as a whole\n  // So if the lower 3 bits is non-zero and the others has at least one\n  // non-zero bit, it guarantees to be a pseudo probe discriminator\n  inline static bool isPseudoProbeDiscriminator(unsigned Discriminator) {\n    return ((Discriminator & 0x7) == 0x7) && (Discriminator & 0xFFFFFFF8);\n  }\n\n  /// Returns a new DILocation with updated \\p Discriminator.\n  inline const DILocation *cloneWithDiscriminator(unsigned Discriminator) const;\n\n  /// Returns a new DILocation with updated base discriminator \\p BD. Only the\n  /// base discriminator is set in the new DILocation, the other encoded values\n  /// are elided.\n  /// If the discriminator cannot be encoded, the function returns None.\n  inline Optional<const DILocation *> cloneWithBaseDiscriminator(unsigned BD) const;\n\n  /// Returns the duplication factor stored in the discriminator, or 1 if no\n  /// duplication factor (or 0) is encoded.\n  inline unsigned getDuplicationFactor() const;\n\n  /// Returns the copy identifier stored in the discriminator.\n  inline unsigned getCopyIdentifier() const;\n\n  /// Returns the base discriminator stored in the discriminator.\n  inline unsigned getBaseDiscriminator() const;\n\n  /// Returns a new DILocation with duplication factor \\p DF * current\n  /// duplication factor encoded in the discriminator. The current duplication\n  /// factor is as defined by getDuplicationFactor().\n  /// Returns None if encoding failed.\n  inline Optional<const DILocation *> cloneByMultiplyingDuplicationFactor(unsigned DF) const;\n\n  /// When two instructions are combined into a single instruction we also\n  /// need to combine the original locations into a single location.\n  ///\n  /// When the locations are the same we can use either location. When they\n  /// differ, we need a third location which is distinct from either. If they\n  /// have the same file/line but have a different discriminator we could\n  /// create a location with a new discriminator. If they are from different\n  /// files/lines the location is ambiguous and can't be represented in a line\n  /// entry. In this case, if \\p GenerateLocation is true, we will set the\n  /// merged debug location as line 0 of the nearest common scope where the two\n  /// locations are inlined from.\n  ///\n  /// \\p GenerateLocation: Whether the merged location can be generated when\n  /// \\p LocA and \\p LocB differ.\n  static const DILocation *getMergedLocation(const DILocation *LocA,\n                                             const DILocation *LocB);\n\n  /// Try to combine the vector of locations passed as input in a single one.\n  /// This function applies getMergedLocation() repeatedly left-to-right.\n  ///\n  /// \\p Locs: The locations to be merged.\n  static\n  const DILocation *getMergedLocations(ArrayRef<const DILocation *> Locs);\n\n  /// Returns the base discriminator for a given encoded discriminator \\p D.\n  static unsigned getBaseDiscriminatorFromDiscriminator(unsigned D) {\n    return getUnsignedFromPrefixEncoding(D);\n  }\n\n  /// Raw encoding of the discriminator. APIs such as cloneWithDuplicationFactor\n  /// have certain special case behavior (e.g. treating empty duplication factor\n  /// as the value '1').\n  /// This API, in conjunction with cloneWithDiscriminator, may be used to encode\n  /// the raw values provided. \\p BD: base discriminator \\p DF: duplication factor\n  /// \\p CI: copy index\n  /// The return is None if the values cannot be encoded in 32 bits - for\n  /// example, values for BD or DF larger than 12 bits. Otherwise, the return\n  /// is the encoded value.\n  static Optional<unsigned> encodeDiscriminator(unsigned BD, unsigned DF, unsigned CI);\n\n  /// Raw decoder for values in an encoded discriminator D.\n  static void decodeDiscriminator(unsigned D, unsigned &BD, unsigned &DF,\n                                  unsigned &CI);\n\n  /// Returns the duplication factor for a given encoded discriminator \\p D, or\n  /// 1 if no value or 0 is encoded.\n  static unsigned getDuplicationFactorFromDiscriminator(unsigned D) {\n    D = getNextComponentInDiscriminator(D);\n    unsigned Ret = getUnsignedFromPrefixEncoding(D);\n    if (Ret == 0)\n      return 1;\n    return Ret;\n  }\n\n  /// Returns the copy identifier for a given encoded discriminator \\p D.\n  static unsigned getCopyIdentifierFromDiscriminator(unsigned D) {\n    return getUnsignedFromPrefixEncoding(getNextComponentInDiscriminator(\n        getNextComponentInDiscriminator(D)));\n  }\n\n\n  Metadata *getRawScope() const { return getOperand(0); }\n  Metadata *getRawInlinedAt() const {\n    if (getNumOperands() == 2)\n      return getOperand(1);\n    return nullptr;\n  }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DILocationKind;\n  }\n};\n\n/// Subprogram description.\nclass DISubprogram : public DILocalScope {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Line;\n  unsigned ScopeLine;\n  unsigned VirtualIndex;\n\n  /// In the MS ABI, the implicit 'this' parameter is adjusted in the prologue\n  /// of method overrides from secondary bases by this amount. It may be\n  /// negative.\n  int ThisAdjustment;\n\npublic:\n  /// Debug info subprogram flags.\n  enum DISPFlags : uint32_t {\n#define HANDLE_DISP_FLAG(ID, NAME) SPFlag##NAME = ID,\n#define DISP_FLAG_LARGEST_NEEDED\n#include \"llvm/IR/DebugInfoFlags.def\"\n    SPFlagNonvirtual = SPFlagZero,\n    SPFlagVirtuality = SPFlagVirtual | SPFlagPureVirtual,\n    LLVM_MARK_AS_BITMASK_ENUM(SPFlagLargest)\n  };\n\n  static DISPFlags getFlag(StringRef Flag);\n  static StringRef getFlagString(DISPFlags Flag);\n\n  /// Split up a flags bitfield for easier printing.\n  ///\n  /// Split \\c Flags into \\c SplitFlags, a vector of its components.  Returns\n  /// any remaining (unrecognized) bits.\n  static DISPFlags splitFlags(DISPFlags Flags,\n                              SmallVectorImpl<DISPFlags> &SplitFlags);\n\n  // Helper for converting old bitfields to new flags word.\n  static DISPFlags toSPFlags(bool IsLocalToUnit, bool IsDefinition,\n                             bool IsOptimized,\n                             unsigned Virtuality = SPFlagNonvirtual,\n                             bool IsMainSubprogram = false) {\n    // We're assuming virtuality is the low-order field.\n    static_assert(\n        int(SPFlagVirtual) == int(dwarf::DW_VIRTUALITY_virtual) &&\n            int(SPFlagPureVirtual) == int(dwarf::DW_VIRTUALITY_pure_virtual),\n        \"Virtuality constant mismatch\");\n    return static_cast<DISPFlags>(\n        (Virtuality & SPFlagVirtuality) |\n        (IsLocalToUnit ? SPFlagLocalToUnit : SPFlagZero) |\n        (IsDefinition ? SPFlagDefinition : SPFlagZero) |\n        (IsOptimized ? SPFlagOptimized : SPFlagZero) |\n        (IsMainSubprogram ? SPFlagMainSubprogram : SPFlagZero));\n  }\n\nprivate:\n  DIFlags Flags;\n  DISPFlags SPFlags;\n\n  DISubprogram(LLVMContext &C, StorageType Storage, unsigned Line,\n               unsigned ScopeLine, unsigned VirtualIndex, int ThisAdjustment,\n               DIFlags Flags, DISPFlags SPFlags, ArrayRef<Metadata *> Ops)\n      : DILocalScope(C, DISubprogramKind, Storage, dwarf::DW_TAG_subprogram,\n                     Ops),\n        Line(Line), ScopeLine(ScopeLine), VirtualIndex(VirtualIndex),\n        ThisAdjustment(ThisAdjustment), Flags(Flags), SPFlags(SPFlags) {\n    static_assert(dwarf::DW_VIRTUALITY_max < 4, \"Virtuality out of range\");\n  }\n  ~DISubprogram() = default;\n\n  static DISubprogram *\n  getImpl(LLVMContext &Context, DIScope *Scope, StringRef Name,\n          StringRef LinkageName, DIFile *File, unsigned Line,\n          DISubroutineType *Type, unsigned ScopeLine, DIType *ContainingType,\n          unsigned VirtualIndex, int ThisAdjustment, DIFlags Flags,\n          DISPFlags SPFlags, DICompileUnit *Unit,\n          DITemplateParameterArray TemplateParams, DISubprogram *Declaration,\n          DINodeArray RetainedNodes, DITypeArray ThrownTypes,\n          StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Scope, getCanonicalMDString(Context, Name),\n                   getCanonicalMDString(Context, LinkageName), File, Line, Type,\n                   ScopeLine, ContainingType, VirtualIndex, ThisAdjustment,\n                   Flags, SPFlags, Unit, TemplateParams.get(), Declaration,\n                   RetainedNodes.get(), ThrownTypes.get(), Storage,\n                   ShouldCreate);\n  }\n  static DISubprogram *getImpl(LLVMContext &Context, Metadata *Scope,\n                               MDString *Name, MDString *LinkageName,\n                               Metadata *File, unsigned Line, Metadata *Type,\n                               unsigned ScopeLine, Metadata *ContainingType,\n                               unsigned VirtualIndex, int ThisAdjustment,\n                               DIFlags Flags, DISPFlags SPFlags, Metadata *Unit,\n                               Metadata *TemplateParams, Metadata *Declaration,\n                               Metadata *RetainedNodes, Metadata *ThrownTypes,\n                               StorageType Storage, bool ShouldCreate = true);\n\n  TempDISubprogram cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getName(), getLinkageName(),\n                        getFile(), getLine(), getType(), getScopeLine(),\n                        getContainingType(), getVirtualIndex(),\n                        getThisAdjustment(), getFlags(), getSPFlags(),\n                        getUnit(), getTemplateParams(), getDeclaration(),\n                        getRetainedNodes(), getThrownTypes());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(\n      DISubprogram,\n      (DIScope * Scope, StringRef Name, StringRef LinkageName, DIFile *File,\n       unsigned Line, DISubroutineType *Type, unsigned ScopeLine,\n       DIType *ContainingType, unsigned VirtualIndex, int ThisAdjustment,\n       DIFlags Flags, DISPFlags SPFlags, DICompileUnit *Unit,\n       DITemplateParameterArray TemplateParams = nullptr,\n       DISubprogram *Declaration = nullptr, DINodeArray RetainedNodes = nullptr,\n       DITypeArray ThrownTypes = nullptr),\n      (Scope, Name, LinkageName, File, Line, Type, ScopeLine, ContainingType,\n       VirtualIndex, ThisAdjustment, Flags, SPFlags, Unit, TemplateParams,\n       Declaration, RetainedNodes, ThrownTypes))\n\n  DEFINE_MDNODE_GET(\n      DISubprogram,\n      (Metadata * Scope, MDString *Name, MDString *LinkageName, Metadata *File,\n       unsigned Line, Metadata *Type, unsigned ScopeLine,\n       Metadata *ContainingType, unsigned VirtualIndex, int ThisAdjustment,\n       DIFlags Flags, DISPFlags SPFlags, Metadata *Unit,\n       Metadata *TemplateParams = nullptr, Metadata *Declaration = nullptr,\n       Metadata *RetainedNodes = nullptr, Metadata *ThrownTypes = nullptr),\n      (Scope, Name, LinkageName, File, Line, Type, ScopeLine, ContainingType,\n       VirtualIndex, ThisAdjustment, Flags, SPFlags, Unit, TemplateParams,\n       Declaration, RetainedNodes, ThrownTypes))\n\n  TempDISubprogram clone() const { return cloneImpl(); }\n\n  /// Returns a new temporary DISubprogram with updated Flags\n  TempDISubprogram cloneWithFlags(DIFlags NewFlags) const {\n    auto NewSP = clone();\n    NewSP->Flags = NewFlags;\n    return NewSP;\n  }\n\npublic:\n  unsigned getLine() const { return Line; }\n  unsigned getVirtuality() const { return getSPFlags() & SPFlagVirtuality; }\n  unsigned getVirtualIndex() const { return VirtualIndex; }\n  int getThisAdjustment() const { return ThisAdjustment; }\n  unsigned getScopeLine() const { return ScopeLine; }\n  void setScopeLine(unsigned L) { assert(isDistinct()); ScopeLine = L; }\n  DIFlags getFlags() const { return Flags; }\n  DISPFlags getSPFlags() const { return SPFlags; }\n  bool isLocalToUnit() const { return getSPFlags() & SPFlagLocalToUnit; }\n  bool isDefinition() const { return getSPFlags() & SPFlagDefinition; }\n  bool isOptimized() const { return getSPFlags() & SPFlagOptimized; }\n  bool isMainSubprogram() const { return getSPFlags() & SPFlagMainSubprogram; }\n\n  bool isArtificial() const { return getFlags() & FlagArtificial; }\n  bool isPrivate() const {\n    return (getFlags() & FlagAccessibility) == FlagPrivate;\n  }\n  bool isProtected() const {\n    return (getFlags() & FlagAccessibility) == FlagProtected;\n  }\n  bool isPublic() const {\n    return (getFlags() & FlagAccessibility) == FlagPublic;\n  }\n  bool isExplicit() const { return getFlags() & FlagExplicit; }\n  bool isPrototyped() const { return getFlags() & FlagPrototyped; }\n  bool areAllCallsDescribed() const {\n    return getFlags() & FlagAllCallsDescribed;\n  }\n  bool isPure() const { return getSPFlags() & SPFlagPure; }\n  bool isElemental() const { return getSPFlags() & SPFlagElemental; }\n  bool isRecursive() const { return getSPFlags() & SPFlagRecursive; }\n  bool isObjCDirect() const { return getSPFlags() & SPFlagObjCDirect; }\n\n  /// Check if this is deleted member function.\n  ///\n  /// Return true if this subprogram is a C++11 special\n  /// member function declared deleted.\n  bool isDeleted() const { return getSPFlags() & SPFlagDeleted; }\n\n  /// Check if this is reference-qualified.\n  ///\n  /// Return true if this subprogram is a C++11 reference-qualified non-static\n  /// member function (void foo() &).\n  bool isLValueReference() const { return getFlags() & FlagLValueReference; }\n\n  /// Check if this is rvalue-reference-qualified.\n  ///\n  /// Return true if this subprogram is a C++11 rvalue-reference-qualified\n  /// non-static member function (void foo() &&).\n  bool isRValueReference() const { return getFlags() & FlagRValueReference; }\n\n  /// Check if this is marked as noreturn.\n  ///\n  /// Return true if this subprogram is C++11 noreturn or C11 _Noreturn\n  bool isNoReturn() const { return getFlags() & FlagNoReturn; }\n\n  // Check if this routine is a compiler-generated thunk.\n  //\n  // Returns true if this subprogram is a thunk generated by the compiler.\n  bool isThunk() const { return getFlags() & FlagThunk; }\n\n  DIScope *getScope() const { return cast_or_null<DIScope>(getRawScope()); }\n\n  StringRef getName() const { return getStringOperand(2); }\n  StringRef getLinkageName() const { return getStringOperand(3); }\n\n  DISubroutineType *getType() const {\n    return cast_or_null<DISubroutineType>(getRawType());\n  }\n  DIType *getContainingType() const {\n    return cast_or_null<DIType>(getRawContainingType());\n  }\n\n  DICompileUnit *getUnit() const {\n    return cast_or_null<DICompileUnit>(getRawUnit());\n  }\n  void replaceUnit(DICompileUnit *CU) { replaceOperandWith(5, CU); }\n  DITemplateParameterArray getTemplateParams() const {\n    return cast_or_null<MDTuple>(getRawTemplateParams());\n  }\n  DISubprogram *getDeclaration() const {\n    return cast_or_null<DISubprogram>(getRawDeclaration());\n  }\n  DINodeArray getRetainedNodes() const {\n    return cast_or_null<MDTuple>(getRawRetainedNodes());\n  }\n  DITypeArray getThrownTypes() const {\n    return cast_or_null<MDTuple>(getRawThrownTypes());\n  }\n\n  Metadata *getRawScope() const { return getOperand(1); }\n  MDString *getRawName() const { return getOperandAs<MDString>(2); }\n  MDString *getRawLinkageName() const { return getOperandAs<MDString>(3); }\n  Metadata *getRawType() const { return getOperand(4); }\n  Metadata *getRawUnit() const { return getOperand(5); }\n  Metadata *getRawDeclaration() const { return getOperand(6); }\n  Metadata *getRawRetainedNodes() const { return getOperand(7); }\n  Metadata *getRawContainingType() const {\n    return getNumOperands() > 8 ? getOperandAs<Metadata>(8) : nullptr;\n  }\n  Metadata *getRawTemplateParams() const {\n    return getNumOperands() > 9 ? getOperandAs<Metadata>(9) : nullptr;\n  }\n  Metadata *getRawThrownTypes() const {\n    return getNumOperands() > 10 ? getOperandAs<Metadata>(10) : nullptr;\n  }\n\n  void replaceRawLinkageName(MDString *LinkageName) {\n    replaceOperandWith(3, LinkageName);\n  }\n\n  /// Check if this subprogram describes the given function.\n  ///\n  /// FIXME: Should this be looking through bitcasts?\n  bool describes(const Function *F) const;\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DISubprogramKind;\n  }\n};\n\nclass DILexicalBlockBase : public DILocalScope {\nprotected:\n  DILexicalBlockBase(LLVMContext &C, unsigned ID, StorageType Storage,\n                     ArrayRef<Metadata *> Ops)\n      : DILocalScope(C, ID, Storage, dwarf::DW_TAG_lexical_block, Ops) {}\n  ~DILexicalBlockBase() = default;\n\npublic:\n  DILocalScope *getScope() const { return cast<DILocalScope>(getRawScope()); }\n\n  Metadata *getRawScope() const { return getOperand(1); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DILexicalBlockKind ||\n           MD->getMetadataID() == DILexicalBlockFileKind;\n  }\n};\n\nclass DILexicalBlock : public DILexicalBlockBase {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Line;\n  uint16_t Column;\n\n  DILexicalBlock(LLVMContext &C, StorageType Storage, unsigned Line,\n                 unsigned Column, ArrayRef<Metadata *> Ops)\n      : DILexicalBlockBase(C, DILexicalBlockKind, Storage, Ops), Line(Line),\n        Column(Column) {\n    assert(Column < (1u << 16) && \"Expected 16-bit column\");\n  }\n  ~DILexicalBlock() = default;\n\n  static DILexicalBlock *getImpl(LLVMContext &Context, DILocalScope *Scope,\n                                 DIFile *File, unsigned Line, unsigned Column,\n                                 StorageType Storage,\n                                 bool ShouldCreate = true) {\n    return getImpl(Context, static_cast<Metadata *>(Scope),\n                   static_cast<Metadata *>(File), Line, Column, Storage,\n                   ShouldCreate);\n  }\n\n  static DILexicalBlock *getImpl(LLVMContext &Context, Metadata *Scope,\n                                 Metadata *File, unsigned Line, unsigned Column,\n                                 StorageType Storage, bool ShouldCreate = true);\n\n  TempDILexicalBlock cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getFile(), getLine(),\n                        getColumn());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DILexicalBlock, (DILocalScope * Scope, DIFile *File,\n                                     unsigned Line, unsigned Column),\n                    (Scope, File, Line, Column))\n  DEFINE_MDNODE_GET(DILexicalBlock, (Metadata * Scope, Metadata *File,\n                                     unsigned Line, unsigned Column),\n                    (Scope, File, Line, Column))\n\n  TempDILexicalBlock clone() const { return cloneImpl(); }\n\n  unsigned getLine() const { return Line; }\n  unsigned getColumn() const { return Column; }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DILexicalBlockKind;\n  }\n};\n\nclass DILexicalBlockFile : public DILexicalBlockBase {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Discriminator;\n\n  DILexicalBlockFile(LLVMContext &C, StorageType Storage,\n                     unsigned Discriminator, ArrayRef<Metadata *> Ops)\n      : DILexicalBlockBase(C, DILexicalBlockFileKind, Storage, Ops),\n        Discriminator(Discriminator) {}\n  ~DILexicalBlockFile() = default;\n\n  static DILexicalBlockFile *getImpl(LLVMContext &Context, DILocalScope *Scope,\n                                     DIFile *File, unsigned Discriminator,\n                                     StorageType Storage,\n                                     bool ShouldCreate = true) {\n    return getImpl(Context, static_cast<Metadata *>(Scope),\n                   static_cast<Metadata *>(File), Discriminator, Storage,\n                   ShouldCreate);\n  }\n\n  static DILexicalBlockFile *getImpl(LLVMContext &Context, Metadata *Scope,\n                                     Metadata *File, unsigned Discriminator,\n                                     StorageType Storage,\n                                     bool ShouldCreate = true);\n\n  TempDILexicalBlockFile cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getFile(),\n                        getDiscriminator());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DILexicalBlockFile, (DILocalScope * Scope, DIFile *File,\n                                         unsigned Discriminator),\n                    (Scope, File, Discriminator))\n  DEFINE_MDNODE_GET(DILexicalBlockFile,\n                    (Metadata * Scope, Metadata *File, unsigned Discriminator),\n                    (Scope, File, Discriminator))\n\n  TempDILexicalBlockFile clone() const { return cloneImpl(); }\n  unsigned getDiscriminator() const { return Discriminator; }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DILexicalBlockFileKind;\n  }\n};\n\nunsigned DILocation::getDiscriminator() const {\n  if (auto *F = dyn_cast<DILexicalBlockFile>(getScope()))\n    return F->getDiscriminator();\n  return 0;\n}\n\nconst DILocation *\nDILocation::cloneWithDiscriminator(unsigned Discriminator) const {\n  DIScope *Scope = getScope();\n  // Skip all parent DILexicalBlockFile that already have a discriminator\n  // assigned. We do not want to have nested DILexicalBlockFiles that have\n  // mutliple discriminators because only the leaf DILexicalBlockFile's\n  // dominator will be used.\n  for (auto *LBF = dyn_cast<DILexicalBlockFile>(Scope);\n       LBF && LBF->getDiscriminator() != 0;\n       LBF = dyn_cast<DILexicalBlockFile>(Scope))\n    Scope = LBF->getScope();\n  DILexicalBlockFile *NewScope =\n      DILexicalBlockFile::get(getContext(), Scope, getFile(), Discriminator);\n  return DILocation::get(getContext(), getLine(), getColumn(), NewScope,\n                         getInlinedAt());\n}\n\nunsigned DILocation::getBaseDiscriminator() const {\n  return getBaseDiscriminatorFromDiscriminator(getDiscriminator());\n}\n\nunsigned DILocation::getDuplicationFactor() const {\n  return getDuplicationFactorFromDiscriminator(getDiscriminator());\n}\n\nunsigned DILocation::getCopyIdentifier() const {\n  return getCopyIdentifierFromDiscriminator(getDiscriminator());\n}\n\nOptional<const DILocation *> DILocation::cloneWithBaseDiscriminator(unsigned D) const {\n  unsigned BD, DF, CI;\n  decodeDiscriminator(getDiscriminator(), BD, DF, CI);\n  if (D == BD)\n    return this;\n  if (Optional<unsigned> Encoded = encodeDiscriminator(D, DF, CI))\n    return cloneWithDiscriminator(*Encoded);\n  return None;\n}\n\nOptional<const DILocation *> DILocation::cloneByMultiplyingDuplicationFactor(unsigned DF) const {\n  DF *= getDuplicationFactor();\n  if (DF <= 1)\n    return this;\n\n  unsigned BD = getBaseDiscriminator();\n  unsigned CI = getCopyIdentifier();\n  if (Optional<unsigned> D = encodeDiscriminator(BD, DF, CI))\n    return cloneWithDiscriminator(*D);\n  return None;\n}\n\nclass DINamespace : public DIScope {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned ExportSymbols : 1;\n\n  DINamespace(LLVMContext &Context, StorageType Storage, bool ExportSymbols,\n              ArrayRef<Metadata *> Ops)\n      : DIScope(Context, DINamespaceKind, Storage, dwarf::DW_TAG_namespace,\n                Ops),\n        ExportSymbols(ExportSymbols) {}\n  ~DINamespace() = default;\n\n  static DINamespace *getImpl(LLVMContext &Context, DIScope *Scope,\n                              StringRef Name, bool ExportSymbols,\n                              StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Scope, getCanonicalMDString(Context, Name),\n                   ExportSymbols, Storage, ShouldCreate);\n  }\n  static DINamespace *getImpl(LLVMContext &Context, Metadata *Scope,\n                              MDString *Name, bool ExportSymbols,\n                              StorageType Storage, bool ShouldCreate = true);\n\n  TempDINamespace cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getName(),\n                        getExportSymbols());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DINamespace,\n                    (DIScope *Scope, StringRef Name, bool ExportSymbols),\n                    (Scope, Name, ExportSymbols))\n  DEFINE_MDNODE_GET(DINamespace,\n                    (Metadata *Scope, MDString *Name, bool ExportSymbols),\n                    (Scope, Name, ExportSymbols))\n\n  TempDINamespace clone() const { return cloneImpl(); }\n\n  bool getExportSymbols() const { return ExportSymbols; }\n  DIScope *getScope() const { return cast_or_null<DIScope>(getRawScope()); }\n  StringRef getName() const { return getStringOperand(2); }\n\n  Metadata *getRawScope() const { return getOperand(1); }\n  MDString *getRawName() const { return getOperandAs<MDString>(2); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DINamespaceKind;\n  }\n};\n\n/// Represents a module in the programming language, for example, a Clang\n/// module, or a Fortran module.\nclass DIModule : public DIScope {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n  unsigned LineNo;\n  bool IsDecl;\n\n  DIModule(LLVMContext &Context, StorageType Storage, unsigned LineNo,\n           bool IsDecl, ArrayRef<Metadata *> Ops)\n      : DIScope(Context, DIModuleKind, Storage, dwarf::DW_TAG_module, Ops),\n        LineNo(LineNo), IsDecl(IsDecl) {}\n  ~DIModule() = default;\n\n  static DIModule *getImpl(LLVMContext &Context, DIFile *File, DIScope *Scope,\n                           StringRef Name, StringRef ConfigurationMacros,\n                           StringRef IncludePath, StringRef APINotesFile,\n                           unsigned LineNo, bool IsDecl, StorageType Storage,\n                           bool ShouldCreate = true) {\n    return getImpl(Context, File, Scope, getCanonicalMDString(Context, Name),\n                   getCanonicalMDString(Context, ConfigurationMacros),\n                   getCanonicalMDString(Context, IncludePath),\n                   getCanonicalMDString(Context, APINotesFile), LineNo, IsDecl,\n                   Storage, ShouldCreate);\n  }\n  static DIModule *getImpl(LLVMContext &Context, Metadata *File,\n                           Metadata *Scope, MDString *Name,\n                           MDString *ConfigurationMacros, MDString *IncludePath,\n                           MDString *APINotesFile, unsigned LineNo, bool IsDecl,\n                           StorageType Storage, bool ShouldCreate = true);\n\n  TempDIModule cloneImpl() const {\n    return getTemporary(getContext(), getFile(), getScope(), getName(),\n                        getConfigurationMacros(), getIncludePath(),\n                        getAPINotesFile(), getLineNo(), getIsDecl());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIModule,\n                    (DIFile * File, DIScope *Scope, StringRef Name,\n                     StringRef ConfigurationMacros, StringRef IncludePath,\n                     StringRef APINotesFile, unsigned LineNo,\n                     bool IsDecl = false),\n                    (File, Scope, Name, ConfigurationMacros, IncludePath,\n                     APINotesFile, LineNo, IsDecl))\n  DEFINE_MDNODE_GET(DIModule,\n                    (Metadata * File, Metadata *Scope, MDString *Name,\n                     MDString *ConfigurationMacros, MDString *IncludePath,\n                     MDString *APINotesFile, unsigned LineNo,\n                     bool IsDecl = false),\n                    (File, Scope, Name, ConfigurationMacros, IncludePath,\n                     APINotesFile, LineNo, IsDecl))\n\n  TempDIModule clone() const { return cloneImpl(); }\n\n  DIScope *getScope() const { return cast_or_null<DIScope>(getRawScope()); }\n  StringRef getName() const { return getStringOperand(2); }\n  StringRef getConfigurationMacros() const { return getStringOperand(3); }\n  StringRef getIncludePath() const { return getStringOperand(4); }\n  StringRef getAPINotesFile() const { return getStringOperand(5); }\n  unsigned getLineNo() const { return LineNo; }\n  bool getIsDecl() const { return IsDecl; }\n\n  Metadata *getRawScope() const { return getOperand(1); }\n  MDString *getRawName() const { return getOperandAs<MDString>(2); }\n  MDString *getRawConfigurationMacros() const {\n    return getOperandAs<MDString>(3);\n  }\n  MDString *getRawIncludePath() const { return getOperandAs<MDString>(4); }\n  MDString *getRawAPINotesFile() const { return getOperandAs<MDString>(5); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIModuleKind;\n  }\n};\n\n/// Base class for template parameters.\nclass DITemplateParameter : public DINode {\nprotected:\n  bool IsDefault;\n\n  DITemplateParameter(LLVMContext &Context, unsigned ID, StorageType Storage,\n                      unsigned Tag, bool IsDefault, ArrayRef<Metadata *> Ops)\n      : DINode(Context, ID, Storage, Tag, Ops), IsDefault(IsDefault) {}\n  ~DITemplateParameter() = default;\n\npublic:\n  StringRef getName() const { return getStringOperand(0); }\n  DIType *getType() const { return cast_or_null<DIType>(getRawType()); }\n\n  MDString *getRawName() const { return getOperandAs<MDString>(0); }\n  Metadata *getRawType() const { return getOperand(1); }\n  bool isDefault() const { return IsDefault; }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DITemplateTypeParameterKind ||\n           MD->getMetadataID() == DITemplateValueParameterKind;\n  }\n};\n\nclass DITemplateTypeParameter : public DITemplateParameter {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  DITemplateTypeParameter(LLVMContext &Context, StorageType Storage,\n                          bool IsDefault, ArrayRef<Metadata *> Ops)\n      : DITemplateParameter(Context, DITemplateTypeParameterKind, Storage,\n                            dwarf::DW_TAG_template_type_parameter, IsDefault,\n                            Ops) {}\n  ~DITemplateTypeParameter() = default;\n\n  static DITemplateTypeParameter *getImpl(LLVMContext &Context, StringRef Name,\n                                          DIType *Type, bool IsDefault,\n                                          StorageType Storage,\n                                          bool ShouldCreate = true) {\n    return getImpl(Context, getCanonicalMDString(Context, Name), Type,\n                   IsDefault, Storage, ShouldCreate);\n  }\n  static DITemplateTypeParameter *getImpl(LLVMContext &Context, MDString *Name,\n                                          Metadata *Type, bool IsDefault,\n                                          StorageType Storage,\n                                          bool ShouldCreate = true);\n\n  TempDITemplateTypeParameter cloneImpl() const {\n    return getTemporary(getContext(), getName(), getType(), isDefault());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DITemplateTypeParameter,\n                    (StringRef Name, DIType *Type, bool IsDefault),\n                    (Name, Type, IsDefault))\n  DEFINE_MDNODE_GET(DITemplateTypeParameter,\n                    (MDString *Name, Metadata *Type, bool IsDefault),\n                    (Name, Type, IsDefault))\n\n  TempDITemplateTypeParameter clone() const { return cloneImpl(); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DITemplateTypeParameterKind;\n  }\n};\n\nclass DITemplateValueParameter : public DITemplateParameter {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  DITemplateValueParameter(LLVMContext &Context, StorageType Storage,\n                           unsigned Tag, bool IsDefault,\n                           ArrayRef<Metadata *> Ops)\n      : DITemplateParameter(Context, DITemplateValueParameterKind, Storage, Tag,\n                            IsDefault, Ops) {}\n  ~DITemplateValueParameter() = default;\n\n  static DITemplateValueParameter *getImpl(LLVMContext &Context, unsigned Tag,\n                                           StringRef Name, DIType *Type,\n                                           bool IsDefault, Metadata *Value,\n                                           StorageType Storage,\n                                           bool ShouldCreate = true) {\n    return getImpl(Context, Tag, getCanonicalMDString(Context, Name), Type,\n                   IsDefault, Value, Storage, ShouldCreate);\n  }\n  static DITemplateValueParameter *getImpl(LLVMContext &Context, unsigned Tag,\n                                           MDString *Name, Metadata *Type,\n                                           bool IsDefault, Metadata *Value,\n                                           StorageType Storage,\n                                           bool ShouldCreate = true);\n\n  TempDITemplateValueParameter cloneImpl() const {\n    return getTemporary(getContext(), getTag(), getName(), getType(),\n                        isDefault(), getValue());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DITemplateValueParameter,\n                    (unsigned Tag, StringRef Name, DIType *Type, bool IsDefault,\n                     Metadata *Value),\n                    (Tag, Name, Type, IsDefault, Value))\n  DEFINE_MDNODE_GET(DITemplateValueParameter,\n                    (unsigned Tag, MDString *Name, Metadata *Type,\n                     bool IsDefault, Metadata *Value),\n                    (Tag, Name, Type, IsDefault, Value))\n\n  TempDITemplateValueParameter clone() const { return cloneImpl(); }\n\n  Metadata *getValue() const { return getOperand(2); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DITemplateValueParameterKind;\n  }\n};\n\n/// Base class for variables.\nclass DIVariable : public DINode {\n  unsigned Line;\n  uint32_t AlignInBits;\n\nprotected:\n  DIVariable(LLVMContext &C, unsigned ID, StorageType Storage, unsigned Line,\n             ArrayRef<Metadata *> Ops, uint32_t AlignInBits = 0)\n      : DINode(C, ID, Storage, dwarf::DW_TAG_variable, Ops), Line(Line),\n        AlignInBits(AlignInBits) {}\n  ~DIVariable() = default;\n\npublic:\n  unsigned getLine() const { return Line; }\n  DIScope *getScope() const { return cast_or_null<DIScope>(getRawScope()); }\n  StringRef getName() const { return getStringOperand(1); }\n  DIFile *getFile() const { return cast_or_null<DIFile>(getRawFile()); }\n  DIType *getType() const { return cast_or_null<DIType>(getRawType()); }\n  uint32_t getAlignInBits() const { return AlignInBits; }\n  uint32_t getAlignInBytes() const { return getAlignInBits() / CHAR_BIT; }\n  /// Determines the size of the variable's type.\n  Optional<uint64_t> getSizeInBits() const;\n\n  /// Return the signedness of this variable's type, or None if this type is\n  /// neither signed nor unsigned.\n  Optional<DIBasicType::Signedness> getSignedness() const {\n    if (auto *BT = dyn_cast<DIBasicType>(getType()))\n      return BT->getSignedness();\n    return None;\n  }\n\n  StringRef getFilename() const {\n    if (auto *F = getFile())\n      return F->getFilename();\n    return \"\";\n  }\n\n  StringRef getDirectory() const {\n    if (auto *F = getFile())\n      return F->getDirectory();\n    return \"\";\n  }\n\n  Optional<StringRef> getSource() const {\n    if (auto *F = getFile())\n      return F->getSource();\n    return None;\n  }\n\n  Metadata *getRawScope() const { return getOperand(0); }\n  MDString *getRawName() const { return getOperandAs<MDString>(1); }\n  Metadata *getRawFile() const { return getOperand(2); }\n  Metadata *getRawType() const { return getOperand(3); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DILocalVariableKind ||\n           MD->getMetadataID() == DIGlobalVariableKind;\n  }\n};\n\n/// DWARF expression.\n///\n/// This is (almost) a DWARF expression that modifies the location of a\n/// variable, or the location of a single piece of a variable, or (when using\n/// DW_OP_stack_value) is the constant variable value.\n///\n/// TODO: Co-allocate the expression elements.\n/// TODO: Separate from MDNode, or otherwise drop Distinct and Temporary\n/// storage types.\nclass DIExpression : public MDNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  std::vector<uint64_t> Elements;\n\n  DIExpression(LLVMContext &C, StorageType Storage, ArrayRef<uint64_t> Elements)\n      : MDNode(C, DIExpressionKind, Storage, None),\n        Elements(Elements.begin(), Elements.end()) {}\n  ~DIExpression() = default;\n\n  static DIExpression *getImpl(LLVMContext &Context,\n                               ArrayRef<uint64_t> Elements, StorageType Storage,\n                               bool ShouldCreate = true);\n\n  TempDIExpression cloneImpl() const {\n    return getTemporary(getContext(), getElements());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIExpression, (ArrayRef<uint64_t> Elements), (Elements))\n\n  TempDIExpression clone() const { return cloneImpl(); }\n\n  ArrayRef<uint64_t> getElements() const { return Elements; }\n\n  unsigned getNumElements() const { return Elements.size(); }\n\n  uint64_t getElement(unsigned I) const {\n    assert(I < Elements.size() && \"Index out of range\");\n    return Elements[I];\n  }\n\n  /// Determine whether this represents a standalone constant value.\n  bool isConstant() const;\n\n  /// Determine whether this represents a standalone signed constant value.\n  bool isSignedConstant() const;\n\n  using element_iterator = ArrayRef<uint64_t>::iterator;\n\n  element_iterator elements_begin() const { return getElements().begin(); }\n  element_iterator elements_end() const { return getElements().end(); }\n\n  /// A lightweight wrapper around an expression operand.\n  ///\n  /// TODO: Store arguments directly and change \\a DIExpression to store a\n  /// range of these.\n  class ExprOperand {\n    const uint64_t *Op = nullptr;\n\n  public:\n    ExprOperand() = default;\n    explicit ExprOperand(const uint64_t *Op) : Op(Op) {}\n\n    const uint64_t *get() const { return Op; }\n\n    /// Get the operand code.\n    uint64_t getOp() const { return *Op; }\n\n    /// Get an argument to the operand.\n    ///\n    /// Never returns the operand itself.\n    uint64_t getArg(unsigned I) const { return Op[I + 1]; }\n\n    unsigned getNumArgs() const { return getSize() - 1; }\n\n    /// Return the size of the operand.\n    ///\n    /// Return the number of elements in the operand (1 + args).\n    unsigned getSize() const;\n\n    /// Append the elements of this operand to \\p V.\n    void appendToVector(SmallVectorImpl<uint64_t> &V) const {\n      V.append(get(), get() + getSize());\n    }\n  };\n\n  /// An iterator for expression operands.\n  class expr_op_iterator\n      : public std::iterator<std::input_iterator_tag, ExprOperand> {\n    ExprOperand Op;\n\n  public:\n    expr_op_iterator() = default;\n    explicit expr_op_iterator(element_iterator I) : Op(I) {}\n\n    element_iterator getBase() const { return Op.get(); }\n    const ExprOperand &operator*() const { return Op; }\n    const ExprOperand *operator->() const { return &Op; }\n\n    expr_op_iterator &operator++() {\n      increment();\n      return *this;\n    }\n    expr_op_iterator operator++(int) {\n      expr_op_iterator T(*this);\n      increment();\n      return T;\n    }\n\n    /// Get the next iterator.\n    ///\n    /// \\a std::next() doesn't work because this is technically an\n    /// input_iterator, but it's a perfectly valid operation.  This is an\n    /// accessor to provide the same functionality.\n    expr_op_iterator getNext() const { return ++expr_op_iterator(*this); }\n\n    bool operator==(const expr_op_iterator &X) const {\n      return getBase() == X.getBase();\n    }\n    bool operator!=(const expr_op_iterator &X) const {\n      return getBase() != X.getBase();\n    }\n\n  private:\n    void increment() { Op = ExprOperand(getBase() + Op.getSize()); }\n  };\n\n  /// Visit the elements via ExprOperand wrappers.\n  ///\n  /// These range iterators visit elements through \\a ExprOperand wrappers.\n  /// This is not guaranteed to be a valid range unless \\a isValid() gives \\c\n  /// true.\n  ///\n  /// \\pre \\a isValid() gives \\c true.\n  /// @{\n  expr_op_iterator expr_op_begin() const {\n    return expr_op_iterator(elements_begin());\n  }\n  expr_op_iterator expr_op_end() const {\n    return expr_op_iterator(elements_end());\n  }\n  iterator_range<expr_op_iterator> expr_ops() const {\n    return {expr_op_begin(), expr_op_end()};\n  }\n  /// @}\n\n  bool isValid() const;\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIExpressionKind;\n  }\n\n  /// Return whether the first element a DW_OP_deref.\n  bool startsWithDeref() const {\n    return getNumElements() > 0 && getElement(0) == dwarf::DW_OP_deref;\n  }\n\n  /// Holds the characteristics of one fragment of a larger variable.\n  struct FragmentInfo {\n    uint64_t SizeInBits;\n    uint64_t OffsetInBits;\n  };\n\n  /// Retrieve the details of this fragment expression.\n  static Optional<FragmentInfo> getFragmentInfo(expr_op_iterator Start,\n                                                expr_op_iterator End);\n\n  /// Retrieve the details of this fragment expression.\n  Optional<FragmentInfo> getFragmentInfo() const {\n    return getFragmentInfo(expr_op_begin(), expr_op_end());\n  }\n\n  /// Return whether this is a piece of an aggregate variable.\n  bool isFragment() const { return getFragmentInfo().hasValue(); }\n\n  /// Return whether this is an implicit location description.\n  bool isImplicit() const;\n\n  /// Return whether the location is computed on the expression stack, meaning\n  /// it cannot be a simple register location.\n  bool isComplex() const;\n\n  /// Append \\p Ops with operations to apply the \\p Offset.\n  static void appendOffset(SmallVectorImpl<uint64_t> &Ops, int64_t Offset);\n\n  /// If this is a constant offset, extract it. If there is no expression,\n  /// return true with an offset of zero.\n  bool extractIfOffset(int64_t &Offset) const;\n\n  /// Checks if the last 4 elements of the expression are DW_OP_constu <DWARF\n  /// Address Space> DW_OP_swap DW_OP_xderef and extracts the <DWARF Address\n  /// Space>.\n  static const DIExpression *extractAddressClass(const DIExpression *Expr,\n                                                 unsigned &AddrClass);\n\n  /// Used for DIExpression::prepend.\n  enum PrependOps : uint8_t {\n    ApplyOffset = 0,\n    DerefBefore = 1 << 0,\n    DerefAfter = 1 << 1,\n    StackValue = 1 << 2,\n    EntryValue = 1 << 3\n  };\n\n  /// Prepend \\p DIExpr with a deref and offset operation and optionally turn it\n  /// into a stack value or/and an entry value.\n  static DIExpression *prepend(const DIExpression *Expr, uint8_t Flags,\n                               int64_t Offset = 0);\n\n  /// Prepend \\p DIExpr with the given opcodes and optionally turn it into a\n  /// stack value.\n  static DIExpression *prependOpcodes(const DIExpression *Expr,\n                                      SmallVectorImpl<uint64_t> &Ops,\n                                      bool StackValue = false,\n                                      bool EntryValue = false);\n\n  /// Append the opcodes \\p Ops to \\p DIExpr. Unlike \\ref appendToStack, the\n  /// returned expression is a stack value only if \\p DIExpr is a stack value.\n  /// If \\p DIExpr describes a fragment, the returned expression will describe\n  /// the same fragment.\n  static DIExpression *append(const DIExpression *Expr, ArrayRef<uint64_t> Ops);\n\n  /// Convert \\p DIExpr into a stack value if it isn't one already by appending\n  /// DW_OP_deref if needed, and appending \\p Ops to the resulting expression.\n  /// If \\p DIExpr describes a fragment, the returned expression will describe\n  /// the same fragment.\n  static DIExpression *appendToStack(const DIExpression *Expr,\n                                     ArrayRef<uint64_t> Ops);\n\n  /// Create a copy of \\p Expr by appending the given list of \\p Ops to each\n  /// instance of the operand `DW_OP_LLVM_arg, \\p ArgNo`. This is used to\n  /// modify a specific location used by \\p Expr, such as when salvaging that\n  /// location.\n  static DIExpression *appendOpsToArg(const DIExpression *Expr,\n                                      ArrayRef<uint64_t> Ops, unsigned ArgNo,\n                                      bool StackValue = false);\n\n  /// Create a copy of \\p Expr with each instance of\n  /// `DW_OP_LLVM_arg, \\p OldArg` replaced with `DW_OP_LLVM_arg, \\p NewArg`,\n  /// and each instance of `DW_OP_LLVM_arg, Arg` with `DW_OP_LLVM_arg, Arg - 1`\n  /// for all Arg > \\p OldArg.\n  /// This is used when replacing one of the operands of a debug value list\n  /// with another operand in the same list and deleting the old operand.\n  static DIExpression *replaceArg(const DIExpression *Expr, uint64_t OldArg,\n                                  uint64_t NewArg);\n\n  /// Create a DIExpression to describe one part of an aggregate variable that\n  /// is fragmented across multiple Values. The DW_OP_LLVM_fragment operation\n  /// will be appended to the elements of \\c Expr. If \\c Expr already contains\n  /// a \\c DW_OP_LLVM_fragment \\c OffsetInBits is interpreted as an offset\n  /// into the existing fragment.\n  ///\n  /// \\param OffsetInBits Offset of the piece in bits.\n  /// \\param SizeInBits   Size of the piece in bits.\n  /// \\return             Creating a fragment expression may fail if \\c Expr\n  ///                     contains arithmetic operations that would be truncated.\n  static Optional<DIExpression *>\n  createFragmentExpression(const DIExpression *Expr, unsigned OffsetInBits,\n                           unsigned SizeInBits);\n\n  /// Determine the relative position of the fragments passed in.\n  /// Returns -1 if this is entirely before Other, 0 if this and Other overlap,\n  /// 1 if this is entirely after Other.\n  static int fragmentCmp(const FragmentInfo &A, const FragmentInfo &B) {\n    uint64_t l1 = A.OffsetInBits;\n    uint64_t l2 = B.OffsetInBits;\n    uint64_t r1 = l1 + A.SizeInBits;\n    uint64_t r2 = l2 + B.SizeInBits;\n    if (r1 <= l2)\n      return -1;\n    else if (r2 <= l1)\n      return 1;\n    else\n      return 0;\n  }\n\n  using ExtOps = std::array<uint64_t, 6>;\n\n  /// Returns the ops for a zero- or sign-extension in a DIExpression.\n  static ExtOps getExtOps(unsigned FromSize, unsigned ToSize, bool Signed);\n\n  /// Append a zero- or sign-extension to \\p Expr. Converts the expression to a\n  /// stack value if it isn't one already.\n  static DIExpression *appendExt(const DIExpression *Expr, unsigned FromSize,\n                                 unsigned ToSize, bool Signed);\n\n  /// Check if fragments overlap between a pair of FragmentInfos.\n  static bool fragmentsOverlap(const FragmentInfo &A, const FragmentInfo &B) {\n    return fragmentCmp(A, B) == 0;\n  }\n\n  /// Determine the relative position of the fragments described by this\n  /// DIExpression and \\p Other. Calls static fragmentCmp implementation.\n  int fragmentCmp(const DIExpression *Other) const {\n    auto Fragment1 = *getFragmentInfo();\n    auto Fragment2 = *Other->getFragmentInfo();\n    return fragmentCmp(Fragment1, Fragment2);\n  }\n\n  /// Check if fragments overlap between this DIExpression and \\p Other.\n  bool fragmentsOverlap(const DIExpression *Other) const {\n    if (!isFragment() || !Other->isFragment())\n      return true;\n    return fragmentCmp(Other) == 0;\n  }\n\n  /// Check if the expression consists of exactly one entry value operand.\n  /// (This is the only configuration of entry values that is supported.)\n  bool isEntryValue() const {\n    return getNumElements() > 0 &&\n           getElement(0) == dwarf::DW_OP_LLVM_entry_value;\n  }\n};\n\ninline bool operator==(const DIExpression::FragmentInfo &A,\n                       const DIExpression::FragmentInfo &B) {\n  return std::tie(A.SizeInBits, A.OffsetInBits) ==\n         std::tie(B.SizeInBits, B.OffsetInBits);\n}\n\ninline bool operator<(const DIExpression::FragmentInfo &A,\n                      const DIExpression::FragmentInfo &B) {\n  return std::tie(A.SizeInBits, A.OffsetInBits) <\n         std::tie(B.SizeInBits, B.OffsetInBits);\n}\n\ntemplate <> struct DenseMapInfo<DIExpression::FragmentInfo> {\n  using FragInfo = DIExpression::FragmentInfo;\n  static const uint64_t MaxVal = std::numeric_limits<uint64_t>::max();\n\n  static inline FragInfo getEmptyKey() { return {MaxVal, MaxVal}; }\n\n  static inline FragInfo getTombstoneKey() { return {MaxVal - 1, MaxVal - 1}; }\n\n  static unsigned getHashValue(const FragInfo &Frag) {\n    return (Frag.SizeInBits & 0xffff) << 16 | (Frag.OffsetInBits & 0xffff);\n  }\n\n  static bool isEqual(const FragInfo &A, const FragInfo &B) { return A == B; }\n};\n\n/// Global variables.\n///\n/// TODO: Remove DisplayName.  It's always equal to Name.\nclass DIGlobalVariable : public DIVariable {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  bool IsLocalToUnit;\n  bool IsDefinition;\n\n  DIGlobalVariable(LLVMContext &C, StorageType Storage, unsigned Line,\n                   bool IsLocalToUnit, bool IsDefinition, uint32_t AlignInBits,\n                   ArrayRef<Metadata *> Ops)\n      : DIVariable(C, DIGlobalVariableKind, Storage, Line, Ops, AlignInBits),\n        IsLocalToUnit(IsLocalToUnit), IsDefinition(IsDefinition) {}\n  ~DIGlobalVariable() = default;\n\n  static DIGlobalVariable *\n  getImpl(LLVMContext &Context, DIScope *Scope, StringRef Name,\n          StringRef LinkageName, DIFile *File, unsigned Line, DIType *Type,\n          bool IsLocalToUnit, bool IsDefinition,\n          DIDerivedType *StaticDataMemberDeclaration, MDTuple *TemplateParams,\n          uint32_t AlignInBits, StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, Scope, getCanonicalMDString(Context, Name),\n                   getCanonicalMDString(Context, LinkageName), File, Line, Type,\n                   IsLocalToUnit, IsDefinition, StaticDataMemberDeclaration,\n                   cast_or_null<Metadata>(TemplateParams), AlignInBits, Storage,\n                   ShouldCreate);\n  }\n  static DIGlobalVariable *\n  getImpl(LLVMContext &Context, Metadata *Scope, MDString *Name,\n          MDString *LinkageName, Metadata *File, unsigned Line, Metadata *Type,\n          bool IsLocalToUnit, bool IsDefinition,\n          Metadata *StaticDataMemberDeclaration, Metadata *TemplateParams,\n          uint32_t AlignInBits, StorageType Storage, bool ShouldCreate = true);\n\n  TempDIGlobalVariable cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getName(), getLinkageName(),\n                        getFile(), getLine(), getType(), isLocalToUnit(),\n                        isDefinition(), getStaticDataMemberDeclaration(),\n                        getTemplateParams(), getAlignInBits());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIGlobalVariable,\n                    (DIScope * Scope, StringRef Name, StringRef LinkageName,\n                     DIFile *File, unsigned Line, DIType *Type,\n                     bool IsLocalToUnit, bool IsDefinition,\n                     DIDerivedType *StaticDataMemberDeclaration,\n                     MDTuple *TemplateParams, uint32_t AlignInBits),\n                    (Scope, Name, LinkageName, File, Line, Type, IsLocalToUnit,\n                     IsDefinition, StaticDataMemberDeclaration, TemplateParams,\n                     AlignInBits))\n  DEFINE_MDNODE_GET(DIGlobalVariable,\n                    (Metadata * Scope, MDString *Name, MDString *LinkageName,\n                     Metadata *File, unsigned Line, Metadata *Type,\n                     bool IsLocalToUnit, bool IsDefinition,\n                     Metadata *StaticDataMemberDeclaration,\n                     Metadata *TemplateParams, uint32_t AlignInBits),\n                    (Scope, Name, LinkageName, File, Line, Type, IsLocalToUnit,\n                     IsDefinition, StaticDataMemberDeclaration, TemplateParams,\n                     AlignInBits))\n\n  TempDIGlobalVariable clone() const { return cloneImpl(); }\n\n  bool isLocalToUnit() const { return IsLocalToUnit; }\n  bool isDefinition() const { return IsDefinition; }\n  StringRef getDisplayName() const { return getStringOperand(4); }\n  StringRef getLinkageName() const { return getStringOperand(5); }\n  DIDerivedType *getStaticDataMemberDeclaration() const {\n    return cast_or_null<DIDerivedType>(getRawStaticDataMemberDeclaration());\n  }\n\n  MDString *getRawLinkageName() const { return getOperandAs<MDString>(5); }\n  Metadata *getRawStaticDataMemberDeclaration() const { return getOperand(6); }\n  Metadata *getRawTemplateParams() const { return getOperand(7); }\n  MDTuple *getTemplateParams() const { return getOperandAs<MDTuple>(7); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIGlobalVariableKind;\n  }\n};\n\nclass DICommonBlock : public DIScope {\n  unsigned LineNo;\n\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  DICommonBlock(LLVMContext &Context, StorageType Storage, unsigned LineNo,\n                ArrayRef<Metadata *> Ops)\n      : DIScope(Context, DICommonBlockKind, Storage, dwarf::DW_TAG_common_block,\n                Ops), LineNo(LineNo) {}\n\n  static DICommonBlock *getImpl(LLVMContext &Context, DIScope *Scope,\n                                DIGlobalVariable *Decl, StringRef Name,\n                                DIFile *File, unsigned LineNo,\n                                StorageType Storage,\n                                bool ShouldCreate = true) {\n    return getImpl(Context, Scope, Decl, getCanonicalMDString(Context, Name),\n                   File, LineNo, Storage, ShouldCreate);\n  }\n  static DICommonBlock *getImpl(LLVMContext &Context, Metadata *Scope,\n                                Metadata *Decl, MDString *Name, Metadata *File,\n                                unsigned LineNo, \n                                StorageType Storage, bool ShouldCreate = true);\n\n  TempDICommonBlock cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getDecl(), getName(),\n                        getFile(), getLineNo());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DICommonBlock,\n                    (DIScope *Scope, DIGlobalVariable *Decl, StringRef Name,\n                     DIFile *File, unsigned LineNo),\n                    (Scope, Decl, Name, File, LineNo))\n  DEFINE_MDNODE_GET(DICommonBlock,\n                    (Metadata *Scope, Metadata *Decl, MDString *Name,\n                     Metadata *File, unsigned LineNo),\n                    (Scope, Decl, Name, File, LineNo))\n\n  TempDICommonBlock clone() const { return cloneImpl(); }\n\n  DIScope *getScope() const { return cast_or_null<DIScope>(getRawScope()); }\n  DIGlobalVariable *getDecl() const {\n    return cast_or_null<DIGlobalVariable>(getRawDecl());\n  }\n  StringRef getName() const { return getStringOperand(2); }\n  DIFile *getFile() const { return cast_or_null<DIFile>(getRawFile()); }\n  unsigned getLineNo() const { return LineNo; }\n\n  Metadata *getRawScope() const { return getOperand(0); }\n  Metadata *getRawDecl() const { return getOperand(1); }\n  MDString *getRawName() const { return getOperandAs<MDString>(2); }\n  Metadata *getRawFile() const { return getOperand(3); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DICommonBlockKind;\n  }\n};\n\n/// Local variable.\n///\n/// TODO: Split up flags.\nclass DILocalVariable : public DIVariable {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Arg : 16;\n  DIFlags Flags;\n\n  DILocalVariable(LLVMContext &C, StorageType Storage, unsigned Line,\n                  unsigned Arg, DIFlags Flags, uint32_t AlignInBits,\n                  ArrayRef<Metadata *> Ops)\n      : DIVariable(C, DILocalVariableKind, Storage, Line, Ops, AlignInBits),\n        Arg(Arg), Flags(Flags) {\n    assert(Arg < (1 << 16) && \"DILocalVariable: Arg out of range\");\n  }\n  ~DILocalVariable() = default;\n\n  static DILocalVariable *getImpl(LLVMContext &Context, DIScope *Scope,\n                                  StringRef Name, DIFile *File, unsigned Line,\n                                  DIType *Type, unsigned Arg, DIFlags Flags,\n                                  uint32_t AlignInBits, StorageType Storage,\n                                  bool ShouldCreate = true) {\n    return getImpl(Context, Scope, getCanonicalMDString(Context, Name), File,\n                   Line, Type, Arg, Flags, AlignInBits, Storage, ShouldCreate);\n  }\n  static DILocalVariable *getImpl(LLVMContext &Context, Metadata *Scope,\n                                  MDString *Name, Metadata *File, unsigned Line,\n                                  Metadata *Type, unsigned Arg, DIFlags Flags,\n                                  uint32_t AlignInBits, StorageType Storage,\n                                  bool ShouldCreate = true);\n\n  TempDILocalVariable cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getName(), getFile(),\n                        getLine(), getType(), getArg(), getFlags(),\n                        getAlignInBits());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DILocalVariable,\n                    (DILocalScope * Scope, StringRef Name, DIFile *File,\n                     unsigned Line, DIType *Type, unsigned Arg, DIFlags Flags,\n                     uint32_t AlignInBits),\n                    (Scope, Name, File, Line, Type, Arg, Flags, AlignInBits))\n  DEFINE_MDNODE_GET(DILocalVariable,\n                    (Metadata * Scope, MDString *Name, Metadata *File,\n                     unsigned Line, Metadata *Type, unsigned Arg,\n                     DIFlags Flags, uint32_t AlignInBits),\n                    (Scope, Name, File, Line, Type, Arg, Flags, AlignInBits))\n\n  TempDILocalVariable clone() const { return cloneImpl(); }\n\n  /// Get the local scope for this variable.\n  ///\n  /// Variables must be defined in a local scope.\n  DILocalScope *getScope() const {\n    return cast<DILocalScope>(DIVariable::getScope());\n  }\n\n  bool isParameter() const { return Arg; }\n  unsigned getArg() const { return Arg; }\n  DIFlags getFlags() const { return Flags; }\n\n  bool isArtificial() const { return getFlags() & FlagArtificial; }\n  bool isObjectPointer() const { return getFlags() & FlagObjectPointer; }\n\n  /// Check that a location is valid for this variable.\n  ///\n  /// Check that \\c DL exists, is in the same subprogram, and has the same\n  /// inlined-at location as \\c this.  (Otherwise, it's not a valid attachment\n  /// to a \\a DbgInfoIntrinsic.)\n  bool isValidLocationForIntrinsic(const DILocation *DL) const {\n    return DL && getScope()->getSubprogram() == DL->getScope()->getSubprogram();\n  }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DILocalVariableKind;\n  }\n};\n\n/// Label.\n///\nclass DILabel : public DINode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Line;\n\n  DILabel(LLVMContext &C, StorageType Storage, unsigned Line,\n          ArrayRef<Metadata *> Ops)\n      : DINode(C, DILabelKind, Storage, dwarf::DW_TAG_label, Ops), Line(Line) {}\n  ~DILabel() = default;\n\n  static DILabel *getImpl(LLVMContext &Context, DIScope *Scope,\n                          StringRef Name, DIFile *File, unsigned Line,\n                          StorageType Storage,\n                          bool ShouldCreate = true) {\n    return getImpl(Context, Scope, getCanonicalMDString(Context, Name), File,\n                   Line, Storage, ShouldCreate);\n  }\n  static DILabel *getImpl(LLVMContext &Context, Metadata *Scope,\n                          MDString *Name, Metadata *File, unsigned Line,\n                          StorageType Storage,\n                          bool ShouldCreate = true);\n\n  TempDILabel cloneImpl() const {\n    return getTemporary(getContext(), getScope(), getName(), getFile(),\n                        getLine());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DILabel,\n                    (DILocalScope * Scope, StringRef Name, DIFile *File,\n                     unsigned Line),\n                    (Scope, Name, File, Line))\n  DEFINE_MDNODE_GET(DILabel,\n                    (Metadata * Scope, MDString *Name, Metadata *File,\n                     unsigned Line),\n                    (Scope, Name, File, Line))\n\n  TempDILabel clone() const { return cloneImpl(); }\n\n  /// Get the local scope for this label.\n  ///\n  /// Labels must be defined in a local scope.\n  DILocalScope *getScope() const {\n    return cast_or_null<DILocalScope>(getRawScope());\n  }\n  unsigned getLine() const { return Line; }\n  StringRef getName() const { return getStringOperand(1); }\n  DIFile *getFile() const { return cast_or_null<DIFile>(getRawFile()); }\n\n  Metadata *getRawScope() const { return getOperand(0); }\n  MDString *getRawName() const { return getOperandAs<MDString>(1); }\n  Metadata *getRawFile() const { return getOperand(2); }\n\n  /// Check that a location is valid for this label.\n  ///\n  /// Check that \\c DL exists, is in the same subprogram, and has the same\n  /// inlined-at location as \\c this.  (Otherwise, it's not a valid attachment\n  /// to a \\a DbgInfoIntrinsic.)\n  bool isValidLocationForIntrinsic(const DILocation *DL) const {\n    return DL && getScope()->getSubprogram() == DL->getScope()->getSubprogram();\n  }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DILabelKind;\n  }\n};\n\nclass DIObjCProperty : public DINode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Line;\n  unsigned Attributes;\n\n  DIObjCProperty(LLVMContext &C, StorageType Storage, unsigned Line,\n                 unsigned Attributes, ArrayRef<Metadata *> Ops)\n      : DINode(C, DIObjCPropertyKind, Storage, dwarf::DW_TAG_APPLE_property,\n               Ops),\n        Line(Line), Attributes(Attributes) {}\n  ~DIObjCProperty() = default;\n\n  static DIObjCProperty *\n  getImpl(LLVMContext &Context, StringRef Name, DIFile *File, unsigned Line,\n          StringRef GetterName, StringRef SetterName, unsigned Attributes,\n          DIType *Type, StorageType Storage, bool ShouldCreate = true) {\n    return getImpl(Context, getCanonicalMDString(Context, Name), File, Line,\n                   getCanonicalMDString(Context, GetterName),\n                   getCanonicalMDString(Context, SetterName), Attributes, Type,\n                   Storage, ShouldCreate);\n  }\n  static DIObjCProperty *getImpl(LLVMContext &Context, MDString *Name,\n                                 Metadata *File, unsigned Line,\n                                 MDString *GetterName, MDString *SetterName,\n                                 unsigned Attributes, Metadata *Type,\n                                 StorageType Storage, bool ShouldCreate = true);\n\n  TempDIObjCProperty cloneImpl() const {\n    return getTemporary(getContext(), getName(), getFile(), getLine(),\n                        getGetterName(), getSetterName(), getAttributes(),\n                        getType());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIObjCProperty,\n                    (StringRef Name, DIFile *File, unsigned Line,\n                     StringRef GetterName, StringRef SetterName,\n                     unsigned Attributes, DIType *Type),\n                    (Name, File, Line, GetterName, SetterName, Attributes,\n                     Type))\n  DEFINE_MDNODE_GET(DIObjCProperty,\n                    (MDString * Name, Metadata *File, unsigned Line,\n                     MDString *GetterName, MDString *SetterName,\n                     unsigned Attributes, Metadata *Type),\n                    (Name, File, Line, GetterName, SetterName, Attributes,\n                     Type))\n\n  TempDIObjCProperty clone() const { return cloneImpl(); }\n\n  unsigned getLine() const { return Line; }\n  unsigned getAttributes() const { return Attributes; }\n  StringRef getName() const { return getStringOperand(0); }\n  DIFile *getFile() const { return cast_or_null<DIFile>(getRawFile()); }\n  StringRef getGetterName() const { return getStringOperand(2); }\n  StringRef getSetterName() const { return getStringOperand(3); }\n  DIType *getType() const { return cast_or_null<DIType>(getRawType()); }\n\n  StringRef getFilename() const {\n    if (auto *F = getFile())\n      return F->getFilename();\n    return \"\";\n  }\n\n  StringRef getDirectory() const {\n    if (auto *F = getFile())\n      return F->getDirectory();\n    return \"\";\n  }\n\n  MDString *getRawName() const { return getOperandAs<MDString>(0); }\n  Metadata *getRawFile() const { return getOperand(1); }\n  MDString *getRawGetterName() const { return getOperandAs<MDString>(2); }\n  MDString *getRawSetterName() const { return getOperandAs<MDString>(3); }\n  Metadata *getRawType() const { return getOperand(4); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIObjCPropertyKind;\n  }\n};\n\n/// An imported module (C++ using directive or similar).\nclass DIImportedEntity : public DINode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Line;\n\n  DIImportedEntity(LLVMContext &C, StorageType Storage, unsigned Tag,\n                   unsigned Line, ArrayRef<Metadata *> Ops)\n      : DINode(C, DIImportedEntityKind, Storage, Tag, Ops), Line(Line) {}\n  ~DIImportedEntity() = default;\n\n  static DIImportedEntity *getImpl(LLVMContext &Context, unsigned Tag,\n                                   DIScope *Scope, DINode *Entity, DIFile *File,\n                                   unsigned Line, StringRef Name,\n                                   StorageType Storage,\n                                   bool ShouldCreate = true) {\n    return getImpl(Context, Tag, Scope, Entity, File, Line,\n                   getCanonicalMDString(Context, Name), Storage, ShouldCreate);\n  }\n  static DIImportedEntity *getImpl(LLVMContext &Context, unsigned Tag,\n                                   Metadata *Scope, Metadata *Entity,\n                                   Metadata *File, unsigned Line,\n                                   MDString *Name, StorageType Storage,\n                                   bool ShouldCreate = true);\n\n  TempDIImportedEntity cloneImpl() const {\n    return getTemporary(getContext(), getTag(), getScope(), getEntity(),\n                        getFile(), getLine(), getName());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIImportedEntity,\n                    (unsigned Tag, DIScope *Scope, DINode *Entity, DIFile *File,\n                     unsigned Line, StringRef Name = \"\"),\n                    (Tag, Scope, Entity, File, Line, Name))\n  DEFINE_MDNODE_GET(DIImportedEntity,\n                    (unsigned Tag, Metadata *Scope, Metadata *Entity,\n                     Metadata *File, unsigned Line, MDString *Name),\n                    (Tag, Scope, Entity, File, Line, Name))\n\n  TempDIImportedEntity clone() const { return cloneImpl(); }\n\n  unsigned getLine() const { return Line; }\n  DIScope *getScope() const { return cast_or_null<DIScope>(getRawScope()); }\n  DINode *getEntity() const { return cast_or_null<DINode>(getRawEntity()); }\n  StringRef getName() const { return getStringOperand(2); }\n  DIFile *getFile() const { return cast_or_null<DIFile>(getRawFile()); }\n\n  Metadata *getRawScope() const { return getOperand(0); }\n  Metadata *getRawEntity() const { return getOperand(1); }\n  MDString *getRawName() const { return getOperandAs<MDString>(2); }\n  Metadata *getRawFile() const { return getOperand(3); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIImportedEntityKind;\n  }\n};\n\n/// A pair of DIGlobalVariable and DIExpression.\nclass DIGlobalVariableExpression : public MDNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  DIGlobalVariableExpression(LLVMContext &C, StorageType Storage,\n                             ArrayRef<Metadata *> Ops)\n      : MDNode(C, DIGlobalVariableExpressionKind, Storage, Ops) {}\n  ~DIGlobalVariableExpression() = default;\n\n  static DIGlobalVariableExpression *\n  getImpl(LLVMContext &Context, Metadata *Variable, Metadata *Expression,\n          StorageType Storage, bool ShouldCreate = true);\n\n  TempDIGlobalVariableExpression cloneImpl() const {\n    return getTemporary(getContext(), getVariable(), getExpression());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIGlobalVariableExpression,\n                    (Metadata * Variable, Metadata *Expression),\n                    (Variable, Expression))\n\n  TempDIGlobalVariableExpression clone() const { return cloneImpl(); }\n\n  Metadata *getRawVariable() const { return getOperand(0); }\n\n  DIGlobalVariable *getVariable() const {\n    return cast_or_null<DIGlobalVariable>(getRawVariable());\n  }\n\n  Metadata *getRawExpression() const { return getOperand(1); }\n\n  DIExpression *getExpression() const {\n    return cast<DIExpression>(getRawExpression());\n  }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIGlobalVariableExpressionKind;\n  }\n};\n\n/// Macro Info DWARF-like metadata node.\n///\n/// A metadata node with a DWARF macro info (i.e., a constant named\n/// \\c DW_MACINFO_*, defined in llvm/BinaryFormat/Dwarf.h).  Called \\a\n/// DIMacroNode\n/// because it's potentially used for non-DWARF output.\nclass DIMacroNode : public MDNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\nprotected:\n  DIMacroNode(LLVMContext &C, unsigned ID, StorageType Storage, unsigned MIType,\n              ArrayRef<Metadata *> Ops1, ArrayRef<Metadata *> Ops2 = None)\n      : MDNode(C, ID, Storage, Ops1, Ops2) {\n    assert(MIType < 1u << 16);\n    SubclassData16 = MIType;\n  }\n  ~DIMacroNode() = default;\n\n  template <class Ty> Ty *getOperandAs(unsigned I) const {\n    return cast_or_null<Ty>(getOperand(I));\n  }\n\n  StringRef getStringOperand(unsigned I) const {\n    if (auto *S = getOperandAs<MDString>(I))\n      return S->getString();\n    return StringRef();\n  }\n\n  static MDString *getCanonicalMDString(LLVMContext &Context, StringRef S) {\n    if (S.empty())\n      return nullptr;\n    return MDString::get(Context, S);\n  }\n\npublic:\n  unsigned getMacinfoType() const { return SubclassData16; }\n\n  static bool classof(const Metadata *MD) {\n    switch (MD->getMetadataID()) {\n    default:\n      return false;\n    case DIMacroKind:\n    case DIMacroFileKind:\n      return true;\n    }\n  }\n};\n\nclass DIMacro : public DIMacroNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Line;\n\n  DIMacro(LLVMContext &C, StorageType Storage, unsigned MIType, unsigned Line,\n          ArrayRef<Metadata *> Ops)\n      : DIMacroNode(C, DIMacroKind, Storage, MIType, Ops), Line(Line) {}\n  ~DIMacro() = default;\n\n  static DIMacro *getImpl(LLVMContext &Context, unsigned MIType, unsigned Line,\n                          StringRef Name, StringRef Value, StorageType Storage,\n                          bool ShouldCreate = true) {\n    return getImpl(Context, MIType, Line, getCanonicalMDString(Context, Name),\n                   getCanonicalMDString(Context, Value), Storage, ShouldCreate);\n  }\n  static DIMacro *getImpl(LLVMContext &Context, unsigned MIType, unsigned Line,\n                          MDString *Name, MDString *Value, StorageType Storage,\n                          bool ShouldCreate = true);\n\n  TempDIMacro cloneImpl() const {\n    return getTemporary(getContext(), getMacinfoType(), getLine(), getName(),\n                        getValue());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIMacro, (unsigned MIType, unsigned Line, StringRef Name,\n                              StringRef Value = \"\"),\n                    (MIType, Line, Name, Value))\n  DEFINE_MDNODE_GET(DIMacro, (unsigned MIType, unsigned Line, MDString *Name,\n                              MDString *Value),\n                    (MIType, Line, Name, Value))\n\n  TempDIMacro clone() const { return cloneImpl(); }\n\n  unsigned getLine() const { return Line; }\n\n  StringRef getName() const { return getStringOperand(0); }\n  StringRef getValue() const { return getStringOperand(1); }\n\n  MDString *getRawName() const { return getOperandAs<MDString>(0); }\n  MDString *getRawValue() const { return getOperandAs<MDString>(1); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIMacroKind;\n  }\n};\n\nclass DIMacroFile : public DIMacroNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n\n  unsigned Line;\n\n  DIMacroFile(LLVMContext &C, StorageType Storage, unsigned MIType,\n              unsigned Line, ArrayRef<Metadata *> Ops)\n      : DIMacroNode(C, DIMacroFileKind, Storage, MIType, Ops), Line(Line) {}\n  ~DIMacroFile() = default;\n\n  static DIMacroFile *getImpl(LLVMContext &Context, unsigned MIType,\n                              unsigned Line, DIFile *File,\n                              DIMacroNodeArray Elements, StorageType Storage,\n                              bool ShouldCreate = true) {\n    return getImpl(Context, MIType, Line, static_cast<Metadata *>(File),\n                   Elements.get(), Storage, ShouldCreate);\n  }\n\n  static DIMacroFile *getImpl(LLVMContext &Context, unsigned MIType,\n                              unsigned Line, Metadata *File, Metadata *Elements,\n                              StorageType Storage, bool ShouldCreate = true);\n\n  TempDIMacroFile cloneImpl() const {\n    return getTemporary(getContext(), getMacinfoType(), getLine(), getFile(),\n                        getElements());\n  }\n\npublic:\n  DEFINE_MDNODE_GET(DIMacroFile, (unsigned MIType, unsigned Line, DIFile *File,\n                                  DIMacroNodeArray Elements),\n                    (MIType, Line, File, Elements))\n  DEFINE_MDNODE_GET(DIMacroFile, (unsigned MIType, unsigned Line,\n                                  Metadata *File, Metadata *Elements),\n                    (MIType, Line, File, Elements))\n\n  TempDIMacroFile clone() const { return cloneImpl(); }\n\n  void replaceElements(DIMacroNodeArray Elements) {\n#ifndef NDEBUG\n    for (DIMacroNode *Op : getElements())\n      assert(is_contained(Elements->operands(), Op) &&\n             \"Lost a macro node during macro node list replacement\");\n#endif\n    replaceOperandWith(1, Elements.get());\n  }\n\n  unsigned getLine() const { return Line; }\n  DIFile *getFile() const { return cast_or_null<DIFile>(getRawFile()); }\n\n  DIMacroNodeArray getElements() const {\n    return cast_or_null<MDTuple>(getRawElements());\n  }\n\n  Metadata *getRawFile() const { return getOperand(0); }\n  Metadata *getRawElements() const { return getOperand(1); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIMacroFileKind;\n  }\n};\n\n/// List of ValueAsMetadata, to be used as an argument to a dbg.value\n/// intrinsic.\nclass DIArgList : public MDNode {\n  friend class LLVMContextImpl;\n  friend class MDNode;\n  using iterator = SmallVectorImpl<ValueAsMetadata *>::iterator;\n\n  SmallVector<ValueAsMetadata *, 4> Args;\n\n  DIArgList(LLVMContext &C, StorageType Storage,\n            ArrayRef<ValueAsMetadata *> Args)\n      : MDNode(C, DIArgListKind, Storage, None),\n        Args(Args.begin(), Args.end()) {\n    track();\n  }\n  ~DIArgList() { untrack(); }\n\n  static DIArgList *getImpl(LLVMContext &Context,\n                            ArrayRef<ValueAsMetadata *> Args,\n                            StorageType Storage, bool ShouldCreate = true);\n\n  TempDIArgList cloneImpl() const {\n    return getTemporary(getContext(), getArgs());\n  }\n\n  void track();\n  void untrack();\n  void dropAllReferences();\n\npublic:\n  DEFINE_MDNODE_GET(DIArgList, (ArrayRef<ValueAsMetadata *> Args), (Args))\n\n  TempDIArgList clone() const { return cloneImpl(); }\n\n  ArrayRef<ValueAsMetadata *> getArgs() const { return Args; }\n\n  iterator args_begin() { return Args.begin(); }\n  iterator args_end() { return Args.end(); }\n\n  static bool classof(const Metadata *MD) {\n    return MD->getMetadataID() == DIArgListKind;\n  }\n\n  void handleChangedOperand(void *Ref, Metadata *New);\n};\n\n/// Identifies a unique instance of a variable.\n///\n/// Storage for identifying a potentially inlined instance of a variable,\n/// or a fragment thereof. This guarantees that exactly one variable instance\n/// may be identified by this class, even when that variable is a fragment of\n/// an aggregate variable and/or there is another inlined instance of the same\n/// source code variable nearby.\n/// This class does not necessarily uniquely identify that variable: it is\n/// possible that a DebugVariable with different parameters may point to the\n/// same variable instance, but not that one DebugVariable points to multiple\n/// variable instances.\nclass DebugVariable {\n  using FragmentInfo = DIExpression::FragmentInfo;\n\n  const DILocalVariable *Variable;\n  Optional<FragmentInfo> Fragment;\n  const DILocation *InlinedAt;\n\n  /// Fragment that will overlap all other fragments. Used as default when\n  /// caller demands a fragment.\n  static const FragmentInfo DefaultFragment;\n\npublic:\n  DebugVariable(const DILocalVariable *Var, Optional<FragmentInfo> FragmentInfo,\n                const DILocation *InlinedAt)\n      : Variable(Var), Fragment(FragmentInfo), InlinedAt(InlinedAt) {}\n\n  DebugVariable(const DILocalVariable *Var, const DIExpression *DIExpr,\n                const DILocation *InlinedAt)\n      : Variable(Var),\n        Fragment(DIExpr ? DIExpr->getFragmentInfo() : NoneType()),\n        InlinedAt(InlinedAt) {}\n\n  const DILocalVariable *getVariable() const { return Variable; }\n  Optional<FragmentInfo> getFragment() const { return Fragment; }\n  const DILocation *getInlinedAt() const { return InlinedAt; }\n\n  FragmentInfo getFragmentOrDefault() const {\n    return Fragment.getValueOr(DefaultFragment);\n  }\n\n  static bool isDefaultFragment(const FragmentInfo F) {\n    return F == DefaultFragment;\n  }\n\n  bool operator==(const DebugVariable &Other) const {\n    return std::tie(Variable, Fragment, InlinedAt) ==\n           std::tie(Other.Variable, Other.Fragment, Other.InlinedAt);\n  }\n\n  bool operator<(const DebugVariable &Other) const {\n    return std::tie(Variable, Fragment, InlinedAt) <\n           std::tie(Other.Variable, Other.Fragment, Other.InlinedAt);\n  }\n};\n\ntemplate <> struct DenseMapInfo<DebugVariable> {\n  using FragmentInfo = DIExpression::FragmentInfo;\n\n  /// Empty key: no key should be generated that has no DILocalVariable.\n  static inline DebugVariable getEmptyKey() {\n    return DebugVariable(nullptr, NoneType(), nullptr);\n  }\n\n  /// Difference in tombstone is that the Optional is meaningful.\n  static inline DebugVariable getTombstoneKey() {\n    return DebugVariable(nullptr, {{0, 0}}, nullptr);\n  }\n\n  static unsigned getHashValue(const DebugVariable &D) {\n    unsigned HV = 0;\n    const Optional<FragmentInfo> Fragment = D.getFragment();\n    if (Fragment)\n      HV = DenseMapInfo<FragmentInfo>::getHashValue(*Fragment);\n\n    return hash_combine(D.getVariable(), HV, D.getInlinedAt());\n  }\n\n  static bool isEqual(const DebugVariable &A, const DebugVariable &B) {\n    return A == B;\n  }\n};\n\n} // end namespace llvm\n\n#undef DEFINE_MDNODE_GET_UNPACK_IMPL\n#undef DEFINE_MDNODE_GET_UNPACK\n#undef DEFINE_MDNODE_GET\n\n#endif // LLVM_IR_DEBUGINFOMETADATA_H\n"}, "66": {"id": 66, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InlineAsm.h", "content": "//===- llvm/InlineAsm.h - Class to represent inline asm strings -*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This class represents the inline asm strings, which are Value*'s that are\n// used as the callee operand of call instructions.  InlineAsm's are uniqued\n// like constants, and created via InlineAsm::get(...).\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_INLINEASM_H\n#define LLVM_IR_INLINEASM_H\n\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <cassert>\n#include <string>\n#include <vector>\n\nnamespace llvm {\n\nclass FunctionType;\nclass PointerType;\ntemplate <class ConstantClass> class ConstantUniqueMap;\n\nclass InlineAsm final : public Value {\npublic:\n  enum AsmDialect {\n    AD_ATT,\n    AD_Intel\n  };\n\nprivate:\n  friend struct InlineAsmKeyType;\n  friend class ConstantUniqueMap<InlineAsm>;\n\n  std::string AsmString, Constraints;\n  FunctionType *FTy;\n  bool HasSideEffects;\n  bool IsAlignStack;\n  AsmDialect Dialect;\n\n  InlineAsm(FunctionType *Ty, const std::string &AsmString,\n            const std::string &Constraints, bool hasSideEffects,\n            bool isAlignStack, AsmDialect asmDialect);\n\n  /// When the ConstantUniqueMap merges two types and makes two InlineAsms\n  /// identical, it destroys one of them with this method.\n  void destroyConstant();\n\npublic:\n  InlineAsm(const InlineAsm &) = delete;\n  InlineAsm &operator=(const InlineAsm &) = delete;\n\n  /// InlineAsm::get - Return the specified uniqued inline asm string.\n  ///\n  static InlineAsm *get(FunctionType *Ty, StringRef AsmString,\n                        StringRef Constraints, bool hasSideEffects,\n                        bool isAlignStack = false,\n                        AsmDialect asmDialect = AD_ATT);\n\n  bool hasSideEffects() const { return HasSideEffects; }\n  bool isAlignStack() const { return IsAlignStack; }\n  AsmDialect getDialect() const { return Dialect; }\n\n  /// getType - InlineAsm's are always pointers.\n  ///\n  PointerType *getType() const {\n    return reinterpret_cast<PointerType*>(Value::getType());\n  }\n\n  /// getFunctionType - InlineAsm's are always pointers to functions.\n  ///\n  FunctionType *getFunctionType() const;\n\n  const std::string &getAsmString() const { return AsmString; }\n  const std::string &getConstraintString() const { return Constraints; }\n\n  /// Verify - This static method can be used by the parser to check to see if\n  /// the specified constraint string is legal for the type.  This returns true\n  /// if legal, false if not.\n  ///\n  static bool Verify(FunctionType *Ty, StringRef Constraints);\n\n  // Constraint String Parsing\n  enum ConstraintPrefix {\n    isInput,            // 'x'\n    isOutput,           // '=x'\n    isClobber           // '~x'\n  };\n\n  using ConstraintCodeVector = std::vector<std::string>;\n\n  struct SubConstraintInfo {\n    /// MatchingInput - If this is not -1, this is an output constraint where an\n    /// input constraint is required to match it (e.g. \"0\").  The value is the\n    /// constraint number that matches this one (for example, if this is\n    /// constraint #0 and constraint #4 has the value \"0\", this will be 4).\n    int MatchingInput = -1;\n\n    /// Code - The constraint code, either the register name (in braces) or the\n    /// constraint letter/number.\n    ConstraintCodeVector Codes;\n\n    /// Default constructor.\n    SubConstraintInfo() = default;\n  };\n\n  using SubConstraintInfoVector = std::vector<SubConstraintInfo>;\n  struct ConstraintInfo;\n  using ConstraintInfoVector = std::vector<ConstraintInfo>;\n\n  struct ConstraintInfo {\n    /// Type - The basic type of the constraint: input/output/clobber\n    ///\n    ConstraintPrefix Type = isInput;\n\n    /// isEarlyClobber - \"&\": output operand writes result before inputs are all\n    /// read.  This is only ever set for an output operand.\n    bool isEarlyClobber = false;\n\n    /// MatchingInput - If this is not -1, this is an output constraint where an\n    /// input constraint is required to match it (e.g. \"0\").  The value is the\n    /// constraint number that matches this one (for example, if this is\n    /// constraint #0 and constraint #4 has the value \"0\", this will be 4).\n    int MatchingInput = -1;\n\n    /// hasMatchingInput - Return true if this is an output constraint that has\n    /// a matching input constraint.\n    bool hasMatchingInput() const { return MatchingInput != -1; }\n\n    /// isCommutative - This is set to true for a constraint that is commutative\n    /// with the next operand.\n    bool isCommutative = false;\n\n    /// isIndirect - True if this operand is an indirect operand.  This means\n    /// that the address of the source or destination is present in the call\n    /// instruction, instead of it being returned or passed in explicitly.  This\n    /// is represented with a '*' in the asm string.\n    bool isIndirect = false;\n\n    /// Code - The constraint code, either the register name (in braces) or the\n    /// constraint letter/number.\n    ConstraintCodeVector Codes;\n\n    /// isMultipleAlternative - '|': has multiple-alternative constraints.\n    bool isMultipleAlternative = false;\n\n    /// multipleAlternatives - If there are multiple alternative constraints,\n    /// this array will contain them.  Otherwise it will be empty.\n    SubConstraintInfoVector multipleAlternatives;\n\n    /// The currently selected alternative constraint index.\n    unsigned currentAlternativeIndex = 0;\n\n    /// Default constructor.\n    ConstraintInfo() = default;\n\n    /// Parse - Analyze the specified string (e.g. \"=*&{eax}\") and fill in the\n    /// fields in this structure.  If the constraint string is not understood,\n    /// return true, otherwise return false.\n    bool Parse(StringRef Str, ConstraintInfoVector &ConstraintsSoFar);\n\n    /// selectAlternative - Point this constraint to the alternative constraint\n    /// indicated by the index.\n    void selectAlternative(unsigned index);\n  };\n\n  /// ParseConstraints - Split up the constraint string into the specific\n  /// constraints and their prefixes.  If this returns an empty vector, and if\n  /// the constraint string itself isn't empty, there was an error parsing.\n  static ConstraintInfoVector ParseConstraints(StringRef ConstraintString);\n\n  /// ParseConstraints - Parse the constraints of this inlineasm object,\n  /// returning them the same way that ParseConstraints(str) does.\n  ConstraintInfoVector ParseConstraints() const {\n    return ParseConstraints(Constraints);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Value *V) {\n    return V->getValueID() == Value::InlineAsmVal;\n  }\n\n  // These are helper methods for dealing with flags in the INLINEASM SDNode\n  // in the backend.\n  //\n  // The encoding of the flag word is currently:\n  //   Bits 2-0 - A Kind_* value indicating the kind of the operand.\n  //   Bits 15-3 - The number of SDNode operands associated with this inline\n  //               assembly operand.\n  //   If bit 31 is set:\n  //     Bit 30-16 - The operand number that this operand must match.\n  //                 When bits 2-0 are Kind_Mem, the Constraint_* value must be\n  //                 obtained from the flags for this operand number.\n  //   Else if bits 2-0 are Kind_Mem:\n  //     Bit 30-16 - A Constraint_* value indicating the original constraint\n  //                 code.\n  //   Else:\n  //     Bit 30-16 - The register class ID to use for the operand.\n\n  enum : uint32_t {\n    // Fixed operands on an INLINEASM SDNode.\n    Op_InputChain = 0,\n    Op_AsmString = 1,\n    Op_MDNode = 2,\n    Op_ExtraInfo = 3,    // HasSideEffects, IsAlignStack, AsmDialect.\n    Op_FirstOperand = 4,\n\n    // Fixed operands on an INLINEASM MachineInstr.\n    MIOp_AsmString = 0,\n    MIOp_ExtraInfo = 1,    // HasSideEffects, IsAlignStack, AsmDialect.\n    MIOp_FirstOperand = 2,\n\n    // Interpretation of the MIOp_ExtraInfo bit field.\n    Extra_HasSideEffects = 1,\n    Extra_IsAlignStack = 2,\n    Extra_AsmDialect = 4,\n    Extra_MayLoad = 8,\n    Extra_MayStore = 16,\n    Extra_IsConvergent = 32,\n\n    // Inline asm operands map to multiple SDNode / MachineInstr operands.\n    // The first operand is an immediate describing the asm operand, the low\n    // bits is the kind:\n    Kind_RegUse = 1,             // Input register, \"r\".\n    Kind_RegDef = 2,             // Output register, \"=r\".\n    Kind_RegDefEarlyClobber = 3, // Early-clobber output register, \"=&r\".\n    Kind_Clobber = 4,            // Clobbered register, \"~r\".\n    Kind_Imm = 5,                // Immediate.\n    Kind_Mem = 6,                // Memory operand, \"m\".\n\n    // Memory constraint codes.\n    // These could be tablegenerated but there's little need to do that since\n    // there's plenty of space in the encoding to support the union of all\n    // constraint codes for all targets.\n    Constraint_Unknown = 0,\n    Constraint_es,\n    Constraint_i,\n    Constraint_m,\n    Constraint_o,\n    Constraint_v,\n    Constraint_A,\n    Constraint_Q,\n    Constraint_R,\n    Constraint_S,\n    Constraint_T,\n    Constraint_Um,\n    Constraint_Un,\n    Constraint_Uq,\n    Constraint_Us,\n    Constraint_Ut,\n    Constraint_Uv,\n    Constraint_Uy,\n    Constraint_X,\n    Constraint_Z,\n    Constraint_ZC,\n    Constraint_Zy,\n    Constraints_Max = Constraint_Zy,\n    Constraints_ShiftAmount = 16,\n\n    Flag_MatchingOperand = 0x80000000\n  };\n\n  static unsigned getFlagWord(unsigned Kind, unsigned NumOps) {\n    assert(((NumOps << 3) & ~0xffff) == 0 && \"Too many inline asm operands!\");\n    assert(Kind >= Kind_RegUse && Kind <= Kind_Mem && \"Invalid Kind\");\n    return Kind | (NumOps << 3);\n  }\n\n  static bool isRegDefKind(unsigned Flag){ return getKind(Flag) == Kind_RegDef;}\n  static bool isImmKind(unsigned Flag) { return getKind(Flag) == Kind_Imm; }\n  static bool isMemKind(unsigned Flag) { return getKind(Flag) == Kind_Mem; }\n  static bool isRegDefEarlyClobberKind(unsigned Flag) {\n    return getKind(Flag) == Kind_RegDefEarlyClobber;\n  }\n  static bool isClobberKind(unsigned Flag) {\n    return getKind(Flag) == Kind_Clobber;\n  }\n\n  /// getFlagWordForMatchingOp - Augment an existing flag word returned by\n  /// getFlagWord with information indicating that this input operand is tied\n  /// to a previous output operand.\n  static unsigned getFlagWordForMatchingOp(unsigned InputFlag,\n                                           unsigned MatchedOperandNo) {\n    assert(MatchedOperandNo <= 0x7fff && \"Too big matched operand\");\n    assert((InputFlag & ~0xffff) == 0 && \"High bits already contain data\");\n    return InputFlag | Flag_MatchingOperand | (MatchedOperandNo << 16);\n  }\n\n  /// getFlagWordForRegClass - Augment an existing flag word returned by\n  /// getFlagWord with the required register class for the following register\n  /// operands.\n  /// A tied use operand cannot have a register class, use the register class\n  /// from the def operand instead.\n  static unsigned getFlagWordForRegClass(unsigned InputFlag, unsigned RC) {\n    // Store RC + 1, reserve the value 0 to mean 'no register class'.\n    ++RC;\n    assert(!isImmKind(InputFlag) && \"Immediates cannot have a register class\");\n    assert(!isMemKind(InputFlag) && \"Memory operand cannot have a register class\");\n    assert(RC <= 0x7fff && \"Too large register class ID\");\n    assert((InputFlag & ~0xffff) == 0 && \"High bits already contain data\");\n    return InputFlag | (RC << 16);\n  }\n\n  /// Augment an existing flag word returned by getFlagWord with the constraint\n  /// code for a memory constraint.\n  static unsigned getFlagWordForMem(unsigned InputFlag, unsigned Constraint) {\n    assert(isMemKind(InputFlag) && \"InputFlag is not a memory constraint!\");\n    assert(Constraint <= 0x7fff && \"Too large a memory constraint ID\");\n    assert(Constraint <= Constraints_Max && \"Unknown constraint ID\");\n    assert((InputFlag & ~0xffff) == 0 && \"High bits already contain data\");\n    return InputFlag | (Constraint << Constraints_ShiftAmount);\n  }\n\n  static unsigned convertMemFlagWordToMatchingFlagWord(unsigned InputFlag) {\n    assert(isMemKind(InputFlag));\n    return InputFlag & ~(0x7fff << Constraints_ShiftAmount);\n  }\n\n  static unsigned getKind(unsigned Flags) {\n    return Flags & 7;\n  }\n\n  static unsigned getMemoryConstraintID(unsigned Flag) {\n    assert(isMemKind(Flag));\n    return (Flag >> Constraints_ShiftAmount) & 0x7fff;\n  }\n\n  /// getNumOperandRegisters - Extract the number of registers field from the\n  /// inline asm operand flag.\n  static unsigned getNumOperandRegisters(unsigned Flag) {\n    return (Flag & 0xffff) >> 3;\n  }\n\n  /// isUseOperandTiedToDef - Return true if the flag of the inline asm\n  /// operand indicates it is an use operand that's matched to a def operand.\n  static bool isUseOperandTiedToDef(unsigned Flag, unsigned &Idx) {\n    if ((Flag & Flag_MatchingOperand) == 0)\n      return false;\n    Idx = (Flag & ~Flag_MatchingOperand) >> 16;\n    return true;\n  }\n\n  /// hasRegClassConstraint - Returns true if the flag contains a register\n  /// class constraint.  Sets RC to the register class ID.\n  static bool hasRegClassConstraint(unsigned Flag, unsigned &RC) {\n    if (Flag & Flag_MatchingOperand)\n      return false;\n    unsigned High = Flag >> 16;\n    // getFlagWordForRegClass() uses 0 to mean no register class, and otherwise\n    // stores RC + 1.\n    if (!High)\n      return false;\n    RC = High - 1;\n    return true;\n  }\n\n  static std::vector<StringRef> getExtraInfoNames(unsigned ExtraInfo) {\n    std::vector<StringRef> Result;\n    if (ExtraInfo & InlineAsm::Extra_HasSideEffects)\n      Result.push_back(\"sideeffect\");\n    if (ExtraInfo & InlineAsm::Extra_MayLoad)\n      Result.push_back(\"mayload\");\n    if (ExtraInfo & InlineAsm::Extra_MayStore)\n      Result.push_back(\"maystore\");\n    if (ExtraInfo & InlineAsm::Extra_IsConvergent)\n      Result.push_back(\"isconvergent\");\n    if (ExtraInfo & InlineAsm::Extra_IsAlignStack)\n      Result.push_back(\"alignstack\");\n\n    AsmDialect Dialect =\n        InlineAsm::AsmDialect((ExtraInfo & InlineAsm::Extra_AsmDialect));\n\n    if (Dialect == InlineAsm::AD_ATT)\n      Result.push_back(\"attdialect\");\n    if (Dialect == InlineAsm::AD_Intel)\n      Result.push_back(\"inteldialect\");\n\n    return Result;\n  }\n\n  static StringRef getKindName(unsigned Kind) {\n    switch (Kind) {\n    case InlineAsm::Kind_RegUse:\n      return \"reguse\";\n    case InlineAsm::Kind_RegDef:\n      return \"regdef\";\n    case InlineAsm::Kind_RegDefEarlyClobber:\n      return \"regdef-ec\";\n    case InlineAsm::Kind_Clobber:\n      return \"clobber\";\n    case InlineAsm::Kind_Imm:\n      return \"imm\";\n    case InlineAsm::Kind_Mem:\n      return \"mem\";\n    default:\n      llvm_unreachable(\"Unknown operand kind\");\n    }\n  }\n\n  static StringRef getMemConstraintName(unsigned Constraint) {\n    switch (Constraint) {\n    case InlineAsm::Constraint_es:\n      return \"es\";\n    case InlineAsm::Constraint_i:\n      return \"i\";\n    case InlineAsm::Constraint_m:\n      return \"m\";\n    case InlineAsm::Constraint_o:\n      return \"o\";\n    case InlineAsm::Constraint_v:\n      return \"v\";\n    case InlineAsm::Constraint_Q:\n      return \"Q\";\n    case InlineAsm::Constraint_R:\n      return \"R\";\n    case InlineAsm::Constraint_S:\n      return \"S\";\n    case InlineAsm::Constraint_T:\n      return \"T\";\n    case InlineAsm::Constraint_Um:\n      return \"Um\";\n    case InlineAsm::Constraint_Un:\n      return \"Un\";\n    case InlineAsm::Constraint_Uq:\n      return \"Uq\";\n    case InlineAsm::Constraint_Us:\n      return \"Us\";\n    case InlineAsm::Constraint_Ut:\n      return \"Ut\";\n    case InlineAsm::Constraint_Uv:\n      return \"Uv\";\n    case InlineAsm::Constraint_Uy:\n      return \"Uy\";\n    case InlineAsm::Constraint_X:\n      return \"X\";\n    case InlineAsm::Constraint_Z:\n      return \"Z\";\n    case InlineAsm::Constraint_ZC:\n      return \"ZC\";\n    case InlineAsm::Constraint_Zy:\n      return \"Zy\";\n    default:\n      llvm_unreachable(\"Unknown memory constraint\");\n    }\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_INLINEASM_H\n"}, "68": {"id": 68, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "content": "//===- llvm/Instructions.h - Instruction subclass definitions ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file exposes the class definitions of all of the subclasses of the\n// Instruction class.  This is meant to be an easy way to get access to all\n// instruction subclasses.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_INSTRUCTIONS_H\n#define LLVM_IR_INSTRUCTIONS_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/Bitfields.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/BasicBlock.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/CFG.h\"\n#include \"llvm/IR/Constant.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/OperandTraits.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/IR/Use.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <iterator>\n\nnamespace llvm {\n\nclass APInt;\nclass ConstantInt;\nclass DataLayout;\nclass LLVMContext;\n\n//===----------------------------------------------------------------------===//\n//                                AllocaInst Class\n//===----------------------------------------------------------------------===//\n\n/// an instruction to allocate memory on the stack\nclass AllocaInst : public UnaryInstruction {\n  Type *AllocatedType;\n\n  using AlignmentField = AlignmentBitfieldElementT<0>;\n  using UsedWithInAllocaField = BoolBitfieldElementT<AlignmentField::NextBit>;\n  using SwiftErrorField = BoolBitfieldElementT<UsedWithInAllocaField::NextBit>;\n  static_assert(Bitfield::areContiguous<AlignmentField, UsedWithInAllocaField,\n                                        SwiftErrorField>(),\n                \"Bitfields must be contiguous\");\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  AllocaInst *cloneImpl() const;\n\npublic:\n  explicit AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize,\n                      const Twine &Name, Instruction *InsertBefore);\n  AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize,\n             const Twine &Name, BasicBlock *InsertAtEnd);\n\n  AllocaInst(Type *Ty, unsigned AddrSpace, const Twine &Name,\n             Instruction *InsertBefore);\n  AllocaInst(Type *Ty, unsigned AddrSpace,\n             const Twine &Name, BasicBlock *InsertAtEnd);\n\n  AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize, Align Align,\n             const Twine &Name = \"\", Instruction *InsertBefore = nullptr);\n  AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize, Align Align,\n             const Twine &Name, BasicBlock *InsertAtEnd);\n\n  /// Return true if there is an allocation size parameter to the allocation\n  /// instruction that is not 1.\n  bool isArrayAllocation() const;\n\n  /// Get the number of elements allocated. For a simple allocation of a single\n  /// element, this will return a constant 1 value.\n  const Value *getArraySize() const { return getOperand(0); }\n  Value *getArraySize() { return getOperand(0); }\n\n  /// Overload to return most specific pointer type.\n  PointerType *getType() const {\n    return cast<PointerType>(Instruction::getType());\n  }\n\n  /// Get allocation size in bits. Returns None if size can't be determined,\n  /// e.g. in case of a VLA.\n  Optional<TypeSize> getAllocationSizeInBits(const DataLayout &DL) const;\n\n  /// Return the type that is being allocated by the instruction.\n  Type *getAllocatedType() const { return AllocatedType; }\n  /// for use only in special circumstances that need to generically\n  /// transform a whole instruction (eg: IR linking and vectorization).\n  void setAllocatedType(Type *Ty) { AllocatedType = Ty; }\n\n  /// Return the alignment of the memory that is being allocated by the\n  /// instruction.\n  Align getAlign() const {\n    return Align(1ULL << getSubclassData<AlignmentField>());\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  // FIXME: Remove this one transition to Align is over.\n  unsigned getAlignment() const { return getAlign().value(); }\n\n  /// Return true if this alloca is in the entry block of the function and is a\n  /// constant size. If so, the code generator will fold it into the\n  /// prolog/epilog code, so it is basically free.\n  bool isStaticAlloca() const;\n\n  /// Return true if this alloca is used as an inalloca argument to a call. Such\n  /// allocas are never considered static even if they are in the entry block.\n  bool isUsedWithInAlloca() const {\n    return getSubclassData<UsedWithInAllocaField>();\n  }\n\n  /// Specify whether this alloca is used to represent the arguments to a call.\n  void setUsedWithInAlloca(bool V) {\n    setSubclassData<UsedWithInAllocaField>(V);\n  }\n\n  /// Return true if this alloca is used as a swifterror argument to a call.\n  bool isSwiftError() const { return getSubclassData<SwiftErrorField>(); }\n  /// Specify whether this alloca is used to represent a swifterror.\n  void setSwiftError(bool V) { setSubclassData<SwiftErrorField>(V); }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Alloca);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                LoadInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction for reading from memory. This uses the SubclassData field in\n/// Value to store whether or not the load is volatile.\nclass LoadInst : public UnaryInstruction {\n  using VolatileField = BoolBitfieldElementT<0>;\n  using AlignmentField = AlignmentBitfieldElementT<VolatileField::NextBit>;\n  using OrderingField = AtomicOrderingBitfieldElementT<AlignmentField::NextBit>;\n  static_assert(\n      Bitfield::areContiguous<VolatileField, AlignmentField, OrderingField>(),\n      \"Bitfields must be contiguous\");\n\n  void AssertOK();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  LoadInst *cloneImpl() const;\n\npublic:\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr,\n           Instruction *InsertBefore);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, BasicBlock *InsertAtEnd);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Instruction *InsertBefore);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           BasicBlock *InsertAtEnd);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, Instruction *InsertBefore = nullptr);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, BasicBlock *InsertAtEnd);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, AtomicOrdering Order,\n           SyncScope::ID SSID = SyncScope::System,\n           Instruction *InsertBefore = nullptr);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, AtomicOrdering Order, SyncScope::ID SSID,\n           BasicBlock *InsertAtEnd);\n\n  /// Return true if this is a load from a volatile memory location.\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile load or not.\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Return the alignment of the access that is being performed.\n  /// FIXME: Remove this function once transition to Align is over.\n  /// Use getAlign() instead.\n  unsigned getAlignment() const { return getAlign().value(); }\n\n  /// Return the alignment of the access that is being performed.\n  Align getAlign() const {\n    return Align(1ULL << (getSubclassData<AlignmentField>()));\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Returns the ordering constraint of this load instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<OrderingField>();\n  }\n  /// Sets the ordering constraint of this load instruction.  May not be Release\n  /// or AcquireRelease.\n  void setOrdering(AtomicOrdering Ordering) {\n    setSubclassData<OrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this load instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this load instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  /// Sets the ordering constraint and the synchronization scope ID of this load\n  /// instruction.\n  void setAtomic(AtomicOrdering Ordering,\n                 SyncScope::ID SSID = SyncScope::System) {\n    setOrdering(Ordering);\n    setSyncScopeID(SSID);\n  }\n\n  bool isSimple() const { return !isAtomic() && !isVolatile(); }\n\n  bool isUnordered() const {\n    return (getOrdering() == AtomicOrdering::NotAtomic ||\n            getOrdering() == AtomicOrdering::Unordered) &&\n           !isVolatile();\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n  Type *getPointerOperandType() const { return getPointerOperand()->getType(); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperandType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Load;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this load instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\n//===----------------------------------------------------------------------===//\n//                                StoreInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction for storing to memory.\nclass StoreInst : public Instruction {\n  using VolatileField = BoolBitfieldElementT<0>;\n  using AlignmentField = AlignmentBitfieldElementT<VolatileField::NextBit>;\n  using OrderingField = AtomicOrderingBitfieldElementT<AlignmentField::NextBit>;\n  static_assert(\n      Bitfield::areContiguous<VolatileField, AlignmentField, OrderingField>(),\n      \"Bitfields must be contiguous\");\n\n  void AssertOK();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  StoreInst *cloneImpl() const;\n\npublic:\n  StoreInst(Value *Val, Value *Ptr, Instruction *InsertBefore);\n  StoreInst(Value *Val, Value *Ptr, BasicBlock *InsertAtEnd);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Instruction *InsertBefore);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, BasicBlock *InsertAtEnd);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            Instruction *InsertBefore = nullptr);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            BasicBlock *InsertAtEnd);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            AtomicOrdering Order, SyncScope::ID SSID = SyncScope::System,\n            Instruction *InsertBefore = nullptr);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            AtomicOrdering Order, SyncScope::ID SSID, BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  /// Return true if this is a store to a volatile memory location.\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile store or not.\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Return the alignment of the access that is being performed\n  /// FIXME: Remove this function once transition to Align is over.\n  /// Use getAlign() instead.\n  unsigned getAlignment() const { return getAlign().value(); }\n\n  Align getAlign() const {\n    return Align(1ULL << (getSubclassData<AlignmentField>()));\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Returns the ordering constraint of this store instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<OrderingField>();\n  }\n\n  /// Sets the ordering constraint of this store instruction.  May not be\n  /// Acquire or AcquireRelease.\n  void setOrdering(AtomicOrdering Ordering) {\n    setSubclassData<OrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this store instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this store instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  /// Sets the ordering constraint and the synchronization scope ID of this\n  /// store instruction.\n  void setAtomic(AtomicOrdering Ordering,\n                 SyncScope::ID SSID = SyncScope::System) {\n    setOrdering(Ordering);\n    setSyncScopeID(SSID);\n  }\n\n  bool isSimple() const { return !isAtomic() && !isVolatile(); }\n\n  bool isUnordered() const {\n    return (getOrdering() == AtomicOrdering::NotAtomic ||\n            getOrdering() == AtomicOrdering::Unordered) &&\n           !isVolatile();\n  }\n\n  Value *getValueOperand() { return getOperand(0); }\n  const Value *getValueOperand() const { return getOperand(0); }\n\n  Value *getPointerOperand() { return getOperand(1); }\n  const Value *getPointerOperand() const { return getOperand(1); }\n  static unsigned getPointerOperandIndex() { return 1U; }\n  Type *getPointerOperandType() const { return getPointerOperand()->getType(); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperandType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Store;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this store instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\ntemplate <>\nstruct OperandTraits<StoreInst> : public FixedNumOperandTraits<StoreInst, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(StoreInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                FenceInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction for ordering other memory operations.\nclass FenceInst : public Instruction {\n  using OrderingField = AtomicOrderingBitfieldElementT<0>;\n\n  void Init(AtomicOrdering Ordering, SyncScope::ID SSID);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  FenceInst *cloneImpl() const;\n\npublic:\n  // Ordering may only be Acquire, Release, AcquireRelease, or\n  // SequentiallyConsistent.\n  FenceInst(LLVMContext &C, AtomicOrdering Ordering,\n            SyncScope::ID SSID = SyncScope::System,\n            Instruction *InsertBefore = nullptr);\n  FenceInst(LLVMContext &C, AtomicOrdering Ordering, SyncScope::ID SSID,\n            BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 0);\n  }\n\n  /// Returns the ordering constraint of this fence instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<OrderingField>();\n  }\n\n  /// Sets the ordering constraint of this fence instruction.  May only be\n  /// Acquire, Release, AcquireRelease, or SequentiallyConsistent.\n  void setOrdering(AtomicOrdering Ordering) {\n    setSubclassData<OrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this fence instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this fence instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Fence;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this fence instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\n//===----------------------------------------------------------------------===//\n//                                AtomicCmpXchgInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction that atomically checks whether a\n/// specified value is in a memory location, and, if it is, stores a new value\n/// there. The value returned by this instruction is a pair containing the\n/// original value as first element, and an i1 indicating success (true) or\n/// failure (false) as second element.\n///\nclass AtomicCmpXchgInst : public Instruction {\n  void Init(Value *Ptr, Value *Cmp, Value *NewVal, Align Align,\n            AtomicOrdering SuccessOrdering, AtomicOrdering FailureOrdering,\n            SyncScope::ID SSID);\n\n  template <unsigned Offset>\n  using AtomicOrderingBitfieldElement =\n      typename Bitfield::Element<AtomicOrdering, Offset, 3,\n                                 AtomicOrdering::LAST>;\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  AtomicCmpXchgInst *cloneImpl() const;\n\npublic:\n  AtomicCmpXchgInst(Value *Ptr, Value *Cmp, Value *NewVal, Align Alignment,\n                    AtomicOrdering SuccessOrdering,\n                    AtomicOrdering FailureOrdering, SyncScope::ID SSID,\n                    Instruction *InsertBefore = nullptr);\n  AtomicCmpXchgInst(Value *Ptr, Value *Cmp, Value *NewVal, Align Alignment,\n                    AtomicOrdering SuccessOrdering,\n                    AtomicOrdering FailureOrdering, SyncScope::ID SSID,\n                    BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly three operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 3);\n  }\n\n  using VolatileField = BoolBitfieldElementT<0>;\n  using WeakField = BoolBitfieldElementT<VolatileField::NextBit>;\n  using SuccessOrderingField =\n      AtomicOrderingBitfieldElementT<WeakField::NextBit>;\n  using FailureOrderingField =\n      AtomicOrderingBitfieldElementT<SuccessOrderingField::NextBit>;\n  using AlignmentField =\n      AlignmentBitfieldElementT<FailureOrderingField::NextBit>;\n  static_assert(\n      Bitfield::areContiguous<VolatileField, WeakField, SuccessOrderingField,\n                              FailureOrderingField, AlignmentField>(),\n      \"Bitfields must be contiguous\");\n\n  /// Return the alignment of the memory that is being allocated by the\n  /// instruction.\n  Align getAlign() const {\n    return Align(1ULL << getSubclassData<AlignmentField>());\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Return true if this is a cmpxchg from a volatile memory\n  /// location.\n  ///\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile cmpxchg.\n  ///\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Return true if this cmpxchg may spuriously fail.\n  bool isWeak() const { return getSubclassData<WeakField>(); }\n\n  void setWeak(bool IsWeak) { setSubclassData<WeakField>(IsWeak); }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Returns the success ordering constraint of this cmpxchg instruction.\n  AtomicOrdering getSuccessOrdering() const {\n    return getSubclassData<SuccessOrderingField>();\n  }\n\n  /// Sets the success ordering constraint of this cmpxchg instruction.\n  void setSuccessOrdering(AtomicOrdering Ordering) {\n    assert(Ordering != AtomicOrdering::NotAtomic &&\n           \"CmpXchg instructions can only be atomic.\");\n    setSubclassData<SuccessOrderingField>(Ordering);\n  }\n\n  /// Returns the failure ordering constraint of this cmpxchg instruction.\n  AtomicOrdering getFailureOrdering() const {\n    return getSubclassData<FailureOrderingField>();\n  }\n\n  /// Sets the failure ordering constraint of this cmpxchg instruction.\n  void setFailureOrdering(AtomicOrdering Ordering) {\n    assert(Ordering != AtomicOrdering::NotAtomic &&\n           \"CmpXchg instructions can only be atomic.\");\n    setSubclassData<FailureOrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this cmpxchg instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this cmpxchg instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  Value *getCompareOperand() { return getOperand(1); }\n  const Value *getCompareOperand() const { return getOperand(1); }\n\n  Value *getNewValOperand() { return getOperand(2); }\n  const Value *getNewValOperand() const { return getOperand(2); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  /// Returns the strongest permitted ordering on failure, given the\n  /// desired ordering on success.\n  ///\n  /// If the comparison in a cmpxchg operation fails, there is no atomic store\n  /// so release semantics cannot be provided. So this function drops explicit\n  /// Release requests from the AtomicOrdering. A SequentiallyConsistent\n  /// operation would remain SequentiallyConsistent.\n  static AtomicOrdering\n  getStrongestFailureOrdering(AtomicOrdering SuccessOrdering) {\n    switch (SuccessOrdering) {\n    default:\n      llvm_unreachable(\"invalid cmpxchg success ordering\");\n    case AtomicOrdering::Release:\n    case AtomicOrdering::Monotonic:\n      return AtomicOrdering::Monotonic;\n    case AtomicOrdering::AcquireRelease:\n    case AtomicOrdering::Acquire:\n      return AtomicOrdering::Acquire;\n    case AtomicOrdering::SequentiallyConsistent:\n      return AtomicOrdering::SequentiallyConsistent;\n    }\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::AtomicCmpXchg;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this cmpxchg instruction.  Not quite\n  /// enough room in SubClassData for everything, so synchronization scope ID\n  /// gets its own field.\n  SyncScope::ID SSID;\n};\n\ntemplate <>\nstruct OperandTraits<AtomicCmpXchgInst> :\n    public FixedNumOperandTraits<AtomicCmpXchgInst, 3> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(AtomicCmpXchgInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                AtomicRMWInst Class\n//===----------------------------------------------------------------------===//\n\n/// an instruction that atomically reads a memory location,\n/// combines it with another value, and then stores the result back.  Returns\n/// the old value.\n///\nclass AtomicRMWInst : public Instruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  AtomicRMWInst *cloneImpl() const;\n\npublic:\n  /// This enumeration lists the possible modifications atomicrmw can make.  In\n  /// the descriptions, 'p' is the pointer to the instruction's memory location,\n  /// 'old' is the initial value of *p, and 'v' is the other value passed to the\n  /// instruction.  These instructions always return 'old'.\n  enum BinOp : unsigned {\n    /// *p = v\n    Xchg,\n    /// *p = old + v\n    Add,\n    /// *p = old - v\n    Sub,\n    /// *p = old & v\n    And,\n    /// *p = ~(old & v)\n    Nand,\n    /// *p = old | v\n    Or,\n    /// *p = old ^ v\n    Xor,\n    /// *p = old >signed v ? old : v\n    Max,\n    /// *p = old <signed v ? old : v\n    Min,\n    /// *p = old >unsigned v ? old : v\n    UMax,\n    /// *p = old <unsigned v ? old : v\n    UMin,\n\n    /// *p = old + v\n    FAdd,\n\n    /// *p = old - v\n    FSub,\n\n    FIRST_BINOP = Xchg,\n    LAST_BINOP = FSub,\n    BAD_BINOP\n  };\n\nprivate:\n  template <unsigned Offset>\n  using AtomicOrderingBitfieldElement =\n      typename Bitfield::Element<AtomicOrdering, Offset, 3,\n                                 AtomicOrdering::LAST>;\n\n  template <unsigned Offset>\n  using BinOpBitfieldElement =\n      typename Bitfield::Element<BinOp, Offset, 4, BinOp::LAST_BINOP>;\n\npublic:\n  AtomicRMWInst(BinOp Operation, Value *Ptr, Value *Val, Align Alignment,\n                AtomicOrdering Ordering, SyncScope::ID SSID,\n                Instruction *InsertBefore = nullptr);\n  AtomicRMWInst(BinOp Operation, Value *Ptr, Value *Val, Align Alignment,\n                AtomicOrdering Ordering, SyncScope::ID SSID,\n                BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  using VolatileField = BoolBitfieldElementT<0>;\n  using AtomicOrderingField =\n      AtomicOrderingBitfieldElementT<VolatileField::NextBit>;\n  using OperationField = BinOpBitfieldElement<AtomicOrderingField::NextBit>;\n  using AlignmentField = AlignmentBitfieldElementT<OperationField::NextBit>;\n  static_assert(Bitfield::areContiguous<VolatileField, AtomicOrderingField,\n                                        OperationField, AlignmentField>(),\n                \"Bitfields must be contiguous\");\n\n  BinOp getOperation() const { return getSubclassData<OperationField>(); }\n\n  static StringRef getOperationName(BinOp Op);\n\n  static bool isFPOperation(BinOp Op) {\n    switch (Op) {\n    case AtomicRMWInst::FAdd:\n    case AtomicRMWInst::FSub:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  void setOperation(BinOp Operation) {\n    setSubclassData<OperationField>(Operation);\n  }\n\n  /// Return the alignment of the memory that is being allocated by the\n  /// instruction.\n  Align getAlign() const {\n    return Align(1ULL << getSubclassData<AlignmentField>());\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Return true if this is a RMW on a volatile memory location.\n  ///\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile RMW or not.\n  ///\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Returns the ordering constraint of this rmw instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<AtomicOrderingField>();\n  }\n\n  /// Sets the ordering constraint of this rmw instruction.\n  void setOrdering(AtomicOrdering Ordering) {\n    assert(Ordering != AtomicOrdering::NotAtomic &&\n           \"atomicrmw instructions can only be atomic.\");\n    setSubclassData<AtomicOrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this rmw instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this rmw instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  Value *getValOperand() { return getOperand(1); }\n  const Value *getValOperand() const { return getOperand(1); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  bool isFloatingPointOperation() const {\n    return isFPOperation(getOperation());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::AtomicRMW;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  void Init(BinOp Operation, Value *Ptr, Value *Val, Align Align,\n            AtomicOrdering Ordering, SyncScope::ID SSID);\n\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this rmw instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\ntemplate <>\nstruct OperandTraits<AtomicRMWInst>\n    : public FixedNumOperandTraits<AtomicRMWInst,2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(AtomicRMWInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                             GetElementPtrInst Class\n//===----------------------------------------------------------------------===//\n\n// checkGEPType - Simple wrapper function to give a better assertion failure\n// message on bad indexes for a gep instruction.\n//\ninline Type *checkGEPType(Type *Ty) {\n  assert(Ty && \"Invalid GetElementPtrInst indices for type!\");\n  return Ty;\n}\n\n/// an instruction for type-safe pointer arithmetic to\n/// access elements of arrays and structs\n///\nclass GetElementPtrInst : public Instruction {\n  Type *SourceElementType;\n  Type *ResultElementType;\n\n  GetElementPtrInst(const GetElementPtrInst &GEPI);\n\n  /// Constructors - Create a getelementptr instruction with a base pointer an\n  /// list of indices. The first ctor can optionally insert before an existing\n  /// instruction, the second appends the new instruction to the specified\n  /// BasicBlock.\n  inline GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                           ArrayRef<Value *> IdxList, unsigned Values,\n                           const Twine &NameStr, Instruction *InsertBefore);\n  inline GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                           ArrayRef<Value *> IdxList, unsigned Values,\n                           const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(Value *Ptr, ArrayRef<Value *> IdxList, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  GetElementPtrInst *cloneImpl() const;\n\npublic:\n  static GetElementPtrInst *Create(Type *PointeeType, Value *Ptr,\n                                   ArrayRef<Value *> IdxList,\n                                   const Twine &NameStr = \"\",\n                                   Instruction *InsertBefore = nullptr) {\n    unsigned Values = 1 + unsigned(IdxList.size());\n    if (!PointeeType)\n      PointeeType =\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType();\n    else\n      assert(\n          PointeeType ==\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType());\n    return new (Values) GetElementPtrInst(PointeeType, Ptr, IdxList, Values,\n                                          NameStr, InsertBefore);\n  }\n\n  static GetElementPtrInst *Create(Type *PointeeType, Value *Ptr,\n                                   ArrayRef<Value *> IdxList,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd) {\n    unsigned Values = 1 + unsigned(IdxList.size());\n    if (!PointeeType)\n      PointeeType =\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType();\n    else\n      assert(\n          PointeeType ==\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType());\n    return new (Values) GetElementPtrInst(PointeeType, Ptr, IdxList, Values,\n                                          NameStr, InsertAtEnd);\n  }\n\n  /// Create an \"inbounds\" getelementptr. See the documentation for the\n  /// \"inbounds\" flag in LangRef.html for details.\n  static GetElementPtrInst *CreateInBounds(Value *Ptr,\n                                           ArrayRef<Value *> IdxList,\n                                           const Twine &NameStr = \"\",\n                                           Instruction *InsertBefore = nullptr){\n    return CreateInBounds(nullptr, Ptr, IdxList, NameStr, InsertBefore);\n  }\n\n  static GetElementPtrInst *\n  CreateInBounds(Type *PointeeType, Value *Ptr, ArrayRef<Value *> IdxList,\n                 const Twine &NameStr = \"\",\n                 Instruction *InsertBefore = nullptr) {\n    GetElementPtrInst *GEP =\n        Create(PointeeType, Ptr, IdxList, NameStr, InsertBefore);\n    GEP->setIsInBounds(true);\n    return GEP;\n  }\n\n  static GetElementPtrInst *CreateInBounds(Value *Ptr,\n                                           ArrayRef<Value *> IdxList,\n                                           const Twine &NameStr,\n                                           BasicBlock *InsertAtEnd) {\n    return CreateInBounds(nullptr, Ptr, IdxList, NameStr, InsertAtEnd);\n  }\n\n  static GetElementPtrInst *CreateInBounds(Type *PointeeType, Value *Ptr,\n                                           ArrayRef<Value *> IdxList,\n                                           const Twine &NameStr,\n                                           BasicBlock *InsertAtEnd) {\n    GetElementPtrInst *GEP =\n        Create(PointeeType, Ptr, IdxList, NameStr, InsertAtEnd);\n    GEP->setIsInBounds(true);\n    return GEP;\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  Type *getSourceElementType() const { return SourceElementType; }\n\n  void setSourceElementType(Type *Ty) { SourceElementType = Ty; }\n  void setResultElementType(Type *Ty) { ResultElementType = Ty; }\n\n  Type *getResultElementType() const {\n    assert(ResultElementType ==\n           cast<PointerType>(getType()->getScalarType())->getElementType());\n    return ResultElementType;\n  }\n\n  /// Returns the address space of this instruction's pointer type.\n  unsigned getAddressSpace() const {\n    // Note that this is always the same as the pointer operand's address space\n    // and that is cheaper to compute, so cheat here.\n    return getPointerAddressSpace();\n  }\n\n  /// Returns the result type of a getelementptr with the given source\n  /// element type and indexes.\n  ///\n  /// Null is returned if the indices are invalid for the specified\n  /// source element type.\n  static Type *getIndexedType(Type *Ty, ArrayRef<Value *> IdxList);\n  static Type *getIndexedType(Type *Ty, ArrayRef<Constant *> IdxList);\n  static Type *getIndexedType(Type *Ty, ArrayRef<uint64_t> IdxList);\n\n  /// Return the type of the element at the given index of an indexable\n  /// type.  This is equivalent to \"getIndexedType(Agg, {Zero, Idx})\".\n  ///\n  /// Returns null if the type can't be indexed, or the given index is not\n  /// legal for the given type.\n  static Type *getTypeAtIndex(Type *Ty, Value *Idx);\n  static Type *getTypeAtIndex(Type *Ty, uint64_t Idx);\n\n  inline op_iterator       idx_begin()       { return op_begin()+1; }\n  inline const_op_iterator idx_begin() const { return op_begin()+1; }\n  inline op_iterator       idx_end()         { return op_end(); }\n  inline const_op_iterator idx_end()   const { return op_end(); }\n\n  inline iterator_range<op_iterator> indices() {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  inline iterator_range<const_op_iterator> indices() const {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  Value *getPointerOperand() {\n    return getOperand(0);\n  }\n  const Value *getPointerOperand() const {\n    return getOperand(0);\n  }\n  static unsigned getPointerOperandIndex() {\n    return 0U;    // get index for modifying correct operand.\n  }\n\n  /// Method to return the pointer operand as a\n  /// PointerType.\n  Type *getPointerOperandType() const {\n    return getPointerOperand()->getType();\n  }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperandType()->getPointerAddressSpace();\n  }\n\n  /// Returns the pointer type returned by the GEP\n  /// instruction, which may be a vector of pointers.\n  static Type *getGEPReturnType(Type *ElTy, Value *Ptr,\n                                ArrayRef<Value *> IdxList) {\n    Type *PtrTy = PointerType::get(checkGEPType(getIndexedType(ElTy, IdxList)),\n                                   Ptr->getType()->getPointerAddressSpace());\n    // Vector GEP\n    if (auto *PtrVTy = dyn_cast<VectorType>(Ptr->getType())) {\n      ElementCount EltCount = PtrVTy->getElementCount();\n      return VectorType::get(PtrTy, EltCount);\n    }\n    for (Value *Index : IdxList)\n      if (auto *IndexVTy = dyn_cast<VectorType>(Index->getType())) {\n        ElementCount EltCount = IndexVTy->getElementCount();\n        return VectorType::get(PtrTy, EltCount);\n      }\n    // Scalar GEP\n    return PtrTy;\n  }\n\n  unsigned getNumIndices() const {  // Note: always non-negative\n    return getNumOperands() - 1;\n  }\n\n  bool hasIndices() const {\n    return getNumOperands() > 1;\n  }\n\n  /// Return true if all of the indices of this GEP are\n  /// zeros.  If so, the result pointer and the first operand have the same\n  /// value, just potentially different types.\n  bool hasAllZeroIndices() const;\n\n  /// Return true if all of the indices of this GEP are\n  /// constant integers.  If so, the result pointer and the first operand have\n  /// a constant offset between them.\n  bool hasAllConstantIndices() const;\n\n  /// Set or clear the inbounds flag on this GEP instruction.\n  /// See LangRef.html for the meaning of inbounds on a getelementptr.\n  void setIsInBounds(bool b = true);\n\n  /// Determine whether the GEP has the inbounds flag.\n  bool isInBounds() const;\n\n  /// Accumulate the constant address offset of this GEP if possible.\n  ///\n  /// This routine accepts an APInt into which it will accumulate the constant\n  /// offset of this GEP if the GEP is in fact constant. If the GEP is not\n  /// all-constant, it returns false and the value of the offset APInt is\n  /// undefined (it is *not* preserved!). The APInt passed into this routine\n  /// must be at least as wide as the IntPtr type for the address space of\n  /// the base GEP pointer.\n  bool accumulateConstantOffset(const DataLayout &DL, APInt &Offset) const;\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::GetElementPtr);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<GetElementPtrInst> :\n  public VariadicOperandTraits<GetElementPtrInst, 1> {\n};\n\nGetElementPtrInst::GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                                     ArrayRef<Value *> IdxList, unsigned Values,\n                                     const Twine &NameStr,\n                                     Instruction *InsertBefore)\n    : Instruction(getGEPReturnType(PointeeType, Ptr, IdxList), GetElementPtr,\n                  OperandTraits<GetElementPtrInst>::op_end(this) - Values,\n                  Values, InsertBefore),\n      SourceElementType(PointeeType),\n      ResultElementType(getIndexedType(PointeeType, IdxList)) {\n  assert(ResultElementType ==\n         cast<PointerType>(getType()->getScalarType())->getElementType());\n  init(Ptr, IdxList, NameStr);\n}\n\nGetElementPtrInst::GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                                     ArrayRef<Value *> IdxList, unsigned Values,\n                                     const Twine &NameStr,\n                                     BasicBlock *InsertAtEnd)\n    : Instruction(getGEPReturnType(PointeeType, Ptr, IdxList), GetElementPtr,\n                  OperandTraits<GetElementPtrInst>::op_end(this) - Values,\n                  Values, InsertAtEnd),\n      SourceElementType(PointeeType),\n      ResultElementType(getIndexedType(PointeeType, IdxList)) {\n  assert(ResultElementType ==\n         cast<PointerType>(getType()->getScalarType())->getElementType());\n  init(Ptr, IdxList, NameStr);\n}\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(GetElementPtrInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               ICmpInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction compares its operands according to the predicate given\n/// to the constructor. It only operates on integers or pointers. The operands\n/// must be identical types.\n/// Represent an integer comparison operator.\nclass ICmpInst: public CmpInst {\n  void AssertOK() {\n    assert(isIntPredicate() &&\n           \"Invalid ICmp predicate value\");\n    assert(getOperand(0)->getType() == getOperand(1)->getType() &&\n          \"Both operands to ICmp instruction are not of the same type!\");\n    // Check that the operands are the right type\n    assert((getOperand(0)->getType()->isIntOrIntVectorTy() ||\n            getOperand(0)->getType()->isPtrOrPtrVectorTy()) &&\n           \"Invalid operand types for ICmp instruction\");\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical ICmpInst\n  ICmpInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics.\n  ICmpInst(\n    Instruction *InsertBefore,  ///< Where to insert\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::ICmp, pred, LHS, RHS, NameStr,\n              InsertBefore) {\n#ifndef NDEBUG\n  AssertOK();\n#endif\n  }\n\n  /// Constructor with insert-at-end semantics.\n  ICmpInst(\n    BasicBlock &InsertAtEnd, ///< Block to insert into.\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::ICmp, pred, LHS, RHS, NameStr,\n              &InsertAtEnd) {\n#ifndef NDEBUG\n  AssertOK();\n#endif\n  }\n\n  /// Constructor with no-insertion semantics\n  ICmpInst(\n    Predicate pred, ///< The predicate to use for the comparison\n    Value *LHS,     ///< The left-hand-side of the expression\n    Value *RHS,     ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\" ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::ICmp, pred, LHS, RHS, NameStr) {\n#ifndef NDEBUG\n  AssertOK();\n#endif\n  }\n\n  /// For example, EQ->EQ, SLE->SLE, UGT->SGT, etc.\n  /// @returns the predicate that would be the result if the operand were\n  /// regarded as signed.\n  /// Return the signed version of the predicate\n  Predicate getSignedPredicate() const {\n    return getSignedPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction.\n  /// Return the signed version of the predicate.\n  static Predicate getSignedPredicate(Predicate pred);\n\n  /// For example, EQ->EQ, SLE->ULE, UGT->UGT, etc.\n  /// @returns the predicate that would be the result if the operand were\n  /// regarded as unsigned.\n  /// Return the unsigned version of the predicate\n  Predicate getUnsignedPredicate() const {\n    return getUnsignedPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction.\n  /// Return the unsigned version of the predicate.\n  static Predicate getUnsignedPredicate(Predicate pred);\n\n  /// Return true if this predicate is either EQ or NE.  This also\n  /// tests for commutativity.\n  static bool isEquality(Predicate P) {\n    return P == ICMP_EQ || P == ICMP_NE;\n  }\n\n  /// Return true if this predicate is either EQ or NE.  This also\n  /// tests for commutativity.\n  bool isEquality() const {\n    return isEquality(getPredicate());\n  }\n\n  /// @returns true if the predicate of this ICmpInst is commutative\n  /// Determine if this relation is commutative.\n  bool isCommutative() const { return isEquality(); }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  ///\n  bool isRelational() const {\n    return !isEquality();\n  }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  ///\n  static bool isRelational(Predicate P) {\n    return !isEquality(P);\n  }\n\n  /// Return true if the predicate is SGT or UGT.\n  ///\n  static bool isGT(Predicate P) {\n    return P == ICMP_SGT || P == ICMP_UGT;\n  }\n\n  /// Return true if the predicate is SLT or ULT.\n  ///\n  static bool isLT(Predicate P) {\n    return P == ICMP_SLT || P == ICMP_ULT;\n  }\n\n  /// Return true if the predicate is SGE or UGE.\n  ///\n  static bool isGE(Predicate P) {\n    return P == ICMP_SGE || P == ICMP_UGE;\n  }\n\n  /// Return true if the predicate is SLE or ULE.\n  ///\n  static bool isLE(Predicate P) {\n    return P == ICMP_SLE || P == ICMP_ULE;\n  }\n\n  /// Exchange the two operands to this instruction in such a way that it does\n  /// not modify the semantics of the instruction. The predicate value may be\n  /// changed to retain the same result if the predicate is order dependent\n  /// (e.g. ult).\n  /// Swap operands and adjust predicate.\n  void swapOperands() {\n    setPredicate(getSwappedPredicate());\n    Op<0>().swap(Op<1>());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ICmp;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               FCmpInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction compares its operands according to the predicate given\n/// to the constructor. It only operates on floating point values or packed\n/// vectors of floating point values. The operands must be identical types.\n/// Represents a floating point comparison operator.\nclass FCmpInst: public CmpInst {\n  void AssertOK() {\n    assert(isFPPredicate() && \"Invalid FCmp predicate value\");\n    assert(getOperand(0)->getType() == getOperand(1)->getType() &&\n           \"Both operands to FCmp instruction are not of the same type!\");\n    // Check that the operands are the right type\n    assert(getOperand(0)->getType()->isFPOrFPVectorTy() &&\n           \"Invalid operand types for FCmp instruction\");\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FCmpInst\n  FCmpInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics.\n  FCmpInst(\n    Instruction *InsertBefore, ///< Where to insert\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::FCmp, pred, LHS, RHS, NameStr,\n              InsertBefore) {\n    AssertOK();\n  }\n\n  /// Constructor with insert-at-end semantics.\n  FCmpInst(\n    BasicBlock &InsertAtEnd, ///< Block to insert into.\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::FCmp, pred, LHS, RHS, NameStr,\n              &InsertAtEnd) {\n    AssertOK();\n  }\n\n  /// Constructor with no-insertion semantics\n  FCmpInst(\n    Predicate Pred, ///< The predicate to use for the comparison\n    Value *LHS,     ///< The left-hand-side of the expression\n    Value *RHS,     ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\", ///< Name of the instruction\n    Instruction *FlagsSource = nullptr\n  ) : CmpInst(makeCmpResultType(LHS->getType()), Instruction::FCmp, Pred, LHS,\n              RHS, NameStr, nullptr, FlagsSource) {\n    AssertOK();\n  }\n\n  /// @returns true if the predicate of this instruction is EQ or NE.\n  /// Determine if this is an equality predicate.\n  static bool isEquality(Predicate Pred) {\n    return Pred == FCMP_OEQ || Pred == FCMP_ONE || Pred == FCMP_UEQ ||\n           Pred == FCMP_UNE;\n  }\n\n  /// @returns true if the predicate of this instruction is EQ or NE.\n  /// Determine if this is an equality predicate.\n  bool isEquality() const { return isEquality(getPredicate()); }\n\n  /// @returns true if the predicate of this instruction is commutative.\n  /// Determine if this is a commutative predicate.\n  bool isCommutative() const {\n    return isEquality() ||\n           getPredicate() == FCMP_FALSE ||\n           getPredicate() == FCMP_TRUE ||\n           getPredicate() == FCMP_ORD ||\n           getPredicate() == FCMP_UNO;\n  }\n\n  /// @returns true if the predicate is relational (not EQ or NE).\n  /// Determine if this a relational predicate.\n  bool isRelational() const { return !isEquality(); }\n\n  /// Exchange the two operands to this instruction in such a way that it does\n  /// not modify the semantics of the instruction. The predicate value may be\n  /// changed to retain the same result if the predicate is order dependent\n  /// (e.g. ult).\n  /// Swap operands and adjust predicate.\n  void swapOperands() {\n    setPredicate(getSwappedPredicate());\n    Op<0>().swap(Op<1>());\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::FCmp;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n/// This class represents a function call, abstracting a target\n/// machine's calling convention.  This class uses low bit of the SubClassData\n/// field to indicate whether or not this is a tail call.  The rest of the bits\n/// hold the calling convention of the call.\n///\nclass CallInst : public CallBase {\n  CallInst(const CallInst &CI);\n\n  /// Construct a CallInst given a range of arguments.\n  /// Construct a CallInst from a range of arguments\n  inline CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                  ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                  Instruction *InsertBefore);\n\n  inline CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                  const Twine &NameStr, Instruction *InsertBefore)\n      : CallInst(Ty, Func, Args, None, NameStr, InsertBefore) {}\n\n  /// Construct a CallInst given a range of arguments.\n  /// Construct a CallInst from a range of arguments\n  inline CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                  ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                  BasicBlock *InsertAtEnd);\n\n  explicit CallInst(FunctionType *Ty, Value *F, const Twine &NameStr,\n                    Instruction *InsertBefore);\n\n  CallInst(FunctionType *ty, Value *F, const Twine &NameStr,\n           BasicBlock *InsertAtEnd);\n\n  void init(FunctionType *FTy, Value *Func, ArrayRef<Value *> Args,\n            ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr);\n  void init(FunctionType *FTy, Value *Func, const Twine &NameStr);\n\n  /// Compute the number of operands to allocate.\n  static int ComputeNumOperands(int NumArgs, int NumBundleInputs = 0) {\n    // We need one operand for the called function, plus the input operand\n    // counts provided.\n    return 1 + NumArgs + NumBundleInputs;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CallInst *cloneImpl() const;\n\npublic:\n  static CallInst *Create(FunctionType *Ty, Value *F, const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    return new (ComputeNumOperands(0)) CallInst(Ty, F, NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr,\n                          Instruction *InsertBefore = nullptr) {\n    return new (ComputeNumOperands(Args.size()))\n        CallInst(Ty, Func, Args, None, NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles = None,\n                          const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    const int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    const unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallInst(Ty, Func, Args, Bundles, NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *F, const Twine &NameStr,\n                          BasicBlock *InsertAtEnd) {\n    return new (ComputeNumOperands(0)) CallInst(Ty, F, NameStr, InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return new (ComputeNumOperands(Args.size()))\n        CallInst(Ty, Func, Args, None, NameStr, InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    const int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    const unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallInst(Ty, Func, Args, Bundles, NameStr, InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionCallee Func, const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), NameStr,\n                  InsertBefore);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles = None,\n                          const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, Bundles,\n                  NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr,\n                          Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, NameStr,\n                  InsertBefore);\n  }\n\n  static CallInst *Create(FunctionCallee Func, const Twine &NameStr,\n                          BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), NameStr,\n                  InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, NameStr,\n                  InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, Bundles,\n                  NameStr, InsertAtEnd);\n  }\n\n  /// Create a clone of \\p CI with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned call instruction is identical \\p CI in every way except that\n  /// the operand bundles for the new instruction are set to the operand bundles\n  /// in \\p Bundles.\n  static CallInst *Create(CallInst *CI, ArrayRef<OperandBundleDef> Bundles,\n                          Instruction *InsertPt = nullptr);\n\n  /// Generate the IR for a call to malloc:\n  /// 1. Compute the malloc call's argument as the specified type's size,\n  ///    possibly multiplied by the array size if the array size is not\n  ///    constant 1.\n  /// 2. Call malloc with that argument.\n  /// 3. Bitcast the result of the malloc call to the specified type.\n  static Instruction *CreateMalloc(Instruction *InsertBefore, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  static Instruction *CreateMalloc(BasicBlock *InsertAtEnd, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  static Instruction *CreateMalloc(Instruction *InsertBefore, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   ArrayRef<OperandBundleDef> Bundles = None,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  static Instruction *CreateMalloc(BasicBlock *InsertAtEnd, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   ArrayRef<OperandBundleDef> Bundles = None,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  /// Generate the IR for a call to the builtin free function.\n  static Instruction *CreateFree(Value *Source, Instruction *InsertBefore);\n  static Instruction *CreateFree(Value *Source, BasicBlock *InsertAtEnd);\n  static Instruction *CreateFree(Value *Source,\n                                 ArrayRef<OperandBundleDef> Bundles,\n                                 Instruction *InsertBefore);\n  static Instruction *CreateFree(Value *Source,\n                                 ArrayRef<OperandBundleDef> Bundles,\n                                 BasicBlock *InsertAtEnd);\n\n  // Note that 'musttail' implies 'tail'.\n  enum TailCallKind : unsigned {\n    TCK_None = 0,\n    TCK_Tail = 1,\n    TCK_MustTail = 2,\n    TCK_NoTail = 3,\n    TCK_LAST = TCK_NoTail\n  };\n\n  using TailCallKindField = Bitfield::Element<TailCallKind, 0, 2, TCK_LAST>;\n  static_assert(\n      Bitfield::areContiguous<TailCallKindField, CallBase::CallingConvField>(),\n      \"Bitfields must be contiguous\");\n\n  TailCallKind getTailCallKind() const {\n    return getSubclassData<TailCallKindField>();\n  }\n\n  bool isTailCall() const {\n    TailCallKind Kind = getTailCallKind();\n    return Kind == TCK_Tail || Kind == TCK_MustTail;\n  }\n\n  bool isMustTailCall() const { return getTailCallKind() == TCK_MustTail; }\n\n  bool isNoTailCall() const { return getTailCallKind() == TCK_NoTail; }\n\n  void setTailCallKind(TailCallKind TCK) {\n    setSubclassData<TailCallKindField>(TCK);\n  }\n\n  void setTailCall(bool IsTc = true) {\n    setTailCallKind(IsTc ? TCK_Tail : TCK_None);\n  }\n\n  /// Return true if the call can return twice\n  bool canReturnTwice() const { return hasFnAttr(Attribute::ReturnsTwice); }\n  void setCanReturnTwice() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ReturnsTwice);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Call;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  /// Updates profile metadata by scaling it by \\p S / \\p T.\n  void updateProfWeight(uint64_t S, uint64_t T);\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\nCallInst::CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                   ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                   BasicBlock *InsertAtEnd)\n    : CallBase(Ty->getReturnType(), Instruction::Call,\n               OperandTraits<CallBase>::op_end(this) -\n                   (Args.size() + CountBundleInputs(Bundles) + 1),\n               unsigned(Args.size() + CountBundleInputs(Bundles) + 1),\n               InsertAtEnd) {\n  init(Ty, Func, Args, Bundles, NameStr);\n}\n\nCallInst::CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                   ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                   Instruction *InsertBefore)\n    : CallBase(Ty->getReturnType(), Instruction::Call,\n               OperandTraits<CallBase>::op_end(this) -\n                   (Args.size() + CountBundleInputs(Bundles) + 1),\n               unsigned(Args.size() + CountBundleInputs(Bundles) + 1),\n               InsertBefore) {\n  init(Ty, Func, Args, Bundles, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                               SelectInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents the LLVM 'select' instruction.\n///\nclass SelectInst : public Instruction {\n  SelectInst(Value *C, Value *S1, Value *S2, const Twine &NameStr,\n             Instruction *InsertBefore)\n    : Instruction(S1->getType(), Instruction::Select,\n                  &Op<0>(), 3, InsertBefore) {\n    init(C, S1, S2);\n    setName(NameStr);\n  }\n\n  SelectInst(Value *C, Value *S1, Value *S2, const Twine &NameStr,\n             BasicBlock *InsertAtEnd)\n    : Instruction(S1->getType(), Instruction::Select,\n                  &Op<0>(), 3, InsertAtEnd) {\n    init(C, S1, S2);\n    setName(NameStr);\n  }\n\n  void init(Value *C, Value *S1, Value *S2) {\n    assert(!areInvalidOperands(C, S1, S2) && \"Invalid operands for select\");\n    Op<0>() = C;\n    Op<1>() = S1;\n    Op<2>() = S2;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  SelectInst *cloneImpl() const;\n\npublic:\n  static SelectInst *Create(Value *C, Value *S1, Value *S2,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr,\n                            Instruction *MDFrom = nullptr) {\n    SelectInst *Sel = new(3) SelectInst(C, S1, S2, NameStr, InsertBefore);\n    if (MDFrom)\n      Sel->copyMetadata(*MDFrom);\n    return Sel;\n  }\n\n  static SelectInst *Create(Value *C, Value *S1, Value *S2,\n                            const Twine &NameStr,\n                            BasicBlock *InsertAtEnd) {\n    return new(3) SelectInst(C, S1, S2, NameStr, InsertAtEnd);\n  }\n\n  const Value *getCondition() const { return Op<0>(); }\n  const Value *getTrueValue() const { return Op<1>(); }\n  const Value *getFalseValue() const { return Op<2>(); }\n  Value *getCondition() { return Op<0>(); }\n  Value *getTrueValue() { return Op<1>(); }\n  Value *getFalseValue() { return Op<2>(); }\n\n  void setCondition(Value *V) { Op<0>() = V; }\n  void setTrueValue(Value *V) { Op<1>() = V; }\n  void setFalseValue(Value *V) { Op<2>() = V; }\n\n  /// Swap the true and false values of the select instruction.\n  /// This doesn't swap prof metadata.\n  void swapValues() { Op<1>().swap(Op<2>()); }\n\n  /// Return a string if the specified operands are invalid\n  /// for a select operation, otherwise return null.\n  static const char *areInvalidOperands(Value *Cond, Value *True, Value *False);\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  OtherOps getOpcode() const {\n    return static_cast<OtherOps>(Instruction::getOpcode());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Select;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<SelectInst> : public FixedNumOperandTraits<SelectInst, 3> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(SelectInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                VAArgInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents the va_arg llvm instruction, which returns\n/// an argument of the specified type given a va_list and increments that list\n///\nclass VAArgInst : public UnaryInstruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  VAArgInst *cloneImpl() const;\n\npublic:\n  VAArgInst(Value *List, Type *Ty, const Twine &NameStr = \"\",\n             Instruction *InsertBefore = nullptr)\n    : UnaryInstruction(Ty, VAArg, List, InsertBefore) {\n    setName(NameStr);\n  }\n\n  VAArgInst(Value *List, Type *Ty, const Twine &NameStr,\n            BasicBlock *InsertAtEnd)\n    : UnaryInstruction(Ty, VAArg, List, InsertAtEnd) {\n    setName(NameStr);\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == VAArg;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                ExtractElementInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction extracts a single (scalar)\n/// element from a VectorType value\n///\nclass ExtractElementInst : public Instruction {\n  ExtractElementInst(Value *Vec, Value *Idx, const Twine &NameStr = \"\",\n                     Instruction *InsertBefore = nullptr);\n  ExtractElementInst(Value *Vec, Value *Idx, const Twine &NameStr,\n                     BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ExtractElementInst *cloneImpl() const;\n\npublic:\n  static ExtractElementInst *Create(Value *Vec, Value *Idx,\n                                   const Twine &NameStr = \"\",\n                                   Instruction *InsertBefore = nullptr) {\n    return new(2) ExtractElementInst(Vec, Idx, NameStr, InsertBefore);\n  }\n\n  static ExtractElementInst *Create(Value *Vec, Value *Idx,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd) {\n    return new(2) ExtractElementInst(Vec, Idx, NameStr, InsertAtEnd);\n  }\n\n  /// Return true if an extractelement instruction can be\n  /// formed with the specified operands.\n  static bool isValidOperands(const Value *Vec, const Value *Idx);\n\n  Value *getVectorOperand() { return Op<0>(); }\n  Value *getIndexOperand() { return Op<1>(); }\n  const Value *getVectorOperand() const { return Op<0>(); }\n  const Value *getIndexOperand() const { return Op<1>(); }\n\n  VectorType *getVectorOperandType() const {\n    return cast<VectorType>(getVectorOperand()->getType());\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ExtractElement;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ExtractElementInst> :\n  public FixedNumOperandTraits<ExtractElementInst, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ExtractElementInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                InsertElementInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction inserts a single (scalar)\n/// element into a VectorType value\n///\nclass InsertElementInst : public Instruction {\n  InsertElementInst(Value *Vec, Value *NewElt, Value *Idx,\n                    const Twine &NameStr = \"\",\n                    Instruction *InsertBefore = nullptr);\n  InsertElementInst(Value *Vec, Value *NewElt, Value *Idx, const Twine &NameStr,\n                    BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  InsertElementInst *cloneImpl() const;\n\npublic:\n  static InsertElementInst *Create(Value *Vec, Value *NewElt, Value *Idx,\n                                   const Twine &NameStr = \"\",\n                                   Instruction *InsertBefore = nullptr) {\n    return new(3) InsertElementInst(Vec, NewElt, Idx, NameStr, InsertBefore);\n  }\n\n  static InsertElementInst *Create(Value *Vec, Value *NewElt, Value *Idx,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd) {\n    return new(3) InsertElementInst(Vec, NewElt, Idx, NameStr, InsertAtEnd);\n  }\n\n  /// Return true if an insertelement instruction can be\n  /// formed with the specified operands.\n  static bool isValidOperands(const Value *Vec, const Value *NewElt,\n                              const Value *Idx);\n\n  /// Overload to return most specific vector type.\n  ///\n  VectorType *getType() const {\n    return cast<VectorType>(Instruction::getType());\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::InsertElement;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<InsertElementInst> :\n  public FixedNumOperandTraits<InsertElementInst, 3> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(InsertElementInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                           ShuffleVectorInst Class\n//===----------------------------------------------------------------------===//\n\nconstexpr int UndefMaskElem = -1;\n\n/// This instruction constructs a fixed permutation of two\n/// input vectors.\n///\n/// For each element of the result vector, the shuffle mask selects an element\n/// from one of the input vectors to copy to the result. Non-negative elements\n/// in the mask represent an index into the concatenated pair of input vectors.\n/// UndefMaskElem (-1) specifies that the result element is undefined.\n///\n/// For scalable vectors, all the elements of the mask must be 0 or -1. This\n/// requirement may be relaxed in the future.\nclass ShuffleVectorInst : public Instruction {\n  SmallVector<int, 4> ShuffleMask;\n  Constant *ShuffleMaskForBitcode;\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ShuffleVectorInst *cloneImpl() const;\n\npublic:\n  ShuffleVectorInst(Value *V1, Value *V2, Value *Mask,\n                    const Twine &NameStr = \"\",\n                    Instruction *InsertBefor = nullptr);\n  ShuffleVectorInst(Value *V1, Value *V2, Value *Mask,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n  ShuffleVectorInst(Value *V1, Value *V2, ArrayRef<int> Mask,\n                    const Twine &NameStr = \"\",\n                    Instruction *InsertBefor = nullptr);\n  ShuffleVectorInst(Value *V1, Value *V2, ArrayRef<int> Mask,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void *operator new(size_t s) { return User::operator new(s, 2); }\n\n  /// Swap the operands and adjust the mask to preserve the semantics\n  /// of the instruction.\n  void commute();\n\n  /// Return true if a shufflevector instruction can be\n  /// formed with the specified operands.\n  static bool isValidOperands(const Value *V1, const Value *V2,\n                              const Value *Mask);\n  static bool isValidOperands(const Value *V1, const Value *V2,\n                              ArrayRef<int> Mask);\n\n  /// Overload to return most specific vector type.\n  ///\n  VectorType *getType() const {\n    return cast<VectorType>(Instruction::getType());\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Return the shuffle mask value of this instruction for the given element\n  /// index. Return UndefMaskElem if the element is undef.\n  int getMaskValue(unsigned Elt) const { return ShuffleMask[Elt]; }\n\n  /// Convert the input shuffle mask operand to a vector of integers. Undefined\n  /// elements of the mask are returned as UndefMaskElem.\n  static void getShuffleMask(const Constant *Mask,\n                             SmallVectorImpl<int> &Result);\n\n  /// Return the mask for this instruction as a vector of integers. Undefined\n  /// elements of the mask are returned as UndefMaskElem.\n  void getShuffleMask(SmallVectorImpl<int> &Result) const {\n    Result.assign(ShuffleMask.begin(), ShuffleMask.end());\n  }\n\n  /// Return the mask for this instruction, for use in bitcode.\n  ///\n  /// TODO: This is temporary until we decide a new bitcode encoding for\n  /// shufflevector.\n  Constant *getShuffleMaskForBitcode() const { return ShuffleMaskForBitcode; }\n\n  static Constant *convertShuffleMaskForBitcode(ArrayRef<int> Mask,\n                                                Type *ResultTy);\n\n  void setShuffleMask(ArrayRef<int> Mask);\n\n  ArrayRef<int> getShuffleMask() const { return ShuffleMask; }\n\n  /// Return true if this shuffle returns a vector with a different number of\n  /// elements than its source vectors.\n  /// Examples: shufflevector <4 x n> A, <4 x n> B, <1,2,3>\n  ///           shufflevector <4 x n> A, <4 x n> B, <1,2,3,4,5>\n  bool changesLength() const {\n    unsigned NumSourceElts = cast<VectorType>(Op<0>()->getType())\n                                 ->getElementCount()\n                                 .getKnownMinValue();\n    unsigned NumMaskElts = ShuffleMask.size();\n    return NumSourceElts != NumMaskElts;\n  }\n\n  /// Return true if this shuffle returns a vector with a greater number of\n  /// elements than its source vectors.\n  /// Example: shufflevector <2 x n> A, <2 x n> B, <1,2,3>\n  bool increasesLength() const {\n    unsigned NumSourceElts = cast<VectorType>(Op<0>()->getType())\n                                 ->getElementCount()\n                                 .getKnownMinValue();\n    unsigned NumMaskElts = ShuffleMask.size();\n    return NumSourceElts < NumMaskElts;\n  }\n\n  /// Return true if this shuffle mask chooses elements from exactly one source\n  /// vector.\n  /// Example: <7,5,undef,7>\n  /// This assumes that vector operands are the same length as the mask.\n  static bool isSingleSourceMask(ArrayRef<int> Mask);\n  static bool isSingleSourceMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isSingleSourceMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle chooses elements from exactly one source\n  /// vector without changing the length of that vector.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <3,0,undef,3>\n  /// TODO: Optionally allow length-changing shuffles.\n  bool isSingleSource() const {\n    return !changesLength() && isSingleSourceMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask chooses elements from exactly one source\n  /// vector without lane crossings. A shuffle using this mask is not\n  /// necessarily a no-op because it may change the number of elements from its\n  /// input vectors or it may provide demanded bits knowledge via undef lanes.\n  /// Example: <undef,undef,2,3>\n  static bool isIdentityMask(ArrayRef<int> Mask);\n  static bool isIdentityMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isIdentityMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle chooses elements from exactly one source\n  /// vector without lane crossings and does not change the number of elements\n  /// from its input vectors.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <4,undef,6,undef>\n  bool isIdentity() const {\n    return !changesLength() && isIdentityMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle lengthens exactly one source vector with\n  /// undefs in the high elements.\n  bool isIdentityWithPadding() const;\n\n  /// Return true if this shuffle extracts the first N elements of exactly one\n  /// source vector.\n  bool isIdentityWithExtract() const;\n\n  /// Return true if this shuffle concatenates its 2 source vectors. This\n  /// returns false if either input is undefined. In that case, the shuffle is\n  /// is better classified as an identity with padding operation.\n  bool isConcat() const;\n\n  /// Return true if this shuffle mask chooses elements from its source vectors\n  /// without lane crossings. A shuffle using this mask would be\n  /// equivalent to a vector select with a constant condition operand.\n  /// Example: <4,1,6,undef>\n  /// This returns false if the mask does not choose from both input vectors.\n  /// In that case, the shuffle is better classified as an identity shuffle.\n  /// This assumes that vector operands are the same length as the mask\n  /// (a length-changing shuffle can never be equivalent to a vector select).\n  static bool isSelectMask(ArrayRef<int> Mask);\n  static bool isSelectMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isSelectMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle chooses elements from its source vectors\n  /// without lane crossings and all operands have the same number of elements.\n  /// In other words, this shuffle is equivalent to a vector select with a\n  /// constant condition operand.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <undef,1,6,3>\n  /// This returns false if the mask does not choose from both input vectors.\n  /// In that case, the shuffle is better classified as an identity shuffle.\n  /// TODO: Optionally allow length-changing shuffles.\n  bool isSelect() const {\n    return !changesLength() && isSelectMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask swaps the order of elements from exactly\n  /// one source vector.\n  /// Example: <7,6,undef,4>\n  /// This assumes that vector operands are the same length as the mask.\n  static bool isReverseMask(ArrayRef<int> Mask);\n  static bool isReverseMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isReverseMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle swaps the order of elements from exactly\n  /// one source vector.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <3,undef,1,undef>\n  /// TODO: Optionally allow length-changing shuffles.\n  bool isReverse() const {\n    return !changesLength() && isReverseMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask chooses all elements with the same value\n  /// as the first element of exactly one source vector.\n  /// Example: <4,undef,undef,4>\n  /// This assumes that vector operands are the same length as the mask.\n  static bool isZeroEltSplatMask(ArrayRef<int> Mask);\n  static bool isZeroEltSplatMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isZeroEltSplatMask(MaskAsInts);\n  }\n\n  /// Return true if all elements of this shuffle are the same value as the\n  /// first element of exactly one source vector without changing the length\n  /// of that vector.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <undef,0,undef,0>\n  /// TODO: Optionally allow length-changing shuffles.\n  /// TODO: Optionally allow splats from other elements.\n  bool isZeroEltSplat() const {\n    return !changesLength() && isZeroEltSplatMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask is a transpose mask.\n  /// Transpose vector masks transpose a 2xn matrix. They read corresponding\n  /// even- or odd-numbered vector elements from two n-dimensional source\n  /// vectors and write each result into consecutive elements of an\n  /// n-dimensional destination vector. Two shuffles are necessary to complete\n  /// the transpose, one for the even elements and another for the odd elements.\n  /// This description closely follows how the TRN1 and TRN2 AArch64\n  /// instructions operate.\n  ///\n  /// For example, a simple 2x2 matrix can be transposed with:\n  ///\n  ///   ; Original matrix\n  ///   m0 = < a, b >\n  ///   m1 = < c, d >\n  ///\n  ///   ; Transposed matrix\n  ///   t0 = < a, c > = shufflevector m0, m1, < 0, 2 >\n  ///   t1 = < b, d > = shufflevector m0, m1, < 1, 3 >\n  ///\n  /// For matrices having greater than n columns, the resulting nx2 transposed\n  /// matrix is stored in two result vectors such that one vector contains\n  /// interleaved elements from all the even-numbered rows and the other vector\n  /// contains interleaved elements from all the odd-numbered rows. For example,\n  /// a 2x4 matrix can be transposed with:\n  ///\n  ///   ; Original matrix\n  ///   m0 = < a, b, c, d >\n  ///   m1 = < e, f, g, h >\n  ///\n  ///   ; Transposed matrix\n  ///   t0 = < a, e, c, g > = shufflevector m0, m1 < 0, 4, 2, 6 >\n  ///   t1 = < b, f, d, h > = shufflevector m0, m1 < 1, 5, 3, 7 >\n  static bool isTransposeMask(ArrayRef<int> Mask);\n  static bool isTransposeMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isTransposeMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle transposes the elements of its inputs without\n  /// changing the length of the vectors. This operation may also be known as a\n  /// merge or interleave. See the description for isTransposeMask() for the\n  /// exact specification.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <0,4,2,6>\n  bool isTranspose() const {\n    return !changesLength() && isTransposeMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask is an extract subvector mask.\n  /// A valid extract subvector mask returns a smaller vector from a single\n  /// source operand. The base extraction index is returned as well.\n  static bool isExtractSubvectorMask(ArrayRef<int> Mask, int NumSrcElts,\n                                     int &Index);\n  static bool isExtractSubvectorMask(const Constant *Mask, int NumSrcElts,\n                                     int &Index) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    // Not possible to express a shuffle mask for a scalable vector for this\n    // case.\n    if (isa<ScalableVectorType>(Mask->getType()))\n      return false;\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isExtractSubvectorMask(MaskAsInts, NumSrcElts, Index);\n  }\n\n  /// Return true if this shuffle mask is an extract subvector mask.\n  bool isExtractSubvectorMask(int &Index) const {\n    // Not possible to express a shuffle mask for a scalable vector for this\n    // case.\n    if (isa<ScalableVectorType>(getType()))\n      return false;\n\n    int NumSrcElts =\n        cast<FixedVectorType>(Op<0>()->getType())->getNumElements();\n    return isExtractSubvectorMask(ShuffleMask, NumSrcElts, Index);\n  }\n\n  /// Change values in a shuffle permute mask assuming the two vector operands\n  /// of length InVecNumElts have swapped position.\n  static void commuteShuffleMask(MutableArrayRef<int> Mask,\n                                 unsigned InVecNumElts) {\n    for (int &Idx : Mask) {\n      if (Idx == -1)\n        continue;\n      Idx = Idx < (int)InVecNumElts ? Idx + InVecNumElts : Idx - InVecNumElts;\n      assert(Idx >= 0 && Idx < (int)InVecNumElts * 2 &&\n             \"shufflevector mask index out of range\");\n    }\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ShuffleVector;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ShuffleVectorInst>\n    : public FixedNumOperandTraits<ShuffleVectorInst, 2> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ShuffleVectorInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                ExtractValueInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction extracts a struct member or array\n/// element value from an aggregate value.\n///\nclass ExtractValueInst : public UnaryInstruction {\n  SmallVector<unsigned, 4> Indices;\n\n  ExtractValueInst(const ExtractValueInst &EVI);\n\n  /// Constructors - Create a extractvalue instruction with a base aggregate\n  /// value and a list of indices.  The first ctor can optionally insert before\n  /// an existing instruction, the second appends the new instruction to the\n  /// specified BasicBlock.\n  inline ExtractValueInst(Value *Agg,\n                          ArrayRef<unsigned> Idxs,\n                          const Twine &NameStr,\n                          Instruction *InsertBefore);\n  inline ExtractValueInst(Value *Agg,\n                          ArrayRef<unsigned> Idxs,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(ArrayRef<unsigned> Idxs, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ExtractValueInst *cloneImpl() const;\n\npublic:\n  static ExtractValueInst *Create(Value *Agg,\n                                  ArrayRef<unsigned> Idxs,\n                                  const Twine &NameStr = \"\",\n                                  Instruction *InsertBefore = nullptr) {\n    return new\n      ExtractValueInst(Agg, Idxs, NameStr, InsertBefore);\n  }\n\n  static ExtractValueInst *Create(Value *Agg,\n                                  ArrayRef<unsigned> Idxs,\n                                  const Twine &NameStr,\n                                  BasicBlock *InsertAtEnd) {\n    return new ExtractValueInst(Agg, Idxs, NameStr, InsertAtEnd);\n  }\n\n  /// Returns the type of the element that would be extracted\n  /// with an extractvalue instruction with the specified parameters.\n  ///\n  /// Null is returned if the indices are invalid for the specified type.\n  static Type *getIndexedType(Type *Agg, ArrayRef<unsigned> Idxs);\n\n  using idx_iterator = const unsigned*;\n\n  inline idx_iterator idx_begin() const { return Indices.begin(); }\n  inline idx_iterator idx_end()   const { return Indices.end(); }\n  inline iterator_range<idx_iterator> indices() const {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  Value *getAggregateOperand() {\n    return getOperand(0);\n  }\n  const Value *getAggregateOperand() const {\n    return getOperand(0);\n  }\n  static unsigned getAggregateOperandIndex() {\n    return 0U;                      // get index for modifying correct operand\n  }\n\n  ArrayRef<unsigned> getIndices() const {\n    return Indices;\n  }\n\n  unsigned getNumIndices() const {\n    return (unsigned)Indices.size();\n  }\n\n  bool hasIndices() const {\n    return true;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ExtractValue;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\nExtractValueInst::ExtractValueInst(Value *Agg,\n                                   ArrayRef<unsigned> Idxs,\n                                   const Twine &NameStr,\n                                   Instruction *InsertBefore)\n  : UnaryInstruction(checkGEPType(getIndexedType(Agg->getType(), Idxs)),\n                     ExtractValue, Agg, InsertBefore) {\n  init(Idxs, NameStr);\n}\n\nExtractValueInst::ExtractValueInst(Value *Agg,\n                                   ArrayRef<unsigned> Idxs,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd)\n  : UnaryInstruction(checkGEPType(getIndexedType(Agg->getType(), Idxs)),\n                     ExtractValue, Agg, InsertAtEnd) {\n  init(Idxs, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                                InsertValueInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction inserts a struct field of array element\n/// value into an aggregate value.\n///\nclass InsertValueInst : public Instruction {\n  SmallVector<unsigned, 4> Indices;\n\n  InsertValueInst(const InsertValueInst &IVI);\n\n  /// Constructors - Create a insertvalue instruction with a base aggregate\n  /// value, a value to insert, and a list of indices.  The first ctor can\n  /// optionally insert before an existing instruction, the second appends\n  /// the new instruction to the specified BasicBlock.\n  inline InsertValueInst(Value *Agg, Value *Val,\n                         ArrayRef<unsigned> Idxs,\n                         const Twine &NameStr,\n                         Instruction *InsertBefore);\n  inline InsertValueInst(Value *Agg, Value *Val,\n                         ArrayRef<unsigned> Idxs,\n                         const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  /// Constructors - These two constructors are convenience methods because one\n  /// and two index insertvalue instructions are so common.\n  InsertValueInst(Value *Agg, Value *Val, unsigned Idx,\n                  const Twine &NameStr = \"\",\n                  Instruction *InsertBefore = nullptr);\n  InsertValueInst(Value *Agg, Value *Val, unsigned Idx, const Twine &NameStr,\n                  BasicBlock *InsertAtEnd);\n\n  void init(Value *Agg, Value *Val, ArrayRef<unsigned> Idxs,\n            const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  InsertValueInst *cloneImpl() const;\n\npublic:\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  static InsertValueInst *Create(Value *Agg, Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr = \"\",\n                                 Instruction *InsertBefore = nullptr) {\n    return new InsertValueInst(Agg, Val, Idxs, NameStr, InsertBefore);\n  }\n\n  static InsertValueInst *Create(Value *Agg, Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr,\n                                 BasicBlock *InsertAtEnd) {\n    return new InsertValueInst(Agg, Val, Idxs, NameStr, InsertAtEnd);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  using idx_iterator = const unsigned*;\n\n  inline idx_iterator idx_begin() const { return Indices.begin(); }\n  inline idx_iterator idx_end()   const { return Indices.end(); }\n  inline iterator_range<idx_iterator> indices() const {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  Value *getAggregateOperand() {\n    return getOperand(0);\n  }\n  const Value *getAggregateOperand() const {\n    return getOperand(0);\n  }\n  static unsigned getAggregateOperandIndex() {\n    return 0U;                      // get index for modifying correct operand\n  }\n\n  Value *getInsertedValueOperand() {\n    return getOperand(1);\n  }\n  const Value *getInsertedValueOperand() const {\n    return getOperand(1);\n  }\n  static unsigned getInsertedValueOperandIndex() {\n    return 1U;                      // get index for modifying correct operand\n  }\n\n  ArrayRef<unsigned> getIndices() const {\n    return Indices;\n  }\n\n  unsigned getNumIndices() const {\n    return (unsigned)Indices.size();\n  }\n\n  bool hasIndices() const {\n    return true;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::InsertValue;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<InsertValueInst> :\n  public FixedNumOperandTraits<InsertValueInst, 2> {\n};\n\nInsertValueInst::InsertValueInst(Value *Agg,\n                                 Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr,\n                                 Instruction *InsertBefore)\n  : Instruction(Agg->getType(), InsertValue,\n                OperandTraits<InsertValueInst>::op_begin(this),\n                2, InsertBefore) {\n  init(Agg, Val, Idxs, NameStr);\n}\n\nInsertValueInst::InsertValueInst(Value *Agg,\n                                 Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr,\n                                 BasicBlock *InsertAtEnd)\n  : Instruction(Agg->getType(), InsertValue,\n                OperandTraits<InsertValueInst>::op_begin(this),\n                2, InsertAtEnd) {\n  init(Agg, Val, Idxs, NameStr);\n}\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(InsertValueInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               PHINode Class\n//===----------------------------------------------------------------------===//\n\n// PHINode - The PHINode class is used to represent the magical mystical PHI\n// node, that can not exist in nature, but can be synthesized in a computer\n// scientist's overactive imagination.\n//\nclass PHINode : public Instruction {\n  /// The number of operands actually allocated.  NumOperands is\n  /// the number actually in use.\n  unsigned ReservedSpace;\n\n  PHINode(const PHINode &PN);\n\n  explicit PHINode(Type *Ty, unsigned NumReservedValues,\n                   const Twine &NameStr = \"\",\n                   Instruction *InsertBefore = nullptr)\n    : Instruction(Ty, Instruction::PHI, nullptr, 0, InsertBefore),\n      ReservedSpace(NumReservedValues) {\n    setName(NameStr);\n    allocHungoffUses(ReservedSpace);\n  }\n\n  PHINode(Type *Ty, unsigned NumReservedValues, const Twine &NameStr,\n          BasicBlock *InsertAtEnd)\n    : Instruction(Ty, Instruction::PHI, nullptr, 0, InsertAtEnd),\n      ReservedSpace(NumReservedValues) {\n    setName(NameStr);\n    allocHungoffUses(ReservedSpace);\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  PHINode *cloneImpl() const;\n\n  // allocHungoffUses - this is more complicated than the generic\n  // User::allocHungoffUses, because we have to allocate Uses for the incoming\n  // values and pointers to the incoming blocks, all in one allocation.\n  void allocHungoffUses(unsigned N) {\n    User::allocHungoffUses(N, /* IsPhi */ true);\n  }\n\npublic:\n  /// Constructors - NumReservedValues is a hint for the number of incoming\n  /// edges that this phi node will have (use 0 if you really have no idea).\n  static PHINode *Create(Type *Ty, unsigned NumReservedValues,\n                         const Twine &NameStr = \"\",\n                         Instruction *InsertBefore = nullptr) {\n    return new PHINode(Ty, NumReservedValues, NameStr, InsertBefore);\n  }\n\n  static PHINode *Create(Type *Ty, unsigned NumReservedValues,\n                         const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return new PHINode(Ty, NumReservedValues, NameStr, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Block iterator interface. This provides access to the list of incoming\n  // basic blocks, which parallels the list of incoming values.\n\n  using block_iterator = BasicBlock **;\n  using const_block_iterator = BasicBlock * const *;\n\n  block_iterator block_begin() {\n    return reinterpret_cast<block_iterator>(op_begin() + ReservedSpace);\n  }\n\n  const_block_iterator block_begin() const {\n    return reinterpret_cast<const_block_iterator>(op_begin() + ReservedSpace);\n  }\n\n  block_iterator block_end() {\n    return block_begin() + getNumOperands();\n  }\n\n  const_block_iterator block_end() const {\n    return block_begin() + getNumOperands();\n  }\n\n  iterator_range<block_iterator> blocks() {\n    return make_range(block_begin(), block_end());\n  }\n\n  iterator_range<const_block_iterator> blocks() const {\n    return make_range(block_begin(), block_end());\n  }\n\n  op_range incoming_values() { return operands(); }\n\n  const_op_range incoming_values() const { return operands(); }\n\n  /// Return the number of incoming edges\n  ///\n  unsigned getNumIncomingValues() const { return getNumOperands(); }\n\n  /// Return incoming value number x\n  ///\n  Value *getIncomingValue(unsigned i) const {\n    return getOperand(i);\n  }\n  void setIncomingValue(unsigned i, Value *V) {\n    assert(V && \"PHI node got a null value!\");\n    assert(getType() == V->getType() &&\n           \"All operands to PHI node must be the same type as the PHI node!\");\n    setOperand(i, V);\n  }\n\n  static unsigned getOperandNumForIncomingValue(unsigned i) {\n    return i;\n  }\n\n  static unsigned getIncomingValueNumForOperand(unsigned i) {\n    return i;\n  }\n\n  /// Return incoming basic block number @p i.\n  ///\n  BasicBlock *getIncomingBlock(unsigned i) const {\n    return block_begin()[i];\n  }\n\n  /// Return incoming basic block corresponding\n  /// to an operand of the PHI.\n  ///\n  BasicBlock *getIncomingBlock(const Use &U) const {\n    assert(this == U.getUser() && \"Iterator doesn't point to PHI's Uses?\");\n    return getIncomingBlock(unsigned(&U - op_begin()));\n  }\n\n  /// Return incoming basic block corresponding\n  /// to value use iterator.\n  ///\n  BasicBlock *getIncomingBlock(Value::const_user_iterator I) const {\n    return getIncomingBlock(I.getUse());\n  }\n\n  void setIncomingBlock(unsigned i, BasicBlock *BB) {\n    assert(BB && \"PHI node got a null basic block!\");\n    block_begin()[i] = BB;\n  }\n\n  /// Replace every incoming basic block \\p Old to basic block \\p New.\n  void replaceIncomingBlockWith(const BasicBlock *Old, BasicBlock *New) {\n    assert(New && Old && \"PHI node got a null basic block!\");\n    for (unsigned Op = 0, NumOps = getNumOperands(); Op != NumOps; ++Op)\n      if (getIncomingBlock(Op) == Old)\n        setIncomingBlock(Op, New);\n  }\n\n  /// Add an incoming value to the end of the PHI list\n  ///\n  void addIncoming(Value *V, BasicBlock *BB) {\n    if (getNumOperands() == ReservedSpace)\n      growOperands();  // Get more space!\n    // Initialize some new operands.\n    setNumHungOffUseOperands(getNumOperands() + 1);\n    setIncomingValue(getNumOperands() - 1, V);\n    setIncomingBlock(getNumOperands() - 1, BB);\n  }\n\n  /// Remove an incoming value.  This is useful if a\n  /// predecessor basic block is deleted.  The value removed is returned.\n  ///\n  /// If the last incoming value for a PHI node is removed (and DeletePHIIfEmpty\n  /// is true), the PHI node is destroyed and any uses of it are replaced with\n  /// dummy values.  The only time there should be zero incoming values to a PHI\n  /// node is when the block is dead, so this strategy is sound.\n  ///\n  Value *removeIncomingValue(unsigned Idx, bool DeletePHIIfEmpty = true);\n\n  Value *removeIncomingValue(const BasicBlock *BB, bool DeletePHIIfEmpty=true) {\n    int Idx = getBasicBlockIndex(BB);\n    assert(Idx >= 0 && \"Invalid basic block argument to remove!\");\n    return removeIncomingValue(Idx, DeletePHIIfEmpty);\n  }\n\n  /// Return the first index of the specified basic\n  /// block in the value list for this PHI.  Returns -1 if no instance.\n  ///\n  int getBasicBlockIndex(const BasicBlock *BB) const {\n    for (unsigned i = 0, e = getNumOperands(); i != e; ++i)\n      if (block_begin()[i] == BB)\n        return i;\n    return -1;\n  }\n\n  Value *getIncomingValueForBlock(const BasicBlock *BB) const {\n    int Idx = getBasicBlockIndex(BB);\n    assert(Idx >= 0 && \"Invalid basic block argument!\");\n    return getIncomingValue(Idx);\n  }\n\n  /// Set every incoming value(s) for block \\p BB to \\p V.\n  void setIncomingValueForBlock(const BasicBlock *BB, Value *V) {\n    assert(BB && \"PHI node got a null basic block!\");\n    bool Found = false;\n    for (unsigned Op = 0, NumOps = getNumOperands(); Op != NumOps; ++Op)\n      if (getIncomingBlock(Op) == BB) {\n        Found = true;\n        setIncomingValue(Op, V);\n      }\n    (void)Found;\n    assert(Found && \"Invalid basic block argument to set!\");\n  }\n\n  /// If the specified PHI node always merges together the\n  /// same value, return the value, otherwise return null.\n  Value *hasConstantValue() const;\n\n  /// Whether the specified PHI node always merges\n  /// together the same value, assuming undefs are equal to a unique\n  /// non-undef value.\n  bool hasConstantOrUndefValue() const;\n\n  /// If the PHI node is complete which means all of its parent's predecessors\n  /// have incoming value in this PHI, return true, otherwise return false.\n  bool isComplete() const {\n    return llvm::all_of(predecessors(getParent()),\n                        [this](const BasicBlock *Pred) {\n                          return getBasicBlockIndex(Pred) >= 0;\n                        });\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::PHI;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  void growOperands();\n};\n\ntemplate <>\nstruct OperandTraits<PHINode> : public HungoffOperandTraits<2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(PHINode, Value)\n\n//===----------------------------------------------------------------------===//\n//                           LandingPadInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// The landingpad instruction holds all of the information\n/// necessary to generate correct exception handling. The landingpad instruction\n/// cannot be moved from the top of a landing pad block, which itself is\n/// accessible only from the 'unwind' edge of an invoke. This uses the\n/// SubclassData field in Value to store whether or not the landingpad is a\n/// cleanup.\n///\nclass LandingPadInst : public Instruction {\n  using CleanupField = BoolBitfieldElementT<0>;\n\n  /// The number of operands actually allocated.  NumOperands is\n  /// the number actually in use.\n  unsigned ReservedSpace;\n\n  LandingPadInst(const LandingPadInst &LP);\n\npublic:\n  enum ClauseType { Catch, Filter };\n\nprivate:\n  explicit LandingPadInst(Type *RetTy, unsigned NumReservedValues,\n                          const Twine &NameStr, Instruction *InsertBefore);\n  explicit LandingPadInst(Type *RetTy, unsigned NumReservedValues,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  // Allocate space for exactly zero operands.\n  void *operator new(size_t s) {\n    return User::operator new(s);\n  }\n\n  void growOperands(unsigned Size);\n  void init(unsigned NumReservedValues, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  LandingPadInst *cloneImpl() const;\n\npublic:\n  /// Constructors - NumReservedClauses is a hint for the number of incoming\n  /// clauses that this landingpad will have (use 0 if you really have no idea).\n  static LandingPadInst *Create(Type *RetTy, unsigned NumReservedClauses,\n                                const Twine &NameStr = \"\",\n                                Instruction *InsertBefore = nullptr);\n  static LandingPadInst *Create(Type *RetTy, unsigned NumReservedClauses,\n                                const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Return 'true' if this landingpad instruction is a\n  /// cleanup. I.e., it should be run when unwinding even if its landing pad\n  /// doesn't catch the exception.\n  bool isCleanup() const { return getSubclassData<CleanupField>(); }\n\n  /// Indicate that this landingpad instruction is a cleanup.\n  void setCleanup(bool V) { setSubclassData<CleanupField>(V); }\n\n  /// Add a catch or filter clause to the landing pad.\n  void addClause(Constant *ClauseVal);\n\n  /// Get the value of the clause at index Idx. Use isCatch/isFilter to\n  /// determine what type of clause this is.\n  Constant *getClause(unsigned Idx) const {\n    return cast<Constant>(getOperandList()[Idx]);\n  }\n\n  /// Return 'true' if the clause and index Idx is a catch clause.\n  bool isCatch(unsigned Idx) const {\n    return !isa<ArrayType>(getOperandList()[Idx]->getType());\n  }\n\n  /// Return 'true' if the clause and index Idx is a filter clause.\n  bool isFilter(unsigned Idx) const {\n    return isa<ArrayType>(getOperandList()[Idx]->getType());\n  }\n\n  /// Get the number of clauses for this landing pad.\n  unsigned getNumClauses() const { return getNumOperands(); }\n\n  /// Grow the size of the operand list to accommodate the new\n  /// number of clauses.\n  void reserveClauses(unsigned Size) { growOperands(Size); }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::LandingPad;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<LandingPadInst> : public HungoffOperandTraits<1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(LandingPadInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               ReturnInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Return a value (possibly void), from a function.  Execution\n/// does not continue in this function any longer.\n///\nclass ReturnInst : public Instruction {\n  ReturnInst(const ReturnInst &RI);\n\nprivate:\n  // ReturnInst constructors:\n  // ReturnInst()                  - 'ret void' instruction\n  // ReturnInst(    null)          - 'ret void' instruction\n  // ReturnInst(Value* X)          - 'ret X'    instruction\n  // ReturnInst(    null, Inst *I) - 'ret void' instruction, insert before I\n  // ReturnInst(Value* X, Inst *I) - 'ret X'    instruction, insert before I\n  // ReturnInst(    null, BB *B)   - 'ret void' instruction, insert @ end of B\n  // ReturnInst(Value* X, BB *B)   - 'ret X'    instruction, insert @ end of B\n  //\n  // NOTE: If the Value* passed is of type void then the constructor behaves as\n  // if it was passed NULL.\n  explicit ReturnInst(LLVMContext &C, Value *retVal = nullptr,\n                      Instruction *InsertBefore = nullptr);\n  ReturnInst(LLVMContext &C, Value *retVal, BasicBlock *InsertAtEnd);\n  explicit ReturnInst(LLVMContext &C, BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ReturnInst *cloneImpl() const;\n\npublic:\n  static ReturnInst* Create(LLVMContext &C, Value *retVal = nullptr,\n                            Instruction *InsertBefore = nullptr) {\n    return new(!!retVal) ReturnInst(C, retVal, InsertBefore);\n  }\n\n  static ReturnInst* Create(LLVMContext &C, Value *retVal,\n                            BasicBlock *InsertAtEnd) {\n    return new(!!retVal) ReturnInst(C, retVal, InsertAtEnd);\n  }\n\n  static ReturnInst* Create(LLVMContext &C, BasicBlock *InsertAtEnd) {\n    return new(0) ReturnInst(C, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Convenience accessor. Returns null if there is no return value.\n  Value *getReturnValue() const {\n    return getNumOperands() != 0 ? getOperand(0) : nullptr;\n  }\n\n  unsigned getNumSuccessors() const { return 0; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Ret);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned idx) const {\n    llvm_unreachable(\"ReturnInst has no successors!\");\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *B) {\n    llvm_unreachable(\"ReturnInst has no successors!\");\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ReturnInst> : public VariadicOperandTraits<ReturnInst> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ReturnInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               BranchInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Conditional or Unconditional Branch instruction.\n///\nclass BranchInst : public Instruction {\n  /// Ops list - Branches are strange.  The operands are ordered:\n  ///  [Cond, FalseDest,] TrueDest.  This makes some accessors faster because\n  /// they don't have to check for cond/uncond branchness. These are mostly\n  /// accessed relative from op_end().\n  BranchInst(const BranchInst &BI);\n  // BranchInst constructors (where {B, T, F} are blocks, and C is a condition):\n  // BranchInst(BB *B)                           - 'br B'\n  // BranchInst(BB* T, BB *F, Value *C)          - 'br C, T, F'\n  // BranchInst(BB* B, Inst *I)                  - 'br B'        insert before I\n  // BranchInst(BB* T, BB *F, Value *C, Inst *I) - 'br C, T, F', insert before I\n  // BranchInst(BB* B, BB *I)                    - 'br B'        insert at end\n  // BranchInst(BB* T, BB *F, Value *C, BB *I)   - 'br C, T, F', insert at end\n  explicit BranchInst(BasicBlock *IfTrue, Instruction *InsertBefore = nullptr);\n  BranchInst(BasicBlock *IfTrue, BasicBlock *IfFalse, Value *Cond,\n             Instruction *InsertBefore = nullptr);\n  BranchInst(BasicBlock *IfTrue, BasicBlock *InsertAtEnd);\n  BranchInst(BasicBlock *IfTrue, BasicBlock *IfFalse, Value *Cond,\n             BasicBlock *InsertAtEnd);\n\n  void AssertOK();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  BranchInst *cloneImpl() const;\n\npublic:\n  /// Iterator type that casts an operand to a basic block.\n  ///\n  /// This only makes sense because the successors are stored as adjacent\n  /// operands for branch instructions.\n  struct succ_op_iterator\n      : iterator_adaptor_base<succ_op_iterator, value_op_iterator,\n                              std::random_access_iterator_tag, BasicBlock *,\n                              ptrdiff_t, BasicBlock *, BasicBlock *> {\n    explicit succ_op_iterator(value_op_iterator I) : iterator_adaptor_base(I) {}\n\n    BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    BasicBlock *operator->() const { return operator*(); }\n  };\n\n  /// The const version of `succ_op_iterator`.\n  struct const_succ_op_iterator\n      : iterator_adaptor_base<const_succ_op_iterator, const_value_op_iterator,\n                              std::random_access_iterator_tag,\n                              const BasicBlock *, ptrdiff_t, const BasicBlock *,\n                              const BasicBlock *> {\n    explicit const_succ_op_iterator(const_value_op_iterator I)\n        : iterator_adaptor_base(I) {}\n\n    const BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    const BasicBlock *operator->() const { return operator*(); }\n  };\n\n  static BranchInst *Create(BasicBlock *IfTrue,\n                            Instruction *InsertBefore = nullptr) {\n    return new(1) BranchInst(IfTrue, InsertBefore);\n  }\n\n  static BranchInst *Create(BasicBlock *IfTrue, BasicBlock *IfFalse,\n                            Value *Cond, Instruction *InsertBefore = nullptr) {\n    return new(3) BranchInst(IfTrue, IfFalse, Cond, InsertBefore);\n  }\n\n  static BranchInst *Create(BasicBlock *IfTrue, BasicBlock *InsertAtEnd) {\n    return new(1) BranchInst(IfTrue, InsertAtEnd);\n  }\n\n  static BranchInst *Create(BasicBlock *IfTrue, BasicBlock *IfFalse,\n                            Value *Cond, BasicBlock *InsertAtEnd) {\n    return new(3) BranchInst(IfTrue, IfFalse, Cond, InsertAtEnd);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  bool isUnconditional() const { return getNumOperands() == 1; }\n  bool isConditional()   const { return getNumOperands() == 3; }\n\n  Value *getCondition() const {\n    assert(isConditional() && \"Cannot get condition of an uncond branch!\");\n    return Op<-3>();\n  }\n\n  void setCondition(Value *V) {\n    assert(isConditional() && \"Cannot set condition of unconditional branch!\");\n    Op<-3>() = V;\n  }\n\n  unsigned getNumSuccessors() const { return 1+isConditional(); }\n\n  BasicBlock *getSuccessor(unsigned i) const {\n    assert(i < getNumSuccessors() && \"Successor # out of range for Branch!\");\n    return cast_or_null<BasicBlock>((&Op<-1>() - i)->get());\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *NewSucc) {\n    assert(idx < getNumSuccessors() && \"Successor # out of range for Branch!\");\n    *(&Op<-1>() - idx) = NewSucc;\n  }\n\n  /// Swap the successors of this branch instruction.\n  ///\n  /// Swaps the successors of the branch instruction. This also swaps any\n  /// branch weight metadata associated with the instruction so that it\n  /// continues to map correctly to each operand.\n  void swapSuccessors();\n\n  iterator_range<succ_op_iterator> successors() {\n    return make_range(\n        succ_op_iterator(std::next(value_op_begin(), isConditional() ? 1 : 0)),\n        succ_op_iterator(value_op_end()));\n  }\n\n  iterator_range<const_succ_op_iterator> successors() const {\n    return make_range(const_succ_op_iterator(\n                          std::next(value_op_begin(), isConditional() ? 1 : 0)),\n                      const_succ_op_iterator(value_op_end()));\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Br);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<BranchInst> : public VariadicOperandTraits<BranchInst, 1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(BranchInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               SwitchInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Multiway switch\n///\nclass SwitchInst : public Instruction {\n  unsigned ReservedSpace;\n\n  // Operand[0]    = Value to switch on\n  // Operand[1]    = Default basic block destination\n  // Operand[2n  ] = Value to match\n  // Operand[2n+1] = BasicBlock to go to on match\n  SwitchInst(const SwitchInst &SI);\n\n  /// Create a new switch instruction, specifying a value to switch on and a\n  /// default destination. The number of additional cases can be specified here\n  /// to make memory allocation more efficient. This constructor can also\n  /// auto-insert before another instruction.\n  SwitchInst(Value *Value, BasicBlock *Default, unsigned NumCases,\n             Instruction *InsertBefore);\n\n  /// Create a new switch instruction, specifying a value to switch on and a\n  /// default destination. The number of additional cases can be specified here\n  /// to make memory allocation more efficient. This constructor also\n  /// auto-inserts at the end of the specified BasicBlock.\n  SwitchInst(Value *Value, BasicBlock *Default, unsigned NumCases,\n             BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s);\n  }\n\n  void init(Value *Value, BasicBlock *Default, unsigned NumReserved);\n  void growOperands();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  SwitchInst *cloneImpl() const;\n\npublic:\n  // -2\n  static const unsigned DefaultPseudoIndex = static_cast<unsigned>(~0L-1);\n\n  template <typename CaseHandleT> class CaseIteratorImpl;\n\n  /// A handle to a particular switch case. It exposes a convenient interface\n  /// to both the case value and the successor block.\n  ///\n  /// We define this as a template and instantiate it to form both a const and\n  /// non-const handle.\n  template <typename SwitchInstT, typename ConstantIntT, typename BasicBlockT>\n  class CaseHandleImpl {\n    // Directly befriend both const and non-const iterators.\n    friend class SwitchInst::CaseIteratorImpl<\n        CaseHandleImpl<SwitchInstT, ConstantIntT, BasicBlockT>>;\n\n  protected:\n    // Expose the switch type we're parameterized with to the iterator.\n    using SwitchInstType = SwitchInstT;\n\n    SwitchInstT *SI;\n    ptrdiff_t Index;\n\n    CaseHandleImpl() = default;\n    CaseHandleImpl(SwitchInstT *SI, ptrdiff_t Index) : SI(SI), Index(Index) {}\n\n  public:\n    /// Resolves case value for current case.\n    ConstantIntT *getCaseValue() const {\n      assert((unsigned)Index < SI->getNumCases() &&\n             \"Index out the number of cases.\");\n      return reinterpret_cast<ConstantIntT *>(SI->getOperand(2 + Index * 2));\n    }\n\n    /// Resolves successor for current case.\n    BasicBlockT *getCaseSuccessor() const {\n      assert(((unsigned)Index < SI->getNumCases() ||\n              (unsigned)Index == DefaultPseudoIndex) &&\n             \"Index out the number of cases.\");\n      return SI->getSuccessor(getSuccessorIndex());\n    }\n\n    /// Returns number of current case.\n    unsigned getCaseIndex() const { return Index; }\n\n    /// Returns successor index for current case successor.\n    unsigned getSuccessorIndex() const {\n      assert(((unsigned)Index == DefaultPseudoIndex ||\n              (unsigned)Index < SI->getNumCases()) &&\n             \"Index out the number of cases.\");\n      return (unsigned)Index != DefaultPseudoIndex ? Index + 1 : 0;\n    }\n\n    bool operator==(const CaseHandleImpl &RHS) const {\n      assert(SI == RHS.SI && \"Incompatible operators.\");\n      return Index == RHS.Index;\n    }\n  };\n\n  using ConstCaseHandle =\n      CaseHandleImpl<const SwitchInst, const ConstantInt, const BasicBlock>;\n\n  class CaseHandle\n      : public CaseHandleImpl<SwitchInst, ConstantInt, BasicBlock> {\n    friend class SwitchInst::CaseIteratorImpl<CaseHandle>;\n\n  public:\n    CaseHandle(SwitchInst *SI, ptrdiff_t Index) : CaseHandleImpl(SI, Index) {}\n\n    /// Sets the new value for current case.\n    void setValue(ConstantInt *V) {\n      assert((unsigned)Index < SI->getNumCases() &&\n             \"Index out the number of cases.\");\n      SI->setOperand(2 + Index*2, reinterpret_cast<Value*>(V));\n    }\n\n    /// Sets the new successor for current case.\n    void setSuccessor(BasicBlock *S) {\n      SI->setSuccessor(getSuccessorIndex(), S);\n    }\n  };\n\n  template <typename CaseHandleT>\n  class CaseIteratorImpl\n      : public iterator_facade_base<CaseIteratorImpl<CaseHandleT>,\n                                    std::random_access_iterator_tag,\n                                    CaseHandleT> {\n    using SwitchInstT = typename CaseHandleT::SwitchInstType;\n\n    CaseHandleT Case;\n\n  public:\n    /// Default constructed iterator is in an invalid state until assigned to\n    /// a case for a particular switch.\n    CaseIteratorImpl() = default;\n\n    /// Initializes case iterator for given SwitchInst and for given\n    /// case number.\n    CaseIteratorImpl(SwitchInstT *SI, unsigned CaseNum) : Case(SI, CaseNum) {}\n\n    /// Initializes case iterator for given SwitchInst and for given\n    /// successor index.\n    static CaseIteratorImpl fromSuccessorIndex(SwitchInstT *SI,\n                                               unsigned SuccessorIndex) {\n      assert(SuccessorIndex < SI->getNumSuccessors() &&\n             \"Successor index # out of range!\");\n      return SuccessorIndex != 0 ? CaseIteratorImpl(SI, SuccessorIndex - 1)\n                                 : CaseIteratorImpl(SI, DefaultPseudoIndex);\n    }\n\n    /// Support converting to the const variant. This will be a no-op for const\n    /// variant.\n    operator CaseIteratorImpl<ConstCaseHandle>() const {\n      return CaseIteratorImpl<ConstCaseHandle>(Case.SI, Case.Index);\n    }\n\n    CaseIteratorImpl &operator+=(ptrdiff_t N) {\n      // Check index correctness after addition.\n      // Note: Index == getNumCases() means end().\n      assert(Case.Index + N >= 0 &&\n             (unsigned)(Case.Index + N) <= Case.SI->getNumCases() &&\n             \"Case.Index out the number of cases.\");\n      Case.Index += N;\n      return *this;\n    }\n    CaseIteratorImpl &operator-=(ptrdiff_t N) {\n      // Check index correctness after subtraction.\n      // Note: Case.Index == getNumCases() means end().\n      assert(Case.Index - N >= 0 &&\n             (unsigned)(Case.Index - N) <= Case.SI->getNumCases() &&\n             \"Case.Index out the number of cases.\");\n      Case.Index -= N;\n      return *this;\n    }\n    ptrdiff_t operator-(const CaseIteratorImpl &RHS) const {\n      assert(Case.SI == RHS.Case.SI && \"Incompatible operators.\");\n      return Case.Index - RHS.Case.Index;\n    }\n    bool operator==(const CaseIteratorImpl &RHS) const {\n      return Case == RHS.Case;\n    }\n    bool operator<(const CaseIteratorImpl &RHS) const {\n      assert(Case.SI == RHS.Case.SI && \"Incompatible operators.\");\n      return Case.Index < RHS.Case.Index;\n    }\n    CaseHandleT &operator*() { return Case; }\n    const CaseHandleT &operator*() const { return Case; }\n  };\n\n  using CaseIt = CaseIteratorImpl<CaseHandle>;\n  using ConstCaseIt = CaseIteratorImpl<ConstCaseHandle>;\n\n  static SwitchInst *Create(Value *Value, BasicBlock *Default,\n                            unsigned NumCases,\n                            Instruction *InsertBefore = nullptr) {\n    return new SwitchInst(Value, Default, NumCases, InsertBefore);\n  }\n\n  static SwitchInst *Create(Value *Value, BasicBlock *Default,\n                            unsigned NumCases, BasicBlock *InsertAtEnd) {\n    return new SwitchInst(Value, Default, NumCases, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Accessor Methods for Switch stmt\n  Value *getCondition() const { return getOperand(0); }\n  void setCondition(Value *V) { setOperand(0, V); }\n\n  BasicBlock *getDefaultDest() const {\n    return cast<BasicBlock>(getOperand(1));\n  }\n\n  void setDefaultDest(BasicBlock *DefaultCase) {\n    setOperand(1, reinterpret_cast<Value*>(DefaultCase));\n  }\n\n  /// Return the number of 'cases' in this switch instruction, excluding the\n  /// default case.\n  unsigned getNumCases() const {\n    return getNumOperands()/2 - 1;\n  }\n\n  /// Returns a read/write iterator that points to the first case in the\n  /// SwitchInst.\n  CaseIt case_begin() {\n    return CaseIt(this, 0);\n  }\n\n  /// Returns a read-only iterator that points to the first case in the\n  /// SwitchInst.\n  ConstCaseIt case_begin() const {\n    return ConstCaseIt(this, 0);\n  }\n\n  /// Returns a read/write iterator that points one past the last in the\n  /// SwitchInst.\n  CaseIt case_end() {\n    return CaseIt(this, getNumCases());\n  }\n\n  /// Returns a read-only iterator that points one past the last in the\n  /// SwitchInst.\n  ConstCaseIt case_end() const {\n    return ConstCaseIt(this, getNumCases());\n  }\n\n  /// Iteration adapter for range-for loops.\n  iterator_range<CaseIt> cases() {\n    return make_range(case_begin(), case_end());\n  }\n\n  /// Constant iteration adapter for range-for loops.\n  iterator_range<ConstCaseIt> cases() const {\n    return make_range(case_begin(), case_end());\n  }\n\n  /// Returns an iterator that points to the default case.\n  /// Note: this iterator allows to resolve successor only. Attempt\n  /// to resolve case value causes an assertion.\n  /// Also note, that increment and decrement also causes an assertion and\n  /// makes iterator invalid.\n  CaseIt case_default() {\n    return CaseIt(this, DefaultPseudoIndex);\n  }\n  ConstCaseIt case_default() const {\n    return ConstCaseIt(this, DefaultPseudoIndex);\n  }\n\n  /// Search all of the case values for the specified constant. If it is\n  /// explicitly handled, return the case iterator of it, otherwise return\n  /// default case iterator to indicate that it is handled by the default\n  /// handler.\n  CaseIt findCaseValue(const ConstantInt *C) {\n    CaseIt I = llvm::find_if(\n        cases(), [C](CaseHandle &Case) { return Case.getCaseValue() == C; });\n    if (I != case_end())\n      return I;\n\n    return case_default();\n  }\n  ConstCaseIt findCaseValue(const ConstantInt *C) const {\n    ConstCaseIt I = llvm::find_if(cases(), [C](ConstCaseHandle &Case) {\n      return Case.getCaseValue() == C;\n    });\n    if (I != case_end())\n      return I;\n\n    return case_default();\n  }\n\n  /// Finds the unique case value for a given successor. Returns null if the\n  /// successor is not found, not unique, or is the default case.\n  ConstantInt *findCaseDest(BasicBlock *BB) {\n    if (BB == getDefaultDest())\n      return nullptr;\n\n    ConstantInt *CI = nullptr;\n    for (auto Case : cases()) {\n      if (Case.getCaseSuccessor() != BB)\n        continue;\n\n      if (CI)\n        return nullptr; // Multiple cases lead to BB.\n\n      CI = Case.getCaseValue();\n    }\n\n    return CI;\n  }\n\n  /// Add an entry to the switch instruction.\n  /// Note:\n  /// This action invalidates case_end(). Old case_end() iterator will\n  /// point to the added case.\n  void addCase(ConstantInt *OnVal, BasicBlock *Dest);\n\n  /// This method removes the specified case and its successor from the switch\n  /// instruction. Note that this operation may reorder the remaining cases at\n  /// index idx and above.\n  /// Note:\n  /// This action invalidates iterators for all cases following the one removed,\n  /// including the case_end() iterator. It returns an iterator for the next\n  /// case.\n  CaseIt removeCase(CaseIt I);\n\n  unsigned getNumSuccessors() const { return getNumOperands()/2; }\n  BasicBlock *getSuccessor(unsigned idx) const {\n    assert(idx < getNumSuccessors() &&\"Successor idx out of range for switch!\");\n    return cast<BasicBlock>(getOperand(idx*2+1));\n  }\n  void setSuccessor(unsigned idx, BasicBlock *NewSucc) {\n    assert(idx < getNumSuccessors() && \"Successor # out of range for switch!\");\n    setOperand(idx * 2 + 1, NewSucc);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Switch;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n/// A wrapper class to simplify modification of SwitchInst cases along with\n/// their prof branch_weights metadata.\nclass SwitchInstProfUpdateWrapper {\n  SwitchInst &SI;\n  Optional<SmallVector<uint32_t, 8> > Weights = None;\n  bool Changed = false;\n\nprotected:\n  static MDNode *getProfBranchWeightsMD(const SwitchInst &SI);\n\n  MDNode *buildProfBranchWeightsMD();\n\n  void init();\n\npublic:\n  using CaseWeightOpt = Optional<uint32_t>;\n  SwitchInst *operator->() { return &SI; }\n  SwitchInst &operator*() { return SI; }\n  operator SwitchInst *() { return &SI; }\n\n  SwitchInstProfUpdateWrapper(SwitchInst &SI) : SI(SI) { init(); }\n\n  ~SwitchInstProfUpdateWrapper() {\n    if (Changed)\n      SI.setMetadata(LLVMContext::MD_prof, buildProfBranchWeightsMD());\n  }\n\n  /// Delegate the call to the underlying SwitchInst::removeCase() and remove\n  /// correspondent branch weight.\n  SwitchInst::CaseIt removeCase(SwitchInst::CaseIt I);\n\n  /// Delegate the call to the underlying SwitchInst::addCase() and set the\n  /// specified branch weight for the added case.\n  void addCase(ConstantInt *OnVal, BasicBlock *Dest, CaseWeightOpt W);\n\n  /// Delegate the call to the underlying SwitchInst::eraseFromParent() and mark\n  /// this object to not touch the underlying SwitchInst in destructor.\n  SymbolTableList<Instruction>::iterator eraseFromParent();\n\n  void setSuccessorWeight(unsigned idx, CaseWeightOpt W);\n  CaseWeightOpt getSuccessorWeight(unsigned idx);\n\n  static CaseWeightOpt getSuccessorWeight(const SwitchInst &SI, unsigned idx);\n};\n\ntemplate <>\nstruct OperandTraits<SwitchInst> : public HungoffOperandTraits<2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(SwitchInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                             IndirectBrInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Indirect Branch Instruction.\n///\nclass IndirectBrInst : public Instruction {\n  unsigned ReservedSpace;\n\n  // Operand[0]   = Address to jump to\n  // Operand[n+1] = n-th destination\n  IndirectBrInst(const IndirectBrInst &IBI);\n\n  /// Create a new indirectbr instruction, specifying an\n  /// Address to jump to.  The number of expected destinations can be specified\n  /// here to make memory allocation more efficient.  This constructor can also\n  /// autoinsert before another instruction.\n  IndirectBrInst(Value *Address, unsigned NumDests, Instruction *InsertBefore);\n\n  /// Create a new indirectbr instruction, specifying an\n  /// Address to jump to.  The number of expected destinations can be specified\n  /// here to make memory allocation more efficient.  This constructor also\n  /// autoinserts at the end of the specified BasicBlock.\n  IndirectBrInst(Value *Address, unsigned NumDests, BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s);\n  }\n\n  void init(Value *Address, unsigned NumDests);\n  void growOperands();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  IndirectBrInst *cloneImpl() const;\n\npublic:\n  /// Iterator type that casts an operand to a basic block.\n  ///\n  /// This only makes sense because the successors are stored as adjacent\n  /// operands for indirectbr instructions.\n  struct succ_op_iterator\n      : iterator_adaptor_base<succ_op_iterator, value_op_iterator,\n                              std::random_access_iterator_tag, BasicBlock *,\n                              ptrdiff_t, BasicBlock *, BasicBlock *> {\n    explicit succ_op_iterator(value_op_iterator I) : iterator_adaptor_base(I) {}\n\n    BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    BasicBlock *operator->() const { return operator*(); }\n  };\n\n  /// The const version of `succ_op_iterator`.\n  struct const_succ_op_iterator\n      : iterator_adaptor_base<const_succ_op_iterator, const_value_op_iterator,\n                              std::random_access_iterator_tag,\n                              const BasicBlock *, ptrdiff_t, const BasicBlock *,\n                              const BasicBlock *> {\n    explicit const_succ_op_iterator(const_value_op_iterator I)\n        : iterator_adaptor_base(I) {}\n\n    const BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    const BasicBlock *operator->() const { return operator*(); }\n  };\n\n  static IndirectBrInst *Create(Value *Address, unsigned NumDests,\n                                Instruction *InsertBefore = nullptr) {\n    return new IndirectBrInst(Address, NumDests, InsertBefore);\n  }\n\n  static IndirectBrInst *Create(Value *Address, unsigned NumDests,\n                                BasicBlock *InsertAtEnd) {\n    return new IndirectBrInst(Address, NumDests, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Accessor Methods for IndirectBrInst instruction.\n  Value *getAddress() { return getOperand(0); }\n  const Value *getAddress() const { return getOperand(0); }\n  void setAddress(Value *V) { setOperand(0, V); }\n\n  /// return the number of possible destinations in this\n  /// indirectbr instruction.\n  unsigned getNumDestinations() const { return getNumOperands()-1; }\n\n  /// Return the specified destination.\n  BasicBlock *getDestination(unsigned i) { return getSuccessor(i); }\n  const BasicBlock *getDestination(unsigned i) const { return getSuccessor(i); }\n\n  /// Add a destination.\n  ///\n  void addDestination(BasicBlock *Dest);\n\n  /// This method removes the specified successor from the\n  /// indirectbr instruction.\n  void removeDestination(unsigned i);\n\n  unsigned getNumSuccessors() const { return getNumOperands()-1; }\n  BasicBlock *getSuccessor(unsigned i) const {\n    return cast<BasicBlock>(getOperand(i+1));\n  }\n  void setSuccessor(unsigned i, BasicBlock *NewSucc) {\n    setOperand(i + 1, NewSucc);\n  }\n\n  iterator_range<succ_op_iterator> successors() {\n    return make_range(succ_op_iterator(std::next(value_op_begin())),\n                      succ_op_iterator(value_op_end()));\n  }\n\n  iterator_range<const_succ_op_iterator> successors() const {\n    return make_range(const_succ_op_iterator(std::next(value_op_begin())),\n                      const_succ_op_iterator(value_op_end()));\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::IndirectBr;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<IndirectBrInst> : public HungoffOperandTraits<1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(IndirectBrInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               InvokeInst Class\n//===----------------------------------------------------------------------===//\n\n/// Invoke instruction.  The SubclassData field is used to hold the\n/// calling convention of the call.\n///\nclass InvokeInst : public CallBase {\n  /// The number of operands for this call beyond the called function,\n  /// arguments, and operand bundles.\n  static constexpr int NumExtraOperands = 2;\n\n  /// The index from the end of the operand array to the normal destination.\n  static constexpr int NormalDestOpEndIdx = -3;\n\n  /// The index from the end of the operand array to the unwind destination.\n  static constexpr int UnwindDestOpEndIdx = -2;\n\n  InvokeInst(const InvokeInst &BI);\n\n  /// Construct an InvokeInst given a range of arguments.\n  ///\n  /// Construct an InvokeInst from a range of arguments\n  inline InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                    BasicBlock *IfException, ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, Instruction *InsertBefore);\n\n  inline InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                    BasicBlock *IfException, ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n            BasicBlock *IfException, ArrayRef<Value *> Args,\n            ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr);\n\n  /// Compute the number of operands to allocate.\n  static int ComputeNumOperands(int NumArgs, int NumBundleInputs = 0) {\n    // We need one operand for the called function, plus our extra operands and\n    // the input operand counts provided.\n    return 1 + NumExtraOperands + NumArgs + NumBundleInputs;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  InvokeInst *cloneImpl() const;\n\npublic:\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands = ComputeNumOperands(Args.size());\n    return new (NumOperands)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, None, NumOperands,\n                   NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, Bundles, NumOperands,\n                   NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    int NumOperands = ComputeNumOperands(Args.size());\n    return new (NumOperands)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, None, NumOperands,\n                   NameStr, InsertAtEnd);\n  }\n\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, Bundles, NumOperands,\n                   NameStr, InsertAtEnd);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, None, NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, Bundles, NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, NameStr, InsertAtEnd);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, Bundles, NameStr, InsertAtEnd);\n  }\n\n  /// Create a clone of \\p II with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned invoke instruction is identical to \\p II in every way except\n  /// that the operand bundles for the new instruction are set to the operand\n  /// bundles in \\p Bundles.\n  static InvokeInst *Create(InvokeInst *II, ArrayRef<OperandBundleDef> Bundles,\n                            Instruction *InsertPt = nullptr);\n\n  // get*Dest - Return the destination basic blocks...\n  BasicBlock *getNormalDest() const {\n    return cast<BasicBlock>(Op<NormalDestOpEndIdx>());\n  }\n  BasicBlock *getUnwindDest() const {\n    return cast<BasicBlock>(Op<UnwindDestOpEndIdx>());\n  }\n  void setNormalDest(BasicBlock *B) {\n    Op<NormalDestOpEndIdx>() = reinterpret_cast<Value *>(B);\n  }\n  void setUnwindDest(BasicBlock *B) {\n    Op<UnwindDestOpEndIdx>() = reinterpret_cast<Value *>(B);\n  }\n\n  /// Get the landingpad instruction from the landing pad\n  /// block (the unwind destination).\n  LandingPadInst *getLandingPadInst() const;\n\n  BasicBlock *getSuccessor(unsigned i) const {\n    assert(i < 2 && \"Successor # out of range for invoke!\");\n    return i == 0 ? getNormalDest() : getUnwindDest();\n  }\n\n  void setSuccessor(unsigned i, BasicBlock *NewSucc) {\n    assert(i < 2 && \"Successor # out of range for invoke!\");\n    if (i == 0)\n      setNormalDest(NewSucc);\n    else\n      setUnwindDest(NewSucc);\n  }\n\n  unsigned getNumSuccessors() const { return 2; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Invoke);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\nInvokeInst::InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                       BasicBlock *IfException, ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, Instruction *InsertBefore)\n    : CallBase(Ty->getReturnType(), Instruction::Invoke,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertBefore) {\n  init(Ty, Func, IfNormal, IfException, Args, Bundles, NameStr);\n}\n\nInvokeInst::InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                       BasicBlock *IfException, ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, BasicBlock *InsertAtEnd)\n    : CallBase(Ty->getReturnType(), Instruction::Invoke,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertAtEnd) {\n  init(Ty, Func, IfNormal, IfException, Args, Bundles, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                              CallBrInst Class\n//===----------------------------------------------------------------------===//\n\n/// CallBr instruction, tracking function calls that may not return control but\n/// instead transfer it to a third location. The SubclassData field is used to\n/// hold the calling convention of the call.\n///\nclass CallBrInst : public CallBase {\n\n  unsigned NumIndirectDests;\n\n  CallBrInst(const CallBrInst &BI);\n\n  /// Construct a CallBrInst given a range of arguments.\n  ///\n  /// Construct a CallBrInst from a range of arguments\n  inline CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                    ArrayRef<BasicBlock *> IndirectDests,\n                    ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, Instruction *InsertBefore);\n\n  inline CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                    ArrayRef<BasicBlock *> IndirectDests,\n                    ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(FunctionType *FTy, Value *Func, BasicBlock *DefaultDest,\n            ArrayRef<BasicBlock *> IndirectDests, ArrayRef<Value *> Args,\n            ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr);\n\n  /// Should the Indirect Destinations change, scan + update the Arg list.\n  void updateArgBlockAddresses(unsigned i, BasicBlock *B);\n\n  /// Compute the number of operands to allocate.\n  static int ComputeNumOperands(int NumArgs, int NumIndirectDests,\n                                int NumBundleInputs = 0) {\n    // We need one operand for the called function, plus our extra operands and\n    // the input operand counts provided.\n    return 2 + NumIndirectDests + NumArgs + NumBundleInputs;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CallBrInst *cloneImpl() const;\n\npublic:\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size());\n    return new (NumOperands)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, None,\n                   NumOperands, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size(),\n                                         CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, Bundles,\n                   NumOperands, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            BasicBlock *InsertAtEnd) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size());\n    return new (NumOperands)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, None,\n                   NumOperands, NameStr, InsertAtEnd);\n  }\n\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size(),\n                                         CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, Bundles,\n                   NumOperands, NameStr, InsertAtEnd);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func, BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func, BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, Bundles, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func, BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, NameStr, InsertAtEnd);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, Bundles, NameStr, InsertAtEnd);\n  }\n\n  /// Create a clone of \\p CBI with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned callbr instruction is identical to \\p CBI in every way\n  /// except that the operand bundles for the new instruction are set to the\n  /// operand bundles in \\p Bundles.\n  static CallBrInst *Create(CallBrInst *CBI,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            Instruction *InsertPt = nullptr);\n\n  /// Return the number of callbr indirect dest labels.\n  ///\n  unsigned getNumIndirectDests() const { return NumIndirectDests; }\n\n  /// getIndirectDestLabel - Return the i-th indirect dest label.\n  ///\n  Value *getIndirectDestLabel(unsigned i) const {\n    assert(i < getNumIndirectDests() && \"Out of bounds!\");\n    return getOperand(i + getNumArgOperands() + getNumTotalBundleOperands() +\n                      1);\n  }\n\n  Value *getIndirectDestLabelUse(unsigned i) const {\n    assert(i < getNumIndirectDests() && \"Out of bounds!\");\n    return getOperandUse(i + getNumArgOperands() + getNumTotalBundleOperands() +\n                         1);\n  }\n\n  // Return the destination basic blocks...\n  BasicBlock *getDefaultDest() const {\n    return cast<BasicBlock>(*(&Op<-1>() - getNumIndirectDests() - 1));\n  }\n  BasicBlock *getIndirectDest(unsigned i) const {\n    return cast_or_null<BasicBlock>(*(&Op<-1>() - getNumIndirectDests() + i));\n  }\n  SmallVector<BasicBlock *, 16> getIndirectDests() const {\n    SmallVector<BasicBlock *, 16> IndirectDests;\n    for (unsigned i = 0, e = getNumIndirectDests(); i < e; ++i)\n      IndirectDests.push_back(getIndirectDest(i));\n    return IndirectDests;\n  }\n  void setDefaultDest(BasicBlock *B) {\n    *(&Op<-1>() - getNumIndirectDests() - 1) = reinterpret_cast<Value *>(B);\n  }\n  void setIndirectDest(unsigned i, BasicBlock *B) {\n    updateArgBlockAddresses(i, B);\n    *(&Op<-1>() - getNumIndirectDests() + i) = reinterpret_cast<Value *>(B);\n  }\n\n  BasicBlock *getSuccessor(unsigned i) const {\n    assert(i < getNumSuccessors() + 1 &&\n           \"Successor # out of range for callbr!\");\n    return i == 0 ? getDefaultDest() : getIndirectDest(i - 1);\n  }\n\n  void setSuccessor(unsigned i, BasicBlock *NewSucc) {\n    assert(i < getNumIndirectDests() + 1 &&\n           \"Successor # out of range for callbr!\");\n    return i == 0 ? setDefaultDest(NewSucc) : setIndirectDest(i - 1, NewSucc);\n  }\n\n  unsigned getNumSuccessors() const { return getNumIndirectDests() + 1; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::CallBr);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\nCallBrInst::CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                       ArrayRef<BasicBlock *> IndirectDests,\n                       ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, Instruction *InsertBefore)\n    : CallBase(Ty->getReturnType(), Instruction::CallBr,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertBefore) {\n  init(Ty, Func, DefaultDest, IndirectDests, Args, Bundles, NameStr);\n}\n\nCallBrInst::CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                       ArrayRef<BasicBlock *> IndirectDests,\n                       ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, BasicBlock *InsertAtEnd)\n    : CallBase(Ty->getReturnType(), Instruction::CallBr,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertAtEnd) {\n  init(Ty, Func, DefaultDest, IndirectDests, Args, Bundles, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                              ResumeInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Resume the propagation of an exception.\n///\nclass ResumeInst : public Instruction {\n  ResumeInst(const ResumeInst &RI);\n\n  explicit ResumeInst(Value *Exn, Instruction *InsertBefore=nullptr);\n  ResumeInst(Value *Exn, BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ResumeInst *cloneImpl() const;\n\npublic:\n  static ResumeInst *Create(Value *Exn, Instruction *InsertBefore = nullptr) {\n    return new(1) ResumeInst(Exn, InsertBefore);\n  }\n\n  static ResumeInst *Create(Value *Exn, BasicBlock *InsertAtEnd) {\n    return new(1) ResumeInst(Exn, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Convenience accessor.\n  Value *getValue() const { return Op<0>(); }\n\n  unsigned getNumSuccessors() const { return 0; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Resume;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned idx) const {\n    llvm_unreachable(\"ResumeInst has no successors!\");\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *NewSucc) {\n    llvm_unreachable(\"ResumeInst has no successors!\");\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ResumeInst> :\n    public FixedNumOperandTraits<ResumeInst, 1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ResumeInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                         CatchSwitchInst Class\n//===----------------------------------------------------------------------===//\nclass CatchSwitchInst : public Instruction {\n  using UnwindDestField = BoolBitfieldElementT<0>;\n\n  /// The number of operands actually allocated.  NumOperands is\n  /// the number actually in use.\n  unsigned ReservedSpace;\n\n  // Operand[0] = Outer scope\n  // Operand[1] = Unwind block destination\n  // Operand[n] = BasicBlock to go to on match\n  CatchSwitchInst(const CatchSwitchInst &CSI);\n\n  /// Create a new switch instruction, specifying a\n  /// default destination.  The number of additional handlers can be specified\n  /// here to make memory allocation more efficient.\n  /// This constructor can also autoinsert before another instruction.\n  CatchSwitchInst(Value *ParentPad, BasicBlock *UnwindDest,\n                  unsigned NumHandlers, const Twine &NameStr,\n                  Instruction *InsertBefore);\n\n  /// Create a new switch instruction, specifying a\n  /// default destination.  The number of additional handlers can be specified\n  /// here to make memory allocation more efficient.\n  /// This constructor also autoinserts at the end of the specified BasicBlock.\n  CatchSwitchInst(Value *ParentPad, BasicBlock *UnwindDest,\n                  unsigned NumHandlers, const Twine &NameStr,\n                  BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) { return User::operator new(s); }\n\n  void init(Value *ParentPad, BasicBlock *UnwindDest, unsigned NumReserved);\n  void growOperands(unsigned Size);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CatchSwitchInst *cloneImpl() const;\n\npublic:\n  static CatchSwitchInst *Create(Value *ParentPad, BasicBlock *UnwindDest,\n                                 unsigned NumHandlers,\n                                 const Twine &NameStr = \"\",\n                                 Instruction *InsertBefore = nullptr) {\n    return new CatchSwitchInst(ParentPad, UnwindDest, NumHandlers, NameStr,\n                               InsertBefore);\n  }\n\n  static CatchSwitchInst *Create(Value *ParentPad, BasicBlock *UnwindDest,\n                                 unsigned NumHandlers, const Twine &NameStr,\n                                 BasicBlock *InsertAtEnd) {\n    return new CatchSwitchInst(ParentPad, UnwindDest, NumHandlers, NameStr,\n                               InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Accessor Methods for CatchSwitch stmt\n  Value *getParentPad() const { return getOperand(0); }\n  void setParentPad(Value *ParentPad) { setOperand(0, ParentPad); }\n\n  // Accessor Methods for CatchSwitch stmt\n  bool hasUnwindDest() const { return getSubclassData<UnwindDestField>(); }\n  bool unwindsToCaller() const { return !hasUnwindDest(); }\n  BasicBlock *getUnwindDest() const {\n    if (hasUnwindDest())\n      return cast<BasicBlock>(getOperand(1));\n    return nullptr;\n  }\n  void setUnwindDest(BasicBlock *UnwindDest) {\n    assert(UnwindDest);\n    assert(hasUnwindDest());\n    setOperand(1, UnwindDest);\n  }\n\n  /// return the number of 'handlers' in this catchswitch\n  /// instruction, except the default handler\n  unsigned getNumHandlers() const {\n    if (hasUnwindDest())\n      return getNumOperands() - 2;\n    return getNumOperands() - 1;\n  }\n\nprivate:\n  static BasicBlock *handler_helper(Value *V) { return cast<BasicBlock>(V); }\n  static const BasicBlock *handler_helper(const Value *V) {\n    return cast<BasicBlock>(V);\n  }\n\npublic:\n  using DerefFnTy = BasicBlock *(*)(Value *);\n  using handler_iterator = mapped_iterator<op_iterator, DerefFnTy>;\n  using handler_range = iterator_range<handler_iterator>;\n  using ConstDerefFnTy = const BasicBlock *(*)(const Value *);\n  using const_handler_iterator =\n      mapped_iterator<const_op_iterator, ConstDerefFnTy>;\n  using const_handler_range = iterator_range<const_handler_iterator>;\n\n  /// Returns an iterator that points to the first handler in CatchSwitchInst.\n  handler_iterator handler_begin() {\n    op_iterator It = op_begin() + 1;\n    if (hasUnwindDest())\n      ++It;\n    return handler_iterator(It, DerefFnTy(handler_helper));\n  }\n\n  /// Returns an iterator that points to the first handler in the\n  /// CatchSwitchInst.\n  const_handler_iterator handler_begin() const {\n    const_op_iterator It = op_begin() + 1;\n    if (hasUnwindDest())\n      ++It;\n    return const_handler_iterator(It, ConstDerefFnTy(handler_helper));\n  }\n\n  /// Returns a read-only iterator that points one past the last\n  /// handler in the CatchSwitchInst.\n  handler_iterator handler_end() {\n    return handler_iterator(op_end(), DerefFnTy(handler_helper));\n  }\n\n  /// Returns an iterator that points one past the last handler in the\n  /// CatchSwitchInst.\n  const_handler_iterator handler_end() const {\n    return const_handler_iterator(op_end(), ConstDerefFnTy(handler_helper));\n  }\n\n  /// iteration adapter for range-for loops.\n  handler_range handlers() {\n    return make_range(handler_begin(), handler_end());\n  }\n\n  /// iteration adapter for range-for loops.\n  const_handler_range handlers() const {\n    return make_range(handler_begin(), handler_end());\n  }\n\n  /// Add an entry to the switch instruction...\n  /// Note:\n  /// This action invalidates handler_end(). Old handler_end() iterator will\n  /// point to the added handler.\n  void addHandler(BasicBlock *Dest);\n\n  void removeHandler(handler_iterator HI);\n\n  unsigned getNumSuccessors() const { return getNumOperands() - 1; }\n  BasicBlock *getSuccessor(unsigned Idx) const {\n    assert(Idx < getNumSuccessors() &&\n           \"Successor # out of range for catchswitch!\");\n    return cast<BasicBlock>(getOperand(Idx + 1));\n  }\n  void setSuccessor(unsigned Idx, BasicBlock *NewSucc) {\n    assert(Idx < getNumSuccessors() &&\n           \"Successor # out of range for catchswitch!\");\n    setOperand(Idx + 1, NewSucc);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::CatchSwitch;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CatchSwitchInst> : public HungoffOperandTraits<2> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CatchSwitchInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               CleanupPadInst Class\n//===----------------------------------------------------------------------===//\nclass CleanupPadInst : public FuncletPadInst {\nprivate:\n  explicit CleanupPadInst(Value *ParentPad, ArrayRef<Value *> Args,\n                          unsigned Values, const Twine &NameStr,\n                          Instruction *InsertBefore)\n      : FuncletPadInst(Instruction::CleanupPad, ParentPad, Args, Values,\n                       NameStr, InsertBefore) {}\n  explicit CleanupPadInst(Value *ParentPad, ArrayRef<Value *> Args,\n                          unsigned Values, const Twine &NameStr,\n                          BasicBlock *InsertAtEnd)\n      : FuncletPadInst(Instruction::CleanupPad, ParentPad, Args, Values,\n                       NameStr, InsertAtEnd) {}\n\npublic:\n  static CleanupPadInst *Create(Value *ParentPad, ArrayRef<Value *> Args = None,\n                                const Twine &NameStr = \"\",\n                                Instruction *InsertBefore = nullptr) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CleanupPadInst(ParentPad, Args, Values, NameStr, InsertBefore);\n  }\n\n  static CleanupPadInst *Create(Value *ParentPad, ArrayRef<Value *> Args,\n                                const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CleanupPadInst(ParentPad, Args, Values, NameStr, InsertAtEnd);\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::CleanupPad;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               CatchPadInst Class\n//===----------------------------------------------------------------------===//\nclass CatchPadInst : public FuncletPadInst {\nprivate:\n  explicit CatchPadInst(Value *CatchSwitch, ArrayRef<Value *> Args,\n                        unsigned Values, const Twine &NameStr,\n                        Instruction *InsertBefore)\n      : FuncletPadInst(Instruction::CatchPad, CatchSwitch, Args, Values,\n                       NameStr, InsertBefore) {}\n  explicit CatchPadInst(Value *CatchSwitch, ArrayRef<Value *> Args,\n                        unsigned Values, const Twine &NameStr,\n                        BasicBlock *InsertAtEnd)\n      : FuncletPadInst(Instruction::CatchPad, CatchSwitch, Args, Values,\n                       NameStr, InsertAtEnd) {}\n\npublic:\n  static CatchPadInst *Create(Value *CatchSwitch, ArrayRef<Value *> Args,\n                              const Twine &NameStr = \"\",\n                              Instruction *InsertBefore = nullptr) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CatchPadInst(CatchSwitch, Args, Values, NameStr, InsertBefore);\n  }\n\n  static CatchPadInst *Create(Value *CatchSwitch, ArrayRef<Value *> Args,\n                              const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CatchPadInst(CatchSwitch, Args, Values, NameStr, InsertAtEnd);\n  }\n\n  /// Convenience accessors\n  CatchSwitchInst *getCatchSwitch() const {\n    return cast<CatchSwitchInst>(Op<-1>());\n  }\n  void setCatchSwitch(Value *CatchSwitch) {\n    assert(CatchSwitch);\n    Op<-1>() = CatchSwitch;\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::CatchPad;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               CatchReturnInst Class\n//===----------------------------------------------------------------------===//\n\nclass CatchReturnInst : public Instruction {\n  CatchReturnInst(const CatchReturnInst &RI);\n  CatchReturnInst(Value *CatchPad, BasicBlock *BB, Instruction *InsertBefore);\n  CatchReturnInst(Value *CatchPad, BasicBlock *BB, BasicBlock *InsertAtEnd);\n\n  void init(Value *CatchPad, BasicBlock *BB);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CatchReturnInst *cloneImpl() const;\n\npublic:\n  static CatchReturnInst *Create(Value *CatchPad, BasicBlock *BB,\n                                 Instruction *InsertBefore = nullptr) {\n    assert(CatchPad);\n    assert(BB);\n    return new (2) CatchReturnInst(CatchPad, BB, InsertBefore);\n  }\n\n  static CatchReturnInst *Create(Value *CatchPad, BasicBlock *BB,\n                                 BasicBlock *InsertAtEnd) {\n    assert(CatchPad);\n    assert(BB);\n    return new (2) CatchReturnInst(CatchPad, BB, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Convenience accessors.\n  CatchPadInst *getCatchPad() const { return cast<CatchPadInst>(Op<0>()); }\n  void setCatchPad(CatchPadInst *CatchPad) {\n    assert(CatchPad);\n    Op<0>() = CatchPad;\n  }\n\n  BasicBlock *getSuccessor() const { return cast<BasicBlock>(Op<1>()); }\n  void setSuccessor(BasicBlock *NewSucc) {\n    assert(NewSucc);\n    Op<1>() = NewSucc;\n  }\n  unsigned getNumSuccessors() const { return 1; }\n\n  /// Get the parentPad of this catchret's catchpad's catchswitch.\n  /// The successor block is implicitly a member of this funclet.\n  Value *getCatchSwitchParentPad() const {\n    return getCatchPad()->getCatchSwitch()->getParentPad();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::CatchRet);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned Idx) const {\n    assert(Idx < getNumSuccessors() && \"Successor # out of range for catchret!\");\n    return getSuccessor();\n  }\n\n  void setSuccessor(unsigned Idx, BasicBlock *B) {\n    assert(Idx < getNumSuccessors() && \"Successor # out of range for catchret!\");\n    setSuccessor(B);\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CatchReturnInst>\n    : public FixedNumOperandTraits<CatchReturnInst, 2> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CatchReturnInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               CleanupReturnInst Class\n//===----------------------------------------------------------------------===//\n\nclass CleanupReturnInst : public Instruction {\n  using UnwindDestField = BoolBitfieldElementT<0>;\n\nprivate:\n  CleanupReturnInst(const CleanupReturnInst &RI);\n  CleanupReturnInst(Value *CleanupPad, BasicBlock *UnwindBB, unsigned Values,\n                    Instruction *InsertBefore = nullptr);\n  CleanupReturnInst(Value *CleanupPad, BasicBlock *UnwindBB, unsigned Values,\n                    BasicBlock *InsertAtEnd);\n\n  void init(Value *CleanupPad, BasicBlock *UnwindBB);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CleanupReturnInst *cloneImpl() const;\n\npublic:\n  static CleanupReturnInst *Create(Value *CleanupPad,\n                                   BasicBlock *UnwindBB = nullptr,\n                                   Instruction *InsertBefore = nullptr) {\n    assert(CleanupPad);\n    unsigned Values = 1;\n    if (UnwindBB)\n      ++Values;\n    return new (Values)\n        CleanupReturnInst(CleanupPad, UnwindBB, Values, InsertBefore);\n  }\n\n  static CleanupReturnInst *Create(Value *CleanupPad, BasicBlock *UnwindBB,\n                                   BasicBlock *InsertAtEnd) {\n    assert(CleanupPad);\n    unsigned Values = 1;\n    if (UnwindBB)\n      ++Values;\n    return new (Values)\n        CleanupReturnInst(CleanupPad, UnwindBB, Values, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  bool hasUnwindDest() const { return getSubclassData<UnwindDestField>(); }\n  bool unwindsToCaller() const { return !hasUnwindDest(); }\n\n  /// Convenience accessor.\n  CleanupPadInst *getCleanupPad() const {\n    return cast<CleanupPadInst>(Op<0>());\n  }\n  void setCleanupPad(CleanupPadInst *CleanupPad) {\n    assert(CleanupPad);\n    Op<0>() = CleanupPad;\n  }\n\n  unsigned getNumSuccessors() const { return hasUnwindDest() ? 1 : 0; }\n\n  BasicBlock *getUnwindDest() const {\n    return hasUnwindDest() ? cast<BasicBlock>(Op<1>()) : nullptr;\n  }\n  void setUnwindDest(BasicBlock *NewDest) {\n    assert(NewDest);\n    assert(hasUnwindDest());\n    Op<1>() = NewDest;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::CleanupRet);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned Idx) const {\n    assert(Idx == 0);\n    return getUnwindDest();\n  }\n\n  void setSuccessor(unsigned Idx, BasicBlock *B) {\n    assert(Idx == 0);\n    setUnwindDest(B);\n  }\n\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CleanupReturnInst>\n    : public VariadicOperandTraits<CleanupReturnInst, /*MINARITY=*/1> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CleanupReturnInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                           UnreachableInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// This function has undefined behavior.  In particular, the\n/// presence of this instruction indicates some higher level knowledge that the\n/// end of the block cannot be reached.\n///\nclass UnreachableInst : public Instruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  UnreachableInst *cloneImpl() const;\n\npublic:\n  explicit UnreachableInst(LLVMContext &C, Instruction *InsertBefore = nullptr);\n  explicit UnreachableInst(LLVMContext &C, BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 0);\n  }\n\n  unsigned getNumSuccessors() const { return 0; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Unreachable;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned idx) const {\n    llvm_unreachable(\"UnreachableInst has no successors!\");\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *B) {\n    llvm_unreachable(\"UnreachableInst has no successors!\");\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 TruncInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a truncation of integer types.\nclass TruncInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical TruncInst\n  TruncInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  TruncInst(\n    Value *S,                           ///< The value to be truncated\n    Type *Ty,                           ///< The (smaller) type to truncate to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  TruncInst(\n    Value *S,                     ///< The value to be truncated\n    Type *Ty,                     ///< The (smaller) type to truncate to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Trunc;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 ZExtInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents zero extension of integer types.\nclass ZExtInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical ZExtInst\n  ZExtInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  ZExtInst(\n    Value *S,                           ///< The value to be zero extended\n    Type *Ty,                           ///< The type to zero extend to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end semantics.\n  ZExtInst(\n    Value *S,                     ///< The value to be zero extended\n    Type *Ty,                     ///< The type to zero extend to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == ZExt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 SExtInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a sign extension of integer types.\nclass SExtInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical SExtInst\n  SExtInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  SExtInst(\n    Value *S,                           ///< The value to be sign extended\n    Type *Ty,                           ///< The type to sign extend to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  SExtInst(\n    Value *S,                     ///< The value to be sign extended\n    Type *Ty,                     ///< The type to sign extend to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == SExt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPTruncInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a truncation of floating point types.\nclass FPTruncInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPTruncInst\n  FPTruncInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPTruncInst(\n    Value *S,                           ///< The value to be truncated\n    Type *Ty,                           ///< The type to truncate to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-before-instruction semantics\n  FPTruncInst(\n    Value *S,                     ///< The value to be truncated\n    Type *Ty,                     ///< The type to truncate to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPTrunc;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPExtInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents an extension of floating point types.\nclass FPExtInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPExtInst\n  FPExtInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPExtInst(\n    Value *S,                           ///< The value to be extended\n    Type *Ty,                           ///< The type to extend to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  FPExtInst(\n    Value *S,                     ///< The value to be extended\n    Type *Ty,                     ///< The type to extend to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPExt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 UIToFPInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast unsigned integer to floating point.\nclass UIToFPInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical UIToFPInst\n  UIToFPInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  UIToFPInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  UIToFPInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == UIToFP;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 SIToFPInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from signed integer to floating point.\nclass SIToFPInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical SIToFPInst\n  SIToFPInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  SIToFPInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  SIToFPInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == SIToFP;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPToUIInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from floating point to unsigned integer\nclass FPToUIInst  : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPToUIInst\n  FPToUIInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPToUIInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  FPToUIInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< Where to insert the new instruction\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPToUI;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPToSIInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from floating point to signed integer.\nclass FPToSIInst  : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPToSIInst\n  FPToSIInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPToSIInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  FPToSIInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPToSI;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 IntToPtrInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from an integer to a pointer.\nclass IntToPtrInst : public CastInst {\npublic:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Constructor with insert-before-instruction semantics\n  IntToPtrInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  IntToPtrInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Clone an identical IntToPtrInst.\n  IntToPtrInst *cloneImpl() const;\n\n  /// Returns the address space of this instruction's pointer type.\n  unsigned getAddressSpace() const {\n    return getType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == IntToPtr;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 PtrToIntInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from a pointer to an integer.\nclass PtrToIntInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical PtrToIntInst.\n  PtrToIntInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  PtrToIntInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  PtrToIntInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Gets the pointer operand.\n  Value *getPointerOperand() { return getOperand(0); }\n  /// Gets the pointer operand.\n  const Value *getPointerOperand() const { return getOperand(0); }\n  /// Gets the operand index of the pointer operand.\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == PtrToInt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                             BitCastInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a no-op cast from one type to another.\nclass BitCastInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical BitCastInst.\n  BitCastInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  BitCastInst(\n    Value *S,                           ///< The value to be casted\n    Type *Ty,                           ///< The type to casted to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  BitCastInst(\n    Value *S,                     ///< The value to be casted\n    Type *Ty,                     ///< The type to casted to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == BitCast;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                          AddrSpaceCastInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a conversion between pointers from one address space\n/// to another.\nclass AddrSpaceCastInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical AddrSpaceCastInst.\n  AddrSpaceCastInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  AddrSpaceCastInst(\n    Value *S,                           ///< The value to be casted\n    Type *Ty,                           ///< The type to casted to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  AddrSpaceCastInst(\n    Value *S,                     ///< The value to be casted\n    Type *Ty,                     ///< The type to casted to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == AddrSpaceCast;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  /// Gets the pointer operand.\n  Value *getPointerOperand() {\n    return getOperand(0);\n  }\n\n  /// Gets the pointer operand.\n  const Value *getPointerOperand() const {\n    return getOperand(0);\n  }\n\n  /// Gets the operand index of the pointer operand.\n  static unsigned getPointerOperandIndex() {\n    return 0U;\n  }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getSrcAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  /// Returns the address space of the result.\n  unsigned getDestAddressSpace() const {\n    return getType()->getPointerAddressSpace();\n  }\n};\n\n/// A helper function that returns the pointer operand of a load or store\n/// instruction. Returns nullptr if not load or store.\ninline const Value *getLoadStorePointerOperand(const Value *V) {\n  if (auto *Load = dyn_cast<LoadInst>(V))\n    return Load->getPointerOperand();\n  if (auto *Store = dyn_cast<StoreInst>(V))\n    return Store->getPointerOperand();\n  return nullptr;\n}\ninline Value *getLoadStorePointerOperand(Value *V) {\n  return const_cast<Value *>(\n      getLoadStorePointerOperand(static_cast<const Value *>(V)));\n}\n\n/// A helper function that returns the pointer operand of a load, store\n/// or GEP instruction. Returns nullptr if not load, store, or GEP.\ninline const Value *getPointerOperand(const Value *V) {\n  if (auto *Ptr = getLoadStorePointerOperand(V))\n    return Ptr;\n  if (auto *Gep = dyn_cast<GetElementPtrInst>(V))\n    return Gep->getPointerOperand();\n  return nullptr;\n}\ninline Value *getPointerOperand(Value *V) {\n  return const_cast<Value *>(getPointerOperand(static_cast<const Value *>(V)));\n}\n\n/// A helper function that returns the alignment of load or store instruction.\ninline Align getLoadStoreAlignment(Value *I) {\n  assert((isa<LoadInst>(I) || isa<StoreInst>(I)) &&\n         \"Expected Load or Store instruction\");\n  if (auto *LI = dyn_cast<LoadInst>(I))\n    return LI->getAlign();\n  return cast<StoreInst>(I)->getAlign();\n}\n\n/// A helper function that returns the address space of the pointer operand of\n/// load or store instruction.\ninline unsigned getLoadStoreAddressSpace(Value *I) {\n  assert((isa<LoadInst>(I) || isa<StoreInst>(I)) &&\n         \"Expected Load or Store instruction\");\n  if (auto *LI = dyn_cast<LoadInst>(I))\n    return LI->getPointerAddressSpace();\n  return cast<StoreInst>(I)->getPointerAddressSpace();\n}\n\n//===----------------------------------------------------------------------===//\n//                              FreezeInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a freeze function that returns random concrete\n/// value if an operand is either a poison value or an undef value\nclass FreezeInst : public UnaryInstruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FreezeInst\n  FreezeInst *cloneImpl() const;\n\npublic:\n  explicit FreezeInst(Value *S,\n                      const Twine &NameStr = \"\",\n                      Instruction *InsertBefore = nullptr);\n  FreezeInst(Value *S, const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static inline bool classof(const Instruction *I) {\n    return I->getOpcode() == Freeze;\n  }\n  static inline bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_INSTRUCTIONS_H\n"}, "77": {"id": 77, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCAsmMacro.h", "content": "//===- MCAsmMacro.h - Assembly Macros ---------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_MC_MCASMMACRO_H\n#define LLVM_MC_MCASMMACRO_H\n\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Support/SMLoc.h\"\n#include <vector>\n\nnamespace llvm {\n\n/// Target independent representation for an assembler token.\nclass AsmToken {\npublic:\n  enum TokenKind {\n    // Markers\n    Eof, Error,\n\n    // String values.\n    Identifier,\n    String,\n\n    // Integer values.\n    Integer,\n    BigNum, // larger than 64 bits\n\n    // Real values.\n    Real,\n\n    // Comments\n    Comment,\n    HashDirective,\n    // No-value.\n    EndOfStatement,\n    Colon,\n    Space,\n    Plus, Minus, Tilde,\n    Slash,     // '/'\n    BackSlash, // '\\'\n    LParen, RParen, LBrac, RBrac, LCurly, RCurly,\n    Star, Dot, Comma, Dollar, Equal, EqualEqual,\n\n    Pipe, PipePipe, Caret,\n    Amp, AmpAmp, Exclaim, ExclaimEqual, Percent, Hash,\n    Less, LessEqual, LessLess, LessGreater,\n    Greater, GreaterEqual, GreaterGreater, At, MinusGreater,\n\n    // MIPS unary expression operators such as %neg.\n    PercentCall16, PercentCall_Hi, PercentCall_Lo, PercentDtprel_Hi,\n    PercentDtprel_Lo, PercentGot, PercentGot_Disp, PercentGot_Hi, PercentGot_Lo,\n    PercentGot_Ofst, PercentGot_Page, PercentGottprel, PercentGp_Rel, PercentHi,\n    PercentHigher, PercentHighest, PercentLo, PercentNeg, PercentPcrel_Hi,\n    PercentPcrel_Lo, PercentTlsgd, PercentTlsldm, PercentTprel_Hi,\n    PercentTprel_Lo\n  };\n\nprivate:\n  TokenKind Kind;\n\n  /// A reference to the entire token contents; this is always a pointer into\n  /// a memory buffer owned by the source manager.\n  StringRef Str;\n\n  APInt IntVal;\n\npublic:\n  AsmToken() = default;\n  AsmToken(TokenKind Kind, StringRef Str, APInt IntVal)\n      : Kind(Kind), Str(Str), IntVal(std::move(IntVal)) {}\n  AsmToken(TokenKind Kind, StringRef Str, int64_t IntVal = 0)\n      : Kind(Kind), Str(Str), IntVal(64, IntVal, true) {}\n\n  TokenKind getKind() const { return Kind; }\n  bool is(TokenKind K) const { return Kind == K; }\n  bool isNot(TokenKind K) const { return Kind != K; }\n\n  SMLoc getLoc() const;\n  SMLoc getEndLoc() const;\n  SMRange getLocRange() const;\n\n  /// Get the contents of a string token (without quotes).\n  StringRef getStringContents() const {\n    assert(Kind == String && \"This token isn't a string!\");\n    return Str.slice(1, Str.size() - 1);\n  }\n\n  /// Get the identifier string for the current token, which should be an\n  /// identifier or a string. This gets the portion of the string which should\n  /// be used as the identifier, e.g., it does not include the quotes on\n  /// strings.\n  StringRef getIdentifier() const {\n    if (Kind == Identifier)\n      return getString();\n    return getStringContents();\n  }\n\n  /// Get the string for the current token, this includes all characters (for\n  /// example, the quotes on strings) in the token.\n  ///\n  /// The returned StringRef points into the source manager's memory buffer, and\n  /// is safe to store across calls to Lex().\n  StringRef getString() const { return Str; }\n\n  // FIXME: Don't compute this in advance, it makes every token larger, and is\n  // also not generally what we want (it is nicer for recovery etc. to lex 123br\n  // as a single token, then diagnose as an invalid number).\n  int64_t getIntVal() const {\n    assert(Kind == Integer && \"This token isn't an integer!\");\n    return IntVal.getZExtValue();\n  }\n\n  APInt getAPIntVal() const {\n    assert((Kind == Integer || Kind == BigNum) &&\n           \"This token isn't an integer!\");\n    return IntVal;\n  }\n\n  void dump(raw_ostream &OS) const;\n};\n\nstruct MCAsmMacroParameter {\n  StringRef Name;\n  std::vector<AsmToken> Value;\n  bool Required = false;\n  bool Vararg = false;\n\n#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)\n  void dump() const { dump(dbgs()); }\n  LLVM_DUMP_METHOD void dump(raw_ostream &OS) const;\n#endif\n};\n\ntypedef std::vector<MCAsmMacroParameter> MCAsmMacroParameters;\nstruct MCAsmMacro {\n  StringRef Name;\n  StringRef Body;\n  MCAsmMacroParameters Parameters;\n  std::vector<std::string> Locals;\n  bool IsFunction = false;\n\npublic:\n  MCAsmMacro(StringRef N, StringRef B, MCAsmMacroParameters P)\n      : Name(N), Body(B), Parameters(std::move(P)) {}\n  MCAsmMacro(StringRef N, StringRef B, MCAsmMacroParameters P,\n             std::vector<std::string> L, bool F)\n      : Name(N), Body(B), Parameters(std::move(P)), Locals(std::move(L)),\n        IsFunction(F) {}\n\n#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)\n  void dump() const { dump(dbgs()); }\n  LLVM_DUMP_METHOD void dump(raw_ostream &OS) const;\n#endif\n};\n} // namespace llvm\n\n#endif\n"}, "78": {"id": 78, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCContext.h", "content": "//===- MCContext.h - Machine Code Context -----------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_MC_MCCONTEXT_H\n#define LLVM_MC_MCCONTEXT_H\n\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/SetVector.h\"\n#include \"llvm/ADT/SmallString.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringMap.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/BinaryFormat/Dwarf.h\"\n#include \"llvm/BinaryFormat/ELF.h\"\n#include \"llvm/BinaryFormat/XCOFF.h\"\n#include \"llvm/MC/MCAsmMacro.h\"\n#include \"llvm/MC/MCDwarf.h\"\n#include \"llvm/MC/MCPseudoProbe.h\"\n#include \"llvm/MC/MCSubtargetInfo.h\"\n#include \"llvm/MC/MCTargetOptions.h\"\n#include \"llvm/MC/SectionKind.h\"\n#include \"llvm/Support/Allocator.h\"\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/Error.h\"\n#include \"llvm/Support/MD5.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <functional>\n#include <map>\n#include <memory>\n#include <string>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\n  class CodeViewContext;\n  class MCAsmInfo;\n  class MCLabel;\n  class MCObjectFileInfo;\n  class MCRegisterInfo;\n  class MCSection;\n  class MCSectionCOFF;\n  class MCSectionELF;\n  class MCSectionMachO;\n  class MCSectionWasm;\n  class MCSectionXCOFF;\n  class MCStreamer;\n  class MCSymbol;\n  class MCSymbolELF;\n  class MCSymbolWasm;\n  class MCSymbolXCOFF;\n  class MDNode;\n  class SMDiagnostic;\n  class SMLoc;\n  class SourceMgr;\n\n  /// Context object for machine code objects.  This class owns all of the\n  /// sections that it creates.\n  ///\n  class MCContext {\n  public:\n    using SymbolTable = StringMap<MCSymbol *, BumpPtrAllocator &>;\n    using DiagHandlerTy =\n        std::function<void(const SMDiagnostic &, bool, const SourceMgr &,\n                           std::vector<const MDNode *> &)>;\n\n  private:\n    /// The SourceMgr for this object, if any.\n    const SourceMgr *SrcMgr;\n\n    /// The SourceMgr for inline assembly, if any.\n    std::unique_ptr<SourceMgr> InlineSrcMgr;\n    std::vector<const MDNode *> LocInfos;\n\n    DiagHandlerTy DiagHandler;\n\n    /// The MCAsmInfo for this target.\n    const MCAsmInfo *MAI;\n\n    /// The MCRegisterInfo for this target.\n    const MCRegisterInfo *MRI;\n\n    /// The MCObjectFileInfo for this target.\n    const MCObjectFileInfo *MOFI;\n\n    std::unique_ptr<CodeViewContext> CVContext;\n\n    /// Allocator object used for creating machine code objects.\n    ///\n    /// We use a bump pointer allocator to avoid the need to track all allocated\n    /// objects.\n    BumpPtrAllocator Allocator;\n\n    SpecificBumpPtrAllocator<MCSectionCOFF> COFFAllocator;\n    SpecificBumpPtrAllocator<MCSectionELF> ELFAllocator;\n    SpecificBumpPtrAllocator<MCSectionMachO> MachOAllocator;\n    SpecificBumpPtrAllocator<MCSectionWasm> WasmAllocator;\n    SpecificBumpPtrAllocator<MCSectionXCOFF> XCOFFAllocator;\n    SpecificBumpPtrAllocator<MCInst> MCInstAllocator;\n\n    /// Bindings of names to symbols.\n    SymbolTable Symbols;\n\n    /// A mapping from a local label number and an instance count to a symbol.\n    /// For example, in the assembly\n    ///     1:\n    ///     2:\n    ///     1:\n    /// We have three labels represented by the pairs (1, 0), (2, 0) and (1, 1)\n    DenseMap<std::pair<unsigned, unsigned>, MCSymbol *> LocalSymbols;\n\n    /// Keeps tracks of names that were used both for used declared and\n    /// artificial symbols. The value is \"true\" if the name has been used for a\n    /// non-section symbol (there can be at most one of those, plus an unlimited\n    /// number of section symbols with the same name).\n    StringMap<bool, BumpPtrAllocator &> UsedNames;\n\n    /// Keeps track of labels that are used in inline assembly.\n    SymbolTable InlineAsmUsedLabelNames;\n\n    /// The next ID to dole out to an unnamed assembler temporary symbol with\n    /// a given prefix.\n    StringMap<unsigned> NextID;\n\n    /// Instances of directional local labels.\n    DenseMap<unsigned, MCLabel *> Instances;\n    /// NextInstance() creates the next instance of the directional local label\n    /// for the LocalLabelVal and adds it to the map if needed.\n    unsigned NextInstance(unsigned LocalLabelVal);\n    /// GetInstance() gets the current instance of the directional local label\n    /// for the LocalLabelVal and adds it to the map if needed.\n    unsigned GetInstance(unsigned LocalLabelVal);\n\n    /// The file name of the log file from the environment variable\n    /// AS_SECURE_LOG_FILE.  Which must be set before the .secure_log_unique\n    /// directive is used or it is an error.\n    char *SecureLogFile;\n    /// The stream that gets written to for the .secure_log_unique directive.\n    std::unique_ptr<raw_fd_ostream> SecureLog;\n    /// Boolean toggled when .secure_log_unique / .secure_log_reset is seen to\n    /// catch errors if .secure_log_unique appears twice without\n    /// .secure_log_reset appearing between them.\n    bool SecureLogUsed = false;\n\n    /// The compilation directory to use for DW_AT_comp_dir.\n    SmallString<128> CompilationDir;\n\n    /// Prefix replacement map for source file information.\n    std::map<const std::string, const std::string> DebugPrefixMap;\n\n    /// The main file name if passed in explicitly.\n    std::string MainFileName;\n\n    /// The dwarf file and directory tables from the dwarf .file directive.\n    /// We now emit a line table for each compile unit. To reduce the prologue\n    /// size of each line table, the files and directories used by each compile\n    /// unit are separated.\n    std::map<unsigned, MCDwarfLineTable> MCDwarfLineTablesCUMap;\n\n    /// The current dwarf line information from the last dwarf .loc directive.\n    MCDwarfLoc CurrentDwarfLoc;\n    bool DwarfLocSeen = false;\n\n    /// Generate dwarf debugging info for assembly source files.\n    bool GenDwarfForAssembly = false;\n\n    /// The current dwarf file number when generate dwarf debugging info for\n    /// assembly source files.\n    unsigned GenDwarfFileNumber = 0;\n\n    /// Sections for generating the .debug_ranges and .debug_aranges sections.\n    SetVector<MCSection *> SectionsForRanges;\n\n    /// The information gathered from labels that will have dwarf label\n    /// entries when generating dwarf assembly source files.\n    std::vector<MCGenDwarfLabelEntry> MCGenDwarfLabelEntries;\n\n    /// The string to embed in the debug information for the compile unit, if\n    /// non-empty.\n    StringRef DwarfDebugFlags;\n\n    /// The string to embed in as the dwarf AT_producer for the compile unit, if\n    /// non-empty.\n    StringRef DwarfDebugProducer;\n\n    /// The maximum version of dwarf that we should emit.\n    uint16_t DwarfVersion = 4;\n\n    /// The format of dwarf that we emit.\n    dwarf::DwarfFormat DwarfFormat = dwarf::DWARF32;\n\n    /// Honor temporary labels, this is useful for debugging semantic\n    /// differences between temporary and non-temporary labels (primarily on\n    /// Darwin).\n    bool AllowTemporaryLabels = true;\n    bool UseNamesOnTempLabels = false;\n\n    /// The Compile Unit ID that we are currently processing.\n    unsigned DwarfCompileUnitID = 0;\n\n    /// A collection of MCPseudoProbe in the current module\n    MCPseudoProbeTable PseudoProbeTable;\n\n    // Sections are differentiated by the quadruple (section_name, group_name,\n    // unique_id, link_to_symbol_name). Sections sharing the same quadruple are\n    // combined into one section.\n    struct ELFSectionKey {\n      std::string SectionName;\n      StringRef GroupName;\n      StringRef LinkedToName;\n      unsigned UniqueID;\n\n      ELFSectionKey(StringRef SectionName, StringRef GroupName,\n                    StringRef LinkedToName, unsigned UniqueID)\n          : SectionName(SectionName), GroupName(GroupName),\n            LinkedToName(LinkedToName), UniqueID(UniqueID) {}\n\n      bool operator<(const ELFSectionKey &Other) const {\n        if (SectionName != Other.SectionName)\n          return SectionName < Other.SectionName;\n        if (GroupName != Other.GroupName)\n          return GroupName < Other.GroupName;\n        if (int O = LinkedToName.compare(Other.LinkedToName))\n          return O < 0;\n        return UniqueID < Other.UniqueID;\n      }\n    };\n\n    struct COFFSectionKey {\n      std::string SectionName;\n      StringRef GroupName;\n      int SelectionKey;\n      unsigned UniqueID;\n\n      COFFSectionKey(StringRef SectionName, StringRef GroupName,\n                     int SelectionKey, unsigned UniqueID)\n          : SectionName(SectionName), GroupName(GroupName),\n            SelectionKey(SelectionKey), UniqueID(UniqueID) {}\n\n      bool operator<(const COFFSectionKey &Other) const {\n        if (SectionName != Other.SectionName)\n          return SectionName < Other.SectionName;\n        if (GroupName != Other.GroupName)\n          return GroupName < Other.GroupName;\n        if (SelectionKey != Other.SelectionKey)\n          return SelectionKey < Other.SelectionKey;\n        return UniqueID < Other.UniqueID;\n      }\n    };\n\n    struct WasmSectionKey {\n      std::string SectionName;\n      StringRef GroupName;\n      unsigned UniqueID;\n\n      WasmSectionKey(StringRef SectionName, StringRef GroupName,\n                     unsigned UniqueID)\n          : SectionName(SectionName), GroupName(GroupName), UniqueID(UniqueID) {\n      }\n\n      bool operator<(const WasmSectionKey &Other) const {\n        if (SectionName != Other.SectionName)\n          return SectionName < Other.SectionName;\n        if (GroupName != Other.GroupName)\n          return GroupName < Other.GroupName;\n        return UniqueID < Other.UniqueID;\n      }\n    };\n\n    struct XCOFFSectionKey {\n      // Section name.\n      std::string SectionName;\n      // Section property.\n      // For csect section, it is storage mapping class.\n      // For debug section, it is section type flags.\n      union {\n        XCOFF::StorageMappingClass MappingClass;\n        XCOFF::DwarfSectionSubtypeFlags DwarfSubtypeFlags;\n      };\n      bool IsCsect;\n\n      XCOFFSectionKey(StringRef SectionName,\n                      XCOFF::StorageMappingClass MappingClass)\n          : SectionName(SectionName), MappingClass(MappingClass),\n            IsCsect(true) {}\n\n      XCOFFSectionKey(StringRef SectionName,\n                      XCOFF::DwarfSectionSubtypeFlags DwarfSubtypeFlags)\n          : SectionName(SectionName), DwarfSubtypeFlags(DwarfSubtypeFlags),\n            IsCsect(false) {}\n\n      bool operator<(const XCOFFSectionKey &Other) const {\n        if (IsCsect && Other.IsCsect)\n          return std::tie(SectionName, MappingClass) <\n                 std::tie(Other.SectionName, Other.MappingClass);\n        if (IsCsect != Other.IsCsect)\n          return IsCsect;\n        return std::tie(SectionName, DwarfSubtypeFlags) <\n               std::tie(Other.SectionName, Other.DwarfSubtypeFlags);\n      }\n    };\n\n    StringMap<MCSectionMachO *> MachOUniquingMap;\n    std::map<ELFSectionKey, MCSectionELF *> ELFUniquingMap;\n    std::map<COFFSectionKey, MCSectionCOFF *> COFFUniquingMap;\n    std::map<WasmSectionKey, MCSectionWasm *> WasmUniquingMap;\n    std::map<XCOFFSectionKey, MCSectionXCOFF *> XCOFFUniquingMap;\n    StringMap<bool> RelSecNames;\n\n    SpecificBumpPtrAllocator<MCSubtargetInfo> MCSubtargetAllocator;\n\n    /// Do automatic reset in destructor\n    bool AutoReset;\n\n    MCTargetOptions const *TargetOptions;\n\n    bool HadError = false;\n\n    void reportCommon(SMLoc Loc,\n                      std::function<void(SMDiagnostic &, const SourceMgr *)>);\n\n    MCSymbol *createSymbolImpl(const StringMapEntry<bool> *Name,\n                               bool CanBeUnnamed);\n    MCSymbol *createSymbol(StringRef Name, bool AlwaysAddSuffix,\n                           bool IsTemporary);\n\n    MCSymbol *getOrCreateDirectionalLocalSymbol(unsigned LocalLabelVal,\n                                                unsigned Instance);\n\n    MCSectionELF *createELFSectionImpl(StringRef Section, unsigned Type,\n                                       unsigned Flags, SectionKind K,\n                                       unsigned EntrySize,\n                                       const MCSymbolELF *Group, bool IsComdat,\n                                       unsigned UniqueID,\n                                       const MCSymbolELF *LinkedToSym);\n\n    MCSymbolXCOFF *createXCOFFSymbolImpl(const StringMapEntry<bool> *Name,\n                                         bool IsTemporary);\n\n    /// Map of currently defined macros.\n    StringMap<MCAsmMacro> MacroMap;\n\n    struct ELFEntrySizeKey {\n      std::string SectionName;\n      unsigned Flags;\n      unsigned EntrySize;\n\n      ELFEntrySizeKey(StringRef SectionName, unsigned Flags, unsigned EntrySize)\n          : SectionName(SectionName), Flags(Flags), EntrySize(EntrySize) {}\n\n      bool operator<(const ELFEntrySizeKey &Other) const {\n        if (SectionName != Other.SectionName)\n          return SectionName < Other.SectionName;\n        if ((Flags & ELF::SHF_STRINGS) != (Other.Flags & ELF::SHF_STRINGS))\n          return Other.Flags & ELF::SHF_STRINGS;\n        return EntrySize < Other.EntrySize;\n      }\n    };\n\n    // Symbols must be assigned to a section with a compatible entry\n    // size. This map is used to assign unique IDs to sections to\n    // distinguish between sections with identical names but incompatible entry\n    // sizes. This can occur when a symbol is explicitly assigned to a\n    // section, e.g. via __attribute__((section(\"myname\"))).\n    std::map<ELFEntrySizeKey, unsigned> ELFEntrySizeMap;\n\n    // This set is used to record the generic mergeable section names seen.\n    // These are sections that are created as mergeable e.g. .debug_str. We need\n    // to avoid assigning non-mergeable symbols to these sections. It is used\n    // to prevent non-mergeable symbols being explicitly assigned  to mergeable\n    // sections (e.g. via _attribute_((section(\"myname\")))).\n    DenseSet<StringRef> ELFSeenGenericMergeableSections;\n\n  public:\n    explicit MCContext(const MCAsmInfo *MAI, const MCRegisterInfo *MRI,\n                       const MCObjectFileInfo *MOFI,\n                       const SourceMgr *Mgr = nullptr,\n                       MCTargetOptions const *TargetOpts = nullptr,\n                       bool DoAutoReset = true);\n    MCContext(const MCContext &) = delete;\n    MCContext &operator=(const MCContext &) = delete;\n    ~MCContext();\n\n    const SourceMgr *getSourceManager() const { return SrcMgr; }\n\n    void initInlineSourceManager();\n    SourceMgr *getInlineSourceManager() {\n      assert(InlineSrcMgr);\n      return InlineSrcMgr.get();\n    }\n    std::vector<const MDNode *> &getLocInfos() { return LocInfos; }\n    void setDiagnosticHandler(DiagHandlerTy DiagHandler) {\n      this->DiagHandler = DiagHandler;\n    }\n\n    const MCAsmInfo *getAsmInfo() const { return MAI; }\n\n    const MCRegisterInfo *getRegisterInfo() const { return MRI; }\n\n    const MCObjectFileInfo *getObjectFileInfo() const { return MOFI; }\n\n    CodeViewContext &getCVContext();\n\n    void setAllowTemporaryLabels(bool Value) { AllowTemporaryLabels = Value; }\n    void setUseNamesOnTempLabels(bool Value) { UseNamesOnTempLabels = Value; }\n\n    /// \\name Module Lifetime Management\n    /// @{\n\n    /// reset - return object to right after construction state to prepare\n    /// to process a new module\n    void reset();\n\n    /// @}\n\n    /// \\name McInst Management\n\n    /// Create and return a new MC instruction.\n    MCInst *createMCInst();\n\n    /// \\name Symbol Management\n    /// @{\n\n    /// Create and return a new linker temporary symbol with a unique but\n    /// unspecified name.\n    MCSymbol *createLinkerPrivateTempSymbol();\n\n    /// Create a temporary symbol with a unique name. The name will be omitted\n    /// in the symbol table if UseNamesOnTempLabels is false (default except\n    /// MCAsmStreamer). The overload without Name uses an unspecified name.\n    MCSymbol *createTempSymbol();\n    MCSymbol *createTempSymbol(const Twine &Name, bool AlwaysAddSuffix = true);\n\n    /// Create a temporary symbol with a unique name whose name cannot be\n    /// omitted in the symbol table. This is rarely used.\n    MCSymbol *createNamedTempSymbol();\n    MCSymbol *createNamedTempSymbol(const Twine &Name);\n\n    /// Create the definition of a directional local symbol for numbered label\n    /// (used for \"1:\" definitions).\n    MCSymbol *createDirectionalLocalSymbol(unsigned LocalLabelVal);\n\n    /// Create and return a directional local symbol for numbered label (used\n    /// for \"1b\" or 1f\" references).\n    MCSymbol *getDirectionalLocalSymbol(unsigned LocalLabelVal, bool Before);\n\n    /// Lookup the symbol inside with the specified \\p Name.  If it exists,\n    /// return it.  If not, create a forward reference and return it.\n    ///\n    /// \\param Name - The symbol name, which must be unique across all symbols.\n    MCSymbol *getOrCreateSymbol(const Twine &Name);\n\n    /// Gets a symbol that will be defined to the final stack offset of a local\n    /// variable after codegen.\n    ///\n    /// \\param Idx - The index of a local variable passed to \\@llvm.localescape.\n    MCSymbol *getOrCreateFrameAllocSymbol(StringRef FuncName, unsigned Idx);\n\n    MCSymbol *getOrCreateParentFrameOffsetSymbol(StringRef FuncName);\n\n    MCSymbol *getOrCreateLSDASymbol(StringRef FuncName);\n\n    /// Get the symbol for \\p Name, or null.\n    MCSymbol *lookupSymbol(const Twine &Name) const;\n\n    /// Set value for a symbol.\n    void setSymbolValue(MCStreamer &Streamer, StringRef Sym, uint64_t Val);\n\n    /// getSymbols - Get a reference for the symbol table for clients that\n    /// want to, for example, iterate over all symbols. 'const' because we\n    /// still want any modifications to the table itself to use the MCContext\n    /// APIs.\n    const SymbolTable &getSymbols() const { return Symbols; }\n\n    /// isInlineAsmLabel - Return true if the name is a label referenced in\n    /// inline assembly.\n    MCSymbol *getInlineAsmLabel(StringRef Name) const {\n      return InlineAsmUsedLabelNames.lookup(Name);\n    }\n\n    /// registerInlineAsmLabel - Records that the name is a label referenced in\n    /// inline assembly.\n    void registerInlineAsmLabel(MCSymbol *Sym);\n\n    /// @}\n\n    /// \\name Section Management\n    /// @{\n\n    enum : unsigned {\n      /// Pass this value as the UniqueID during section creation to get the\n      /// generic section with the given name and characteristics. The usual\n      /// sections such as .text use this ID.\n      GenericSectionID = ~0U\n    };\n\n    /// Return the MCSection for the specified mach-o section.  This requires\n    /// the operands to be valid.\n    MCSectionMachO *getMachOSection(StringRef Segment, StringRef Section,\n                                    unsigned TypeAndAttributes,\n                                    unsigned Reserved2, SectionKind K,\n                                    const char *BeginSymName = nullptr);\n\n    MCSectionMachO *getMachOSection(StringRef Segment, StringRef Section,\n                                    unsigned TypeAndAttributes, SectionKind K,\n                                    const char *BeginSymName = nullptr) {\n      return getMachOSection(Segment, Section, TypeAndAttributes, 0, K,\n                             BeginSymName);\n    }\n\n    MCSectionELF *getELFSection(const Twine &Section, unsigned Type,\n                                unsigned Flags) {\n      return getELFSection(Section, Type, Flags, 0, \"\", false);\n    }\n\n    MCSectionELF *getELFSection(const Twine &Section, unsigned Type,\n                                unsigned Flags, unsigned EntrySize) {\n      return getELFSection(Section, Type, Flags, EntrySize, \"\", false,\n                           MCSection::NonUniqueID, nullptr);\n    }\n\n    MCSectionELF *getELFSection(const Twine &Section, unsigned Type,\n                                unsigned Flags, unsigned EntrySize,\n                                const Twine &Group, bool IsComdat) {\n      return getELFSection(Section, Type, Flags, EntrySize, Group, IsComdat,\n                           MCSection::NonUniqueID, nullptr);\n    }\n\n    MCSectionELF *getELFSection(const Twine &Section, unsigned Type,\n                                unsigned Flags, unsigned EntrySize,\n                                const Twine &Group, bool IsComdat,\n                                unsigned UniqueID,\n                                const MCSymbolELF *LinkedToSym);\n\n    MCSectionELF *getELFSection(const Twine &Section, unsigned Type,\n                                unsigned Flags, unsigned EntrySize,\n                                const MCSymbolELF *Group, bool IsComdat,\n                                unsigned UniqueID,\n                                const MCSymbolELF *LinkedToSym);\n\n    /// Get a section with the provided group identifier. This section is\n    /// named by concatenating \\p Prefix with '.' then \\p Suffix. The \\p Type\n    /// describes the type of the section and \\p Flags are used to further\n    /// configure this named section.\n    MCSectionELF *getELFNamedSection(const Twine &Prefix, const Twine &Suffix,\n                                     unsigned Type, unsigned Flags,\n                                     unsigned EntrySize = 0);\n\n    MCSectionELF *createELFRelSection(const Twine &Name, unsigned Type,\n                                      unsigned Flags, unsigned EntrySize,\n                                      const MCSymbolELF *Group,\n                                      const MCSectionELF *RelInfoSection);\n\n    void renameELFSection(MCSectionELF *Section, StringRef Name);\n\n    MCSectionELF *createELFGroupSection(const MCSymbolELF *Group,\n                                        bool IsComdat);\n\n    void recordELFMergeableSectionInfo(StringRef SectionName, unsigned Flags,\n                                       unsigned UniqueID, unsigned EntrySize);\n\n    bool isELFImplicitMergeableSectionNamePrefix(StringRef Name);\n\n    bool isELFGenericMergeableSection(StringRef Name);\n\n    Optional<unsigned> getELFUniqueIDForEntsize(StringRef SectionName,\n                                                unsigned Flags,\n                                                unsigned EntrySize);\n\n    MCSectionCOFF *getCOFFSection(StringRef Section, unsigned Characteristics,\n                                  SectionKind Kind, StringRef COMDATSymName,\n                                  int Selection,\n                                  unsigned UniqueID = GenericSectionID,\n                                  const char *BeginSymName = nullptr);\n\n    MCSectionCOFF *getCOFFSection(StringRef Section, unsigned Characteristics,\n                                  SectionKind Kind,\n                                  const char *BeginSymName = nullptr);\n\n    /// Gets or creates a section equivalent to Sec that is associated with the\n    /// section containing KeySym. For example, to create a debug info section\n    /// associated with an inline function, pass the normal debug info section\n    /// as Sec and the function symbol as KeySym.\n    MCSectionCOFF *\n    getAssociativeCOFFSection(MCSectionCOFF *Sec, const MCSymbol *KeySym,\n                              unsigned UniqueID = GenericSectionID);\n\n    MCSectionWasm *getWasmSection(const Twine &Section, SectionKind K) {\n      return getWasmSection(Section, K, nullptr);\n    }\n\n    MCSectionWasm *getWasmSection(const Twine &Section, SectionKind K,\n                                  const char *BeginSymName) {\n      return getWasmSection(Section, K, \"\", ~0, BeginSymName);\n    }\n\n    MCSectionWasm *getWasmSection(const Twine &Section, SectionKind K,\n                                  const Twine &Group, unsigned UniqueID) {\n      return getWasmSection(Section, K, Group, UniqueID, nullptr);\n    }\n\n    MCSectionWasm *getWasmSection(const Twine &Section, SectionKind K,\n                                  const Twine &Group, unsigned UniqueID,\n                                  const char *BeginSymName);\n\n    MCSectionWasm *getWasmSection(const Twine &Section, SectionKind K,\n                                  const MCSymbolWasm *Group, unsigned UniqueID,\n                                  const char *BeginSymName);\n\n    MCSectionXCOFF *getXCOFFSection(\n        StringRef Section, SectionKind K,\n        Optional<XCOFF::CsectProperties> CsectProp = None,\n        bool MultiSymbolsAllowed = false, const char *BeginSymName = nullptr,\n        Optional<XCOFF::DwarfSectionSubtypeFlags> DwarfSubtypeFlags = None);\n\n    // Create and save a copy of STI and return a reference to the copy.\n    MCSubtargetInfo &getSubtargetCopy(const MCSubtargetInfo &STI);\n\n    /// @}\n\n    /// \\name Dwarf Management\n    /// @{\n\n    /// Get the compilation directory for DW_AT_comp_dir\n    /// The compilation directory should be set with \\c setCompilationDir before\n    /// calling this function. If it is unset, an empty string will be returned.\n    StringRef getCompilationDir() const { return CompilationDir; }\n\n    /// Set the compilation directory for DW_AT_comp_dir\n    void setCompilationDir(StringRef S) { CompilationDir = S.str(); }\n\n    /// Add an entry to the debug prefix map.\n    void addDebugPrefixMapEntry(const std::string &From, const std::string &To);\n\n    // Remaps all debug directory paths in-place as per the debug prefix map.\n    void RemapDebugPaths();\n\n    /// Get the main file name for use in error messages and debug\n    /// info. This can be set to ensure we've got the correct file name\n    /// after preprocessing or for -save-temps.\n    const std::string &getMainFileName() const { return MainFileName; }\n\n    /// Set the main file name and override the default.\n    void setMainFileName(StringRef S) { MainFileName = std::string(S); }\n\n    /// Creates an entry in the dwarf file and directory tables.\n    Expected<unsigned> getDwarfFile(StringRef Directory, StringRef FileName,\n                                    unsigned FileNumber,\n                                    Optional<MD5::MD5Result> Checksum,\n                                    Optional<StringRef> Source, unsigned CUID);\n\n    bool isValidDwarfFileNumber(unsigned FileNumber, unsigned CUID = 0);\n\n    const std::map<unsigned, MCDwarfLineTable> &getMCDwarfLineTables() const {\n      return MCDwarfLineTablesCUMap;\n    }\n\n    MCDwarfLineTable &getMCDwarfLineTable(unsigned CUID) {\n      return MCDwarfLineTablesCUMap[CUID];\n    }\n\n    const MCDwarfLineTable &getMCDwarfLineTable(unsigned CUID) const {\n      auto I = MCDwarfLineTablesCUMap.find(CUID);\n      assert(I != MCDwarfLineTablesCUMap.end());\n      return I->second;\n    }\n\n    const SmallVectorImpl<MCDwarfFile> &getMCDwarfFiles(unsigned CUID = 0) {\n      return getMCDwarfLineTable(CUID).getMCDwarfFiles();\n    }\n\n    const SmallVectorImpl<std::string> &getMCDwarfDirs(unsigned CUID = 0) {\n      return getMCDwarfLineTable(CUID).getMCDwarfDirs();\n    }\n\n    unsigned getDwarfCompileUnitID() { return DwarfCompileUnitID; }\n\n    void setDwarfCompileUnitID(unsigned CUIndex) {\n      DwarfCompileUnitID = CUIndex;\n    }\n\n    /// Specifies the \"root\" file and directory of the compilation unit.\n    /// These are \"file 0\" and \"directory 0\" in DWARF v5.\n    void setMCLineTableRootFile(unsigned CUID, StringRef CompilationDir,\n                                StringRef Filename,\n                                Optional<MD5::MD5Result> Checksum,\n                                Optional<StringRef> Source) {\n      getMCDwarfLineTable(CUID).setRootFile(CompilationDir, Filename, Checksum,\n                                            Source);\n    }\n\n    /// Reports whether MD5 checksum usage is consistent (all-or-none).\n    bool isDwarfMD5UsageConsistent(unsigned CUID) const {\n      return getMCDwarfLineTable(CUID).isMD5UsageConsistent();\n    }\n\n    /// Saves the information from the currently parsed dwarf .loc directive\n    /// and sets DwarfLocSeen.  When the next instruction is assembled an entry\n    /// in the line number table with this information and the address of the\n    /// instruction will be created.\n    void setCurrentDwarfLoc(unsigned FileNum, unsigned Line, unsigned Column,\n                            unsigned Flags, unsigned Isa,\n                            unsigned Discriminator) {\n      CurrentDwarfLoc.setFileNum(FileNum);\n      CurrentDwarfLoc.setLine(Line);\n      CurrentDwarfLoc.setColumn(Column);\n      CurrentDwarfLoc.setFlags(Flags);\n      CurrentDwarfLoc.setIsa(Isa);\n      CurrentDwarfLoc.setDiscriminator(Discriminator);\n      DwarfLocSeen = true;\n    }\n\n    void clearDwarfLocSeen() { DwarfLocSeen = false; }\n\n    bool getDwarfLocSeen() { return DwarfLocSeen; }\n    const MCDwarfLoc &getCurrentDwarfLoc() { return CurrentDwarfLoc; }\n\n    bool getGenDwarfForAssembly() { return GenDwarfForAssembly; }\n    void setGenDwarfForAssembly(bool Value) { GenDwarfForAssembly = Value; }\n    unsigned getGenDwarfFileNumber() { return GenDwarfFileNumber; }\n\n    void setGenDwarfFileNumber(unsigned FileNumber) {\n      GenDwarfFileNumber = FileNumber;\n    }\n\n    /// Specifies information about the \"root file\" for assembler clients\n    /// (e.g., llvm-mc). Assumes compilation dir etc. have been set up.\n    void setGenDwarfRootFile(StringRef FileName, StringRef Buffer);\n\n    const SetVector<MCSection *> &getGenDwarfSectionSyms() {\n      return SectionsForRanges;\n    }\n\n    bool addGenDwarfSection(MCSection *Sec) {\n      return SectionsForRanges.insert(Sec);\n    }\n\n    void finalizeDwarfSections(MCStreamer &MCOS);\n\n    const std::vector<MCGenDwarfLabelEntry> &getMCGenDwarfLabelEntries() const {\n      return MCGenDwarfLabelEntries;\n    }\n\n    void addMCGenDwarfLabelEntry(const MCGenDwarfLabelEntry &E) {\n      MCGenDwarfLabelEntries.push_back(E);\n    }\n\n    void setDwarfDebugFlags(StringRef S) { DwarfDebugFlags = S; }\n    StringRef getDwarfDebugFlags() { return DwarfDebugFlags; }\n\n    void setDwarfDebugProducer(StringRef S) { DwarfDebugProducer = S; }\n    StringRef getDwarfDebugProducer() { return DwarfDebugProducer; }\n\n    void setDwarfFormat(dwarf::DwarfFormat f) { DwarfFormat = f; }\n    dwarf::DwarfFormat getDwarfFormat() const { return DwarfFormat; }\n\n    void setDwarfVersion(uint16_t v) { DwarfVersion = v; }\n    uint16_t getDwarfVersion() const { return DwarfVersion; }\n\n    /// @}\n\n    char *getSecureLogFile() { return SecureLogFile; }\n    raw_fd_ostream *getSecureLog() { return SecureLog.get(); }\n\n    void setSecureLog(std::unique_ptr<raw_fd_ostream> Value) {\n      SecureLog = std::move(Value);\n    }\n\n    bool getSecureLogUsed() { return SecureLogUsed; }\n    void setSecureLogUsed(bool Value) { SecureLogUsed = Value; }\n\n    void *allocate(unsigned Size, unsigned Align = 8) {\n      return Allocator.Allocate(Size, Align);\n    }\n\n    void deallocate(void *Ptr) {}\n\n    bool hadError() { return HadError; }\n    void diagnose(const SMDiagnostic &SMD);\n    void reportError(SMLoc L, const Twine &Msg);\n    void reportWarning(SMLoc L, const Twine &Msg);\n    // Unrecoverable error has occurred. Display the best diagnostic we can\n    // and bail via exit(1). For now, most MC backend errors are unrecoverable.\n    // FIXME: We should really do something about that.\n    LLVM_ATTRIBUTE_NORETURN void reportFatalError(SMLoc L, const Twine &Msg);\n\n    const MCAsmMacro *lookupMacro(StringRef Name) {\n      StringMap<MCAsmMacro>::iterator I = MacroMap.find(Name);\n      return (I == MacroMap.end()) ? nullptr : &I->getValue();\n    }\n\n    void defineMacro(StringRef Name, MCAsmMacro Macro) {\n      MacroMap.insert(std::make_pair(Name, std::move(Macro)));\n    }\n\n    void undefineMacro(StringRef Name) { MacroMap.erase(Name); }\n\n    MCPseudoProbeTable &getMCPseudoProbeTable() { return PseudoProbeTable; }\n  };\n\n} // end namespace llvm\n\n// operator new and delete aren't allowed inside namespaces.\n// The throw specifications are mandated by the standard.\n/// Placement new for using the MCContext's allocator.\n///\n/// This placement form of operator new uses the MCContext's allocator for\n/// obtaining memory. It is a non-throwing new, which means that it returns\n/// null on error. (If that is what the allocator does. The current does, so if\n/// this ever changes, this operator will have to be changed, too.)\n/// Usage looks like this (assuming there's an MCContext 'Context' in scope):\n/// \\code\n/// // Default alignment (8)\n/// IntegerLiteral *Ex = new (Context) IntegerLiteral(arguments);\n/// // Specific alignment\n/// IntegerLiteral *Ex2 = new (Context, 4) IntegerLiteral(arguments);\n/// \\endcode\n/// Please note that you cannot use delete on the pointer; it must be\n/// deallocated using an explicit destructor call followed by\n/// \\c Context.Deallocate(Ptr).\n///\n/// \\param Bytes The number of bytes to allocate. Calculated by the compiler.\n/// \\param C The MCContext that provides the allocator.\n/// \\param Alignment The alignment of the allocated memory (if the underlying\n///                  allocator supports it).\n/// \\return The allocated memory. Could be NULL.\ninline void *operator new(size_t Bytes, llvm::MCContext &C,\n                          size_t Alignment = 8) noexcept {\n  return C.allocate(Bytes, Alignment);\n}\n/// Placement delete companion to the new above.\n///\n/// This operator is just a companion to the new above. There is no way of\n/// invoking it directly; see the new operator for more details. This operator\n/// is called implicitly by the compiler if a placement new expression using\n/// the MCContext throws in the object constructor.\ninline void operator delete(void *Ptr, llvm::MCContext &C, size_t) noexcept {\n  C.deallocate(Ptr);\n}\n\n/// This placement form of operator new[] uses the MCContext's allocator for\n/// obtaining memory. It is a non-throwing new[], which means that it returns\n/// null on error.\n/// Usage looks like this (assuming there's an MCContext 'Context' in scope):\n/// \\code\n/// // Default alignment (8)\n/// char *data = new (Context) char[10];\n/// // Specific alignment\n/// char *data = new (Context, 4) char[10];\n/// \\endcode\n/// Please note that you cannot use delete on the pointer; it must be\n/// deallocated using an explicit destructor call followed by\n/// \\c Context.Deallocate(Ptr).\n///\n/// \\param Bytes The number of bytes to allocate. Calculated by the compiler.\n/// \\param C The MCContext that provides the allocator.\n/// \\param Alignment The alignment of the allocated memory (if the underlying\n///                  allocator supports it).\n/// \\return The allocated memory. Could be NULL.\ninline void *operator new[](size_t Bytes, llvm::MCContext &C,\n                            size_t Alignment = 8) noexcept {\n  return C.allocate(Bytes, Alignment);\n}\n\n/// Placement delete[] companion to the new[] above.\n///\n/// This operator is just a companion to the new[] above. There is no way of\n/// invoking it directly; see the new[] operator for more details. This operator\n/// is called implicitly by the compiler if a placement new[] expression using\n/// the MCContext throws in the object constructor.\ninline void operator delete[](void *Ptr, llvm::MCContext &C) noexcept {\n  C.deallocate(Ptr);\n}\n\n#endif // LLVM_MC_MCCONTEXT_H\n"}, "83": {"id": 83, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCInstrInfo.h", "content": "//===-- llvm/MC/MCInstrInfo.h - Target Instruction Info ---------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file describes the target machine instruction set.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_MC_MCINSTRINFO_H\n#define LLVM_MC_MCINSTRINFO_H\n\n#include \"llvm/MC/MCInstrDesc.h\"\n#include <cassert>\n\nnamespace llvm {\n\nclass MCSubtargetInfo;\n\n//---------------------------------------------------------------------------\n/// Interface to description of machine instruction set.\nclass MCInstrInfo {\npublic:\n  using ComplexDeprecationPredicate = bool (*)(MCInst &,\n                                               const MCSubtargetInfo &,\n                                               std::string &);\n\nprivate:\n  const MCInstrDesc *Desc;          // Raw array to allow static init'n\n  const unsigned *InstrNameIndices; // Array for name indices in InstrNameData\n  const char *InstrNameData;        // Instruction name string pool\n  // Subtarget feature that an instruction is deprecated on, if any\n  // -1 implies this is not deprecated by any single feature. It may still be\n  // deprecated due to a \"complex\" reason, below.\n  const uint8_t *DeprecatedFeatures;\n  // A complex method to determine if a certain instruction is deprecated or\n  // not, and return the reason for deprecation.\n  const ComplexDeprecationPredicate *ComplexDeprecationInfos;\n  unsigned NumOpcodes;              // Number of entries in the desc array\n\npublic:\n  /// Initialize MCInstrInfo, called by TableGen auto-generated routines.\n  /// *DO NOT USE*.\n  void InitMCInstrInfo(const MCInstrDesc *D, const unsigned *NI, const char *ND,\n                       const uint8_t *DF,\n                       const ComplexDeprecationPredicate *CDI, unsigned NO) {\n    Desc = D;\n    InstrNameIndices = NI;\n    InstrNameData = ND;\n    DeprecatedFeatures = DF;\n    ComplexDeprecationInfos = CDI;\n    NumOpcodes = NO;\n  }\n\n  unsigned getNumOpcodes() const { return NumOpcodes; }\n\n  /// Return the machine instruction descriptor that corresponds to the\n  /// specified instruction opcode.\n  const MCInstrDesc &get(unsigned Opcode) const {\n    assert(Opcode < NumOpcodes && \"Invalid opcode!\");\n    return Desc[Opcode];\n  }\n\n  /// Returns the name for the instructions with the given opcode.\n  StringRef getName(unsigned Opcode) const {\n    assert(Opcode < NumOpcodes && \"Invalid opcode!\");\n    return StringRef(&InstrNameData[InstrNameIndices[Opcode]]);\n  }\n\n  /// Returns true if a certain instruction is deprecated and if so\n  /// returns the reason in \\p Info.\n  bool getDeprecatedInfo(MCInst &MI, const MCSubtargetInfo &STI,\n                         std::string &Info) const;\n};\n\n} // End llvm namespace\n\n#endif\n"}, "84": {"id": 84, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCRegisterInfo.h", "content": "//===- MC/MCRegisterInfo.h - Target Register Description --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file describes an abstract interface used to get information about a\n// target machines register file.  This information is used for a variety of\n// purposed, especially register allocation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_MC_MCREGISTERINFO_H\n#define LLVM_MC_MCREGISTERINFO_H\n\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/MC/LaneBitmask.h\"\n#include \"llvm/MC/MCRegister.h\"\n#include <cassert>\n#include <cstdint>\n#include <iterator>\n#include <utility>\n\nnamespace llvm {\n\n/// MCRegisterClass - Base class of TargetRegisterClass.\nclass MCRegisterClass {\npublic:\n  using iterator = const MCPhysReg*;\n  using const_iterator = const MCPhysReg*;\n\n  const iterator RegsBegin;\n  const uint8_t *const RegSet;\n  const uint32_t NameIdx;\n  const uint16_t RegsSize;\n  const uint16_t RegSetSize;\n  const uint16_t ID;\n  const int8_t CopyCost;\n  const bool Allocatable;\n\n  /// getID() - Return the register class ID number.\n  ///\n  unsigned getID() const { return ID; }\n\n  /// begin/end - Return all of the registers in this class.\n  ///\n  iterator       begin() const { return RegsBegin; }\n  iterator         end() const { return RegsBegin + RegsSize; }\n\n  /// getNumRegs - Return the number of registers in this class.\n  ///\n  unsigned getNumRegs() const { return RegsSize; }\n\n  /// getRegister - Return the specified register in the class.\n  ///\n  unsigned getRegister(unsigned i) const {\n    assert(i < getNumRegs() && \"Register number out of range!\");\n    return RegsBegin[i];\n  }\n\n  /// contains - Return true if the specified register is included in this\n  /// register class.  This does not include virtual registers.\n  bool contains(MCRegister Reg) const {\n    unsigned RegNo = unsigned(Reg);\n    unsigned InByte = RegNo % 8;\n    unsigned Byte = RegNo / 8;\n    if (Byte >= RegSetSize)\n      return false;\n    return (RegSet[Byte] & (1 << InByte)) != 0;\n  }\n\n  /// contains - Return true if both registers are in this class.\n  bool contains(MCRegister Reg1, MCRegister Reg2) const {\n    return contains(Reg1) && contains(Reg2);\n  }\n\n  /// getCopyCost - Return the cost of copying a value between two registers in\n  /// this class. A negative number means the register class is very expensive\n  /// to copy e.g. status flag register classes.\n  int getCopyCost() const { return CopyCost; }\n\n  /// isAllocatable - Return true if this register class may be used to create\n  /// virtual registers.\n  bool isAllocatable() const { return Allocatable; }\n};\n\n/// MCRegisterDesc - This record contains information about a particular\n/// register.  The SubRegs field is a zero terminated array of registers that\n/// are sub-registers of the specific register, e.g. AL, AH are sub-registers\n/// of AX. The SuperRegs field is a zero terminated array of registers that are\n/// super-registers of the specific register, e.g. RAX, EAX, are\n/// super-registers of AX.\n///\nstruct MCRegisterDesc {\n  uint32_t Name;      // Printable name for the reg (for debugging)\n  uint32_t SubRegs;   // Sub-register set, described above\n  uint32_t SuperRegs; // Super-register set, described above\n\n  // Offset into MCRI::SubRegIndices of a list of sub-register indices for each\n  // sub-register in SubRegs.\n  uint32_t SubRegIndices;\n\n  // RegUnits - Points to the list of register units. The low 4 bits holds the\n  // Scale, the high bits hold an offset into DiffLists. See MCRegUnitIterator.\n  uint32_t RegUnits;\n\n  /// Index into list with lane mask sequences. The sequence contains a lanemask\n  /// for every register unit.\n  uint16_t RegUnitLaneMasks;\n};\n\n/// MCRegisterInfo base class - We assume that the target defines a static\n/// array of MCRegisterDesc objects that represent all of the machine\n/// registers that the target has.  As such, we simply have to track a pointer\n/// to this array so that we can turn register number into a register\n/// descriptor.\n///\n/// Note this class is designed to be a base class of TargetRegisterInfo, which\n/// is the interface used by codegen. However, specific targets *should never*\n/// specialize this class. MCRegisterInfo should only contain getters to access\n/// TableGen generated physical register data. It must not be extended with\n/// virtual methods.\n///\nclass MCRegisterInfo {\npublic:\n  using regclass_iterator = const MCRegisterClass *;\n\n  /// DwarfLLVMRegPair - Emitted by tablegen so Dwarf<->LLVM reg mappings can be\n  /// performed with a binary search.\n  struct DwarfLLVMRegPair {\n    unsigned FromReg;\n    unsigned ToReg;\n\n    bool operator<(DwarfLLVMRegPair RHS) const { return FromReg < RHS.FromReg; }\n  };\n\n  /// SubRegCoveredBits - Emitted by tablegen: bit range covered by a subreg\n  /// index, -1 in any being invalid.\n  struct SubRegCoveredBits {\n    uint16_t Offset;\n    uint16_t Size;\n  };\n\nprivate:\n  const MCRegisterDesc *Desc;                 // Pointer to the descriptor array\n  unsigned NumRegs;                           // Number of entries in the array\n  MCRegister RAReg;                           // Return address register\n  MCRegister PCReg;                           // Program counter register\n  const MCRegisterClass *Classes;             // Pointer to the regclass array\n  unsigned NumClasses;                        // Number of entries in the array\n  unsigned NumRegUnits;                       // Number of regunits.\n  const MCPhysReg (*RegUnitRoots)[2];         // Pointer to regunit root table.\n  const MCPhysReg *DiffLists;                 // Pointer to the difflists array\n  const LaneBitmask *RegUnitMaskSequences;    // Pointer to lane mask sequences\n                                              // for register units.\n  const char *RegStrings;                     // Pointer to the string table.\n  const char *RegClassStrings;                // Pointer to the class strings.\n  const uint16_t *SubRegIndices;              // Pointer to the subreg lookup\n                                              // array.\n  const SubRegCoveredBits *SubRegIdxRanges;   // Pointer to the subreg covered\n                                              // bit ranges array.\n  unsigned NumSubRegIndices;                  // Number of subreg indices.\n  const uint16_t *RegEncodingTable;           // Pointer to array of register\n                                              // encodings.\n\n  unsigned L2DwarfRegsSize;\n  unsigned EHL2DwarfRegsSize;\n  unsigned Dwarf2LRegsSize;\n  unsigned EHDwarf2LRegsSize;\n  const DwarfLLVMRegPair *L2DwarfRegs;        // LLVM to Dwarf regs mapping\n  const DwarfLLVMRegPair *EHL2DwarfRegs;      // LLVM to Dwarf regs mapping EH\n  const DwarfLLVMRegPair *Dwarf2LRegs;        // Dwarf to LLVM regs mapping\n  const DwarfLLVMRegPair *EHDwarf2LRegs;      // Dwarf to LLVM regs mapping EH\n  DenseMap<MCRegister, int> L2SEHRegs;        // LLVM to SEH regs mapping\n  DenseMap<MCRegister, int> L2CVRegs;         // LLVM to CV regs mapping\n\npublic:\n  // Forward declaration to become a friend class of DiffListIterator.\n  template <class SubT> class mc_difflist_iterator;\n\n  /// DiffListIterator - Base iterator class that can traverse the\n  /// differentially encoded register and regunit lists in DiffLists.\n  /// Don't use this class directly, use one of the specialized sub-classes\n  /// defined below.\n  class DiffListIterator {\n    uint16_t Val = 0;\n    const MCPhysReg *List = nullptr;\n\n  protected:\n    /// Create an invalid iterator. Call init() to point to something useful.\n    DiffListIterator() = default;\n\n    /// init - Point the iterator to InitVal, decoding subsequent values from\n    /// DiffList. The iterator will initially point to InitVal, sub-classes are\n    /// responsible for skipping the seed value if it is not part of the list.\n    void init(MCPhysReg InitVal, const MCPhysReg *DiffList) {\n      Val = InitVal;\n      List = DiffList;\n    }\n\n    /// advance - Move to the next list position, return the applied\n    /// differential. This function does not detect the end of the list, that\n    /// is the caller's responsibility (by checking for a 0 return value).\n    MCRegister advance() {\n      assert(isValid() && \"Cannot move off the end of the list.\");\n      MCPhysReg D = *List++;\n      Val += D;\n      return D;\n    }\n\n  public:\n    /// isValid - returns true if this iterator is not yet at the end.\n    bool isValid() const { return List; }\n\n    /// Dereference the iterator to get the value at the current position.\n    MCRegister operator*() const { return Val; }\n\n    /// Pre-increment to move to the next position.\n    void operator++() {\n      // The end of the list is encoded as a 0 differential.\n      if (!advance())\n        List = nullptr;\n    }\n\n    template <class SubT> friend class MCRegisterInfo::mc_difflist_iterator;\n  };\n\n  /// Forward iterator using DiffListIterator.\n  template <class SubT>\n  class mc_difflist_iterator\n      : public iterator_facade_base<mc_difflist_iterator<SubT>,\n                                    std::forward_iterator_tag, MCPhysReg> {\n    MCRegisterInfo::DiffListIterator Iter;\n    /// Current value as MCPhysReg, so we can return a reference to it.\n    MCPhysReg Val;\n\n  protected:\n    mc_difflist_iterator(MCRegisterInfo::DiffListIterator Iter) : Iter(Iter) {}\n\n    // Allow conversion between instantiations where valid.\n    mc_difflist_iterator(MCRegister Reg, const MCPhysReg *DiffList) {\n      Iter.init(Reg, DiffList);\n      Val = *Iter;\n    }\n\n  public:\n    // Allow default construction to build variables, but this doesn't build\n    // a useful iterator.\n    mc_difflist_iterator() = default;\n\n    /// Return an iterator past the last element.\n    static SubT end() {\n      SubT End;\n      End.Iter.List = nullptr;\n      return End;\n    }\n\n    bool operator==(const mc_difflist_iterator &Arg) const {\n      return Iter.List == Arg.Iter.List;\n    }\n\n    const MCPhysReg &operator*() const { return Val; }\n\n    using mc_difflist_iterator::iterator_facade_base::operator++;\n    void operator++() {\n      assert(Iter.List && \"Cannot increment the end iterator!\");\n      ++Iter;\n      Val = *Iter;\n    }\n  };\n\n  /// Forward iterator over all sub-registers.\n  /// TODO: Replace remaining uses of MCSubRegIterator.\n  class mc_subreg_iterator : public mc_difflist_iterator<mc_subreg_iterator> {\n  public:\n    mc_subreg_iterator(MCRegisterInfo::DiffListIterator Iter)\n        : mc_difflist_iterator(Iter) {}\n    mc_subreg_iterator() = default;\n    mc_subreg_iterator(MCRegister Reg, const MCRegisterInfo *MCRI)\n        : mc_difflist_iterator(Reg, MCRI->DiffLists + MCRI->get(Reg).SubRegs) {}\n  };\n\n  /// Forward iterator over all super-registers.\n  /// TODO: Replace remaining uses of MCSuperRegIterator.\n  class mc_superreg_iterator\n      : public mc_difflist_iterator<mc_superreg_iterator> {\n  public:\n    mc_superreg_iterator(MCRegisterInfo::DiffListIterator Iter)\n        : mc_difflist_iterator(Iter) {}\n    mc_superreg_iterator() = default;\n    mc_superreg_iterator(MCRegister Reg, const MCRegisterInfo *MCRI)\n        : mc_difflist_iterator(Reg,\n                               MCRI->DiffLists + MCRI->get(Reg).SuperRegs) {}\n  };\n\n  /// Return an iterator range over all sub-registers of \\p Reg, excluding \\p\n  /// Reg.\n  iterator_range<mc_subreg_iterator> subregs(MCRegister Reg) const {\n    return make_range(std::next(mc_subreg_iterator(Reg, this)),\n                      mc_subreg_iterator::end());\n  }\n\n  /// Return an iterator range over all sub-registers of \\p Reg, including \\p\n  /// Reg.\n  iterator_range<mc_subreg_iterator> subregs_inclusive(MCRegister Reg) const {\n    return make_range({Reg, this}, mc_subreg_iterator::end());\n  }\n\n  /// Return an iterator range over all super-registers of \\p Reg, excluding \\p\n  /// Reg.\n  iterator_range<mc_superreg_iterator> superregs(MCRegister Reg) const {\n    return make_range(std::next(mc_superreg_iterator(Reg, this)),\n                      mc_superreg_iterator::end());\n  }\n\n  /// Return an iterator range over all super-registers of \\p Reg, including \\p\n  /// Reg.\n  iterator_range<mc_superreg_iterator>\n  superregs_inclusive(MCRegister Reg) const {\n    return make_range({Reg, this}, mc_superreg_iterator::end());\n  }\n\n  /// Return an iterator range over all sub- and super-registers of \\p Reg,\n  /// including \\p Reg.\n  detail::concat_range<const MCPhysReg, iterator_range<mc_subreg_iterator>,\n                       iterator_range<mc_superreg_iterator>>\n  sub_and_superregs_inclusive(MCRegister Reg) const {\n    return concat<const MCPhysReg>(subregs_inclusive(Reg), superregs(Reg));\n  }\n\n  // These iterators are allowed to sub-class DiffListIterator and access\n  // internal list pointers.\n  friend class MCSubRegIterator;\n  friend class MCSubRegIndexIterator;\n  friend class MCSuperRegIterator;\n  friend class MCRegUnitIterator;\n  friend class MCRegUnitMaskIterator;\n  friend class MCRegUnitRootIterator;\n\n  /// Initialize MCRegisterInfo, called by TableGen\n  /// auto-generated routines. *DO NOT USE*.\n  void InitMCRegisterInfo(const MCRegisterDesc *D, unsigned NR, unsigned RA,\n                          unsigned PC,\n                          const MCRegisterClass *C, unsigned NC,\n                          const MCPhysReg (*RURoots)[2],\n                          unsigned NRU,\n                          const MCPhysReg *DL,\n                          const LaneBitmask *RUMS,\n                          const char *Strings,\n                          const char *ClassStrings,\n                          const uint16_t *SubIndices,\n                          unsigned NumIndices,\n                          const SubRegCoveredBits *SubIdxRanges,\n                          const uint16_t *RET) {\n    Desc = D;\n    NumRegs = NR;\n    RAReg = RA;\n    PCReg = PC;\n    Classes = C;\n    DiffLists = DL;\n    RegUnitMaskSequences = RUMS;\n    RegStrings = Strings;\n    RegClassStrings = ClassStrings;\n    NumClasses = NC;\n    RegUnitRoots = RURoots;\n    NumRegUnits = NRU;\n    SubRegIndices = SubIndices;\n    NumSubRegIndices = NumIndices;\n    SubRegIdxRanges = SubIdxRanges;\n    RegEncodingTable = RET;\n\n    // Initialize DWARF register mapping variables\n    EHL2DwarfRegs = nullptr;\n    EHL2DwarfRegsSize = 0;\n    L2DwarfRegs = nullptr;\n    L2DwarfRegsSize = 0;\n    EHDwarf2LRegs = nullptr;\n    EHDwarf2LRegsSize = 0;\n    Dwarf2LRegs = nullptr;\n    Dwarf2LRegsSize = 0;\n  }\n\n  /// Used to initialize LLVM register to Dwarf\n  /// register number mapping. Called by TableGen auto-generated routines.\n  /// *DO NOT USE*.\n  void mapLLVMRegsToDwarfRegs(const DwarfLLVMRegPair *Map, unsigned Size,\n                              bool isEH) {\n    if (isEH) {\n      EHL2DwarfRegs = Map;\n      EHL2DwarfRegsSize = Size;\n    } else {\n      L2DwarfRegs = Map;\n      L2DwarfRegsSize = Size;\n    }\n  }\n\n  /// Used to initialize Dwarf register to LLVM\n  /// register number mapping. Called by TableGen auto-generated routines.\n  /// *DO NOT USE*.\n  void mapDwarfRegsToLLVMRegs(const DwarfLLVMRegPair *Map, unsigned Size,\n                              bool isEH) {\n    if (isEH) {\n      EHDwarf2LRegs = Map;\n      EHDwarf2LRegsSize = Size;\n    } else {\n      Dwarf2LRegs = Map;\n      Dwarf2LRegsSize = Size;\n    }\n  }\n\n  /// mapLLVMRegToSEHReg - Used to initialize LLVM register to SEH register\n  /// number mapping. By default the SEH register number is just the same\n  /// as the LLVM register number.\n  /// FIXME: TableGen these numbers. Currently this requires target specific\n  /// initialization code.\n  void mapLLVMRegToSEHReg(MCRegister LLVMReg, int SEHReg) {\n    L2SEHRegs[LLVMReg] = SEHReg;\n  }\n\n  void mapLLVMRegToCVReg(MCRegister LLVMReg, int CVReg) {\n    L2CVRegs[LLVMReg] = CVReg;\n  }\n\n  /// This method should return the register where the return\n  /// address can be found.\n  MCRegister getRARegister() const {\n    return RAReg;\n  }\n\n  /// Return the register which is the program counter.\n  MCRegister getProgramCounter() const {\n    return PCReg;\n  }\n\n  const MCRegisterDesc &operator[](MCRegister RegNo) const {\n    assert(RegNo < NumRegs &&\n           \"Attempting to access record for invalid register number!\");\n    return Desc[RegNo];\n  }\n\n  /// Provide a get method, equivalent to [], but more useful with a\n  /// pointer to this object.\n  const MCRegisterDesc &get(MCRegister RegNo) const {\n    return operator[](RegNo);\n  }\n\n  /// Returns the physical register number of sub-register \"Index\"\n  /// for physical register RegNo. Return zero if the sub-register does not\n  /// exist.\n  MCRegister getSubReg(MCRegister Reg, unsigned Idx) const;\n\n  /// Return a super-register of the specified register\n  /// Reg so its sub-register of index SubIdx is Reg.\n  MCRegister getMatchingSuperReg(MCRegister Reg, unsigned SubIdx,\n                                 const MCRegisterClass *RC) const;\n\n  /// For a given register pair, return the sub-register index\n  /// if the second register is a sub-register of the first. Return zero\n  /// otherwise.\n  unsigned getSubRegIndex(MCRegister RegNo, MCRegister SubRegNo) const;\n\n  /// Get the size of the bit range covered by a sub-register index.\n  /// If the index isn't continuous, return the sum of the sizes of its parts.\n  /// If the index is used to access subregisters of different sizes, return -1.\n  unsigned getSubRegIdxSize(unsigned Idx) const;\n\n  /// Get the offset of the bit range covered by a sub-register index.\n  /// If an Offset doesn't make sense (the index isn't continuous, or is used to\n  /// access sub-registers at different offsets), return -1.\n  unsigned getSubRegIdxOffset(unsigned Idx) const;\n\n  /// Return the human-readable symbolic target-specific name for the\n  /// specified physical register.\n  const char *getName(MCRegister RegNo) const {\n    return RegStrings + get(RegNo).Name;\n  }\n\n  /// Return the number of registers this target has (useful for\n  /// sizing arrays holding per register information)\n  unsigned getNumRegs() const {\n    return NumRegs;\n  }\n\n  /// Return the number of sub-register indices\n  /// understood by the target. Index 0 is reserved for the no-op sub-register,\n  /// while 1 to getNumSubRegIndices() - 1 represent real sub-registers.\n  unsigned getNumSubRegIndices() const {\n    return NumSubRegIndices;\n  }\n\n  /// Return the number of (native) register units in the\n  /// target. Register units are numbered from 0 to getNumRegUnits() - 1. They\n  /// can be accessed through MCRegUnitIterator defined below.\n  unsigned getNumRegUnits() const {\n    return NumRegUnits;\n  }\n\n  /// Map a target register to an equivalent dwarf register\n  /// number.  Returns -1 if there is no equivalent value.  The second\n  /// parameter allows targets to use different numberings for EH info and\n  /// debugging info.\n  int getDwarfRegNum(MCRegister RegNum, bool isEH) const;\n\n  /// Map a dwarf register back to a target register. Returns None is there is\n  /// no mapping.\n  Optional<unsigned> getLLVMRegNum(unsigned RegNum, bool isEH) const;\n\n  /// Map a target EH register number to an equivalent DWARF register\n  /// number.\n  int getDwarfRegNumFromDwarfEHRegNum(unsigned RegNum) const;\n\n  /// Map a target register to an equivalent SEH register\n  /// number.  Returns LLVM register number if there is no equivalent value.\n  int getSEHRegNum(MCRegister RegNum) const;\n\n  /// Map a target register to an equivalent CodeView register\n  /// number.\n  int getCodeViewRegNum(MCRegister RegNum) const;\n\n  regclass_iterator regclass_begin() const { return Classes; }\n  regclass_iterator regclass_end() const { return Classes+NumClasses; }\n  iterator_range<regclass_iterator> regclasses() const {\n    return make_range(regclass_begin(), regclass_end());\n  }\n\n  unsigned getNumRegClasses() const {\n    return (unsigned)(regclass_end()-regclass_begin());\n  }\n\n  /// Returns the register class associated with the enumeration\n  /// value.  See class MCOperandInfo.\n  const MCRegisterClass& getRegClass(unsigned i) const {\n    assert(i < getNumRegClasses() && \"Register Class ID out of range\");\n    return Classes[i];\n  }\n\n  const char *getRegClassName(const MCRegisterClass *Class) const {\n    return RegClassStrings + Class->NameIdx;\n  }\n\n   /// Returns the encoding for RegNo\n  uint16_t getEncodingValue(MCRegister RegNo) const {\n    assert(RegNo < NumRegs &&\n           \"Attempting to get encoding for invalid register number!\");\n    return RegEncodingTable[RegNo];\n  }\n\n  /// Returns true if RegB is a sub-register of RegA.\n  bool isSubRegister(MCRegister RegA, MCRegister RegB) const {\n    return isSuperRegister(RegB, RegA);\n  }\n\n  /// Returns true if RegB is a super-register of RegA.\n  bool isSuperRegister(MCRegister RegA, MCRegister RegB) const;\n\n  /// Returns true if RegB is a sub-register of RegA or if RegB == RegA.\n  bool isSubRegisterEq(MCRegister RegA, MCRegister RegB) const {\n    return isSuperRegisterEq(RegB, RegA);\n  }\n\n  /// Returns true if RegB is a super-register of RegA or if\n  /// RegB == RegA.\n  bool isSuperRegisterEq(MCRegister RegA, MCRegister RegB) const {\n    return RegA == RegB || isSuperRegister(RegA, RegB);\n  }\n\n  /// Returns true if RegB is a super-register or sub-register of RegA\n  /// or if RegB == RegA.\n  bool isSuperOrSubRegisterEq(MCRegister RegA, MCRegister RegB) const {\n    return isSubRegisterEq(RegA, RegB) || isSuperRegister(RegA, RegB);\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                          Register List Iterators\n//===----------------------------------------------------------------------===//\n\n// MCRegisterInfo provides lists of super-registers, sub-registers, and\n// aliasing registers. Use these iterator classes to traverse the lists.\n\n/// MCSubRegIterator enumerates all sub-registers of Reg.\n/// If IncludeSelf is set, Reg itself is included in the list.\nclass MCSubRegIterator : public MCRegisterInfo::DiffListIterator {\npublic:\n  MCSubRegIterator(MCRegister Reg, const MCRegisterInfo *MCRI,\n                   bool IncludeSelf = false) {\n    init(Reg, MCRI->DiffLists + MCRI->get(Reg).SubRegs);\n    // Initially, the iterator points to Reg itself.\n    if (!IncludeSelf)\n      ++*this;\n  }\n};\n\n/// Iterator that enumerates the sub-registers of a Reg and the associated\n/// sub-register indices.\nclass MCSubRegIndexIterator {\n  MCSubRegIterator SRIter;\n  const uint16_t *SRIndex;\n\npublic:\n  /// Constructs an iterator that traverses subregisters and their\n  /// associated subregister indices.\n  MCSubRegIndexIterator(MCRegister Reg, const MCRegisterInfo *MCRI)\n    : SRIter(Reg, MCRI) {\n    SRIndex = MCRI->SubRegIndices + MCRI->get(Reg).SubRegIndices;\n  }\n\n  /// Returns current sub-register.\n  MCRegister getSubReg() const {\n    return *SRIter;\n  }\n\n  /// Returns sub-register index of the current sub-register.\n  unsigned getSubRegIndex() const {\n    return *SRIndex;\n  }\n\n  /// Returns true if this iterator is not yet at the end.\n  bool isValid() const { return SRIter.isValid(); }\n\n  /// Moves to the next position.\n  void operator++() {\n    ++SRIter;\n    ++SRIndex;\n  }\n};\n\n/// MCSuperRegIterator enumerates all super-registers of Reg.\n/// If IncludeSelf is set, Reg itself is included in the list.\nclass MCSuperRegIterator : public MCRegisterInfo::DiffListIterator {\npublic:\n  MCSuperRegIterator() = default;\n\n  MCSuperRegIterator(MCRegister Reg, const MCRegisterInfo *MCRI,\n                     bool IncludeSelf = false) {\n    init(Reg, MCRI->DiffLists + MCRI->get(Reg).SuperRegs);\n    // Initially, the iterator points to Reg itself.\n    if (!IncludeSelf)\n      ++*this;\n  }\n};\n\n// Definition for isSuperRegister. Put it down here since it needs the\n// iterator defined above in addition to the MCRegisterInfo class itself.\ninline bool MCRegisterInfo::isSuperRegister(MCRegister RegA, MCRegister RegB) const{\n  for (MCSuperRegIterator I(RegA, this); I.isValid(); ++I)\n    if (*I == RegB)\n      return true;\n  return false;\n}\n\n//===----------------------------------------------------------------------===//\n//                               Register Units\n//===----------------------------------------------------------------------===//\n\n// Register units are used to compute register aliasing. Every register has at\n// least one register unit, but it can have more. Two registers overlap if and\n// only if they have a common register unit.\n//\n// A target with a complicated sub-register structure will typically have many\n// fewer register units than actual registers. MCRI::getNumRegUnits() returns\n// the number of register units in the target.\n\n// MCRegUnitIterator enumerates a list of register units for Reg. The list is\n// in ascending numerical order.\nclass MCRegUnitIterator : public MCRegisterInfo::DiffListIterator {\npublic:\n  /// MCRegUnitIterator - Create an iterator that traverses the register units\n  /// in Reg.\n  MCRegUnitIterator() = default;\n\n  MCRegUnitIterator(MCRegister Reg, const MCRegisterInfo *MCRI) {\n    assert(Reg && \"Null register has no regunits\");\n    assert(MCRegister::isPhysicalRegister(Reg.id()));\n    // Decode the RegUnits MCRegisterDesc field.\n    unsigned RU = MCRI->get(Reg).RegUnits;\n    unsigned Scale = RU & 15;\n    unsigned Offset = RU >> 4;\n\n    // Initialize the iterator to Reg * Scale, and the List pointer to\n    // DiffLists + Offset.\n    init(Reg * Scale, MCRI->DiffLists + Offset);\n\n    // That may not be a valid unit, we need to advance by one to get the real\n    // unit number. The first differential can be 0 which would normally\n    // terminate the list, but since we know every register has at least one\n    // unit, we can allow a 0 differential here.\n    advance();\n  }\n};\n\n/// MCRegUnitMaskIterator enumerates a list of register units and their\n/// associated lane masks for Reg. The register units are in ascending\n/// numerical order.\nclass MCRegUnitMaskIterator {\n  MCRegUnitIterator RUIter;\n  const LaneBitmask *MaskListIter;\n\npublic:\n  MCRegUnitMaskIterator() = default;\n\n  /// Constructs an iterator that traverses the register units and their\n  /// associated LaneMasks in Reg.\n  MCRegUnitMaskIterator(MCRegister Reg, const MCRegisterInfo *MCRI)\n    : RUIter(Reg, MCRI) {\n      uint16_t Idx = MCRI->get(Reg).RegUnitLaneMasks;\n      MaskListIter = &MCRI->RegUnitMaskSequences[Idx];\n  }\n\n  /// Returns a (RegUnit, LaneMask) pair.\n  std::pair<unsigned,LaneBitmask> operator*() const {\n    return std::make_pair(*RUIter, *MaskListIter);\n  }\n\n  /// Returns true if this iterator is not yet at the end.\n  bool isValid() const { return RUIter.isValid(); }\n\n  /// Moves to the next position.\n  void operator++() {\n    ++MaskListIter;\n    ++RUIter;\n  }\n};\n\n// Each register unit has one or two root registers. The complete set of\n// registers containing a register unit is the union of the roots and their\n// super-registers. All registers aliasing Unit can be visited like this:\n//\n//   for (MCRegUnitRootIterator RI(Unit, MCRI); RI.isValid(); ++RI) {\n//     for (MCSuperRegIterator SI(*RI, MCRI, true); SI.isValid(); ++SI)\n//       visit(*SI);\n//    }\n\n/// MCRegUnitRootIterator enumerates the root registers of a register unit.\nclass MCRegUnitRootIterator {\n  uint16_t Reg0 = 0;\n  uint16_t Reg1 = 0;\n\npublic:\n  MCRegUnitRootIterator() = default;\n\n  MCRegUnitRootIterator(unsigned RegUnit, const MCRegisterInfo *MCRI) {\n    assert(RegUnit < MCRI->getNumRegUnits() && \"Invalid register unit\");\n    Reg0 = MCRI->RegUnitRoots[RegUnit][0];\n    Reg1 = MCRI->RegUnitRoots[RegUnit][1];\n  }\n\n  /// Dereference to get the current root register.\n  unsigned operator*() const {\n    return Reg0;\n  }\n\n  /// Check if the iterator is at the end of the list.\n  bool isValid() const {\n    return Reg0;\n  }\n\n  /// Preincrement to move to the next root register.\n  void operator++() {\n    assert(isValid() && \"Cannot move off the end of the list.\");\n    Reg0 = Reg1;\n    Reg1 = 0;\n  }\n};\n\n/// MCRegAliasIterator enumerates all registers aliasing Reg.  If IncludeSelf is\n/// set, Reg itself is included in the list.  This iterator does not guarantee\n/// any ordering or that entries are unique.\nclass MCRegAliasIterator {\nprivate:\n  MCRegister Reg;\n  const MCRegisterInfo *MCRI;\n  bool IncludeSelf;\n\n  MCRegUnitIterator RI;\n  MCRegUnitRootIterator RRI;\n  MCSuperRegIterator SI;\n\npublic:\n  MCRegAliasIterator(MCRegister Reg, const MCRegisterInfo *MCRI,\n                     bool IncludeSelf)\n    : Reg(Reg), MCRI(MCRI), IncludeSelf(IncludeSelf) {\n    // Initialize the iterators.\n    for (RI = MCRegUnitIterator(Reg, MCRI); RI.isValid(); ++RI) {\n      for (RRI = MCRegUnitRootIterator(*RI, MCRI); RRI.isValid(); ++RRI) {\n        for (SI = MCSuperRegIterator(*RRI, MCRI, true); SI.isValid(); ++SI) {\n          if (!(!IncludeSelf && Reg == *SI))\n            return;\n        }\n      }\n    }\n  }\n\n  bool isValid() const { return RI.isValid(); }\n\n  MCRegister operator*() const {\n    assert(SI.isValid() && \"Cannot dereference an invalid iterator.\");\n    return *SI;\n  }\n\n  void advance() {\n    // Assuming SI is valid.\n    ++SI;\n    if (SI.isValid()) return;\n\n    ++RRI;\n    if (RRI.isValid()) {\n      SI = MCSuperRegIterator(*RRI, MCRI, true);\n      return;\n    }\n\n    ++RI;\n    if (RI.isValid()) {\n      RRI = MCRegUnitRootIterator(*RI, MCRI);\n      SI = MCSuperRegIterator(*RRI, MCRI, true);\n    }\n  }\n\n  void operator++() {\n    assert(isValid() && \"Cannot move off the end of the list.\");\n    do advance();\n    while (!IncludeSelf && isValid() && *SI == Reg);\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_MC_MCREGISTERINFO_H\n"}, "110": {"id": 110, "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/GISelMITest.h", "content": "//===- GISelMITest.h --------------------------------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n#ifndef LLVM_UNITTEST_CODEGEN_GLOBALISEL_GISELMI_H\n#define LLVM_UNITTEST_CODEGEN_GLOBALISEL_GISELMI_H\n\n#include \"llvm/CodeGen/GlobalISel/GISelChangeObserver.h\"\n#include \"llvm/CodeGen/GlobalISel/LegalizerHelper.h\"\n#include \"llvm/CodeGen/GlobalISel/LegalizerInfo.h\"\n#include \"llvm/CodeGen/GlobalISel/MIPatternMatch.h\"\n#include \"llvm/CodeGen/GlobalISel/MachineIRBuilder.h\"\n#include \"llvm/CodeGen/GlobalISel/Utils.h\"\n#include \"llvm/CodeGen/MIRParser/MIRParser.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineModuleInfo.h\"\n#include \"llvm/CodeGen/TargetFrameLowering.h\"\n#include \"llvm/CodeGen/TargetInstrInfo.h\"\n#include \"llvm/CodeGen/TargetLowering.h\"\n#include \"llvm/CodeGen/TargetSubtargetInfo.h\"\n#include \"llvm/FileCheck/FileCheck.h\"\n#include \"llvm/InitializePasses.h\"\n#include \"llvm/Support/SourceMgr.h\"\n#include \"llvm/Support/TargetRegistry.h\"\n#include \"llvm/Support/TargetSelect.h\"\n#include \"llvm/Target/TargetMachine.h\"\n#include \"llvm/Target/TargetOptions.h\"\n#include \"gtest/gtest.h\"\n\nusing namespace llvm;\nusing namespace MIPatternMatch;\n\nstatic inline void initLLVM() {\n  InitializeAllTargets();\n  InitializeAllTargetMCs();\n  InitializeAllAsmPrinters();\n  InitializeAllAsmParsers();\n\n  PassRegistry *Registry = PassRegistry::getPassRegistry();\n  initializeCore(*Registry);\n  initializeCodeGen(*Registry);\n}\n\n// Define a printers to help debugging when things go wrong.\nnamespace llvm {\nstd::ostream &\noperator<<(std::ostream &OS, const LLT Ty);\n\nstd::ostream &\noperator<<(std::ostream &OS, const MachineFunction &MF);\n}\n\nstatic std::unique_ptr<Module> parseMIR(LLVMContext &Context,\n                                        std::unique_ptr<MIRParser> &MIR,\n                                        const TargetMachine &TM,\n                                        StringRef MIRCode, const char *FuncName,\n                                        MachineModuleInfo &MMI) {\n  SMDiagnostic Diagnostic;\n  std::unique_ptr<MemoryBuffer> MBuffer = MemoryBuffer::getMemBuffer(MIRCode);\n  MIR = createMIRParser(std::move(MBuffer), Context);\n  if (!MIR)\n    return nullptr;\n\n  std::unique_ptr<Module> M = MIR->parseIRModule();\n  if (!M)\n    return nullptr;\n\n  M->setDataLayout(TM.createDataLayout());\n\n  if (MIR->parseMachineFunctions(*M, MMI))\n    return nullptr;\n\n  return M;\n}\nstatic std::pair<std::unique_ptr<Module>, std::unique_ptr<MachineModuleInfo>>\ncreateDummyModule(LLVMContext &Context, const LLVMTargetMachine &TM,\n                  StringRef MIRString, const char *FuncName) {\n  std::unique_ptr<MIRParser> MIR;\n  auto MMI = std::make_unique<MachineModuleInfo>(&TM);\n  std::unique_ptr<Module> M =\n      parseMIR(Context, MIR, TM, MIRString, FuncName, *MMI);\n  return make_pair(std::move(M), std::move(MMI));\n}\n\nstatic MachineFunction *getMFFromMMI(const Module *M,\n                                     const MachineModuleInfo *MMI) {\n  Function *F = M->getFunction(\"func\");\n  auto *MF = MMI->getMachineFunction(*F);\n  return MF;\n}\n\nstatic void collectCopies(SmallVectorImpl<Register> &Copies,\n                          MachineFunction *MF) {\n  for (auto &MBB : *MF)\n    for (MachineInstr &MI : MBB) {\n      if (MI.getOpcode() == TargetOpcode::COPY)\n        Copies.push_back(MI.getOperand(0).getReg());\n    }\n}\n\nclass GISelMITest : public ::testing::Test {\nprotected:\n  GISelMITest() : ::testing::Test() {}\n\n  /// Prepare a target specific LLVMTargetMachine.\n  virtual std::unique_ptr<LLVMTargetMachine> createTargetMachine() const = 0;\n\n  /// Get the stub sample MIR test function.\n  virtual void getTargetTestModuleString(SmallString<512> &S,\n                                         StringRef MIRFunc) const = 0;\n\n  void setUp(StringRef ExtraAssembly = \"\") {\n    TM = createTargetMachine();\n    if (!TM)\n      return;\n\n    SmallString<512> MIRString;\n    getTargetTestModuleString(MIRString, ExtraAssembly);\n\n    ModuleMMIPair = createDummyModule(Context, *TM, MIRString, \"func\");\n    MF = getMFFromMMI(ModuleMMIPair.first.get(), ModuleMMIPair.second.get());\n    collectCopies(Copies, MF);\n    EntryMBB = &*MF->begin();\n    B.setMF(*MF);\n    MRI = &MF->getRegInfo();\n    B.setInsertPt(*EntryMBB, EntryMBB->end());\n  }\n\n  LLVMContext Context;\n  std::unique_ptr<LLVMTargetMachine> TM;\n  MachineFunction *MF;\n  std::pair<std::unique_ptr<Module>, std::unique_ptr<MachineModuleInfo>>\n      ModuleMMIPair;\n  SmallVector<Register, 4> Copies;\n  MachineBasicBlock *EntryMBB;\n  MachineIRBuilder B;\n  MachineRegisterInfo *MRI;\n};\n\nclass AArch64GISelMITest : public GISelMITest {\n  std::unique_ptr<LLVMTargetMachine> createTargetMachine() const override;\n  void getTargetTestModuleString(SmallString<512> &S,\n                                 StringRef MIRFunc) const override;\n};\n\nclass AMDGPUGISelMITest : public GISelMITest {\n  std::unique_ptr<LLVMTargetMachine> createTargetMachine() const override;\n  void getTargetTestModuleString(SmallString<512> &S,\n                                 StringRef MIRFunc) const override;\n};\n\n#define DefineLegalizerInfo(Name, SettingUpActionsBlock)                       \\\n  class Name##Info : public LegalizerInfo {                                    \\\n  public:                                                                      \\\n    Name##Info(const TargetSubtargetInfo &ST) {                                \\\n      using namespace TargetOpcode;                                            \\\n      const LLT s8 = LLT::scalar(8);                                           \\\n      (void)s8;                                                                \\\n      const LLT s16 = LLT::scalar(16);                                         \\\n      (void)s16;                                                               \\\n      const LLT s32 = LLT::scalar(32);                                         \\\n      (void)s32;                                                               \\\n      const LLT s64 = LLT::scalar(64);                                         \\\n      (void)s64;                                                               \\\n      const LLT s128 = LLT::scalar(128);                                       \\\n      (void)s128;                                                              \\\n      do                                                                       \\\n        SettingUpActionsBlock while (0);                                       \\\n      computeTables();                                                         \\\n      verify(*ST.getInstrInfo());                                              \\\n    }                                                                          \\\n  };\n\nstatic inline bool CheckMachineFunction(const MachineFunction &MF,\n                                        StringRef CheckStr) {\n  SmallString<512> Msg;\n  raw_svector_ostream OS(Msg);\n  MF.print(OS);\n  auto OutputBuf = MemoryBuffer::getMemBuffer(Msg, \"Output\", false);\n  auto CheckBuf = MemoryBuffer::getMemBuffer(CheckStr, \"\");\n  SmallString<4096> CheckFileBuffer;\n  FileCheckRequest Req;\n  FileCheck FC(Req);\n  StringRef CheckFileText =\n      FC.CanonicalizeFile(*CheckBuf.get(), CheckFileBuffer);\n  SourceMgr SM;\n  SM.AddNewSourceBuffer(MemoryBuffer::getMemBuffer(CheckFileText, \"CheckFile\"),\n                        SMLoc());\n  Regex PrefixRE = FC.buildCheckPrefixRegex();\n  if (FC.readCheckFile(SM, CheckFileText, PrefixRE))\n    return false;\n\n  auto OutBuffer = OutputBuf->getBuffer();\n  SM.AddNewSourceBuffer(std::move(OutputBuf), SMLoc());\n  return FC.checkInput(SM, OutBuffer);\n}\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 7, "file": 32, "line": 191}, "message": "'CCState' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/CallingConvLower.h", "reportHash": "bcbfddec78a005af2c3946770189f43a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 32, "line": 234}, "message": "'ByValInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/CallingConvLower.h", "reportHash": "61414fd87eff4315081472ac5eb751cb", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 33, "line": 220}, "message": "'GISelCSEAnalysisWrapperPass' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CSEInfo.h", "reportHash": "31116cb419ba9a712dcdd16686c21071", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 34, "line": 47}, "message": "'BaseArgInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CallLowering.h", "reportHash": "f3524d38c2a70b6b2348a85dacc8abdc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 34, "line": 208}, "message": "'IncomingValueHandler' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CallLowering.h", "reportHash": "2b83467e61278d53043839781cd46a56", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 34, "line": 222}, "message": "'OutgoingValueHandler' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/CallLowering.h", "reportHash": "e6540622c24a82250f27adeb3d11a082", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 35, "line": 65}, "message": "'GISelObserverWrapper' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/GISelChangeObserver.h", "reportHash": "0074dda0ad93fc8828a6bb41491755cf", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 36, "line": 338}, "message": "'LegalizeRule' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "reportHash": "5304ec3ad01161a71a73d7bedd3f784d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 36, "line": 363}, "message": "'LegalizeRuleSet' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "reportHash": "98c2858241b7a08a371d120dec0d0abf", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 36, "line": 892}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "reportHash": "45a828742d35bc3291e3f00d3580ec23", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 36, "line": 976}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "reportHash": "45a828742d35bc3291e3f00d3580ec23", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 36, "line": 994}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/LegalizerInfo.h", "reportHash": "45a828742d35bc3291e3f00d3580ec23", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 37, "line": 37}, "message": "'MachineIRBuilderState' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "reportHash": "af1304005e1f88750bd2c380139028aa", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 37, "line": 58}, "message": "'DstOp' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "reportHash": "69f2e71c18398b47a94385eb73919bc0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 37, "line": 59}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "reportHash": "d0f1d216bff30a0f7d80fb5872c09d32", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 37, "line": 119}, "message": "'SrcOp' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "reportHash": "6b7aa75c9c59f8623d7c9d9e803e6d38", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 37, "line": 120}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "reportHash": "d0f1d216bff30a0f7d80fb5872c09d32", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 37, "line": 208}, "message": "'FlagsOp' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/MachineIRBuilder.h", "reportHash": "27b552d7a4db32f1c2037acec5483f8d", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 38, "line": 28}, "message": "'RegisterBank' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/RegisterBank.h", "reportHash": "49a79b1d6d913c066ce3368838d7a83a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 39, "line": 141}, "message": "'ValueAndVReg' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/GlobalISel/Utils.h", "reportHash": "c010a159b5cbcd9565dc6b16a1ae9b8b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 40, "line": 30}, "message": "'LiveRegUnits' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/LiveRegUnits.h", "reportHash": "c60a34e51fbf8e7ff768ff6f799aa2db", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 42, "line": 107}, "message": "'MachineFrameInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineFrameInfo.h", "reportHash": "be1715b96269f39980b4cf9b1b560a73", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 42, "line": 124}, "message": "'StackObject' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineFrameInfo.h", "reportHash": "2b892a169e91c3f9879f765a12336eef", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 45, "line": 166}, "message": "'MIBundleOperands' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineInstrBundle.h", "reportHash": "500acbb955269b6309fd1d2e2e378456", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 45, "line": 185}, "message": "'ConstMIBundleOperands' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineInstrBundle.h", "reportHash": "6b4ea8da6c19d91ed118842d5b069078", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 47, "line": 158}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 47, "line": 158}, "message": "'MachineModuleInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "reportHash": "528ceee13865b333b0311b69c4dd5951", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 47, "line": 272}, "message": "'MachineModuleInfoWrapperPass' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/MachineModuleInfo.h", "reportHash": "7ad8451910e46b35797eafc4ee5cc266", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 50, "line": 30}, "message": "'RegisterClassInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/RegisterClassInfo.h", "reportHash": "725eceb2e71475d85c100afbfcc03fe0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 51, "line": 314}, "message": "'DAGNodeDeletedListener' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAG.h", "reportHash": "c82e44d2a360a272ad3668bdaa821a59", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 455}, "message": "'SDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "c00c441a140a9d3aeb792cafc3923e41", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 52, "line": 476}, "message": "'SDNodeBitfields' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "5da9623461f4e143e893f4a67e0fe704", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 52, "line": 548}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "f053d880362da06a002192eb4bca03df", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 52, "line": 908}, "message": "'value_op_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "cfff60e4b4dcc08fd7c5945283d847ff", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 1078}, "message": "'SDLoc' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "e8a57778cbfb456e743569152dc9a111", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 1246}, "message": "'MemSDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "99f2786e712f5c9fb15ef60f6077ffbf", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 52, "line": 1280}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "e3b28bad1da593b03fe47708cbfd4ec8", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 1811}, "message": "'ConstantPoolSDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "ef610ec5736b45e03e54eb0fed3724e0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 52, "line": 1814}, "message": "'' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "f053d880362da06a002192eb4bca03df", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 2217}, "message": "'LSBaseSDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "20b9046a126af9027490a3659340bc26", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 2309}, "message": "'MaskedLoadStoreSDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "e19ba1b19824e484a69ddc80bb1d975f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 2416}, "message": "'MaskedGatherScatterSDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "4bf03d3635cb263e3e5270ef443f5a74", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 52, "line": 2576}, "message": "'AssertAlignSDNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/SelectionDAGNodes.h", "reportHash": "4d26e1c1413c95f871348db9a3abb606", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 53, "line": 27}, "message": "'ArgFlagsTy' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetCallingConv.h", "reportHash": "5f40770d30cae51bf31a3c6126fc63da", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 54, "line": 68}, "message": "'DestSourcePair' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetInstrInfo.h", "reportHash": "214c00a03d6087ae58a68018548d71df", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 55, "line": 270}, "message": "'ArgListEntry' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "reportHash": "52414a6804e8bba67798050fe00bcce1", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 55, "line": 891}, "message": "'ValueTypeActionImpl' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "reportHash": "e1fe7da295205674b344a99e50a4f8bd", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 55, "line": 978}, "message": "'IntrinsicInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "reportHash": "1203b9ba7be4e6379bff989ffdb6bf17", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 55, "line": 4129}, "message": "'AsmOperandInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h", "reportHash": "786dc639e7cdc8c50f7d073bac1b8379", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 61, "line": 28}, "message": "'ConstantFolder' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/ConstantFolder.h", "reportHash": "1eb5d2619715d087ca0402b2b0bcc9f3", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 64, "line": 63}, "message": "'DITypeRefArray' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/DebugInfoMetadata.h", "reportHash": "4c095dfa3c70f36f7b60495d7664bd93", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 66, "line": 118}, "message": "'ConstraintInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InlineAsm.h", "reportHash": "7b2a4b80768383ce0cd3832006f07610", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 68, "line": 174}, "message": "'LoadInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "6ee4891f5bd7da6d00a6cb7e71c40e85", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 77, "line": 21}, "message": "'AsmToken' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCAsmMacro.h", "reportHash": "2dca453e97cfc6b92f1dd233baed142c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 78, "line": 218}, "message": "'ELFSectionKey' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCContext.h", "reportHash": "5f1cc772838a06ec30fd64af87737d49", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 78, "line": 240}, "message": "'COFFSectionKey' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCContext.h", "reportHash": "9cd1257fbbaf99a9eb035b8c915152ba", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 78, "line": 262}, "message": "'WasmSectionKey' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCContext.h", "reportHash": "ae3002c51f435695cf6e1c5fb5e8081b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 78, "line": 281}, "message": "'XCOFFSectionKey' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCContext.h", "reportHash": "ecb000fbe3e524db1a8d4a3546a1a447", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 12, "file": 78, "line": 354}, "message": "'ELFEntrySizeKey' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCContext.h", "reportHash": "16f04705ec03343790129a46806becf5", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 25}, "message": "'MCInstrInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCInstrInfo.h", "reportHash": "4ce89f56062d508dc690edd5219f8e0b", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 84, "line": 699}, "message": "'MCRegUnitMaskIterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCRegisterInfo.h", "reportHash": "6520512c31d077be9275f7f617077295", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 84, "line": 773}, "message": "'MCRegAliasIterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/MC/MCRegisterInfo.h", "reportHash": "f6dbc574a3111b21d03406d9aee008ac", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 110, "line": 104}, "message": "'GISelMITest' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/GISelMITest.h", "reportHash": "33959d97e1fa021b25d8f37084763b1e", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 110, "line": 143}, "message": "'AArch64GISelMITest' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/unittests/CodeGen/GlobalISel/GISelMITest.h", "reportHash": "c531705a336c6d7083c92fc3ec31c222", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
