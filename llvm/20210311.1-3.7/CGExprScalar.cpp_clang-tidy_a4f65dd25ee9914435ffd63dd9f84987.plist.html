<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"8": {"id": 8, "path": "/home/vsts/work/1/llvm-project/clang/include/clang/AST/Expr.h", "content": "//===--- Expr.h - Classes for representing expressions ----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n//  This file defines the Expr interface and subclasses.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_AST_EXPR_H\n#define LLVM_CLANG_AST_EXPR_H\n\n#include \"clang/AST/APValue.h\"\n#include \"clang/AST/ASTVector.h\"\n#include \"clang/AST/ComputeDependence.h\"\n#include \"clang/AST/Decl.h\"\n#include \"clang/AST/DeclAccessPair.h\"\n#include \"clang/AST/DependenceFlags.h\"\n#include \"clang/AST/OperationKinds.h\"\n#include \"clang/AST/Stmt.h\"\n#include \"clang/AST/TemplateBase.h\"\n#include \"clang/AST/Type.h\"\n#include \"clang/Basic/CharInfo.h\"\n#include \"clang/Basic/LangOptions.h\"\n#include \"clang/Basic/SyncScope.h\"\n#include \"clang/Basic/TypeTraits.h\"\n#include \"llvm/ADT/APFloat.h\"\n#include \"llvm/ADT/APSInt.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/TrailingObjects.h\"\n\nnamespace clang {\n  class APValue;\n  class ASTContext;\n  class BlockDecl;\n  class CXXBaseSpecifier;\n  class CXXMemberCallExpr;\n  class CXXOperatorCallExpr;\n  class CastExpr;\n  class Decl;\n  class IdentifierInfo;\n  class MaterializeTemporaryExpr;\n  class NamedDecl;\n  class ObjCPropertyRefExpr;\n  class OpaqueValueExpr;\n  class ParmVarDecl;\n  class StringLiteral;\n  class TargetInfo;\n  class ValueDecl;\n\n/// A simple array of base specifiers.\ntypedef SmallVector<CXXBaseSpecifier*, 4> CXXCastPath;\n\n/// An adjustment to be made to the temporary created when emitting a\n/// reference binding, which accesses a particular subobject of that temporary.\nstruct SubobjectAdjustment {\n  enum {\n    DerivedToBaseAdjustment,\n    FieldAdjustment,\n    MemberPointerAdjustment\n  } Kind;\n\n  struct DTB {\n    const CastExpr *BasePath;\n    const CXXRecordDecl *DerivedClass;\n  };\n\n  struct P {\n    const MemberPointerType *MPT;\n    Expr *RHS;\n  };\n\n  union {\n    struct DTB DerivedToBase;\n    FieldDecl *Field;\n    struct P Ptr;\n  };\n\n  SubobjectAdjustment(const CastExpr *BasePath,\n                      const CXXRecordDecl *DerivedClass)\n    : Kind(DerivedToBaseAdjustment) {\n    DerivedToBase.BasePath = BasePath;\n    DerivedToBase.DerivedClass = DerivedClass;\n  }\n\n  SubobjectAdjustment(FieldDecl *Field)\n    : Kind(FieldAdjustment) {\n    this->Field = Field;\n  }\n\n  SubobjectAdjustment(const MemberPointerType *MPT, Expr *RHS)\n    : Kind(MemberPointerAdjustment) {\n    this->Ptr.MPT = MPT;\n    this->Ptr.RHS = RHS;\n  }\n};\n\n/// This represents one expression.  Note that Expr's are subclasses of Stmt.\n/// This allows an expression to be transparently used any place a Stmt is\n/// required.\nclass Expr : public ValueStmt {\n  QualType TR;\n\npublic:\n  Expr() = delete;\n  Expr(const Expr&) = delete;\n  Expr(Expr &&) = delete;\n  Expr &operator=(const Expr&) = delete;\n  Expr &operator=(Expr&&) = delete;\n\nprotected:\n  Expr(StmtClass SC, QualType T, ExprValueKind VK, ExprObjectKind OK)\n      : ValueStmt(SC) {\n    ExprBits.Dependent = 0;\n    ExprBits.ValueKind = VK;\n    ExprBits.ObjectKind = OK;\n    assert(ExprBits.ObjectKind == OK && \"truncated kind\");\n    setType(T);\n  }\n\n  /// Construct an empty expression.\n  explicit Expr(StmtClass SC, EmptyShell) : ValueStmt(SC) { }\n\n  /// Each concrete expr subclass is expected to compute its dependence and call\n  /// this in the constructor.\n  void setDependence(ExprDependence Deps) {\n    ExprBits.Dependent = static_cast<unsigned>(Deps);\n  }\n  friend class ASTImporter; // Sets dependence dircetly.\n  friend class ASTStmtReader; // Sets dependence dircetly.\n\npublic:\n  QualType getType() const { return TR; }\n  void setType(QualType t) {\n    // In C++, the type of an expression is always adjusted so that it\n    // will not have reference type (C++ [expr]p6). Use\n    // QualType::getNonReferenceType() to retrieve the non-reference\n    // type. Additionally, inspect Expr::isLvalue to determine whether\n    // an expression that is adjusted in this manner should be\n    // considered an lvalue.\n    assert((t.isNull() || !t->isReferenceType()) &&\n           \"Expressions can't have reference type\");\n\n    TR = t;\n  }\n\n  ExprDependence getDependence() const {\n    return static_cast<ExprDependence>(ExprBits.Dependent);\n  }\n\n  /// Determines whether the value of this expression depends on\n  ///   - a template parameter (C++ [temp.dep.constexpr])\n  ///   - or an error, whose resolution is unknown\n  ///\n  /// For example, the array bound of \"Chars\" in the following example is\n  /// value-dependent.\n  /// @code\n  /// template<int Size, char (&Chars)[Size]> struct meta_string;\n  /// @endcode\n  bool isValueDependent() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Value);\n  }\n\n  /// Determines whether the type of this expression depends on\n  ///   - a template paramter (C++ [temp.dep.expr], which means that its type\n  ///     could change from one template instantiation to the next)\n  ///   - or an error\n  ///\n  /// For example, the expressions \"x\" and \"x + y\" are type-dependent in\n  /// the following code, but \"y\" is not type-dependent:\n  /// @code\n  /// template<typename T>\n  /// void add(T x, int y) {\n  ///   x + y;\n  /// }\n  /// @endcode\n  bool isTypeDependent() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Type);\n  }\n\n  /// Whether this expression is instantiation-dependent, meaning that\n  /// it depends in some way on\n  ///    - a template parameter (even if neither its type nor (constant) value\n  ///      can change due to the template instantiation)\n  ///    - or an error\n  ///\n  /// In the following example, the expression \\c sizeof(sizeof(T() + T())) is\n  /// instantiation-dependent (since it involves a template parameter \\c T), but\n  /// is neither type- nor value-dependent, since the type of the inner\n  /// \\c sizeof is known (\\c std::size_t) and therefore the size of the outer\n  /// \\c sizeof is known.\n  ///\n  /// \\code\n  /// template<typename T>\n  /// void f(T x, T y) {\n  ///   sizeof(sizeof(T() + T());\n  /// }\n  /// \\endcode\n  ///\n  /// \\code\n  /// void func(int) {\n  ///   func(); // the expression is instantiation-dependent, because it depends\n  ///           // on an error.\n  /// }\n  /// \\endcode\n  bool isInstantiationDependent() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Instantiation);\n  }\n\n  /// Whether this expression contains an unexpanded parameter\n  /// pack (for C++11 variadic templates).\n  ///\n  /// Given the following function template:\n  ///\n  /// \\code\n  /// template<typename F, typename ...Types>\n  /// void forward(const F &f, Types &&...args) {\n  ///   f(static_cast<Types&&>(args)...);\n  /// }\n  /// \\endcode\n  ///\n  /// The expressions \\c args and \\c static_cast<Types&&>(args) both\n  /// contain parameter packs.\n  bool containsUnexpandedParameterPack() const {\n    return static_cast<bool>(getDependence() & ExprDependence::UnexpandedPack);\n  }\n\n  /// Whether this expression contains subexpressions which had errors, e.g. a\n  /// TypoExpr.\n  bool containsErrors() const {\n    return static_cast<bool>(getDependence() & ExprDependence::Error);\n  }\n\n  /// getExprLoc - Return the preferred location for the arrow when diagnosing\n  /// a problem with a generic expression.\n  SourceLocation getExprLoc() const LLVM_READONLY;\n\n  /// Determine whether an lvalue-to-rvalue conversion should implicitly be\n  /// applied to this expression if it appears as a discarded-value expression\n  /// in C++11 onwards. This applies to certain forms of volatile glvalues.\n  bool isReadIfDiscardedInCPlusPlus11() const;\n\n  /// isUnusedResultAWarning - Return true if this immediate expression should\n  /// be warned about if the result is unused.  If so, fill in expr, location,\n  /// and ranges with expr to warn on and source locations/ranges appropriate\n  /// for a warning.\n  bool isUnusedResultAWarning(const Expr *&WarnExpr, SourceLocation &Loc,\n                              SourceRange &R1, SourceRange &R2,\n                              ASTContext &Ctx) const;\n\n  /// isLValue - True if this expression is an \"l-value\" according to\n  /// the rules of the current language.  C and C++ give somewhat\n  /// different rules for this concept, but in general, the result of\n  /// an l-value expression identifies a specific object whereas the\n  /// result of an r-value expression is a value detached from any\n  /// specific storage.\n  ///\n  /// C++11 divides the concept of \"r-value\" into pure r-values\n  /// (\"pr-values\") and so-called expiring values (\"x-values\"), which\n  /// identify specific objects that can be safely cannibalized for\n  /// their resources.  This is an unfortunate abuse of terminology on\n  /// the part of the C++ committee.  In Clang, when we say \"r-value\",\n  /// we generally mean a pr-value.\n  bool isLValue() const { return getValueKind() == VK_LValue; }\n  bool isRValue() const { return getValueKind() == VK_RValue; }\n  bool isXValue() const { return getValueKind() == VK_XValue; }\n  bool isGLValue() const { return getValueKind() != VK_RValue; }\n\n  enum LValueClassification {\n    LV_Valid,\n    LV_NotObjectType,\n    LV_IncompleteVoidType,\n    LV_DuplicateVectorComponents,\n    LV_InvalidExpression,\n    LV_InvalidMessageExpression,\n    LV_MemberFunction,\n    LV_SubObjCPropertySetting,\n    LV_ClassTemporary,\n    LV_ArrayTemporary\n  };\n  /// Reasons why an expression might not be an l-value.\n  LValueClassification ClassifyLValue(ASTContext &Ctx) const;\n\n  enum isModifiableLvalueResult {\n    MLV_Valid,\n    MLV_NotObjectType,\n    MLV_IncompleteVoidType,\n    MLV_DuplicateVectorComponents,\n    MLV_InvalidExpression,\n    MLV_LValueCast,           // Specialized form of MLV_InvalidExpression.\n    MLV_IncompleteType,\n    MLV_ConstQualified,\n    MLV_ConstQualifiedField,\n    MLV_ConstAddrSpace,\n    MLV_ArrayType,\n    MLV_NoSetterProperty,\n    MLV_MemberFunction,\n    MLV_SubObjCPropertySetting,\n    MLV_InvalidMessageExpression,\n    MLV_ClassTemporary,\n    MLV_ArrayTemporary\n  };\n  /// isModifiableLvalue - C99 6.3.2.1: an lvalue that does not have array type,\n  /// does not have an incomplete type, does not have a const-qualified type,\n  /// and if it is a structure or union, does not have any member (including,\n  /// recursively, any member or element of all contained aggregates or unions)\n  /// with a const-qualified type.\n  ///\n  /// \\param Loc [in,out] - A source location which *may* be filled\n  /// in with the location of the expression making this a\n  /// non-modifiable lvalue, if specified.\n  isModifiableLvalueResult\n  isModifiableLvalue(ASTContext &Ctx, SourceLocation *Loc = nullptr) const;\n\n  /// The return type of classify(). Represents the C++11 expression\n  ///        taxonomy.\n  class Classification {\n  public:\n    /// The various classification results. Most of these mean prvalue.\n    enum Kinds {\n      CL_LValue,\n      CL_XValue,\n      CL_Function, // Functions cannot be lvalues in C.\n      CL_Void, // Void cannot be an lvalue in C.\n      CL_AddressableVoid, // Void expression whose address can be taken in C.\n      CL_DuplicateVectorComponents, // A vector shuffle with dupes.\n      CL_MemberFunction, // An expression referring to a member function\n      CL_SubObjCPropertySetting,\n      CL_ClassTemporary, // A temporary of class type, or subobject thereof.\n      CL_ArrayTemporary, // A temporary of array type.\n      CL_ObjCMessageRValue, // ObjC message is an rvalue\n      CL_PRValue // A prvalue for any other reason, of any other type\n    };\n    /// The results of modification testing.\n    enum ModifiableType {\n      CM_Untested, // testModifiable was false.\n      CM_Modifiable,\n      CM_RValue, // Not modifiable because it's an rvalue\n      CM_Function, // Not modifiable because it's a function; C++ only\n      CM_LValueCast, // Same as CM_RValue, but indicates GCC cast-as-lvalue ext\n      CM_NoSetterProperty,// Implicit assignment to ObjC property without setter\n      CM_ConstQualified,\n      CM_ConstQualifiedField,\n      CM_ConstAddrSpace,\n      CM_ArrayType,\n      CM_IncompleteType\n    };\n\n  private:\n    friend class Expr;\n\n    unsigned short Kind;\n    unsigned short Modifiable;\n\n    explicit Classification(Kinds k, ModifiableType m)\n      : Kind(k), Modifiable(m)\n    {}\n\n  public:\n    Classification() {}\n\n    Kinds getKind() const { return static_cast<Kinds>(Kind); }\n    ModifiableType getModifiable() const {\n      assert(Modifiable != CM_Untested && \"Did not test for modifiability.\");\n      return static_cast<ModifiableType>(Modifiable);\n    }\n    bool isLValue() const { return Kind == CL_LValue; }\n    bool isXValue() const { return Kind == CL_XValue; }\n    bool isGLValue() const { return Kind <= CL_XValue; }\n    bool isPRValue() const { return Kind >= CL_Function; }\n    bool isRValue() const { return Kind >= CL_XValue; }\n    bool isModifiable() const { return getModifiable() == CM_Modifiable; }\n\n    /// Create a simple, modifiably lvalue\n    static Classification makeSimpleLValue() {\n      return Classification(CL_LValue, CM_Modifiable);\n    }\n\n  };\n  /// Classify - Classify this expression according to the C++11\n  ///        expression taxonomy.\n  ///\n  /// C++11 defines ([basic.lval]) a new taxonomy of expressions to replace the\n  /// old lvalue vs rvalue. This function determines the type of expression this\n  /// is. There are three expression types:\n  /// - lvalues are classical lvalues as in C++03.\n  /// - prvalues are equivalent to rvalues in C++03.\n  /// - xvalues are expressions yielding unnamed rvalue references, e.g. a\n  ///   function returning an rvalue reference.\n  /// lvalues and xvalues are collectively referred to as glvalues, while\n  /// prvalues and xvalues together form rvalues.\n  Classification Classify(ASTContext &Ctx) const {\n    return ClassifyImpl(Ctx, nullptr);\n  }\n\n  /// ClassifyModifiable - Classify this expression according to the\n  ///        C++11 expression taxonomy, and see if it is valid on the left side\n  ///        of an assignment.\n  ///\n  /// This function extends classify in that it also tests whether the\n  /// expression is modifiable (C99 6.3.2.1p1).\n  /// \\param Loc A source location that might be filled with a relevant location\n  ///            if the expression is not modifiable.\n  Classification ClassifyModifiable(ASTContext &Ctx, SourceLocation &Loc) const{\n    return ClassifyImpl(Ctx, &Loc);\n  }\n\n  /// Returns the set of floating point options that apply to this expression.\n  /// Only meaningful for operations on floating point values.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const;\n\n  /// getValueKindForType - Given a formal return or parameter type,\n  /// give its value kind.\n  static ExprValueKind getValueKindForType(QualType T) {\n    if (const ReferenceType *RT = T->getAs<ReferenceType>())\n      return (isa<LValueReferenceType>(RT)\n                ? VK_LValue\n                : (RT->getPointeeType()->isFunctionType()\n                     ? VK_LValue : VK_XValue));\n    return VK_RValue;\n  }\n\n  /// getValueKind - The value kind that this expression produces.\n  ExprValueKind getValueKind() const {\n    return static_cast<ExprValueKind>(ExprBits.ValueKind);\n  }\n\n  /// getObjectKind - The object kind that this expression produces.\n  /// Object kinds are meaningful only for expressions that yield an\n  /// l-value or x-value.\n  ExprObjectKind getObjectKind() const {\n    return static_cast<ExprObjectKind>(ExprBits.ObjectKind);\n  }\n\n  bool isOrdinaryOrBitFieldObject() const {\n    ExprObjectKind OK = getObjectKind();\n    return (OK == OK_Ordinary || OK == OK_BitField);\n  }\n\n  /// setValueKind - Set the value kind produced by this expression.\n  void setValueKind(ExprValueKind Cat) { ExprBits.ValueKind = Cat; }\n\n  /// setObjectKind - Set the object kind produced by this expression.\n  void setObjectKind(ExprObjectKind Cat) { ExprBits.ObjectKind = Cat; }\n\nprivate:\n  Classification ClassifyImpl(ASTContext &Ctx, SourceLocation *Loc) const;\n\npublic:\n\n  /// Returns true if this expression is a gl-value that\n  /// potentially refers to a bit-field.\n  ///\n  /// In C++, whether a gl-value refers to a bitfield is essentially\n  /// an aspect of the value-kind type system.\n  bool refersToBitField() const { return getObjectKind() == OK_BitField; }\n\n  /// If this expression refers to a bit-field, retrieve the\n  /// declaration of that bit-field.\n  ///\n  /// Note that this returns a non-null pointer in subtly different\n  /// places than refersToBitField returns true.  In particular, this can\n  /// return a non-null pointer even for r-values loaded from\n  /// bit-fields, but it will return null for a conditional bit-field.\n  FieldDecl *getSourceBitField();\n\n  const FieldDecl *getSourceBitField() const {\n    return const_cast<Expr*>(this)->getSourceBitField();\n  }\n\n  Decl *getReferencedDeclOfCallee();\n  const Decl *getReferencedDeclOfCallee() const {\n    return const_cast<Expr*>(this)->getReferencedDeclOfCallee();\n  }\n\n  /// If this expression is an l-value for an Objective C\n  /// property, find the underlying property reference expression.\n  const ObjCPropertyRefExpr *getObjCProperty() const;\n\n  /// Check if this expression is the ObjC 'self' implicit parameter.\n  bool isObjCSelfExpr() const;\n\n  /// Returns whether this expression refers to a vector element.\n  bool refersToVectorElement() const;\n\n  /// Returns whether this expression refers to a matrix element.\n  bool refersToMatrixElement() const {\n    return getObjectKind() == OK_MatrixComponent;\n  }\n\n  /// Returns whether this expression refers to a global register\n  /// variable.\n  bool refersToGlobalRegisterVar() const;\n\n  /// Returns whether this expression has a placeholder type.\n  bool hasPlaceholderType() const {\n    return getType()->isPlaceholderType();\n  }\n\n  /// Returns whether this expression has a specific placeholder type.\n  bool hasPlaceholderType(BuiltinType::Kind K) const {\n    assert(BuiltinType::isPlaceholderTypeKind(K));\n    if (const BuiltinType *BT = dyn_cast<BuiltinType>(getType()))\n      return BT->getKind() == K;\n    return false;\n  }\n\n  /// isKnownToHaveBooleanValue - Return true if this is an integer expression\n  /// that is known to return 0 or 1.  This happens for _Bool/bool expressions\n  /// but also int expressions which are produced by things like comparisons in\n  /// C.\n  ///\n  /// \\param Semantic If true, only return true for expressions that are known\n  /// to be semantically boolean, which might not be true even for expressions\n  /// that are known to evaluate to 0/1. For instance, reading an unsigned\n  /// bit-field with width '1' will evaluate to 0/1, but doesn't necessarily\n  /// semantically correspond to a bool.\n  bool isKnownToHaveBooleanValue(bool Semantic = true) const;\n\n  /// isIntegerConstantExpr - Return the value if this expression is a valid\n  /// integer constant expression.  If not a valid i-c-e, return None and fill\n  /// in Loc (if specified) with the location of the invalid expression.\n  ///\n  /// Note: This does not perform the implicit conversions required by C++11\n  /// [expr.const]p5.\n  Optional<llvm::APSInt> getIntegerConstantExpr(const ASTContext &Ctx,\n                                                SourceLocation *Loc = nullptr,\n                                                bool isEvaluated = true) const;\n  bool isIntegerConstantExpr(const ASTContext &Ctx,\n                             SourceLocation *Loc = nullptr) const;\n\n  /// isCXX98IntegralConstantExpr - Return true if this expression is an\n  /// integral constant expression in C++98. Can only be used in C++.\n  bool isCXX98IntegralConstantExpr(const ASTContext &Ctx) const;\n\n  /// isCXX11ConstantExpr - Return true if this expression is a constant\n  /// expression in C++11. Can only be used in C++.\n  ///\n  /// Note: This does not perform the implicit conversions required by C++11\n  /// [expr.const]p5.\n  bool isCXX11ConstantExpr(const ASTContext &Ctx, APValue *Result = nullptr,\n                           SourceLocation *Loc = nullptr) const;\n\n  /// isPotentialConstantExpr - Return true if this function's definition\n  /// might be usable in a constant expression in C++11, if it were marked\n  /// constexpr. Return false if the function can never produce a constant\n  /// expression, along with diagnostics describing why not.\n  static bool isPotentialConstantExpr(const FunctionDecl *FD,\n                                      SmallVectorImpl<\n                                        PartialDiagnosticAt> &Diags);\n\n  /// isPotentialConstantExprUnevaluted - Return true if this expression might\n  /// be usable in a constant expression in C++11 in an unevaluated context, if\n  /// it were in function FD marked constexpr. Return false if the function can\n  /// never produce a constant expression, along with diagnostics describing\n  /// why not.\n  static bool isPotentialConstantExprUnevaluated(Expr *E,\n                                                 const FunctionDecl *FD,\n                                                 SmallVectorImpl<\n                                                   PartialDiagnosticAt> &Diags);\n\n  /// isConstantInitializer - Returns true if this expression can be emitted to\n  /// IR as a constant, and thus can be used as a constant initializer in C.\n  /// If this expression is not constant and Culprit is non-null,\n  /// it is used to store the address of first non constant expr.\n  bool isConstantInitializer(ASTContext &Ctx, bool ForRef,\n                             const Expr **Culprit = nullptr) const;\n\n  /// EvalStatus is a struct with detailed info about an evaluation in progress.\n  struct EvalStatus {\n    /// Whether the evaluated expression has side effects.\n    /// For example, (f() && 0) can be folded, but it still has side effects.\n    bool HasSideEffects;\n\n    /// Whether the evaluation hit undefined behavior.\n    /// For example, 1.0 / 0.0 can be folded to Inf, but has undefined behavior.\n    /// Likewise, INT_MAX + 1 can be folded to INT_MIN, but has UB.\n    bool HasUndefinedBehavior;\n\n    /// Diag - If this is non-null, it will be filled in with a stack of notes\n    /// indicating why evaluation failed (or why it failed to produce a constant\n    /// expression).\n    /// If the expression is unfoldable, the notes will indicate why it's not\n    /// foldable. If the expression is foldable, but not a constant expression,\n    /// the notes will describes why it isn't a constant expression. If the\n    /// expression *is* a constant expression, no notes will be produced.\n    SmallVectorImpl<PartialDiagnosticAt> *Diag;\n\n    EvalStatus()\n        : HasSideEffects(false), HasUndefinedBehavior(false), Diag(nullptr) {}\n\n    // hasSideEffects - Return true if the evaluated expression has\n    // side effects.\n    bool hasSideEffects() const {\n      return HasSideEffects;\n    }\n  };\n\n  /// EvalResult is a struct with detailed info about an evaluated expression.\n  struct EvalResult : EvalStatus {\n    /// Val - This is the value the expression can be folded to.\n    APValue Val;\n\n    // isGlobalLValue - Return true if the evaluated lvalue expression\n    // is global.\n    bool isGlobalLValue() const;\n  };\n\n  /// EvaluateAsRValue - Return true if this is a constant which we can fold to\n  /// an rvalue using any crazy technique (that has nothing to do with language\n  /// standards) that we want to, even if the expression has side-effects. If\n  /// this function returns true, it returns the folded constant in Result. If\n  /// the expression is a glvalue, an lvalue-to-rvalue conversion will be\n  /// applied.\n  bool EvaluateAsRValue(EvalResult &Result, const ASTContext &Ctx,\n                        bool InConstantContext = false) const;\n\n  /// EvaluateAsBooleanCondition - Return true if this is a constant\n  /// which we can fold and convert to a boolean condition using\n  /// any crazy technique that we want to, even if the expression has\n  /// side-effects.\n  bool EvaluateAsBooleanCondition(bool &Result, const ASTContext &Ctx,\n                                  bool InConstantContext = false) const;\n\n  enum SideEffectsKind {\n    SE_NoSideEffects,          ///< Strictly evaluate the expression.\n    SE_AllowUndefinedBehavior, ///< Allow UB that we can give a value, but not\n                               ///< arbitrary unmodeled side effects.\n    SE_AllowSideEffects        ///< Allow any unmodeled side effect.\n  };\n\n  /// EvaluateAsInt - Return true if this is a constant which we can fold and\n  /// convert to an integer, using any crazy technique that we want to.\n  bool EvaluateAsInt(EvalResult &Result, const ASTContext &Ctx,\n                     SideEffectsKind AllowSideEffects = SE_NoSideEffects,\n                     bool InConstantContext = false) const;\n\n  /// EvaluateAsFloat - Return true if this is a constant which we can fold and\n  /// convert to a floating point value, using any crazy technique that we\n  /// want to.\n  bool EvaluateAsFloat(llvm::APFloat &Result, const ASTContext &Ctx,\n                       SideEffectsKind AllowSideEffects = SE_NoSideEffects,\n                       bool InConstantContext = false) const;\n\n  /// EvaluateAsFloat - Return true if this is a constant which we can fold and\n  /// convert to a fixed point value.\n  bool EvaluateAsFixedPoint(EvalResult &Result, const ASTContext &Ctx,\n                            SideEffectsKind AllowSideEffects = SE_NoSideEffects,\n                            bool InConstantContext = false) const;\n\n  /// isEvaluatable - Call EvaluateAsRValue to see if this expression can be\n  /// constant folded without side-effects, but discard the result.\n  bool isEvaluatable(const ASTContext &Ctx,\n                     SideEffectsKind AllowSideEffects = SE_NoSideEffects) const;\n\n  /// HasSideEffects - This routine returns true for all those expressions\n  /// which have any effect other than producing a value. Example is a function\n  /// call, volatile variable read, or throwing an exception. If\n  /// IncludePossibleEffects is false, this call treats certain expressions with\n  /// potential side effects (such as function call-like expressions,\n  /// instantiation-dependent expressions, or invocations from a macro) as not\n  /// having side effects.\n  bool HasSideEffects(const ASTContext &Ctx,\n                      bool IncludePossibleEffects = true) const;\n\n  /// Determine whether this expression involves a call to any function\n  /// that is not trivial.\n  bool hasNonTrivialCall(const ASTContext &Ctx) const;\n\n  /// EvaluateKnownConstInt - Call EvaluateAsRValue and return the folded\n  /// integer. This must be called on an expression that constant folds to an\n  /// integer.\n  llvm::APSInt EvaluateKnownConstInt(\n      const ASTContext &Ctx,\n      SmallVectorImpl<PartialDiagnosticAt> *Diag = nullptr) const;\n\n  llvm::APSInt EvaluateKnownConstIntCheckOverflow(\n      const ASTContext &Ctx,\n      SmallVectorImpl<PartialDiagnosticAt> *Diag = nullptr) const;\n\n  void EvaluateForOverflow(const ASTContext &Ctx) const;\n\n  /// EvaluateAsLValue - Evaluate an expression to see if we can fold it to an\n  /// lvalue with link time known address, with no side-effects.\n  bool EvaluateAsLValue(EvalResult &Result, const ASTContext &Ctx,\n                        bool InConstantContext = false) const;\n\n  /// EvaluateAsInitializer - Evaluate an expression as if it were the\n  /// initializer of the given declaration. Returns true if the initializer\n  /// can be folded to a constant, and produces any relevant notes. In C++11,\n  /// notes will be produced if the expression is not a constant expression.\n  bool EvaluateAsInitializer(APValue &Result, const ASTContext &Ctx,\n                             const VarDecl *VD,\n                             SmallVectorImpl<PartialDiagnosticAt> &Notes,\n                             bool IsConstantInitializer) const;\n\n  /// EvaluateWithSubstitution - Evaluate an expression as if from the context\n  /// of a call to the given function with the given arguments, inside an\n  /// unevaluated context. Returns true if the expression could be folded to a\n  /// constant.\n  bool EvaluateWithSubstitution(APValue &Value, ASTContext &Ctx,\n                                const FunctionDecl *Callee,\n                                ArrayRef<const Expr*> Args,\n                                const Expr *This = nullptr) const;\n\n  enum class ConstantExprKind {\n    /// An integer constant expression (an array bound, enumerator, case value,\n    /// bit-field width, or similar) or similar.\n    Normal,\n    /// A non-class template argument. Such a value is only used for mangling,\n    /// not for code generation, so can refer to dllimported functions.\n    NonClassTemplateArgument,\n    /// A class template argument. Such a value is used for code generation.\n    ClassTemplateArgument,\n    /// An immediate invocation. The destruction of the end result of this\n    /// evaluation is not part of the evaluation, but all other temporaries\n    /// are destroyed.\n    ImmediateInvocation,\n  };\n\n  /// Evaluate an expression that is required to be a constant expression. Does\n  /// not check the syntactic constraints for C and C++98 constant expressions.\n  bool EvaluateAsConstantExpr(\n      EvalResult &Result, const ASTContext &Ctx,\n      ConstantExprKind Kind = ConstantExprKind::Normal) const;\n\n  /// If the current Expr is a pointer, this will try to statically\n  /// determine the number of bytes available where the pointer is pointing.\n  /// Returns true if all of the above holds and we were able to figure out the\n  /// size, false otherwise.\n  ///\n  /// \\param Type - How to evaluate the size of the Expr, as defined by the\n  /// \"type\" parameter of __builtin_object_size\n  bool tryEvaluateObjectSize(uint64_t &Result, ASTContext &Ctx,\n                             unsigned Type) const;\n\n  /// Enumeration used to describe the kind of Null pointer constant\n  /// returned from \\c isNullPointerConstant().\n  enum NullPointerConstantKind {\n    /// Expression is not a Null pointer constant.\n    NPCK_NotNull = 0,\n\n    /// Expression is a Null pointer constant built from a zero integer\n    /// expression that is not a simple, possibly parenthesized, zero literal.\n    /// C++ Core Issue 903 will classify these expressions as \"not pointers\"\n    /// once it is adopted.\n    /// http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_active.html#903\n    NPCK_ZeroExpression,\n\n    /// Expression is a Null pointer constant built from a literal zero.\n    NPCK_ZeroLiteral,\n\n    /// Expression is a C++11 nullptr.\n    NPCK_CXX11_nullptr,\n\n    /// Expression is a GNU-style __null constant.\n    NPCK_GNUNull\n  };\n\n  /// Enumeration used to describe how \\c isNullPointerConstant()\n  /// should cope with value-dependent expressions.\n  enum NullPointerConstantValueDependence {\n    /// Specifies that the expression should never be value-dependent.\n    NPC_NeverValueDependent = 0,\n\n    /// Specifies that a value-dependent expression of integral or\n    /// dependent type should be considered a null pointer constant.\n    NPC_ValueDependentIsNull,\n\n    /// Specifies that a value-dependent expression should be considered\n    /// to never be a null pointer constant.\n    NPC_ValueDependentIsNotNull\n  };\n\n  /// isNullPointerConstant - C99 6.3.2.3p3 - Test if this reduces down to\n  /// a Null pointer constant. The return value can further distinguish the\n  /// kind of NULL pointer constant that was detected.\n  NullPointerConstantKind isNullPointerConstant(\n      ASTContext &Ctx,\n      NullPointerConstantValueDependence NPC) const;\n\n  /// isOBJCGCCandidate - Return true if this expression may be used in a read/\n  /// write barrier.\n  bool isOBJCGCCandidate(ASTContext &Ctx) const;\n\n  /// Returns true if this expression is a bound member function.\n  bool isBoundMemberFunction(ASTContext &Ctx) const;\n\n  /// Given an expression of bound-member type, find the type\n  /// of the member.  Returns null if this is an *overloaded* bound\n  /// member expression.\n  static QualType findBoundMemberType(const Expr *expr);\n\n  /// Skip past any invisble AST nodes which might surround this\n  /// statement, such as ExprWithCleanups or ImplicitCastExpr nodes,\n  /// but also injected CXXMemberExpr and CXXConstructExpr which represent\n  /// implicit conversions.\n  Expr *IgnoreUnlessSpelledInSource();\n  const Expr *IgnoreUnlessSpelledInSource() const {\n    return const_cast<Expr *>(this)->IgnoreUnlessSpelledInSource();\n  }\n\n  /// Skip past any implicit casts which might surround this expression until\n  /// reaching a fixed point. Skips:\n  /// * ImplicitCastExpr\n  /// * FullExpr\n  Expr *IgnoreImpCasts() LLVM_READONLY;\n  const Expr *IgnoreImpCasts() const {\n    return const_cast<Expr *>(this)->IgnoreImpCasts();\n  }\n\n  /// Skip past any casts which might surround this expression until reaching\n  /// a fixed point. Skips:\n  /// * CastExpr\n  /// * FullExpr\n  /// * MaterializeTemporaryExpr\n  /// * SubstNonTypeTemplateParmExpr\n  Expr *IgnoreCasts() LLVM_READONLY;\n  const Expr *IgnoreCasts() const {\n    return const_cast<Expr *>(this)->IgnoreCasts();\n  }\n\n  /// Skip past any implicit AST nodes which might surround this expression\n  /// until reaching a fixed point. Skips:\n  /// * What IgnoreImpCasts() skips\n  /// * MaterializeTemporaryExpr\n  /// * CXXBindTemporaryExpr\n  Expr *IgnoreImplicit() LLVM_READONLY;\n  const Expr *IgnoreImplicit() const {\n    return const_cast<Expr *>(this)->IgnoreImplicit();\n  }\n\n  /// Skip past any implicit AST nodes which might surround this expression\n  /// until reaching a fixed point. Same as IgnoreImplicit, except that it\n  /// also skips over implicit calls to constructors and conversion functions.\n  ///\n  /// FIXME: Should IgnoreImplicit do this?\n  Expr *IgnoreImplicitAsWritten() LLVM_READONLY;\n  const Expr *IgnoreImplicitAsWritten() const {\n    return const_cast<Expr *>(this)->IgnoreImplicitAsWritten();\n  }\n\n  /// Skip past any parentheses which might surround this expression until\n  /// reaching a fixed point. Skips:\n  /// * ParenExpr\n  /// * UnaryOperator if `UO_Extension`\n  /// * GenericSelectionExpr if `!isResultDependent()`\n  /// * ChooseExpr if `!isConditionDependent()`\n  /// * ConstantExpr\n  Expr *IgnoreParens() LLVM_READONLY;\n  const Expr *IgnoreParens() const {\n    return const_cast<Expr *>(this)->IgnoreParens();\n  }\n\n  /// Skip past any parentheses and implicit casts which might surround this\n  /// expression until reaching a fixed point.\n  /// FIXME: IgnoreParenImpCasts really ought to be equivalent to\n  /// IgnoreParens() + IgnoreImpCasts() until reaching a fixed point. However\n  /// this is currently not the case. Instead IgnoreParenImpCasts() skips:\n  /// * What IgnoreParens() skips\n  /// * What IgnoreImpCasts() skips\n  /// * MaterializeTemporaryExpr\n  /// * SubstNonTypeTemplateParmExpr\n  Expr *IgnoreParenImpCasts() LLVM_READONLY;\n  const Expr *IgnoreParenImpCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenImpCasts();\n  }\n\n  /// Skip past any parentheses and casts which might surround this expression\n  /// until reaching a fixed point. Skips:\n  /// * What IgnoreParens() skips\n  /// * What IgnoreCasts() skips\n  Expr *IgnoreParenCasts() LLVM_READONLY;\n  const Expr *IgnoreParenCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenCasts();\n  }\n\n  /// Skip conversion operators. If this Expr is a call to a conversion\n  /// operator, return the argument.\n  Expr *IgnoreConversionOperatorSingleStep() LLVM_READONLY;\n  const Expr *IgnoreConversionOperatorSingleStep() const {\n    return const_cast<Expr *>(this)->IgnoreConversionOperatorSingleStep();\n  }\n\n  /// Skip past any parentheses and lvalue casts which might surround this\n  /// expression until reaching a fixed point. Skips:\n  /// * What IgnoreParens() skips\n  /// * What IgnoreCasts() skips, except that only lvalue-to-rvalue\n  ///   casts are skipped\n  /// FIXME: This is intended purely as a temporary workaround for code\n  /// that hasn't yet been rewritten to do the right thing about those\n  /// casts, and may disappear along with the last internal use.\n  Expr *IgnoreParenLValueCasts() LLVM_READONLY;\n  const Expr *IgnoreParenLValueCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenLValueCasts();\n  }\n\n  /// Skip past any parenthese and casts which do not change the value\n  /// (including ptr->int casts of the same size) until reaching a fixed point.\n  /// Skips:\n  /// * What IgnoreParens() skips\n  /// * CastExpr which do not change the value\n  /// * SubstNonTypeTemplateParmExpr\n  Expr *IgnoreParenNoopCasts(const ASTContext &Ctx) LLVM_READONLY;\n  const Expr *IgnoreParenNoopCasts(const ASTContext &Ctx) const {\n    return const_cast<Expr *>(this)->IgnoreParenNoopCasts(Ctx);\n  }\n\n  /// Skip past any parentheses and derived-to-base casts until reaching a\n  /// fixed point. Skips:\n  /// * What IgnoreParens() skips\n  /// * CastExpr which represent a derived-to-base cast (CK_DerivedToBase,\n  ///   CK_UncheckedDerivedToBase and CK_NoOp)\n  Expr *IgnoreParenBaseCasts() LLVM_READONLY;\n  const Expr *IgnoreParenBaseCasts() const {\n    return const_cast<Expr *>(this)->IgnoreParenBaseCasts();\n  }\n\n  /// Determine whether this expression is a default function argument.\n  ///\n  /// Default arguments are implicitly generated in the abstract syntax tree\n  /// by semantic analysis for function calls, object constructions, etc. in\n  /// C++. Default arguments are represented by \\c CXXDefaultArgExpr nodes;\n  /// this routine also looks through any implicit casts to determine whether\n  /// the expression is a default argument.\n  bool isDefaultArgument() const;\n\n  /// Determine whether the result of this expression is a\n  /// temporary object of the given class type.\n  bool isTemporaryObject(ASTContext &Ctx, const CXXRecordDecl *TempTy) const;\n\n  /// Whether this expression is an implicit reference to 'this' in C++.\n  bool isImplicitCXXThis() const;\n\n  static bool hasAnyTypeDependentArguments(ArrayRef<Expr *> Exprs);\n\n  /// For an expression of class type or pointer to class type,\n  /// return the most derived class decl the expression is known to refer to.\n  ///\n  /// If this expression is a cast, this method looks through it to find the\n  /// most derived decl that can be inferred from the expression.\n  /// This is valid because derived-to-base conversions have undefined\n  /// behavior if the object isn't dynamically of the derived type.\n  const CXXRecordDecl *getBestDynamicClassType() const;\n\n  /// Get the inner expression that determines the best dynamic class.\n  /// If this is a prvalue, we guarantee that it is of the most-derived type\n  /// for the object itself.\n  const Expr *getBestDynamicClassTypeExpr() const;\n\n  /// Walk outwards from an expression we want to bind a reference to and\n  /// find the expression whose lifetime needs to be extended. Record\n  /// the LHSs of comma expressions and adjustments needed along the path.\n  const Expr *skipRValueSubobjectAdjustments(\n      SmallVectorImpl<const Expr *> &CommaLHS,\n      SmallVectorImpl<SubobjectAdjustment> &Adjustments) const;\n  const Expr *skipRValueSubobjectAdjustments() const {\n    SmallVector<const Expr *, 8> CommaLHSs;\n    SmallVector<SubobjectAdjustment, 8> Adjustments;\n    return skipRValueSubobjectAdjustments(CommaLHSs, Adjustments);\n  }\n\n  /// Checks that the two Expr's will refer to the same value as a comparison\n  /// operand.  The caller must ensure that the values referenced by the Expr's\n  /// are not modified between E1 and E2 or the result my be invalid.\n  static bool isSameComparisonOperand(const Expr* E1, const Expr* E2);\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstExprConstant &&\n           T->getStmtClass() <= lastExprConstant;\n  }\n};\n// PointerLikeTypeTraits is specialized so it can be used with a forward-decl of\n// Expr. Verify that we got it right.\nstatic_assert(llvm::PointerLikeTypeTraits<Expr *>::NumLowBitsAvailable <=\n                  llvm::detail::ConstantLog2<alignof(Expr)>::value,\n              \"PointerLikeTypeTraits<Expr*> assumes too much alignment.\");\n\nusing ConstantExprKind = Expr::ConstantExprKind;\n\n//===----------------------------------------------------------------------===//\n// Wrapper Expressions.\n//===----------------------------------------------------------------------===//\n\n/// FullExpr - Represents a \"full-expression\" node.\nclass FullExpr : public Expr {\nprotected:\n Stmt *SubExpr;\n\n FullExpr(StmtClass SC, Expr *subexpr)\n     : Expr(SC, subexpr->getType(), subexpr->getValueKind(),\n            subexpr->getObjectKind()),\n       SubExpr(subexpr) {\n   setDependence(computeDependence(this));\n }\n  FullExpr(StmtClass SC, EmptyShell Empty)\n    : Expr(SC, Empty) {}\npublic:\n  const Expr *getSubExpr() const { return cast<Expr>(SubExpr); }\n  Expr *getSubExpr() { return cast<Expr>(SubExpr); }\n\n  /// As with any mutator of the AST, be very careful when modifying an\n  /// existing AST to preserve its invariants.\n  void setSubExpr(Expr *E) { SubExpr = E; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstFullExprConstant &&\n           T->getStmtClass() <= lastFullExprConstant;\n  }\n};\n\n/// ConstantExpr - An expression that occurs in a constant context and\n/// optionally the result of evaluating the expression.\nclass ConstantExpr final\n    : public FullExpr,\n      private llvm::TrailingObjects<ConstantExpr, APValue, uint64_t> {\n  static_assert(std::is_same<uint64_t, llvm::APInt::WordType>::value,\n                \"ConstantExpr assumes that llvm::APInt::WordType is uint64_t \"\n                \"for tail-allocated storage\");\n  friend TrailingObjects;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n\npublic:\n  /// Describes the kind of result that can be tail-allocated.\n  enum ResultStorageKind { RSK_None, RSK_Int64, RSK_APValue };\n\nprivate:\n  size_t numTrailingObjects(OverloadToken<APValue>) const {\n    return ConstantExprBits.ResultKind == ConstantExpr::RSK_APValue;\n  }\n  size_t numTrailingObjects(OverloadToken<uint64_t>) const {\n    return ConstantExprBits.ResultKind == ConstantExpr::RSK_Int64;\n  }\n\n  uint64_t &Int64Result() {\n    assert(ConstantExprBits.ResultKind == ConstantExpr::RSK_Int64 &&\n           \"invalid accessor\");\n    return *getTrailingObjects<uint64_t>();\n  }\n  const uint64_t &Int64Result() const {\n    return const_cast<ConstantExpr *>(this)->Int64Result();\n  }\n  APValue &APValueResult() {\n    assert(ConstantExprBits.ResultKind == ConstantExpr::RSK_APValue &&\n           \"invalid accessor\");\n    return *getTrailingObjects<APValue>();\n  }\n  APValue &APValueResult() const {\n    return const_cast<ConstantExpr *>(this)->APValueResult();\n  }\n\n  ConstantExpr(Expr *SubExpr, ResultStorageKind StorageKind,\n               bool IsImmediateInvocation);\n  ConstantExpr(EmptyShell Empty, ResultStorageKind StorageKind);\n\npublic:\n  static ConstantExpr *Create(const ASTContext &Context, Expr *E,\n                              const APValue &Result);\n  static ConstantExpr *Create(const ASTContext &Context, Expr *E,\n                              ResultStorageKind Storage = RSK_None,\n                              bool IsImmediateInvocation = false);\n  static ConstantExpr *CreateEmpty(const ASTContext &Context,\n                                   ResultStorageKind StorageKind);\n\n  static ResultStorageKind getStorageKind(const APValue &Value);\n  static ResultStorageKind getStorageKind(const Type *T,\n                                          const ASTContext &Context);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return SubExpr->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return SubExpr->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConstantExprClass;\n  }\n\n  void SetResult(APValue Value, const ASTContext &Context) {\n    MoveIntoResult(Value, Context);\n  }\n  void MoveIntoResult(APValue &Value, const ASTContext &Context);\n\n  APValue::ValueKind getResultAPValueKind() const {\n    return static_cast<APValue::ValueKind>(ConstantExprBits.APValueKind);\n  }\n  ResultStorageKind getResultStorageKind() const {\n    return static_cast<ResultStorageKind>(ConstantExprBits.ResultKind);\n  }\n  bool isImmediateInvocation() const {\n    return ConstantExprBits.IsImmediateInvocation;\n  }\n  bool hasAPValueResult() const {\n    return ConstantExprBits.APValueKind != APValue::None;\n  }\n  APValue getAPValueResult() const;\n  APValue &getResultAsAPValue() const { return APValueResult(); }\n  llvm::APSInt getResultAsAPSInt() const;\n  // Iterators\n  child_range children() { return child_range(&SubExpr, &SubExpr+1); }\n  const_child_range children() const {\n    return const_child_range(&SubExpr, &SubExpr + 1);\n  }\n};\n\n//===----------------------------------------------------------------------===//\n// Primary Expressions.\n//===----------------------------------------------------------------------===//\n\n/// OpaqueValueExpr - An expression referring to an opaque object of a\n/// fixed type and value class.  These don't correspond to concrete\n/// syntax; instead they're used to express operations (usually copy\n/// operations) on values whose source is generally obvious from\n/// context.\nclass OpaqueValueExpr : public Expr {\n  friend class ASTStmtReader;\n  Expr *SourceExpr;\n\npublic:\n  OpaqueValueExpr(SourceLocation Loc, QualType T, ExprValueKind VK,\n                  ExprObjectKind OK = OK_Ordinary, Expr *SourceExpr = nullptr)\n      : Expr(OpaqueValueExprClass, T, VK, OK), SourceExpr(SourceExpr) {\n    setIsUnique(false);\n    OpaqueValueExprBits.Loc = Loc;\n    setDependence(computeDependence(this));\n  }\n\n  /// Given an expression which invokes a copy constructor --- i.e.  a\n  /// CXXConstructExpr, possibly wrapped in an ExprWithCleanups ---\n  /// find the OpaqueValueExpr that's the source of the construction.\n  static const OpaqueValueExpr *findInCopyConstruct(const Expr *expr);\n\n  explicit OpaqueValueExpr(EmptyShell Empty)\n    : Expr(OpaqueValueExprClass, Empty) {}\n\n  /// Retrieve the location of this expression.\n  SourceLocation getLocation() const { return OpaqueValueExprBits.Loc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return SourceExpr ? SourceExpr->getBeginLoc() : getLocation();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return SourceExpr ? SourceExpr->getEndLoc() : getLocation();\n  }\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return SourceExpr ? SourceExpr->getExprLoc() : getLocation();\n  }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n\n  /// The source expression of an opaque value expression is the\n  /// expression which originally generated the value.  This is\n  /// provided as a convenience for analyses that don't wish to\n  /// precisely model the execution behavior of the program.\n  ///\n  /// The source expression is typically set when building the\n  /// expression which binds the opaque value expression in the first\n  /// place.\n  Expr *getSourceExpr() const { return SourceExpr; }\n\n  void setIsUnique(bool V) {\n    assert((!V || SourceExpr) &&\n           \"unique OVEs are expected to have source expressions\");\n    OpaqueValueExprBits.IsUnique = V;\n  }\n\n  bool isUnique() const { return OpaqueValueExprBits.IsUnique; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == OpaqueValueExprClass;\n  }\n};\n\n/// A reference to a declared variable, function, enum, etc.\n/// [C99 6.5.1p2]\n///\n/// This encodes all the information about how a declaration is referenced\n/// within an expression.\n///\n/// There are several optional constructs attached to DeclRefExprs only when\n/// they apply in order to conserve memory. These are laid out past the end of\n/// the object, and flags in the DeclRefExprBitfield track whether they exist:\n///\n///   DeclRefExprBits.HasQualifier:\n///       Specifies when this declaration reference expression has a C++\n///       nested-name-specifier.\n///   DeclRefExprBits.HasFoundDecl:\n///       Specifies when this declaration reference expression has a record of\n///       a NamedDecl (different from the referenced ValueDecl) which was found\n///       during name lookup and/or overload resolution.\n///   DeclRefExprBits.HasTemplateKWAndArgsInfo:\n///       Specifies when this declaration reference expression has an explicit\n///       C++ template keyword and/or template argument list.\n///   DeclRefExprBits.RefersToEnclosingVariableOrCapture\n///       Specifies when this declaration reference expression (validly)\n///       refers to an enclosed local or a captured variable.\nclass DeclRefExpr final\n    : public Expr,\n      private llvm::TrailingObjects<DeclRefExpr, NestedNameSpecifierLoc,\n                                    NamedDecl *, ASTTemplateKWAndArgsInfo,\n                                    TemplateArgumentLoc> {\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n  friend TrailingObjects;\n\n  /// The declaration that we are referencing.\n  ValueDecl *D;\n\n  /// Provides source/type location info for the declaration name\n  /// embedded in D.\n  DeclarationNameLoc DNLoc;\n\n  size_t numTrailingObjects(OverloadToken<NestedNameSpecifierLoc>) const {\n    return hasQualifier();\n  }\n\n  size_t numTrailingObjects(OverloadToken<NamedDecl *>) const {\n    return hasFoundDecl();\n  }\n\n  size_t numTrailingObjects(OverloadToken<ASTTemplateKWAndArgsInfo>) const {\n    return hasTemplateKWAndArgsInfo();\n  }\n\n  /// Test whether there is a distinct FoundDecl attached to the end of\n  /// this DRE.\n  bool hasFoundDecl() const { return DeclRefExprBits.HasFoundDecl; }\n\n  DeclRefExpr(const ASTContext &Ctx, NestedNameSpecifierLoc QualifierLoc,\n              SourceLocation TemplateKWLoc, ValueDecl *D,\n              bool RefersToEnlosingVariableOrCapture,\n              const DeclarationNameInfo &NameInfo, NamedDecl *FoundD,\n              const TemplateArgumentListInfo *TemplateArgs, QualType T,\n              ExprValueKind VK, NonOdrUseReason NOUR);\n\n  /// Construct an empty declaration reference expression.\n  explicit DeclRefExpr(EmptyShell Empty) : Expr(DeclRefExprClass, Empty) {}\n\npublic:\n  DeclRefExpr(const ASTContext &Ctx, ValueDecl *D,\n              bool RefersToEnclosingVariableOrCapture, QualType T,\n              ExprValueKind VK, SourceLocation L,\n              const DeclarationNameLoc &LocInfo = DeclarationNameLoc(),\n              NonOdrUseReason NOUR = NOUR_None);\n\n  static DeclRefExpr *\n  Create(const ASTContext &Context, NestedNameSpecifierLoc QualifierLoc,\n         SourceLocation TemplateKWLoc, ValueDecl *D,\n         bool RefersToEnclosingVariableOrCapture, SourceLocation NameLoc,\n         QualType T, ExprValueKind VK, NamedDecl *FoundD = nullptr,\n         const TemplateArgumentListInfo *TemplateArgs = nullptr,\n         NonOdrUseReason NOUR = NOUR_None);\n\n  static DeclRefExpr *\n  Create(const ASTContext &Context, NestedNameSpecifierLoc QualifierLoc,\n         SourceLocation TemplateKWLoc, ValueDecl *D,\n         bool RefersToEnclosingVariableOrCapture,\n         const DeclarationNameInfo &NameInfo, QualType T, ExprValueKind VK,\n         NamedDecl *FoundD = nullptr,\n         const TemplateArgumentListInfo *TemplateArgs = nullptr,\n         NonOdrUseReason NOUR = NOUR_None);\n\n  /// Construct an empty declaration reference expression.\n  static DeclRefExpr *CreateEmpty(const ASTContext &Context, bool HasQualifier,\n                                  bool HasFoundDecl,\n                                  bool HasTemplateKWAndArgsInfo,\n                                  unsigned NumTemplateArgs);\n\n  ValueDecl *getDecl() { return D; }\n  const ValueDecl *getDecl() const { return D; }\n  void setDecl(ValueDecl *NewD);\n\n  DeclarationNameInfo getNameInfo() const {\n    return DeclarationNameInfo(getDecl()->getDeclName(), getLocation(), DNLoc);\n  }\n\n  SourceLocation getLocation() const { return DeclRefExprBits.Loc; }\n  void setLocation(SourceLocation L) { DeclRefExprBits.Loc = L; }\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  /// Determine whether this declaration reference was preceded by a\n  /// C++ nested-name-specifier, e.g., \\c N::foo.\n  bool hasQualifier() const { return DeclRefExprBits.HasQualifier; }\n\n  /// If the name was qualified, retrieves the nested-name-specifier\n  /// that precedes the name, with source-location information.\n  NestedNameSpecifierLoc getQualifierLoc() const {\n    if (!hasQualifier())\n      return NestedNameSpecifierLoc();\n    return *getTrailingObjects<NestedNameSpecifierLoc>();\n  }\n\n  /// If the name was qualified, retrieves the nested-name-specifier\n  /// that precedes the name. Otherwise, returns NULL.\n  NestedNameSpecifier *getQualifier() const {\n    return getQualifierLoc().getNestedNameSpecifier();\n  }\n\n  /// Get the NamedDecl through which this reference occurred.\n  ///\n  /// This Decl may be different from the ValueDecl actually referred to in the\n  /// presence of using declarations, etc. It always returns non-NULL, and may\n  /// simple return the ValueDecl when appropriate.\n\n  NamedDecl *getFoundDecl() {\n    return hasFoundDecl() ? *getTrailingObjects<NamedDecl *>() : D;\n  }\n\n  /// Get the NamedDecl through which this reference occurred.\n  /// See non-const variant.\n  const NamedDecl *getFoundDecl() const {\n    return hasFoundDecl() ? *getTrailingObjects<NamedDecl *>() : D;\n  }\n\n  bool hasTemplateKWAndArgsInfo() const {\n    return DeclRefExprBits.HasTemplateKWAndArgsInfo;\n  }\n\n  /// Retrieve the location of the template keyword preceding\n  /// this name, if any.\n  SourceLocation getTemplateKeywordLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->TemplateKWLoc;\n  }\n\n  /// Retrieve the location of the left angle bracket starting the\n  /// explicit template argument list following the name, if any.\n  SourceLocation getLAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->LAngleLoc;\n  }\n\n  /// Retrieve the location of the right angle bracket ending the\n  /// explicit template argument list following the name, if any.\n  SourceLocation getRAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->RAngleLoc;\n  }\n\n  /// Determines whether the name in this declaration reference\n  /// was preceded by the template keyword.\n  bool hasTemplateKeyword() const { return getTemplateKeywordLoc().isValid(); }\n\n  /// Determines whether this declaration reference was followed by an\n  /// explicit template argument list.\n  bool hasExplicitTemplateArgs() const { return getLAngleLoc().isValid(); }\n\n  /// Copies the template arguments (if present) into the given\n  /// structure.\n  void copyTemplateArgumentsInto(TemplateArgumentListInfo &List) const {\n    if (hasExplicitTemplateArgs())\n      getTrailingObjects<ASTTemplateKWAndArgsInfo>()->copyInto(\n          getTrailingObjects<TemplateArgumentLoc>(), List);\n  }\n\n  /// Retrieve the template arguments provided as part of this\n  /// template-id.\n  const TemplateArgumentLoc *getTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return nullptr;\n    return getTrailingObjects<TemplateArgumentLoc>();\n  }\n\n  /// Retrieve the number of template arguments provided as part of this\n  /// template-id.\n  unsigned getNumTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return 0;\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->NumTemplateArgs;\n  }\n\n  ArrayRef<TemplateArgumentLoc> template_arguments() const {\n    return {getTemplateArgs(), getNumTemplateArgs()};\n  }\n\n  /// Returns true if this expression refers to a function that\n  /// was resolved from an overloaded set having size greater than 1.\n  bool hadMultipleCandidates() const {\n    return DeclRefExprBits.HadMultipleCandidates;\n  }\n  /// Sets the flag telling whether this expression refers to\n  /// a function that was resolved from an overloaded set having size\n  /// greater than 1.\n  void setHadMultipleCandidates(bool V = true) {\n    DeclRefExprBits.HadMultipleCandidates = V;\n  }\n\n  /// Is this expression a non-odr-use reference, and if so, why?\n  NonOdrUseReason isNonOdrUse() const {\n    return static_cast<NonOdrUseReason>(DeclRefExprBits.NonOdrUseReason);\n  }\n\n  /// Does this DeclRefExpr refer to an enclosing local or a captured\n  /// variable?\n  bool refersToEnclosingVariableOrCapture() const {\n    return DeclRefExprBits.RefersToEnclosingVariableOrCapture;\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == DeclRefExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// Used by IntegerLiteral/FloatingLiteral to store the numeric without\n/// leaking memory.\n///\n/// For large floats/integers, APFloat/APInt will allocate memory from the heap\n/// to represent these numbers.  Unfortunately, when we use a BumpPtrAllocator\n/// to allocate IntegerLiteral/FloatingLiteral nodes the memory associated with\n/// the APFloat/APInt values will never get freed. APNumericStorage uses\n/// ASTContext's allocator for memory allocation.\nclass APNumericStorage {\n  union {\n    uint64_t VAL;    ///< Used to store the <= 64 bits integer value.\n    uint64_t *pVal;  ///< Used to store the >64 bits integer value.\n  };\n  unsigned BitWidth;\n\n  bool hasAllocation() const { return llvm::APInt::getNumWords(BitWidth) > 1; }\n\n  APNumericStorage(const APNumericStorage &) = delete;\n  void operator=(const APNumericStorage &) = delete;\n\nprotected:\n  APNumericStorage() : VAL(0), BitWidth(0) { }\n\n  llvm::APInt getIntValue() const {\n    unsigned NumWords = llvm::APInt::getNumWords(BitWidth);\n    if (NumWords > 1)\n      return llvm::APInt(BitWidth, NumWords, pVal);\n    else\n      return llvm::APInt(BitWidth, VAL);\n  }\n  void setIntValue(const ASTContext &C, const llvm::APInt &Val);\n};\n\nclass APIntStorage : private APNumericStorage {\npublic:\n  llvm::APInt getValue() const { return getIntValue(); }\n  void setValue(const ASTContext &C, const llvm::APInt &Val) {\n    setIntValue(C, Val);\n  }\n};\n\nclass APFloatStorage : private APNumericStorage {\npublic:\n  llvm::APFloat getValue(const llvm::fltSemantics &Semantics) const {\n    return llvm::APFloat(Semantics, getIntValue());\n  }\n  void setValue(const ASTContext &C, const llvm::APFloat &Val) {\n    setIntValue(C, Val.bitcastToAPInt());\n  }\n};\n\nclass IntegerLiteral : public Expr, public APIntStorage {\n  SourceLocation Loc;\n\n  /// Construct an empty integer literal.\n  explicit IntegerLiteral(EmptyShell Empty)\n    : Expr(IntegerLiteralClass, Empty) { }\n\npublic:\n  // type should be IntTy, LongTy, LongLongTy, UnsignedIntTy, UnsignedLongTy,\n  // or UnsignedLongLongTy\n  IntegerLiteral(const ASTContext &C, const llvm::APInt &V, QualType type,\n                 SourceLocation l);\n\n  /// Returns a new integer literal with value 'V' and type 'type'.\n  /// \\param type - either IntTy, LongTy, LongLongTy, UnsignedIntTy,\n  /// UnsignedLongTy, or UnsignedLongLongTy which should match the size of V\n  /// \\param V - the value that the returned integer literal contains.\n  static IntegerLiteral *Create(const ASTContext &C, const llvm::APInt &V,\n                                QualType type, SourceLocation l);\n  /// Returns a new empty integer literal.\n  static IntegerLiteral *Create(const ASTContext &C, EmptyShell Empty);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  /// Retrieve the location of the literal.\n  SourceLocation getLocation() const { return Loc; }\n\n  void setLocation(SourceLocation Location) { Loc = Location; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == IntegerLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass FixedPointLiteral : public Expr, public APIntStorage {\n  SourceLocation Loc;\n  unsigned Scale;\n\n  /// \\brief Construct an empty fixed-point literal.\n  explicit FixedPointLiteral(EmptyShell Empty)\n      : Expr(FixedPointLiteralClass, Empty) {}\n\n public:\n  FixedPointLiteral(const ASTContext &C, const llvm::APInt &V, QualType type,\n                    SourceLocation l, unsigned Scale);\n\n  // Store the int as is without any bit shifting.\n  static FixedPointLiteral *CreateFromRawInt(const ASTContext &C,\n                                             const llvm::APInt &V,\n                                             QualType type, SourceLocation l,\n                                             unsigned Scale);\n\n  /// Returns an empty fixed-point literal.\n  static FixedPointLiteral *Create(const ASTContext &C, EmptyShell Empty);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  /// \\brief Retrieve the location of the literal.\n  SourceLocation getLocation() const { return Loc; }\n\n  void setLocation(SourceLocation Location) { Loc = Location; }\n\n  unsigned getScale() const { return Scale; }\n  void setScale(unsigned S) { Scale = S; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == FixedPointLiteralClass;\n  }\n\n  std::string getValueAsString(unsigned Radix) const;\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass CharacterLiteral : public Expr {\npublic:\n  enum CharacterKind {\n    Ascii,\n    Wide,\n    UTF8,\n    UTF16,\n    UTF32\n  };\n\nprivate:\n  unsigned Value;\n  SourceLocation Loc;\npublic:\n  // type should be IntTy\n  CharacterLiteral(unsigned value, CharacterKind kind, QualType type,\n                   SourceLocation l)\n      : Expr(CharacterLiteralClass, type, VK_RValue, OK_Ordinary), Value(value),\n        Loc(l) {\n    CharacterLiteralBits.Kind = kind;\n    setDependence(ExprDependence::None);\n  }\n\n  /// Construct an empty character literal.\n  CharacterLiteral(EmptyShell Empty) : Expr(CharacterLiteralClass, Empty) { }\n\n  SourceLocation getLocation() const { return Loc; }\n  CharacterKind getKind() const {\n    return static_cast<CharacterKind>(CharacterLiteralBits.Kind);\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  unsigned getValue() const { return Value; }\n\n  void setLocation(SourceLocation Location) { Loc = Location; }\n  void setKind(CharacterKind kind) { CharacterLiteralBits.Kind = kind; }\n  void setValue(unsigned Val) { Value = Val; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == CharacterLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass FloatingLiteral : public Expr, private APFloatStorage {\n  SourceLocation Loc;\n\n  FloatingLiteral(const ASTContext &C, const llvm::APFloat &V, bool isexact,\n                  QualType Type, SourceLocation L);\n\n  /// Construct an empty floating-point literal.\n  explicit FloatingLiteral(const ASTContext &C, EmptyShell Empty);\n\npublic:\n  static FloatingLiteral *Create(const ASTContext &C, const llvm::APFloat &V,\n                                 bool isexact, QualType Type, SourceLocation L);\n  static FloatingLiteral *Create(const ASTContext &C, EmptyShell Empty);\n\n  llvm::APFloat getValue() const {\n    return APFloatStorage::getValue(getSemantics());\n  }\n  void setValue(const ASTContext &C, const llvm::APFloat &Val) {\n    assert(&getSemantics() == &Val.getSemantics() && \"Inconsistent semantics\");\n    APFloatStorage::setValue(C, Val);\n  }\n\n  /// Get a raw enumeration value representing the floating-point semantics of\n  /// this literal (32-bit IEEE, x87, ...), suitable for serialisation.\n  llvm::APFloatBase::Semantics getRawSemantics() const {\n    return static_cast<llvm::APFloatBase::Semantics>(\n        FloatingLiteralBits.Semantics);\n  }\n\n  /// Set the raw enumeration value representing the floating-point semantics of\n  /// this literal (32-bit IEEE, x87, ...), suitable for serialisation.\n  void setRawSemantics(llvm::APFloatBase::Semantics Sem) {\n    FloatingLiteralBits.Semantics = Sem;\n  }\n\n  /// Return the APFloat semantics this literal uses.\n  const llvm::fltSemantics &getSemantics() const {\n    return llvm::APFloatBase::EnumToSemantics(\n        static_cast<llvm::APFloatBase::Semantics>(\n            FloatingLiteralBits.Semantics));\n  }\n\n  /// Set the APFloat semantics this literal uses.\n  void setSemantics(const llvm::fltSemantics &Sem) {\n    FloatingLiteralBits.Semantics = llvm::APFloatBase::SemanticsToEnum(Sem);\n  }\n\n  bool isExact() const { return FloatingLiteralBits.IsExact; }\n  void setExact(bool E) { FloatingLiteralBits.IsExact = E; }\n\n  /// getValueAsApproximateDouble - This returns the value as an inaccurate\n  /// double.  Note that this may cause loss of precision, but is useful for\n  /// debugging dumps, etc.\n  double getValueAsApproximateDouble() const;\n\n  SourceLocation getLocation() const { return Loc; }\n  void setLocation(SourceLocation L) { Loc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Loc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Loc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == FloatingLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// ImaginaryLiteral - We support imaginary integer and floating point literals,\n/// like \"1.0i\".  We represent these as a wrapper around FloatingLiteral and\n/// IntegerLiteral classes.  Instances of this class always have a Complex type\n/// whose element type matches the subexpression.\n///\nclass ImaginaryLiteral : public Expr {\n  Stmt *Val;\npublic:\n  ImaginaryLiteral(Expr *val, QualType Ty)\n      : Expr(ImaginaryLiteralClass, Ty, VK_RValue, OK_Ordinary), Val(val) {\n    setDependence(ExprDependence::None);\n  }\n\n  /// Build an empty imaginary literal.\n  explicit ImaginaryLiteral(EmptyShell Empty)\n    : Expr(ImaginaryLiteralClass, Empty) { }\n\n  const Expr *getSubExpr() const { return cast<Expr>(Val); }\n  Expr *getSubExpr() { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return Val->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Val->getEndLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ImaginaryLiteralClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n};\n\n/// StringLiteral - This represents a string literal expression, e.g. \"foo\"\n/// or L\"bar\" (wide strings). The actual string data can be obtained with\n/// getBytes() and is NOT null-terminated. The length of the string data is\n/// determined by calling getByteLength().\n///\n/// The C type for a string is always a ConstantArrayType. In C++, the char\n/// type is const qualified, in C it is not.\n///\n/// Note that strings in C can be formed by concatenation of multiple string\n/// literal pptokens in translation phase #6. This keeps track of the locations\n/// of each of these pieces.\n///\n/// Strings in C can also be truncated and extended by assigning into arrays,\n/// e.g. with constructs like:\n///   char X[2] = \"foobar\";\n/// In this case, getByteLength() will return 6, but the string literal will\n/// have type \"char[2]\".\nclass StringLiteral final\n    : public Expr,\n      private llvm::TrailingObjects<StringLiteral, unsigned, SourceLocation,\n                                    char> {\n  friend class ASTStmtReader;\n  friend TrailingObjects;\n\n  /// StringLiteral is followed by several trailing objects. They are in order:\n  ///\n  /// * A single unsigned storing the length in characters of this string. The\n  ///   length in bytes is this length times the width of a single character.\n  ///   Always present and stored as a trailing objects because storing it in\n  ///   StringLiteral would increase the size of StringLiteral by sizeof(void *)\n  ///   due to alignment requirements. If you add some data to StringLiteral,\n  ///   consider moving it inside StringLiteral.\n  ///\n  /// * An array of getNumConcatenated() SourceLocation, one for each of the\n  ///   token this string is made of.\n  ///\n  /// * An array of getByteLength() char used to store the string data.\n\npublic:\n  enum StringKind { Ascii, Wide, UTF8, UTF16, UTF32 };\n\nprivate:\n  unsigned numTrailingObjects(OverloadToken<unsigned>) const { return 1; }\n  unsigned numTrailingObjects(OverloadToken<SourceLocation>) const {\n    return getNumConcatenated();\n  }\n\n  unsigned numTrailingObjects(OverloadToken<char>) const {\n    return getByteLength();\n  }\n\n  char *getStrDataAsChar() { return getTrailingObjects<char>(); }\n  const char *getStrDataAsChar() const { return getTrailingObjects<char>(); }\n\n  const uint16_t *getStrDataAsUInt16() const {\n    return reinterpret_cast<const uint16_t *>(getTrailingObjects<char>());\n  }\n\n  const uint32_t *getStrDataAsUInt32() const {\n    return reinterpret_cast<const uint32_t *>(getTrailingObjects<char>());\n  }\n\n  /// Build a string literal.\n  StringLiteral(const ASTContext &Ctx, StringRef Str, StringKind Kind,\n                bool Pascal, QualType Ty, const SourceLocation *Loc,\n                unsigned NumConcatenated);\n\n  /// Build an empty string literal.\n  StringLiteral(EmptyShell Empty, unsigned NumConcatenated, unsigned Length,\n                unsigned CharByteWidth);\n\n  /// Map a target and string kind to the appropriate character width.\n  static unsigned mapCharByteWidth(TargetInfo const &Target, StringKind SK);\n\n  /// Set one of the string literal token.\n  void setStrTokenLoc(unsigned TokNum, SourceLocation L) {\n    assert(TokNum < getNumConcatenated() && \"Invalid tok number\");\n    getTrailingObjects<SourceLocation>()[TokNum] = L;\n  }\n\npublic:\n  /// This is the \"fully general\" constructor that allows representation of\n  /// strings formed from multiple concatenated tokens.\n  static StringLiteral *Create(const ASTContext &Ctx, StringRef Str,\n                               StringKind Kind, bool Pascal, QualType Ty,\n                               const SourceLocation *Loc,\n                               unsigned NumConcatenated);\n\n  /// Simple constructor for string literals made from one token.\n  static StringLiteral *Create(const ASTContext &Ctx, StringRef Str,\n                               StringKind Kind, bool Pascal, QualType Ty,\n                               SourceLocation Loc) {\n    return Create(Ctx, Str, Kind, Pascal, Ty, &Loc, 1);\n  }\n\n  /// Construct an empty string literal.\n  static StringLiteral *CreateEmpty(const ASTContext &Ctx,\n                                    unsigned NumConcatenated, unsigned Length,\n                                    unsigned CharByteWidth);\n\n  StringRef getString() const {\n    assert(getCharByteWidth() == 1 &&\n           \"This function is used in places that assume strings use char\");\n    return StringRef(getStrDataAsChar(), getByteLength());\n  }\n\n  /// Allow access to clients that need the byte representation, such as\n  /// ASTWriterStmt::VisitStringLiteral().\n  StringRef getBytes() const {\n    // FIXME: StringRef may not be the right type to use as a result for this.\n    return StringRef(getStrDataAsChar(), getByteLength());\n  }\n\n  void outputString(raw_ostream &OS) const;\n\n  uint32_t getCodeUnit(size_t i) const {\n    assert(i < getLength() && \"out of bounds access\");\n    switch (getCharByteWidth()) {\n    case 1:\n      return static_cast<unsigned char>(getStrDataAsChar()[i]);\n    case 2:\n      return getStrDataAsUInt16()[i];\n    case 4:\n      return getStrDataAsUInt32()[i];\n    }\n    llvm_unreachable(\"Unsupported character width!\");\n  }\n\n  unsigned getByteLength() const { return getCharByteWidth() * getLength(); }\n  unsigned getLength() const { return *getTrailingObjects<unsigned>(); }\n  unsigned getCharByteWidth() const { return StringLiteralBits.CharByteWidth; }\n\n  StringKind getKind() const {\n    return static_cast<StringKind>(StringLiteralBits.Kind);\n  }\n\n  bool isAscii() const { return getKind() == Ascii; }\n  bool isWide() const { return getKind() == Wide; }\n  bool isUTF8() const { return getKind() == UTF8; }\n  bool isUTF16() const { return getKind() == UTF16; }\n  bool isUTF32() const { return getKind() == UTF32; }\n  bool isPascal() const { return StringLiteralBits.IsPascal; }\n\n  bool containsNonAscii() const {\n    for (auto c : getString())\n      if (!isASCII(c))\n        return true;\n    return false;\n  }\n\n  bool containsNonAsciiOrNull() const {\n    for (auto c : getString())\n      if (!isASCII(c) || !c)\n        return true;\n    return false;\n  }\n\n  /// getNumConcatenated - Get the number of string literal tokens that were\n  /// concatenated in translation phase #6 to form this string literal.\n  unsigned getNumConcatenated() const {\n    return StringLiteralBits.NumConcatenated;\n  }\n\n  /// Get one of the string literal token.\n  SourceLocation getStrTokenLoc(unsigned TokNum) const {\n    assert(TokNum < getNumConcatenated() && \"Invalid tok number\");\n    return getTrailingObjects<SourceLocation>()[TokNum];\n  }\n\n  /// getLocationOfByte - Return a source location that points to the specified\n  /// byte of this string literal.\n  ///\n  /// Strings are amazingly complex.  They can be formed from multiple tokens\n  /// and can have escape sequences in them in addition to the usual trigraph\n  /// and escaped newline business.  This routine handles this complexity.\n  ///\n  SourceLocation\n  getLocationOfByte(unsigned ByteNo, const SourceManager &SM,\n                    const LangOptions &Features, const TargetInfo &Target,\n                    unsigned *StartToken = nullptr,\n                    unsigned *StartTokenByteOffset = nullptr) const;\n\n  typedef const SourceLocation *tokloc_iterator;\n\n  tokloc_iterator tokloc_begin() const {\n    return getTrailingObjects<SourceLocation>();\n  }\n\n  tokloc_iterator tokloc_end() const {\n    return getTrailingObjects<SourceLocation>() + getNumConcatenated();\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return *tokloc_begin(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return *(tokloc_end() - 1); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == StringLiteralClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// [C99 6.4.2.2] - A predefined identifier such as __func__.\nclass PredefinedExpr final\n    : public Expr,\n      private llvm::TrailingObjects<PredefinedExpr, Stmt *> {\n  friend class ASTStmtReader;\n  friend TrailingObjects;\n\n  // PredefinedExpr is optionally followed by a single trailing\n  // \"Stmt *\" for the predefined identifier. It is present if and only if\n  // hasFunctionName() is true and is always a \"StringLiteral *\".\n\npublic:\n  enum IdentKind {\n    Func,\n    Function,\n    LFunction, // Same as Function, but as wide string.\n    FuncDName,\n    FuncSig,\n    LFuncSig, // Same as FuncSig, but as as wide string\n    PrettyFunction,\n    /// The same as PrettyFunction, except that the\n    /// 'virtual' keyword is omitted for virtual member functions.\n    PrettyFunctionNoVirtual\n  };\n\nprivate:\n  PredefinedExpr(SourceLocation L, QualType FNTy, IdentKind IK,\n                 StringLiteral *SL);\n\n  explicit PredefinedExpr(EmptyShell Empty, bool HasFunctionName);\n\n  /// True if this PredefinedExpr has storage for a function name.\n  bool hasFunctionName() const { return PredefinedExprBits.HasFunctionName; }\n\n  void setFunctionName(StringLiteral *SL) {\n    assert(hasFunctionName() &&\n           \"This PredefinedExpr has no storage for a function name!\");\n    *getTrailingObjects<Stmt *>() = SL;\n  }\n\npublic:\n  /// Create a PredefinedExpr.\n  static PredefinedExpr *Create(const ASTContext &Ctx, SourceLocation L,\n                                QualType FNTy, IdentKind IK, StringLiteral *SL);\n\n  /// Create an empty PredefinedExpr.\n  static PredefinedExpr *CreateEmpty(const ASTContext &Ctx,\n                                     bool HasFunctionName);\n\n  IdentKind getIdentKind() const {\n    return static_cast<IdentKind>(PredefinedExprBits.Kind);\n  }\n\n  SourceLocation getLocation() const { return PredefinedExprBits.Loc; }\n  void setLocation(SourceLocation L) { PredefinedExprBits.Loc = L; }\n\n  StringLiteral *getFunctionName() {\n    return hasFunctionName()\n               ? static_cast<StringLiteral *>(*getTrailingObjects<Stmt *>())\n               : nullptr;\n  }\n\n  const StringLiteral *getFunctionName() const {\n    return hasFunctionName()\n               ? static_cast<StringLiteral *>(*getTrailingObjects<Stmt *>())\n               : nullptr;\n  }\n\n  static StringRef getIdentKindName(IdentKind IK);\n  StringRef getIdentKindName() const {\n    return getIdentKindName(getIdentKind());\n  }\n\n  static std::string ComputeName(IdentKind IK, const Decl *CurrentDecl);\n\n  SourceLocation getBeginLoc() const { return getLocation(); }\n  SourceLocation getEndLoc() const { return getLocation(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == PredefinedExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(getTrailingObjects<Stmt *>(),\n                       getTrailingObjects<Stmt *>() + hasFunctionName());\n  }\n\n  const_child_range children() const {\n    return const_child_range(getTrailingObjects<Stmt *>(),\n                             getTrailingObjects<Stmt *>() + hasFunctionName());\n  }\n};\n\n/// ParenExpr - This represents a parethesized expression, e.g. \"(1)\".  This\n/// AST node is only formed if full location information is requested.\nclass ParenExpr : public Expr {\n  SourceLocation L, R;\n  Stmt *Val;\npublic:\n  ParenExpr(SourceLocation l, SourceLocation r, Expr *val)\n      : Expr(ParenExprClass, val->getType(), val->getValueKind(),\n             val->getObjectKind()),\n        L(l), R(r), Val(val) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Construct an empty parenthesized expression.\n  explicit ParenExpr(EmptyShell Empty)\n    : Expr(ParenExprClass, Empty) { }\n\n  const Expr *getSubExpr() const { return cast<Expr>(Val); }\n  Expr *getSubExpr() { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return L; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return R; }\n\n  /// Get the location of the left parentheses '('.\n  SourceLocation getLParen() const { return L; }\n  void setLParen(SourceLocation Loc) { L = Loc; }\n\n  /// Get the location of the right parentheses ')'.\n  SourceLocation getRParen() const { return R; }\n  void setRParen(SourceLocation Loc) { R = Loc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ParenExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n};\n\n/// UnaryOperator - This represents the unary-expression's (except sizeof and\n/// alignof), the postinc/postdec operators from postfix-expression, and various\n/// extensions.\n///\n/// Notes on various nodes:\n///\n/// Real/Imag - These return the real/imag part of a complex operand.  If\n///   applied to a non-complex value, the former returns its operand and the\n///   later returns zero in the type of the operand.\n///\nclass UnaryOperator final\n    : public Expr,\n      private llvm::TrailingObjects<UnaryOperator, FPOptionsOverride> {\n  Stmt *Val;\n\n  size_t numTrailingObjects(OverloadToken<FPOptionsOverride>) const {\n    return UnaryOperatorBits.HasFPFeatures ? 1 : 0;\n  }\n\n  FPOptionsOverride &getTrailingFPFeatures() {\n    assert(UnaryOperatorBits.HasFPFeatures);\n    return *getTrailingObjects<FPOptionsOverride>();\n  }\n\n  const FPOptionsOverride &getTrailingFPFeatures() const {\n    assert(UnaryOperatorBits.HasFPFeatures);\n    return *getTrailingObjects<FPOptionsOverride>();\n  }\n\npublic:\n  typedef UnaryOperatorKind Opcode;\n\nprotected:\n  UnaryOperator(const ASTContext &Ctx, Expr *input, Opcode opc, QualType type,\n                ExprValueKind VK, ExprObjectKind OK, SourceLocation l,\n                bool CanOverflow, FPOptionsOverride FPFeatures);\n\n  /// Build an empty unary operator.\n  explicit UnaryOperator(bool HasFPFeatures, EmptyShell Empty)\n      : Expr(UnaryOperatorClass, Empty) {\n    UnaryOperatorBits.Opc = UO_AddrOf;\n    UnaryOperatorBits.HasFPFeatures = HasFPFeatures;\n  }\n\npublic:\n  static UnaryOperator *CreateEmpty(const ASTContext &C, bool hasFPFeatures);\n\n  static UnaryOperator *Create(const ASTContext &C, Expr *input, Opcode opc,\n                               QualType type, ExprValueKind VK,\n                               ExprObjectKind OK, SourceLocation l,\n                               bool CanOverflow, FPOptionsOverride FPFeatures);\n\n  Opcode getOpcode() const {\n    return static_cast<Opcode>(UnaryOperatorBits.Opc);\n  }\n  void setOpcode(Opcode Opc) { UnaryOperatorBits.Opc = Opc; }\n\n  Expr *getSubExpr() const { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  /// getOperatorLoc - Return the location of the operator.\n  SourceLocation getOperatorLoc() const { return UnaryOperatorBits.Loc; }\n  void setOperatorLoc(SourceLocation L) { UnaryOperatorBits.Loc = L; }\n\n  /// Returns true if the unary operator can cause an overflow. For instance,\n  ///   signed int i = INT_MAX; i++;\n  ///   signed char c = CHAR_MAX; c++;\n  /// Due to integer promotions, c++ is promoted to an int before the postfix\n  /// increment, and the result is an int that cannot overflow. However, i++\n  /// can overflow.\n  bool canOverflow() const { return UnaryOperatorBits.CanOverflow; }\n  void setCanOverflow(bool C) { UnaryOperatorBits.CanOverflow = C; }\n\n  // Get the FP contractability status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFPContractableWithinStatement(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).allowFPContractWithinStatement();\n  }\n\n  // Get the FENV_ACCESS status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFEnvAccessOn(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).getAllowFEnvAccess();\n  }\n\n  /// isPostfix - Return true if this is a postfix operation, like x++.\n  static bool isPostfix(Opcode Op) {\n    return Op == UO_PostInc || Op == UO_PostDec;\n  }\n\n  /// isPrefix - Return true if this is a prefix operation, like --x.\n  static bool isPrefix(Opcode Op) {\n    return Op == UO_PreInc || Op == UO_PreDec;\n  }\n\n  bool isPrefix() const { return isPrefix(getOpcode()); }\n  bool isPostfix() const { return isPostfix(getOpcode()); }\n\n  static bool isIncrementOp(Opcode Op) {\n    return Op == UO_PreInc || Op == UO_PostInc;\n  }\n  bool isIncrementOp() const {\n    return isIncrementOp(getOpcode());\n  }\n\n  static bool isDecrementOp(Opcode Op) {\n    return Op == UO_PreDec || Op == UO_PostDec;\n  }\n  bool isDecrementOp() const {\n    return isDecrementOp(getOpcode());\n  }\n\n  static bool isIncrementDecrementOp(Opcode Op) { return Op <= UO_PreDec; }\n  bool isIncrementDecrementOp() const {\n    return isIncrementDecrementOp(getOpcode());\n  }\n\n  static bool isArithmeticOp(Opcode Op) {\n    return Op >= UO_Plus && Op <= UO_LNot;\n  }\n  bool isArithmeticOp() const { return isArithmeticOp(getOpcode()); }\n\n  /// getOpcodeStr - Turn an Opcode enum value into the punctuation char it\n  /// corresponds to, e.g. \"sizeof\" or \"[pre]++\"\n  static StringRef getOpcodeStr(Opcode Op);\n\n  /// Retrieve the unary opcode that corresponds to the given\n  /// overloaded operator.\n  static Opcode getOverloadedOpcode(OverloadedOperatorKind OO, bool Postfix);\n\n  /// Retrieve the overloaded operator kind that corresponds to\n  /// the given unary opcode.\n  static OverloadedOperatorKind getOverloadedOperator(Opcode Opc);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return isPostfix() ? Val->getBeginLoc() : getOperatorLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return isPostfix() ? getOperatorLoc() : Val->getEndLoc();\n  }\n  SourceLocation getExprLoc() const { return getOperatorLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == UnaryOperatorClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n\n  /// Is FPFeatures in Trailing Storage?\n  bool hasStoredFPFeatures() const { return UnaryOperatorBits.HasFPFeatures; }\n\n  /// Get FPFeatures from trailing storage.\n  FPOptionsOverride getStoredFPFeatures() const {\n    return getTrailingFPFeatures();\n  }\n\nprotected:\n  /// Set FPFeatures in trailing storage, used only by Serialization\n  void setStoredFPFeatures(FPOptionsOverride F) { getTrailingFPFeatures() = F; }\n\npublic:\n  // Get the FP features status of this operator. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (UnaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n  FPOptionsOverride getFPOptionsOverride() const {\n    if (UnaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  friend TrailingObjects;\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n/// Helper class for OffsetOfExpr.\n\n// __builtin_offsetof(type, identifier(.identifier|[expr])*)\nclass OffsetOfNode {\npublic:\n  /// The kind of offsetof node we have.\n  enum Kind {\n    /// An index into an array.\n    Array = 0x00,\n    /// A field.\n    Field = 0x01,\n    /// A field in a dependent type, known only by its name.\n    Identifier = 0x02,\n    /// An implicit indirection through a C++ base class, when the\n    /// field found is in a base class.\n    Base = 0x03\n  };\n\nprivate:\n  enum { MaskBits = 2, Mask = 0x03 };\n\n  /// The source range that covers this part of the designator.\n  SourceRange Range;\n\n  /// The data describing the designator, which comes in three\n  /// different forms, depending on the lower two bits.\n  ///   - An unsigned index into the array of Expr*'s stored after this node\n  ///     in memory, for [constant-expression] designators.\n  ///   - A FieldDecl*, for references to a known field.\n  ///   - An IdentifierInfo*, for references to a field with a given name\n  ///     when the class type is dependent.\n  ///   - A CXXBaseSpecifier*, for references that look at a field in a\n  ///     base class.\n  uintptr_t Data;\n\npublic:\n  /// Create an offsetof node that refers to an array element.\n  OffsetOfNode(SourceLocation LBracketLoc, unsigned Index,\n               SourceLocation RBracketLoc)\n      : Range(LBracketLoc, RBracketLoc), Data((Index << 2) | Array) {}\n\n  /// Create an offsetof node that refers to a field.\n  OffsetOfNode(SourceLocation DotLoc, FieldDecl *Field, SourceLocation NameLoc)\n      : Range(DotLoc.isValid() ? DotLoc : NameLoc, NameLoc),\n        Data(reinterpret_cast<uintptr_t>(Field) | OffsetOfNode::Field) {}\n\n  /// Create an offsetof node that refers to an identifier.\n  OffsetOfNode(SourceLocation DotLoc, IdentifierInfo *Name,\n               SourceLocation NameLoc)\n      : Range(DotLoc.isValid() ? DotLoc : NameLoc, NameLoc),\n        Data(reinterpret_cast<uintptr_t>(Name) | Identifier) {}\n\n  /// Create an offsetof node that refers into a C++ base class.\n  explicit OffsetOfNode(const CXXBaseSpecifier *Base)\n      : Range(), Data(reinterpret_cast<uintptr_t>(Base) | OffsetOfNode::Base) {}\n\n  /// Determine what kind of offsetof node this is.\n  Kind getKind() const { return static_cast<Kind>(Data & Mask); }\n\n  /// For an array element node, returns the index into the array\n  /// of expressions.\n  unsigned getArrayExprIndex() const {\n    assert(getKind() == Array);\n    return Data >> 2;\n  }\n\n  /// For a field offsetof node, returns the field.\n  FieldDecl *getField() const {\n    assert(getKind() == Field);\n    return reinterpret_cast<FieldDecl *>(Data & ~(uintptr_t)Mask);\n  }\n\n  /// For a field or identifier offsetof node, returns the name of\n  /// the field.\n  IdentifierInfo *getFieldName() const;\n\n  /// For a base class node, returns the base specifier.\n  CXXBaseSpecifier *getBase() const {\n    assert(getKind() == Base);\n    return reinterpret_cast<CXXBaseSpecifier *>(Data & ~(uintptr_t)Mask);\n  }\n\n  /// Retrieve the source range that covers this offsetof node.\n  ///\n  /// For an array element node, the source range contains the locations of\n  /// the square brackets. For a field or identifier node, the source range\n  /// contains the location of the period (if there is one) and the\n  /// identifier.\n  SourceRange getSourceRange() const LLVM_READONLY { return Range; }\n  SourceLocation getBeginLoc() const LLVM_READONLY { return Range.getBegin(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return Range.getEnd(); }\n};\n\n/// OffsetOfExpr - [C99 7.17] - This represents an expression of the form\n/// offsetof(record-type, member-designator). For example, given:\n/// @code\n/// struct S {\n///   float f;\n///   double d;\n/// };\n/// struct T {\n///   int i;\n///   struct S s[10];\n/// };\n/// @endcode\n/// we can represent and evaluate the expression @c offsetof(struct T, s[2].d).\n\nclass OffsetOfExpr final\n    : public Expr,\n      private llvm::TrailingObjects<OffsetOfExpr, OffsetOfNode, Expr *> {\n  SourceLocation OperatorLoc, RParenLoc;\n  // Base type;\n  TypeSourceInfo *TSInfo;\n  // Number of sub-components (i.e. instances of OffsetOfNode).\n  unsigned NumComps;\n  // Number of sub-expressions (i.e. array subscript expressions).\n  unsigned NumExprs;\n\n  size_t numTrailingObjects(OverloadToken<OffsetOfNode>) const {\n    return NumComps;\n  }\n\n  OffsetOfExpr(const ASTContext &C, QualType type,\n               SourceLocation OperatorLoc, TypeSourceInfo *tsi,\n               ArrayRef<OffsetOfNode> comps, ArrayRef<Expr*> exprs,\n               SourceLocation RParenLoc);\n\n  explicit OffsetOfExpr(unsigned numComps, unsigned numExprs)\n    : Expr(OffsetOfExprClass, EmptyShell()),\n      TSInfo(nullptr), NumComps(numComps), NumExprs(numExprs) {}\n\npublic:\n\n  static OffsetOfExpr *Create(const ASTContext &C, QualType type,\n                              SourceLocation OperatorLoc, TypeSourceInfo *tsi,\n                              ArrayRef<OffsetOfNode> comps,\n                              ArrayRef<Expr*> exprs, SourceLocation RParenLoc);\n\n  static OffsetOfExpr *CreateEmpty(const ASTContext &C,\n                                   unsigned NumComps, unsigned NumExprs);\n\n  /// getOperatorLoc - Return the location of the operator.\n  SourceLocation getOperatorLoc() const { return OperatorLoc; }\n  void setOperatorLoc(SourceLocation L) { OperatorLoc = L; }\n\n  /// Return the location of the right parentheses.\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation R) { RParenLoc = R; }\n\n  TypeSourceInfo *getTypeSourceInfo() const {\n    return TSInfo;\n  }\n  void setTypeSourceInfo(TypeSourceInfo *tsi) {\n    TSInfo = tsi;\n  }\n\n  const OffsetOfNode &getComponent(unsigned Idx) const {\n    assert(Idx < NumComps && \"Subscript out of range\");\n    return getTrailingObjects<OffsetOfNode>()[Idx];\n  }\n\n  void setComponent(unsigned Idx, OffsetOfNode ON) {\n    assert(Idx < NumComps && \"Subscript out of range\");\n    getTrailingObjects<OffsetOfNode>()[Idx] = ON;\n  }\n\n  unsigned getNumComponents() const {\n    return NumComps;\n  }\n\n  Expr* getIndexExpr(unsigned Idx) {\n    assert(Idx < NumExprs && \"Subscript out of range\");\n    return getTrailingObjects<Expr *>()[Idx];\n  }\n\n  const Expr *getIndexExpr(unsigned Idx) const {\n    assert(Idx < NumExprs && \"Subscript out of range\");\n    return getTrailingObjects<Expr *>()[Idx];\n  }\n\n  void setIndexExpr(unsigned Idx, Expr* E) {\n    assert(Idx < NumComps && \"Subscript out of range\");\n    getTrailingObjects<Expr *>()[Idx] = E;\n  }\n\n  unsigned getNumExpressions() const {\n    return NumExprs;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return OperatorLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == OffsetOfExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    Stmt **begin = reinterpret_cast<Stmt **>(getTrailingObjects<Expr *>());\n    return child_range(begin, begin + NumExprs);\n  }\n  const_child_range children() const {\n    Stmt *const *begin =\n        reinterpret_cast<Stmt *const *>(getTrailingObjects<Expr *>());\n    return const_child_range(begin, begin + NumExprs);\n  }\n  friend TrailingObjects;\n};\n\n/// UnaryExprOrTypeTraitExpr - expression with either a type or (unevaluated)\n/// expression operand.  Used for sizeof/alignof (C99 6.5.3.4) and\n/// vec_step (OpenCL 1.1 6.11.12).\nclass UnaryExprOrTypeTraitExpr : public Expr {\n  union {\n    TypeSourceInfo *Ty;\n    Stmt *Ex;\n  } Argument;\n  SourceLocation OpLoc, RParenLoc;\n\npublic:\n  UnaryExprOrTypeTraitExpr(UnaryExprOrTypeTrait ExprKind, TypeSourceInfo *TInfo,\n                           QualType resultType, SourceLocation op,\n                           SourceLocation rp)\n      : Expr(UnaryExprOrTypeTraitExprClass, resultType, VK_RValue, OK_Ordinary),\n        OpLoc(op), RParenLoc(rp) {\n    assert(ExprKind <= UETT_Last && \"invalid enum value!\");\n    UnaryExprOrTypeTraitExprBits.Kind = ExprKind;\n    assert(static_cast<unsigned>(ExprKind) ==\n               UnaryExprOrTypeTraitExprBits.Kind &&\n           \"UnaryExprOrTypeTraitExprBits.Kind overflow!\");\n    UnaryExprOrTypeTraitExprBits.IsType = true;\n    Argument.Ty = TInfo;\n    setDependence(computeDependence(this));\n  }\n\n  UnaryExprOrTypeTraitExpr(UnaryExprOrTypeTrait ExprKind, Expr *E,\n                           QualType resultType, SourceLocation op,\n                           SourceLocation rp);\n\n  /// Construct an empty sizeof/alignof expression.\n  explicit UnaryExprOrTypeTraitExpr(EmptyShell Empty)\n    : Expr(UnaryExprOrTypeTraitExprClass, Empty) { }\n\n  UnaryExprOrTypeTrait getKind() const {\n    return static_cast<UnaryExprOrTypeTrait>(UnaryExprOrTypeTraitExprBits.Kind);\n  }\n  void setKind(UnaryExprOrTypeTrait K) {\n    assert(K <= UETT_Last && \"invalid enum value!\");\n    UnaryExprOrTypeTraitExprBits.Kind = K;\n    assert(static_cast<unsigned>(K) == UnaryExprOrTypeTraitExprBits.Kind &&\n           \"UnaryExprOrTypeTraitExprBits.Kind overflow!\");\n  }\n\n  bool isArgumentType() const { return UnaryExprOrTypeTraitExprBits.IsType; }\n  QualType getArgumentType() const {\n    return getArgumentTypeInfo()->getType();\n  }\n  TypeSourceInfo *getArgumentTypeInfo() const {\n    assert(isArgumentType() && \"calling getArgumentType() when arg is expr\");\n    return Argument.Ty;\n  }\n  Expr *getArgumentExpr() {\n    assert(!isArgumentType() && \"calling getArgumentExpr() when arg is type\");\n    return static_cast<Expr*>(Argument.Ex);\n  }\n  const Expr *getArgumentExpr() const {\n    return const_cast<UnaryExprOrTypeTraitExpr*>(this)->getArgumentExpr();\n  }\n\n  void setArgument(Expr *E) {\n    Argument.Ex = E;\n    UnaryExprOrTypeTraitExprBits.IsType = false;\n  }\n  void setArgument(TypeSourceInfo *TInfo) {\n    Argument.Ty = TInfo;\n    UnaryExprOrTypeTraitExprBits.IsType = true;\n  }\n\n  /// Gets the argument type, or the type of the argument expression, whichever\n  /// is appropriate.\n  QualType getTypeOfArgument() const {\n    return isArgumentType() ? getArgumentType() : getArgumentExpr()->getType();\n  }\n\n  SourceLocation getOperatorLoc() const { return OpLoc; }\n  void setOperatorLoc(SourceLocation L) { OpLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return OpLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == UnaryExprOrTypeTraitExprClass;\n  }\n\n  // Iterators\n  child_range children();\n  const_child_range children() const;\n};\n\n//===----------------------------------------------------------------------===//\n// Postfix Operators.\n//===----------------------------------------------------------------------===//\n\n/// ArraySubscriptExpr - [C99 6.5.2.1] Array Subscripting.\nclass ArraySubscriptExpr : public Expr {\n  enum { LHS, RHS, END_EXPR };\n  Stmt *SubExprs[END_EXPR];\n\n  bool lhsIsBase() const { return getRHS()->getType()->isIntegerType(); }\n\npublic:\n  ArraySubscriptExpr(Expr *lhs, Expr *rhs, QualType t, ExprValueKind VK,\n                     ExprObjectKind OK, SourceLocation rbracketloc)\n      : Expr(ArraySubscriptExprClass, t, VK, OK) {\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = rbracketloc;\n    setDependence(computeDependence(this));\n  }\n\n  /// Create an empty array subscript expression.\n  explicit ArraySubscriptExpr(EmptyShell Shell)\n    : Expr(ArraySubscriptExprClass, Shell) { }\n\n  /// An array access can be written A[4] or 4[A] (both are equivalent).\n  /// - getBase() and getIdx() always present the normalized view: A[4].\n  ///    In this case getBase() returns \"A\" and getIdx() returns \"4\".\n  /// - getLHS() and getRHS() present the syntactic view. e.g. for\n  ///    4[A] getLHS() returns \"4\".\n  /// Note: Because vector element access is also written A[4] we must\n  /// predicate the format conversion in getBase and getIdx only on the\n  /// the type of the RHS, as it is possible for the LHS to be a vector of\n  /// integer type\n  Expr *getLHS() { return cast<Expr>(SubExprs[LHS]); }\n  const Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  void setLHS(Expr *E) { SubExprs[LHS] = E; }\n\n  Expr *getRHS() { return cast<Expr>(SubExprs[RHS]); }\n  const Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n  void setRHS(Expr *E) { SubExprs[RHS] = E; }\n\n  Expr *getBase() { return lhsIsBase() ? getLHS() : getRHS(); }\n  const Expr *getBase() const { return lhsIsBase() ? getLHS() : getRHS(); }\n\n  Expr *getIdx() { return lhsIsBase() ? getRHS() : getLHS(); }\n  const Expr *getIdx() const { return lhsIsBase() ? getRHS() : getLHS(); }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getLHS()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const { return getRBracketLoc(); }\n\n  SourceLocation getRBracketLoc() const {\n    return ArrayOrMatrixSubscriptExprBits.RBracketLoc;\n  }\n  void setRBracketLoc(SourceLocation L) {\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = L;\n  }\n\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return getBase()->getExprLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ArraySubscriptExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// MatrixSubscriptExpr - Matrix subscript expression for the MatrixType\n/// extension.\n/// MatrixSubscriptExpr can be either incomplete (only Base and RowIdx are set\n/// so far, the type is IncompleteMatrixIdx) or complete (Base, RowIdx and\n/// ColumnIdx refer to valid expressions). Incomplete matrix expressions only\n/// exist during the initial construction of the AST.\nclass MatrixSubscriptExpr : public Expr {\n  enum { BASE, ROW_IDX, COLUMN_IDX, END_EXPR };\n  Stmt *SubExprs[END_EXPR];\n\npublic:\n  MatrixSubscriptExpr(Expr *Base, Expr *RowIdx, Expr *ColumnIdx, QualType T,\n                      SourceLocation RBracketLoc)\n      : Expr(MatrixSubscriptExprClass, T, Base->getValueKind(),\n             OK_MatrixComponent) {\n    SubExprs[BASE] = Base;\n    SubExprs[ROW_IDX] = RowIdx;\n    SubExprs[COLUMN_IDX] = ColumnIdx;\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = RBracketLoc;\n    setDependence(computeDependence(this));\n  }\n\n  /// Create an empty matrix subscript expression.\n  explicit MatrixSubscriptExpr(EmptyShell Shell)\n      : Expr(MatrixSubscriptExprClass, Shell) {}\n\n  bool isIncomplete() const {\n    bool IsIncomplete = hasPlaceholderType(BuiltinType::IncompleteMatrixIdx);\n    assert((SubExprs[COLUMN_IDX] || IsIncomplete) &&\n           \"expressions without column index must be marked as incomplete\");\n    return IsIncomplete;\n  }\n  Expr *getBase() { return cast<Expr>(SubExprs[BASE]); }\n  const Expr *getBase() const { return cast<Expr>(SubExprs[BASE]); }\n  void setBase(Expr *E) { SubExprs[BASE] = E; }\n\n  Expr *getRowIdx() { return cast<Expr>(SubExprs[ROW_IDX]); }\n  const Expr *getRowIdx() const { return cast<Expr>(SubExprs[ROW_IDX]); }\n  void setRowIdx(Expr *E) { SubExprs[ROW_IDX] = E; }\n\n  Expr *getColumnIdx() { return cast_or_null<Expr>(SubExprs[COLUMN_IDX]); }\n  const Expr *getColumnIdx() const {\n    assert(!isIncomplete() &&\n           \"cannot get the column index of an incomplete expression\");\n    return cast<Expr>(SubExprs[COLUMN_IDX]);\n  }\n  void setColumnIdx(Expr *E) { SubExprs[COLUMN_IDX] = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getBase()->getBeginLoc();\n  }\n\n  SourceLocation getEndLoc() const { return getRBracketLoc(); }\n\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return getBase()->getExprLoc();\n  }\n\n  SourceLocation getRBracketLoc() const {\n    return ArrayOrMatrixSubscriptExprBits.RBracketLoc;\n  }\n  void setRBracketLoc(SourceLocation L) {\n    ArrayOrMatrixSubscriptExprBits.RBracketLoc = L;\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == MatrixSubscriptExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// CallExpr - Represents a function call (C99 6.5.2.2, C++ [expr.call]).\n/// CallExpr itself represents a normal function call, e.g., \"f(x, 2)\",\n/// while its subclasses may represent alternative syntax that (semantically)\n/// results in a function call. For example, CXXOperatorCallExpr is\n/// a subclass for overloaded operator calls that use operator syntax, e.g.,\n/// \"str1 + str2\" to resolve to a function call.\nclass CallExpr : public Expr {\n  enum { FN = 0, PREARGS_START = 1 };\n\n  /// The number of arguments in the call expression.\n  unsigned NumArgs;\n\n  /// The location of the right parenthese. This has a different meaning for\n  /// the derived classes of CallExpr.\n  SourceLocation RParenLoc;\n\n  // CallExpr store some data in trailing objects. However since CallExpr\n  // is used a base of other expression classes we cannot use\n  // llvm::TrailingObjects. Instead we manually perform the pointer arithmetic\n  // and casts.\n  //\n  // The trailing objects are in order:\n  //\n  // * A single \"Stmt *\" for the callee expression.\n  //\n  // * An array of getNumPreArgs() \"Stmt *\" for the pre-argument expressions.\n  //\n  // * An array of getNumArgs() \"Stmt *\" for the argument expressions.\n  //\n  // * An optional of type FPOptionsOverride.\n  //\n  // Note that we store the offset in bytes from the this pointer to the start\n  // of the trailing objects. It would be perfectly possible to compute it\n  // based on the dynamic kind of the CallExpr. However 1.) we have plenty of\n  // space in the bit-fields of Stmt. 2.) It was benchmarked to be faster to\n  // compute this once and then load the offset from the bit-fields of Stmt,\n  // instead of re-computing the offset each time the trailing objects are\n  // accessed.\n\n  /// Return a pointer to the start of the trailing array of \"Stmt *\".\n  Stmt **getTrailingStmts() {\n    return reinterpret_cast<Stmt **>(reinterpret_cast<char *>(this) +\n                                     CallExprBits.OffsetToTrailingObjects);\n  }\n  Stmt *const *getTrailingStmts() const {\n    return const_cast<CallExpr *>(this)->getTrailingStmts();\n  }\n\n  /// Map a statement class to the appropriate offset in bytes from the\n  /// this pointer to the trailing objects.\n  static unsigned offsetToTrailingObjects(StmtClass SC);\n\n  unsigned getSizeOfTrailingStmts() const {\n    return (1 + getNumPreArgs() + getNumArgs()) * sizeof(Stmt *);\n  }\n\n  size_t getOffsetOfTrailingFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return CallExprBits.OffsetToTrailingObjects + getSizeOfTrailingStmts();\n  }\n\npublic:\n  enum class ADLCallKind : bool { NotADL, UsesADL };\n  static constexpr ADLCallKind NotADL = ADLCallKind::NotADL;\n  static constexpr ADLCallKind UsesADL = ADLCallKind::UsesADL;\n\nprotected:\n  /// Build a call expression, assuming that appropriate storage has been\n  /// allocated for the trailing objects.\n  CallExpr(StmtClass SC, Expr *Fn, ArrayRef<Expr *> PreArgs,\n           ArrayRef<Expr *> Args, QualType Ty, ExprValueKind VK,\n           SourceLocation RParenLoc, FPOptionsOverride FPFeatures,\n           unsigned MinNumArgs, ADLCallKind UsesADL);\n\n  /// Build an empty call expression, for deserialization.\n  CallExpr(StmtClass SC, unsigned NumPreArgs, unsigned NumArgs,\n           bool hasFPFeatures, EmptyShell Empty);\n\n  /// Return the size in bytes needed for the trailing objects.\n  /// Used by the derived classes to allocate the right amount of storage.\n  static unsigned sizeOfTrailingObjects(unsigned NumPreArgs, unsigned NumArgs,\n                                        bool HasFPFeatures) {\n    return (1 + NumPreArgs + NumArgs) * sizeof(Stmt *) +\n           HasFPFeatures * sizeof(FPOptionsOverride);\n  }\n\n  Stmt *getPreArg(unsigned I) {\n    assert(I < getNumPreArgs() && \"Prearg access out of range!\");\n    return getTrailingStmts()[PREARGS_START + I];\n  }\n  const Stmt *getPreArg(unsigned I) const {\n    assert(I < getNumPreArgs() && \"Prearg access out of range!\");\n    return getTrailingStmts()[PREARGS_START + I];\n  }\n  void setPreArg(unsigned I, Stmt *PreArg) {\n    assert(I < getNumPreArgs() && \"Prearg access out of range!\");\n    getTrailingStmts()[PREARGS_START + I] = PreArg;\n  }\n\n  unsigned getNumPreArgs() const { return CallExprBits.NumPreArgs; }\n\n  /// Return a pointer to the trailing FPOptions\n  FPOptionsOverride *getTrailingFPFeatures() {\n    assert(hasStoredFPFeatures());\n    return reinterpret_cast<FPOptionsOverride *>(\n        reinterpret_cast<char *>(this) + CallExprBits.OffsetToTrailingObjects +\n        getSizeOfTrailingStmts());\n  }\n  const FPOptionsOverride *getTrailingFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return reinterpret_cast<const FPOptionsOverride *>(\n        reinterpret_cast<const char *>(this) +\n        CallExprBits.OffsetToTrailingObjects + getSizeOfTrailingStmts());\n  }\n\npublic:\n  /// Create a call expression.\n  /// \\param Fn     The callee expression,\n  /// \\param Args   The argument array,\n  /// \\param Ty     The type of the call expression (which is *not* the return\n  ///               type in general),\n  /// \\param VK     The value kind of the call expression (lvalue, rvalue, ...),\n  /// \\param RParenLoc  The location of the right parenthesis in the call\n  ///                   expression.\n  /// \\param FPFeatures Floating-point features associated with the call,\n  /// \\param MinNumArgs Specifies the minimum number of arguments. The actual\n  ///                   number of arguments will be the greater of Args.size()\n  ///                   and MinNumArgs. This is used in a few places to allocate\n  ///                   enough storage for the default arguments.\n  /// \\param UsesADL    Specifies whether the callee was found through\n  ///                   argument-dependent lookup.\n  ///\n  /// Note that you can use CreateTemporary if you need a temporary call\n  /// expression on the stack.\n  static CallExpr *Create(const ASTContext &Ctx, Expr *Fn,\n                          ArrayRef<Expr *> Args, QualType Ty, ExprValueKind VK,\n                          SourceLocation RParenLoc,\n                          FPOptionsOverride FPFeatures, unsigned MinNumArgs = 0,\n                          ADLCallKind UsesADL = NotADL);\n\n  /// Create a temporary call expression with no arguments in the memory\n  /// pointed to by Mem. Mem must points to at least sizeof(CallExpr)\n  /// + sizeof(Stmt *) bytes of storage, aligned to alignof(CallExpr):\n  ///\n  /// \\code{.cpp}\n  ///   alignas(CallExpr) char Buffer[sizeof(CallExpr) + sizeof(Stmt *)];\n  ///   CallExpr *TheCall = CallExpr::CreateTemporary(Buffer, etc);\n  /// \\endcode\n  static CallExpr *CreateTemporary(void *Mem, Expr *Fn, QualType Ty,\n                                   ExprValueKind VK, SourceLocation RParenLoc,\n                                   ADLCallKind UsesADL = NotADL);\n\n  /// Create an empty call expression, for deserialization.\n  static CallExpr *CreateEmpty(const ASTContext &Ctx, unsigned NumArgs,\n                               bool HasFPFeatures, EmptyShell Empty);\n\n  Expr *getCallee() { return cast<Expr>(getTrailingStmts()[FN]); }\n  const Expr *getCallee() const { return cast<Expr>(getTrailingStmts()[FN]); }\n  void setCallee(Expr *F) { getTrailingStmts()[FN] = F; }\n\n  ADLCallKind getADLCallKind() const {\n    return static_cast<ADLCallKind>(CallExprBits.UsesADL);\n  }\n  void setADLCallKind(ADLCallKind V = UsesADL) {\n    CallExprBits.UsesADL = static_cast<bool>(V);\n  }\n  bool usesADL() const { return getADLCallKind() == UsesADL; }\n\n  bool hasStoredFPFeatures() const { return CallExprBits.HasFPFeatures; }\n\n  Decl *getCalleeDecl() { return getCallee()->getReferencedDeclOfCallee(); }\n  const Decl *getCalleeDecl() const {\n    return getCallee()->getReferencedDeclOfCallee();\n  }\n\n  /// If the callee is a FunctionDecl, return it. Otherwise return null.\n  FunctionDecl *getDirectCallee() {\n    return dyn_cast_or_null<FunctionDecl>(getCalleeDecl());\n  }\n  const FunctionDecl *getDirectCallee() const {\n    return dyn_cast_or_null<FunctionDecl>(getCalleeDecl());\n  }\n\n  /// getNumArgs - Return the number of actual arguments to this call.\n  unsigned getNumArgs() const { return NumArgs; }\n\n  /// Retrieve the call arguments.\n  Expr **getArgs() {\n    return reinterpret_cast<Expr **>(getTrailingStmts() + PREARGS_START +\n                                     getNumPreArgs());\n  }\n  const Expr *const *getArgs() const {\n    return reinterpret_cast<const Expr *const *>(\n        getTrailingStmts() + PREARGS_START + getNumPreArgs());\n  }\n\n  /// getArg - Return the specified argument.\n  Expr *getArg(unsigned Arg) {\n    assert(Arg < getNumArgs() && \"Arg access out of range!\");\n    return getArgs()[Arg];\n  }\n  const Expr *getArg(unsigned Arg) const {\n    assert(Arg < getNumArgs() && \"Arg access out of range!\");\n    return getArgs()[Arg];\n  }\n\n  /// setArg - Set the specified argument.\n  void setArg(unsigned Arg, Expr *ArgExpr) {\n    assert(Arg < getNumArgs() && \"Arg access out of range!\");\n    getArgs()[Arg] = ArgExpr;\n  }\n\n  /// Reduce the number of arguments in this call expression. This is used for\n  /// example during error recovery to drop extra arguments. There is no way\n  /// to perform the opposite because: 1.) We don't track how much storage\n  /// we have for the argument array 2.) This would potentially require growing\n  /// the argument array, something we cannot support since the arguments are\n  /// stored in a trailing array.\n  void shrinkNumArgs(unsigned NewNumArgs) {\n    assert((NewNumArgs <= getNumArgs()) &&\n           \"shrinkNumArgs cannot increase the number of arguments!\");\n    NumArgs = NewNumArgs;\n  }\n\n  /// Bluntly set a new number of arguments without doing any checks whatsoever.\n  /// Only used during construction of a CallExpr in a few places in Sema.\n  /// FIXME: Find a way to remove it.\n  void setNumArgsUnsafe(unsigned NewNumArgs) { NumArgs = NewNumArgs; }\n\n  typedef ExprIterator arg_iterator;\n  typedef ConstExprIterator const_arg_iterator;\n  typedef llvm::iterator_range<arg_iterator> arg_range;\n  typedef llvm::iterator_range<const_arg_iterator> const_arg_range;\n\n  arg_range arguments() { return arg_range(arg_begin(), arg_end()); }\n  const_arg_range arguments() const {\n    return const_arg_range(arg_begin(), arg_end());\n  }\n\n  arg_iterator arg_begin() {\n    return getTrailingStmts() + PREARGS_START + getNumPreArgs();\n  }\n  arg_iterator arg_end() { return arg_begin() + getNumArgs(); }\n\n  const_arg_iterator arg_begin() const {\n    return getTrailingStmts() + PREARGS_START + getNumPreArgs();\n  }\n  const_arg_iterator arg_end() const { return arg_begin() + getNumArgs(); }\n\n  /// This method provides fast access to all the subexpressions of\n  /// a CallExpr without going through the slower virtual child_iterator\n  /// interface.  This provides efficient reverse iteration of the\n  /// subexpressions.  This is currently used for CFG construction.\n  ArrayRef<Stmt *> getRawSubExprs() {\n    return llvm::makeArrayRef(getTrailingStmts(),\n                              PREARGS_START + getNumPreArgs() + getNumArgs());\n  }\n\n  /// getNumCommas - Return the number of commas that must have been present in\n  /// this function call.\n  unsigned getNumCommas() const { return getNumArgs() ? getNumArgs() - 1 : 0; }\n\n  /// Get FPOptionsOverride from trailing storage.\n  FPOptionsOverride getStoredFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return *getTrailingFPFeatures();\n  }\n  /// Set FPOptionsOverride in trailing storage. Used only by Serialization.\n  void setStoredFPFeatures(FPOptionsOverride F) {\n    assert(hasStoredFPFeatures());\n    *getTrailingFPFeatures() = F;\n  }\n\n  // Get the FP features status of this operator. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n\n  FPOptionsOverride getFPFeatures() const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  /// getBuiltinCallee - If this is a call to a builtin, return the builtin ID\n  /// of the callee. If not, return 0.\n  unsigned getBuiltinCallee() const;\n\n  /// Returns \\c true if this is a call to a builtin which does not\n  /// evaluate side-effects within its arguments.\n  bool isUnevaluatedBuiltinCall(const ASTContext &Ctx) const;\n\n  /// getCallReturnType - Get the return type of the call expr. This is not\n  /// always the type of the expr itself, if the return type is a reference\n  /// type.\n  QualType getCallReturnType(const ASTContext &Ctx) const;\n\n  /// Returns the WarnUnusedResultAttr that is either declared on the called\n  /// function, or its return type declaration.\n  const Attr *getUnusedResultAttr(const ASTContext &Ctx) const;\n\n  /// Returns true if this call expression should warn on unused results.\n  bool hasUnusedResultAttr(const ASTContext &Ctx) const {\n    return getUnusedResultAttr(Ctx) != nullptr;\n  }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  /// Return true if this is a call to __assume() or __builtin_assume() with\n  /// a non-value-dependent constant parameter evaluating as false.\n  bool isBuiltinAssumeFalse(const ASTContext &Ctx) const;\n\n  /// Used by Sema to implement MSVC-compatible delayed name lookup.\n  /// (Usually Exprs themselves should set dependence).\n  void markDependentForPostponedNameLookup() {\n    setDependence(getDependence() | ExprDependence::TypeValueInstantiation);\n  }\n\n  bool isCallToStdMove() const {\n    const FunctionDecl *FD = getDirectCallee();\n    return getNumArgs() == 1 && FD && FD->isInStdNamespace() &&\n           FD->getIdentifier() && FD->getIdentifier()->isStr(\"move\");\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstCallExprConstant &&\n           T->getStmtClass() <= lastCallExprConstant;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(getTrailingStmts(), getTrailingStmts() + PREARGS_START +\n                                               getNumPreArgs() + getNumArgs());\n  }\n\n  const_child_range children() const {\n    return const_child_range(getTrailingStmts(),\n                             getTrailingStmts() + PREARGS_START +\n                                 getNumPreArgs() + getNumArgs());\n  }\n};\n\n/// Extra data stored in some MemberExpr objects.\nstruct MemberExprNameQualifier {\n  /// The nested-name-specifier that qualifies the name, including\n  /// source-location information.\n  NestedNameSpecifierLoc QualifierLoc;\n\n  /// The DeclAccessPair through which the MemberDecl was found due to\n  /// name qualifiers.\n  DeclAccessPair FoundDecl;\n};\n\n/// MemberExpr - [C99 6.5.2.3] Structure and Union Members.  X->F and X.F.\n///\nclass MemberExpr final\n    : public Expr,\n      private llvm::TrailingObjects<MemberExpr, MemberExprNameQualifier,\n                                    ASTTemplateKWAndArgsInfo,\n                                    TemplateArgumentLoc> {\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n  friend TrailingObjects;\n\n  /// Base - the expression for the base pointer or structure references.  In\n  /// X.F, this is \"X\".\n  Stmt *Base;\n\n  /// MemberDecl - This is the decl being referenced by the field/member name.\n  /// In X.F, this is the decl referenced by F.\n  ValueDecl *MemberDecl;\n\n  /// MemberDNLoc - Provides source/type location info for the\n  /// declaration name embedded in MemberDecl.\n  DeclarationNameLoc MemberDNLoc;\n\n  /// MemberLoc - This is the location of the member name.\n  SourceLocation MemberLoc;\n\n  size_t numTrailingObjects(OverloadToken<MemberExprNameQualifier>) const {\n    return hasQualifierOrFoundDecl();\n  }\n\n  size_t numTrailingObjects(OverloadToken<ASTTemplateKWAndArgsInfo>) const {\n    return hasTemplateKWAndArgsInfo();\n  }\n\n  bool hasQualifierOrFoundDecl() const {\n    return MemberExprBits.HasQualifierOrFoundDecl;\n  }\n\n  bool hasTemplateKWAndArgsInfo() const {\n    return MemberExprBits.HasTemplateKWAndArgsInfo;\n  }\n\n  MemberExpr(Expr *Base, bool IsArrow, SourceLocation OperatorLoc,\n             ValueDecl *MemberDecl, const DeclarationNameInfo &NameInfo,\n             QualType T, ExprValueKind VK, ExprObjectKind OK,\n             NonOdrUseReason NOUR);\n  MemberExpr(EmptyShell Empty)\n      : Expr(MemberExprClass, Empty), Base(), MemberDecl() {}\n\npublic:\n  static MemberExpr *Create(const ASTContext &C, Expr *Base, bool IsArrow,\n                            SourceLocation OperatorLoc,\n                            NestedNameSpecifierLoc QualifierLoc,\n                            SourceLocation TemplateKWLoc, ValueDecl *MemberDecl,\n                            DeclAccessPair FoundDecl,\n                            DeclarationNameInfo MemberNameInfo,\n                            const TemplateArgumentListInfo *TemplateArgs,\n                            QualType T, ExprValueKind VK, ExprObjectKind OK,\n                            NonOdrUseReason NOUR);\n\n  /// Create an implicit MemberExpr, with no location, qualifier, template\n  /// arguments, and so on. Suitable only for non-static member access.\n  static MemberExpr *CreateImplicit(const ASTContext &C, Expr *Base,\n                                    bool IsArrow, ValueDecl *MemberDecl,\n                                    QualType T, ExprValueKind VK,\n                                    ExprObjectKind OK) {\n    return Create(C, Base, IsArrow, SourceLocation(), NestedNameSpecifierLoc(),\n                  SourceLocation(), MemberDecl,\n                  DeclAccessPair::make(MemberDecl, MemberDecl->getAccess()),\n                  DeclarationNameInfo(), nullptr, T, VK, OK, NOUR_None);\n  }\n\n  static MemberExpr *CreateEmpty(const ASTContext &Context, bool HasQualifier,\n                                 bool HasFoundDecl,\n                                 bool HasTemplateKWAndArgsInfo,\n                                 unsigned NumTemplateArgs);\n\n  void setBase(Expr *E) { Base = E; }\n  Expr *getBase() const { return cast<Expr>(Base); }\n\n  /// Retrieve the member declaration to which this expression refers.\n  ///\n  /// The returned declaration will be a FieldDecl or (in C++) a VarDecl (for\n  /// static data members), a CXXMethodDecl, or an EnumConstantDecl.\n  ValueDecl *getMemberDecl() const { return MemberDecl; }\n  void setMemberDecl(ValueDecl *D);\n\n  /// Retrieves the declaration found by lookup.\n  DeclAccessPair getFoundDecl() const {\n    if (!hasQualifierOrFoundDecl())\n      return DeclAccessPair::make(getMemberDecl(),\n                                  getMemberDecl()->getAccess());\n    return getTrailingObjects<MemberExprNameQualifier>()->FoundDecl;\n  }\n\n  /// Determines whether this member expression actually had\n  /// a C++ nested-name-specifier prior to the name of the member, e.g.,\n  /// x->Base::foo.\n  bool hasQualifier() const { return getQualifier() != nullptr; }\n\n  /// If the member name was qualified, retrieves the\n  /// nested-name-specifier that precedes the member name, with source-location\n  /// information.\n  NestedNameSpecifierLoc getQualifierLoc() const {\n    if (!hasQualifierOrFoundDecl())\n      return NestedNameSpecifierLoc();\n    return getTrailingObjects<MemberExprNameQualifier>()->QualifierLoc;\n  }\n\n  /// If the member name was qualified, retrieves the\n  /// nested-name-specifier that precedes the member name. Otherwise, returns\n  /// NULL.\n  NestedNameSpecifier *getQualifier() const {\n    return getQualifierLoc().getNestedNameSpecifier();\n  }\n\n  /// Retrieve the location of the template keyword preceding\n  /// the member name, if any.\n  SourceLocation getTemplateKeywordLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->TemplateKWLoc;\n  }\n\n  /// Retrieve the location of the left angle bracket starting the\n  /// explicit template argument list following the member name, if any.\n  SourceLocation getLAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->LAngleLoc;\n  }\n\n  /// Retrieve the location of the right angle bracket ending the\n  /// explicit template argument list following the member name, if any.\n  SourceLocation getRAngleLoc() const {\n    if (!hasTemplateKWAndArgsInfo())\n      return SourceLocation();\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->RAngleLoc;\n  }\n\n  /// Determines whether the member name was preceded by the template keyword.\n  bool hasTemplateKeyword() const { return getTemplateKeywordLoc().isValid(); }\n\n  /// Determines whether the member name was followed by an\n  /// explicit template argument list.\n  bool hasExplicitTemplateArgs() const { return getLAngleLoc().isValid(); }\n\n  /// Copies the template arguments (if present) into the given\n  /// structure.\n  void copyTemplateArgumentsInto(TemplateArgumentListInfo &List) const {\n    if (hasExplicitTemplateArgs())\n      getTrailingObjects<ASTTemplateKWAndArgsInfo>()->copyInto(\n          getTrailingObjects<TemplateArgumentLoc>(), List);\n  }\n\n  /// Retrieve the template arguments provided as part of this\n  /// template-id.\n  const TemplateArgumentLoc *getTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return nullptr;\n\n    return getTrailingObjects<TemplateArgumentLoc>();\n  }\n\n  /// Retrieve the number of template arguments provided as part of this\n  /// template-id.\n  unsigned getNumTemplateArgs() const {\n    if (!hasExplicitTemplateArgs())\n      return 0;\n\n    return getTrailingObjects<ASTTemplateKWAndArgsInfo>()->NumTemplateArgs;\n  }\n\n  ArrayRef<TemplateArgumentLoc> template_arguments() const {\n    return {getTemplateArgs(), getNumTemplateArgs()};\n  }\n\n  /// Retrieve the member declaration name info.\n  DeclarationNameInfo getMemberNameInfo() const {\n    return DeclarationNameInfo(MemberDecl->getDeclName(),\n                               MemberLoc, MemberDNLoc);\n  }\n\n  SourceLocation getOperatorLoc() const { return MemberExprBits.OperatorLoc; }\n\n  bool isArrow() const { return MemberExprBits.IsArrow; }\n  void setArrow(bool A) { MemberExprBits.IsArrow = A; }\n\n  /// getMemberLoc - Return the location of the \"member\", in X->F, it is the\n  /// location of 'F'.\n  SourceLocation getMemberLoc() const { return MemberLoc; }\n  void setMemberLoc(SourceLocation L) { MemberLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  SourceLocation getExprLoc() const LLVM_READONLY { return MemberLoc; }\n\n  /// Determine whether the base of this explicit is implicit.\n  bool isImplicitAccess() const {\n    return getBase() && getBase()->isImplicitCXXThis();\n  }\n\n  /// Returns true if this member expression refers to a method that\n  /// was resolved from an overloaded set having size greater than 1.\n  bool hadMultipleCandidates() const {\n    return MemberExprBits.HadMultipleCandidates;\n  }\n  /// Sets the flag telling whether this expression refers to\n  /// a method that was resolved from an overloaded set having size\n  /// greater than 1.\n  void setHadMultipleCandidates(bool V = true) {\n    MemberExprBits.HadMultipleCandidates = V;\n  }\n\n  /// Returns true if virtual dispatch is performed.\n  /// If the member access is fully qualified, (i.e. X::f()), virtual\n  /// dispatching is not performed. In -fapple-kext mode qualified\n  /// calls to virtual method will still go through the vtable.\n  bool performsVirtualDispatch(const LangOptions &LO) const {\n    return LO.AppleKext || !hasQualifier();\n  }\n\n  /// Is this expression a non-odr-use reference, and if so, why?\n  /// This is only meaningful if the named member is a static member.\n  NonOdrUseReason isNonOdrUse() const {\n    return static_cast<NonOdrUseReason>(MemberExprBits.NonOdrUseReason);\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == MemberExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Base, &Base+1); }\n  const_child_range children() const {\n    return const_child_range(&Base, &Base + 1);\n  }\n};\n\n/// CompoundLiteralExpr - [C99 6.5.2.5]\n///\nclass CompoundLiteralExpr : public Expr {\n  /// LParenLoc - If non-null, this is the location of the left paren in a\n  /// compound literal like \"(int){4}\".  This can be null if this is a\n  /// synthesized compound expression.\n  SourceLocation LParenLoc;\n\n  /// The type as written.  This can be an incomplete array type, in\n  /// which case the actual expression type will be different.\n  /// The int part of the pair stores whether this expr is file scope.\n  llvm::PointerIntPair<TypeSourceInfo *, 1, bool> TInfoAndScope;\n  Stmt *Init;\npublic:\n  CompoundLiteralExpr(SourceLocation lparenloc, TypeSourceInfo *tinfo,\n                      QualType T, ExprValueKind VK, Expr *init, bool fileScope)\n      : Expr(CompoundLiteralExprClass, T, VK, OK_Ordinary),\n        LParenLoc(lparenloc), TInfoAndScope(tinfo, fileScope), Init(init) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Construct an empty compound literal.\n  explicit CompoundLiteralExpr(EmptyShell Empty)\n    : Expr(CompoundLiteralExprClass, Empty) { }\n\n  const Expr *getInitializer() const { return cast<Expr>(Init); }\n  Expr *getInitializer() { return cast<Expr>(Init); }\n  void setInitializer(Expr *E) { Init = E; }\n\n  bool isFileScope() const { return TInfoAndScope.getInt(); }\n  void setFileScope(bool FS) { TInfoAndScope.setInt(FS); }\n\n  SourceLocation getLParenLoc() const { return LParenLoc; }\n  void setLParenLoc(SourceLocation L) { LParenLoc = L; }\n\n  TypeSourceInfo *getTypeSourceInfo() const {\n    return TInfoAndScope.getPointer();\n  }\n  void setTypeSourceInfo(TypeSourceInfo *tinfo) {\n    TInfoAndScope.setPointer(tinfo);\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    // FIXME: Init should never be null.\n    if (!Init)\n      return SourceLocation();\n    if (LParenLoc.isInvalid())\n      return Init->getBeginLoc();\n    return LParenLoc;\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    // FIXME: Init should never be null.\n    if (!Init)\n      return SourceLocation();\n    return Init->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == CompoundLiteralExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Init, &Init+1); }\n  const_child_range children() const {\n    return const_child_range(&Init, &Init + 1);\n  }\n};\n\n/// CastExpr - Base class for type casts, including both implicit\n/// casts (ImplicitCastExpr) and explicit casts that have some\n/// representation in the source code (ExplicitCastExpr's derived\n/// classes).\nclass CastExpr : public Expr {\n  Stmt *Op;\n\n  bool CastConsistency() const;\n\n  const CXXBaseSpecifier * const *path_buffer() const {\n    return const_cast<CastExpr*>(this)->path_buffer();\n  }\n  CXXBaseSpecifier **path_buffer();\n\n  friend class ASTStmtReader;\n\nprotected:\n  CastExpr(StmtClass SC, QualType ty, ExprValueKind VK, const CastKind kind,\n           Expr *op, unsigned BasePathSize, bool HasFPFeatures)\n      : Expr(SC, ty, VK, OK_Ordinary), Op(op) {\n    CastExprBits.Kind = kind;\n    CastExprBits.PartOfExplicitCast = false;\n    CastExprBits.BasePathSize = BasePathSize;\n    assert((CastExprBits.BasePathSize == BasePathSize) &&\n           \"BasePathSize overflow!\");\n    setDependence(computeDependence(this));\n    assert(CastConsistency());\n    CastExprBits.HasFPFeatures = HasFPFeatures;\n  }\n\n  /// Construct an empty cast.\n  CastExpr(StmtClass SC, EmptyShell Empty, unsigned BasePathSize,\n           bool HasFPFeatures)\n      : Expr(SC, Empty) {\n    CastExprBits.PartOfExplicitCast = false;\n    CastExprBits.BasePathSize = BasePathSize;\n    CastExprBits.HasFPFeatures = HasFPFeatures;\n    assert((CastExprBits.BasePathSize == BasePathSize) &&\n           \"BasePathSize overflow!\");\n  }\n\n  /// Return a pointer to the trailing FPOptions.\n  /// \\pre hasStoredFPFeatures() == true\n  FPOptionsOverride *getTrailingFPFeatures();\n  const FPOptionsOverride *getTrailingFPFeatures() const {\n    return const_cast<CastExpr *>(this)->getTrailingFPFeatures();\n  }\n\npublic:\n  CastKind getCastKind() const { return (CastKind) CastExprBits.Kind; }\n  void setCastKind(CastKind K) { CastExprBits.Kind = K; }\n\n  static const char *getCastKindName(CastKind CK);\n  const char *getCastKindName() const { return getCastKindName(getCastKind()); }\n\n  Expr *getSubExpr() { return cast<Expr>(Op); }\n  const Expr *getSubExpr() const { return cast<Expr>(Op); }\n  void setSubExpr(Expr *E) { Op = E; }\n\n  /// Retrieve the cast subexpression as it was written in the source\n  /// code, looking through any implicit casts or other intermediate nodes\n  /// introduced by semantic analysis.\n  Expr *getSubExprAsWritten();\n  const Expr *getSubExprAsWritten() const {\n    return const_cast<CastExpr *>(this)->getSubExprAsWritten();\n  }\n\n  /// If this cast applies a user-defined conversion, retrieve the conversion\n  /// function that it invokes.\n  NamedDecl *getConversionFunction() const;\n\n  typedef CXXBaseSpecifier **path_iterator;\n  typedef const CXXBaseSpecifier *const *path_const_iterator;\n  bool path_empty() const { return path_size() == 0; }\n  unsigned path_size() const { return CastExprBits.BasePathSize; }\n  path_iterator path_begin() { return path_buffer(); }\n  path_iterator path_end() { return path_buffer() + path_size(); }\n  path_const_iterator path_begin() const { return path_buffer(); }\n  path_const_iterator path_end() const { return path_buffer() + path_size(); }\n\n  llvm::iterator_range<path_iterator> path() {\n    return llvm::make_range(path_begin(), path_end());\n  }\n  llvm::iterator_range<path_const_iterator> path() const {\n    return llvm::make_range(path_begin(), path_end());\n  }\n\n  const FieldDecl *getTargetUnionField() const {\n    assert(getCastKind() == CK_ToUnion);\n    return getTargetFieldForToUnionCast(getType(), getSubExpr()->getType());\n  }\n\n  bool hasStoredFPFeatures() const { return CastExprBits.HasFPFeatures; }\n\n  /// Get FPOptionsOverride from trailing storage.\n  FPOptionsOverride getStoredFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return *getTrailingFPFeatures();\n  }\n\n  // Get the FP features status of this operation. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n\n  FPOptionsOverride getFPFeatures() const {\n    if (hasStoredFPFeatures())\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  static const FieldDecl *getTargetFieldForToUnionCast(QualType unionType,\n                                                       QualType opType);\n  static const FieldDecl *getTargetFieldForToUnionCast(const RecordDecl *RD,\n                                                       QualType opType);\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() >= firstCastExprConstant &&\n           T->getStmtClass() <= lastCastExprConstant;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Op, &Op+1); }\n  const_child_range children() const { return const_child_range(&Op, &Op + 1); }\n};\n\n/// ImplicitCastExpr - Allows us to explicitly represent implicit type\n/// conversions, which have no direct representation in the original\n/// source code. For example: converting T[]->T*, void f()->void\n/// (*f)(), float->double, short->int, etc.\n///\n/// In C, implicit casts always produce rvalues. However, in C++, an\n/// implicit cast whose result is being bound to a reference will be\n/// an lvalue or xvalue. For example:\n///\n/// @code\n/// class Base { };\n/// class Derived : public Base { };\n/// Derived &&ref();\n/// void f(Derived d) {\n///   Base& b = d; // initializer is an ImplicitCastExpr\n///                // to an lvalue of type Base\n///   Base&& r = ref(); // initializer is an ImplicitCastExpr\n///                     // to an xvalue of type Base\n/// }\n/// @endcode\nclass ImplicitCastExpr final\n    : public CastExpr,\n      private llvm::TrailingObjects<ImplicitCastExpr, CXXBaseSpecifier *,\n                                    FPOptionsOverride> {\n\n  ImplicitCastExpr(QualType ty, CastKind kind, Expr *op,\n                   unsigned BasePathLength, FPOptionsOverride FPO,\n                   ExprValueKind VK)\n      : CastExpr(ImplicitCastExprClass, ty, VK, kind, op, BasePathLength,\n                 FPO.requiresTrailingStorage()) {\n    if (hasStoredFPFeatures())\n      *getTrailingFPFeatures() = FPO;\n  }\n\n  /// Construct an empty implicit cast.\n  explicit ImplicitCastExpr(EmptyShell Shell, unsigned PathSize,\n                            bool HasFPFeatures)\n      : CastExpr(ImplicitCastExprClass, Shell, PathSize, HasFPFeatures) {}\n\n  unsigned numTrailingObjects(OverloadToken<CXXBaseSpecifier *>) const {\n    return path_size();\n  }\n\npublic:\n  enum OnStack_t { OnStack };\n  ImplicitCastExpr(OnStack_t _, QualType ty, CastKind kind, Expr *op,\n                   ExprValueKind VK, FPOptionsOverride FPO)\n      : CastExpr(ImplicitCastExprClass, ty, VK, kind, op, 0,\n                 FPO.requiresTrailingStorage()) {\n    if (hasStoredFPFeatures())\n      *getTrailingFPFeatures() = FPO;\n  }\n\n  bool isPartOfExplicitCast() const { return CastExprBits.PartOfExplicitCast; }\n  void setIsPartOfExplicitCast(bool PartOfExplicitCast) {\n    CastExprBits.PartOfExplicitCast = PartOfExplicitCast;\n  }\n\n  static ImplicitCastExpr *Create(const ASTContext &Context, QualType T,\n                                  CastKind Kind, Expr *Operand,\n                                  const CXXCastPath *BasePath,\n                                  ExprValueKind Cat, FPOptionsOverride FPO);\n\n  static ImplicitCastExpr *CreateEmpty(const ASTContext &Context,\n                                       unsigned PathSize, bool HasFPFeatures);\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getSubExpr()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getSubExpr()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ImplicitCastExprClass;\n  }\n\n  friend TrailingObjects;\n  friend class CastExpr;\n};\n\n/// ExplicitCastExpr - An explicit cast written in the source\n/// code.\n///\n/// This class is effectively an abstract class, because it provides\n/// the basic representation of an explicitly-written cast without\n/// specifying which kind of cast (C cast, functional cast, static\n/// cast, etc.) was written; specific derived classes represent the\n/// particular style of cast and its location information.\n///\n/// Unlike implicit casts, explicit cast nodes have two different\n/// types: the type that was written into the source code, and the\n/// actual type of the expression as determined by semantic\n/// analysis. These types may differ slightly. For example, in C++ one\n/// can cast to a reference type, which indicates that the resulting\n/// expression will be an lvalue or xvalue. The reference type, however,\n/// will not be used as the type of the expression.\nclass ExplicitCastExpr : public CastExpr {\n  /// TInfo - Source type info for the (written) type\n  /// this expression is casting to.\n  TypeSourceInfo *TInfo;\n\nprotected:\n  ExplicitCastExpr(StmtClass SC, QualType exprTy, ExprValueKind VK,\n                   CastKind kind, Expr *op, unsigned PathSize,\n                   bool HasFPFeatures, TypeSourceInfo *writtenTy)\n      : CastExpr(SC, exprTy, VK, kind, op, PathSize, HasFPFeatures),\n        TInfo(writtenTy) {}\n\n  /// Construct an empty explicit cast.\n  ExplicitCastExpr(StmtClass SC, EmptyShell Shell, unsigned PathSize,\n                   bool HasFPFeatures)\n      : CastExpr(SC, Shell, PathSize, HasFPFeatures) {}\n\npublic:\n  /// getTypeInfoAsWritten - Returns the type source info for the type\n  /// that this expression is casting to.\n  TypeSourceInfo *getTypeInfoAsWritten() const { return TInfo; }\n  void setTypeInfoAsWritten(TypeSourceInfo *writtenTy) { TInfo = writtenTy; }\n\n  /// getTypeAsWritten - Returns the type that this expression is\n  /// casting to, as written in the source code.\n  QualType getTypeAsWritten() const { return TInfo->getType(); }\n\n  static bool classof(const Stmt *T) {\n     return T->getStmtClass() >= firstExplicitCastExprConstant &&\n            T->getStmtClass() <= lastExplicitCastExprConstant;\n  }\n};\n\n/// CStyleCastExpr - An explicit cast in C (C99 6.5.4) or a C-style\n/// cast in C++ (C++ [expr.cast]), which uses the syntax\n/// (Type)expr. For example: @c (int)f.\nclass CStyleCastExpr final\n    : public ExplicitCastExpr,\n      private llvm::TrailingObjects<CStyleCastExpr, CXXBaseSpecifier *,\n                                    FPOptionsOverride> {\n  SourceLocation LPLoc; // the location of the left paren\n  SourceLocation RPLoc; // the location of the right paren\n\n  CStyleCastExpr(QualType exprTy, ExprValueKind vk, CastKind kind, Expr *op,\n                 unsigned PathSize, FPOptionsOverride FPO,\n                 TypeSourceInfo *writtenTy, SourceLocation l, SourceLocation r)\n      : ExplicitCastExpr(CStyleCastExprClass, exprTy, vk, kind, op, PathSize,\n                         FPO.requiresTrailingStorage(), writtenTy),\n        LPLoc(l), RPLoc(r) {\n    if (hasStoredFPFeatures())\n      *getTrailingFPFeatures() = FPO;\n  }\n\n  /// Construct an empty C-style explicit cast.\n  explicit CStyleCastExpr(EmptyShell Shell, unsigned PathSize,\n                          bool HasFPFeatures)\n      : ExplicitCastExpr(CStyleCastExprClass, Shell, PathSize, HasFPFeatures) {}\n\n  unsigned numTrailingObjects(OverloadToken<CXXBaseSpecifier *>) const {\n    return path_size();\n  }\n\npublic:\n  static CStyleCastExpr *\n  Create(const ASTContext &Context, QualType T, ExprValueKind VK, CastKind K,\n         Expr *Op, const CXXCastPath *BasePath, FPOptionsOverride FPO,\n         TypeSourceInfo *WrittenTy, SourceLocation L, SourceLocation R);\n\n  static CStyleCastExpr *CreateEmpty(const ASTContext &Context,\n                                     unsigned PathSize, bool HasFPFeatures);\n\n  SourceLocation getLParenLoc() const { return LPLoc; }\n  void setLParenLoc(SourceLocation L) { LPLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RPLoc; }\n  void setRParenLoc(SourceLocation L) { RPLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return LPLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getSubExpr()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == CStyleCastExprClass;\n  }\n\n  friend TrailingObjects;\n  friend class CastExpr;\n};\n\n/// A builtin binary operation expression such as \"x + y\" or \"x <= y\".\n///\n/// This expression node kind describes a builtin binary operation,\n/// such as \"x + y\" for integer values \"x\" and \"y\". The operands will\n/// already have been converted to appropriate types (e.g., by\n/// performing promotions or conversions).\n///\n/// In C++, where operators may be overloaded, a different kind of\n/// expression node (CXXOperatorCallExpr) is used to express the\n/// invocation of an overloaded operator with operator syntax. Within\n/// a C++ template, whether BinaryOperator or CXXOperatorCallExpr is\n/// used to store an expression \"x + y\" depends on the subexpressions\n/// for x and y. If neither x or y is type-dependent, and the \"+\"\n/// operator resolves to a built-in operation, BinaryOperator will be\n/// used to express the computation (x and y may still be\n/// value-dependent). If either x or y is type-dependent, or if the\n/// \"+\" resolves to an overloaded operator, CXXOperatorCallExpr will\n/// be used to express the computation.\nclass BinaryOperator : public Expr {\n  enum { LHS, RHS, END_EXPR };\n  Stmt *SubExprs[END_EXPR];\n\npublic:\n  typedef BinaryOperatorKind Opcode;\n\nprotected:\n  size_t offsetOfTrailingStorage() const;\n\n  /// Return a pointer to the trailing FPOptions\n  FPOptionsOverride *getTrailingFPFeatures() {\n    assert(BinaryOperatorBits.HasFPFeatures);\n    return reinterpret_cast<FPOptionsOverride *>(\n        reinterpret_cast<char *>(this) + offsetOfTrailingStorage());\n  }\n  const FPOptionsOverride *getTrailingFPFeatures() const {\n    assert(BinaryOperatorBits.HasFPFeatures);\n    return reinterpret_cast<const FPOptionsOverride *>(\n        reinterpret_cast<const char *>(this) + offsetOfTrailingStorage());\n  }\n\n  /// Build a binary operator, assuming that appropriate storage has been\n  /// allocated for the trailing objects when needed.\n  BinaryOperator(const ASTContext &Ctx, Expr *lhs, Expr *rhs, Opcode opc,\n                 QualType ResTy, ExprValueKind VK, ExprObjectKind OK,\n                 SourceLocation opLoc, FPOptionsOverride FPFeatures);\n\n  /// Construct an empty binary operator.\n  explicit BinaryOperator(EmptyShell Empty) : Expr(BinaryOperatorClass, Empty) {\n    BinaryOperatorBits.Opc = BO_Comma;\n  }\n\npublic:\n  static BinaryOperator *CreateEmpty(const ASTContext &C, bool hasFPFeatures);\n\n  static BinaryOperator *Create(const ASTContext &C, Expr *lhs, Expr *rhs,\n                                Opcode opc, QualType ResTy, ExprValueKind VK,\n                                ExprObjectKind OK, SourceLocation opLoc,\n                                FPOptionsOverride FPFeatures);\n  SourceLocation getExprLoc() const { return getOperatorLoc(); }\n  SourceLocation getOperatorLoc() const { return BinaryOperatorBits.OpLoc; }\n  void setOperatorLoc(SourceLocation L) { BinaryOperatorBits.OpLoc = L; }\n\n  Opcode getOpcode() const {\n    return static_cast<Opcode>(BinaryOperatorBits.Opc);\n  }\n  void setOpcode(Opcode Opc) { BinaryOperatorBits.Opc = Opc; }\n\n  Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  void setLHS(Expr *E) { SubExprs[LHS] = E; }\n  Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n  void setRHS(Expr *E) { SubExprs[RHS] = E; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getLHS()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getRHS()->getEndLoc();\n  }\n\n  /// getOpcodeStr - Turn an Opcode enum value into the punctuation char it\n  /// corresponds to, e.g. \"<<=\".\n  static StringRef getOpcodeStr(Opcode Op);\n\n  StringRef getOpcodeStr() const { return getOpcodeStr(getOpcode()); }\n\n  /// Retrieve the binary opcode that corresponds to the given\n  /// overloaded operator.\n  static Opcode getOverloadedOpcode(OverloadedOperatorKind OO);\n\n  /// Retrieve the overloaded operator kind that corresponds to\n  /// the given binary opcode.\n  static OverloadedOperatorKind getOverloadedOperator(Opcode Opc);\n\n  /// predicates to categorize the respective opcodes.\n  static bool isPtrMemOp(Opcode Opc) {\n    return Opc == BO_PtrMemD || Opc == BO_PtrMemI;\n  }\n  bool isPtrMemOp() const { return isPtrMemOp(getOpcode()); }\n\n  static bool isMultiplicativeOp(Opcode Opc) {\n    return Opc >= BO_Mul && Opc <= BO_Rem;\n  }\n  bool isMultiplicativeOp() const { return isMultiplicativeOp(getOpcode()); }\n  static bool isAdditiveOp(Opcode Opc) { return Opc == BO_Add || Opc==BO_Sub; }\n  bool isAdditiveOp() const { return isAdditiveOp(getOpcode()); }\n  static bool isShiftOp(Opcode Opc) { return Opc == BO_Shl || Opc == BO_Shr; }\n  bool isShiftOp() const { return isShiftOp(getOpcode()); }\n\n  static bool isBitwiseOp(Opcode Opc) { return Opc >= BO_And && Opc <= BO_Or; }\n  bool isBitwiseOp() const { return isBitwiseOp(getOpcode()); }\n\n  static bool isRelationalOp(Opcode Opc) { return Opc >= BO_LT && Opc<=BO_GE; }\n  bool isRelationalOp() const { return isRelationalOp(getOpcode()); }\n\n  static bool isEqualityOp(Opcode Opc) { return Opc == BO_EQ || Opc == BO_NE; }\n  bool isEqualityOp() const { return isEqualityOp(getOpcode()); }\n\n  static bool isComparisonOp(Opcode Opc) { return Opc >= BO_Cmp && Opc<=BO_NE; }\n  bool isComparisonOp() const { return isComparisonOp(getOpcode()); }\n\n  static bool isCommaOp(Opcode Opc) { return Opc == BO_Comma; }\n  bool isCommaOp() const { return isCommaOp(getOpcode()); }\n\n  static Opcode negateComparisonOp(Opcode Opc) {\n    switch (Opc) {\n    default:\n      llvm_unreachable(\"Not a comparison operator.\");\n    case BO_LT: return BO_GE;\n    case BO_GT: return BO_LE;\n    case BO_LE: return BO_GT;\n    case BO_GE: return BO_LT;\n    case BO_EQ: return BO_NE;\n    case BO_NE: return BO_EQ;\n    }\n  }\n\n  static Opcode reverseComparisonOp(Opcode Opc) {\n    switch (Opc) {\n    default:\n      llvm_unreachable(\"Not a comparison operator.\");\n    case BO_LT: return BO_GT;\n    case BO_GT: return BO_LT;\n    case BO_LE: return BO_GE;\n    case BO_GE: return BO_LE;\n    case BO_EQ:\n    case BO_NE:\n      return Opc;\n    }\n  }\n\n  static bool isLogicalOp(Opcode Opc) { return Opc == BO_LAnd || Opc==BO_LOr; }\n  bool isLogicalOp() const { return isLogicalOp(getOpcode()); }\n\n  static bool isAssignmentOp(Opcode Opc) {\n    return Opc >= BO_Assign && Opc <= BO_OrAssign;\n  }\n  bool isAssignmentOp() const { return isAssignmentOp(getOpcode()); }\n\n  static bool isCompoundAssignmentOp(Opcode Opc) {\n    return Opc > BO_Assign && Opc <= BO_OrAssign;\n  }\n  bool isCompoundAssignmentOp() const {\n    return isCompoundAssignmentOp(getOpcode());\n  }\n  static Opcode getOpForCompoundAssignment(Opcode Opc) {\n    assert(isCompoundAssignmentOp(Opc));\n    if (Opc >= BO_AndAssign)\n      return Opcode(unsigned(Opc) - BO_AndAssign + BO_And);\n    else\n      return Opcode(unsigned(Opc) - BO_MulAssign + BO_Mul);\n  }\n\n  static bool isShiftAssignOp(Opcode Opc) {\n    return Opc == BO_ShlAssign || Opc == BO_ShrAssign;\n  }\n  bool isShiftAssignOp() const {\n    return isShiftAssignOp(getOpcode());\n  }\n\n  // Return true if a binary operator using the specified opcode and operands\n  // would match the 'p = (i8*)nullptr + n' idiom for casting a pointer-sized\n  // integer to a pointer.\n  static bool isNullPointerArithmeticExtension(ASTContext &Ctx, Opcode Opc,\n                                               Expr *LHS, Expr *RHS);\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() >= firstBinaryOperatorConstant &&\n           S->getStmtClass() <= lastBinaryOperatorConstant;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n\n  /// Set and fetch the bit that shows whether FPFeatures needs to be\n  /// allocated in Trailing Storage\n  void setHasStoredFPFeatures(bool B) { BinaryOperatorBits.HasFPFeatures = B; }\n  bool hasStoredFPFeatures() const { return BinaryOperatorBits.HasFPFeatures; }\n\n  /// Get FPFeatures from trailing storage\n  FPOptionsOverride getStoredFPFeatures() const {\n    assert(hasStoredFPFeatures());\n    return *getTrailingFPFeatures();\n  }\n  /// Set FPFeatures in trailing storage, used only by Serialization\n  void setStoredFPFeatures(FPOptionsOverride F) {\n    assert(BinaryOperatorBits.HasFPFeatures);\n    *getTrailingFPFeatures() = F;\n  }\n\n  // Get the FP features status of this operator. Only meaningful for\n  // operations on floating point types.\n  FPOptions getFPFeaturesInEffect(const LangOptions &LO) const {\n    if (BinaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures().applyOverrides(LO);\n    return FPOptions::defaultWithoutTrailingStorage(LO);\n  }\n\n  // This is used in ASTImporter\n  FPOptionsOverride getFPFeatures(const LangOptions &LO) const {\n    if (BinaryOperatorBits.HasFPFeatures)\n      return getStoredFPFeatures();\n    return FPOptionsOverride();\n  }\n\n  // Get the FP contractability status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFPContractableWithinStatement(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).allowFPContractWithinStatement();\n  }\n\n  // Get the FENV_ACCESS status of this operator. Only meaningful for\n  // operations on floating point types.\n  bool isFEnvAccessOn(const LangOptions &LO) const {\n    return getFPFeaturesInEffect(LO).getAllowFEnvAccess();\n  }\n\nprotected:\n  BinaryOperator(const ASTContext &Ctx, Expr *lhs, Expr *rhs, Opcode opc,\n                 QualType ResTy, ExprValueKind VK, ExprObjectKind OK,\n                 SourceLocation opLoc, FPOptionsOverride FPFeatures,\n                 bool dead2);\n\n  /// Construct an empty BinaryOperator, SC is CompoundAssignOperator.\n  BinaryOperator(StmtClass SC, EmptyShell Empty) : Expr(SC, Empty) {\n    BinaryOperatorBits.Opc = BO_MulAssign;\n  }\n\n  /// Return the size in bytes needed for the trailing objects.\n  /// Used to allocate the right amount of storage.\n  static unsigned sizeOfTrailingObjects(bool HasFPFeatures) {\n    return HasFPFeatures * sizeof(FPOptionsOverride);\n  }\n};\n\n/// CompoundAssignOperator - For compound assignments (e.g. +=), we keep\n/// track of the type the operation is performed in.  Due to the semantics of\n/// these operators, the operands are promoted, the arithmetic performed, an\n/// implicit conversion back to the result type done, then the assignment takes\n/// place.  This captures the intermediate type which the computation is done\n/// in.\nclass CompoundAssignOperator : public BinaryOperator {\n  QualType ComputationLHSType;\n  QualType ComputationResultType;\n\n  /// Construct an empty CompoundAssignOperator.\n  explicit CompoundAssignOperator(const ASTContext &C, EmptyShell Empty,\n                                  bool hasFPFeatures)\n      : BinaryOperator(CompoundAssignOperatorClass, Empty) {}\n\nprotected:\n  CompoundAssignOperator(const ASTContext &C, Expr *lhs, Expr *rhs, Opcode opc,\n                         QualType ResType, ExprValueKind VK, ExprObjectKind OK,\n                         SourceLocation OpLoc, FPOptionsOverride FPFeatures,\n                         QualType CompLHSType, QualType CompResultType)\n      : BinaryOperator(C, lhs, rhs, opc, ResType, VK, OK, OpLoc, FPFeatures,\n                       true),\n        ComputationLHSType(CompLHSType), ComputationResultType(CompResultType) {\n    assert(isCompoundAssignmentOp() &&\n           \"Only should be used for compound assignments\");\n  }\n\npublic:\n  static CompoundAssignOperator *CreateEmpty(const ASTContext &C,\n                                             bool hasFPFeatures);\n\n  static CompoundAssignOperator *\n  Create(const ASTContext &C, Expr *lhs, Expr *rhs, Opcode opc, QualType ResTy,\n         ExprValueKind VK, ExprObjectKind OK, SourceLocation opLoc,\n         FPOptionsOverride FPFeatures, QualType CompLHSType = QualType(),\n         QualType CompResultType = QualType());\n\n  // The two computation types are the type the LHS is converted\n  // to for the computation and the type of the result; the two are\n  // distinct in a few cases (specifically, int+=ptr and ptr-=ptr).\n  QualType getComputationLHSType() const { return ComputationLHSType; }\n  void setComputationLHSType(QualType T) { ComputationLHSType = T; }\n\n  QualType getComputationResultType() const { return ComputationResultType; }\n  void setComputationResultType(QualType T) { ComputationResultType = T; }\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() == CompoundAssignOperatorClass;\n  }\n};\n\ninline size_t BinaryOperator::offsetOfTrailingStorage() const {\n  assert(BinaryOperatorBits.HasFPFeatures);\n  return isa<CompoundAssignOperator>(this) ? sizeof(CompoundAssignOperator)\n                                           : sizeof(BinaryOperator);\n}\n\n/// AbstractConditionalOperator - An abstract base class for\n/// ConditionalOperator and BinaryConditionalOperator.\nclass AbstractConditionalOperator : public Expr {\n  SourceLocation QuestionLoc, ColonLoc;\n  friend class ASTStmtReader;\n\nprotected:\n  AbstractConditionalOperator(StmtClass SC, QualType T, ExprValueKind VK,\n                              ExprObjectKind OK, SourceLocation qloc,\n                              SourceLocation cloc)\n      : Expr(SC, T, VK, OK), QuestionLoc(qloc), ColonLoc(cloc) {}\n\n  AbstractConditionalOperator(StmtClass SC, EmptyShell Empty)\n    : Expr(SC, Empty) { }\n\npublic:\n  // getCond - Return the expression representing the condition for\n  //   the ?: operator.\n  Expr *getCond() const;\n\n  // getTrueExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to true.\n  Expr *getTrueExpr() const;\n\n  // getFalseExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to false.  This is\n  //   the same as getRHS.\n  Expr *getFalseExpr() const;\n\n  SourceLocation getQuestionLoc() const { return QuestionLoc; }\n  SourceLocation getColonLoc() const { return ColonLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConditionalOperatorClass ||\n           T->getStmtClass() == BinaryConditionalOperatorClass;\n  }\n};\n\n/// ConditionalOperator - The ?: ternary operator.  The GNU \"missing\n/// middle\" extension is a BinaryConditionalOperator.\nclass ConditionalOperator : public AbstractConditionalOperator {\n  enum { COND, LHS, RHS, END_EXPR };\n  Stmt* SubExprs[END_EXPR]; // Left/Middle/Right hand sides.\n\n  friend class ASTStmtReader;\npublic:\n  ConditionalOperator(Expr *cond, SourceLocation QLoc, Expr *lhs,\n                      SourceLocation CLoc, Expr *rhs, QualType t,\n                      ExprValueKind VK, ExprObjectKind OK)\n      : AbstractConditionalOperator(ConditionalOperatorClass, t, VK, OK, QLoc,\n                                    CLoc) {\n    SubExprs[COND] = cond;\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty conditional operator.\n  explicit ConditionalOperator(EmptyShell Empty)\n    : AbstractConditionalOperator(ConditionalOperatorClass, Empty) { }\n\n  // getCond - Return the expression representing the condition for\n  //   the ?: operator.\n  Expr *getCond() const { return cast<Expr>(SubExprs[COND]); }\n\n  // getTrueExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to true.\n  Expr *getTrueExpr() const { return cast<Expr>(SubExprs[LHS]); }\n\n  // getFalseExpr - Return the subexpression representing the value of\n  //   the expression if the condition evaluates to false.  This is\n  //   the same as getRHS.\n  Expr *getFalseExpr() const { return cast<Expr>(SubExprs[RHS]); }\n\n  Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCond()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getRHS()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConditionalOperatorClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// BinaryConditionalOperator - The GNU extension to the conditional\n/// operator which allows the middle operand to be omitted.\n///\n/// This is a different expression kind on the assumption that almost\n/// every client ends up needing to know that these are different.\nclass BinaryConditionalOperator : public AbstractConditionalOperator {\n  enum { COMMON, COND, LHS, RHS, NUM_SUBEXPRS };\n\n  /// - the common condition/left-hand-side expression, which will be\n  ///   evaluated as the opaque value\n  /// - the condition, expressed in terms of the opaque value\n  /// - the left-hand-side, expressed in terms of the opaque value\n  /// - the right-hand-side\n  Stmt *SubExprs[NUM_SUBEXPRS];\n  OpaqueValueExpr *OpaqueValue;\n\n  friend class ASTStmtReader;\npublic:\n  BinaryConditionalOperator(Expr *common, OpaqueValueExpr *opaqueValue,\n                            Expr *cond, Expr *lhs, Expr *rhs,\n                            SourceLocation qloc, SourceLocation cloc,\n                            QualType t, ExprValueKind VK, ExprObjectKind OK)\n      : AbstractConditionalOperator(BinaryConditionalOperatorClass, t, VK, OK,\n                                    qloc, cloc),\n        OpaqueValue(opaqueValue) {\n    SubExprs[COMMON] = common;\n    SubExprs[COND] = cond;\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n    assert(OpaqueValue->getSourceExpr() == common && \"Wrong opaque value\");\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty conditional operator.\n  explicit BinaryConditionalOperator(EmptyShell Empty)\n    : AbstractConditionalOperator(BinaryConditionalOperatorClass, Empty) { }\n\n  /// getCommon - Return the common expression, written to the\n  ///   left of the condition.  The opaque value will be bound to the\n  ///   result of this expression.\n  Expr *getCommon() const { return cast<Expr>(SubExprs[COMMON]); }\n\n  /// getOpaqueValue - Return the opaque value placeholder.\n  OpaqueValueExpr *getOpaqueValue() const { return OpaqueValue; }\n\n  /// getCond - Return the condition expression; this is defined\n  ///   in terms of the opaque value.\n  Expr *getCond() const { return cast<Expr>(SubExprs[COND]); }\n\n  /// getTrueExpr - Return the subexpression which will be\n  ///   evaluated if the condition evaluates to true;  this is defined\n  ///   in terms of the opaque value.\n  Expr *getTrueExpr() const {\n    return cast<Expr>(SubExprs[LHS]);\n  }\n\n  /// getFalseExpr - Return the subexpression which will be\n  ///   evaluated if the condnition evaluates to false; this is\n  ///   defined in terms of the opaque value.\n  Expr *getFalseExpr() const {\n    return cast<Expr>(SubExprs[RHS]);\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCommon()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getFalseExpr()->getEndLoc();\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == BinaryConditionalOperatorClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(SubExprs, SubExprs + NUM_SUBEXPRS);\n  }\n  const_child_range children() const {\n    return const_child_range(SubExprs, SubExprs + NUM_SUBEXPRS);\n  }\n};\n\ninline Expr *AbstractConditionalOperator::getCond() const {\n  if (const ConditionalOperator *co = dyn_cast<ConditionalOperator>(this))\n    return co->getCond();\n  return cast<BinaryConditionalOperator>(this)->getCond();\n}\n\ninline Expr *AbstractConditionalOperator::getTrueExpr() const {\n  if (const ConditionalOperator *co = dyn_cast<ConditionalOperator>(this))\n    return co->getTrueExpr();\n  return cast<BinaryConditionalOperator>(this)->getTrueExpr();\n}\n\ninline Expr *AbstractConditionalOperator::getFalseExpr() const {\n  if (const ConditionalOperator *co = dyn_cast<ConditionalOperator>(this))\n    return co->getFalseExpr();\n  return cast<BinaryConditionalOperator>(this)->getFalseExpr();\n}\n\n/// AddrLabelExpr - The GNU address of label extension, representing &&label.\nclass AddrLabelExpr : public Expr {\n  SourceLocation AmpAmpLoc, LabelLoc;\n  LabelDecl *Label;\npublic:\n  AddrLabelExpr(SourceLocation AALoc, SourceLocation LLoc, LabelDecl *L,\n                QualType t)\n      : Expr(AddrLabelExprClass, t, VK_RValue, OK_Ordinary), AmpAmpLoc(AALoc),\n        LabelLoc(LLoc), Label(L) {\n    setDependence(ExprDependence::None);\n  }\n\n  /// Build an empty address of a label expression.\n  explicit AddrLabelExpr(EmptyShell Empty)\n    : Expr(AddrLabelExprClass, Empty) { }\n\n  SourceLocation getAmpAmpLoc() const { return AmpAmpLoc; }\n  void setAmpAmpLoc(SourceLocation L) { AmpAmpLoc = L; }\n  SourceLocation getLabelLoc() const { return LabelLoc; }\n  void setLabelLoc(SourceLocation L) { LabelLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return AmpAmpLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return LabelLoc; }\n\n  LabelDecl *getLabel() const { return Label; }\n  void setLabel(LabelDecl *L) { Label = L; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == AddrLabelExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// StmtExpr - This is the GNU Statement Expression extension: ({int X=4; X;}).\n/// The StmtExpr contains a single CompoundStmt node, which it evaluates and\n/// takes the value of the last subexpression.\n///\n/// A StmtExpr is always an r-value; values \"returned\" out of a\n/// StmtExpr will be copied.\nclass StmtExpr : public Expr {\n  Stmt *SubStmt;\n  SourceLocation LParenLoc, RParenLoc;\npublic:\n  StmtExpr(CompoundStmt *SubStmt, QualType T, SourceLocation LParenLoc,\n           SourceLocation RParenLoc, unsigned TemplateDepth)\n      : Expr(StmtExprClass, T, VK_RValue, OK_Ordinary), SubStmt(SubStmt),\n        LParenLoc(LParenLoc), RParenLoc(RParenLoc) {\n    setDependence(computeDependence(this, TemplateDepth));\n    // FIXME: A templated statement expression should have an associated\n    // DeclContext so that nested declarations always have a dependent context.\n    StmtExprBits.TemplateDepth = TemplateDepth;\n  }\n\n  /// Build an empty statement expression.\n  explicit StmtExpr(EmptyShell Empty) : Expr(StmtExprClass, Empty) { }\n\n  CompoundStmt *getSubStmt() { return cast<CompoundStmt>(SubStmt); }\n  const CompoundStmt *getSubStmt() const { return cast<CompoundStmt>(SubStmt); }\n  void setSubStmt(CompoundStmt *S) { SubStmt = S; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return LParenLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  SourceLocation getLParenLoc() const { return LParenLoc; }\n  void setLParenLoc(SourceLocation L) { LParenLoc = L; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  unsigned getTemplateDepth() const { return StmtExprBits.TemplateDepth; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == StmtExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&SubStmt, &SubStmt+1); }\n  const_child_range children() const {\n    return const_child_range(&SubStmt, &SubStmt + 1);\n  }\n};\n\n/// ShuffleVectorExpr - clang-specific builtin-in function\n/// __builtin_shufflevector.\n/// This AST node represents a operator that does a constant\n/// shuffle, similar to LLVM's shufflevector instruction. It takes\n/// two vectors and a variable number of constant indices,\n/// and returns the appropriately shuffled vector.\nclass ShuffleVectorExpr : public Expr {\n  SourceLocation BuiltinLoc, RParenLoc;\n\n  // SubExprs - the list of values passed to the __builtin_shufflevector\n  // function. The first two are vectors, and the rest are constant\n  // indices.  The number of values in this list is always\n  // 2+the number of indices in the vector type.\n  Stmt **SubExprs;\n  unsigned NumExprs;\n\npublic:\n  ShuffleVectorExpr(const ASTContext &C, ArrayRef<Expr*> args, QualType Type,\n                    SourceLocation BLoc, SourceLocation RP);\n\n  /// Build an empty vector-shuffle expression.\n  explicit ShuffleVectorExpr(EmptyShell Empty)\n    : Expr(ShuffleVectorExprClass, Empty), SubExprs(nullptr) { }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  void setBuiltinLoc(SourceLocation L) { BuiltinLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ShuffleVectorExprClass;\n  }\n\n  /// getNumSubExprs - Return the size of the SubExprs array.  This includes the\n  /// constant expression, the actual arguments passed in, and the function\n  /// pointers.\n  unsigned getNumSubExprs() const { return NumExprs; }\n\n  /// Retrieve the array of expressions.\n  Expr **getSubExprs() { return reinterpret_cast<Expr **>(SubExprs); }\n\n  /// getExpr - Return the Expr at the specified index.\n  Expr *getExpr(unsigned Index) {\n    assert((Index < NumExprs) && \"Arg access out of range!\");\n    return cast<Expr>(SubExprs[Index]);\n  }\n  const Expr *getExpr(unsigned Index) const {\n    assert((Index < NumExprs) && \"Arg access out of range!\");\n    return cast<Expr>(SubExprs[Index]);\n  }\n\n  void setExprs(const ASTContext &C, ArrayRef<Expr *> Exprs);\n\n  llvm::APSInt getShuffleMaskIdx(const ASTContext &Ctx, unsigned N) const {\n    assert((N < NumExprs - 2) && \"Shuffle idx out of range!\");\n    return getExpr(N+2)->EvaluateKnownConstInt(Ctx);\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+NumExprs);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + NumExprs);\n  }\n};\n\n/// ConvertVectorExpr - Clang builtin function __builtin_convertvector\n/// This AST node provides support for converting a vector type to another\n/// vector type of the same arity.\nclass ConvertVectorExpr : public Expr {\nprivate:\n  Stmt *SrcExpr;\n  TypeSourceInfo *TInfo;\n  SourceLocation BuiltinLoc, RParenLoc;\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  explicit ConvertVectorExpr(EmptyShell Empty) : Expr(ConvertVectorExprClass, Empty) {}\n\npublic:\n  ConvertVectorExpr(Expr *SrcExpr, TypeSourceInfo *TI, QualType DstType,\n                    ExprValueKind VK, ExprObjectKind OK,\n                    SourceLocation BuiltinLoc, SourceLocation RParenLoc)\n      : Expr(ConvertVectorExprClass, DstType, VK, OK), SrcExpr(SrcExpr),\n        TInfo(TI), BuiltinLoc(BuiltinLoc), RParenLoc(RParenLoc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// getSrcExpr - Return the Expr to be converted.\n  Expr *getSrcExpr() const { return cast<Expr>(SrcExpr); }\n\n  /// getTypeSourceInfo - Return the destination type.\n  TypeSourceInfo *getTypeSourceInfo() const {\n    return TInfo;\n  }\n  void setTypeSourceInfo(TypeSourceInfo *ti) {\n    TInfo = ti;\n  }\n\n  /// getBuiltinLoc - Return the location of the __builtin_convertvector token.\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n\n  /// getRParenLoc - Return the location of final right parenthesis.\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ConvertVectorExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&SrcExpr, &SrcExpr+1); }\n  const_child_range children() const {\n    return const_child_range(&SrcExpr, &SrcExpr + 1);\n  }\n};\n\n/// ChooseExpr - GNU builtin-in function __builtin_choose_expr.\n/// This AST node is similar to the conditional operator (?:) in C, with\n/// the following exceptions:\n/// - the test expression must be a integer constant expression.\n/// - the expression returned acts like the chosen subexpression in every\n///   visible way: the type is the same as that of the chosen subexpression,\n///   and all predicates (whether it's an l-value, whether it's an integer\n///   constant expression, etc.) return the same result as for the chosen\n///   sub-expression.\nclass ChooseExpr : public Expr {\n  enum { COND, LHS, RHS, END_EXPR };\n  Stmt* SubExprs[END_EXPR]; // Left/Middle/Right hand sides.\n  SourceLocation BuiltinLoc, RParenLoc;\n  bool CondIsTrue;\npublic:\n  ChooseExpr(SourceLocation BLoc, Expr *cond, Expr *lhs, Expr *rhs, QualType t,\n             ExprValueKind VK, ExprObjectKind OK, SourceLocation RP,\n             bool condIsTrue)\n      : Expr(ChooseExprClass, t, VK, OK), BuiltinLoc(BLoc), RParenLoc(RP),\n        CondIsTrue(condIsTrue) {\n    SubExprs[COND] = cond;\n    SubExprs[LHS] = lhs;\n    SubExprs[RHS] = rhs;\n\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty __builtin_choose_expr.\n  explicit ChooseExpr(EmptyShell Empty) : Expr(ChooseExprClass, Empty) { }\n\n  /// isConditionTrue - Return whether the condition is true (i.e. not\n  /// equal to zero).\n  bool isConditionTrue() const {\n    assert(!isConditionDependent() &&\n           \"Dependent condition isn't true or false\");\n    return CondIsTrue;\n  }\n  void setIsConditionTrue(bool isTrue) { CondIsTrue = isTrue; }\n\n  bool isConditionDependent() const {\n    return getCond()->isTypeDependent() || getCond()->isValueDependent();\n  }\n\n  /// getChosenSubExpr - Return the subexpression chosen according to the\n  /// condition.\n  Expr *getChosenSubExpr() const {\n    return isConditionTrue() ? getLHS() : getRHS();\n  }\n\n  Expr *getCond() const { return cast<Expr>(SubExprs[COND]); }\n  void setCond(Expr *E) { SubExprs[COND] = E; }\n  Expr *getLHS() const { return cast<Expr>(SubExprs[LHS]); }\n  void setLHS(Expr *E) { SubExprs[LHS] = E; }\n  Expr *getRHS() const { return cast<Expr>(SubExprs[RHS]); }\n  void setRHS(Expr *E) { SubExprs[RHS] = E; }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  void setBuiltinLoc(SourceLocation L) { BuiltinLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ChooseExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(&SubExprs[0], &SubExprs[0]+END_EXPR);\n  }\n  const_child_range children() const {\n    return const_child_range(&SubExprs[0], &SubExprs[0] + END_EXPR);\n  }\n};\n\n/// GNUNullExpr - Implements the GNU __null extension, which is a name\n/// for a null pointer constant that has integral type (e.g., int or\n/// long) and is the same size and alignment as a pointer. The __null\n/// extension is typically only used by system headers, which define\n/// NULL as __null in C++ rather than using 0 (which is an integer\n/// that may not match the size of a pointer).\nclass GNUNullExpr : public Expr {\n  /// TokenLoc - The location of the __null keyword.\n  SourceLocation TokenLoc;\n\npublic:\n  GNUNullExpr(QualType Ty, SourceLocation Loc)\n      : Expr(GNUNullExprClass, Ty, VK_RValue, OK_Ordinary), TokenLoc(Loc) {\n    setDependence(ExprDependence::None);\n  }\n\n  /// Build an empty GNU __null expression.\n  explicit GNUNullExpr(EmptyShell Empty) : Expr(GNUNullExprClass, Empty) { }\n\n  /// getTokenLocation - The location of the __null token.\n  SourceLocation getTokenLocation() const { return TokenLoc; }\n  void setTokenLocation(SourceLocation L) { TokenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return TokenLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return TokenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == GNUNullExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// Represents a call to the builtin function \\c __builtin_va_arg.\nclass VAArgExpr : public Expr {\n  Stmt *Val;\n  llvm::PointerIntPair<TypeSourceInfo *, 1, bool> TInfo;\n  SourceLocation BuiltinLoc, RParenLoc;\npublic:\n  VAArgExpr(SourceLocation BLoc, Expr *e, TypeSourceInfo *TInfo,\n            SourceLocation RPLoc, QualType t, bool IsMS)\n      : Expr(VAArgExprClass, t, VK_RValue, OK_Ordinary), Val(e),\n        TInfo(TInfo, IsMS), BuiltinLoc(BLoc), RParenLoc(RPLoc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Create an empty __builtin_va_arg expression.\n  explicit VAArgExpr(EmptyShell Empty)\n      : Expr(VAArgExprClass, Empty), Val(nullptr), TInfo(nullptr, false) {}\n\n  const Expr *getSubExpr() const { return cast<Expr>(Val); }\n  Expr *getSubExpr() { return cast<Expr>(Val); }\n  void setSubExpr(Expr *E) { Val = E; }\n\n  /// Returns whether this is really a Win64 ABI va_arg expression.\n  bool isMicrosoftABI() const { return TInfo.getInt(); }\n  void setIsMicrosoftABI(bool IsMS) { TInfo.setInt(IsMS); }\n\n  TypeSourceInfo *getWrittenTypeInfo() const { return TInfo.getPointer(); }\n  void setWrittenTypeInfo(TypeSourceInfo *TI) { TInfo.setPointer(TI); }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  void setBuiltinLoc(SourceLocation L) { BuiltinLoc = L; }\n\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  void setRParenLoc(SourceLocation L) { RParenLoc = L; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == VAArgExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Val, &Val+1); }\n  const_child_range children() const {\n    return const_child_range(&Val, &Val + 1);\n  }\n};\n\n/// Represents a function call to one of __builtin_LINE(), __builtin_COLUMN(),\n/// __builtin_FUNCTION(), or __builtin_FILE().\nclass SourceLocExpr final : public Expr {\n  SourceLocation BuiltinLoc, RParenLoc;\n  DeclContext *ParentContext;\n\npublic:\n  enum IdentKind { Function, File, Line, Column };\n\n  SourceLocExpr(const ASTContext &Ctx, IdentKind Type, SourceLocation BLoc,\n                SourceLocation RParenLoc, DeclContext *Context);\n\n  /// Build an empty call expression.\n  explicit SourceLocExpr(EmptyShell Empty) : Expr(SourceLocExprClass, Empty) {}\n\n  /// Return the result of evaluating this SourceLocExpr in the specified\n  /// (and possibly null) default argument or initialization context.\n  APValue EvaluateInContext(const ASTContext &Ctx,\n                            const Expr *DefaultExpr) const;\n\n  /// Return a string representing the name of the specific builtin function.\n  StringRef getBuiltinStr() const;\n\n  IdentKind getIdentKind() const {\n    return static_cast<IdentKind>(SourceLocExprBits.Kind);\n  }\n\n  bool isStringType() const {\n    switch (getIdentKind()) {\n    case File:\n    case Function:\n      return true;\n    case Line:\n    case Column:\n      return false;\n    }\n    llvm_unreachable(\"unknown source location expression kind\");\n  }\n  bool isIntType() const LLVM_READONLY { return !isStringType(); }\n\n  /// If the SourceLocExpr has been resolved return the subexpression\n  /// representing the resolved value. Otherwise return null.\n  const DeclContext *getParentContext() const { return ParentContext; }\n  DeclContext *getParentContext() { return ParentContext; }\n\n  SourceLocation getLocation() const { return BuiltinLoc; }\n  SourceLocation getBeginLoc() const { return BuiltinLoc; }\n  SourceLocation getEndLoc() const { return RParenLoc; }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n\n  const_child_range children() const {\n    return const_child_range(child_iterator(), child_iterator());\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == SourceLocExprClass;\n  }\n\nprivate:\n  friend class ASTStmtReader;\n};\n\n/// Describes an C or C++ initializer list.\n///\n/// InitListExpr describes an initializer list, which can be used to\n/// initialize objects of different types, including\n/// struct/class/union types, arrays, and vectors. For example:\n///\n/// @code\n/// struct foo x = { 1, { 2, 3 } };\n/// @endcode\n///\n/// Prior to semantic analysis, an initializer list will represent the\n/// initializer list as written by the user, but will have the\n/// placeholder type \"void\". This initializer list is called the\n/// syntactic form of the initializer, and may contain C99 designated\n/// initializers (represented as DesignatedInitExprs), initializations\n/// of subobject members without explicit braces, and so on. Clients\n/// interested in the original syntax of the initializer list should\n/// use the syntactic form of the initializer list.\n///\n/// After semantic analysis, the initializer list will represent the\n/// semantic form of the initializer, where the initializations of all\n/// subobjects are made explicit with nested InitListExpr nodes and\n/// C99 designators have been eliminated by placing the designated\n/// initializations into the subobject they initialize. Additionally,\n/// any \"holes\" in the initialization, where no initializer has been\n/// specified for a particular subobject, will be replaced with\n/// implicitly-generated ImplicitValueInitExpr expressions that\n/// value-initialize the subobjects. Note, however, that the\n/// initializer lists may still have fewer initializers than there are\n/// elements to initialize within the object.\n///\n/// After semantic analysis has completed, given an initializer list,\n/// method isSemanticForm() returns true if and only if this is the\n/// semantic form of the initializer list (note: the same AST node\n/// may at the same time be the syntactic form).\n/// Given the semantic form of the initializer list, one can retrieve\n/// the syntactic form of that initializer list (when different)\n/// using method getSyntacticForm(); the method returns null if applied\n/// to a initializer list which is already in syntactic form.\n/// Similarly, given the syntactic form (i.e., an initializer list such\n/// that isSemanticForm() returns false), one can retrieve the semantic\n/// form using method getSemanticForm().\n/// Since many initializer lists have the same syntactic and semantic forms,\n/// getSyntacticForm() may return NULL, indicating that the current\n/// semantic initializer list also serves as its syntactic form.\nclass InitListExpr : public Expr {\n  // FIXME: Eliminate this vector in favor of ASTContext allocation\n  typedef ASTVector<Stmt *> InitExprsTy;\n  InitExprsTy InitExprs;\n  SourceLocation LBraceLoc, RBraceLoc;\n\n  /// The alternative form of the initializer list (if it exists).\n  /// The int part of the pair stores whether this initializer list is\n  /// in semantic form. If not null, the pointer points to:\n  ///   - the syntactic form, if this is in semantic form;\n  ///   - the semantic form, if this is in syntactic form.\n  llvm::PointerIntPair<InitListExpr *, 1, bool> AltForm;\n\n  /// Either:\n  ///  If this initializer list initializes an array with more elements than\n  ///  there are initializers in the list, specifies an expression to be used\n  ///  for value initialization of the rest of the elements.\n  /// Or\n  ///  If this initializer list initializes a union, specifies which\n  ///  field within the union will be initialized.\n  llvm::PointerUnion<Expr *, FieldDecl *> ArrayFillerOrUnionFieldInit;\n\npublic:\n  InitListExpr(const ASTContext &C, SourceLocation lbraceloc,\n               ArrayRef<Expr*> initExprs, SourceLocation rbraceloc);\n\n  /// Build an empty initializer list.\n  explicit InitListExpr(EmptyShell Empty)\n    : Expr(InitListExprClass, Empty), AltForm(nullptr, true) { }\n\n  unsigned getNumInits() const { return InitExprs.size(); }\n\n  /// Retrieve the set of initializers.\n  Expr **getInits() { return reinterpret_cast<Expr **>(InitExprs.data()); }\n\n  /// Retrieve the set of initializers.\n  Expr * const *getInits() const {\n    return reinterpret_cast<Expr * const *>(InitExprs.data());\n  }\n\n  ArrayRef<Expr *> inits() {\n    return llvm::makeArrayRef(getInits(), getNumInits());\n  }\n\n  ArrayRef<Expr *> inits() const {\n    return llvm::makeArrayRef(getInits(), getNumInits());\n  }\n\n  const Expr *getInit(unsigned Init) const {\n    assert(Init < getNumInits() && \"Initializer access out of range!\");\n    return cast_or_null<Expr>(InitExprs[Init]);\n  }\n\n  Expr *getInit(unsigned Init) {\n    assert(Init < getNumInits() && \"Initializer access out of range!\");\n    return cast_or_null<Expr>(InitExprs[Init]);\n  }\n\n  void setInit(unsigned Init, Expr *expr) {\n    assert(Init < getNumInits() && \"Initializer access out of range!\");\n    InitExprs[Init] = expr;\n\n    if (expr)\n      setDependence(getDependence() | expr->getDependence());\n  }\n\n  /// Mark the semantic form of the InitListExpr as error when the semantic\n  /// analysis fails.\n  void markError() {\n    assert(isSemanticForm());\n    setDependence(getDependence() | ExprDependence::ErrorDependent);\n  }\n\n  /// Reserve space for some number of initializers.\n  void reserveInits(const ASTContext &C, unsigned NumInits);\n\n  /// Specify the number of initializers\n  ///\n  /// If there are more than @p NumInits initializers, the remaining\n  /// initializers will be destroyed. If there are fewer than @p\n  /// NumInits initializers, NULL expressions will be added for the\n  /// unknown initializers.\n  void resizeInits(const ASTContext &Context, unsigned NumInits);\n\n  /// Updates the initializer at index @p Init with the new\n  /// expression @p expr, and returns the old expression at that\n  /// location.\n  ///\n  /// When @p Init is out of range for this initializer list, the\n  /// initializer list will be extended with NULL expressions to\n  /// accommodate the new entry.\n  Expr *updateInit(const ASTContext &C, unsigned Init, Expr *expr);\n\n  /// If this initializer list initializes an array with more elements\n  /// than there are initializers in the list, specifies an expression to be\n  /// used for value initialization of the rest of the elements.\n  Expr *getArrayFiller() {\n    return ArrayFillerOrUnionFieldInit.dyn_cast<Expr *>();\n  }\n  const Expr *getArrayFiller() const {\n    return const_cast<InitListExpr *>(this)->getArrayFiller();\n  }\n  void setArrayFiller(Expr *filler);\n\n  /// Return true if this is an array initializer and its array \"filler\"\n  /// has been set.\n  bool hasArrayFiller() const { return getArrayFiller(); }\n\n  /// If this initializes a union, specifies which field in the\n  /// union to initialize.\n  ///\n  /// Typically, this field is the first named field within the\n  /// union. However, a designated initializer can specify the\n  /// initialization of a different field within the union.\n  FieldDecl *getInitializedFieldInUnion() {\n    return ArrayFillerOrUnionFieldInit.dyn_cast<FieldDecl *>();\n  }\n  const FieldDecl *getInitializedFieldInUnion() const {\n    return const_cast<InitListExpr *>(this)->getInitializedFieldInUnion();\n  }\n  void setInitializedFieldInUnion(FieldDecl *FD) {\n    assert((FD == nullptr\n            || getInitializedFieldInUnion() == nullptr\n            || getInitializedFieldInUnion() == FD)\n           && \"Only one field of a union may be initialized at a time!\");\n    ArrayFillerOrUnionFieldInit = FD;\n  }\n\n  // Explicit InitListExpr's originate from source code (and have valid source\n  // locations). Implicit InitListExpr's are created by the semantic analyzer.\n  // FIXME: This is wrong; InitListExprs created by semantic analysis have\n  // valid source locations too!\n  bool isExplicit() const {\n    return LBraceLoc.isValid() && RBraceLoc.isValid();\n  }\n\n  // Is this an initializer for an array of characters, initialized by a string\n  // literal or an @encode?\n  bool isStringLiteralInit() const;\n\n  /// Is this a transparent initializer list (that is, an InitListExpr that is\n  /// purely syntactic, and whose semantics are that of the sole contained\n  /// initializer)?\n  bool isTransparent() const;\n\n  /// Is this the zero initializer {0} in a language which considers it\n  /// idiomatic?\n  bool isIdiomaticZeroInitializer(const LangOptions &LangOpts) const;\n\n  SourceLocation getLBraceLoc() const { return LBraceLoc; }\n  void setLBraceLoc(SourceLocation Loc) { LBraceLoc = Loc; }\n  SourceLocation getRBraceLoc() const { return RBraceLoc; }\n  void setRBraceLoc(SourceLocation Loc) { RBraceLoc = Loc; }\n\n  bool isSemanticForm() const { return AltForm.getInt(); }\n  InitListExpr *getSemanticForm() const {\n    return isSemanticForm() ? nullptr : AltForm.getPointer();\n  }\n  bool isSyntacticForm() const {\n    return !AltForm.getInt() || !AltForm.getPointer();\n  }\n  InitListExpr *getSyntacticForm() const {\n    return isSemanticForm() ? AltForm.getPointer() : nullptr;\n  }\n\n  void setSyntacticForm(InitListExpr *Init) {\n    AltForm.setPointer(Init);\n    AltForm.setInt(true);\n    Init->AltForm.setPointer(this);\n    Init->AltForm.setInt(false);\n  }\n\n  bool hadArrayRangeDesignator() const {\n    return InitListExprBits.HadArrayRangeDesignator != 0;\n  }\n  void sawArrayRangeDesignator(bool ARD = true) {\n    InitListExprBits.HadArrayRangeDesignator = ARD;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == InitListExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    const_child_range CCR = const_cast<const InitListExpr *>(this)->children();\n    return child_range(cast_away_const(CCR.begin()),\n                       cast_away_const(CCR.end()));\n  }\n\n  const_child_range children() const {\n    // FIXME: This does not include the array filler expression.\n    if (InitExprs.empty())\n      return const_child_range(const_child_iterator(), const_child_iterator());\n    return const_child_range(&InitExprs[0], &InitExprs[0] + InitExprs.size());\n  }\n\n  typedef InitExprsTy::iterator iterator;\n  typedef InitExprsTy::const_iterator const_iterator;\n  typedef InitExprsTy::reverse_iterator reverse_iterator;\n  typedef InitExprsTy::const_reverse_iterator const_reverse_iterator;\n\n  iterator begin() { return InitExprs.begin(); }\n  const_iterator begin() const { return InitExprs.begin(); }\n  iterator end() { return InitExprs.end(); }\n  const_iterator end() const { return InitExprs.end(); }\n  reverse_iterator rbegin() { return InitExprs.rbegin(); }\n  const_reverse_iterator rbegin() const { return InitExprs.rbegin(); }\n  reverse_iterator rend() { return InitExprs.rend(); }\n  const_reverse_iterator rend() const { return InitExprs.rend(); }\n\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n/// Represents a C99 designated initializer expression.\n///\n/// A designated initializer expression (C99 6.7.8) contains one or\n/// more designators (which can be field designators, array\n/// designators, or GNU array-range designators) followed by an\n/// expression that initializes the field or element(s) that the\n/// designators refer to. For example, given:\n///\n/// @code\n/// struct point {\n///   double x;\n///   double y;\n/// };\n/// struct point ptarray[10] = { [2].y = 1.0, [2].x = 2.0, [0].x = 1.0 };\n/// @endcode\n///\n/// The InitListExpr contains three DesignatedInitExprs, the first of\n/// which covers @c [2].y=1.0. This DesignatedInitExpr will have two\n/// designators, one array designator for @c [2] followed by one field\n/// designator for @c .y. The initialization expression will be 1.0.\nclass DesignatedInitExpr final\n    : public Expr,\n      private llvm::TrailingObjects<DesignatedInitExpr, Stmt *> {\npublic:\n  /// Forward declaration of the Designator class.\n  class Designator;\n\nprivate:\n  /// The location of the '=' or ':' prior to the actual initializer\n  /// expression.\n  SourceLocation EqualOrColonLoc;\n\n  /// Whether this designated initializer used the GNU deprecated\n  /// syntax rather than the C99 '=' syntax.\n  unsigned GNUSyntax : 1;\n\n  /// The number of designators in this initializer expression.\n  unsigned NumDesignators : 15;\n\n  /// The number of subexpressions of this initializer expression,\n  /// which contains both the initializer and any additional\n  /// expressions used by array and array-range designators.\n  unsigned NumSubExprs : 16;\n\n  /// The designators in this designated initialization\n  /// expression.\n  Designator *Designators;\n\n  DesignatedInitExpr(const ASTContext &C, QualType Ty,\n                     llvm::ArrayRef<Designator> Designators,\n                     SourceLocation EqualOrColonLoc, bool GNUSyntax,\n                     ArrayRef<Expr *> IndexExprs, Expr *Init);\n\n  explicit DesignatedInitExpr(unsigned NumSubExprs)\n    : Expr(DesignatedInitExprClass, EmptyShell()),\n      NumDesignators(0), NumSubExprs(NumSubExprs), Designators(nullptr) { }\n\npublic:\n  /// A field designator, e.g., \".x\".\n  struct FieldDesignator {\n    /// Refers to the field that is being initialized. The low bit\n    /// of this field determines whether this is actually a pointer\n    /// to an IdentifierInfo (if 1) or a FieldDecl (if 0). When\n    /// initially constructed, a field designator will store an\n    /// IdentifierInfo*. After semantic analysis has resolved that\n    /// name, the field designator will instead store a FieldDecl*.\n    uintptr_t NameOrField;\n\n    /// The location of the '.' in the designated initializer.\n    SourceLocation DotLoc;\n\n    /// The location of the field name in the designated initializer.\n    SourceLocation FieldLoc;\n  };\n\n  /// An array or GNU array-range designator, e.g., \"[9]\" or \"[10..15]\".\n  struct ArrayOrRangeDesignator {\n    /// Location of the first index expression within the designated\n    /// initializer expression's list of subexpressions.\n    unsigned Index;\n    /// The location of the '[' starting the array range designator.\n    SourceLocation LBracketLoc;\n    /// The location of the ellipsis separating the start and end\n    /// indices. Only valid for GNU array-range designators.\n    SourceLocation EllipsisLoc;\n    /// The location of the ']' terminating the array range designator.\n    SourceLocation RBracketLoc;\n  };\n\n  /// Represents a single C99 designator.\n  ///\n  /// @todo This class is infuriatingly similar to clang::Designator,\n  /// but minor differences (storing indices vs. storing pointers)\n  /// keep us from reusing it. Try harder, later, to rectify these\n  /// differences.\n  class Designator {\n    /// The kind of designator this describes.\n    enum {\n      FieldDesignator,\n      ArrayDesignator,\n      ArrayRangeDesignator\n    } Kind;\n\n    union {\n      /// A field designator, e.g., \".x\".\n      struct FieldDesignator Field;\n      /// An array or GNU array-range designator, e.g., \"[9]\" or \"[10..15]\".\n      struct ArrayOrRangeDesignator ArrayOrRange;\n    };\n    friend class DesignatedInitExpr;\n\n  public:\n    Designator() {}\n\n    /// Initializes a field designator.\n    Designator(const IdentifierInfo *FieldName, SourceLocation DotLoc,\n               SourceLocation FieldLoc)\n      : Kind(FieldDesignator) {\n      new (&Field) DesignatedInitExpr::FieldDesignator;\n      Field.NameOrField = reinterpret_cast<uintptr_t>(FieldName) | 0x01;\n      Field.DotLoc = DotLoc;\n      Field.FieldLoc = FieldLoc;\n    }\n\n    /// Initializes an array designator.\n    Designator(unsigned Index, SourceLocation LBracketLoc,\n               SourceLocation RBracketLoc)\n      : Kind(ArrayDesignator) {\n      new (&ArrayOrRange) DesignatedInitExpr::ArrayOrRangeDesignator;\n      ArrayOrRange.Index = Index;\n      ArrayOrRange.LBracketLoc = LBracketLoc;\n      ArrayOrRange.EllipsisLoc = SourceLocation();\n      ArrayOrRange.RBracketLoc = RBracketLoc;\n    }\n\n    /// Initializes a GNU array-range designator.\n    Designator(unsigned Index, SourceLocation LBracketLoc,\n               SourceLocation EllipsisLoc, SourceLocation RBracketLoc)\n      : Kind(ArrayRangeDesignator) {\n      new (&ArrayOrRange) DesignatedInitExpr::ArrayOrRangeDesignator;\n      ArrayOrRange.Index = Index;\n      ArrayOrRange.LBracketLoc = LBracketLoc;\n      ArrayOrRange.EllipsisLoc = EllipsisLoc;\n      ArrayOrRange.RBracketLoc = RBracketLoc;\n    }\n\n    bool isFieldDesignator() const { return Kind == FieldDesignator; }\n    bool isArrayDesignator() const { return Kind == ArrayDesignator; }\n    bool isArrayRangeDesignator() const { return Kind == ArrayRangeDesignator; }\n\n    IdentifierInfo *getFieldName() const;\n\n    FieldDecl *getField() const {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      if (Field.NameOrField & 0x01)\n        return nullptr;\n      else\n        return reinterpret_cast<FieldDecl *>(Field.NameOrField);\n    }\n\n    void setField(FieldDecl *FD) {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      Field.NameOrField = reinterpret_cast<uintptr_t>(FD);\n    }\n\n    SourceLocation getDotLoc() const {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      return Field.DotLoc;\n    }\n\n    SourceLocation getFieldLoc() const {\n      assert(Kind == FieldDesignator && \"Only valid on a field designator\");\n      return Field.FieldLoc;\n    }\n\n    SourceLocation getLBracketLoc() const {\n      assert((Kind == ArrayDesignator || Kind == ArrayRangeDesignator) &&\n             \"Only valid on an array or array-range designator\");\n      return ArrayOrRange.LBracketLoc;\n    }\n\n    SourceLocation getRBracketLoc() const {\n      assert((Kind == ArrayDesignator || Kind == ArrayRangeDesignator) &&\n             \"Only valid on an array or array-range designator\");\n      return ArrayOrRange.RBracketLoc;\n    }\n\n    SourceLocation getEllipsisLoc() const {\n      assert(Kind == ArrayRangeDesignator &&\n             \"Only valid on an array-range designator\");\n      return ArrayOrRange.EllipsisLoc;\n    }\n\n    unsigned getFirstExprIndex() const {\n      assert((Kind == ArrayDesignator || Kind == ArrayRangeDesignator) &&\n             \"Only valid on an array or array-range designator\");\n      return ArrayOrRange.Index;\n    }\n\n    SourceLocation getBeginLoc() const LLVM_READONLY {\n      if (Kind == FieldDesignator)\n        return getDotLoc().isInvalid()? getFieldLoc() : getDotLoc();\n      else\n        return getLBracketLoc();\n    }\n    SourceLocation getEndLoc() const LLVM_READONLY {\n      return Kind == FieldDesignator ? getFieldLoc() : getRBracketLoc();\n    }\n    SourceRange getSourceRange() const LLVM_READONLY {\n      return SourceRange(getBeginLoc(), getEndLoc());\n    }\n  };\n\n  static DesignatedInitExpr *Create(const ASTContext &C,\n                                    llvm::ArrayRef<Designator> Designators,\n                                    ArrayRef<Expr*> IndexExprs,\n                                    SourceLocation EqualOrColonLoc,\n                                    bool GNUSyntax, Expr *Init);\n\n  static DesignatedInitExpr *CreateEmpty(const ASTContext &C,\n                                         unsigned NumIndexExprs);\n\n  /// Returns the number of designators in this initializer.\n  unsigned size() const { return NumDesignators; }\n\n  // Iterator access to the designators.\n  llvm::MutableArrayRef<Designator> designators() {\n    return {Designators, NumDesignators};\n  }\n\n  llvm::ArrayRef<Designator> designators() const {\n    return {Designators, NumDesignators};\n  }\n\n  Designator *getDesignator(unsigned Idx) { return &designators()[Idx]; }\n  const Designator *getDesignator(unsigned Idx) const {\n    return &designators()[Idx];\n  }\n\n  void setDesignators(const ASTContext &C, const Designator *Desigs,\n                      unsigned NumDesigs);\n\n  Expr *getArrayIndex(const Designator &D) const;\n  Expr *getArrayRangeStart(const Designator &D) const;\n  Expr *getArrayRangeEnd(const Designator &D) const;\n\n  /// Retrieve the location of the '=' that precedes the\n  /// initializer value itself, if present.\n  SourceLocation getEqualOrColonLoc() const { return EqualOrColonLoc; }\n  void setEqualOrColonLoc(SourceLocation L) { EqualOrColonLoc = L; }\n\n  /// Whether this designated initializer should result in direct-initialization\n  /// of the designated subobject (eg, '{.foo{1, 2, 3}}').\n  bool isDirectInit() const { return EqualOrColonLoc.isInvalid(); }\n\n  /// Determines whether this designated initializer used the\n  /// deprecated GNU syntax for designated initializers.\n  bool usesGNUSyntax() const { return GNUSyntax; }\n  void setGNUSyntax(bool GNU) { GNUSyntax = GNU; }\n\n  /// Retrieve the initializer value.\n  Expr *getInit() const {\n    return cast<Expr>(*const_cast<DesignatedInitExpr*>(this)->child_begin());\n  }\n\n  void setInit(Expr *init) {\n    *child_begin() = init;\n  }\n\n  /// Retrieve the total number of subexpressions in this\n  /// designated initializer expression, including the actual\n  /// initialized value and any expressions that occur within array\n  /// and array-range designators.\n  unsigned getNumSubExprs() const { return NumSubExprs; }\n\n  Expr *getSubExpr(unsigned Idx) const {\n    assert(Idx < NumSubExprs && \"Subscript out of range\");\n    return cast<Expr>(getTrailingObjects<Stmt *>()[Idx]);\n  }\n\n  void setSubExpr(unsigned Idx, Expr *E) {\n    assert(Idx < NumSubExprs && \"Subscript out of range\");\n    getTrailingObjects<Stmt *>()[Idx] = E;\n  }\n\n  /// Replaces the designator at index @p Idx with the series\n  /// of designators in [First, Last).\n  void ExpandDesignator(const ASTContext &C, unsigned Idx,\n                        const Designator *First, const Designator *Last);\n\n  SourceRange getDesignatorsSourceRange() const;\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == DesignatedInitExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    Stmt **begin = getTrailingObjects<Stmt *>();\n    return child_range(begin, begin + NumSubExprs);\n  }\n  const_child_range children() const {\n    Stmt * const *begin = getTrailingObjects<Stmt *>();\n    return const_child_range(begin, begin + NumSubExprs);\n  }\n\n  friend TrailingObjects;\n};\n\n/// Represents a place-holder for an object not to be initialized by\n/// anything.\n///\n/// This only makes sense when it appears as part of an updater of a\n/// DesignatedInitUpdateExpr (see below). The base expression of a DIUE\n/// initializes a big object, and the NoInitExpr's mark the spots within the\n/// big object not to be overwritten by the updater.\n///\n/// \\see DesignatedInitUpdateExpr\nclass NoInitExpr : public Expr {\npublic:\n  explicit NoInitExpr(QualType ty)\n      : Expr(NoInitExprClass, ty, VK_RValue, OK_Ordinary) {\n    setDependence(computeDependence(this));\n  }\n\n  explicit NoInitExpr(EmptyShell Empty)\n    : Expr(NoInitExprClass, Empty) { }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == NoInitExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return SourceLocation(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return SourceLocation(); }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n// In cases like:\n//   struct Q { int a, b, c; };\n//   Q *getQ();\n//   void foo() {\n//     struct A { Q q; } a = { *getQ(), .q.b = 3 };\n//   }\n//\n// We will have an InitListExpr for a, with type A, and then a\n// DesignatedInitUpdateExpr for \"a.q\" with type Q. The \"base\" for this DIUE\n// is the call expression *getQ(); the \"updater\" for the DIUE is \".q.b = 3\"\n//\nclass DesignatedInitUpdateExpr : public Expr {\n  // BaseAndUpdaterExprs[0] is the base expression;\n  // BaseAndUpdaterExprs[1] is an InitListExpr overwriting part of the base.\n  Stmt *BaseAndUpdaterExprs[2];\n\npublic:\n  DesignatedInitUpdateExpr(const ASTContext &C, SourceLocation lBraceLoc,\n                           Expr *baseExprs, SourceLocation rBraceLoc);\n\n  explicit DesignatedInitUpdateExpr(EmptyShell Empty)\n    : Expr(DesignatedInitUpdateExprClass, Empty) { }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY;\n  SourceLocation getEndLoc() const LLVM_READONLY;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == DesignatedInitUpdateExprClass;\n  }\n\n  Expr *getBase() const { return cast<Expr>(BaseAndUpdaterExprs[0]); }\n  void setBase(Expr *Base) { BaseAndUpdaterExprs[0] = Base; }\n\n  InitListExpr *getUpdater() const {\n    return cast<InitListExpr>(BaseAndUpdaterExprs[1]);\n  }\n  void setUpdater(Expr *Updater) { BaseAndUpdaterExprs[1] = Updater; }\n\n  // Iterators\n  // children = the base and the updater\n  child_range children() {\n    return child_range(&BaseAndUpdaterExprs[0], &BaseAndUpdaterExprs[0] + 2);\n  }\n  const_child_range children() const {\n    return const_child_range(&BaseAndUpdaterExprs[0],\n                             &BaseAndUpdaterExprs[0] + 2);\n  }\n};\n\n/// Represents a loop initializing the elements of an array.\n///\n/// The need to initialize the elements of an array occurs in a number of\n/// contexts:\n///\n///  * in the implicit copy/move constructor for a class with an array member\n///  * when a lambda-expression captures an array by value\n///  * when a decomposition declaration decomposes an array\n///\n/// There are two subexpressions: a common expression (the source array)\n/// that is evaluated once up-front, and a per-element initializer that\n/// runs once for each array element.\n///\n/// Within the per-element initializer, the common expression may be referenced\n/// via an OpaqueValueExpr, and the current index may be obtained via an\n/// ArrayInitIndexExpr.\nclass ArrayInitLoopExpr : public Expr {\n  Stmt *SubExprs[2];\n\n  explicit ArrayInitLoopExpr(EmptyShell Empty)\n      : Expr(ArrayInitLoopExprClass, Empty), SubExprs{} {}\n\npublic:\n  explicit ArrayInitLoopExpr(QualType T, Expr *CommonInit, Expr *ElementInit)\n      : Expr(ArrayInitLoopExprClass, T, VK_RValue, OK_Ordinary),\n        SubExprs{CommonInit, ElementInit} {\n    setDependence(computeDependence(this));\n  }\n\n  /// Get the common subexpression shared by all initializations (the source\n  /// array).\n  OpaqueValueExpr *getCommonExpr() const {\n    return cast<OpaqueValueExpr>(SubExprs[0]);\n  }\n\n  /// Get the initializer to use for each array element.\n  Expr *getSubExpr() const { return cast<Expr>(SubExprs[1]); }\n\n  llvm::APInt getArraySize() const {\n    return cast<ConstantArrayType>(getType()->castAsArrayTypeUnsafe())\n        ->getSize();\n  }\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() == ArrayInitLoopExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCommonExpr()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getCommonExpr()->getEndLoc();\n  }\n\n  child_range children() {\n    return child_range(SubExprs, SubExprs + 2);\n  }\n  const_child_range children() const {\n    return const_child_range(SubExprs, SubExprs + 2);\n  }\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n/// Represents the index of the current element of an array being\n/// initialized by an ArrayInitLoopExpr. This can only appear within the\n/// subexpression of an ArrayInitLoopExpr.\nclass ArrayInitIndexExpr : public Expr {\n  explicit ArrayInitIndexExpr(EmptyShell Empty)\n      : Expr(ArrayInitIndexExprClass, Empty) {}\n\npublic:\n  explicit ArrayInitIndexExpr(QualType T)\n      : Expr(ArrayInitIndexExprClass, T, VK_RValue, OK_Ordinary) {\n    setDependence(ExprDependence::None);\n  }\n\n  static bool classof(const Stmt *S) {\n    return S->getStmtClass() == ArrayInitIndexExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return SourceLocation(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return SourceLocation(); }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n};\n\n/// Represents an implicitly-generated value initialization of\n/// an object of a given type.\n///\n/// Implicit value initializations occur within semantic initializer\n/// list expressions (InitListExpr) as placeholders for subobject\n/// initializations not explicitly specified by the user.\n///\n/// \\see InitListExpr\nclass ImplicitValueInitExpr : public Expr {\npublic:\n  explicit ImplicitValueInitExpr(QualType ty)\n      : Expr(ImplicitValueInitExprClass, ty, VK_RValue, OK_Ordinary) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Construct an empty implicit value initialization.\n  explicit ImplicitValueInitExpr(EmptyShell Empty)\n    : Expr(ImplicitValueInitExprClass, Empty) { }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ImplicitValueInitExprClass;\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return SourceLocation(); }\n  SourceLocation getEndLoc() const LLVM_READONLY { return SourceLocation(); }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\nclass ParenListExpr final\n    : public Expr,\n      private llvm::TrailingObjects<ParenListExpr, Stmt *> {\n  friend class ASTStmtReader;\n  friend TrailingObjects;\n\n  /// The location of the left and right parentheses.\n  SourceLocation LParenLoc, RParenLoc;\n\n  /// Build a paren list.\n  ParenListExpr(SourceLocation LParenLoc, ArrayRef<Expr *> Exprs,\n                SourceLocation RParenLoc);\n\n  /// Build an empty paren list.\n  ParenListExpr(EmptyShell Empty, unsigned NumExprs);\n\npublic:\n  /// Create a paren list.\n  static ParenListExpr *Create(const ASTContext &Ctx, SourceLocation LParenLoc,\n                               ArrayRef<Expr *> Exprs,\n                               SourceLocation RParenLoc);\n\n  /// Create an empty paren list.\n  static ParenListExpr *CreateEmpty(const ASTContext &Ctx, unsigned NumExprs);\n\n  /// Return the number of expressions in this paren list.\n  unsigned getNumExprs() const { return ParenListExprBits.NumExprs; }\n\n  Expr *getExpr(unsigned Init) {\n    assert(Init < getNumExprs() && \"Initializer access out of range!\");\n    return getExprs()[Init];\n  }\n\n  const Expr *getExpr(unsigned Init) const {\n    return const_cast<ParenListExpr *>(this)->getExpr(Init);\n  }\n\n  Expr **getExprs() {\n    return reinterpret_cast<Expr **>(getTrailingObjects<Stmt *>());\n  }\n\n  ArrayRef<Expr *> exprs() {\n    return llvm::makeArrayRef(getExprs(), getNumExprs());\n  }\n\n  SourceLocation getLParenLoc() const { return LParenLoc; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  SourceLocation getBeginLoc() const { return getLParenLoc(); }\n  SourceLocation getEndLoc() const { return getRParenLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ParenListExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(getTrailingObjects<Stmt *>(),\n                       getTrailingObjects<Stmt *>() + getNumExprs());\n  }\n  const_child_range children() const {\n    return const_child_range(getTrailingObjects<Stmt *>(),\n                             getTrailingObjects<Stmt *>() + getNumExprs());\n  }\n};\n\n/// Represents a C11 generic selection.\n///\n/// A generic selection (C11 6.5.1.1) contains an unevaluated controlling\n/// expression, followed by one or more generic associations.  Each generic\n/// association specifies a type name and an expression, or \"default\" and an\n/// expression (in which case it is known as a default generic association).\n/// The type and value of the generic selection are identical to those of its\n/// result expression, which is defined as the expression in the generic\n/// association with a type name that is compatible with the type of the\n/// controlling expression, or the expression in the default generic association\n/// if no types are compatible.  For example:\n///\n/// @code\n/// _Generic(X, double: 1, float: 2, default: 3)\n/// @endcode\n///\n/// The above expression evaluates to 1 if 1.0 is substituted for X, 2 if 1.0f\n/// or 3 if \"hello\".\n///\n/// As an extension, generic selections are allowed in C++, where the following\n/// additional semantics apply:\n///\n/// Any generic selection whose controlling expression is type-dependent or\n/// which names a dependent type in its association list is result-dependent,\n/// which means that the choice of result expression is dependent.\n/// Result-dependent generic associations are both type- and value-dependent.\nclass GenericSelectionExpr final\n    : public Expr,\n      private llvm::TrailingObjects<GenericSelectionExpr, Stmt *,\n                                    TypeSourceInfo *> {\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n  friend TrailingObjects;\n\n  /// The number of association expressions and the index of the result\n  /// expression in the case where the generic selection expression is not\n  /// result-dependent. The result index is equal to ResultDependentIndex\n  /// if and only if the generic selection expression is result-dependent.\n  unsigned NumAssocs, ResultIndex;\n  enum : unsigned {\n    ResultDependentIndex = std::numeric_limits<unsigned>::max(),\n    ControllingIndex = 0,\n    AssocExprStartIndex = 1\n  };\n\n  /// The location of the \"default\" and of the right parenthesis.\n  SourceLocation DefaultLoc, RParenLoc;\n\n  // GenericSelectionExpr is followed by several trailing objects.\n  // They are (in order):\n  //\n  // * A single Stmt * for the controlling expression.\n  // * An array of getNumAssocs() Stmt * for the association expressions.\n  // * An array of getNumAssocs() TypeSourceInfo *, one for each of the\n  //   association expressions.\n  unsigned numTrailingObjects(OverloadToken<Stmt *>) const {\n    // Add one to account for the controlling expression; the remainder\n    // are the associated expressions.\n    return 1 + getNumAssocs();\n  }\n\n  unsigned numTrailingObjects(OverloadToken<TypeSourceInfo *>) const {\n    return getNumAssocs();\n  }\n\n  template <bool Const> class AssociationIteratorTy;\n  /// Bundle together an association expression and its TypeSourceInfo.\n  /// The Const template parameter is for the const and non-const versions\n  /// of AssociationTy.\n  template <bool Const> class AssociationTy {\n    friend class GenericSelectionExpr;\n    template <bool OtherConst> friend class AssociationIteratorTy;\n    using ExprPtrTy = std::conditional_t<Const, const Expr *, Expr *>;\n    using TSIPtrTy =\n        std::conditional_t<Const, const TypeSourceInfo *, TypeSourceInfo *>;\n    ExprPtrTy E;\n    TSIPtrTy TSI;\n    bool Selected;\n    AssociationTy(ExprPtrTy E, TSIPtrTy TSI, bool Selected)\n        : E(E), TSI(TSI), Selected(Selected) {}\n\n  public:\n    ExprPtrTy getAssociationExpr() const { return E; }\n    TSIPtrTy getTypeSourceInfo() const { return TSI; }\n    QualType getType() const { return TSI ? TSI->getType() : QualType(); }\n    bool isSelected() const { return Selected; }\n    AssociationTy *operator->() { return this; }\n    const AssociationTy *operator->() const { return this; }\n  }; // class AssociationTy\n\n  /// Iterator over const and non-const Association objects. The Association\n  /// objects are created on the fly when the iterator is dereferenced.\n  /// This abstract over how exactly the association expressions and the\n  /// corresponding TypeSourceInfo * are stored.\n  template <bool Const>\n  class AssociationIteratorTy\n      : public llvm::iterator_facade_base<\n            AssociationIteratorTy<Const>, std::input_iterator_tag,\n            AssociationTy<Const>, std::ptrdiff_t, AssociationTy<Const>,\n            AssociationTy<Const>> {\n    friend class GenericSelectionExpr;\n    // FIXME: This iterator could conceptually be a random access iterator, and\n    // it would be nice if we could strengthen the iterator category someday.\n    // However this iterator does not satisfy two requirements of forward\n    // iterators:\n    // a) reference = T& or reference = const T&\n    // b) If It1 and It2 are both dereferenceable, then It1 == It2 if and only\n    //    if *It1 and *It2 are bound to the same objects.\n    // An alternative design approach was discussed during review;\n    // store an Association object inside the iterator, and return a reference\n    // to it when dereferenced. This idea was discarded beacuse of nasty\n    // lifetime issues:\n    //    AssociationIterator It = ...;\n    //    const Association &Assoc = *It++; // Oops, Assoc is dangling.\n    using BaseTy = typename AssociationIteratorTy::iterator_facade_base;\n    using StmtPtrPtrTy =\n        std::conditional_t<Const, const Stmt *const *, Stmt **>;\n    using TSIPtrPtrTy = std::conditional_t<Const, const TypeSourceInfo *const *,\n                                           TypeSourceInfo **>;\n    StmtPtrPtrTy E; // = nullptr; FIXME: Once support for gcc 4.8 is dropped.\n    TSIPtrPtrTy TSI; // Kept in sync with E.\n    unsigned Offset = 0, SelectedOffset = 0;\n    AssociationIteratorTy(StmtPtrPtrTy E, TSIPtrPtrTy TSI, unsigned Offset,\n                          unsigned SelectedOffset)\n        : E(E), TSI(TSI), Offset(Offset), SelectedOffset(SelectedOffset) {}\n\n  public:\n    AssociationIteratorTy() : E(nullptr), TSI(nullptr) {}\n    typename BaseTy::reference operator*() const {\n      return AssociationTy<Const>(cast<Expr>(*E), *TSI,\n                                  Offset == SelectedOffset);\n    }\n    typename BaseTy::pointer operator->() const { return **this; }\n    using BaseTy::operator++;\n    AssociationIteratorTy &operator++() {\n      ++E;\n      ++TSI;\n      ++Offset;\n      return *this;\n    }\n    bool operator==(AssociationIteratorTy Other) const { return E == Other.E; }\n  }; // class AssociationIterator\n\n  /// Build a non-result-dependent generic selection expression.\n  GenericSelectionExpr(const ASTContext &Context, SourceLocation GenericLoc,\n                       Expr *ControllingExpr,\n                       ArrayRef<TypeSourceInfo *> AssocTypes,\n                       ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n                       SourceLocation RParenLoc,\n                       bool ContainsUnexpandedParameterPack,\n                       unsigned ResultIndex);\n\n  /// Build a result-dependent generic selection expression.\n  GenericSelectionExpr(const ASTContext &Context, SourceLocation GenericLoc,\n                       Expr *ControllingExpr,\n                       ArrayRef<TypeSourceInfo *> AssocTypes,\n                       ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n                       SourceLocation RParenLoc,\n                       bool ContainsUnexpandedParameterPack);\n\n  /// Build an empty generic selection expression for deserialization.\n  explicit GenericSelectionExpr(EmptyShell Empty, unsigned NumAssocs);\n\npublic:\n  /// Create a non-result-dependent generic selection expression.\n  static GenericSelectionExpr *\n  Create(const ASTContext &Context, SourceLocation GenericLoc,\n         Expr *ControllingExpr, ArrayRef<TypeSourceInfo *> AssocTypes,\n         ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n         SourceLocation RParenLoc, bool ContainsUnexpandedParameterPack,\n         unsigned ResultIndex);\n\n  /// Create a result-dependent generic selection expression.\n  static GenericSelectionExpr *\n  Create(const ASTContext &Context, SourceLocation GenericLoc,\n         Expr *ControllingExpr, ArrayRef<TypeSourceInfo *> AssocTypes,\n         ArrayRef<Expr *> AssocExprs, SourceLocation DefaultLoc,\n         SourceLocation RParenLoc, bool ContainsUnexpandedParameterPack);\n\n  /// Create an empty generic selection expression for deserialization.\n  static GenericSelectionExpr *CreateEmpty(const ASTContext &Context,\n                                           unsigned NumAssocs);\n\n  using Association = AssociationTy<false>;\n  using ConstAssociation = AssociationTy<true>;\n  using AssociationIterator = AssociationIteratorTy<false>;\n  using ConstAssociationIterator = AssociationIteratorTy<true>;\n  using association_range = llvm::iterator_range<AssociationIterator>;\n  using const_association_range =\n      llvm::iterator_range<ConstAssociationIterator>;\n\n  /// The number of association expressions.\n  unsigned getNumAssocs() const { return NumAssocs; }\n\n  /// The zero-based index of the result expression's generic association in\n  /// the generic selection's association list.  Defined only if the\n  /// generic selection is not result-dependent.\n  unsigned getResultIndex() const {\n    assert(!isResultDependent() &&\n           \"Generic selection is result-dependent but getResultIndex called!\");\n    return ResultIndex;\n  }\n\n  /// Whether this generic selection is result-dependent.\n  bool isResultDependent() const { return ResultIndex == ResultDependentIndex; }\n\n  /// Return the controlling expression of this generic selection expression.\n  Expr *getControllingExpr() {\n    return cast<Expr>(getTrailingObjects<Stmt *>()[ControllingIndex]);\n  }\n  const Expr *getControllingExpr() const {\n    return cast<Expr>(getTrailingObjects<Stmt *>()[ControllingIndex]);\n  }\n\n  /// Return the result expression of this controlling expression. Defined if\n  /// and only if the generic selection expression is not result-dependent.\n  Expr *getResultExpr() {\n    return cast<Expr>(\n        getTrailingObjects<Stmt *>()[AssocExprStartIndex + getResultIndex()]);\n  }\n  const Expr *getResultExpr() const {\n    return cast<Expr>(\n        getTrailingObjects<Stmt *>()[AssocExprStartIndex + getResultIndex()]);\n  }\n\n  ArrayRef<Expr *> getAssocExprs() const {\n    return {reinterpret_cast<Expr *const *>(getTrailingObjects<Stmt *>() +\n                                            AssocExprStartIndex),\n            NumAssocs};\n  }\n  ArrayRef<TypeSourceInfo *> getAssocTypeSourceInfos() const {\n    return {getTrailingObjects<TypeSourceInfo *>(), NumAssocs};\n  }\n\n  /// Return the Ith association expression with its TypeSourceInfo,\n  /// bundled together in GenericSelectionExpr::(Const)Association.\n  Association getAssociation(unsigned I) {\n    assert(I < getNumAssocs() &&\n           \"Out-of-range index in GenericSelectionExpr::getAssociation!\");\n    return Association(\n        cast<Expr>(getTrailingObjects<Stmt *>()[AssocExprStartIndex + I]),\n        getTrailingObjects<TypeSourceInfo *>()[I],\n        !isResultDependent() && (getResultIndex() == I));\n  }\n  ConstAssociation getAssociation(unsigned I) const {\n    assert(I < getNumAssocs() &&\n           \"Out-of-range index in GenericSelectionExpr::getAssociation!\");\n    return ConstAssociation(\n        cast<Expr>(getTrailingObjects<Stmt *>()[AssocExprStartIndex + I]),\n        getTrailingObjects<TypeSourceInfo *>()[I],\n        !isResultDependent() && (getResultIndex() == I));\n  }\n\n  association_range associations() {\n    AssociationIterator Begin(getTrailingObjects<Stmt *>() +\n                                  AssocExprStartIndex,\n                              getTrailingObjects<TypeSourceInfo *>(),\n                              /*Offset=*/0, ResultIndex);\n    AssociationIterator End(Begin.E + NumAssocs, Begin.TSI + NumAssocs,\n                            /*Offset=*/NumAssocs, ResultIndex);\n    return llvm::make_range(Begin, End);\n  }\n\n  const_association_range associations() const {\n    ConstAssociationIterator Begin(getTrailingObjects<Stmt *>() +\n                                       AssocExprStartIndex,\n                                   getTrailingObjects<TypeSourceInfo *>(),\n                                   /*Offset=*/0, ResultIndex);\n    ConstAssociationIterator End(Begin.E + NumAssocs, Begin.TSI + NumAssocs,\n                                 /*Offset=*/NumAssocs, ResultIndex);\n    return llvm::make_range(Begin, End);\n  }\n\n  SourceLocation getGenericLoc() const {\n    return GenericSelectionExprBits.GenericLoc;\n  }\n  SourceLocation getDefaultLoc() const { return DefaultLoc; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n  SourceLocation getBeginLoc() const { return getGenericLoc(); }\n  SourceLocation getEndLoc() const { return getRParenLoc(); }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == GenericSelectionExprClass;\n  }\n\n  child_range children() {\n    return child_range(getTrailingObjects<Stmt *>(),\n                       getTrailingObjects<Stmt *>() +\n                           numTrailingObjects(OverloadToken<Stmt *>()));\n  }\n  const_child_range children() const {\n    return const_child_range(getTrailingObjects<Stmt *>(),\n                             getTrailingObjects<Stmt *>() +\n                                 numTrailingObjects(OverloadToken<Stmt *>()));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n// Clang Extensions\n//===----------------------------------------------------------------------===//\n\n/// ExtVectorElementExpr - This represents access to specific elements of a\n/// vector, and may occur on the left hand side or right hand side.  For example\n/// the following is legal:  \"V.xy = V.zw\" if V is a 4 element extended vector.\n///\n/// Note that the base may have either vector or pointer to vector type, just\n/// like a struct field reference.\n///\nclass ExtVectorElementExpr : public Expr {\n  Stmt *Base;\n  IdentifierInfo *Accessor;\n  SourceLocation AccessorLoc;\npublic:\n  ExtVectorElementExpr(QualType ty, ExprValueKind VK, Expr *base,\n                       IdentifierInfo &accessor, SourceLocation loc)\n      : Expr(ExtVectorElementExprClass, ty, VK,\n             (VK == VK_RValue ? OK_Ordinary : OK_VectorComponent)),\n        Base(base), Accessor(&accessor), AccessorLoc(loc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty vector element expression.\n  explicit ExtVectorElementExpr(EmptyShell Empty)\n    : Expr(ExtVectorElementExprClass, Empty) { }\n\n  const Expr *getBase() const { return cast<Expr>(Base); }\n  Expr *getBase() { return cast<Expr>(Base); }\n  void setBase(Expr *E) { Base = E; }\n\n  IdentifierInfo &getAccessor() const { return *Accessor; }\n  void setAccessor(IdentifierInfo *II) { Accessor = II; }\n\n  SourceLocation getAccessorLoc() const { return AccessorLoc; }\n  void setAccessorLoc(SourceLocation L) { AccessorLoc = L; }\n\n  /// getNumElements - Get the number of components being selected.\n  unsigned getNumElements() const;\n\n  /// containsDuplicateElements - Return true if any element access is\n  /// repeated.\n  bool containsDuplicateElements() const;\n\n  /// getEncodedElementAccess - Encode the elements accessed into an llvm\n  /// aggregate Constant of ConstantInt(s).\n  void getEncodedElementAccess(SmallVectorImpl<uint32_t> &Elts) const;\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getBase()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY { return AccessorLoc; }\n\n  /// isArrow - Return true if the base expression is a pointer to vector,\n  /// return false if the base expression is a vector.\n  bool isArrow() const;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == ExtVectorElementExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&Base, &Base+1); }\n  const_child_range children() const {\n    return const_child_range(&Base, &Base + 1);\n  }\n};\n\n/// BlockExpr - Adaptor class for mixing a BlockDecl with expressions.\n/// ^{ statement-body }   or   ^(int arg1, float arg2){ statement-body }\nclass BlockExpr : public Expr {\nprotected:\n  BlockDecl *TheBlock;\npublic:\n  BlockExpr(BlockDecl *BD, QualType ty)\n      : Expr(BlockExprClass, ty, VK_RValue, OK_Ordinary), TheBlock(BD) {\n    setDependence(computeDependence(this));\n  }\n\n  /// Build an empty block expression.\n  explicit BlockExpr(EmptyShell Empty) : Expr(BlockExprClass, Empty) { }\n\n  const BlockDecl *getBlockDecl() const { return TheBlock; }\n  BlockDecl *getBlockDecl() { return TheBlock; }\n  void setBlockDecl(BlockDecl *BD) { TheBlock = BD; }\n\n  // Convenience functions for probing the underlying BlockDecl.\n  SourceLocation getCaretLocation() const;\n  const Stmt *getBody() const;\n  Stmt *getBody();\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getCaretLocation();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getBody()->getEndLoc();\n  }\n\n  /// getFunctionType - Return the underlying function type for this block.\n  const FunctionProtoType *getFunctionType() const;\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == BlockExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n};\n\n/// Copy initialization expr of a __block variable and a boolean flag that\n/// indicates whether the expression can throw.\nstruct BlockVarCopyInit {\n  BlockVarCopyInit() = default;\n  BlockVarCopyInit(Expr *CopyExpr, bool CanThrow)\n      : ExprAndFlag(CopyExpr, CanThrow) {}\n  void setExprAndFlag(Expr *CopyExpr, bool CanThrow) {\n    ExprAndFlag.setPointerAndInt(CopyExpr, CanThrow);\n  }\n  Expr *getCopyExpr() const { return ExprAndFlag.getPointer(); }\n  bool canThrow() const { return ExprAndFlag.getInt(); }\n  llvm::PointerIntPair<Expr *, 1, bool> ExprAndFlag;\n};\n\n/// AsTypeExpr - Clang builtin function __builtin_astype [OpenCL 6.2.4.2]\n/// This AST node provides support for reinterpreting a type to another\n/// type of the same size.\nclass AsTypeExpr : public Expr {\nprivate:\n  Stmt *SrcExpr;\n  SourceLocation BuiltinLoc, RParenLoc;\n\n  friend class ASTReader;\n  friend class ASTStmtReader;\n  explicit AsTypeExpr(EmptyShell Empty) : Expr(AsTypeExprClass, Empty) {}\n\npublic:\n  AsTypeExpr(Expr *SrcExpr, QualType DstType, ExprValueKind VK,\n             ExprObjectKind OK, SourceLocation BuiltinLoc,\n             SourceLocation RParenLoc)\n      : Expr(AsTypeExprClass, DstType, VK, OK), SrcExpr(SrcExpr),\n        BuiltinLoc(BuiltinLoc), RParenLoc(RParenLoc) {\n    setDependence(computeDependence(this));\n  }\n\n  /// getSrcExpr - Return the Expr to be converted.\n  Expr *getSrcExpr() const { return cast<Expr>(SrcExpr); }\n\n  /// getBuiltinLoc - Return the location of the __builtin_astype token.\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n\n  /// getRParenLoc - Return the location of final right parenthesis.\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == AsTypeExprClass;\n  }\n\n  // Iterators\n  child_range children() { return child_range(&SrcExpr, &SrcExpr+1); }\n  const_child_range children() const {\n    return const_child_range(&SrcExpr, &SrcExpr + 1);\n  }\n};\n\n/// PseudoObjectExpr - An expression which accesses a pseudo-object\n/// l-value.  A pseudo-object is an abstract object, accesses to which\n/// are translated to calls.  The pseudo-object expression has a\n/// syntactic form, which shows how the expression was actually\n/// written in the source code, and a semantic form, which is a series\n/// of expressions to be executed in order which detail how the\n/// operation is actually evaluated.  Optionally, one of the semantic\n/// forms may also provide a result value for the expression.\n///\n/// If any of the semantic-form expressions is an OpaqueValueExpr,\n/// that OVE is required to have a source expression, and it is bound\n/// to the result of that source expression.  Such OVEs may appear\n/// only in subsequent semantic-form expressions and as\n/// sub-expressions of the syntactic form.\n///\n/// PseudoObjectExpr should be used only when an operation can be\n/// usefully described in terms of fairly simple rewrite rules on\n/// objects and functions that are meant to be used by end-developers.\n/// For example, under the Itanium ABI, dynamic casts are implemented\n/// as a call to a runtime function called __dynamic_cast; using this\n/// class to describe that would be inappropriate because that call is\n/// not really part of the user-visible semantics, and instead the\n/// cast is properly reflected in the AST and IR-generation has been\n/// taught to generate the call as necessary.  In contrast, an\n/// Objective-C property access is semantically defined to be\n/// equivalent to a particular message send, and this is very much\n/// part of the user model.  The name of this class encourages this\n/// modelling design.\nclass PseudoObjectExpr final\n    : public Expr,\n      private llvm::TrailingObjects<PseudoObjectExpr, Expr *> {\n  // PseudoObjectExprBits.NumSubExprs - The number of sub-expressions.\n  // Always at least two, because the first sub-expression is the\n  // syntactic form.\n\n  // PseudoObjectExprBits.ResultIndex - The index of the\n  // sub-expression holding the result.  0 means the result is void,\n  // which is unambiguous because it's the index of the syntactic\n  // form.  Note that this is therefore 1 higher than the value passed\n  // in to Create, which is an index within the semantic forms.\n  // Note also that ASTStmtWriter assumes this encoding.\n\n  Expr **getSubExprsBuffer() { return getTrailingObjects<Expr *>(); }\n  const Expr * const *getSubExprsBuffer() const {\n    return getTrailingObjects<Expr *>();\n  }\n\n  PseudoObjectExpr(QualType type, ExprValueKind VK,\n                   Expr *syntactic, ArrayRef<Expr*> semantic,\n                   unsigned resultIndex);\n\n  PseudoObjectExpr(EmptyShell shell, unsigned numSemanticExprs);\n\n  unsigned getNumSubExprs() const {\n    return PseudoObjectExprBits.NumSubExprs;\n  }\n\npublic:\n  /// NoResult - A value for the result index indicating that there is\n  /// no semantic result.\n  enum : unsigned { NoResult = ~0U };\n\n  static PseudoObjectExpr *Create(const ASTContext &Context, Expr *syntactic,\n                                  ArrayRef<Expr*> semantic,\n                                  unsigned resultIndex);\n\n  static PseudoObjectExpr *Create(const ASTContext &Context, EmptyShell shell,\n                                  unsigned numSemanticExprs);\n\n  /// Return the syntactic form of this expression, i.e. the\n  /// expression it actually looks like.  Likely to be expressed in\n  /// terms of OpaqueValueExprs bound in the semantic form.\n  Expr *getSyntacticForm() { return getSubExprsBuffer()[0]; }\n  const Expr *getSyntacticForm() const { return getSubExprsBuffer()[0]; }\n\n  /// Return the index of the result-bearing expression into the semantics\n  /// expressions, or PseudoObjectExpr::NoResult if there is none.\n  unsigned getResultExprIndex() const {\n    if (PseudoObjectExprBits.ResultIndex == 0) return NoResult;\n    return PseudoObjectExprBits.ResultIndex - 1;\n  }\n\n  /// Return the result-bearing expression, or null if there is none.\n  Expr *getResultExpr() {\n    if (PseudoObjectExprBits.ResultIndex == 0)\n      return nullptr;\n    return getSubExprsBuffer()[PseudoObjectExprBits.ResultIndex];\n  }\n  const Expr *getResultExpr() const {\n    return const_cast<PseudoObjectExpr*>(this)->getResultExpr();\n  }\n\n  unsigned getNumSemanticExprs() const { return getNumSubExprs() - 1; }\n\n  typedef Expr * const *semantics_iterator;\n  typedef const Expr * const *const_semantics_iterator;\n  semantics_iterator semantics_begin() {\n    return getSubExprsBuffer() + 1;\n  }\n  const_semantics_iterator semantics_begin() const {\n    return getSubExprsBuffer() + 1;\n  }\n  semantics_iterator semantics_end() {\n    return getSubExprsBuffer() + getNumSubExprs();\n  }\n  const_semantics_iterator semantics_end() const {\n    return getSubExprsBuffer() + getNumSubExprs();\n  }\n\n  llvm::iterator_range<semantics_iterator> semantics() {\n    return llvm::make_range(semantics_begin(), semantics_end());\n  }\n  llvm::iterator_range<const_semantics_iterator> semantics() const {\n    return llvm::make_range(semantics_begin(), semantics_end());\n  }\n\n  Expr *getSemanticExpr(unsigned index) {\n    assert(index + 1 < getNumSubExprs());\n    return getSubExprsBuffer()[index + 1];\n  }\n  const Expr *getSemanticExpr(unsigned index) const {\n    return const_cast<PseudoObjectExpr*>(this)->getSemanticExpr(index);\n  }\n\n  SourceLocation getExprLoc() const LLVM_READONLY {\n    return getSyntacticForm()->getExprLoc();\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY {\n    return getSyntacticForm()->getBeginLoc();\n  }\n  SourceLocation getEndLoc() const LLVM_READONLY {\n    return getSyntacticForm()->getEndLoc();\n  }\n\n  child_range children() {\n    const_child_range CCR =\n        const_cast<const PseudoObjectExpr *>(this)->children();\n    return child_range(cast_away_const(CCR.begin()),\n                       cast_away_const(CCR.end()));\n  }\n  const_child_range children() const {\n    Stmt *const *cs = const_cast<Stmt *const *>(\n        reinterpret_cast<const Stmt *const *>(getSubExprsBuffer()));\n    return const_child_range(cs, cs + getNumSubExprs());\n  }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == PseudoObjectExprClass;\n  }\n\n  friend TrailingObjects;\n  friend class ASTStmtReader;\n};\n\n/// AtomicExpr - Variadic atomic builtins: __atomic_exchange, __atomic_fetch_*,\n/// __atomic_load, __atomic_store, and __atomic_compare_exchange_*, for the\n/// similarly-named C++11 instructions, and __c11 variants for <stdatomic.h>,\n/// and corresponding __opencl_atomic_* for OpenCL 2.0.\n/// All of these instructions take one primary pointer, at least one memory\n/// order. The instructions for which getScopeModel returns non-null value\n/// take one synch scope.\nclass AtomicExpr : public Expr {\npublic:\n  enum AtomicOp {\n#define BUILTIN(ID, TYPE, ATTRS)\n#define ATOMIC_BUILTIN(ID, TYPE, ATTRS) AO ## ID,\n#include \"clang/Basic/Builtins.def\"\n    // Avoid trailing comma\n    BI_First = 0\n  };\n\nprivate:\n  /// Location of sub-expressions.\n  /// The location of Scope sub-expression is NumSubExprs - 1, which is\n  /// not fixed, therefore is not defined in enum.\n  enum { PTR, ORDER, VAL1, ORDER_FAIL, VAL2, WEAK, END_EXPR };\n  Stmt *SubExprs[END_EXPR + 1];\n  unsigned NumSubExprs;\n  SourceLocation BuiltinLoc, RParenLoc;\n  AtomicOp Op;\n\n  friend class ASTStmtReader;\npublic:\n  AtomicExpr(SourceLocation BLoc, ArrayRef<Expr*> args, QualType t,\n             AtomicOp op, SourceLocation RP);\n\n  /// Determine the number of arguments the specified atomic builtin\n  /// should have.\n  static unsigned getNumSubExprs(AtomicOp Op);\n\n  /// Build an empty AtomicExpr.\n  explicit AtomicExpr(EmptyShell Empty) : Expr(AtomicExprClass, Empty) { }\n\n  Expr *getPtr() const {\n    return cast<Expr>(SubExprs[PTR]);\n  }\n  Expr *getOrder() const {\n    return cast<Expr>(SubExprs[ORDER]);\n  }\n  Expr *getScope() const {\n    assert(getScopeModel() && \"No scope\");\n    return cast<Expr>(SubExprs[NumSubExprs - 1]);\n  }\n  Expr *getVal1() const {\n    if (Op == AO__c11_atomic_init || Op == AO__opencl_atomic_init)\n      return cast<Expr>(SubExprs[ORDER]);\n    assert(NumSubExprs > VAL1);\n    return cast<Expr>(SubExprs[VAL1]);\n  }\n  Expr *getOrderFail() const {\n    assert(NumSubExprs > ORDER_FAIL);\n    return cast<Expr>(SubExprs[ORDER_FAIL]);\n  }\n  Expr *getVal2() const {\n    if (Op == AO__atomic_exchange)\n      return cast<Expr>(SubExprs[ORDER_FAIL]);\n    assert(NumSubExprs > VAL2);\n    return cast<Expr>(SubExprs[VAL2]);\n  }\n  Expr *getWeak() const {\n    assert(NumSubExprs > WEAK);\n    return cast<Expr>(SubExprs[WEAK]);\n  }\n  QualType getValueType() const;\n\n  AtomicOp getOp() const { return Op; }\n  unsigned getNumSubExprs() const { return NumSubExprs; }\n\n  Expr **getSubExprs() { return reinterpret_cast<Expr **>(SubExprs); }\n  const Expr * const *getSubExprs() const {\n    return reinterpret_cast<Expr * const *>(SubExprs);\n  }\n\n  bool isVolatile() const {\n    return getPtr()->getType()->getPointeeType().isVolatileQualified();\n  }\n\n  bool isCmpXChg() const {\n    return getOp() == AO__c11_atomic_compare_exchange_strong ||\n           getOp() == AO__c11_atomic_compare_exchange_weak ||\n           getOp() == AO__opencl_atomic_compare_exchange_strong ||\n           getOp() == AO__opencl_atomic_compare_exchange_weak ||\n           getOp() == AO__atomic_compare_exchange ||\n           getOp() == AO__atomic_compare_exchange_n;\n  }\n\n  bool isOpenCL() const {\n    return getOp() >= AO__opencl_atomic_init &&\n           getOp() <= AO__opencl_atomic_fetch_max;\n  }\n\n  SourceLocation getBuiltinLoc() const { return BuiltinLoc; }\n  SourceLocation getRParenLoc() const { return RParenLoc; }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return BuiltinLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return RParenLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == AtomicExprClass;\n  }\n\n  // Iterators\n  child_range children() {\n    return child_range(SubExprs, SubExprs+NumSubExprs);\n  }\n  const_child_range children() const {\n    return const_child_range(SubExprs, SubExprs + NumSubExprs);\n  }\n\n  /// Get atomic scope model for the atomic op code.\n  /// \\return empty atomic scope model if the atomic op code does not have\n  ///   scope operand.\n  static std::unique_ptr<AtomicScopeModel> getScopeModel(AtomicOp Op) {\n    auto Kind =\n        (Op >= AO__opencl_atomic_load && Op <= AO__opencl_atomic_fetch_max)\n            ? AtomicScopeModelKind::OpenCL\n            : AtomicScopeModelKind::None;\n    return AtomicScopeModel::create(Kind);\n  }\n\n  /// Get atomic scope model.\n  /// \\return empty atomic scope model if this atomic expression does not have\n  ///   scope operand.\n  std::unique_ptr<AtomicScopeModel> getScopeModel() const {\n    return getScopeModel(getOp());\n  }\n};\n\n/// TypoExpr - Internal placeholder for expressions where typo correction\n/// still needs to be performed and/or an error diagnostic emitted.\nclass TypoExpr : public Expr {\n  // The location for the typo name.\n  SourceLocation TypoLoc;\n\npublic:\n  TypoExpr(QualType T, SourceLocation TypoLoc)\n      : Expr(TypoExprClass, T, VK_LValue, OK_Ordinary), TypoLoc(TypoLoc) {\n    assert(T->isDependentType() && \"TypoExpr given a non-dependent type\");\n    setDependence(ExprDependence::TypeValueInstantiation |\n                  ExprDependence::Error);\n  }\n\n  child_range children() {\n    return child_range(child_iterator(), child_iterator());\n  }\n  const_child_range children() const {\n    return const_child_range(const_child_iterator(), const_child_iterator());\n  }\n\n  SourceLocation getBeginLoc() const LLVM_READONLY { return TypoLoc; }\n  SourceLocation getEndLoc() const LLVM_READONLY { return TypoLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == TypoExprClass;\n  }\n\n};\n\n/// Frontend produces RecoveryExprs on semantic errors that prevent creating\n/// other well-formed expressions. E.g. when type-checking of a binary operator\n/// fails, we cannot produce a BinaryOperator expression. Instead, we can choose\n/// to produce a recovery expression storing left and right operands.\n///\n/// RecoveryExpr does not have any semantic meaning in C++, it is only useful to\n/// preserve expressions in AST that would otherwise be dropped. It captures\n/// subexpressions of some expression that we could not construct and source\n/// range covered by the expression.\n///\n/// By default, RecoveryExpr uses dependence-bits to take advantage of existing\n/// machinery to deal with dependent code in C++, e.g. RecoveryExpr is preserved\n/// in `decltype(<broken-expr>)` as part of the `DependentDecltypeType`. In\n/// addition to that, clang does not report most errors on dependent\n/// expressions, so we get rid of bogus errors for free. However, note that\n/// unlike other dependent expressions, RecoveryExpr can be produced in\n/// non-template contexts.\n///\n/// We will preserve the type in RecoveryExpr when the type is known, e.g.\n/// preserving the return type for a broken non-overloaded function call, a\n/// overloaded call where all candidates have the same return type. In this\n/// case, the expression is not type-dependent (unless the known type is itself\n/// dependent)\n///\n/// One can also reliably suppress all bogus errors on expressions containing\n/// recovery expressions by examining results of Expr::containsErrors().\nclass RecoveryExpr final : public Expr,\n                           private llvm::TrailingObjects<RecoveryExpr, Expr *> {\npublic:\n  static RecoveryExpr *Create(ASTContext &Ctx, QualType T,\n                              SourceLocation BeginLoc, SourceLocation EndLoc,\n                              ArrayRef<Expr *> SubExprs);\n  static RecoveryExpr *CreateEmpty(ASTContext &Ctx, unsigned NumSubExprs);\n\n  ArrayRef<Expr *> subExpressions() {\n    auto *B = getTrailingObjects<Expr *>();\n    return llvm::makeArrayRef(B, B + NumExprs);\n  }\n\n  ArrayRef<const Expr *> subExpressions() const {\n    return const_cast<RecoveryExpr *>(this)->subExpressions();\n  }\n\n  child_range children() {\n    Stmt **B = reinterpret_cast<Stmt **>(getTrailingObjects<Expr *>());\n    return child_range(B, B + NumExprs);\n  }\n\n  SourceLocation getBeginLoc() const { return BeginLoc; }\n  SourceLocation getEndLoc() const { return EndLoc; }\n\n  static bool classof(const Stmt *T) {\n    return T->getStmtClass() == RecoveryExprClass;\n  }\n\nprivate:\n  RecoveryExpr(ASTContext &Ctx, QualType T, SourceLocation BeginLoc,\n               SourceLocation EndLoc, ArrayRef<Expr *> SubExprs);\n  RecoveryExpr(EmptyShell Empty, unsigned NumSubExprs)\n      : Expr(RecoveryExprClass, Empty), NumExprs(NumSubExprs) {}\n\n  size_t numTrailingObjects(OverloadToken<Stmt *>) const { return NumExprs; }\n\n  SourceLocation BeginLoc, EndLoc;\n  unsigned NumExprs;\n  friend TrailingObjects;\n  friend class ASTStmtReader;\n  friend class ASTStmtWriter;\n};\n\n} // end namespace clang\n\n#endif // LLVM_CLANG_AST_EXPR_H\n"}, "34": {"id": 34, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCXXABI.h", "content": "//===----- CGCXXABI.h - Interface to C++ ABIs -------------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This provides an abstract class for C++ code generation. Concrete subclasses\n// of this implement code generation for specific C++ ABIs.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CGCXXABI_H\n#define LLVM_CLANG_LIB_CODEGEN_CGCXXABI_H\n\n#include \"CodeGenFunction.h\"\n#include \"clang/Basic/LLVM.h\"\n#include \"clang/CodeGen/CodeGenABITypes.h\"\n\nnamespace llvm {\nclass Constant;\nclass Type;\nclass Value;\nclass CallInst;\n}\n\nnamespace clang {\nclass CastExpr;\nclass CXXConstructorDecl;\nclass CXXDestructorDecl;\nclass CXXMethodDecl;\nclass CXXRecordDecl;\nclass FieldDecl;\nclass MangleContext;\n\nnamespace CodeGen {\nclass CGCallee;\nclass CodeGenFunction;\nclass CodeGenModule;\nstruct CatchTypeInfo;\n\n/// Implements C++ ABI-specific code generation functions.\nclass CGCXXABI {\nprotected:\n  CodeGenModule &CGM;\n  std::unique_ptr<MangleContext> MangleCtx;\n\n  CGCXXABI(CodeGenModule &CGM)\n    : CGM(CGM), MangleCtx(CGM.getContext().createMangleContext()) {}\n\nprotected:\n  ImplicitParamDecl *getThisDecl(CodeGenFunction &CGF) {\n    return CGF.CXXABIThisDecl;\n  }\n  llvm::Value *getThisValue(CodeGenFunction &CGF) {\n    return CGF.CXXABIThisValue;\n  }\n  Address getThisAddress(CodeGenFunction &CGF) {\n    return Address(CGF.CXXABIThisValue, CGF.CXXABIThisAlignment);\n  }\n\n  /// Issue a diagnostic about unsupported features in the ABI.\n  void ErrorUnsupportedABI(CodeGenFunction &CGF, StringRef S);\n\n  /// Get a null value for unsupported member pointers.\n  llvm::Constant *GetBogusMemberPointer(QualType T);\n\n  ImplicitParamDecl *&getStructorImplicitParamDecl(CodeGenFunction &CGF) {\n    return CGF.CXXStructorImplicitParamDecl;\n  }\n  llvm::Value *&getStructorImplicitParamValue(CodeGenFunction &CGF) {\n    return CGF.CXXStructorImplicitParamValue;\n  }\n\n  /// Loads the incoming C++ this pointer as it was passed by the caller.\n  llvm::Value *loadIncomingCXXThis(CodeGenFunction &CGF);\n\n  void setCXXABIThisValue(CodeGenFunction &CGF, llvm::Value *ThisPtr);\n\n  ASTContext &getContext() const { return CGM.getContext(); }\n\n  virtual bool requiresArrayCookie(const CXXDeleteExpr *E, QualType eltType);\n  virtual bool requiresArrayCookie(const CXXNewExpr *E);\n\n  /// Determine whether there's something special about the rules of\n  /// the ABI tell us that 'this' is a complete object within the\n  /// given function.  Obvious common logic like being defined on a\n  /// final class will have been taken care of by the caller.\n  virtual bool isThisCompleteObject(GlobalDecl GD) const = 0;\n\npublic:\n\n  virtual ~CGCXXABI();\n\n  /// Gets the mangle context.\n  MangleContext &getMangleContext() {\n    return *MangleCtx;\n  }\n\n  /// Returns true if the given constructor or destructor is one of the\n  /// kinds that the ABI says returns 'this' (only applies when called\n  /// non-virtually for destructors).\n  ///\n  /// There currently is no way to indicate if a destructor returns 'this'\n  /// when called virtually, and code generation does not support the case.\n  virtual bool HasThisReturn(GlobalDecl GD) const { return false; }\n\n  virtual bool hasMostDerivedReturn(GlobalDecl GD) const { return false; }\n\n  virtual bool useSinitAndSterm() const { return false; }\n\n  /// Returns true if the target allows calling a function through a pointer\n  /// with a different signature than the actual function (or equivalently,\n  /// bitcasting a function or function pointer to a different function type).\n  /// In principle in the most general case this could depend on the target, the\n  /// calling convention, and the actual types of the arguments and return\n  /// value. Here it just means whether the signature mismatch could *ever* be\n  /// allowed; in other words, does the target do strict checking of signatures\n  /// for all calls.\n  virtual bool canCallMismatchedFunctionType() const { return true; }\n\n  /// If the C++ ABI requires the given type be returned in a particular way,\n  /// this method sets RetAI and returns true.\n  virtual bool classifyReturnType(CGFunctionInfo &FI) const = 0;\n\n  /// Specify how one should pass an argument of a record type.\n  enum RecordArgABI {\n    /// Pass it using the normal C aggregate rules for the ABI, potentially\n    /// introducing extra copies and passing some or all of it in registers.\n    RAA_Default = 0,\n\n    /// Pass it on the stack using its defined layout.  The argument must be\n    /// evaluated directly into the correct stack position in the arguments area,\n    /// and the call machinery must not move it or introduce extra copies.\n    RAA_DirectInMemory,\n\n    /// Pass it as a pointer to temporary memory.\n    RAA_Indirect\n  };\n\n  /// Returns how an argument of the given record type should be passed.\n  virtual RecordArgABI getRecordArgABI(const CXXRecordDecl *RD) const = 0;\n\n  /// Returns true if the implicit 'sret' parameter comes after the implicit\n  /// 'this' parameter of C++ instance methods.\n  virtual bool isSRetParameterAfterThis() const { return false; }\n\n  /// Returns true if the ABI permits the argument to be a homogeneous\n  /// aggregate.\n  virtual bool\n  isPermittedToBeHomogeneousAggregate(const CXXRecordDecl *RD) const {\n    return true;\n  };\n\n  /// Find the LLVM type used to represent the given member pointer\n  /// type.\n  virtual llvm::Type *\n  ConvertMemberPointerType(const MemberPointerType *MPT);\n\n  /// Load a member function from an object and a member function\n  /// pointer.  Apply the this-adjustment and set 'This' to the\n  /// adjusted value.\n  virtual CGCallee EmitLoadOfMemberFunctionPointer(\n      CodeGenFunction &CGF, const Expr *E, Address This,\n      llvm::Value *&ThisPtrForCall, llvm::Value *MemPtr,\n      const MemberPointerType *MPT);\n\n  /// Calculate an l-value from an object and a data member pointer.\n  virtual llvm::Value *\n  EmitMemberDataPointerAddress(CodeGenFunction &CGF, const Expr *E,\n                               Address Base, llvm::Value *MemPtr,\n                               const MemberPointerType *MPT);\n\n  /// Perform a derived-to-base, base-to-derived, or bitcast member\n  /// pointer conversion.\n  virtual llvm::Value *EmitMemberPointerConversion(CodeGenFunction &CGF,\n                                                   const CastExpr *E,\n                                                   llvm::Value *Src);\n\n  /// Perform a derived-to-base, base-to-derived, or bitcast member\n  /// pointer conversion on a constant value.\n  virtual llvm::Constant *EmitMemberPointerConversion(const CastExpr *E,\n                                                      llvm::Constant *Src);\n\n  /// Return true if the given member pointer can be zero-initialized\n  /// (in the C++ sense) with an LLVM zeroinitializer.\n  virtual bool isZeroInitializable(const MemberPointerType *MPT);\n\n  /// Return whether or not a member pointers type is convertible to an IR type.\n  virtual bool isMemberPointerConvertible(const MemberPointerType *MPT) const {\n    return true;\n  }\n\n  /// Create a null member pointer of the given type.\n  virtual llvm::Constant *EmitNullMemberPointer(const MemberPointerType *MPT);\n\n  /// Create a member pointer for the given method.\n  virtual llvm::Constant *EmitMemberFunctionPointer(const CXXMethodDecl *MD);\n\n  /// Create a member pointer for the given field.\n  virtual llvm::Constant *EmitMemberDataPointer(const MemberPointerType *MPT,\n                                                CharUnits offset);\n\n  /// Create a member pointer for the given member pointer constant.\n  virtual llvm::Constant *EmitMemberPointer(const APValue &MP, QualType MPT);\n\n  /// Emit a comparison between two member pointers.  Returns an i1.\n  virtual llvm::Value *\n  EmitMemberPointerComparison(CodeGenFunction &CGF,\n                              llvm::Value *L,\n                              llvm::Value *R,\n                              const MemberPointerType *MPT,\n                              bool Inequality);\n\n  /// Determine if a member pointer is non-null.  Returns an i1.\n  virtual llvm::Value *\n  EmitMemberPointerIsNotNull(CodeGenFunction &CGF,\n                             llvm::Value *MemPtr,\n                             const MemberPointerType *MPT);\n\nprotected:\n  /// A utility method for computing the offset required for the given\n  /// base-to-derived or derived-to-base member-pointer conversion.\n  /// Does not handle virtual conversions (in case we ever fully\n  /// support an ABI that allows this).  Returns null if no adjustment\n  /// is required.\n  llvm::Constant *getMemberPointerAdjustment(const CastExpr *E);\n\npublic:\n  virtual void emitVirtualObjectDelete(CodeGenFunction &CGF,\n                                       const CXXDeleteExpr *DE,\n                                       Address Ptr, QualType ElementType,\n                                       const CXXDestructorDecl *Dtor) = 0;\n  virtual void emitRethrow(CodeGenFunction &CGF, bool isNoReturn) = 0;\n  virtual void emitThrow(CodeGenFunction &CGF, const CXXThrowExpr *E) = 0;\n  virtual llvm::GlobalVariable *getThrowInfo(QualType T) { return nullptr; }\n\n  /// Determine whether it's possible to emit a vtable for \\p RD, even\n  /// though we do not know that the vtable has been marked as used by semantic\n  /// analysis.\n  virtual bool canSpeculativelyEmitVTable(const CXXRecordDecl *RD) const = 0;\n\n  virtual void emitBeginCatch(CodeGenFunction &CGF, const CXXCatchStmt *C) = 0;\n\n  virtual llvm::CallInst *\n  emitTerminateForUnexpectedException(CodeGenFunction &CGF,\n                                      llvm::Value *Exn);\n\n  virtual llvm::Constant *getAddrOfRTTIDescriptor(QualType Ty) = 0;\n  virtual CatchTypeInfo\n  getAddrOfCXXCatchHandlerType(QualType Ty, QualType CatchHandlerType) = 0;\n  virtual CatchTypeInfo getCatchAllTypeInfo();\n\n  virtual bool shouldTypeidBeNullChecked(bool IsDeref,\n                                         QualType SrcRecordTy) = 0;\n  virtual void EmitBadTypeidCall(CodeGenFunction &CGF) = 0;\n  virtual llvm::Value *EmitTypeid(CodeGenFunction &CGF, QualType SrcRecordTy,\n                                  Address ThisPtr,\n                                  llvm::Type *StdTypeInfoPtrTy) = 0;\n\n  virtual bool shouldDynamicCastCallBeNullChecked(bool SrcIsPtr,\n                                                  QualType SrcRecordTy) = 0;\n\n  virtual llvm::Value *\n  EmitDynamicCastCall(CodeGenFunction &CGF, Address Value,\n                      QualType SrcRecordTy, QualType DestTy,\n                      QualType DestRecordTy, llvm::BasicBlock *CastEnd) = 0;\n\n  virtual llvm::Value *EmitDynamicCastToVoid(CodeGenFunction &CGF,\n                                             Address Value,\n                                             QualType SrcRecordTy,\n                                             QualType DestTy) = 0;\n\n  virtual bool EmitBadCastCall(CodeGenFunction &CGF) = 0;\n\n  virtual llvm::Value *GetVirtualBaseClassOffset(CodeGenFunction &CGF,\n                                                 Address This,\n                                                 const CXXRecordDecl *ClassDecl,\n                                        const CXXRecordDecl *BaseClassDecl) = 0;\n\n  virtual llvm::BasicBlock *EmitCtorCompleteObjectHandler(CodeGenFunction &CGF,\n                                                          const CXXRecordDecl *RD);\n\n  /// Emit the code to initialize hidden members required\n  /// to handle virtual inheritance, if needed by the ABI.\n  virtual void\n  initializeHiddenVirtualInheritanceMembers(CodeGenFunction &CGF,\n                                            const CXXRecordDecl *RD) {}\n\n  /// Emit constructor variants required by this ABI.\n  virtual void EmitCXXConstructors(const CXXConstructorDecl *D) = 0;\n\n  /// Additional implicit arguments to add to the beginning (Prefix) and end\n  /// (Suffix) of a constructor / destructor arg list.\n  ///\n  /// Note that Prefix should actually be inserted *after* the first existing\n  /// arg; `this` arguments always come first.\n  struct AddedStructorArgs {\n    struct Arg {\n      llvm::Value *Value;\n      QualType Type;\n    };\n    SmallVector<Arg, 1> Prefix;\n    SmallVector<Arg, 1> Suffix;\n    AddedStructorArgs() = default;\n    AddedStructorArgs(SmallVector<Arg, 1> P, SmallVector<Arg, 1> S)\n        : Prefix(std::move(P)), Suffix(std::move(S)) {}\n    static AddedStructorArgs prefix(SmallVector<Arg, 1> Args) {\n      return {std::move(Args), {}};\n    }\n    static AddedStructorArgs suffix(SmallVector<Arg, 1> Args) {\n      return {{}, std::move(Args)};\n    }\n  };\n\n  /// Similar to AddedStructorArgs, but only notes the number of additional\n  /// arguments.\n  struct AddedStructorArgCounts {\n    unsigned Prefix = 0;\n    unsigned Suffix = 0;\n    AddedStructorArgCounts() = default;\n    AddedStructorArgCounts(unsigned P, unsigned S) : Prefix(P), Suffix(S) {}\n    static AddedStructorArgCounts prefix(unsigned N) { return {N, 0}; }\n    static AddedStructorArgCounts suffix(unsigned N) { return {0, N}; }\n  };\n\n  /// Build the signature of the given constructor or destructor variant by\n  /// adding any required parameters.  For convenience, ArgTys has been\n  /// initialized with the type of 'this'.\n  virtual AddedStructorArgCounts\n  buildStructorSignature(GlobalDecl GD,\n                         SmallVectorImpl<CanQualType> &ArgTys) = 0;\n\n  /// Returns true if the given destructor type should be emitted as a linkonce\n  /// delegating thunk, regardless of whether the dtor is defined in this TU or\n  /// not.\n  virtual bool useThunkForDtorVariant(const CXXDestructorDecl *Dtor,\n                                      CXXDtorType DT) const = 0;\n\n  virtual void setCXXDestructorDLLStorage(llvm::GlobalValue *GV,\n                                          const CXXDestructorDecl *Dtor,\n                                          CXXDtorType DT) const;\n\n  virtual llvm::GlobalValue::LinkageTypes\n  getCXXDestructorLinkage(GVALinkage Linkage, const CXXDestructorDecl *Dtor,\n                          CXXDtorType DT) const;\n\n  /// Emit destructor variants required by this ABI.\n  virtual void EmitCXXDestructors(const CXXDestructorDecl *D) = 0;\n\n  /// Get the type of the implicit \"this\" parameter used by a method. May return\n  /// zero if no specific type is applicable, e.g. if the ABI expects the \"this\"\n  /// parameter to point to some artificial offset in a complete object due to\n  /// vbases being reordered.\n  virtual const CXXRecordDecl *\n  getThisArgumentTypeForMethod(const CXXMethodDecl *MD) {\n    return MD->getParent();\n  }\n\n  /// Perform ABI-specific \"this\" argument adjustment required prior to\n  /// a call of a virtual function.\n  /// The \"VirtualCall\" argument is true iff the call itself is virtual.\n  virtual Address\n  adjustThisArgumentForVirtualFunctionCall(CodeGenFunction &CGF, GlobalDecl GD,\n                                           Address This, bool VirtualCall) {\n    return This;\n  }\n\n  /// Build a parameter variable suitable for 'this'.\n  void buildThisParam(CodeGenFunction &CGF, FunctionArgList &Params);\n\n  /// Insert any ABI-specific implicit parameters into the parameter list for a\n  /// function.  This generally involves extra data for constructors and\n  /// destructors.\n  ///\n  /// ABIs may also choose to override the return type, which has been\n  /// initialized with the type of 'this' if HasThisReturn(CGF.CurGD) is true or\n  /// the formal return type of the function otherwise.\n  virtual void addImplicitStructorParams(CodeGenFunction &CGF, QualType &ResTy,\n                                         FunctionArgList &Params) = 0;\n\n  /// Get the ABI-specific \"this\" parameter adjustment to apply in the prologue\n  /// of a virtual function.\n  virtual CharUnits getVirtualFunctionPrologueThisAdjustment(GlobalDecl GD) {\n    return CharUnits::Zero();\n  }\n\n  /// Emit the ABI-specific prolog for the function.\n  virtual void EmitInstanceFunctionProlog(CodeGenFunction &CGF) = 0;\n\n  virtual AddedStructorArgs\n  getImplicitConstructorArgs(CodeGenFunction &CGF, const CXXConstructorDecl *D,\n                             CXXCtorType Type, bool ForVirtualBase,\n                             bool Delegating) = 0;\n\n  /// Add any ABI-specific implicit arguments needed to call a constructor.\n  ///\n  /// \\return The number of arguments added at the beginning and end of the\n  /// call, which is typically zero or one.\n  AddedStructorArgCounts\n  addImplicitConstructorArgs(CodeGenFunction &CGF, const CXXConstructorDecl *D,\n                             CXXCtorType Type, bool ForVirtualBase,\n                             bool Delegating, CallArgList &Args);\n\n  /// Get the implicit (second) parameter that comes after the \"this\" pointer,\n  /// or nullptr if there is isn't one.\n  virtual llvm::Value *\n  getCXXDestructorImplicitParam(CodeGenFunction &CGF,\n                                const CXXDestructorDecl *DD, CXXDtorType Type,\n                                bool ForVirtualBase, bool Delegating) = 0;\n\n  /// Emit the destructor call.\n  virtual void EmitDestructorCall(CodeGenFunction &CGF,\n                                  const CXXDestructorDecl *DD, CXXDtorType Type,\n                                  bool ForVirtualBase, bool Delegating,\n                                  Address This, QualType ThisTy) = 0;\n\n  /// Emits the VTable definitions required for the given record type.\n  virtual void emitVTableDefinitions(CodeGenVTables &CGVT,\n                                     const CXXRecordDecl *RD) = 0;\n\n  /// Checks if ABI requires extra virtual offset for vtable field.\n  virtual bool\n  isVirtualOffsetNeededForVTableField(CodeGenFunction &CGF,\n                                      CodeGenFunction::VPtr Vptr) = 0;\n\n  /// Checks if ABI requires to initialize vptrs for given dynamic class.\n  virtual bool doStructorsInitializeVPtrs(const CXXRecordDecl *VTableClass) = 0;\n\n  /// Get the address point of the vtable for the given base subobject.\n  virtual llvm::Constant *\n  getVTableAddressPoint(BaseSubobject Base,\n                        const CXXRecordDecl *VTableClass) = 0;\n\n  /// Get the address point of the vtable for the given base subobject while\n  /// building a constructor or a destructor.\n  virtual llvm::Value *\n  getVTableAddressPointInStructor(CodeGenFunction &CGF, const CXXRecordDecl *RD,\n                                  BaseSubobject Base,\n                                  const CXXRecordDecl *NearestVBase) = 0;\n\n  /// Get the address point of the vtable for the given base subobject while\n  /// building a constexpr.\n  virtual llvm::Constant *\n  getVTableAddressPointForConstExpr(BaseSubobject Base,\n                                    const CXXRecordDecl *VTableClass) = 0;\n\n  /// Get the address of the vtable for the given record decl which should be\n  /// used for the vptr at the given offset in RD.\n  virtual llvm::GlobalVariable *getAddrOfVTable(const CXXRecordDecl *RD,\n                                                CharUnits VPtrOffset) = 0;\n\n  /// Build a virtual function pointer in the ABI-specific way.\n  virtual CGCallee getVirtualFunctionPointer(CodeGenFunction &CGF,\n                                             GlobalDecl GD, Address This,\n                                             llvm::Type *Ty,\n                                             SourceLocation Loc) = 0;\n\n  using DeleteOrMemberCallExpr =\n      llvm::PointerUnion<const CXXDeleteExpr *, const CXXMemberCallExpr *>;\n\n  /// Emit the ABI-specific virtual destructor call.\n  virtual llvm::Value *EmitVirtualDestructorCall(CodeGenFunction &CGF,\n                                                 const CXXDestructorDecl *Dtor,\n                                                 CXXDtorType DtorType,\n                                                 Address This,\n                                                 DeleteOrMemberCallExpr E) = 0;\n\n  virtual void adjustCallArgsForDestructorThunk(CodeGenFunction &CGF,\n                                                GlobalDecl GD,\n                                                CallArgList &CallArgs) {}\n\n  /// Emit any tables needed to implement virtual inheritance.  For Itanium,\n  /// this emits virtual table tables.  For the MSVC++ ABI, this emits virtual\n  /// base tables.\n  virtual void emitVirtualInheritanceTables(const CXXRecordDecl *RD) = 0;\n\n  virtual bool exportThunk() = 0;\n  virtual void setThunkLinkage(llvm::Function *Thunk, bool ForVTable,\n                               GlobalDecl GD, bool ReturnAdjustment) = 0;\n\n  virtual llvm::Value *performThisAdjustment(CodeGenFunction &CGF,\n                                             Address This,\n                                             const ThisAdjustment &TA) = 0;\n\n  virtual llvm::Value *performReturnAdjustment(CodeGenFunction &CGF,\n                                               Address Ret,\n                                               const ReturnAdjustment &RA) = 0;\n\n  virtual void EmitReturnFromThunk(CodeGenFunction &CGF,\n                                   RValue RV, QualType ResultType);\n\n  virtual size_t getSrcArgforCopyCtor(const CXXConstructorDecl *,\n                                      FunctionArgList &Args) const = 0;\n\n  /// Gets the offsets of all the virtual base pointers in a given class.\n  virtual std::vector<CharUnits> getVBPtrOffsets(const CXXRecordDecl *RD);\n\n  /// Gets the pure virtual member call function.\n  virtual StringRef GetPureVirtualCallName() = 0;\n\n  /// Gets the deleted virtual member call name.\n  virtual StringRef GetDeletedVirtualCallName() = 0;\n\n  /**************************** Array cookies ******************************/\n\n  /// Returns the extra size required in order to store the array\n  /// cookie for the given new-expression.  May return 0 to indicate that no\n  /// array cookie is required.\n  ///\n  /// Several cases are filtered out before this method is called:\n  ///   - non-array allocations never need a cookie\n  ///   - calls to \\::operator new(size_t, void*) never need a cookie\n  ///\n  /// \\param expr - the new-expression being allocated.\n  virtual CharUnits GetArrayCookieSize(const CXXNewExpr *expr);\n\n  /// Initialize the array cookie for the given allocation.\n  ///\n  /// \\param NewPtr - a char* which is the presumed-non-null\n  ///   return value of the allocation function\n  /// \\param NumElements - the computed number of elements,\n  ///   potentially collapsed from the multidimensional array case;\n  ///   always a size_t\n  /// \\param ElementType - the base element allocated type,\n  ///   i.e. the allocated type after stripping all array types\n  virtual Address InitializeArrayCookie(CodeGenFunction &CGF,\n                                        Address NewPtr,\n                                        llvm::Value *NumElements,\n                                        const CXXNewExpr *expr,\n                                        QualType ElementType);\n\n  /// Reads the array cookie associated with the given pointer,\n  /// if it has one.\n  ///\n  /// \\param Ptr - a pointer to the first element in the array\n  /// \\param ElementType - the base element type of elements of the array\n  /// \\param NumElements - an out parameter which will be initialized\n  ///   with the number of elements allocated, or zero if there is no\n  ///   cookie\n  /// \\param AllocPtr - an out parameter which will be initialized\n  ///   with a char* pointing to the address returned by the allocation\n  ///   function\n  /// \\param CookieSize - an out parameter which will be initialized\n  ///   with the size of the cookie, or zero if there is no cookie\n  virtual void ReadArrayCookie(CodeGenFunction &CGF, Address Ptr,\n                               const CXXDeleteExpr *expr,\n                               QualType ElementType, llvm::Value *&NumElements,\n                               llvm::Value *&AllocPtr, CharUnits &CookieSize);\n\n  /// Return whether the given global decl needs a VTT parameter.\n  virtual bool NeedsVTTParameter(GlobalDecl GD);\n\nprotected:\n  /// Returns the extra size required in order to store the array\n  /// cookie for the given type.  Assumes that an array cookie is\n  /// required.\n  virtual CharUnits getArrayCookieSizeImpl(QualType elementType);\n\n  /// Reads the array cookie for an allocation which is known to have one.\n  /// This is called by the standard implementation of ReadArrayCookie.\n  ///\n  /// \\param ptr - a pointer to the allocation made for an array, as a char*\n  /// \\param cookieSize - the computed cookie size of an array\n  ///\n  /// Other parameters are as above.\n  ///\n  /// \\return a size_t\n  virtual llvm::Value *readArrayCookieImpl(CodeGenFunction &IGF, Address ptr,\n                                           CharUnits cookieSize);\n\npublic:\n\n  /*************************** Static local guards ****************************/\n\n  /// Emits the guarded initializer and destructor setup for the given\n  /// variable, given that it couldn't be emitted as a constant.\n  /// If \\p PerformInit is false, the initialization has been folded to a\n  /// constant and should not be performed.\n  ///\n  /// The variable may be:\n  ///   - a static local variable\n  ///   - a static data member of a class template instantiation\n  virtual void EmitGuardedInit(CodeGenFunction &CGF, const VarDecl &D,\n                               llvm::GlobalVariable *DeclPtr,\n                               bool PerformInit) = 0;\n\n  /// Emit code to force the execution of a destructor during global\n  /// teardown.  The default implementation of this uses atexit.\n  ///\n  /// \\param Dtor - a function taking a single pointer argument\n  /// \\param Addr - a pointer to pass to the destructor function.\n  virtual void registerGlobalDtor(CodeGenFunction &CGF, const VarDecl &D,\n                                  llvm::FunctionCallee Dtor,\n                                  llvm::Constant *Addr) = 0;\n\n  /*************************** thread_local initialization ********************/\n\n  /// Emits ABI-required functions necessary to initialize thread_local\n  /// variables in this translation unit.\n  ///\n  /// \\param CXXThreadLocals - The thread_local declarations in this translation\n  ///        unit.\n  /// \\param CXXThreadLocalInits - If this translation unit contains any\n  ///        non-constant initialization or non-trivial destruction for\n  ///        thread_local variables, a list of functions to perform the\n  ///        initialization.\n  virtual void EmitThreadLocalInitFuncs(\n      CodeGenModule &CGM, ArrayRef<const VarDecl *> CXXThreadLocals,\n      ArrayRef<llvm::Function *> CXXThreadLocalInits,\n      ArrayRef<const VarDecl *> CXXThreadLocalInitVars) = 0;\n\n  // Determine if references to thread_local global variables can be made\n  // directly or require access through a thread wrapper function.\n  virtual bool usesThreadWrapperFunction(const VarDecl *VD) const = 0;\n\n  /// Emit a reference to a non-local thread_local variable (including\n  /// triggering the initialization of all thread_local variables in its\n  /// translation unit).\n  virtual LValue EmitThreadLocalVarDeclLValue(CodeGenFunction &CGF,\n                                              const VarDecl *VD,\n                                              QualType LValType) = 0;\n\n  /// Emit a single constructor/destructor with the given type from a C++\n  /// constructor Decl.\n  virtual void emitCXXStructor(GlobalDecl GD) = 0;\n\n  /// Load a vtable from This, an object of polymorphic type RD, or from one of\n  /// its virtual bases if it does not have its own vtable. Returns the vtable\n  /// and the class from which the vtable was loaded.\n  virtual std::pair<llvm::Value *, const CXXRecordDecl *>\n  LoadVTablePtr(CodeGenFunction &CGF, Address This,\n                const CXXRecordDecl *RD) = 0;\n};\n\n// Create an instance of a C++ ABI class:\n\n/// Creates an Itanium-family ABI.\nCGCXXABI *CreateItaniumCXXABI(CodeGenModule &CGM);\n\n/// Creates a Microsoft-family ABI.\nCGCXXABI *CreateMicrosoftCXXABI(CodeGenModule &CGM);\n\nstruct CatchRetScope final : EHScopeStack::Cleanup {\n  llvm::CatchPadInst *CPI;\n\n  CatchRetScope(llvm::CatchPadInst *CPI) : CPI(CPI) {}\n\n  void Emit(CodeGenFunction &CGF, Flags flags) override {\n    llvm::BasicBlock *BB = CGF.createBasicBlock(\"catchret.dest\");\n    CGF.Builder.CreateCatchRet(CPI, BB);\n    CGF.EmitBlock(BB);\n  }\n};\n}\n}\n\n#endif\n"}, "35": {"id": 35, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCleanup.h", "content": "//===-- CGCleanup.h - Classes for cleanups IR generation --------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// These classes support the generation of LLVM IR for cleanups.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CGCLEANUP_H\n#define LLVM_CLANG_LIB_CODEGEN_CGCLEANUP_H\n\n#include \"EHScopeStack.h\"\n\n#include \"Address.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n\nnamespace llvm {\nclass BasicBlock;\nclass Value;\nclass ConstantInt;\nclass AllocaInst;\n}\n\nnamespace clang {\nclass FunctionDecl;\nnamespace CodeGen {\nclass CodeGenModule;\nclass CodeGenFunction;\n\n/// The MS C++ ABI needs a pointer to RTTI data plus some flags to describe the\n/// type of a catch handler, so we use this wrapper.\nstruct CatchTypeInfo {\n  llvm::Constant *RTTI;\n  unsigned Flags;\n};\n\n/// A protected scope for zero-cost EH handling.\nclass EHScope {\n  llvm::BasicBlock *CachedLandingPad;\n  llvm::BasicBlock *CachedEHDispatchBlock;\n\n  EHScopeStack::stable_iterator EnclosingEHScope;\n\n  class CommonBitFields {\n    friend class EHScope;\n    unsigned Kind : 3;\n  };\n  enum { NumCommonBits = 3 };\n\nprotected:\n  class CatchBitFields {\n    friend class EHCatchScope;\n    unsigned : NumCommonBits;\n\n    unsigned NumHandlers : 32 - NumCommonBits;\n  };\n\n  class CleanupBitFields {\n    friend class EHCleanupScope;\n    unsigned : NumCommonBits;\n\n    /// Whether this cleanup needs to be run along normal edges.\n    unsigned IsNormalCleanup : 1;\n\n    /// Whether this cleanup needs to be run along exception edges.\n    unsigned IsEHCleanup : 1;\n\n    /// Whether this cleanup is currently active.\n    unsigned IsActive : 1;\n\n    /// Whether this cleanup is a lifetime marker\n    unsigned IsLifetimeMarker : 1;\n\n    /// Whether the normal cleanup should test the activation flag.\n    unsigned TestFlagInNormalCleanup : 1;\n\n    /// Whether the EH cleanup should test the activation flag.\n    unsigned TestFlagInEHCleanup : 1;\n\n    /// The amount of extra storage needed by the Cleanup.\n    /// Always a multiple of the scope-stack alignment.\n    unsigned CleanupSize : 12;\n  };\n\n  class FilterBitFields {\n    friend class EHFilterScope;\n    unsigned : NumCommonBits;\n\n    unsigned NumFilters : 32 - NumCommonBits;\n  };\n\n  union {\n    CommonBitFields CommonBits;\n    CatchBitFields CatchBits;\n    CleanupBitFields CleanupBits;\n    FilterBitFields FilterBits;\n  };\n\npublic:\n  enum Kind { Cleanup, Catch, Terminate, Filter };\n\n  EHScope(Kind kind, EHScopeStack::stable_iterator enclosingEHScope)\n    : CachedLandingPad(nullptr), CachedEHDispatchBlock(nullptr),\n      EnclosingEHScope(enclosingEHScope) {\n    CommonBits.Kind = kind;\n  }\n\n  Kind getKind() const { return static_cast<Kind>(CommonBits.Kind); }\n\n  llvm::BasicBlock *getCachedLandingPad() const {\n    return CachedLandingPad;\n  }\n\n  void setCachedLandingPad(llvm::BasicBlock *block) {\n    CachedLandingPad = block;\n  }\n\n  llvm::BasicBlock *getCachedEHDispatchBlock() const {\n    return CachedEHDispatchBlock;\n  }\n\n  void setCachedEHDispatchBlock(llvm::BasicBlock *block) {\n    CachedEHDispatchBlock = block;\n  }\n\n  bool hasEHBranches() const {\n    if (llvm::BasicBlock *block = getCachedEHDispatchBlock())\n      return !block->use_empty();\n    return false;\n  }\n\n  EHScopeStack::stable_iterator getEnclosingEHScope() const {\n    return EnclosingEHScope;\n  }\n};\n\n/// A scope which attempts to handle some, possibly all, types of\n/// exceptions.\n///\n/// Objective C \\@finally blocks are represented using a cleanup scope\n/// after the catch scope.\nclass EHCatchScope : public EHScope {\n  // In effect, we have a flexible array member\n  //   Handler Handlers[0];\n  // But that's only standard in C99, not C++, so we have to do\n  // annoying pointer arithmetic instead.\n\npublic:\n  struct Handler {\n    /// A type info value, or null (C++ null, not an LLVM null pointer)\n    /// for a catch-all.\n    CatchTypeInfo Type;\n\n    /// The catch handler for this type.\n    llvm::BasicBlock *Block;\n\n    bool isCatchAll() const { return Type.RTTI == nullptr; }\n  };\n\nprivate:\n  friend class EHScopeStack;\n\n  Handler *getHandlers() {\n    return reinterpret_cast<Handler*>(this+1);\n  }\n\n  const Handler *getHandlers() const {\n    return reinterpret_cast<const Handler*>(this+1);\n  }\n\npublic:\n  static size_t getSizeForNumHandlers(unsigned N) {\n    return sizeof(EHCatchScope) + N * sizeof(Handler);\n  }\n\n  EHCatchScope(unsigned numHandlers,\n               EHScopeStack::stable_iterator enclosingEHScope)\n    : EHScope(Catch, enclosingEHScope) {\n    CatchBits.NumHandlers = numHandlers;\n    assert(CatchBits.NumHandlers == numHandlers && \"NumHandlers overflow?\");\n  }\n\n  unsigned getNumHandlers() const {\n    return CatchBits.NumHandlers;\n  }\n\n  void setCatchAllHandler(unsigned I, llvm::BasicBlock *Block) {\n    setHandler(I, CatchTypeInfo{nullptr, 0}, Block);\n  }\n\n  void setHandler(unsigned I, llvm::Constant *Type, llvm::BasicBlock *Block) {\n    assert(I < getNumHandlers());\n    getHandlers()[I].Type = CatchTypeInfo{Type, 0};\n    getHandlers()[I].Block = Block;\n  }\n\n  void setHandler(unsigned I, CatchTypeInfo Type, llvm::BasicBlock *Block) {\n    assert(I < getNumHandlers());\n    getHandlers()[I].Type = Type;\n    getHandlers()[I].Block = Block;\n  }\n\n  const Handler &getHandler(unsigned I) const {\n    assert(I < getNumHandlers());\n    return getHandlers()[I];\n  }\n\n  // Clear all handler blocks.\n  // FIXME: it's better to always call clearHandlerBlocks in DTOR and have a\n  // 'takeHandler' or some such function which removes ownership from the\n  // EHCatchScope object if the handlers should live longer than EHCatchScope.\n  void clearHandlerBlocks() {\n    for (unsigned I = 0, N = getNumHandlers(); I != N; ++I)\n      delete getHandler(I).Block;\n  }\n\n  typedef const Handler *iterator;\n  iterator begin() const { return getHandlers(); }\n  iterator end() const { return getHandlers() + getNumHandlers(); }\n\n  static bool classof(const EHScope *Scope) {\n    return Scope->getKind() == Catch;\n  }\n};\n\n/// A cleanup scope which generates the cleanup blocks lazily.\nclass alignas(8) EHCleanupScope : public EHScope {\n  /// The nearest normal cleanup scope enclosing this one.\n  EHScopeStack::stable_iterator EnclosingNormal;\n\n  /// The nearest EH scope enclosing this one.\n  EHScopeStack::stable_iterator EnclosingEH;\n\n  /// The dual entry/exit block along the normal edge.  This is lazily\n  /// created if needed before the cleanup is popped.\n  llvm::BasicBlock *NormalBlock;\n\n  /// An optional i1 variable indicating whether this cleanup has been\n  /// activated yet.\n  llvm::AllocaInst *ActiveFlag;\n\n  /// Extra information required for cleanups that have resolved\n  /// branches through them.  This has to be allocated on the side\n  /// because everything on the cleanup stack has be trivially\n  /// movable.\n  struct ExtInfo {\n    /// The destinations of normal branch-afters and branch-throughs.\n    llvm::SmallPtrSet<llvm::BasicBlock*, 4> Branches;\n\n    /// Normal branch-afters.\n    SmallVector<std::pair<llvm::BasicBlock*,llvm::ConstantInt*>, 4>\n      BranchAfters;\n  };\n  mutable struct ExtInfo *ExtInfo;\n\n  /// The number of fixups required by enclosing scopes (not including\n  /// this one).  If this is the top cleanup scope, all the fixups\n  /// from this index onwards belong to this scope.\n  unsigned FixupDepth;\n\n  struct ExtInfo &getExtInfo() {\n    if (!ExtInfo) ExtInfo = new struct ExtInfo();\n    return *ExtInfo;\n  }\n\n  const struct ExtInfo &getExtInfo() const {\n    if (!ExtInfo) ExtInfo = new struct ExtInfo();\n    return *ExtInfo;\n  }\n\npublic:\n  /// Gets the size required for a lazy cleanup scope with the given\n  /// cleanup-data requirements.\n  static size_t getSizeForCleanupSize(size_t Size) {\n    return sizeof(EHCleanupScope) + Size;\n  }\n\n  size_t getAllocatedSize() const {\n    return sizeof(EHCleanupScope) + CleanupBits.CleanupSize;\n  }\n\n  EHCleanupScope(bool isNormal, bool isEH, unsigned cleanupSize,\n                 unsigned fixupDepth,\n                 EHScopeStack::stable_iterator enclosingNormal,\n                 EHScopeStack::stable_iterator enclosingEH)\n      : EHScope(EHScope::Cleanup, enclosingEH),\n        EnclosingNormal(enclosingNormal), NormalBlock(nullptr),\n        ActiveFlag(nullptr), ExtInfo(nullptr), FixupDepth(fixupDepth) {\n    CleanupBits.IsNormalCleanup = isNormal;\n    CleanupBits.IsEHCleanup = isEH;\n    CleanupBits.IsActive = true;\n    CleanupBits.IsLifetimeMarker = false;\n    CleanupBits.TestFlagInNormalCleanup = false;\n    CleanupBits.TestFlagInEHCleanup = false;\n    CleanupBits.CleanupSize = cleanupSize;\n\n    assert(CleanupBits.CleanupSize == cleanupSize && \"cleanup size overflow\");\n  }\n\n  void Destroy() {\n    delete ExtInfo;\n  }\n  // Objects of EHCleanupScope are not destructed. Use Destroy().\n  ~EHCleanupScope() = delete;\n\n  bool isNormalCleanup() const { return CleanupBits.IsNormalCleanup; }\n  llvm::BasicBlock *getNormalBlock() const { return NormalBlock; }\n  void setNormalBlock(llvm::BasicBlock *BB) { NormalBlock = BB; }\n\n  bool isEHCleanup() const { return CleanupBits.IsEHCleanup; }\n\n  bool isActive() const { return CleanupBits.IsActive; }\n  void setActive(bool A) { CleanupBits.IsActive = A; }\n\n  bool isLifetimeMarker() const { return CleanupBits.IsLifetimeMarker; }\n  void setLifetimeMarker() { CleanupBits.IsLifetimeMarker = true; }\n\n  bool hasActiveFlag() const { return ActiveFlag != nullptr; }\n  Address getActiveFlag() const {\n    return Address(ActiveFlag, CharUnits::One());\n  }\n  void setActiveFlag(Address Var) {\n    assert(Var.getAlignment().isOne());\n    ActiveFlag = cast<llvm::AllocaInst>(Var.getPointer());\n  }\n\n  void setTestFlagInNormalCleanup() {\n    CleanupBits.TestFlagInNormalCleanup = true;\n  }\n  bool shouldTestFlagInNormalCleanup() const {\n    return CleanupBits.TestFlagInNormalCleanup;\n  }\n\n  void setTestFlagInEHCleanup() {\n    CleanupBits.TestFlagInEHCleanup = true;\n  }\n  bool shouldTestFlagInEHCleanup() const {\n    return CleanupBits.TestFlagInEHCleanup;\n  }\n\n  unsigned getFixupDepth() const { return FixupDepth; }\n  EHScopeStack::stable_iterator getEnclosingNormalCleanup() const {\n    return EnclosingNormal;\n  }\n\n  size_t getCleanupSize() const { return CleanupBits.CleanupSize; }\n  void *getCleanupBuffer() { return this + 1; }\n\n  EHScopeStack::Cleanup *getCleanup() {\n    return reinterpret_cast<EHScopeStack::Cleanup*>(getCleanupBuffer());\n  }\n\n  /// True if this cleanup scope has any branch-afters or branch-throughs.\n  bool hasBranches() const { return ExtInfo && !ExtInfo->Branches.empty(); }\n\n  /// Add a branch-after to this cleanup scope.  A branch-after is a\n  /// branch from a point protected by this (normal) cleanup to a\n  /// point in the normal cleanup scope immediately containing it.\n  /// For example,\n  ///   for (;;) { A a; break; }\n  /// contains a branch-after.\n  ///\n  /// Branch-afters each have their own destination out of the\n  /// cleanup, guaranteed distinct from anything else threaded through\n  /// it.  Therefore branch-afters usually force a switch after the\n  /// cleanup.\n  void addBranchAfter(llvm::ConstantInt *Index,\n                      llvm::BasicBlock *Block) {\n    struct ExtInfo &ExtInfo = getExtInfo();\n    if (ExtInfo.Branches.insert(Block).second)\n      ExtInfo.BranchAfters.push_back(std::make_pair(Block, Index));\n  }\n\n  /// Return the number of unique branch-afters on this scope.\n  unsigned getNumBranchAfters() const {\n    return ExtInfo ? ExtInfo->BranchAfters.size() : 0;\n  }\n\n  llvm::BasicBlock *getBranchAfterBlock(unsigned I) const {\n    assert(I < getNumBranchAfters());\n    return ExtInfo->BranchAfters[I].first;\n  }\n\n  llvm::ConstantInt *getBranchAfterIndex(unsigned I) const {\n    assert(I < getNumBranchAfters());\n    return ExtInfo->BranchAfters[I].second;\n  }\n\n  /// Add a branch-through to this cleanup scope.  A branch-through is\n  /// a branch from a scope protected by this (normal) cleanup to an\n  /// enclosing scope other than the immediately-enclosing normal\n  /// cleanup scope.\n  ///\n  /// In the following example, the branch through B's scope is a\n  /// branch-through, while the branch through A's scope is a\n  /// branch-after:\n  ///   for (;;) { A a; B b; break; }\n  ///\n  /// All branch-throughs have a common destination out of the\n  /// cleanup, one possibly shared with the fall-through.  Therefore\n  /// branch-throughs usually don't force a switch after the cleanup.\n  ///\n  /// \\return true if the branch-through was new to this scope\n  bool addBranchThrough(llvm::BasicBlock *Block) {\n    return getExtInfo().Branches.insert(Block).second;\n  }\n\n  /// Determines if this cleanup scope has any branch throughs.\n  bool hasBranchThroughs() const {\n    if (!ExtInfo) return false;\n    return (ExtInfo->BranchAfters.size() != ExtInfo->Branches.size());\n  }\n\n  static bool classof(const EHScope *Scope) {\n    return (Scope->getKind() == Cleanup);\n  }\n};\n// NOTE: there's a bunch of different data classes tacked on after an\n// EHCleanupScope. It is asserted (in EHScopeStack::pushCleanup*) that\n// they don't require greater alignment than ScopeStackAlignment. So,\n// EHCleanupScope ought to have alignment equal to that -- not more\n// (would be misaligned by the stack allocator), and not less (would\n// break the appended classes).\nstatic_assert(alignof(EHCleanupScope) == EHScopeStack::ScopeStackAlignment,\n              \"EHCleanupScope expected alignment\");\n\n/// An exceptions scope which filters exceptions thrown through it.\n/// Only exceptions matching the filter types will be permitted to be\n/// thrown.\n///\n/// This is used to implement C++ exception specifications.\nclass EHFilterScope : public EHScope {\n  // Essentially ends in a flexible array member:\n  // llvm::Value *FilterTypes[0];\n\n  llvm::Value **getFilters() {\n    return reinterpret_cast<llvm::Value**>(this+1);\n  }\n\n  llvm::Value * const *getFilters() const {\n    return reinterpret_cast<llvm::Value* const *>(this+1);\n  }\n\npublic:\n  EHFilterScope(unsigned numFilters)\n    : EHScope(Filter, EHScopeStack::stable_end()) {\n    FilterBits.NumFilters = numFilters;\n    assert(FilterBits.NumFilters == numFilters && \"NumFilters overflow\");\n  }\n\n  static size_t getSizeForNumFilters(unsigned numFilters) {\n    return sizeof(EHFilterScope) + numFilters * sizeof(llvm::Value*);\n  }\n\n  unsigned getNumFilters() const { return FilterBits.NumFilters; }\n\n  void setFilter(unsigned i, llvm::Value *filterValue) {\n    assert(i < getNumFilters());\n    getFilters()[i] = filterValue;\n  }\n\n  llvm::Value *getFilter(unsigned i) const {\n    assert(i < getNumFilters());\n    return getFilters()[i];\n  }\n\n  static bool classof(const EHScope *scope) {\n    return scope->getKind() == Filter;\n  }\n};\n\n/// An exceptions scope which calls std::terminate if any exception\n/// reaches it.\nclass EHTerminateScope : public EHScope {\npublic:\n  EHTerminateScope(EHScopeStack::stable_iterator enclosingEHScope)\n    : EHScope(Terminate, enclosingEHScope) {}\n  static size_t getSize() { return sizeof(EHTerminateScope); }\n\n  static bool classof(const EHScope *scope) {\n    return scope->getKind() == Terminate;\n  }\n};\n\n/// A non-stable pointer into the scope stack.\nclass EHScopeStack::iterator {\n  char *Ptr;\n\n  friend class EHScopeStack;\n  explicit iterator(char *Ptr) : Ptr(Ptr) {}\n\npublic:\n  iterator() : Ptr(nullptr) {}\n\n  EHScope *get() const {\n    return reinterpret_cast<EHScope*>(Ptr);\n  }\n\n  EHScope *operator->() const { return get(); }\n  EHScope &operator*() const { return *get(); }\n\n  iterator &operator++() {\n    size_t Size;\n    switch (get()->getKind()) {\n    case EHScope::Catch:\n      Size = EHCatchScope::getSizeForNumHandlers(\n          static_cast<const EHCatchScope *>(get())->getNumHandlers());\n      break;\n\n    case EHScope::Filter:\n      Size = EHFilterScope::getSizeForNumFilters(\n          static_cast<const EHFilterScope *>(get())->getNumFilters());\n      break;\n\n    case EHScope::Cleanup:\n      Size = static_cast<const EHCleanupScope *>(get())->getAllocatedSize();\n      break;\n\n    case EHScope::Terminate:\n      Size = EHTerminateScope::getSize();\n      break;\n    }\n    Ptr += llvm::alignTo(Size, ScopeStackAlignment);\n    return *this;\n  }\n\n  iterator next() {\n    iterator copy = *this;\n    ++copy;\n    return copy;\n  }\n\n  iterator operator++(int) {\n    iterator copy = *this;\n    operator++();\n    return copy;\n  }\n\n  bool encloses(iterator other) const { return Ptr >= other.Ptr; }\n  bool strictlyEncloses(iterator other) const { return Ptr > other.Ptr; }\n\n  bool operator==(iterator other) const { return Ptr == other.Ptr; }\n  bool operator!=(iterator other) const { return Ptr != other.Ptr; }\n};\n\ninline EHScopeStack::iterator EHScopeStack::begin() const {\n  return iterator(StartOfData);\n}\n\ninline EHScopeStack::iterator EHScopeStack::end() const {\n  return iterator(EndOfBuffer);\n}\n\ninline void EHScopeStack::popCatch() {\n  assert(!empty() && \"popping exception stack when not empty\");\n\n  EHCatchScope &scope = cast<EHCatchScope>(*begin());\n  InnermostEHScope = scope.getEnclosingEHScope();\n  deallocate(EHCatchScope::getSizeForNumHandlers(scope.getNumHandlers()));\n}\n\ninline void EHScopeStack::popTerminate() {\n  assert(!empty() && \"popping exception stack when not empty\");\n\n  EHTerminateScope &scope = cast<EHTerminateScope>(*begin());\n  InnermostEHScope = scope.getEnclosingEHScope();\n  deallocate(EHTerminateScope::getSize());\n}\n\ninline EHScopeStack::iterator EHScopeStack::find(stable_iterator sp) const {\n  assert(sp.isValid() && \"finding invalid savepoint\");\n  assert(sp.Size <= stable_begin().Size && \"finding savepoint after pop\");\n  return iterator(EndOfBuffer - sp.Size);\n}\n\ninline EHScopeStack::stable_iterator\nEHScopeStack::stabilize(iterator ir) const {\n  assert(StartOfData <= ir.Ptr && ir.Ptr <= EndOfBuffer);\n  return stable_iterator(EndOfBuffer - ir.Ptr);\n}\n\n/// The exceptions personality for a function.\nstruct EHPersonality {\n  const char *PersonalityFn;\n\n  // If this is non-null, this personality requires a non-standard\n  // function for rethrowing an exception after a catchall cleanup.\n  // This function must have prototype void(void*).\n  const char *CatchallRethrowFn;\n\n  static const EHPersonality &get(CodeGenModule &CGM, const FunctionDecl *FD);\n  static const EHPersonality &get(CodeGenFunction &CGF);\n\n  static const EHPersonality GNU_C;\n  static const EHPersonality GNU_C_SJLJ;\n  static const EHPersonality GNU_C_SEH;\n  static const EHPersonality GNU_ObjC;\n  static const EHPersonality GNU_ObjC_SJLJ;\n  static const EHPersonality GNU_ObjC_SEH;\n  static const EHPersonality GNUstep_ObjC;\n  static const EHPersonality GNU_ObjCXX;\n  static const EHPersonality NeXT_ObjC;\n  static const EHPersonality GNU_CPlusPlus;\n  static const EHPersonality GNU_CPlusPlus_SJLJ;\n  static const EHPersonality GNU_CPlusPlus_SEH;\n  static const EHPersonality MSVC_except_handler;\n  static const EHPersonality MSVC_C_specific_handler;\n  static const EHPersonality MSVC_CxxFrameHandler3;\n  static const EHPersonality GNU_Wasm_CPlusPlus;\n  static const EHPersonality XL_CPlusPlus;\n\n  /// Does this personality use landingpads or the family of pad instructions\n  /// designed to form funclets?\n  bool usesFuncletPads() const {\n    return isMSVCPersonality() || isWasmPersonality();\n  }\n\n  bool isMSVCPersonality() const {\n    return this == &MSVC_except_handler || this == &MSVC_C_specific_handler ||\n           this == &MSVC_CxxFrameHandler3;\n  }\n\n  bool isWasmPersonality() const { return this == &GNU_Wasm_CPlusPlus; }\n\n  bool isMSVCXXPersonality() const { return this == &MSVC_CxxFrameHandler3; }\n};\n}\n}\n\n#endif\n"}, "37": {"id": 37, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "content": "//===--- CGExprScalar.cpp - Emit LLVM Code for Scalar Exprs ---------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This contains code to emit Expr nodes with scalar LLVM types as LLVM code.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"CGCXXABI.h\"\n#include \"CGCleanup.h\"\n#include \"CGDebugInfo.h\"\n#include \"CGObjCRuntime.h\"\n#include \"CGOpenMPRuntime.h\"\n#include \"CodeGenFunction.h\"\n#include \"CodeGenModule.h\"\n#include \"ConstantEmitter.h\"\n#include \"TargetInfo.h\"\n#include \"clang/AST/ASTContext.h\"\n#include \"clang/AST/Attr.h\"\n#include \"clang/AST/DeclObjC.h\"\n#include \"clang/AST/Expr.h\"\n#include \"clang/AST/RecordLayout.h\"\n#include \"clang/AST/StmtVisitor.h\"\n#include \"clang/Basic/CodeGenOptions.h\"\n#include \"clang/Basic/TargetInfo.h\"\n#include \"llvm/ADT/APFixedPoint.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/IR/CFG.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/FixedPointBuilder.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/GetElementPtrTypeIterator.h\"\n#include \"llvm/IR/GlobalVariable.h\"\n#include \"llvm/IR/Intrinsics.h\"\n#include \"llvm/IR/IntrinsicsPowerPC.h\"\n#include \"llvm/IR/MatrixBuilder.h\"\n#include \"llvm/IR/Module.h\"\n#include <cstdarg>\n\nusing namespace clang;\nusing namespace CodeGen;\nusing llvm::Value;\n\n//===----------------------------------------------------------------------===//\n//                         Scalar Expression Emitter\n//===----------------------------------------------------------------------===//\n\nnamespace {\n\n/// Determine whether the given binary operation may overflow.\n/// Sets \\p Result to the value of the operation for BO_Add, BO_Sub, BO_Mul,\n/// and signed BO_{Div,Rem}. For these opcodes, and for unsigned BO_{Div,Rem},\n/// the returned overflow check is precise. The returned value is 'true' for\n/// all other opcodes, to be conservative.\nbool mayHaveIntegerOverflow(llvm::ConstantInt *LHS, llvm::ConstantInt *RHS,\n                             BinaryOperator::Opcode Opcode, bool Signed,\n                             llvm::APInt &Result) {\n  // Assume overflow is possible, unless we can prove otherwise.\n  bool Overflow = true;\n  const auto &LHSAP = LHS->getValue();\n  const auto &RHSAP = RHS->getValue();\n  if (Opcode == BO_Add) {\n    if (Signed)\n      Result = LHSAP.sadd_ov(RHSAP, Overflow);\n    else\n      Result = LHSAP.uadd_ov(RHSAP, Overflow);\n  } else if (Opcode == BO_Sub) {\n    if (Signed)\n      Result = LHSAP.ssub_ov(RHSAP, Overflow);\n    else\n      Result = LHSAP.usub_ov(RHSAP, Overflow);\n  } else if (Opcode == BO_Mul) {\n    if (Signed)\n      Result = LHSAP.smul_ov(RHSAP, Overflow);\n    else\n      Result = LHSAP.umul_ov(RHSAP, Overflow);\n  } else if (Opcode == BO_Div || Opcode == BO_Rem) {\n    if (Signed && !RHS->isZero())\n      Result = LHSAP.sdiv_ov(RHSAP, Overflow);\n    else\n      return false;\n  }\n  return Overflow;\n}\n\nstruct BinOpInfo {\n  Value *LHS;\n  Value *RHS;\n  QualType Ty;  // Computation Type.\n  BinaryOperator::Opcode Opcode; // Opcode of BinOp to perform\n  FPOptions FPFeatures;\n  const Expr *E;      // Entire expr, for error unsupported.  May not be binop.\n\n  /// Check if the binop can result in integer overflow.\n  bool mayHaveIntegerOverflow() const {\n    // Without constant input, we can't rule out overflow.\n    auto *LHSCI = dyn_cast<llvm::ConstantInt>(LHS);\n    auto *RHSCI = dyn_cast<llvm::ConstantInt>(RHS);\n    if (!LHSCI || !RHSCI)\n      return true;\n\n    llvm::APInt Result;\n    return ::mayHaveIntegerOverflow(\n        LHSCI, RHSCI, Opcode, Ty->hasSignedIntegerRepresentation(), Result);\n  }\n\n  /// Check if the binop computes a division or a remainder.\n  bool isDivremOp() const {\n    return Opcode == BO_Div || Opcode == BO_Rem || Opcode == BO_DivAssign ||\n           Opcode == BO_RemAssign;\n  }\n\n  /// Check if the binop can result in an integer division by zero.\n  bool mayHaveIntegerDivisionByZero() const {\n    if (isDivremOp())\n      if (auto *CI = dyn_cast<llvm::ConstantInt>(RHS))\n        return CI->isZero();\n    return true;\n  }\n\n  /// Check if the binop can result in a float division by zero.\n  bool mayHaveFloatDivisionByZero() const {\n    if (isDivremOp())\n      if (auto *CFP = dyn_cast<llvm::ConstantFP>(RHS))\n        return CFP->isZero();\n    return true;\n  }\n\n  /// Check if at least one operand is a fixed point type. In such cases, this\n  /// operation did not follow usual arithmetic conversion and both operands\n  /// might not be of the same type.\n  bool isFixedPointOp() const {\n    // We cannot simply check the result type since comparison operations return\n    // an int.\n    if (const auto *BinOp = dyn_cast<BinaryOperator>(E)) {\n      QualType LHSType = BinOp->getLHS()->getType();\n      QualType RHSType = BinOp->getRHS()->getType();\n      return LHSType->isFixedPointType() || RHSType->isFixedPointType();\n    }\n    if (const auto *UnOp = dyn_cast<UnaryOperator>(E))\n      return UnOp->getSubExpr()->getType()->isFixedPointType();\n    return false;\n  }\n};\n\nstatic bool MustVisitNullValue(const Expr *E) {\n  // If a null pointer expression's type is the C++0x nullptr_t, then\n  // it's not necessarily a simple constant and it must be evaluated\n  // for its potential side effects.\n  return E->getType()->isNullPtrType();\n}\n\n/// If \\p E is a widened promoted integer, get its base (unpromoted) type.\nstatic llvm::Optional<QualType> getUnwidenedIntegerType(const ASTContext &Ctx,\n                                                        const Expr *E) {\n  const Expr *Base = E->IgnoreImpCasts();\n  if (E == Base)\n    return llvm::None;\n\n  QualType BaseTy = Base->getType();\n  if (!BaseTy->isPromotableIntegerType() ||\n      Ctx.getTypeSize(BaseTy) >= Ctx.getTypeSize(E->getType()))\n    return llvm::None;\n\n  return BaseTy;\n}\n\n/// Check if \\p E is a widened promoted integer.\nstatic bool IsWidenedIntegerOp(const ASTContext &Ctx, const Expr *E) {\n  return getUnwidenedIntegerType(Ctx, E).hasValue();\n}\n\n/// Check if we can skip the overflow check for \\p Op.\nstatic bool CanElideOverflowCheck(const ASTContext &Ctx, const BinOpInfo &Op) {\n  assert((isa<UnaryOperator>(Op.E) || isa<BinaryOperator>(Op.E)) &&\n         \"Expected a unary or binary operator\");\n\n  // If the binop has constant inputs and we can prove there is no overflow,\n  // we can elide the overflow check.\n  if (!Op.mayHaveIntegerOverflow())\n    return true;\n\n  // If a unary op has a widened operand, the op cannot overflow.\n  if (const auto *UO = dyn_cast<UnaryOperator>(Op.E))\n    return !UO->canOverflow();\n\n  // We usually don't need overflow checks for binops with widened operands.\n  // Multiplication with promoted unsigned operands is a special case.\n  const auto *BO = cast<BinaryOperator>(Op.E);\n  auto OptionalLHSTy = getUnwidenedIntegerType(Ctx, BO->getLHS());\n  if (!OptionalLHSTy)\n    return false;\n\n  auto OptionalRHSTy = getUnwidenedIntegerType(Ctx, BO->getRHS());\n  if (!OptionalRHSTy)\n    return false;\n\n  QualType LHSTy = *OptionalLHSTy;\n  QualType RHSTy = *OptionalRHSTy;\n\n  // This is the simple case: binops without unsigned multiplication, and with\n  // widened operands. No overflow check is needed here.\n  if ((Op.Opcode != BO_Mul && Op.Opcode != BO_MulAssign) ||\n      !LHSTy->isUnsignedIntegerType() || !RHSTy->isUnsignedIntegerType())\n    return true;\n\n  // For unsigned multiplication the overflow check can be elided if either one\n  // of the unpromoted types are less than half the size of the promoted type.\n  unsigned PromotedSize = Ctx.getTypeSize(Op.E->getType());\n  return (2 * Ctx.getTypeSize(LHSTy)) < PromotedSize ||\n         (2 * Ctx.getTypeSize(RHSTy)) < PromotedSize;\n}\n\nclass ScalarExprEmitter\n  : public StmtVisitor<ScalarExprEmitter, Value*> {\n  CodeGenFunction &CGF;\n  CGBuilderTy &Builder;\n  bool IgnoreResultAssign;\n  llvm::LLVMContext &VMContext;\npublic:\n\n  ScalarExprEmitter(CodeGenFunction &cgf, bool ira=false)\n    : CGF(cgf), Builder(CGF.Builder), IgnoreResultAssign(ira),\n      VMContext(cgf.getLLVMContext()) {\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                               Utilities\n  //===--------------------------------------------------------------------===//\n\n  bool TestAndClearIgnoreResultAssign() {\n    bool I = IgnoreResultAssign;\n    IgnoreResultAssign = false;\n    return I;\n  }\n\n  llvm::Type *ConvertType(QualType T) { return CGF.ConvertType(T); }\n  LValue EmitLValue(const Expr *E) { return CGF.EmitLValue(E); }\n  LValue EmitCheckedLValue(const Expr *E, CodeGenFunction::TypeCheckKind TCK) {\n    return CGF.EmitCheckedLValue(E, TCK);\n  }\n\n  void EmitBinOpCheck(ArrayRef<std::pair<Value *, SanitizerMask>> Checks,\n                      const BinOpInfo &Info);\n\n  Value *EmitLoadOfLValue(LValue LV, SourceLocation Loc) {\n    return CGF.EmitLoadOfLValue(LV, Loc).getScalarVal();\n  }\n\n  void EmitLValueAlignmentAssumption(const Expr *E, Value *V) {\n    const AlignValueAttr *AVAttr = nullptr;\n    if (const auto *DRE = dyn_cast<DeclRefExpr>(E)) {\n      const ValueDecl *VD = DRE->getDecl();\n\n      if (VD->getType()->isReferenceType()) {\n        if (const auto *TTy =\n            dyn_cast<TypedefType>(VD->getType().getNonReferenceType()))\n          AVAttr = TTy->getDecl()->getAttr<AlignValueAttr>();\n      } else {\n        // Assumptions for function parameters are emitted at the start of the\n        // function, so there is no need to repeat that here,\n        // unless the alignment-assumption sanitizer is enabled,\n        // then we prefer the assumption over alignment attribute\n        // on IR function param.\n        if (isa<ParmVarDecl>(VD) && !CGF.SanOpts.has(SanitizerKind::Alignment))\n          return;\n\n        AVAttr = VD->getAttr<AlignValueAttr>();\n      }\n    }\n\n    if (!AVAttr)\n      if (const auto *TTy =\n          dyn_cast<TypedefType>(E->getType()))\n        AVAttr = TTy->getDecl()->getAttr<AlignValueAttr>();\n\n    if (!AVAttr)\n      return;\n\n    Value *AlignmentValue = CGF.EmitScalarExpr(AVAttr->getAlignment());\n    llvm::ConstantInt *AlignmentCI = cast<llvm::ConstantInt>(AlignmentValue);\n    CGF.emitAlignmentAssumption(V, E, AVAttr->getLocation(), AlignmentCI);\n  }\n\n  /// EmitLoadOfLValue - Given an expression with complex type that represents a\n  /// value l-value, this method emits the address of the l-value, then loads\n  /// and returns the result.\n  Value *EmitLoadOfLValue(const Expr *E) {\n    Value *V = EmitLoadOfLValue(EmitCheckedLValue(E, CodeGenFunction::TCK_Load),\n                                E->getExprLoc());\n\n    EmitLValueAlignmentAssumption(E, V);\n    return V;\n  }\n\n  /// EmitConversionToBool - Convert the specified expression value to a\n  /// boolean (i1) truth value.  This is equivalent to \"Val != 0\".\n  Value *EmitConversionToBool(Value *Src, QualType DstTy);\n\n  /// Emit a check that a conversion from a floating-point type does not\n  /// overflow.\n  void EmitFloatConversionCheck(Value *OrigSrc, QualType OrigSrcType,\n                                Value *Src, QualType SrcType, QualType DstType,\n                                llvm::Type *DstTy, SourceLocation Loc);\n\n  /// Known implicit conversion check kinds.\n  /// Keep in sync with the enum of the same name in ubsan_handlers.h\n  enum ImplicitConversionCheckKind : unsigned char {\n    ICCK_IntegerTruncation = 0, // Legacy, was only used by clang 7.\n    ICCK_UnsignedIntegerTruncation = 1,\n    ICCK_SignedIntegerTruncation = 2,\n    ICCK_IntegerSignChange = 3,\n    ICCK_SignedIntegerTruncationOrSignChange = 4,\n  };\n\n  /// Emit a check that an [implicit] truncation of an integer  does not\n  /// discard any bits. It is not UB, so we use the value after truncation.\n  void EmitIntegerTruncationCheck(Value *Src, QualType SrcType, Value *Dst,\n                                  QualType DstType, SourceLocation Loc);\n\n  /// Emit a check that an [implicit] conversion of an integer does not change\n  /// the sign of the value. It is not UB, so we use the value after conversion.\n  /// NOTE: Src and Dst may be the exact same value! (point to the same thing)\n  void EmitIntegerSignChangeCheck(Value *Src, QualType SrcType, Value *Dst,\n                                  QualType DstType, SourceLocation Loc);\n\n  /// Emit a conversion from the specified type to the specified destination\n  /// type, both of which are LLVM scalar types.\n  struct ScalarConversionOpts {\n    bool TreatBooleanAsSigned;\n    bool EmitImplicitIntegerTruncationChecks;\n    bool EmitImplicitIntegerSignChangeChecks;\n\n    ScalarConversionOpts()\n        : TreatBooleanAsSigned(false),\n          EmitImplicitIntegerTruncationChecks(false),\n          EmitImplicitIntegerSignChangeChecks(false) {}\n\n    ScalarConversionOpts(clang::SanitizerSet SanOpts)\n        : TreatBooleanAsSigned(false),\n          EmitImplicitIntegerTruncationChecks(\n              SanOpts.hasOneOf(SanitizerKind::ImplicitIntegerTruncation)),\n          EmitImplicitIntegerSignChangeChecks(\n              SanOpts.has(SanitizerKind::ImplicitIntegerSignChange)) {}\n  };\n  Value *\n  EmitScalarConversion(Value *Src, QualType SrcTy, QualType DstTy,\n                       SourceLocation Loc,\n                       ScalarConversionOpts Opts = ScalarConversionOpts());\n\n  /// Convert between either a fixed point and other fixed point or fixed point\n  /// and an integer.\n  Value *EmitFixedPointConversion(Value *Src, QualType SrcTy, QualType DstTy,\n                                  SourceLocation Loc);\n\n  /// Emit a conversion from the specified complex type to the specified\n  /// destination type, where the destination type is an LLVM scalar type.\n  Value *EmitComplexToScalarConversion(CodeGenFunction::ComplexPairTy Src,\n                                       QualType SrcTy, QualType DstTy,\n                                       SourceLocation Loc);\n\n  /// EmitNullValue - Emit a value that corresponds to null for the given type.\n  Value *EmitNullValue(QualType Ty);\n\n  /// EmitFloatToBoolConversion - Perform an FP to boolean conversion.\n  Value *EmitFloatToBoolConversion(Value *V) {\n    // Compare against 0.0 for fp scalars.\n    llvm::Value *Zero = llvm::Constant::getNullValue(V->getType());\n    return Builder.CreateFCmpUNE(V, Zero, \"tobool\");\n  }\n\n  /// EmitPointerToBoolConversion - Perform a pointer to boolean conversion.\n  Value *EmitPointerToBoolConversion(Value *V, QualType QT) {\n    Value *Zero = CGF.CGM.getNullPointer(cast<llvm::PointerType>(V->getType()), QT);\n\n    return Builder.CreateICmpNE(V, Zero, \"tobool\");\n  }\n\n  Value *EmitIntToBoolConversion(Value *V) {\n    // Because of the type rules of C, we often end up computing a\n    // logical value, then zero extending it to int, then wanting it\n    // as a logical value again.  Optimize this common case.\n    if (llvm::ZExtInst *ZI = dyn_cast<llvm::ZExtInst>(V)) {\n      if (ZI->getOperand(0)->getType() == Builder.getInt1Ty()) {\n        Value *Result = ZI->getOperand(0);\n        // If there aren't any more uses, zap the instruction to save space.\n        // Note that there can be more uses, for example if this\n        // is the result of an assignment.\n        if (ZI->use_empty())\n          ZI->eraseFromParent();\n        return Result;\n      }\n    }\n\n    return Builder.CreateIsNotNull(V, \"tobool\");\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                            Visitor Methods\n  //===--------------------------------------------------------------------===//\n\n  Value *Visit(Expr *E) {\n    ApplyDebugLocation DL(CGF, E);\n    return StmtVisitor<ScalarExprEmitter, Value*>::Visit(E);\n  }\n\n  Value *VisitStmt(Stmt *S) {\n    S->dump(llvm::errs(), CGF.getContext());\n    llvm_unreachable(\"Stmt can't have complex result type!\");\n  }\n  Value *VisitExpr(Expr *S);\n\n  Value *VisitConstantExpr(ConstantExpr *E) {\n    if (Value *Result = ConstantEmitter(CGF).tryEmitConstantExpr(E)) {\n      if (E->isGLValue())\n        return CGF.Builder.CreateLoad(Address(\n            Result, CGF.getContext().getTypeAlignInChars(E->getType())));\n      return Result;\n    }\n    return Visit(E->getSubExpr());\n  }\n  Value *VisitParenExpr(ParenExpr *PE) {\n    return Visit(PE->getSubExpr());\n  }\n  Value *VisitSubstNonTypeTemplateParmExpr(SubstNonTypeTemplateParmExpr *E) {\n    return Visit(E->getReplacement());\n  }\n  Value *VisitGenericSelectionExpr(GenericSelectionExpr *GE) {\n    return Visit(GE->getResultExpr());\n  }\n  Value *VisitCoawaitExpr(CoawaitExpr *S) {\n    return CGF.EmitCoawaitExpr(*S).getScalarVal();\n  }\n  Value *VisitCoyieldExpr(CoyieldExpr *S) {\n    return CGF.EmitCoyieldExpr(*S).getScalarVal();\n  }\n  Value *VisitUnaryCoawait(const UnaryOperator *E) {\n    return Visit(E->getSubExpr());\n  }\n\n  // Leaves.\n  Value *VisitIntegerLiteral(const IntegerLiteral *E) {\n    return Builder.getInt(E->getValue());\n  }\n  Value *VisitFixedPointLiteral(const FixedPointLiteral *E) {\n    return Builder.getInt(E->getValue());\n  }\n  Value *VisitFloatingLiteral(const FloatingLiteral *E) {\n    return llvm::ConstantFP::get(VMContext, E->getValue());\n  }\n  Value *VisitCharacterLiteral(const CharacterLiteral *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n  Value *VisitObjCBoolLiteralExpr(const ObjCBoolLiteralExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n  Value *VisitCXXBoolLiteralExpr(const CXXBoolLiteralExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n  Value *VisitCXXScalarValueInitExpr(const CXXScalarValueInitExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n  Value *VisitGNUNullExpr(const GNUNullExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n  Value *VisitOffsetOfExpr(OffsetOfExpr *E);\n  Value *VisitUnaryExprOrTypeTraitExpr(const UnaryExprOrTypeTraitExpr *E);\n  Value *VisitAddrLabelExpr(const AddrLabelExpr *E) {\n    llvm::Value *V = CGF.GetAddrOfLabel(E->getLabel());\n    return Builder.CreateBitCast(V, ConvertType(E->getType()));\n  }\n\n  Value *VisitSizeOfPackExpr(SizeOfPackExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()),E->getPackLength());\n  }\n\n  Value *VisitPseudoObjectExpr(PseudoObjectExpr *E) {\n    return CGF.EmitPseudoObjectRValue(E).getScalarVal();\n  }\n\n  Value *VisitOpaqueValueExpr(OpaqueValueExpr *E) {\n    if (E->isGLValue())\n      return EmitLoadOfLValue(CGF.getOrCreateOpaqueLValueMapping(E),\n                              E->getExprLoc());\n\n    // Otherwise, assume the mapping is the scalar directly.\n    return CGF.getOrCreateOpaqueRValueMapping(E).getScalarVal();\n  }\n\n  // l-values.\n  Value *VisitDeclRefExpr(DeclRefExpr *E) {\n    if (CodeGenFunction::ConstantEmission Constant = CGF.tryEmitAsConstant(E))\n      return CGF.emitScalarConstant(Constant, E);\n    return EmitLoadOfLValue(E);\n  }\n\n  Value *VisitObjCSelectorExpr(ObjCSelectorExpr *E) {\n    return CGF.EmitObjCSelectorExpr(E);\n  }\n  Value *VisitObjCProtocolExpr(ObjCProtocolExpr *E) {\n    return CGF.EmitObjCProtocolExpr(E);\n  }\n  Value *VisitObjCIvarRefExpr(ObjCIvarRefExpr *E) {\n    return EmitLoadOfLValue(E);\n  }\n  Value *VisitObjCMessageExpr(ObjCMessageExpr *E) {\n    if (E->getMethodDecl() &&\n        E->getMethodDecl()->getReturnType()->isReferenceType())\n      return EmitLoadOfLValue(E);\n    return CGF.EmitObjCMessageExpr(E).getScalarVal();\n  }\n\n  Value *VisitObjCIsaExpr(ObjCIsaExpr *E) {\n    LValue LV = CGF.EmitObjCIsaExpr(E);\n    Value *V = CGF.EmitLoadOfLValue(LV, E->getExprLoc()).getScalarVal();\n    return V;\n  }\n\n  Value *VisitObjCAvailabilityCheckExpr(ObjCAvailabilityCheckExpr *E) {\n    VersionTuple Version = E->getVersion();\n\n    // If we're checking for a platform older than our minimum deployment\n    // target, we can fold the check away.\n    if (Version <= CGF.CGM.getTarget().getPlatformMinVersion())\n      return llvm::ConstantInt::get(Builder.getInt1Ty(), 1);\n\n    return CGF.EmitBuiltinAvailable(Version);\n  }\n\n  Value *VisitArraySubscriptExpr(ArraySubscriptExpr *E);\n  Value *VisitMatrixSubscriptExpr(MatrixSubscriptExpr *E);\n  Value *VisitShuffleVectorExpr(ShuffleVectorExpr *E);\n  Value *VisitConvertVectorExpr(ConvertVectorExpr *E);\n  Value *VisitMemberExpr(MemberExpr *E);\n  Value *VisitExtVectorElementExpr(Expr *E) { return EmitLoadOfLValue(E); }\n  Value *VisitCompoundLiteralExpr(CompoundLiteralExpr *E) {\n    // Strictly speaking, we shouldn't be calling EmitLoadOfLValue, which\n    // transitively calls EmitCompoundLiteralLValue, here in C++ since compound\n    // literals aren't l-values in C++. We do so simply because that's the\n    // cleanest way to handle compound literals in C++.\n    // See the discussion here: https://reviews.llvm.org/D64464\n    return EmitLoadOfLValue(E);\n  }\n\n  Value *VisitInitListExpr(InitListExpr *E);\n\n  Value *VisitArrayInitIndexExpr(ArrayInitIndexExpr *E) {\n    assert(CGF.getArrayInitIndex() &&\n           \"ArrayInitIndexExpr not inside an ArrayInitLoopExpr?\");\n    return CGF.getArrayInitIndex();\n  }\n\n  Value *VisitImplicitValueInitExpr(const ImplicitValueInitExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n  Value *VisitExplicitCastExpr(ExplicitCastExpr *E) {\n    CGF.CGM.EmitExplicitCastExprType(E, &CGF);\n    return VisitCastExpr(E);\n  }\n  Value *VisitCastExpr(CastExpr *E);\n\n  Value *VisitCallExpr(const CallExpr *E) {\n    if (E->getCallReturnType(CGF.getContext())->isReferenceType())\n      return EmitLoadOfLValue(E);\n\n    Value *V = CGF.EmitCallExpr(E).getScalarVal();\n\n    EmitLValueAlignmentAssumption(E, V);\n    return V;\n  }\n\n  Value *VisitStmtExpr(const StmtExpr *E);\n\n  // Unary Operators.\n  Value *VisitUnaryPostDec(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, false, false);\n  }\n  Value *VisitUnaryPostInc(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, true, false);\n  }\n  Value *VisitUnaryPreDec(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, false, true);\n  }\n  Value *VisitUnaryPreInc(const UnaryOperator *E) {\n    LValue LV = EmitLValue(E->getSubExpr());\n    return EmitScalarPrePostIncDec(E, LV, true, true);\n  }\n\n  llvm::Value *EmitIncDecConsiderOverflowBehavior(const UnaryOperator *E,\n                                                  llvm::Value *InVal,\n                                                  bool IsInc);\n\n  llvm::Value *EmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                       bool isInc, bool isPre);\n\n\n  Value *VisitUnaryAddrOf(const UnaryOperator *E) {\n    if (isa<MemberPointerType>(E->getType())) // never sugared\n      return CGF.CGM.getMemberPointerConstant(E);\n\n    return EmitLValue(E->getSubExpr()).getPointer(CGF);\n  }\n  Value *VisitUnaryDeref(const UnaryOperator *E) {\n    if (E->getType()->isVoidType())\n      return Visit(E->getSubExpr()); // the actual value should be unused\n    return EmitLoadOfLValue(E);\n  }\n  Value *VisitUnaryPlus(const UnaryOperator *E) {\n    // This differs from gcc, though, most likely due to a bug in gcc.\n    TestAndClearIgnoreResultAssign();\n    return Visit(E->getSubExpr());\n  }\n  Value *VisitUnaryMinus    (const UnaryOperator *E);\n  Value *VisitUnaryNot      (const UnaryOperator *E);\n  Value *VisitUnaryLNot     (const UnaryOperator *E);\n  Value *VisitUnaryReal     (const UnaryOperator *E);\n  Value *VisitUnaryImag     (const UnaryOperator *E);\n  Value *VisitUnaryExtension(const UnaryOperator *E) {\n    return Visit(E->getSubExpr());\n  }\n\n  // C++\n  Value *VisitMaterializeTemporaryExpr(const MaterializeTemporaryExpr *E) {\n    return EmitLoadOfLValue(E);\n  }\n  Value *VisitSourceLocExpr(SourceLocExpr *SLE) {\n    auto &Ctx = CGF.getContext();\n    APValue Evaluated =\n        SLE->EvaluateInContext(Ctx, CGF.CurSourceLocExprScope.getDefaultExpr());\n    return ConstantEmitter(CGF).emitAbstract(SLE->getLocation(), Evaluated,\n                                             SLE->getType());\n  }\n\n  Value *VisitCXXDefaultArgExpr(CXXDefaultArgExpr *DAE) {\n    CodeGenFunction::CXXDefaultArgExprScope Scope(CGF, DAE);\n    return Visit(DAE->getExpr());\n  }\n  Value *VisitCXXDefaultInitExpr(CXXDefaultInitExpr *DIE) {\n    CodeGenFunction::CXXDefaultInitExprScope Scope(CGF, DIE);\n    return Visit(DIE->getExpr());\n  }\n  Value *VisitCXXThisExpr(CXXThisExpr *TE) {\n    return CGF.LoadCXXThis();\n  }\n\n  Value *VisitExprWithCleanups(ExprWithCleanups *E);\n  Value *VisitCXXNewExpr(const CXXNewExpr *E) {\n    return CGF.EmitCXXNewExpr(E);\n  }\n  Value *VisitCXXDeleteExpr(const CXXDeleteExpr *E) {\n    CGF.EmitCXXDeleteExpr(E);\n    return nullptr;\n  }\n\n  Value *VisitTypeTraitExpr(const TypeTraitExpr *E) {\n    return llvm::ConstantInt::get(ConvertType(E->getType()), E->getValue());\n  }\n\n  Value *VisitConceptSpecializationExpr(const ConceptSpecializationExpr *E) {\n    return Builder.getInt1(E->isSatisfied());\n  }\n\n  Value *VisitRequiresExpr(const RequiresExpr *E) {\n    return Builder.getInt1(E->isSatisfied());\n  }\n\n  Value *VisitArrayTypeTraitExpr(const ArrayTypeTraitExpr *E) {\n    return llvm::ConstantInt::get(Builder.getInt32Ty(), E->getValue());\n  }\n\n  Value *VisitExpressionTraitExpr(const ExpressionTraitExpr *E) {\n    return llvm::ConstantInt::get(Builder.getInt1Ty(), E->getValue());\n  }\n\n  Value *VisitCXXPseudoDestructorExpr(const CXXPseudoDestructorExpr *E) {\n    // C++ [expr.pseudo]p1:\n    //   The result shall only be used as the operand for the function call\n    //   operator (), and the result of such a call has type void. The only\n    //   effect is the evaluation of the postfix-expression before the dot or\n    //   arrow.\n    CGF.EmitScalarExpr(E->getBase());\n    return nullptr;\n  }\n\n  Value *VisitCXXNullPtrLiteralExpr(const CXXNullPtrLiteralExpr *E) {\n    return EmitNullValue(E->getType());\n  }\n\n  Value *VisitCXXThrowExpr(const CXXThrowExpr *E) {\n    CGF.EmitCXXThrowExpr(E);\n    return nullptr;\n  }\n\n  Value *VisitCXXNoexceptExpr(const CXXNoexceptExpr *E) {\n    return Builder.getInt1(E->getValue());\n  }\n\n  // Binary Operators.\n  Value *EmitMul(const BinOpInfo &Ops) {\n    if (Ops.Ty->isSignedIntegerOrEnumerationType()) {\n      switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n      case LangOptions::SOB_Defined:\n        return Builder.CreateMul(Ops.LHS, Ops.RHS, \"mul\");\n      case LangOptions::SOB_Undefined:\n        if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n          return Builder.CreateNSWMul(Ops.LHS, Ops.RHS, \"mul\");\n        LLVM_FALLTHROUGH;\n      case LangOptions::SOB_Trapping:\n        if (CanElideOverflowCheck(CGF.getContext(), Ops))\n          return Builder.CreateNSWMul(Ops.LHS, Ops.RHS, \"mul\");\n        return EmitOverflowCheckedBinOp(Ops);\n      }\n    }\n\n    if (Ops.Ty->isConstantMatrixType()) {\n      llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n      // We need to check the types of the operands of the operator to get the\n      // correct matrix dimensions.\n      auto *BO = cast<BinaryOperator>(Ops.E);\n      auto *LHSMatTy = dyn_cast<ConstantMatrixType>(\n          BO->getLHS()->getType().getCanonicalType());\n      auto *RHSMatTy = dyn_cast<ConstantMatrixType>(\n          BO->getRHS()->getType().getCanonicalType());\n      if (LHSMatTy && RHSMatTy)\n        return MB.CreateMatrixMultiply(Ops.LHS, Ops.RHS, LHSMatTy->getNumRows(),\n                                       LHSMatTy->getNumColumns(),\n                                       RHSMatTy->getNumColumns());\n      return MB.CreateScalarMultiply(Ops.LHS, Ops.RHS);\n    }\n\n    if (Ops.Ty->isUnsignedIntegerType() &&\n        CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow) &&\n        !CanElideOverflowCheck(CGF.getContext(), Ops))\n      return EmitOverflowCheckedBinOp(Ops);\n\n    if (Ops.LHS->getType()->isFPOrFPVectorTy()) {\n      //  Preserve the old values\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, Ops.FPFeatures);\n      return Builder.CreateFMul(Ops.LHS, Ops.RHS, \"mul\");\n    }\n    if (Ops.isFixedPointOp())\n      return EmitFixedPointBinOp(Ops);\n    return Builder.CreateMul(Ops.LHS, Ops.RHS, \"mul\");\n  }\n  /// Create a binary op that checks for overflow.\n  /// Currently only supports +, - and *.\n  Value *EmitOverflowCheckedBinOp(const BinOpInfo &Ops);\n\n  // Check for undefined division and modulus behaviors.\n  void EmitUndefinedBehaviorIntegerDivAndRemCheck(const BinOpInfo &Ops,\n                                                  llvm::Value *Zero,bool isDiv);\n  // Common helper for getting how wide LHS of shift is.\n  static Value *GetWidthMinusOneValue(Value* LHS,Value* RHS);\n\n  // Used for shifting constraints for OpenCL, do mask for powers of 2, URem for\n  // non powers of two.\n  Value *ConstrainShiftValue(Value *LHS, Value *RHS, const Twine &Name);\n\n  Value *EmitDiv(const BinOpInfo &Ops);\n  Value *EmitRem(const BinOpInfo &Ops);\n  Value *EmitAdd(const BinOpInfo &Ops);\n  Value *EmitSub(const BinOpInfo &Ops);\n  Value *EmitShl(const BinOpInfo &Ops);\n  Value *EmitShr(const BinOpInfo &Ops);\n  Value *EmitAnd(const BinOpInfo &Ops) {\n    return Builder.CreateAnd(Ops.LHS, Ops.RHS, \"and\");\n  }\n  Value *EmitXor(const BinOpInfo &Ops) {\n    return Builder.CreateXor(Ops.LHS, Ops.RHS, \"xor\");\n  }\n  Value *EmitOr (const BinOpInfo &Ops) {\n    return Builder.CreateOr(Ops.LHS, Ops.RHS, \"or\");\n  }\n\n  // Helper functions for fixed point binary operations.\n  Value *EmitFixedPointBinOp(const BinOpInfo &Ops);\n\n  BinOpInfo EmitBinOps(const BinaryOperator *E);\n  LValue EmitCompoundAssignLValue(const CompoundAssignOperator *E,\n                            Value *(ScalarExprEmitter::*F)(const BinOpInfo &),\n                                  Value *&Result);\n\n  Value *EmitCompoundAssign(const CompoundAssignOperator *E,\n                            Value *(ScalarExprEmitter::*F)(const BinOpInfo &));\n\n  // Binary operators and binary compound assignment operators.\n#define HANDLEBINOP(OP) \\\n  Value *VisitBin ## OP(const BinaryOperator *E) {                         \\\n    return Emit ## OP(EmitBinOps(E));                                      \\\n  }                                                                        \\\n  Value *VisitBin ## OP ## Assign(const CompoundAssignOperator *E) {       \\\n    return EmitCompoundAssign(E, &ScalarExprEmitter::Emit ## OP);          \\\n  }\n  HANDLEBINOP(Mul)\n  HANDLEBINOP(Div)\n  HANDLEBINOP(Rem)\n  HANDLEBINOP(Add)\n  HANDLEBINOP(Sub)\n  HANDLEBINOP(Shl)\n  HANDLEBINOP(Shr)\n  HANDLEBINOP(And)\n  HANDLEBINOP(Xor)\n  HANDLEBINOP(Or)\n#undef HANDLEBINOP\n\n  // Comparisons.\n  Value *EmitCompare(const BinaryOperator *E, llvm::CmpInst::Predicate UICmpOpc,\n                     llvm::CmpInst::Predicate SICmpOpc,\n                     llvm::CmpInst::Predicate FCmpOpc, bool IsSignaling);\n#define VISITCOMP(CODE, UI, SI, FP, SIG) \\\n    Value *VisitBin##CODE(const BinaryOperator *E) { \\\n      return EmitCompare(E, llvm::ICmpInst::UI, llvm::ICmpInst::SI, \\\n                         llvm::FCmpInst::FP, SIG); }\n  VISITCOMP(LT, ICMP_ULT, ICMP_SLT, FCMP_OLT, true)\n  VISITCOMP(GT, ICMP_UGT, ICMP_SGT, FCMP_OGT, true)\n  VISITCOMP(LE, ICMP_ULE, ICMP_SLE, FCMP_OLE, true)\n  VISITCOMP(GE, ICMP_UGE, ICMP_SGE, FCMP_OGE, true)\n  VISITCOMP(EQ, ICMP_EQ , ICMP_EQ , FCMP_OEQ, false)\n  VISITCOMP(NE, ICMP_NE , ICMP_NE , FCMP_UNE, false)\n#undef VISITCOMP\n\n  Value *VisitBinAssign     (const BinaryOperator *E);\n\n  Value *VisitBinLAnd       (const BinaryOperator *E);\n  Value *VisitBinLOr        (const BinaryOperator *E);\n  Value *VisitBinComma      (const BinaryOperator *E);\n\n  Value *VisitBinPtrMemD(const Expr *E) { return EmitLoadOfLValue(E); }\n  Value *VisitBinPtrMemI(const Expr *E) { return EmitLoadOfLValue(E); }\n\n  Value *VisitCXXRewrittenBinaryOperator(CXXRewrittenBinaryOperator *E) {\n    return Visit(E->getSemanticForm());\n  }\n\n  // Other Operators.\n  Value *VisitBlockExpr(const BlockExpr *BE);\n  Value *VisitAbstractConditionalOperator(const AbstractConditionalOperator *);\n  Value *VisitChooseExpr(ChooseExpr *CE);\n  Value *VisitVAArgExpr(VAArgExpr *VE);\n  Value *VisitObjCStringLiteral(const ObjCStringLiteral *E) {\n    return CGF.EmitObjCStringLiteral(E);\n  }\n  Value *VisitObjCBoxedExpr(ObjCBoxedExpr *E) {\n    return CGF.EmitObjCBoxedExpr(E);\n  }\n  Value *VisitObjCArrayLiteral(ObjCArrayLiteral *E) {\n    return CGF.EmitObjCArrayLiteral(E);\n  }\n  Value *VisitObjCDictionaryLiteral(ObjCDictionaryLiteral *E) {\n    return CGF.EmitObjCDictionaryLiteral(E);\n  }\n  Value *VisitAsTypeExpr(AsTypeExpr *CE);\n  Value *VisitAtomicExpr(AtomicExpr *AE);\n};\n}  // end anonymous namespace.\n\n//===----------------------------------------------------------------------===//\n//                                Utilities\n//===----------------------------------------------------------------------===//\n\n/// EmitConversionToBool - Convert the specified expression value to a\n/// boolean (i1) truth value.  This is equivalent to \"Val != 0\".\nValue *ScalarExprEmitter::EmitConversionToBool(Value *Src, QualType SrcType) {\n  assert(SrcType.isCanonical() && \"EmitScalarConversion strips typedefs\");\n\n  if (SrcType->isRealFloatingType())\n    return EmitFloatToBoolConversion(Src);\n\n  if (const MemberPointerType *MPT = dyn_cast<MemberPointerType>(SrcType))\n    return CGF.CGM.getCXXABI().EmitMemberPointerIsNotNull(CGF, Src, MPT);\n\n  assert((SrcType->isIntegerType() || isa<llvm::PointerType>(Src->getType())) &&\n         \"Unknown scalar type to convert\");\n\n  if (isa<llvm::IntegerType>(Src->getType()))\n    return EmitIntToBoolConversion(Src);\n\n  assert(isa<llvm::PointerType>(Src->getType()));\n  return EmitPointerToBoolConversion(Src, SrcType);\n}\n\nvoid ScalarExprEmitter::EmitFloatConversionCheck(\n    Value *OrigSrc, QualType OrigSrcType, Value *Src, QualType SrcType,\n    QualType DstType, llvm::Type *DstTy, SourceLocation Loc) {\n  assert(SrcType->isFloatingType() && \"not a conversion from floating point\");\n  if (!isa<llvm::IntegerType>(DstTy))\n    return;\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n  using llvm::APFloat;\n  using llvm::APSInt;\n\n  llvm::Value *Check = nullptr;\n  const llvm::fltSemantics &SrcSema =\n    CGF.getContext().getFloatTypeSemantics(OrigSrcType);\n\n  // Floating-point to integer. This has undefined behavior if the source is\n  // +-Inf, NaN, or doesn't fit into the destination type (after truncation\n  // to an integer).\n  unsigned Width = CGF.getContext().getIntWidth(DstType);\n  bool Unsigned = DstType->isUnsignedIntegerOrEnumerationType();\n\n  APSInt Min = APSInt::getMinValue(Width, Unsigned);\n  APFloat MinSrc(SrcSema, APFloat::uninitialized);\n  if (MinSrc.convertFromAPInt(Min, !Unsigned, APFloat::rmTowardZero) &\n      APFloat::opOverflow)\n    // Don't need an overflow check for lower bound. Just check for\n    // -Inf/NaN.\n    MinSrc = APFloat::getInf(SrcSema, true);\n  else\n    // Find the largest value which is too small to represent (before\n    // truncation toward zero).\n    MinSrc.subtract(APFloat(SrcSema, 1), APFloat::rmTowardNegative);\n\n  APSInt Max = APSInt::getMaxValue(Width, Unsigned);\n  APFloat MaxSrc(SrcSema, APFloat::uninitialized);\n  if (MaxSrc.convertFromAPInt(Max, !Unsigned, APFloat::rmTowardZero) &\n      APFloat::opOverflow)\n    // Don't need an overflow check for upper bound. Just check for\n    // +Inf/NaN.\n    MaxSrc = APFloat::getInf(SrcSema, false);\n  else\n    // Find the smallest value which is too large to represent (before\n    // truncation toward zero).\n    MaxSrc.add(APFloat(SrcSema, 1), APFloat::rmTowardPositive);\n\n  // If we're converting from __half, convert the range to float to match\n  // the type of src.\n  if (OrigSrcType->isHalfType()) {\n    const llvm::fltSemantics &Sema =\n      CGF.getContext().getFloatTypeSemantics(SrcType);\n    bool IsInexact;\n    MinSrc.convert(Sema, APFloat::rmTowardZero, &IsInexact);\n    MaxSrc.convert(Sema, APFloat::rmTowardZero, &IsInexact);\n  }\n\n  llvm::Value *GE =\n    Builder.CreateFCmpOGT(Src, llvm::ConstantFP::get(VMContext, MinSrc));\n  llvm::Value *LE =\n    Builder.CreateFCmpOLT(Src, llvm::ConstantFP::get(VMContext, MaxSrc));\n  Check = Builder.CreateAnd(GE, LE);\n\n  llvm::Constant *StaticArgs[] = {CGF.EmitCheckSourceLocation(Loc),\n                                  CGF.EmitCheckTypeDescriptor(OrigSrcType),\n                                  CGF.EmitCheckTypeDescriptor(DstType)};\n  CGF.EmitCheck(std::make_pair(Check, SanitizerKind::FloatCastOverflow),\n                SanitizerHandler::FloatCastOverflow, StaticArgs, OrigSrc);\n}\n\n// Should be called within CodeGenFunction::SanitizerScope RAII scope.\n// Returns 'i1 false' when the truncation Src -> Dst was lossy.\nstatic std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n                 std::pair<llvm::Value *, SanitizerMask>>\nEmitIntegerTruncationCheckHelper(Value *Src, QualType SrcType, Value *Dst,\n                                 QualType DstType, CGBuilderTy &Builder) {\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = Dst->getType();\n  (void)DstTy; // Only used in assert()\n\n  // This should be truncation of integral types.\n  assert(Src != Dst);\n  assert(SrcTy->getScalarSizeInBits() > Dst->getType()->getScalarSizeInBits());\n  assert(isa<llvm::IntegerType>(SrcTy) && isa<llvm::IntegerType>(DstTy) &&\n         \"non-integer llvm type\");\n\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n\n  // If both (src and dst) types are unsigned, then it's an unsigned truncation.\n  // Else, it is a signed truncation.\n  ScalarExprEmitter::ImplicitConversionCheckKind Kind;\n  SanitizerMask Mask;\n  if (!SrcSigned && !DstSigned) {\n    Kind = ScalarExprEmitter::ICCK_UnsignedIntegerTruncation;\n    Mask = SanitizerKind::ImplicitUnsignedIntegerTruncation;\n  } else {\n    Kind = ScalarExprEmitter::ICCK_SignedIntegerTruncation;\n    Mask = SanitizerKind::ImplicitSignedIntegerTruncation;\n  }\n\n  llvm::Value *Check = nullptr;\n  // 1. Extend the truncated value back to the same width as the Src.\n  Check = Builder.CreateIntCast(Dst, SrcTy, DstSigned, \"anyext\");\n  // 2. Equality-compare with the original source value\n  Check = Builder.CreateICmpEQ(Check, Src, \"truncheck\");\n  // If the comparison result is 'i1 false', then the truncation was lossy.\n  return std::make_pair(Kind, std::make_pair(Check, Mask));\n}\n\nstatic bool PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(\n    QualType SrcType, QualType DstType) {\n  return SrcType->isIntegerType() && DstType->isIntegerType();\n}\n\nvoid ScalarExprEmitter::EmitIntegerTruncationCheck(Value *Src, QualType SrcType,\n                                                   Value *Dst, QualType DstType,\n                                                   SourceLocation Loc) {\n  if (!CGF.SanOpts.hasOneOf(SanitizerKind::ImplicitIntegerTruncation))\n    return;\n\n  // We only care about int->int conversions here.\n  // We ignore conversions to/from pointer and/or bool.\n  if (!PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(SrcType,\n                                                                       DstType))\n    return;\n\n  unsigned SrcBits = Src->getType()->getScalarSizeInBits();\n  unsigned DstBits = Dst->getType()->getScalarSizeInBits();\n  // This must be truncation. Else we do not care.\n  if (SrcBits <= DstBits)\n    return;\n\n  assert(!DstType->isBooleanType() && \"we should not get here with booleans.\");\n\n  // If the integer sign change sanitizer is enabled,\n  // and we are truncating from larger unsigned type to smaller signed type,\n  // let that next sanitizer deal with it.\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n  if (CGF.SanOpts.has(SanitizerKind::ImplicitIntegerSignChange) &&\n      (!SrcSigned && DstSigned))\n    return;\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n\n  std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n            std::pair<llvm::Value *, SanitizerMask>>\n      Check =\n          EmitIntegerTruncationCheckHelper(Src, SrcType, Dst, DstType, Builder);\n  // If the comparison result is 'i1 false', then the truncation was lossy.\n\n  // Do we care about this type of truncation?\n  if (!CGF.SanOpts.has(Check.second.second))\n    return;\n\n  llvm::Constant *StaticArgs[] = {\n      CGF.EmitCheckSourceLocation(Loc), CGF.EmitCheckTypeDescriptor(SrcType),\n      CGF.EmitCheckTypeDescriptor(DstType),\n      llvm::ConstantInt::get(Builder.getInt8Ty(), Check.first)};\n  CGF.EmitCheck(Check.second, SanitizerHandler::ImplicitConversion, StaticArgs,\n                {Src, Dst});\n}\n\n// Should be called within CodeGenFunction::SanitizerScope RAII scope.\n// Returns 'i1 false' when the conversion Src -> Dst changed the sign.\nstatic std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n                 std::pair<llvm::Value *, SanitizerMask>>\nEmitIntegerSignChangeCheckHelper(Value *Src, QualType SrcType, Value *Dst,\n                                 QualType DstType, CGBuilderTy &Builder) {\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = Dst->getType();\n\n  assert(isa<llvm::IntegerType>(SrcTy) && isa<llvm::IntegerType>(DstTy) &&\n         \"non-integer llvm type\");\n\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n  (void)SrcSigned; // Only used in assert()\n  (void)DstSigned; // Only used in assert()\n  unsigned SrcBits = SrcTy->getScalarSizeInBits();\n  unsigned DstBits = DstTy->getScalarSizeInBits();\n  (void)SrcBits; // Only used in assert()\n  (void)DstBits; // Only used in assert()\n\n  assert(((SrcBits != DstBits) || (SrcSigned != DstSigned)) &&\n         \"either the widths should be different, or the signednesses.\");\n\n  // NOTE: zero value is considered to be non-negative.\n  auto EmitIsNegativeTest = [&Builder](Value *V, QualType VType,\n                                       const char *Name) -> Value * {\n    // Is this value a signed type?\n    bool VSigned = VType->isSignedIntegerOrEnumerationType();\n    llvm::Type *VTy = V->getType();\n    if (!VSigned) {\n      // If the value is unsigned, then it is never negative.\n      // FIXME: can we encounter non-scalar VTy here?\n      return llvm::ConstantInt::getFalse(VTy->getContext());\n    }\n    // Get the zero of the same type with which we will be comparing.\n    llvm::Constant *Zero = llvm::ConstantInt::get(VTy, 0);\n    // %V.isnegative = icmp slt %V, 0\n    // I.e is %V *strictly* less than zero, does it have negative value?\n    return Builder.CreateICmp(llvm::ICmpInst::ICMP_SLT, V, Zero,\n                              llvm::Twine(Name) + \".\" + V->getName() +\n                                  \".negativitycheck\");\n  };\n\n  // 1. Was the old Value negative?\n  llvm::Value *SrcIsNegative = EmitIsNegativeTest(Src, SrcType, \"src\");\n  // 2. Is the new Value negative?\n  llvm::Value *DstIsNegative = EmitIsNegativeTest(Dst, DstType, \"dst\");\n  // 3. Now, was the 'negativity status' preserved during the conversion?\n  //    NOTE: conversion from negative to zero is considered to change the sign.\n  //    (We want to get 'false' when the conversion changed the sign)\n  //    So we should just equality-compare the negativity statuses.\n  llvm::Value *Check = nullptr;\n  Check = Builder.CreateICmpEQ(SrcIsNegative, DstIsNegative, \"signchangecheck\");\n  // If the comparison result is 'false', then the conversion changed the sign.\n  return std::make_pair(\n      ScalarExprEmitter::ICCK_IntegerSignChange,\n      std::make_pair(Check, SanitizerKind::ImplicitIntegerSignChange));\n}\n\nvoid ScalarExprEmitter::EmitIntegerSignChangeCheck(Value *Src, QualType SrcType,\n                                                   Value *Dst, QualType DstType,\n                                                   SourceLocation Loc) {\n  if (!CGF.SanOpts.has(SanitizerKind::ImplicitIntegerSignChange))\n    return;\n\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = Dst->getType();\n\n  // We only care about int->int conversions here.\n  // We ignore conversions to/from pointer and/or bool.\n  if (!PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(SrcType,\n                                                                       DstType))\n    return;\n\n  bool SrcSigned = SrcType->isSignedIntegerOrEnumerationType();\n  bool DstSigned = DstType->isSignedIntegerOrEnumerationType();\n  unsigned SrcBits = SrcTy->getScalarSizeInBits();\n  unsigned DstBits = DstTy->getScalarSizeInBits();\n\n  // Now, we do not need to emit the check in *all* of the cases.\n  // We can avoid emitting it in some obvious cases where it would have been\n  // dropped by the opt passes (instcombine) always anyways.\n  // If it's a cast between effectively the same type, no check.\n  // NOTE: this is *not* equivalent to checking the canonical types.\n  if (SrcSigned == DstSigned && SrcBits == DstBits)\n    return;\n  // At least one of the values needs to have signed type.\n  // If both are unsigned, then obviously, neither of them can be negative.\n  if (!SrcSigned && !DstSigned)\n    return;\n  // If the conversion is to *larger* *signed* type, then no check is needed.\n  // Because either sign-extension happens (so the sign will remain),\n  // or zero-extension will happen (the sign bit will be zero.)\n  if ((DstBits > SrcBits) && DstSigned)\n    return;\n  if (CGF.SanOpts.has(SanitizerKind::ImplicitSignedIntegerTruncation) &&\n      (SrcBits > DstBits) && SrcSigned) {\n    // If the signed integer truncation sanitizer is enabled,\n    // and this is a truncation from signed type, then no check is needed.\n    // Because here sign change check is interchangeable with truncation check.\n    return;\n  }\n  // That's it. We can't rule out any more cases with the data we have.\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n\n  std::pair<ScalarExprEmitter::ImplicitConversionCheckKind,\n            std::pair<llvm::Value *, SanitizerMask>>\n      Check;\n\n  // Each of these checks needs to return 'false' when an issue was detected.\n  ImplicitConversionCheckKind CheckKind;\n  llvm::SmallVector<std::pair<llvm::Value *, SanitizerMask>, 2> Checks;\n  // So we can 'and' all the checks together, and still get 'false',\n  // if at least one of the checks detected an issue.\n\n  Check = EmitIntegerSignChangeCheckHelper(Src, SrcType, Dst, DstType, Builder);\n  CheckKind = Check.first;\n  Checks.emplace_back(Check.second);\n\n  if (CGF.SanOpts.has(SanitizerKind::ImplicitSignedIntegerTruncation) &&\n      (SrcBits > DstBits) && !SrcSigned && DstSigned) {\n    // If the signed integer truncation sanitizer was enabled,\n    // and we are truncating from larger unsigned type to smaller signed type,\n    // let's handle the case we skipped in that check.\n    Check =\n        EmitIntegerTruncationCheckHelper(Src, SrcType, Dst, DstType, Builder);\n    CheckKind = ICCK_SignedIntegerTruncationOrSignChange;\n    Checks.emplace_back(Check.second);\n    // If the comparison result is 'i1 false', then the truncation was lossy.\n  }\n\n  llvm::Constant *StaticArgs[] = {\n      CGF.EmitCheckSourceLocation(Loc), CGF.EmitCheckTypeDescriptor(SrcType),\n      CGF.EmitCheckTypeDescriptor(DstType),\n      llvm::ConstantInt::get(Builder.getInt8Ty(), CheckKind)};\n  // EmitCheck() will 'and' all the checks together.\n  CGF.EmitCheck(Checks, SanitizerHandler::ImplicitConversion, StaticArgs,\n                {Src, Dst});\n}\n\n/// Emit a conversion from the specified type to the specified destination type,\n/// both of which are LLVM scalar types.\nValue *ScalarExprEmitter::EmitScalarConversion(Value *Src, QualType SrcType,\n                                               QualType DstType,\n                                               SourceLocation Loc,\n                                               ScalarConversionOpts Opts) {\n  // All conversions involving fixed point types should be handled by the\n  // EmitFixedPoint family functions. This is done to prevent bloating up this\n  // function more, and although fixed point numbers are represented by\n  // integers, we do not want to follow any logic that assumes they should be\n  // treated as integers.\n  // TODO(leonardchan): When necessary, add another if statement checking for\n  // conversions to fixed point types from other types.\n  if (SrcType->isFixedPointType()) {\n    if (DstType->isBooleanType())\n      // It is important that we check this before checking if the dest type is\n      // an integer because booleans are technically integer types.\n      // We do not need to check the padding bit on unsigned types if unsigned\n      // padding is enabled because overflow into this bit is undefined\n      // behavior.\n      return Builder.CreateIsNotNull(Src, \"tobool\");\n    if (DstType->isFixedPointType() || DstType->isIntegerType() ||\n        DstType->isRealFloatingType())\n      return EmitFixedPointConversion(Src, SrcType, DstType, Loc);\n\n    llvm_unreachable(\n        \"Unhandled scalar conversion from a fixed point type to another type.\");\n  } else if (DstType->isFixedPointType()) {\n    if (SrcType->isIntegerType() || SrcType->isRealFloatingType())\n      // This also includes converting booleans and enums to fixed point types.\n      return EmitFixedPointConversion(Src, SrcType, DstType, Loc);\n\n    llvm_unreachable(\n        \"Unhandled scalar conversion to a fixed point type from another type.\");\n  }\n\n  QualType NoncanonicalSrcType = SrcType;\n  QualType NoncanonicalDstType = DstType;\n\n  SrcType = CGF.getContext().getCanonicalType(SrcType);\n  DstType = CGF.getContext().getCanonicalType(DstType);\n  if (SrcType == DstType) return Src;\n\n  if (DstType->isVoidType()) return nullptr;\n\n  llvm::Value *OrigSrc = Src;\n  QualType OrigSrcType = SrcType;\n  llvm::Type *SrcTy = Src->getType();\n\n  // Handle conversions to bool first, they are special: comparisons against 0.\n  if (DstType->isBooleanType())\n    return EmitConversionToBool(Src, SrcType);\n\n  llvm::Type *DstTy = ConvertType(DstType);\n\n  // Cast from half through float if half isn't a native type.\n  if (SrcType->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n    // Cast to FP using the intrinsic if the half type itself isn't supported.\n    if (DstTy->isFloatingPointTy()) {\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics())\n        return Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_from_fp16, DstTy),\n            Src);\n    } else {\n      // Cast to other types through float, using either the intrinsic or FPExt,\n      // depending on whether the half type itself is supported\n      // (as opposed to operations on half, available with NativeHalfType).\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n        Src = Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_from_fp16,\n                                 CGF.CGM.FloatTy),\n            Src);\n      } else {\n        Src = Builder.CreateFPExt(Src, CGF.CGM.FloatTy, \"conv\");\n      }\n      SrcType = CGF.getContext().FloatTy;\n      SrcTy = CGF.FloatTy;\n    }\n  }\n\n  // Ignore conversions like int -> uint.\n  if (SrcTy == DstTy) {\n    if (Opts.EmitImplicitIntegerSignChangeChecks)\n      EmitIntegerSignChangeCheck(Src, NoncanonicalSrcType, Src,\n                                 NoncanonicalDstType, Loc);\n\n    return Src;\n  }\n\n  // Handle pointer conversions next: pointers can only be converted to/from\n  // other pointers and integers. Check for pointer types in terms of LLVM, as\n  // some native types (like Obj-C id) may map to a pointer type.\n  if (auto DstPT = dyn_cast<llvm::PointerType>(DstTy)) {\n    // The source value may be an integer, or a pointer.\n    if (isa<llvm::PointerType>(SrcTy))\n      return Builder.CreateBitCast(Src, DstTy, \"conv\");\n\n    assert(SrcType->isIntegerType() && \"Not ptr->ptr or int->ptr conversion?\");\n    // First, convert to the correct width so that we control the kind of\n    // extension.\n    llvm::Type *MiddleTy = CGF.CGM.getDataLayout().getIntPtrType(DstPT);\n    bool InputSigned = SrcType->isSignedIntegerOrEnumerationType();\n    llvm::Value* IntResult =\n        Builder.CreateIntCast(Src, MiddleTy, InputSigned, \"conv\");\n    // Then, cast to pointer.\n    return Builder.CreateIntToPtr(IntResult, DstTy, \"conv\");\n  }\n\n  if (isa<llvm::PointerType>(SrcTy)) {\n    // Must be an ptr to int cast.\n    assert(isa<llvm::IntegerType>(DstTy) && \"not ptr->int?\");\n    return Builder.CreatePtrToInt(Src, DstTy, \"conv\");\n  }\n\n  // A scalar can be splatted to an extended vector of the same element type\n  if (DstType->isExtVectorType() && !SrcType->isVectorType()) {\n    // Sema should add casts to make sure that the source expression's type is\n    // the same as the vector's element type (sans qualifiers)\n    assert(DstType->castAs<ExtVectorType>()->getElementType().getTypePtr() ==\n               SrcType.getTypePtr() &&\n           \"Splatted expr doesn't match with vector element type?\");\n\n    // Splat the element across to all elements\n    unsigned NumElements = cast<llvm::FixedVectorType>(DstTy)->getNumElements();\n    return Builder.CreateVectorSplat(NumElements, Src, \"splat\");\n  }\n\n  if (isa<llvm::VectorType>(SrcTy) || isa<llvm::VectorType>(DstTy)) {\n    // Allow bitcast from vector to integer/fp of the same size.\n    unsigned SrcSize = SrcTy->getPrimitiveSizeInBits();\n    unsigned DstSize = DstTy->getPrimitiveSizeInBits();\n    if (SrcSize == DstSize)\n      return Builder.CreateBitCast(Src, DstTy, \"conv\");\n\n    // Conversions between vectors of different sizes are not allowed except\n    // when vectors of half are involved. Operations on storage-only half\n    // vectors require promoting half vector operands to float vectors and\n    // truncating the result, which is either an int or float vector, to a\n    // short or half vector.\n\n    // Source and destination are both expected to be vectors.\n    llvm::Type *SrcElementTy = cast<llvm::VectorType>(SrcTy)->getElementType();\n    llvm::Type *DstElementTy = cast<llvm::VectorType>(DstTy)->getElementType();\n    (void)DstElementTy;\n\n    assert(((SrcElementTy->isIntegerTy() &&\n             DstElementTy->isIntegerTy()) ||\n            (SrcElementTy->isFloatingPointTy() &&\n             DstElementTy->isFloatingPointTy())) &&\n           \"unexpected conversion between a floating-point vector and an \"\n           \"integer vector\");\n\n    // Truncate an i32 vector to an i16 vector.\n    if (SrcElementTy->isIntegerTy())\n      return Builder.CreateIntCast(Src, DstTy, false, \"conv\");\n\n    // Truncate a float vector to a half vector.\n    if (SrcSize > DstSize)\n      return Builder.CreateFPTrunc(Src, DstTy, \"conv\");\n\n    // Promote a half vector to a float vector.\n    return Builder.CreateFPExt(Src, DstTy, \"conv\");\n  }\n\n  // Finally, we have the arithmetic types: real int/float.\n  Value *Res = nullptr;\n  llvm::Type *ResTy = DstTy;\n\n  // An overflowing conversion has undefined behavior if either the source type\n  // or the destination type is a floating-point type. However, we consider the\n  // range of representable values for all floating-point types to be\n  // [-inf,+inf], so no overflow can ever happen when the destination type is a\n  // floating-point type.\n  if (CGF.SanOpts.has(SanitizerKind::FloatCastOverflow) &&\n      OrigSrcType->isFloatingType())\n    EmitFloatConversionCheck(OrigSrc, OrigSrcType, Src, SrcType, DstType, DstTy,\n                             Loc);\n\n  // Cast to half through float if half isn't a native type.\n  if (DstType->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n    // Make sure we cast in a single step if from another FP type.\n    if (SrcTy->isFloatingPointTy()) {\n      // Use the intrinsic if the half type itself isn't supported\n      // (as opposed to operations on half, available with NativeHalfType).\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics())\n        return Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_to_fp16, SrcTy), Src);\n      // If the half type is supported, just use an fptrunc.\n      return Builder.CreateFPTrunc(Src, DstTy);\n    }\n    DstTy = CGF.FloatTy;\n  }\n\n  if (isa<llvm::IntegerType>(SrcTy)) {\n    bool InputSigned = SrcType->isSignedIntegerOrEnumerationType();\n    if (SrcType->isBooleanType() && Opts.TreatBooleanAsSigned) {\n      InputSigned = true;\n    }\n    if (isa<llvm::IntegerType>(DstTy))\n      Res = Builder.CreateIntCast(Src, DstTy, InputSigned, \"conv\");\n    else if (InputSigned)\n      Res = Builder.CreateSIToFP(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateUIToFP(Src, DstTy, \"conv\");\n  } else if (isa<llvm::IntegerType>(DstTy)) {\n    assert(SrcTy->isFloatingPointTy() && \"Unknown real conversion\");\n    if (DstType->isSignedIntegerOrEnumerationType())\n      Res = Builder.CreateFPToSI(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPToUI(Src, DstTy, \"conv\");\n  } else {\n    assert(SrcTy->isFloatingPointTy() && DstTy->isFloatingPointTy() &&\n           \"Unknown real conversion\");\n    if (DstTy->getTypeID() < SrcTy->getTypeID())\n      Res = Builder.CreateFPTrunc(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPExt(Src, DstTy, \"conv\");\n  }\n\n  if (DstTy != ResTy) {\n    if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n      assert(ResTy->isIntegerTy(16) && \"Only half FP requires extra conversion\");\n      Res = Builder.CreateCall(\n        CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_to_fp16, CGF.CGM.FloatTy),\n        Res);\n    } else {\n      Res = Builder.CreateFPTrunc(Res, ResTy, \"conv\");\n    }\n  }\n\n  if (Opts.EmitImplicitIntegerTruncationChecks)\n    EmitIntegerTruncationCheck(Src, NoncanonicalSrcType, Res,\n                               NoncanonicalDstType, Loc);\n\n  if (Opts.EmitImplicitIntegerSignChangeChecks)\n    EmitIntegerSignChangeCheck(Src, NoncanonicalSrcType, Res,\n                               NoncanonicalDstType, Loc);\n\n  return Res;\n}\n\nValue *ScalarExprEmitter::EmitFixedPointConversion(Value *Src, QualType SrcTy,\n                                                   QualType DstTy,\n                                                   SourceLocation Loc) {\n  llvm::FixedPointBuilder<CGBuilderTy> FPBuilder(Builder);\n  llvm::Value *Result;\n  if (SrcTy->isRealFloatingType())\n    Result = FPBuilder.CreateFloatingToFixed(Src,\n        CGF.getContext().getFixedPointSemantics(DstTy));\n  else if (DstTy->isRealFloatingType())\n    Result = FPBuilder.CreateFixedToFloating(Src,\n        CGF.getContext().getFixedPointSemantics(SrcTy),\n        ConvertType(DstTy));\n  else {\n    auto SrcFPSema = CGF.getContext().getFixedPointSemantics(SrcTy);\n    auto DstFPSema = CGF.getContext().getFixedPointSemantics(DstTy);\n\n    if (DstTy->isIntegerType())\n      Result = FPBuilder.CreateFixedToInteger(Src, SrcFPSema,\n                                              DstFPSema.getWidth(),\n                                              DstFPSema.isSigned());\n    else if (SrcTy->isIntegerType())\n      Result =  FPBuilder.CreateIntegerToFixed(Src, SrcFPSema.isSigned(),\n                                               DstFPSema);\n    else\n      Result = FPBuilder.CreateFixedToFixed(Src, SrcFPSema, DstFPSema);\n  }\n  return Result;\n}\n\n/// Emit a conversion from the specified complex type to the specified\n/// destination type, where the destination type is an LLVM scalar type.\nValue *ScalarExprEmitter::EmitComplexToScalarConversion(\n    CodeGenFunction::ComplexPairTy Src, QualType SrcTy, QualType DstTy,\n    SourceLocation Loc) {\n  // Get the source element type.\n  SrcTy = SrcTy->castAs<ComplexType>()->getElementType();\n\n  // Handle conversions to bool first, they are special: comparisons against 0.\n  if (DstTy->isBooleanType()) {\n    //  Complex != 0  -> (Real != 0) | (Imag != 0)\n    Src.first = EmitScalarConversion(Src.first, SrcTy, DstTy, Loc);\n    Src.second = EmitScalarConversion(Src.second, SrcTy, DstTy, Loc);\n    return Builder.CreateOr(Src.first, Src.second, \"tobool\");\n  }\n\n  // C99 6.3.1.7p2: \"When a value of complex type is converted to a real type,\n  // the imaginary part of the complex value is discarded and the value of the\n  // real part is converted according to the conversion rules for the\n  // corresponding real type.\n  return EmitScalarConversion(Src.first, SrcTy, DstTy, Loc);\n}\n\nValue *ScalarExprEmitter::EmitNullValue(QualType Ty) {\n  return CGF.EmitFromMemory(CGF.CGM.EmitNullConstant(Ty), Ty);\n}\n\n/// Emit a sanitization check for the given \"binary\" operation (which\n/// might actually be a unary increment which has been lowered to a binary\n/// operation). The check passes if all values in \\p Checks (which are \\c i1),\n/// are \\c true.\nvoid ScalarExprEmitter::EmitBinOpCheck(\n    ArrayRef<std::pair<Value *, SanitizerMask>> Checks, const BinOpInfo &Info) {\n  assert(CGF.IsSanitizerScope);\n  SanitizerHandler Check;\n  SmallVector<llvm::Constant *, 4> StaticData;\n  SmallVector<llvm::Value *, 2> DynamicData;\n\n  BinaryOperatorKind Opcode = Info.Opcode;\n  if (BinaryOperator::isCompoundAssignmentOp(Opcode))\n    Opcode = BinaryOperator::getOpForCompoundAssignment(Opcode);\n\n  StaticData.push_back(CGF.EmitCheckSourceLocation(Info.E->getExprLoc()));\n  const UnaryOperator *UO = dyn_cast<UnaryOperator>(Info.E);\n  if (UO && UO->getOpcode() == UO_Minus) {\n    Check = SanitizerHandler::NegateOverflow;\n    StaticData.push_back(CGF.EmitCheckTypeDescriptor(UO->getType()));\n    DynamicData.push_back(Info.RHS);\n  } else {\n    if (BinaryOperator::isShiftOp(Opcode)) {\n      // Shift LHS negative or too large, or RHS out of bounds.\n      Check = SanitizerHandler::ShiftOutOfBounds;\n      const BinaryOperator *BO = cast<BinaryOperator>(Info.E);\n      StaticData.push_back(\n        CGF.EmitCheckTypeDescriptor(BO->getLHS()->getType()));\n      StaticData.push_back(\n        CGF.EmitCheckTypeDescriptor(BO->getRHS()->getType()));\n    } else if (Opcode == BO_Div || Opcode == BO_Rem) {\n      // Divide or modulo by zero, or signed overflow (eg INT_MAX / -1).\n      Check = SanitizerHandler::DivremOverflow;\n      StaticData.push_back(CGF.EmitCheckTypeDescriptor(Info.Ty));\n    } else {\n      // Arithmetic overflow (+, -, *).\n      switch (Opcode) {\n      case BO_Add: Check = SanitizerHandler::AddOverflow; break;\n      case BO_Sub: Check = SanitizerHandler::SubOverflow; break;\n      case BO_Mul: Check = SanitizerHandler::MulOverflow; break;\n      default: llvm_unreachable(\"unexpected opcode for bin op check\");\n      }\n      StaticData.push_back(CGF.EmitCheckTypeDescriptor(Info.Ty));\n    }\n    DynamicData.push_back(Info.LHS);\n    DynamicData.push_back(Info.RHS);\n  }\n\n  CGF.EmitCheck(Checks, Check, StaticData, DynamicData);\n}\n\n//===----------------------------------------------------------------------===//\n//                            Visitor Methods\n//===----------------------------------------------------------------------===//\n\nValue *ScalarExprEmitter::VisitExpr(Expr *E) {\n  CGF.ErrorUnsupported(E, \"scalar expression\");\n  if (E->getType()->isVoidType())\n    return nullptr;\n  return llvm::UndefValue::get(CGF.ConvertType(E->getType()));\n}\n\nValue *ScalarExprEmitter::VisitShuffleVectorExpr(ShuffleVectorExpr *E) {\n  // Vector Mask Case\n  if (E->getNumSubExprs() == 2) {\n    Value *LHS = CGF.EmitScalarExpr(E->getExpr(0));\n    Value *RHS = CGF.EmitScalarExpr(E->getExpr(1));\n    Value *Mask;\n\n    auto *LTy = cast<llvm::FixedVectorType>(LHS->getType());\n    unsigned LHSElts = LTy->getNumElements();\n\n    Mask = RHS;\n\n    auto *MTy = cast<llvm::FixedVectorType>(Mask->getType());\n\n    // Mask off the high bits of each shuffle index.\n    Value *MaskBits =\n        llvm::ConstantInt::get(MTy, llvm::NextPowerOf2(LHSElts - 1) - 1);\n    Mask = Builder.CreateAnd(Mask, MaskBits, \"mask\");\n\n    // newv = undef\n    // mask = mask & maskbits\n    // for each elt\n    //   n = extract mask i\n    //   x = extract val n\n    //   newv = insert newv, x, i\n    auto *RTy = llvm::FixedVectorType::get(LTy->getElementType(),\n                                           MTy->getNumElements());\n    Value* NewV = llvm::UndefValue::get(RTy);\n    for (unsigned i = 0, e = MTy->getNumElements(); i != e; ++i) {\n      Value *IIndx = llvm::ConstantInt::get(CGF.SizeTy, i);\n      Value *Indx = Builder.CreateExtractElement(Mask, IIndx, \"shuf_idx\");\n\n      Value *VExt = Builder.CreateExtractElement(LHS, Indx, \"shuf_elt\");\n      NewV = Builder.CreateInsertElement(NewV, VExt, IIndx, \"shuf_ins\");\n    }\n    return NewV;\n  }\n\n  Value* V1 = CGF.EmitScalarExpr(E->getExpr(0));\n  Value* V2 = CGF.EmitScalarExpr(E->getExpr(1));\n\n  SmallVector<int, 32> Indices;\n  for (unsigned i = 2; i < E->getNumSubExprs(); ++i) {\n    llvm::APSInt Idx = E->getShuffleMaskIdx(CGF.getContext(), i-2);\n    // Check for -1 and output it as undef in the IR.\n    if (Idx.isSigned() && Idx.isAllOnesValue())\n      Indices.push_back(-1);\n    else\n      Indices.push_back(Idx.getZExtValue());\n  }\n\n  return Builder.CreateShuffleVector(V1, V2, Indices, \"shuffle\");\n}\n\nValue *ScalarExprEmitter::VisitConvertVectorExpr(ConvertVectorExpr *E) {\n  QualType SrcType = E->getSrcExpr()->getType(),\n           DstType = E->getType();\n\n  Value *Src  = CGF.EmitScalarExpr(E->getSrcExpr());\n\n  SrcType = CGF.getContext().getCanonicalType(SrcType);\n  DstType = CGF.getContext().getCanonicalType(DstType);\n  if (SrcType == DstType) return Src;\n\n  assert(SrcType->isVectorType() &&\n         \"ConvertVector source type must be a vector\");\n  assert(DstType->isVectorType() &&\n         \"ConvertVector destination type must be a vector\");\n\n  llvm::Type *SrcTy = Src->getType();\n  llvm::Type *DstTy = ConvertType(DstType);\n\n  // Ignore conversions like int -> uint.\n  if (SrcTy == DstTy)\n    return Src;\n\n  QualType SrcEltType = SrcType->castAs<VectorType>()->getElementType(),\n           DstEltType = DstType->castAs<VectorType>()->getElementType();\n\n  assert(SrcTy->isVectorTy() &&\n         \"ConvertVector source IR type must be a vector\");\n  assert(DstTy->isVectorTy() &&\n         \"ConvertVector destination IR type must be a vector\");\n\n  llvm::Type *SrcEltTy = cast<llvm::VectorType>(SrcTy)->getElementType(),\n             *DstEltTy = cast<llvm::VectorType>(DstTy)->getElementType();\n\n  if (DstEltType->isBooleanType()) {\n    assert((SrcEltTy->isFloatingPointTy() ||\n            isa<llvm::IntegerType>(SrcEltTy)) && \"Unknown boolean conversion\");\n\n    llvm::Value *Zero = llvm::Constant::getNullValue(SrcTy);\n    if (SrcEltTy->isFloatingPointTy()) {\n      return Builder.CreateFCmpUNE(Src, Zero, \"tobool\");\n    } else {\n      return Builder.CreateICmpNE(Src, Zero, \"tobool\");\n    }\n  }\n\n  // We have the arithmetic types: real int/float.\n  Value *Res = nullptr;\n\n  if (isa<llvm::IntegerType>(SrcEltTy)) {\n    bool InputSigned = SrcEltType->isSignedIntegerOrEnumerationType();\n    if (isa<llvm::IntegerType>(DstEltTy))\n      Res = Builder.CreateIntCast(Src, DstTy, InputSigned, \"conv\");\n    else if (InputSigned)\n      Res = Builder.CreateSIToFP(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateUIToFP(Src, DstTy, \"conv\");\n  } else if (isa<llvm::IntegerType>(DstEltTy)) {\n    assert(SrcEltTy->isFloatingPointTy() && \"Unknown real conversion\");\n    if (DstEltType->isSignedIntegerOrEnumerationType())\n      Res = Builder.CreateFPToSI(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPToUI(Src, DstTy, \"conv\");\n  } else {\n    assert(SrcEltTy->isFloatingPointTy() && DstEltTy->isFloatingPointTy() &&\n           \"Unknown real conversion\");\n    if (DstEltTy->getTypeID() < SrcEltTy->getTypeID())\n      Res = Builder.CreateFPTrunc(Src, DstTy, \"conv\");\n    else\n      Res = Builder.CreateFPExt(Src, DstTy, \"conv\");\n  }\n\n  return Res;\n}\n\nValue *ScalarExprEmitter::VisitMemberExpr(MemberExpr *E) {\n  if (CodeGenFunction::ConstantEmission Constant = CGF.tryEmitAsConstant(E)) {\n    CGF.EmitIgnoredExpr(E->getBase());\n    return CGF.emitScalarConstant(Constant, E);\n  } else {\n    Expr::EvalResult Result;\n    if (E->EvaluateAsInt(Result, CGF.getContext(), Expr::SE_AllowSideEffects)) {\n      llvm::APSInt Value = Result.Val.getInt();\n      CGF.EmitIgnoredExpr(E->getBase());\n      return Builder.getInt(Value);\n    }\n  }\n\n  return EmitLoadOfLValue(E);\n}\n\nValue *ScalarExprEmitter::VisitArraySubscriptExpr(ArraySubscriptExpr *E) {\n  TestAndClearIgnoreResultAssign();\n\n  // Emit subscript expressions in rvalue context's.  For most cases, this just\n  // loads the lvalue formed by the subscript expr.  However, we have to be\n  // careful, because the base of a vector subscript is occasionally an rvalue,\n  // so we can't get it as an lvalue.\n  if (!E->getBase()->getType()->isVectorType())\n    return EmitLoadOfLValue(E);\n\n  // Handle the vector case.  The base must be a vector, the index must be an\n  // integer value.\n  Value *Base = Visit(E->getBase());\n  Value *Idx  = Visit(E->getIdx());\n  QualType IdxTy = E->getIdx()->getType();\n\n  if (CGF.SanOpts.has(SanitizerKind::ArrayBounds))\n    CGF.EmitBoundsCheck(E, E->getBase(), Idx, IdxTy, /*Accessed*/true);\n\n  return Builder.CreateExtractElement(Base, Idx, \"vecext\");\n}\n\nValue *ScalarExprEmitter::VisitMatrixSubscriptExpr(MatrixSubscriptExpr *E) {\n  TestAndClearIgnoreResultAssign();\n\n  // Handle the vector case.  The base must be a vector, the index must be an\n  // integer value.\n  Value *RowIdx = Visit(E->getRowIdx());\n  Value *ColumnIdx = Visit(E->getColumnIdx());\n  Value *Matrix = Visit(E->getBase());\n\n  // TODO: Should we emit bounds checks with SanitizerKind::ArrayBounds?\n  llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n  return MB.CreateExtractElement(\n      Matrix, RowIdx, ColumnIdx,\n      E->getBase()->getType()->getAs<ConstantMatrixType>()->getNumRows());\n}\n\nstatic int getMaskElt(llvm::ShuffleVectorInst *SVI, unsigned Idx,\n                      unsigned Off) {\n  int MV = SVI->getMaskValue(Idx);\n  if (MV == -1)\n    return -1;\n  return Off + MV;\n}\n\nstatic int getAsInt32(llvm::ConstantInt *C, llvm::Type *I32Ty) {\n  assert(llvm::ConstantInt::isValueValidForType(I32Ty, C->getZExtValue()) &&\n         \"Index operand too large for shufflevector mask!\");\n  return C->getZExtValue();\n}\n\nValue *ScalarExprEmitter::VisitInitListExpr(InitListExpr *E) {\n  bool Ignore = TestAndClearIgnoreResultAssign();\n  (void)Ignore;\n  assert (Ignore == false && \"init list ignored\");\n  unsigned NumInitElements = E->getNumInits();\n\n  if (E->hadArrayRangeDesignator())\n    CGF.ErrorUnsupported(E, \"GNU array range designator extension\");\n\n  llvm::VectorType *VType =\n    dyn_cast<llvm::VectorType>(ConvertType(E->getType()));\n\n  if (!VType) {\n    if (NumInitElements == 0) {\n      // C++11 value-initialization for the scalar.\n      return EmitNullValue(E->getType());\n    }\n    // We have a scalar in braces. Just use the first element.\n    return Visit(E->getInit(0));\n  }\n\n  unsigned ResElts = cast<llvm::FixedVectorType>(VType)->getNumElements();\n\n  // Loop over initializers collecting the Value for each, and remembering\n  // whether the source was swizzle (ExtVectorElementExpr).  This will allow\n  // us to fold the shuffle for the swizzle into the shuffle for the vector\n  // initializer, since LLVM optimizers generally do not want to touch\n  // shuffles.\n  unsigned CurIdx = 0;\n  bool VIsUndefShuffle = false;\n  llvm::Value *V = llvm::UndefValue::get(VType);\n  for (unsigned i = 0; i != NumInitElements; ++i) {\n    Expr *IE = E->getInit(i);\n    Value *Init = Visit(IE);\n    SmallVector<int, 16> Args;\n\n    llvm::VectorType *VVT = dyn_cast<llvm::VectorType>(Init->getType());\n\n    // Handle scalar elements.  If the scalar initializer is actually one\n    // element of a different vector of the same width, use shuffle instead of\n    // extract+insert.\n    if (!VVT) {\n      if (isa<ExtVectorElementExpr>(IE)) {\n        llvm::ExtractElementInst *EI = cast<llvm::ExtractElementInst>(Init);\n\n        if (cast<llvm::FixedVectorType>(EI->getVectorOperandType())\n                ->getNumElements() == ResElts) {\n          llvm::ConstantInt *C = cast<llvm::ConstantInt>(EI->getIndexOperand());\n          Value *LHS = nullptr, *RHS = nullptr;\n          if (CurIdx == 0) {\n            // insert into undef -> shuffle (src, undef)\n            // shufflemask must use an i32\n            Args.push_back(getAsInt32(C, CGF.Int32Ty));\n            Args.resize(ResElts, -1);\n\n            LHS = EI->getVectorOperand();\n            RHS = V;\n            VIsUndefShuffle = true;\n          } else if (VIsUndefShuffle) {\n            // insert into undefshuffle && size match -> shuffle (v, src)\n            llvm::ShuffleVectorInst *SVV = cast<llvm::ShuffleVectorInst>(V);\n            for (unsigned j = 0; j != CurIdx; ++j)\n              Args.push_back(getMaskElt(SVV, j, 0));\n            Args.push_back(ResElts + C->getZExtValue());\n            Args.resize(ResElts, -1);\n\n            LHS = cast<llvm::ShuffleVectorInst>(V)->getOperand(0);\n            RHS = EI->getVectorOperand();\n            VIsUndefShuffle = false;\n          }\n          if (!Args.empty()) {\n            V = Builder.CreateShuffleVector(LHS, RHS, Args);\n            ++CurIdx;\n            continue;\n          }\n        }\n      }\n      V = Builder.CreateInsertElement(V, Init, Builder.getInt32(CurIdx),\n                                      \"vecinit\");\n      VIsUndefShuffle = false;\n      ++CurIdx;\n      continue;\n    }\n\n    unsigned InitElts = cast<llvm::FixedVectorType>(VVT)->getNumElements();\n\n    // If the initializer is an ExtVecEltExpr (a swizzle), and the swizzle's\n    // input is the same width as the vector being constructed, generate an\n    // optimized shuffle of the swizzle input into the result.\n    unsigned Offset = (CurIdx == 0) ? 0 : ResElts;\n    if (isa<ExtVectorElementExpr>(IE)) {\n      llvm::ShuffleVectorInst *SVI = cast<llvm::ShuffleVectorInst>(Init);\n      Value *SVOp = SVI->getOperand(0);\n      auto *OpTy = cast<llvm::FixedVectorType>(SVOp->getType());\n\n      if (OpTy->getNumElements() == ResElts) {\n        for (unsigned j = 0; j != CurIdx; ++j) {\n          // If the current vector initializer is a shuffle with undef, merge\n          // this shuffle directly into it.\n          if (VIsUndefShuffle) {\n            Args.push_back(getMaskElt(cast<llvm::ShuffleVectorInst>(V), j, 0));\n          } else {\n            Args.push_back(j);\n          }\n        }\n        for (unsigned j = 0, je = InitElts; j != je; ++j)\n          Args.push_back(getMaskElt(SVI, j, Offset));\n        Args.resize(ResElts, -1);\n\n        if (VIsUndefShuffle)\n          V = cast<llvm::ShuffleVectorInst>(V)->getOperand(0);\n\n        Init = SVOp;\n      }\n    }\n\n    // Extend init to result vector length, and then shuffle its contribution\n    // to the vector initializer into V.\n    if (Args.empty()) {\n      for (unsigned j = 0; j != InitElts; ++j)\n        Args.push_back(j);\n      Args.resize(ResElts, -1);\n      Init = Builder.CreateShuffleVector(Init, Args, \"vext\");\n\n      Args.clear();\n      for (unsigned j = 0; j != CurIdx; ++j)\n        Args.push_back(j);\n      for (unsigned j = 0; j != InitElts; ++j)\n        Args.push_back(j + Offset);\n      Args.resize(ResElts, -1);\n    }\n\n    // If V is undef, make sure it ends up on the RHS of the shuffle to aid\n    // merging subsequent shuffles into this one.\n    if (CurIdx == 0)\n      std::swap(V, Init);\n    V = Builder.CreateShuffleVector(V, Init, Args, \"vecinit\");\n    VIsUndefShuffle = isa<llvm::UndefValue>(Init);\n    CurIdx += InitElts;\n  }\n\n  // FIXME: evaluate codegen vs. shuffling against constant null vector.\n  // Emit remaining default initializers.\n  llvm::Type *EltTy = VType->getElementType();\n\n  // Emit remaining default initializers\n  for (/* Do not initialize i*/; CurIdx < ResElts; ++CurIdx) {\n    Value *Idx = Builder.getInt32(CurIdx);\n    llvm::Value *Init = llvm::Constant::getNullValue(EltTy);\n    V = Builder.CreateInsertElement(V, Init, Idx, \"vecinit\");\n  }\n  return V;\n}\n\nbool CodeGenFunction::ShouldNullCheckClassCastValue(const CastExpr *CE) {\n  const Expr *E = CE->getSubExpr();\n\n  if (CE->getCastKind() == CK_UncheckedDerivedToBase)\n    return false;\n\n  if (isa<CXXThisExpr>(E->IgnoreParens())) {\n    // We always assume that 'this' is never null.\n    return false;\n  }\n\n  if (const ImplicitCastExpr *ICE = dyn_cast<ImplicitCastExpr>(CE)) {\n    // And that glvalue casts are never null.\n    if (ICE->getValueKind() != VK_RValue)\n      return false;\n  }\n\n  return true;\n}\n\n// VisitCastExpr - Emit code for an explicit or implicit cast.  Implicit casts\n// have to handle a more broad range of conversions than explicit casts, as they\n// handle things like function to ptr-to-function decay etc.\nValue *ScalarExprEmitter::VisitCastExpr(CastExpr *CE) {\n  Expr *E = CE->getSubExpr();\n  QualType DestTy = CE->getType();\n  CastKind Kind = CE->getCastKind();\n\n  // These cases are generally not written to ignore the result of\n  // evaluating their sub-expressions, so we clear this now.\n  bool Ignored = TestAndClearIgnoreResultAssign();\n\n  // Since almost all cast kinds apply to scalars, this switch doesn't have\n  // a default case, so the compiler will warn on a missing case.  The cases\n  // are in the same order as in the CastKind enum.\n  switch (Kind) {\n  case CK_Dependent: llvm_unreachable(\"dependent cast kind in IR gen!\");\n  case CK_BuiltinFnToFnPtr:\n    llvm_unreachable(\"builtin functions are handled elsewhere\");\n\n  case CK_LValueBitCast:\n  case CK_ObjCObjectLValueCast: {\n    Address Addr = EmitLValue(E).getAddress(CGF);\n    Addr = Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(DestTy));\n    LValue LV = CGF.MakeAddrLValue(Addr, DestTy);\n    return EmitLoadOfLValue(LV, CE->getExprLoc());\n  }\n\n  case CK_LValueToRValueBitCast: {\n    LValue SourceLVal = CGF.EmitLValue(E);\n    Address Addr = Builder.CreateElementBitCast(SourceLVal.getAddress(CGF),\n                                                CGF.ConvertTypeForMem(DestTy));\n    LValue DestLV = CGF.MakeAddrLValue(Addr, DestTy);\n    DestLV.setTBAAInfo(TBAAAccessInfo::getMayAliasInfo());\n    return EmitLoadOfLValue(DestLV, CE->getExprLoc());\n  }\n\n  case CK_CPointerToObjCPointerCast:\n  case CK_BlockPointerToObjCPointerCast:\n  case CK_AnyPointerToBlockPointerCast:\n  case CK_BitCast: {\n    Value *Src = Visit(const_cast<Expr*>(E));\n    llvm::Type *SrcTy = Src->getType();\n    llvm::Type *DstTy = ConvertType(DestTy);\n    if (SrcTy->isPtrOrPtrVectorTy() && DstTy->isPtrOrPtrVectorTy() &&\n        SrcTy->getPointerAddressSpace() != DstTy->getPointerAddressSpace()) {\n      llvm_unreachable(\"wrong cast for pointers in different address spaces\"\n                       \"(must be an address space cast)!\");\n    }\n\n    if (CGF.SanOpts.has(SanitizerKind::CFIUnrelatedCast)) {\n      if (auto PT = DestTy->getAs<PointerType>())\n        CGF.EmitVTablePtrCheckForCast(PT->getPointeeType(), Src,\n                                      /*MayBeNull=*/true,\n                                      CodeGenFunction::CFITCK_UnrelatedCast,\n                                      CE->getBeginLoc());\n    }\n\n    if (CGF.CGM.getCodeGenOpts().StrictVTablePointers) {\n      const QualType SrcType = E->getType();\n\n      if (SrcType.mayBeNotDynamicClass() && DestTy.mayBeDynamicClass()) {\n        // Casting to pointer that could carry dynamic information (provided by\n        // invariant.group) requires launder.\n        Src = Builder.CreateLaunderInvariantGroup(Src);\n      } else if (SrcType.mayBeDynamicClass() && DestTy.mayBeNotDynamicClass()) {\n        // Casting to pointer that does not carry dynamic information (provided\n        // by invariant.group) requires stripping it.  Note that we don't do it\n        // if the source could not be dynamic type and destination could be\n        // dynamic because dynamic information is already laundered.  It is\n        // because launder(strip(src)) == launder(src), so there is no need to\n        // add extra strip before launder.\n        Src = Builder.CreateStripInvariantGroup(Src);\n      }\n    }\n\n    // Update heapallocsite metadata when there is an explicit pointer cast.\n    if (auto *CI = dyn_cast<llvm::CallBase>(Src)) {\n      if (CI->getMetadata(\"heapallocsite\") && isa<ExplicitCastExpr>(CE)) {\n        QualType PointeeType = DestTy->getPointeeType();\n        if (!PointeeType.isNull())\n          CGF.getDebugInfo()->addHeapAllocSiteMetadata(CI, PointeeType,\n                                                       CE->getExprLoc());\n      }\n    }\n\n    // If Src is a fixed vector and Dst is a scalable vector, and both have the\n    // same element type, use the llvm.experimental.vector.insert intrinsic to\n    // perform the bitcast.\n    if (const auto *FixedSrc = dyn_cast<llvm::FixedVectorType>(SrcTy)) {\n      if (const auto *ScalableDst = dyn_cast<llvm::ScalableVectorType>(DstTy)) {\n        if (FixedSrc->getElementType() == ScalableDst->getElementType()) {\n          llvm::Value *UndefVec = llvm::UndefValue::get(DstTy);\n          llvm::Value *Zero = llvm::Constant::getNullValue(CGF.CGM.Int64Ty);\n          return Builder.CreateInsertVector(DstTy, UndefVec, Src, Zero,\n                                            \"castScalableSve\");\n        }\n      }\n    }\n\n    // If Src is a scalable vector and Dst is a fixed vector, and both have the\n    // same element type, use the llvm.experimental.vector.extract intrinsic to\n    // perform the bitcast.\n    if (const auto *ScalableSrc = dyn_cast<llvm::ScalableVectorType>(SrcTy)) {\n      if (const auto *FixedDst = dyn_cast<llvm::FixedVectorType>(DstTy)) {\n        if (ScalableSrc->getElementType() == FixedDst->getElementType()) {\n          llvm::Value *Zero = llvm::Constant::getNullValue(CGF.CGM.Int64Ty);\n          return Builder.CreateExtractVector(DstTy, Src, Zero, \"castFixedSve\");\n        }\n      }\n    }\n\n    // Perform VLAT <-> VLST bitcast through memory.\n    // TODO: since the llvm.experimental.vector.{insert,extract} intrinsics\n    //       require the element types of the vectors to be the same, we\n    //       need to keep this around for casting between predicates, or more\n    //       generally for bitcasts between VLAT <-> VLST where the element\n    //       types of the vectors are not the same, until we figure out a better\n    //       way of doing these casts.\n    if ((isa<llvm::FixedVectorType>(SrcTy) &&\n         isa<llvm::ScalableVectorType>(DstTy)) ||\n        (isa<llvm::ScalableVectorType>(SrcTy) &&\n         isa<llvm::FixedVectorType>(DstTy))) {\n      if (const CallExpr *CE = dyn_cast<CallExpr>(E)) {\n        // Call expressions can't have a scalar return unless the return type\n        // is a reference type so an lvalue can't be emitted. Create a temp\n        // alloca to store the call, bitcast the address then load.\n        QualType RetTy = CE->getCallReturnType(CGF.getContext());\n        Address Addr =\n            CGF.CreateDefaultAlignTempAlloca(SrcTy, \"saved-call-rvalue\");\n        LValue LV = CGF.MakeAddrLValue(Addr, RetTy);\n        CGF.EmitStoreOfScalar(Src, LV);\n        Addr = Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(DestTy),\n                                            \"castFixedSve\");\n        LValue DestLV = CGF.MakeAddrLValue(Addr, DestTy);\n        DestLV.setTBAAInfo(TBAAAccessInfo::getMayAliasInfo());\n        return EmitLoadOfLValue(DestLV, CE->getExprLoc());\n      }\n\n      Address Addr = EmitLValue(E).getAddress(CGF);\n      Addr = Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(DestTy));\n      LValue DestLV = CGF.MakeAddrLValue(Addr, DestTy);\n      DestLV.setTBAAInfo(TBAAAccessInfo::getMayAliasInfo());\n      return EmitLoadOfLValue(DestLV, CE->getExprLoc());\n    }\n\n    return Builder.CreateBitCast(Src, DstTy);\n  }\n  case CK_AddressSpaceConversion: {\n    Expr::EvalResult Result;\n    if (E->EvaluateAsRValue(Result, CGF.getContext()) &&\n        Result.Val.isNullPointer()) {\n      // If E has side effect, it is emitted even if its final result is a\n      // null pointer. In that case, a DCE pass should be able to\n      // eliminate the useless instructions emitted during translating E.\n      if (Result.HasSideEffects)\n        Visit(E);\n      return CGF.CGM.getNullPointer(cast<llvm::PointerType>(\n          ConvertType(DestTy)), DestTy);\n    }\n    // Since target may map different address spaces in AST to the same address\n    // space, an address space conversion may end up as a bitcast.\n    return CGF.CGM.getTargetCodeGenInfo().performAddrSpaceCast(\n        CGF, Visit(E), E->getType()->getPointeeType().getAddressSpace(),\n        DestTy->getPointeeType().getAddressSpace(), ConvertType(DestTy));\n  }\n  case CK_AtomicToNonAtomic:\n  case CK_NonAtomicToAtomic:\n  case CK_NoOp:\n  case CK_UserDefinedConversion:\n    return Visit(const_cast<Expr*>(E));\n\n  case CK_BaseToDerived: {\n    const CXXRecordDecl *DerivedClassDecl = DestTy->getPointeeCXXRecordDecl();\n    assert(DerivedClassDecl && \"BaseToDerived arg isn't a C++ object pointer!\");\n\n    Address Base = CGF.EmitPointerWithAlignment(E);\n    Address Derived =\n      CGF.GetAddressOfDerivedClass(Base, DerivedClassDecl,\n                                   CE->path_begin(), CE->path_end(),\n                                   CGF.ShouldNullCheckClassCastValue(CE));\n\n    // C++11 [expr.static.cast]p11: Behavior is undefined if a downcast is\n    // performed and the object is not of the derived type.\n    if (CGF.sanitizePerformTypeCheck())\n      CGF.EmitTypeCheck(CodeGenFunction::TCK_DowncastPointer, CE->getExprLoc(),\n                        Derived.getPointer(), DestTy->getPointeeType());\n\n    if (CGF.SanOpts.has(SanitizerKind::CFIDerivedCast))\n      CGF.EmitVTablePtrCheckForCast(\n          DestTy->getPointeeType(), Derived.getPointer(),\n          /*MayBeNull=*/true, CodeGenFunction::CFITCK_DerivedCast,\n          CE->getBeginLoc());\n\n    return Derived.getPointer();\n  }\n  case CK_UncheckedDerivedToBase:\n  case CK_DerivedToBase: {\n    // The EmitPointerWithAlignment path does this fine; just discard\n    // the alignment.\n    return CGF.EmitPointerWithAlignment(CE).getPointer();\n  }\n\n  case CK_Dynamic: {\n    Address V = CGF.EmitPointerWithAlignment(E);\n    const CXXDynamicCastExpr *DCE = cast<CXXDynamicCastExpr>(CE);\n    return CGF.EmitDynamicCast(V, DCE);\n  }\n\n  case CK_ArrayToPointerDecay:\n    return CGF.EmitArrayToPointerDecay(E).getPointer();\n  case CK_FunctionToPointerDecay:\n    return EmitLValue(E).getPointer(CGF);\n\n  case CK_NullToPointer:\n    if (MustVisitNullValue(E))\n      CGF.EmitIgnoredExpr(E);\n\n    return CGF.CGM.getNullPointer(cast<llvm::PointerType>(ConvertType(DestTy)),\n                              DestTy);\n\n  case CK_NullToMemberPointer: {\n    if (MustVisitNullValue(E))\n      CGF.EmitIgnoredExpr(E);\n\n    const MemberPointerType *MPT = CE->getType()->getAs<MemberPointerType>();\n    return CGF.CGM.getCXXABI().EmitNullMemberPointer(MPT);\n  }\n\n  case CK_ReinterpretMemberPointer:\n  case CK_BaseToDerivedMemberPointer:\n  case CK_DerivedToBaseMemberPointer: {\n    Value *Src = Visit(E);\n\n    // Note that the AST doesn't distinguish between checked and\n    // unchecked member pointer conversions, so we always have to\n    // implement checked conversions here.  This is inefficient when\n    // actual control flow may be required in order to perform the\n    // check, which it is for data member pointers (but not member\n    // function pointers on Itanium and ARM).\n    return CGF.CGM.getCXXABI().EmitMemberPointerConversion(CGF, CE, Src);\n  }\n\n  case CK_ARCProduceObject:\n    return CGF.EmitARCRetainScalarExpr(E);\n  case CK_ARCConsumeObject:\n    return CGF.EmitObjCConsumeObject(E->getType(), Visit(E));\n  case CK_ARCReclaimReturnedObject:\n    return CGF.EmitARCReclaimReturnedObject(E, /*allowUnsafe*/ Ignored);\n  case CK_ARCExtendBlockObject:\n    return CGF.EmitARCExtendBlockObject(E);\n\n  case CK_CopyAndAutoreleaseBlockObject:\n    return CGF.EmitBlockCopyAndAutorelease(Visit(E), E->getType());\n\n  case CK_FloatingRealToComplex:\n  case CK_FloatingComplexCast:\n  case CK_IntegralRealToComplex:\n  case CK_IntegralComplexCast:\n  case CK_IntegralComplexToFloatingComplex:\n  case CK_FloatingComplexToIntegralComplex:\n  case CK_ConstructorConversion:\n  case CK_ToUnion:\n    llvm_unreachable(\"scalar cast to non-scalar value\");\n\n  case CK_LValueToRValue:\n    assert(CGF.getContext().hasSameUnqualifiedType(E->getType(), DestTy));\n    assert(E->isGLValue() && \"lvalue-to-rvalue applied to r-value!\");\n    return Visit(const_cast<Expr*>(E));\n\n  case CK_IntegralToPointer: {\n    Value *Src = Visit(const_cast<Expr*>(E));\n\n    // First, convert to the correct width so that we control the kind of\n    // extension.\n    auto DestLLVMTy = ConvertType(DestTy);\n    llvm::Type *MiddleTy = CGF.CGM.getDataLayout().getIntPtrType(DestLLVMTy);\n    bool InputSigned = E->getType()->isSignedIntegerOrEnumerationType();\n    llvm::Value* IntResult =\n      Builder.CreateIntCast(Src, MiddleTy, InputSigned, \"conv\");\n\n    auto *IntToPtr = Builder.CreateIntToPtr(IntResult, DestLLVMTy);\n\n    if (CGF.CGM.getCodeGenOpts().StrictVTablePointers) {\n      // Going from integer to pointer that could be dynamic requires reloading\n      // dynamic information from invariant.group.\n      if (DestTy.mayBeDynamicClass())\n        IntToPtr = Builder.CreateLaunderInvariantGroup(IntToPtr);\n    }\n    return IntToPtr;\n  }\n  case CK_PointerToIntegral: {\n    assert(!DestTy->isBooleanType() && \"bool should use PointerToBool\");\n    auto *PtrExpr = Visit(E);\n\n    if (CGF.CGM.getCodeGenOpts().StrictVTablePointers) {\n      const QualType SrcType = E->getType();\n\n      // Casting to integer requires stripping dynamic information as it does\n      // not carries it.\n      if (SrcType.mayBeDynamicClass())\n        PtrExpr = Builder.CreateStripInvariantGroup(PtrExpr);\n    }\n\n    return Builder.CreatePtrToInt(PtrExpr, ConvertType(DestTy));\n  }\n  case CK_ToVoid: {\n    CGF.EmitIgnoredExpr(E);\n    return nullptr;\n  }\n  case CK_VectorSplat: {\n    llvm::Type *DstTy = ConvertType(DestTy);\n    Value *Elt = Visit(const_cast<Expr*>(E));\n    // Splat the element across to all elements\n    unsigned NumElements = cast<llvm::FixedVectorType>(DstTy)->getNumElements();\n    return Builder.CreateVectorSplat(NumElements, Elt, \"splat\");\n  }\n\n  case CK_FixedPointCast:\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_FixedPointToBoolean:\n    assert(E->getType()->isFixedPointType() &&\n           \"Expected src type to be fixed point type\");\n    assert(DestTy->isBooleanType() && \"Expected dest type to be boolean type\");\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_FixedPointToIntegral:\n    assert(E->getType()->isFixedPointType() &&\n           \"Expected src type to be fixed point type\");\n    assert(DestTy->isIntegerType() && \"Expected dest type to be an integer\");\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_IntegralToFixedPoint:\n    assert(E->getType()->isIntegerType() &&\n           \"Expected src type to be an integer\");\n    assert(DestTy->isFixedPointType() &&\n           \"Expected dest type to be fixed point type\");\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n\n  case CK_IntegralCast: {\n    ScalarConversionOpts Opts;\n    if (auto *ICE = dyn_cast<ImplicitCastExpr>(CE)) {\n      if (!ICE->isPartOfExplicitCast())\n        Opts = ScalarConversionOpts(CGF.SanOpts);\n    }\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc(), Opts);\n  }\n  case CK_IntegralToFloating:\n  case CK_FloatingToIntegral:\n  case CK_FloatingCast:\n  case CK_FixedPointToFloating:\n  case CK_FloatingToFixedPoint: {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, CE);\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc());\n  }\n  case CK_BooleanToSignedIntegral: {\n    ScalarConversionOpts Opts;\n    Opts.TreatBooleanAsSigned = true;\n    return EmitScalarConversion(Visit(E), E->getType(), DestTy,\n                                CE->getExprLoc(), Opts);\n  }\n  case CK_IntegralToBoolean:\n    return EmitIntToBoolConversion(Visit(E));\n  case CK_PointerToBoolean:\n    return EmitPointerToBoolConversion(Visit(E), E->getType());\n  case CK_FloatingToBoolean: {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, CE);\n    return EmitFloatToBoolConversion(Visit(E));\n  }\n  case CK_MemberPointerToBoolean: {\n    llvm::Value *MemPtr = Visit(E);\n    const MemberPointerType *MPT = E->getType()->getAs<MemberPointerType>();\n    return CGF.CGM.getCXXABI().EmitMemberPointerIsNotNull(CGF, MemPtr, MPT);\n  }\n\n  case CK_FloatingComplexToReal:\n  case CK_IntegralComplexToReal:\n    return CGF.EmitComplexExpr(E, false, true).first;\n\n  case CK_FloatingComplexToBoolean:\n  case CK_IntegralComplexToBoolean: {\n    CodeGenFunction::ComplexPairTy V = CGF.EmitComplexExpr(E);\n\n    // TODO: kill this function off, inline appropriate case here\n    return EmitComplexToScalarConversion(V, E->getType(), DestTy,\n                                         CE->getExprLoc());\n  }\n\n  case CK_ZeroToOCLOpaqueType: {\n    assert((DestTy->isEventT() || DestTy->isQueueT() ||\n            DestTy->isOCLIntelSubgroupAVCType()) &&\n           \"CK_ZeroToOCLEvent cast on non-event type\");\n    return llvm::Constant::getNullValue(ConvertType(DestTy));\n  }\n\n  case CK_IntToOCLSampler:\n    return CGF.CGM.createOpenCLIntToSamplerConversion(E, CGF);\n\n  } // end of switch\n\n  llvm_unreachable(\"unknown scalar cast\");\n}\n\nValue *ScalarExprEmitter::VisitStmtExpr(const StmtExpr *E) {\n  CodeGenFunction::StmtExprEvaluation eval(CGF);\n  Address RetAlloca = CGF.EmitCompoundStmt(*E->getSubStmt(),\n                                           !E->getType()->isVoidType());\n  if (!RetAlloca.isValid())\n    return nullptr;\n  return CGF.EmitLoadOfScalar(CGF.MakeAddrLValue(RetAlloca, E->getType()),\n                              E->getExprLoc());\n}\n\nValue *ScalarExprEmitter::VisitExprWithCleanups(ExprWithCleanups *E) {\n  CodeGenFunction::RunCleanupsScope Scope(CGF);\n  Value *V = Visit(E->getSubExpr());\n  // Defend against dominance problems caused by jumps out of expression\n  // evaluation through the shared cleanup block.\n  Scope.ForceCleanup({&V});\n  return V;\n}\n\n//===----------------------------------------------------------------------===//\n//                             Unary Operators\n//===----------------------------------------------------------------------===//\n\nstatic BinOpInfo createBinOpInfoFromIncDec(const UnaryOperator *E,\n                                           llvm::Value *InVal, bool IsInc,\n                                           FPOptions FPFeatures) {\n  BinOpInfo BinOp;\n  BinOp.LHS = InVal;\n  BinOp.RHS = llvm::ConstantInt::get(InVal->getType(), 1, false);\n  BinOp.Ty = E->getType();\n  BinOp.Opcode = IsInc ? BO_Add : BO_Sub;\n  BinOp.FPFeatures = FPFeatures;\n  BinOp.E = E;\n  return BinOp;\n}\n\nllvm::Value *ScalarExprEmitter::EmitIncDecConsiderOverflowBehavior(\n    const UnaryOperator *E, llvm::Value *InVal, bool IsInc) {\n  llvm::Value *Amount =\n      llvm::ConstantInt::get(InVal->getType(), IsInc ? 1 : -1, true);\n  StringRef Name = IsInc ? \"inc\" : \"dec\";\n  switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n  case LangOptions::SOB_Defined:\n    return Builder.CreateAdd(InVal, Amount, Name);\n  case LangOptions::SOB_Undefined:\n    if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n      return Builder.CreateNSWAdd(InVal, Amount, Name);\n    LLVM_FALLTHROUGH;\n  case LangOptions::SOB_Trapping:\n    if (!E->canOverflow())\n      return Builder.CreateNSWAdd(InVal, Amount, Name);\n    return EmitOverflowCheckedBinOp(createBinOpInfoFromIncDec(\n        E, InVal, IsInc, E->getFPFeaturesInEffect(CGF.getLangOpts())));\n  }\n  llvm_unreachable(\"Unknown SignedOverflowBehaviorTy\");\n}\n\nnamespace {\n/// Handles check and update for lastprivate conditional variables.\nclass OMPLastprivateConditionalUpdateRAII {\nprivate:\n  CodeGenFunction &CGF;\n  const UnaryOperator *E;\n\npublic:\n  OMPLastprivateConditionalUpdateRAII(CodeGenFunction &CGF,\n                                      const UnaryOperator *E)\n      : CGF(CGF), E(E) {}\n  ~OMPLastprivateConditionalUpdateRAII() {\n    if (CGF.getLangOpts().OpenMP)\n      CGF.CGM.getOpenMPRuntime().checkAndEmitLastprivateConditional(\n          CGF, E->getSubExpr());\n  }\n};\n} // namespace\n\nllvm::Value *\nScalarExprEmitter::EmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                           bool isInc, bool isPre) {\n  OMPLastprivateConditionalUpdateRAII OMPRegion(CGF, E);\n  QualType type = E->getSubExpr()->getType();\n  llvm::PHINode *atomicPHI = nullptr;\n  llvm::Value *value;\n  llvm::Value *input;\n\n  int amount = (isInc ? 1 : -1);\n  bool isSubtraction = !isInc;\n\n  if (const AtomicType *atomicTy = type->getAs<AtomicType>()) {\n    type = atomicTy->getValueType();\n    if (isInc && type->isBooleanType()) {\n      llvm::Value *True = CGF.EmitToMemory(Builder.getTrue(), type);\n      if (isPre) {\n        Builder.CreateStore(True, LV.getAddress(CGF), LV.isVolatileQualified())\n            ->setAtomic(llvm::AtomicOrdering::SequentiallyConsistent);\n        return Builder.getTrue();\n      }\n      // For atomic bool increment, we just store true and return it for\n      // preincrement, do an atomic swap with true for postincrement\n      return Builder.CreateAtomicRMW(\n          llvm::AtomicRMWInst::Xchg, LV.getPointer(CGF), True,\n          llvm::AtomicOrdering::SequentiallyConsistent);\n    }\n    // Special case for atomic increment / decrement on integers, emit\n    // atomicrmw instructions.  We skip this if we want to be doing overflow\n    // checking, and fall into the slow path with the atomic cmpxchg loop.\n    if (!type->isBooleanType() && type->isIntegerType() &&\n        !(type->isUnsignedIntegerType() &&\n          CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow)) &&\n        CGF.getLangOpts().getSignedOverflowBehavior() !=\n            LangOptions::SOB_Trapping) {\n      llvm::AtomicRMWInst::BinOp aop = isInc ? llvm::AtomicRMWInst::Add :\n        llvm::AtomicRMWInst::Sub;\n      llvm::Instruction::BinaryOps op = isInc ? llvm::Instruction::Add :\n        llvm::Instruction::Sub;\n      llvm::Value *amt = CGF.EmitToMemory(\n          llvm::ConstantInt::get(ConvertType(type), 1, true), type);\n      llvm::Value *old =\n          Builder.CreateAtomicRMW(aop, LV.getPointer(CGF), amt,\n                                  llvm::AtomicOrdering::SequentiallyConsistent);\n      return isPre ? Builder.CreateBinOp(op, old, amt) : old;\n    }\n    value = EmitLoadOfLValue(LV, E->getExprLoc());\n    input = value;\n    // For every other atomic operation, we need to emit a load-op-cmpxchg loop\n    llvm::BasicBlock *startBB = Builder.GetInsertBlock();\n    llvm::BasicBlock *opBB = CGF.createBasicBlock(\"atomic_op\", CGF.CurFn);\n    value = CGF.EmitToMemory(value, type);\n    Builder.CreateBr(opBB);\n    Builder.SetInsertPoint(opBB);\n    atomicPHI = Builder.CreatePHI(value->getType(), 2);\n    atomicPHI->addIncoming(value, startBB);\n    value = atomicPHI;\n  } else {\n    value = EmitLoadOfLValue(LV, E->getExprLoc());\n    input = value;\n  }\n\n  // Special case of integer increment that we have to check first: bool++.\n  // Due to promotion rules, we get:\n  //   bool++ -> bool = bool + 1\n  //          -> bool = (int)bool + 1\n  //          -> bool = ((int)bool + 1 != 0)\n  // An interesting aspect of this is that increment is always true.\n  // Decrement does not have this property.\n  if (isInc && type->isBooleanType()) {\n    value = Builder.getTrue();\n\n  // Most common case by far: integer increment.\n  } else if (type->isIntegerType()) {\n    QualType promotedType;\n    bool canPerformLossyDemotionCheck = false;\n    if (type->isPromotableIntegerType()) {\n      promotedType = CGF.getContext().getPromotedIntegerType(type);\n      assert(promotedType != type && \"Shouldn't promote to the same type.\");\n      canPerformLossyDemotionCheck = true;\n      canPerformLossyDemotionCheck &=\n          CGF.getContext().getCanonicalType(type) !=\n          CGF.getContext().getCanonicalType(promotedType);\n      canPerformLossyDemotionCheck &=\n          PromotionIsPotentiallyEligibleForImplicitIntegerConversionCheck(\n              type, promotedType);\n      assert((!canPerformLossyDemotionCheck ||\n              type->isSignedIntegerOrEnumerationType() ||\n              promotedType->isSignedIntegerOrEnumerationType() ||\n              ConvertType(type)->getScalarSizeInBits() ==\n                  ConvertType(promotedType)->getScalarSizeInBits()) &&\n             \"The following check expects that if we do promotion to different \"\n             \"underlying canonical type, at least one of the types (either \"\n             \"base or promoted) will be signed, or the bitwidths will match.\");\n    }\n    if (CGF.SanOpts.hasOneOf(\n            SanitizerKind::ImplicitIntegerArithmeticValueChange) &&\n        canPerformLossyDemotionCheck) {\n      // While `x += 1` (for `x` with width less than int) is modeled as\n      // promotion+arithmetics+demotion, and we can catch lossy demotion with\n      // ease; inc/dec with width less than int can't overflow because of\n      // promotion rules, so we omit promotion+demotion, which means that we can\n      // not catch lossy \"demotion\". Because we still want to catch these cases\n      // when the sanitizer is enabled, we perform the promotion, then perform\n      // the increment/decrement in the wider type, and finally\n      // perform the demotion. This will catch lossy demotions.\n\n      value = EmitScalarConversion(value, type, promotedType, E->getExprLoc());\n      Value *amt = llvm::ConstantInt::get(value->getType(), amount, true);\n      value = Builder.CreateAdd(value, amt, isInc ? \"inc\" : \"dec\");\n      // Do pass non-default ScalarConversionOpts so that sanitizer check is\n      // emitted.\n      value = EmitScalarConversion(value, promotedType, type, E->getExprLoc(),\n                                   ScalarConversionOpts(CGF.SanOpts));\n\n      // Note that signed integer inc/dec with width less than int can't\n      // overflow because of promotion rules; we're just eliding a few steps\n      // here.\n    } else if (E->canOverflow() && type->isSignedIntegerOrEnumerationType()) {\n      value = EmitIncDecConsiderOverflowBehavior(E, value, isInc);\n    } else if (E->canOverflow() && type->isUnsignedIntegerType() &&\n               CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow)) {\n      value = EmitOverflowCheckedBinOp(createBinOpInfoFromIncDec(\n          E, value, isInc, E->getFPFeaturesInEffect(CGF.getLangOpts())));\n    } else {\n      llvm::Value *amt = llvm::ConstantInt::get(value->getType(), amount, true);\n      value = Builder.CreateAdd(value, amt, isInc ? \"inc\" : \"dec\");\n    }\n\n  // Next most common: pointer increment.\n  } else if (const PointerType *ptr = type->getAs<PointerType>()) {\n    QualType type = ptr->getPointeeType();\n\n    // VLA types don't have constant size.\n    if (const VariableArrayType *vla\n          = CGF.getContext().getAsVariableArrayType(type)) {\n      llvm::Value *numElts = CGF.getVLASize(vla).NumElts;\n      if (!isInc) numElts = Builder.CreateNSWNeg(numElts, \"vla.negsize\");\n      if (CGF.getLangOpts().isSignedOverflowDefined())\n        value = Builder.CreateGEP(value, numElts, \"vla.inc\");\n      else\n        value = CGF.EmitCheckedInBoundsGEP(\n            value, numElts, /*SignedIndices=*/false, isSubtraction,\n            E->getExprLoc(), \"vla.inc\");\n\n    // Arithmetic on function pointers (!) is just +-1.\n    } else if (type->isFunctionType()) {\n      llvm::Value *amt = Builder.getInt32(amount);\n\n      value = CGF.EmitCastToVoidPtr(value);\n      if (CGF.getLangOpts().isSignedOverflowDefined())\n        value = Builder.CreateGEP(value, amt, \"incdec.funcptr\");\n      else\n        value = CGF.EmitCheckedInBoundsGEP(value, amt, /*SignedIndices=*/false,\n                                           isSubtraction, E->getExprLoc(),\n                                           \"incdec.funcptr\");\n      value = Builder.CreateBitCast(value, input->getType());\n\n    // For everything else, we can just do a simple increment.\n    } else {\n      llvm::Value *amt = Builder.getInt32(amount);\n      if (CGF.getLangOpts().isSignedOverflowDefined())\n        value = Builder.CreateGEP(value, amt, \"incdec.ptr\");\n      else\n        value = CGF.EmitCheckedInBoundsGEP(value, amt, /*SignedIndices=*/false,\n                                           isSubtraction, E->getExprLoc(),\n                                           \"incdec.ptr\");\n    }\n\n  // Vector increment/decrement.\n  } else if (type->isVectorType()) {\n    if (type->hasIntegerRepresentation()) {\n      llvm::Value *amt = llvm::ConstantInt::get(value->getType(), amount);\n\n      value = Builder.CreateAdd(value, amt, isInc ? \"inc\" : \"dec\");\n    } else {\n      value = Builder.CreateFAdd(\n                  value,\n                  llvm::ConstantFP::get(value->getType(), amount),\n                  isInc ? \"inc\" : \"dec\");\n    }\n\n  // Floating point.\n  } else if (type->isRealFloatingType()) {\n    // Add the inc/dec to the real part.\n    llvm::Value *amt;\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, E);\n\n    if (type->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n      // Another special case: half FP increment should be done via float\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n        value = Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_from_fp16,\n                                 CGF.CGM.FloatTy),\n            input, \"incdec.conv\");\n      } else {\n        value = Builder.CreateFPExt(input, CGF.CGM.FloatTy, \"incdec.conv\");\n      }\n    }\n\n    if (value->getType()->isFloatTy())\n      amt = llvm::ConstantFP::get(VMContext,\n                                  llvm::APFloat(static_cast<float>(amount)));\n    else if (value->getType()->isDoubleTy())\n      amt = llvm::ConstantFP::get(VMContext,\n                                  llvm::APFloat(static_cast<double>(amount)));\n    else {\n      // Remaining types are Half, LongDouble or __float128. Convert from float.\n      llvm::APFloat F(static_cast<float>(amount));\n      bool ignored;\n      const llvm::fltSemantics *FS;\n      // Don't use getFloatTypeSemantics because Half isn't\n      // necessarily represented using the \"half\" LLVM type.\n      if (value->getType()->isFP128Ty())\n        FS = &CGF.getTarget().getFloat128Format();\n      else if (value->getType()->isHalfTy())\n        FS = &CGF.getTarget().getHalfFormat();\n      else\n        FS = &CGF.getTarget().getLongDoubleFormat();\n      F.convert(*FS, llvm::APFloat::rmTowardZero, &ignored);\n      amt = llvm::ConstantFP::get(VMContext, F);\n    }\n    value = Builder.CreateFAdd(value, amt, isInc ? \"inc\" : \"dec\");\n\n    if (type->isHalfType() && !CGF.getContext().getLangOpts().NativeHalfType) {\n      if (CGF.getContext().getTargetInfo().useFP16ConversionIntrinsics()) {\n        value = Builder.CreateCall(\n            CGF.CGM.getIntrinsic(llvm::Intrinsic::convert_to_fp16,\n                                 CGF.CGM.FloatTy),\n            value, \"incdec.conv\");\n      } else {\n        value = Builder.CreateFPTrunc(value, input->getType(), \"incdec.conv\");\n      }\n    }\n\n  // Fixed-point types.\n  } else if (type->isFixedPointType()) {\n    // Fixed-point types are tricky. In some cases, it isn't possible to\n    // represent a 1 or a -1 in the type at all. Piggyback off of\n    // EmitFixedPointBinOp to avoid having to reimplement saturation.\n    BinOpInfo Info;\n    Info.E = E;\n    Info.Ty = E->getType();\n    Info.Opcode = isInc ? BO_Add : BO_Sub;\n    Info.LHS = value;\n    Info.RHS = llvm::ConstantInt::get(value->getType(), 1, false);\n    // If the type is signed, it's better to represent this as +(-1) or -(-1),\n    // since -1 is guaranteed to be representable.\n    if (type->isSignedFixedPointType()) {\n      Info.Opcode = isInc ? BO_Sub : BO_Add;\n      Info.RHS = Builder.CreateNeg(Info.RHS);\n    }\n    // Now, convert from our invented integer literal to the type of the unary\n    // op. This will upscale and saturate if necessary. This value can become\n    // undef in some cases.\n    llvm::FixedPointBuilder<CGBuilderTy> FPBuilder(Builder);\n    auto DstSema = CGF.getContext().getFixedPointSemantics(Info.Ty);\n    Info.RHS = FPBuilder.CreateIntegerToFixed(Info.RHS, true, DstSema);\n    value = EmitFixedPointBinOp(Info);\n\n  // Objective-C pointer types.\n  } else {\n    const ObjCObjectPointerType *OPT = type->castAs<ObjCObjectPointerType>();\n    value = CGF.EmitCastToVoidPtr(value);\n\n    CharUnits size = CGF.getContext().getTypeSizeInChars(OPT->getObjectType());\n    if (!isInc) size = -size;\n    llvm::Value *sizeValue =\n      llvm::ConstantInt::get(CGF.SizeTy, size.getQuantity());\n\n    if (CGF.getLangOpts().isSignedOverflowDefined())\n      value = Builder.CreateGEP(value, sizeValue, \"incdec.objptr\");\n    else\n      value = CGF.EmitCheckedInBoundsGEP(value, sizeValue,\n                                         /*SignedIndices=*/false, isSubtraction,\n                                         E->getExprLoc(), \"incdec.objptr\");\n    value = Builder.CreateBitCast(value, input->getType());\n  }\n\n  if (atomicPHI) {\n    llvm::BasicBlock *curBlock = Builder.GetInsertBlock();\n    llvm::BasicBlock *contBB = CGF.createBasicBlock(\"atomic_cont\", CGF.CurFn);\n    auto Pair = CGF.EmitAtomicCompareExchange(\n        LV, RValue::get(atomicPHI), RValue::get(value), E->getExprLoc());\n    llvm::Value *old = CGF.EmitToMemory(Pair.first.getScalarVal(), type);\n    llvm::Value *success = Pair.second;\n    atomicPHI->addIncoming(old, curBlock);\n    Builder.CreateCondBr(success, contBB, atomicPHI->getParent());\n    Builder.SetInsertPoint(contBB);\n    return isPre ? value : input;\n  }\n\n  // Store the updated result through the lvalue.\n  if (LV.isBitField())\n    CGF.EmitStoreThroughBitfieldLValue(RValue::get(value), LV, &value);\n  else\n    CGF.EmitStoreThroughLValue(RValue::get(value), LV);\n\n  // If this is a postinc, return the value read from memory, otherwise use the\n  // updated value.\n  return isPre ? value : input;\n}\n\n\n\nValue *ScalarExprEmitter::VisitUnaryMinus(const UnaryOperator *E) {\n  TestAndClearIgnoreResultAssign();\n  Value *Op = Visit(E->getSubExpr());\n\n  // Generate a unary FNeg for FP ops.\n  if (Op->getType()->isFPOrFPVectorTy())\n    return Builder.CreateFNeg(Op, \"fneg\");\n\n  // Emit unary minus with EmitSub so we handle overflow cases etc.\n  BinOpInfo BinOp;\n  BinOp.RHS = Op;\n  BinOp.LHS = llvm::Constant::getNullValue(BinOp.RHS->getType());\n  BinOp.Ty = E->getType();\n  BinOp.Opcode = BO_Sub;\n  BinOp.FPFeatures = E->getFPFeaturesInEffect(CGF.getLangOpts());\n  BinOp.E = E;\n  return EmitSub(BinOp);\n}\n\nValue *ScalarExprEmitter::VisitUnaryNot(const UnaryOperator *E) {\n  TestAndClearIgnoreResultAssign();\n  Value *Op = Visit(E->getSubExpr());\n  return Builder.CreateNot(Op, \"neg\");\n}\n\nValue *ScalarExprEmitter::VisitUnaryLNot(const UnaryOperator *E) {\n  // Perform vector logical not on comparison with zero vector.\n  if (E->getType()->isVectorType() &&\n      E->getType()->castAs<VectorType>()->getVectorKind() ==\n          VectorType::GenericVector) {\n    Value *Oper = Visit(E->getSubExpr());\n    Value *Zero = llvm::Constant::getNullValue(Oper->getType());\n    Value *Result;\n    if (Oper->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(\n          CGF, E->getFPFeaturesInEffect(CGF.getLangOpts()));\n      Result = Builder.CreateFCmp(llvm::CmpInst::FCMP_OEQ, Oper, Zero, \"cmp\");\n    } else\n      Result = Builder.CreateICmp(llvm::CmpInst::ICMP_EQ, Oper, Zero, \"cmp\");\n    return Builder.CreateSExt(Result, ConvertType(E->getType()), \"sext\");\n  }\n\n  // Compare operand to zero.\n  Value *BoolVal = CGF.EvaluateExprAsBool(E->getSubExpr());\n\n  // Invert value.\n  // TODO: Could dynamically modify easy computations here.  For example, if\n  // the operand is an icmp ne, turn into icmp eq.\n  BoolVal = Builder.CreateNot(BoolVal, \"lnot\");\n\n  // ZExt result to the expr type.\n  return Builder.CreateZExt(BoolVal, ConvertType(E->getType()), \"lnot.ext\");\n}\n\nValue *ScalarExprEmitter::VisitOffsetOfExpr(OffsetOfExpr *E) {\n  // Try folding the offsetof to a constant.\n  Expr::EvalResult EVResult;\n  if (E->EvaluateAsInt(EVResult, CGF.getContext())) {\n    llvm::APSInt Value = EVResult.Val.getInt();\n    return Builder.getInt(Value);\n  }\n\n  // Loop over the components of the offsetof to compute the value.\n  unsigned n = E->getNumComponents();\n  llvm::Type* ResultType = ConvertType(E->getType());\n  llvm::Value* Result = llvm::Constant::getNullValue(ResultType);\n  QualType CurrentType = E->getTypeSourceInfo()->getType();\n  for (unsigned i = 0; i != n; ++i) {\n    OffsetOfNode ON = E->getComponent(i);\n    llvm::Value *Offset = nullptr;\n    switch (ON.getKind()) {\n    case OffsetOfNode::Array: {\n      // Compute the index\n      Expr *IdxExpr = E->getIndexExpr(ON.getArrayExprIndex());\n      llvm::Value* Idx = CGF.EmitScalarExpr(IdxExpr);\n      bool IdxSigned = IdxExpr->getType()->isSignedIntegerOrEnumerationType();\n      Idx = Builder.CreateIntCast(Idx, ResultType, IdxSigned, \"conv\");\n\n      // Save the element type\n      CurrentType =\n          CGF.getContext().getAsArrayType(CurrentType)->getElementType();\n\n      // Compute the element size\n      llvm::Value* ElemSize = llvm::ConstantInt::get(ResultType,\n          CGF.getContext().getTypeSizeInChars(CurrentType).getQuantity());\n\n      // Multiply out to compute the result\n      Offset = Builder.CreateMul(Idx, ElemSize);\n      break;\n    }\n\n    case OffsetOfNode::Field: {\n      FieldDecl *MemberDecl = ON.getField();\n      RecordDecl *RD = CurrentType->castAs<RecordType>()->getDecl();\n      const ASTRecordLayout &RL = CGF.getContext().getASTRecordLayout(RD);\n\n      // Compute the index of the field in its parent.\n      unsigned i = 0;\n      // FIXME: It would be nice if we didn't have to loop here!\n      for (RecordDecl::field_iterator Field = RD->field_begin(),\n                                      FieldEnd = RD->field_end();\n           Field != FieldEnd; ++Field, ++i) {\n        if (*Field == MemberDecl)\n          break;\n      }\n      assert(i < RL.getFieldCount() && \"offsetof field in wrong type\");\n\n      // Compute the offset to the field\n      int64_t OffsetInt = RL.getFieldOffset(i) /\n                          CGF.getContext().getCharWidth();\n      Offset = llvm::ConstantInt::get(ResultType, OffsetInt);\n\n      // Save the element type.\n      CurrentType = MemberDecl->getType();\n      break;\n    }\n\n    case OffsetOfNode::Identifier:\n      llvm_unreachable(\"dependent __builtin_offsetof\");\n\n    case OffsetOfNode::Base: {\n      if (ON.getBase()->isVirtual()) {\n        CGF.ErrorUnsupported(E, \"virtual base in offsetof\");\n        continue;\n      }\n\n      RecordDecl *RD = CurrentType->castAs<RecordType>()->getDecl();\n      const ASTRecordLayout &RL = CGF.getContext().getASTRecordLayout(RD);\n\n      // Save the element type.\n      CurrentType = ON.getBase()->getType();\n\n      // Compute the offset to the base.\n      const RecordType *BaseRT = CurrentType->getAs<RecordType>();\n      CXXRecordDecl *BaseRD = cast<CXXRecordDecl>(BaseRT->getDecl());\n      CharUnits OffsetInt = RL.getBaseClassOffset(BaseRD);\n      Offset = llvm::ConstantInt::get(ResultType, OffsetInt.getQuantity());\n      break;\n    }\n    }\n    Result = Builder.CreateAdd(Result, Offset);\n  }\n  return Result;\n}\n\n/// VisitUnaryExprOrTypeTraitExpr - Return the size or alignment of the type of\n/// argument of the sizeof expression as an integer.\nValue *\nScalarExprEmitter::VisitUnaryExprOrTypeTraitExpr(\n                              const UnaryExprOrTypeTraitExpr *E) {\n  QualType TypeToSize = E->getTypeOfArgument();\n  if (E->getKind() == UETT_SizeOf) {\n    if (const VariableArrayType *VAT =\n          CGF.getContext().getAsVariableArrayType(TypeToSize)) {\n      if (E->isArgumentType()) {\n        // sizeof(type) - make sure to emit the VLA size.\n        CGF.EmitVariablyModifiedType(TypeToSize);\n      } else {\n        // C99 6.5.3.4p2: If the argument is an expression of type\n        // VLA, it is evaluated.\n        CGF.EmitIgnoredExpr(E->getArgumentExpr());\n      }\n\n      auto VlaSize = CGF.getVLASize(VAT);\n      llvm::Value *size = VlaSize.NumElts;\n\n      // Scale the number of non-VLA elements by the non-VLA element size.\n      CharUnits eltSize = CGF.getContext().getTypeSizeInChars(VlaSize.Type);\n      if (!eltSize.isOne())\n        size = CGF.Builder.CreateNUWMul(CGF.CGM.getSize(eltSize), size);\n\n      return size;\n    }\n  } else if (E->getKind() == UETT_OpenMPRequiredSimdAlign) {\n    auto Alignment =\n        CGF.getContext()\n            .toCharUnitsFromBits(CGF.getContext().getOpenMPDefaultSimdAlign(\n                E->getTypeOfArgument()->getPointeeType()))\n            .getQuantity();\n    return llvm::ConstantInt::get(CGF.SizeTy, Alignment);\n  }\n\n  // If this isn't sizeof(vla), the result must be constant; use the constant\n  // folding logic so we don't have to duplicate it here.\n  return Builder.getInt(E->EvaluateKnownConstInt(CGF.getContext()));\n}\n\nValue *ScalarExprEmitter::VisitUnaryReal(const UnaryOperator *E) {\n  Expr *Op = E->getSubExpr();\n  if (Op->getType()->isAnyComplexType()) {\n    // If it's an l-value, load through the appropriate subobject l-value.\n    // Note that we have to ask E because Op might be an l-value that\n    // this won't work for, e.g. an Obj-C property.\n    if (E->isGLValue())\n      return CGF.EmitLoadOfLValue(CGF.EmitLValue(E),\n                                  E->getExprLoc()).getScalarVal();\n\n    // Otherwise, calculate and project.\n    return CGF.EmitComplexExpr(Op, false, true).first;\n  }\n\n  return Visit(Op);\n}\n\nValue *ScalarExprEmitter::VisitUnaryImag(const UnaryOperator *E) {\n  Expr *Op = E->getSubExpr();\n  if (Op->getType()->isAnyComplexType()) {\n    // If it's an l-value, load through the appropriate subobject l-value.\n    // Note that we have to ask E because Op might be an l-value that\n    // this won't work for, e.g. an Obj-C property.\n    if (Op->isGLValue())\n      return CGF.EmitLoadOfLValue(CGF.EmitLValue(E),\n                                  E->getExprLoc()).getScalarVal();\n\n    // Otherwise, calculate and project.\n    return CGF.EmitComplexExpr(Op, true, false).second;\n  }\n\n  // __imag on a scalar returns zero.  Emit the subexpr to ensure side\n  // effects are evaluated, but not the actual value.\n  if (Op->isGLValue())\n    CGF.EmitLValue(Op);\n  else\n    CGF.EmitScalarExpr(Op, true);\n  return llvm::Constant::getNullValue(ConvertType(E->getType()));\n}\n\n//===----------------------------------------------------------------------===//\n//                           Binary Operators\n//===----------------------------------------------------------------------===//\n\nBinOpInfo ScalarExprEmitter::EmitBinOps(const BinaryOperator *E) {\n  TestAndClearIgnoreResultAssign();\n  BinOpInfo Result;\n  Result.LHS = Visit(E->getLHS());\n  Result.RHS = Visit(E->getRHS());\n  Result.Ty  = E->getType();\n  Result.Opcode = E->getOpcode();\n  Result.FPFeatures = E->getFPFeaturesInEffect(CGF.getLangOpts());\n  Result.E = E;\n  return Result;\n}\n\nLValue ScalarExprEmitter::EmitCompoundAssignLValue(\n                                              const CompoundAssignOperator *E,\n                        Value *(ScalarExprEmitter::*Func)(const BinOpInfo &),\n                                                   Value *&Result) {\n  QualType LHSTy = E->getLHS()->getType();\n  BinOpInfo OpInfo;\n\n  if (E->getComputationResultType()->isAnyComplexType())\n    return CGF.EmitScalarCompoundAssignWithComplex(E, Result);\n\n  // Emit the RHS first.  __block variables need to have the rhs evaluated\n  // first, plus this should improve codegen a little.\n  OpInfo.RHS = Visit(E->getRHS());\n  OpInfo.Ty = E->getComputationResultType();\n  OpInfo.Opcode = E->getOpcode();\n  OpInfo.FPFeatures = E->getFPFeaturesInEffect(CGF.getLangOpts());\n  OpInfo.E = E;\n  // Load/convert the LHS.\n  LValue LHSLV = EmitCheckedLValue(E->getLHS(), CodeGenFunction::TCK_Store);\n\n  llvm::PHINode *atomicPHI = nullptr;\n  if (const AtomicType *atomicTy = LHSTy->getAs<AtomicType>()) {\n    QualType type = atomicTy->getValueType();\n    if (!type->isBooleanType() && type->isIntegerType() &&\n        !(type->isUnsignedIntegerType() &&\n          CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow)) &&\n        CGF.getLangOpts().getSignedOverflowBehavior() !=\n            LangOptions::SOB_Trapping) {\n      llvm::AtomicRMWInst::BinOp AtomicOp = llvm::AtomicRMWInst::BAD_BINOP;\n      llvm::Instruction::BinaryOps Op;\n      switch (OpInfo.Opcode) {\n        // We don't have atomicrmw operands for *, %, /, <<, >>\n        case BO_MulAssign: case BO_DivAssign:\n        case BO_RemAssign:\n        case BO_ShlAssign:\n        case BO_ShrAssign:\n          break;\n        case BO_AddAssign:\n          AtomicOp = llvm::AtomicRMWInst::Add;\n          Op = llvm::Instruction::Add;\n          break;\n        case BO_SubAssign:\n          AtomicOp = llvm::AtomicRMWInst::Sub;\n          Op = llvm::Instruction::Sub;\n          break;\n        case BO_AndAssign:\n          AtomicOp = llvm::AtomicRMWInst::And;\n          Op = llvm::Instruction::And;\n          break;\n        case BO_XorAssign:\n          AtomicOp = llvm::AtomicRMWInst::Xor;\n          Op = llvm::Instruction::Xor;\n          break;\n        case BO_OrAssign:\n          AtomicOp = llvm::AtomicRMWInst::Or;\n          Op = llvm::Instruction::Or;\n          break;\n        default:\n          llvm_unreachable(\"Invalid compound assignment type\");\n      }\n      if (AtomicOp != llvm::AtomicRMWInst::BAD_BINOP) {\n        llvm::Value *Amt = CGF.EmitToMemory(\n            EmitScalarConversion(OpInfo.RHS, E->getRHS()->getType(), LHSTy,\n                                 E->getExprLoc()),\n            LHSTy);\n        Value *OldVal = Builder.CreateAtomicRMW(\n            AtomicOp, LHSLV.getPointer(CGF), Amt,\n            llvm::AtomicOrdering::SequentiallyConsistent);\n\n        // Since operation is atomic, the result type is guaranteed to be the\n        // same as the input in LLVM terms.\n        Result = Builder.CreateBinOp(Op, OldVal, Amt);\n        return LHSLV;\n      }\n    }\n    // FIXME: For floating point types, we should be saving and restoring the\n    // floating point environment in the loop.\n    llvm::BasicBlock *startBB = Builder.GetInsertBlock();\n    llvm::BasicBlock *opBB = CGF.createBasicBlock(\"atomic_op\", CGF.CurFn);\n    OpInfo.LHS = EmitLoadOfLValue(LHSLV, E->getExprLoc());\n    OpInfo.LHS = CGF.EmitToMemory(OpInfo.LHS, type);\n    Builder.CreateBr(opBB);\n    Builder.SetInsertPoint(opBB);\n    atomicPHI = Builder.CreatePHI(OpInfo.LHS->getType(), 2);\n    atomicPHI->addIncoming(OpInfo.LHS, startBB);\n    OpInfo.LHS = atomicPHI;\n  }\n  else\n    OpInfo.LHS = EmitLoadOfLValue(LHSLV, E->getExprLoc());\n\n  CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, OpInfo.FPFeatures);\n  SourceLocation Loc = E->getExprLoc();\n  OpInfo.LHS =\n      EmitScalarConversion(OpInfo.LHS, LHSTy, E->getComputationLHSType(), Loc);\n\n  // Expand the binary operator.\n  Result = (this->*Func)(OpInfo);\n\n  // Convert the result back to the LHS type,\n  // potentially with Implicit Conversion sanitizer check.\n  Result = EmitScalarConversion(Result, E->getComputationResultType(), LHSTy,\n                                Loc, ScalarConversionOpts(CGF.SanOpts));\n\n  if (atomicPHI) {\n    llvm::BasicBlock *curBlock = Builder.GetInsertBlock();\n    llvm::BasicBlock *contBB = CGF.createBasicBlock(\"atomic_cont\", CGF.CurFn);\n    auto Pair = CGF.EmitAtomicCompareExchange(\n        LHSLV, RValue::get(atomicPHI), RValue::get(Result), E->getExprLoc());\n    llvm::Value *old = CGF.EmitToMemory(Pair.first.getScalarVal(), LHSTy);\n    llvm::Value *success = Pair.second;\n    atomicPHI->addIncoming(old, curBlock);\n    Builder.CreateCondBr(success, contBB, atomicPHI->getParent());\n    Builder.SetInsertPoint(contBB);\n    return LHSLV;\n  }\n\n  // Store the result value into the LHS lvalue. Bit-fields are handled\n  // specially because the result is altered by the store, i.e., [C99 6.5.16p1]\n  // 'An assignment expression has the value of the left operand after the\n  // assignment...'.\n  if (LHSLV.isBitField())\n    CGF.EmitStoreThroughBitfieldLValue(RValue::get(Result), LHSLV, &Result);\n  else\n    CGF.EmitStoreThroughLValue(RValue::get(Result), LHSLV);\n\n  if (CGF.getLangOpts().OpenMP)\n    CGF.CGM.getOpenMPRuntime().checkAndEmitLastprivateConditional(CGF,\n                                                                  E->getLHS());\n  return LHSLV;\n}\n\nValue *ScalarExprEmitter::EmitCompoundAssign(const CompoundAssignOperator *E,\n                      Value *(ScalarExprEmitter::*Func)(const BinOpInfo &)) {\n  bool Ignore = TestAndClearIgnoreResultAssign();\n  Value *RHS = nullptr;\n  LValue LHS = EmitCompoundAssignLValue(E, Func, RHS);\n\n  // If the result is clearly ignored, return now.\n  if (Ignore)\n    return nullptr;\n\n  // The result of an assignment in C is the assigned r-value.\n  if (!CGF.getLangOpts().CPlusPlus)\n    return RHS;\n\n  // If the lvalue is non-volatile, return the computed value of the assignment.\n  if (!LHS.isVolatileQualified())\n    return RHS;\n\n  // Otherwise, reload the value.\n  return EmitLoadOfLValue(LHS, E->getExprLoc());\n}\n\nvoid ScalarExprEmitter::EmitUndefinedBehaviorIntegerDivAndRemCheck(\n    const BinOpInfo &Ops, llvm::Value *Zero, bool isDiv) {\n  SmallVector<std::pair<llvm::Value *, SanitizerMask>, 2> Checks;\n\n  if (CGF.SanOpts.has(SanitizerKind::IntegerDivideByZero)) {\n    Checks.push_back(std::make_pair(Builder.CreateICmpNE(Ops.RHS, Zero),\n                                    SanitizerKind::IntegerDivideByZero));\n  }\n\n  const auto *BO = cast<BinaryOperator>(Ops.E);\n  if (CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow) &&\n      Ops.Ty->hasSignedIntegerRepresentation() &&\n      !IsWidenedIntegerOp(CGF.getContext(), BO->getLHS()) &&\n      Ops.mayHaveIntegerOverflow()) {\n    llvm::IntegerType *Ty = cast<llvm::IntegerType>(Zero->getType());\n\n    llvm::Value *IntMin =\n      Builder.getInt(llvm::APInt::getSignedMinValue(Ty->getBitWidth()));\n    llvm::Value *NegOne = llvm::ConstantInt::get(Ty, -1ULL);\n\n    llvm::Value *LHSCmp = Builder.CreateICmpNE(Ops.LHS, IntMin);\n    llvm::Value *RHSCmp = Builder.CreateICmpNE(Ops.RHS, NegOne);\n    llvm::Value *NotOverflow = Builder.CreateOr(LHSCmp, RHSCmp, \"or\");\n    Checks.push_back(\n        std::make_pair(NotOverflow, SanitizerKind::SignedIntegerOverflow));\n  }\n\n  if (Checks.size() > 0)\n    EmitBinOpCheck(Checks, Ops);\n}\n\nValue *ScalarExprEmitter::EmitDiv(const BinOpInfo &Ops) {\n  {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    if ((CGF.SanOpts.has(SanitizerKind::IntegerDivideByZero) ||\n         CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow)) &&\n        Ops.Ty->isIntegerType() &&\n        (Ops.mayHaveIntegerDivisionByZero() || Ops.mayHaveIntegerOverflow())) {\n      llvm::Value *Zero = llvm::Constant::getNullValue(ConvertType(Ops.Ty));\n      EmitUndefinedBehaviorIntegerDivAndRemCheck(Ops, Zero, true);\n    } else if (CGF.SanOpts.has(SanitizerKind::FloatDivideByZero) &&\n               Ops.Ty->isRealFloatingType() &&\n               Ops.mayHaveFloatDivisionByZero()) {\n      llvm::Value *Zero = llvm::Constant::getNullValue(ConvertType(Ops.Ty));\n      llvm::Value *NonZero = Builder.CreateFCmpUNE(Ops.RHS, Zero);\n      EmitBinOpCheck(std::make_pair(NonZero, SanitizerKind::FloatDivideByZero),\n                     Ops);\n    }\n  }\n\n  if (Ops.LHS->getType()->isFPOrFPVectorTy()) {\n    llvm::Value *Val;\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, Ops.FPFeatures);\n    Val = Builder.CreateFDiv(Ops.LHS, Ops.RHS, \"div\");\n    if (CGF.getLangOpts().OpenCL &&\n        !CGF.CGM.getCodeGenOpts().CorrectlyRoundedDivSqrt) {\n      // OpenCL v1.1 s7.4: minimum accuracy of single precision / is 2.5ulp\n      // OpenCL v1.2 s5.6.4.2: The -cl-fp32-correctly-rounded-divide-sqrt\n      // build option allows an application to specify that single precision\n      // floating-point divide (x/y and 1/x) and sqrt used in the program\n      // source are correctly rounded.\n      llvm::Type *ValTy = Val->getType();\n      if (ValTy->isFloatTy() ||\n          (isa<llvm::VectorType>(ValTy) &&\n           cast<llvm::VectorType>(ValTy)->getElementType()->isFloatTy()))\n        CGF.SetFPAccuracy(Val, 2.5);\n    }\n    return Val;\n  }\n  else if (Ops.isFixedPointOp())\n    return EmitFixedPointBinOp(Ops);\n  else if (Ops.Ty->hasUnsignedIntegerRepresentation())\n    return Builder.CreateUDiv(Ops.LHS, Ops.RHS, \"div\");\n  else\n    return Builder.CreateSDiv(Ops.LHS, Ops.RHS, \"div\");\n}\n\nValue *ScalarExprEmitter::EmitRem(const BinOpInfo &Ops) {\n  // Rem in C can't be a floating point type: C99 6.5.5p2.\n  if ((CGF.SanOpts.has(SanitizerKind::IntegerDivideByZero) ||\n       CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow)) &&\n      Ops.Ty->isIntegerType() &&\n      (Ops.mayHaveIntegerDivisionByZero() || Ops.mayHaveIntegerOverflow())) {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    llvm::Value *Zero = llvm::Constant::getNullValue(ConvertType(Ops.Ty));\n    EmitUndefinedBehaviorIntegerDivAndRemCheck(Ops, Zero, false);\n  }\n\n  if (Ops.Ty->hasUnsignedIntegerRepresentation())\n    return Builder.CreateURem(Ops.LHS, Ops.RHS, \"rem\");\n  else\n    return Builder.CreateSRem(Ops.LHS, Ops.RHS, \"rem\");\n}\n\nValue *ScalarExprEmitter::EmitOverflowCheckedBinOp(const BinOpInfo &Ops) {\n  unsigned IID;\n  unsigned OpID = 0;\n  SanitizerHandler OverflowKind;\n\n  bool isSigned = Ops.Ty->isSignedIntegerOrEnumerationType();\n  switch (Ops.Opcode) {\n  case BO_Add:\n  case BO_AddAssign:\n    OpID = 1;\n    IID = isSigned ? llvm::Intrinsic::sadd_with_overflow :\n                     llvm::Intrinsic::uadd_with_overflow;\n    OverflowKind = SanitizerHandler::AddOverflow;\n    break;\n  case BO_Sub:\n  case BO_SubAssign:\n    OpID = 2;\n    IID = isSigned ? llvm::Intrinsic::ssub_with_overflow :\n                     llvm::Intrinsic::usub_with_overflow;\n    OverflowKind = SanitizerHandler::SubOverflow;\n    break;\n  case BO_Mul:\n  case BO_MulAssign:\n    OpID = 3;\n    IID = isSigned ? llvm::Intrinsic::smul_with_overflow :\n                     llvm::Intrinsic::umul_with_overflow;\n    OverflowKind = SanitizerHandler::MulOverflow;\n    break;\n  default:\n    llvm_unreachable(\"Unsupported operation for overflow detection\");\n  }\n  OpID <<= 1;\n  if (isSigned)\n    OpID |= 1;\n\n  CodeGenFunction::SanitizerScope SanScope(&CGF);\n  llvm::Type *opTy = CGF.CGM.getTypes().ConvertType(Ops.Ty);\n\n  llvm::Function *intrinsic = CGF.CGM.getIntrinsic(IID, opTy);\n\n  Value *resultAndOverflow = Builder.CreateCall(intrinsic, {Ops.LHS, Ops.RHS});\n  Value *result = Builder.CreateExtractValue(resultAndOverflow, 0);\n  Value *overflow = Builder.CreateExtractValue(resultAndOverflow, 1);\n\n  // Handle overflow with llvm.trap if no custom handler has been specified.\n  const std::string *handlerName =\n    &CGF.getLangOpts().OverflowHandler;\n  if (handlerName->empty()) {\n    // If the signed-integer-overflow sanitizer is enabled, emit a call to its\n    // runtime. Otherwise, this is a -ftrapv check, so just emit a trap.\n    if (!isSigned || CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow)) {\n      llvm::Value *NotOverflow = Builder.CreateNot(overflow);\n      SanitizerMask Kind = isSigned ? SanitizerKind::SignedIntegerOverflow\n                              : SanitizerKind::UnsignedIntegerOverflow;\n      EmitBinOpCheck(std::make_pair(NotOverflow, Kind), Ops);\n    } else\n      CGF.EmitTrapCheck(Builder.CreateNot(overflow), OverflowKind);\n    return result;\n  }\n\n  // Branch in case of overflow.\n  llvm::BasicBlock *initialBB = Builder.GetInsertBlock();\n  llvm::BasicBlock *continueBB =\n      CGF.createBasicBlock(\"nooverflow\", CGF.CurFn, initialBB->getNextNode());\n  llvm::BasicBlock *overflowBB = CGF.createBasicBlock(\"overflow\", CGF.CurFn);\n\n  Builder.CreateCondBr(overflow, overflowBB, continueBB);\n\n  // If an overflow handler is set, then we want to call it and then use its\n  // result, if it returns.\n  Builder.SetInsertPoint(overflowBB);\n\n  // Get the overflow handler.\n  llvm::Type *Int8Ty = CGF.Int8Ty;\n  llvm::Type *argTypes[] = { CGF.Int64Ty, CGF.Int64Ty, Int8Ty, Int8Ty };\n  llvm::FunctionType *handlerTy =\n      llvm::FunctionType::get(CGF.Int64Ty, argTypes, true);\n  llvm::FunctionCallee handler =\n      CGF.CGM.CreateRuntimeFunction(handlerTy, *handlerName);\n\n  // Sign extend the args to 64-bit, so that we can use the same handler for\n  // all types of overflow.\n  llvm::Value *lhs = Builder.CreateSExt(Ops.LHS, CGF.Int64Ty);\n  llvm::Value *rhs = Builder.CreateSExt(Ops.RHS, CGF.Int64Ty);\n\n  // Call the handler with the two arguments, the operation, and the size of\n  // the result.\n  llvm::Value *handlerArgs[] = {\n    lhs,\n    rhs,\n    Builder.getInt8(OpID),\n    Builder.getInt8(cast<llvm::IntegerType>(opTy)->getBitWidth())\n  };\n  llvm::Value *handlerResult =\n    CGF.EmitNounwindRuntimeCall(handler, handlerArgs);\n\n  // Truncate the result back to the desired size.\n  handlerResult = Builder.CreateTrunc(handlerResult, opTy);\n  Builder.CreateBr(continueBB);\n\n  Builder.SetInsertPoint(continueBB);\n  llvm::PHINode *phi = Builder.CreatePHI(opTy, 2);\n  phi->addIncoming(result, initialBB);\n  phi->addIncoming(handlerResult, overflowBB);\n\n  return phi;\n}\n\n/// Emit pointer + index arithmetic.\nstatic Value *emitPointerArithmetic(CodeGenFunction &CGF,\n                                    const BinOpInfo &op,\n                                    bool isSubtraction) {\n  // Must have binary (not unary) expr here.  Unary pointer\n  // increment/decrement doesn't use this path.\n  const BinaryOperator *expr = cast<BinaryOperator>(op.E);\n\n  Value *pointer = op.LHS;\n  Expr *pointerOperand = expr->getLHS();\n  Value *index = op.RHS;\n  Expr *indexOperand = expr->getRHS();\n\n  // In a subtraction, the LHS is always the pointer.\n  if (!isSubtraction && !pointer->getType()->isPointerTy()) {\n    std::swap(pointer, index);\n    std::swap(pointerOperand, indexOperand);\n  }\n\n  bool isSigned = indexOperand->getType()->isSignedIntegerOrEnumerationType();\n\n  unsigned width = cast<llvm::IntegerType>(index->getType())->getBitWidth();\n  auto &DL = CGF.CGM.getDataLayout();\n  auto PtrTy = cast<llvm::PointerType>(pointer->getType());\n\n  // Some versions of glibc and gcc use idioms (particularly in their malloc\n  // routines) that add a pointer-sized integer (known to be a pointer value)\n  // to a null pointer in order to cast the value back to an integer or as\n  // part of a pointer alignment algorithm.  This is undefined behavior, but\n  // we'd like to be able to compile programs that use it.\n  //\n  // Normally, we'd generate a GEP with a null-pointer base here in response\n  // to that code, but it's also UB to dereference a pointer created that\n  // way.  Instead (as an acknowledged hack to tolerate the idiom) we will\n  // generate a direct cast of the integer value to a pointer.\n  //\n  // The idiom (p = nullptr + N) is not met if any of the following are true:\n  //\n  //   The operation is subtraction.\n  //   The index is not pointer-sized.\n  //   The pointer type is not byte-sized.\n  //\n  if (BinaryOperator::isNullPointerArithmeticExtension(CGF.getContext(),\n                                                       op.Opcode,\n                                                       expr->getLHS(),\n                                                       expr->getRHS()))\n    return CGF.Builder.CreateIntToPtr(index, pointer->getType());\n\n  if (width != DL.getIndexTypeSizeInBits(PtrTy)) {\n    // Zero-extend or sign-extend the pointer value according to\n    // whether the index is signed or not.\n    index = CGF.Builder.CreateIntCast(index, DL.getIndexType(PtrTy), isSigned,\n                                      \"idx.ext\");\n  }\n\n  // If this is subtraction, negate the index.\n  if (isSubtraction)\n    index = CGF.Builder.CreateNeg(index, \"idx.neg\");\n\n  if (CGF.SanOpts.has(SanitizerKind::ArrayBounds))\n    CGF.EmitBoundsCheck(op.E, pointerOperand, index, indexOperand->getType(),\n                        /*Accessed*/ false);\n\n  const PointerType *pointerType\n    = pointerOperand->getType()->getAs<PointerType>();\n  if (!pointerType) {\n    QualType objectType = pointerOperand->getType()\n                                        ->castAs<ObjCObjectPointerType>()\n                                        ->getPointeeType();\n    llvm::Value *objectSize\n      = CGF.CGM.getSize(CGF.getContext().getTypeSizeInChars(objectType));\n\n    index = CGF.Builder.CreateMul(index, objectSize);\n\n    Value *result = CGF.Builder.CreateBitCast(pointer, CGF.VoidPtrTy);\n    result = CGF.Builder.CreateGEP(result, index, \"add.ptr\");\n    return CGF.Builder.CreateBitCast(result, pointer->getType());\n  }\n\n  QualType elementType = pointerType->getPointeeType();\n  if (const VariableArrayType *vla\n        = CGF.getContext().getAsVariableArrayType(elementType)) {\n    // The element count here is the total number of non-VLA elements.\n    llvm::Value *numElements = CGF.getVLASize(vla).NumElts;\n\n    // Effectively, the multiply by the VLA size is part of the GEP.\n    // GEP indexes are signed, and scaling an index isn't permitted to\n    // signed-overflow, so we use the same semantics for our explicit\n    // multiply.  We suppress this if overflow is not undefined behavior.\n    if (CGF.getLangOpts().isSignedOverflowDefined()) {\n      index = CGF.Builder.CreateMul(index, numElements, \"vla.index\");\n      pointer = CGF.Builder.CreateGEP(pointer, index, \"add.ptr\");\n    } else {\n      index = CGF.Builder.CreateNSWMul(index, numElements, \"vla.index\");\n      pointer =\n          CGF.EmitCheckedInBoundsGEP(pointer, index, isSigned, isSubtraction,\n                                     op.E->getExprLoc(), \"add.ptr\");\n    }\n    return pointer;\n  }\n\n  // Explicitly handle GNU void* and function pointer arithmetic extensions. The\n  // GNU void* casts amount to no-ops since our void* type is i8*, but this is\n  // future proof.\n  if (elementType->isVoidType() || elementType->isFunctionType()) {\n    Value *result = CGF.EmitCastToVoidPtr(pointer);\n    result = CGF.Builder.CreateGEP(result, index, \"add.ptr\");\n    return CGF.Builder.CreateBitCast(result, pointer->getType());\n  }\n\n  if (CGF.getLangOpts().isSignedOverflowDefined())\n    return CGF.Builder.CreateGEP(pointer, index, \"add.ptr\");\n\n  return CGF.EmitCheckedInBoundsGEP(pointer, index, isSigned, isSubtraction,\n                                    op.E->getExprLoc(), \"add.ptr\");\n}\n\n// Construct an fmuladd intrinsic to represent a fused mul-add of MulOp and\n// Addend. Use negMul and negAdd to negate the first operand of the Mul or\n// the add operand respectively. This allows fmuladd to represent a*b-c, or\n// c-a*b. Patterns in LLVM should catch the negated forms and translate them to\n// efficient operations.\nstatic Value* buildFMulAdd(llvm::Instruction *MulOp, Value *Addend,\n                           const CodeGenFunction &CGF, CGBuilderTy &Builder,\n                           bool negMul, bool negAdd) {\n  assert(!(negMul && negAdd) && \"Only one of negMul and negAdd should be set.\");\n\n  Value *MulOp0 = MulOp->getOperand(0);\n  Value *MulOp1 = MulOp->getOperand(1);\n  if (negMul)\n    MulOp0 = Builder.CreateFNeg(MulOp0, \"neg\");\n  if (negAdd)\n    Addend = Builder.CreateFNeg(Addend, \"neg\");\n\n  Value *FMulAdd = nullptr;\n  if (Builder.getIsFPConstrained()) {\n    assert(isa<llvm::ConstrainedFPIntrinsic>(MulOp) &&\n           \"Only constrained operation should be created when Builder is in FP \"\n           \"constrained mode\");\n    FMulAdd = Builder.CreateConstrainedFPCall(\n        CGF.CGM.getIntrinsic(llvm::Intrinsic::experimental_constrained_fmuladd,\n                             Addend->getType()),\n        {MulOp0, MulOp1, Addend});\n  } else {\n    FMulAdd = Builder.CreateCall(\n        CGF.CGM.getIntrinsic(llvm::Intrinsic::fmuladd, Addend->getType()),\n        {MulOp0, MulOp1, Addend});\n  }\n  MulOp->eraseFromParent();\n\n  return FMulAdd;\n}\n\n// Check whether it would be legal to emit an fmuladd intrinsic call to\n// represent op and if so, build the fmuladd.\n//\n// Checks that (a) the operation is fusable, and (b) -ffp-contract=on.\n// Does NOT check the type of the operation - it's assumed that this function\n// will be called from contexts where it's known that the type is contractable.\nstatic Value* tryEmitFMulAdd(const BinOpInfo &op,\n                         const CodeGenFunction &CGF, CGBuilderTy &Builder,\n                         bool isSub=false) {\n\n  assert((op.Opcode == BO_Add || op.Opcode == BO_AddAssign ||\n          op.Opcode == BO_Sub || op.Opcode == BO_SubAssign) &&\n         \"Only fadd/fsub can be the root of an fmuladd.\");\n\n  // Check whether this op is marked as fusable.\n  if (!op.FPFeatures.allowFPContractWithinStatement())\n    return nullptr;\n\n  // We have a potentially fusable op. Look for a mul on one of the operands.\n  // Also, make sure that the mul result isn't used directly. In that case,\n  // there's no point creating a muladd operation.\n  if (auto *LHSBinOp = dyn_cast<llvm::BinaryOperator>(op.LHS)) {\n    if (LHSBinOp->getOpcode() == llvm::Instruction::FMul &&\n        LHSBinOp->use_empty())\n      return buildFMulAdd(LHSBinOp, op.RHS, CGF, Builder, false, isSub);\n  }\n  if (auto *RHSBinOp = dyn_cast<llvm::BinaryOperator>(op.RHS)) {\n    if (RHSBinOp->getOpcode() == llvm::Instruction::FMul &&\n        RHSBinOp->use_empty())\n      return buildFMulAdd(RHSBinOp, op.LHS, CGF, Builder, isSub, false);\n  }\n\n  if (auto *LHSBinOp = dyn_cast<llvm::CallBase>(op.LHS)) {\n    if (LHSBinOp->getIntrinsicID() ==\n            llvm::Intrinsic::experimental_constrained_fmul &&\n        LHSBinOp->use_empty())\n      return buildFMulAdd(LHSBinOp, op.RHS, CGF, Builder, false, isSub);\n  }\n  if (auto *RHSBinOp = dyn_cast<llvm::CallBase>(op.RHS)) {\n    if (RHSBinOp->getIntrinsicID() ==\n            llvm::Intrinsic::experimental_constrained_fmul &&\n        RHSBinOp->use_empty())\n      return buildFMulAdd(RHSBinOp, op.LHS, CGF, Builder, isSub, false);\n  }\n\n  return nullptr;\n}\n\nValue *ScalarExprEmitter::EmitAdd(const BinOpInfo &op) {\n  if (op.LHS->getType()->isPointerTy() ||\n      op.RHS->getType()->isPointerTy())\n    return emitPointerArithmetic(CGF, op, CodeGenFunction::NotSubtraction);\n\n  if (op.Ty->isSignedIntegerOrEnumerationType()) {\n    switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n    case LangOptions::SOB_Defined:\n      return Builder.CreateAdd(op.LHS, op.RHS, \"add\");\n    case LangOptions::SOB_Undefined:\n      if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n        return Builder.CreateNSWAdd(op.LHS, op.RHS, \"add\");\n      LLVM_FALLTHROUGH;\n    case LangOptions::SOB_Trapping:\n      if (CanElideOverflowCheck(CGF.getContext(), op))\n        return Builder.CreateNSWAdd(op.LHS, op.RHS, \"add\");\n      return EmitOverflowCheckedBinOp(op);\n    }\n  }\n\n  if (op.Ty->isConstantMatrixType()) {\n    llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n    return MB.CreateAdd(op.LHS, op.RHS);\n  }\n\n  if (op.Ty->isUnsignedIntegerType() &&\n      CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow) &&\n      !CanElideOverflowCheck(CGF.getContext(), op))\n    return EmitOverflowCheckedBinOp(op);\n\n  if (op.LHS->getType()->isFPOrFPVectorTy()) {\n    CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, op.FPFeatures);\n    // Try to form an fmuladd.\n    if (Value *FMulAdd = tryEmitFMulAdd(op, CGF, Builder))\n      return FMulAdd;\n\n    return Builder.CreateFAdd(op.LHS, op.RHS, \"add\");\n  }\n\n  if (op.isFixedPointOp())\n    return EmitFixedPointBinOp(op);\n\n  return Builder.CreateAdd(op.LHS, op.RHS, \"add\");\n}\n\n/// The resulting value must be calculated with exact precision, so the operands\n/// may not be the same type.\nValue *ScalarExprEmitter::EmitFixedPointBinOp(const BinOpInfo &op) {\n  using llvm::APSInt;\n  using llvm::ConstantInt;\n\n  // This is either a binary operation where at least one of the operands is\n  // a fixed-point type, or a unary operation where the operand is a fixed-point\n  // type. The result type of a binary operation is determined by\n  // Sema::handleFixedPointConversions().\n  QualType ResultTy = op.Ty;\n  QualType LHSTy, RHSTy;\n  if (const auto *BinOp = dyn_cast<BinaryOperator>(op.E)) {\n    RHSTy = BinOp->getRHS()->getType();\n    if (const auto *CAO = dyn_cast<CompoundAssignOperator>(BinOp)) {\n      // For compound assignment, the effective type of the LHS at this point\n      // is the computation LHS type, not the actual LHS type, and the final\n      // result type is not the type of the expression but rather the\n      // computation result type.\n      LHSTy = CAO->getComputationLHSType();\n      ResultTy = CAO->getComputationResultType();\n    } else\n      LHSTy = BinOp->getLHS()->getType();\n  } else if (const auto *UnOp = dyn_cast<UnaryOperator>(op.E)) {\n    LHSTy = UnOp->getSubExpr()->getType();\n    RHSTy = UnOp->getSubExpr()->getType();\n  }\n  ASTContext &Ctx = CGF.getContext();\n  Value *LHS = op.LHS;\n  Value *RHS = op.RHS;\n\n  auto LHSFixedSema = Ctx.getFixedPointSemantics(LHSTy);\n  auto RHSFixedSema = Ctx.getFixedPointSemantics(RHSTy);\n  auto ResultFixedSema = Ctx.getFixedPointSemantics(ResultTy);\n  auto CommonFixedSema = LHSFixedSema.getCommonSemantics(RHSFixedSema);\n\n  // Perform the actual operation.\n  Value *Result;\n  llvm::FixedPointBuilder<CGBuilderTy> FPBuilder(Builder);\n  switch (op.Opcode) {\n  case BO_AddAssign:\n  case BO_Add:\n    Result = FPBuilder.CreateAdd(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_SubAssign:\n  case BO_Sub:\n    Result = FPBuilder.CreateSub(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_MulAssign:\n  case BO_Mul:\n    Result = FPBuilder.CreateMul(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_DivAssign:\n  case BO_Div:\n    Result = FPBuilder.CreateDiv(LHS, LHSFixedSema, RHS, RHSFixedSema);\n    break;\n  case BO_ShlAssign:\n  case BO_Shl:\n    Result = FPBuilder.CreateShl(LHS, LHSFixedSema, RHS);\n    break;\n  case BO_ShrAssign:\n  case BO_Shr:\n    Result = FPBuilder.CreateShr(LHS, LHSFixedSema, RHS);\n    break;\n  case BO_LT:\n    return FPBuilder.CreateLT(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_GT:\n    return FPBuilder.CreateGT(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_LE:\n    return FPBuilder.CreateLE(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_GE:\n    return FPBuilder.CreateGE(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_EQ:\n    // For equality operations, we assume any padding bits on unsigned types are\n    // zero'd out. They could be overwritten through non-saturating operations\n    // that cause overflow, but this leads to undefined behavior.\n    return FPBuilder.CreateEQ(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_NE:\n    return FPBuilder.CreateNE(LHS, LHSFixedSema, RHS, RHSFixedSema);\n  case BO_Cmp:\n  case BO_LAnd:\n  case BO_LOr:\n    llvm_unreachable(\"Found unimplemented fixed point binary operation\");\n  case BO_PtrMemD:\n  case BO_PtrMemI:\n  case BO_Rem:\n  case BO_Xor:\n  case BO_And:\n  case BO_Or:\n  case BO_Assign:\n  case BO_RemAssign:\n  case BO_AndAssign:\n  case BO_XorAssign:\n  case BO_OrAssign:\n  case BO_Comma:\n    llvm_unreachable(\"Found unsupported binary operation for fixed point types.\");\n  }\n\n  bool IsShift = BinaryOperator::isShiftOp(op.Opcode) ||\n                 BinaryOperator::isShiftAssignOp(op.Opcode);\n  // Convert to the result type.\n  return FPBuilder.CreateFixedToFixed(Result, IsShift ? LHSFixedSema\n                                                      : CommonFixedSema,\n                                      ResultFixedSema);\n}\n\nValue *ScalarExprEmitter::EmitSub(const BinOpInfo &op) {\n  // The LHS is always a pointer if either side is.\n  if (!op.LHS->getType()->isPointerTy()) {\n    if (op.Ty->isSignedIntegerOrEnumerationType()) {\n      switch (CGF.getLangOpts().getSignedOverflowBehavior()) {\n      case LangOptions::SOB_Defined:\n        return Builder.CreateSub(op.LHS, op.RHS, \"sub\");\n      case LangOptions::SOB_Undefined:\n        if (!CGF.SanOpts.has(SanitizerKind::SignedIntegerOverflow))\n          return Builder.CreateNSWSub(op.LHS, op.RHS, \"sub\");\n        LLVM_FALLTHROUGH;\n      case LangOptions::SOB_Trapping:\n        if (CanElideOverflowCheck(CGF.getContext(), op))\n          return Builder.CreateNSWSub(op.LHS, op.RHS, \"sub\");\n        return EmitOverflowCheckedBinOp(op);\n      }\n    }\n\n    if (op.Ty->isConstantMatrixType()) {\n      llvm::MatrixBuilder<CGBuilderTy> MB(Builder);\n      return MB.CreateSub(op.LHS, op.RHS);\n    }\n\n    if (op.Ty->isUnsignedIntegerType() &&\n        CGF.SanOpts.has(SanitizerKind::UnsignedIntegerOverflow) &&\n        !CanElideOverflowCheck(CGF.getContext(), op))\n      return EmitOverflowCheckedBinOp(op);\n\n    if (op.LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, op.FPFeatures);\n      // Try to form an fmuladd.\n      if (Value *FMulAdd = tryEmitFMulAdd(op, CGF, Builder, true))\n        return FMulAdd;\n      return Builder.CreateFSub(op.LHS, op.RHS, \"sub\");\n    }\n\n    if (op.isFixedPointOp())\n      return EmitFixedPointBinOp(op);\n\n    return Builder.CreateSub(op.LHS, op.RHS, \"sub\");\n  }\n\n  // If the RHS is not a pointer, then we have normal pointer\n  // arithmetic.\n  if (!op.RHS->getType()->isPointerTy())\n    return emitPointerArithmetic(CGF, op, CodeGenFunction::IsSubtraction);\n\n  // Otherwise, this is a pointer subtraction.\n\n  // Do the raw subtraction part.\n  llvm::Value *LHS\n    = Builder.CreatePtrToInt(op.LHS, CGF.PtrDiffTy, \"sub.ptr.lhs.cast\");\n  llvm::Value *RHS\n    = Builder.CreatePtrToInt(op.RHS, CGF.PtrDiffTy, \"sub.ptr.rhs.cast\");\n  Value *diffInChars = Builder.CreateSub(LHS, RHS, \"sub.ptr.sub\");\n\n  // Okay, figure out the element size.\n  const BinaryOperator *expr = cast<BinaryOperator>(op.E);\n  QualType elementType = expr->getLHS()->getType()->getPointeeType();\n\n  llvm::Value *divisor = nullptr;\n\n  // For a variable-length array, this is going to be non-constant.\n  if (const VariableArrayType *vla\n        = CGF.getContext().getAsVariableArrayType(elementType)) {\n    auto VlaSize = CGF.getVLASize(vla);\n    elementType = VlaSize.Type;\n    divisor = VlaSize.NumElts;\n\n    // Scale the number of non-VLA elements by the non-VLA element size.\n    CharUnits eltSize = CGF.getContext().getTypeSizeInChars(elementType);\n    if (!eltSize.isOne())\n      divisor = CGF.Builder.CreateNUWMul(CGF.CGM.getSize(eltSize), divisor);\n\n  // For everything elese, we can just compute it, safe in the\n  // assumption that Sema won't let anything through that we can't\n  // safely compute the size of.\n  } else {\n    CharUnits elementSize;\n    // Handle GCC extension for pointer arithmetic on void* and\n    // function pointer types.\n    if (elementType->isVoidType() || elementType->isFunctionType())\n      elementSize = CharUnits::One();\n    else\n      elementSize = CGF.getContext().getTypeSizeInChars(elementType);\n\n    // Don't even emit the divide for element size of 1.\n    if (elementSize.isOne())\n      return diffInChars;\n\n    divisor = CGF.CGM.getSize(elementSize);\n  }\n\n  // Otherwise, do a full sdiv. This uses the \"exact\" form of sdiv, since\n  // pointer difference in C is only defined in the case where both operands\n  // are pointing to elements of an array.\n  return Builder.CreateExactSDiv(diffInChars, divisor, \"sub.ptr.div\");\n}\n\nValue *ScalarExprEmitter::GetWidthMinusOneValue(Value* LHS,Value* RHS) {\n  llvm::IntegerType *Ty;\n  if (llvm::VectorType *VT = dyn_cast<llvm::VectorType>(LHS->getType()))\n    Ty = cast<llvm::IntegerType>(VT->getElementType());\n  else\n    Ty = cast<llvm::IntegerType>(LHS->getType());\n  return llvm::ConstantInt::get(RHS->getType(), Ty->getBitWidth() - 1);\n}\n\nValue *ScalarExprEmitter::ConstrainShiftValue(Value *LHS, Value *RHS,\n                                              const Twine &Name) {\n  llvm::IntegerType *Ty;\n  if (auto *VT = dyn_cast<llvm::VectorType>(LHS->getType()))\n    Ty = cast<llvm::IntegerType>(VT->getElementType());\n  else\n    Ty = cast<llvm::IntegerType>(LHS->getType());\n\n  if (llvm::isPowerOf2_64(Ty->getBitWidth()))\n        return Builder.CreateAnd(RHS, GetWidthMinusOneValue(LHS, RHS), Name);\n\n  return Builder.CreateURem(\n      RHS, llvm::ConstantInt::get(RHS->getType(), Ty->getBitWidth()), Name);\n}\n\nValue *ScalarExprEmitter::EmitShl(const BinOpInfo &Ops) {\n  // TODO: This misses out on the sanitizer check below.\n  if (Ops.isFixedPointOp())\n    return EmitFixedPointBinOp(Ops);\n\n  // LLVM requires the LHS and RHS to be the same type: promote or truncate the\n  // RHS to the same size as the LHS.\n  Value *RHS = Ops.RHS;\n  if (Ops.LHS->getType() != RHS->getType())\n    RHS = Builder.CreateIntCast(RHS, Ops.LHS->getType(), false, \"sh_prom\");\n\n  bool SanitizeSignedBase = CGF.SanOpts.has(SanitizerKind::ShiftBase) &&\n                            Ops.Ty->hasSignedIntegerRepresentation() &&\n                            !CGF.getLangOpts().isSignedOverflowDefined() &&\n                            !CGF.getLangOpts().CPlusPlus20;\n  bool SanitizeUnsignedBase =\n      CGF.SanOpts.has(SanitizerKind::UnsignedShiftBase) &&\n      Ops.Ty->hasUnsignedIntegerRepresentation();\n  bool SanitizeBase = SanitizeSignedBase || SanitizeUnsignedBase;\n  bool SanitizeExponent = CGF.SanOpts.has(SanitizerKind::ShiftExponent);\n  // OpenCL 6.3j: shift values are effectively % word size of LHS.\n  if (CGF.getLangOpts().OpenCL)\n    RHS = ConstrainShiftValue(Ops.LHS, RHS, \"shl.mask\");\n  else if ((SanitizeBase || SanitizeExponent) &&\n           isa<llvm::IntegerType>(Ops.LHS->getType())) {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    SmallVector<std::pair<Value *, SanitizerMask>, 2> Checks;\n    llvm::Value *WidthMinusOne = GetWidthMinusOneValue(Ops.LHS, Ops.RHS);\n    llvm::Value *ValidExponent = Builder.CreateICmpULE(Ops.RHS, WidthMinusOne);\n\n    if (SanitizeExponent) {\n      Checks.push_back(\n          std::make_pair(ValidExponent, SanitizerKind::ShiftExponent));\n    }\n\n    if (SanitizeBase) {\n      // Check whether we are shifting any non-zero bits off the top of the\n      // integer. We only emit this check if exponent is valid - otherwise\n      // instructions below will have undefined behavior themselves.\n      llvm::BasicBlock *Orig = Builder.GetInsertBlock();\n      llvm::BasicBlock *Cont = CGF.createBasicBlock(\"cont\");\n      llvm::BasicBlock *CheckShiftBase = CGF.createBasicBlock(\"check\");\n      Builder.CreateCondBr(ValidExponent, CheckShiftBase, Cont);\n      llvm::Value *PromotedWidthMinusOne =\n          (RHS == Ops.RHS) ? WidthMinusOne\n                           : GetWidthMinusOneValue(Ops.LHS, RHS);\n      CGF.EmitBlock(CheckShiftBase);\n      llvm::Value *BitsShiftedOff = Builder.CreateLShr(\n          Ops.LHS, Builder.CreateSub(PromotedWidthMinusOne, RHS, \"shl.zeros\",\n                                     /*NUW*/ true, /*NSW*/ true),\n          \"shl.check\");\n      if (SanitizeUnsignedBase || CGF.getLangOpts().CPlusPlus) {\n        // In C99, we are not permitted to shift a 1 bit into the sign bit.\n        // Under C++11's rules, shifting a 1 bit into the sign bit is\n        // OK, but shifting a 1 bit out of it is not. (C89 and C++03 don't\n        // define signed left shifts, so we use the C99 and C++11 rules there).\n        // Unsigned shifts can always shift into the top bit.\n        llvm::Value *One = llvm::ConstantInt::get(BitsShiftedOff->getType(), 1);\n        BitsShiftedOff = Builder.CreateLShr(BitsShiftedOff, One);\n      }\n      llvm::Value *Zero = llvm::ConstantInt::get(BitsShiftedOff->getType(), 0);\n      llvm::Value *ValidBase = Builder.CreateICmpEQ(BitsShiftedOff, Zero);\n      CGF.EmitBlock(Cont);\n      llvm::PHINode *BaseCheck = Builder.CreatePHI(ValidBase->getType(), 2);\n      BaseCheck->addIncoming(Builder.getTrue(), Orig);\n      BaseCheck->addIncoming(ValidBase, CheckShiftBase);\n      Checks.push_back(std::make_pair(\n          BaseCheck, SanitizeSignedBase ? SanitizerKind::ShiftBase\n                                        : SanitizerKind::UnsignedShiftBase));\n    }\n\n    assert(!Checks.empty());\n    EmitBinOpCheck(Checks, Ops);\n  }\n\n  return Builder.CreateShl(Ops.LHS, RHS, \"shl\");\n}\n\nValue *ScalarExprEmitter::EmitShr(const BinOpInfo &Ops) {\n  // TODO: This misses out on the sanitizer check below.\n  if (Ops.isFixedPointOp())\n    return EmitFixedPointBinOp(Ops);\n\n  // LLVM requires the LHS and RHS to be the same type: promote or truncate the\n  // RHS to the same size as the LHS.\n  Value *RHS = Ops.RHS;\n  if (Ops.LHS->getType() != RHS->getType())\n    RHS = Builder.CreateIntCast(RHS, Ops.LHS->getType(), false, \"sh_prom\");\n\n  // OpenCL 6.3j: shift values are effectively % word size of LHS.\n  if (CGF.getLangOpts().OpenCL)\n    RHS = ConstrainShiftValue(Ops.LHS, RHS, \"shr.mask\");\n  else if (CGF.SanOpts.has(SanitizerKind::ShiftExponent) &&\n           isa<llvm::IntegerType>(Ops.LHS->getType())) {\n    CodeGenFunction::SanitizerScope SanScope(&CGF);\n    llvm::Value *Valid =\n        Builder.CreateICmpULE(RHS, GetWidthMinusOneValue(Ops.LHS, RHS));\n    EmitBinOpCheck(std::make_pair(Valid, SanitizerKind::ShiftExponent), Ops);\n  }\n\n  if (Ops.Ty->hasUnsignedIntegerRepresentation())\n    return Builder.CreateLShr(Ops.LHS, RHS, \"shr\");\n  return Builder.CreateAShr(Ops.LHS, RHS, \"shr\");\n}\n\nenum IntrinsicType { VCMPEQ, VCMPGT };\n// return corresponding comparison intrinsic for given vector type\nstatic llvm::Intrinsic::ID GetIntrinsic(IntrinsicType IT,\n                                        BuiltinType::Kind ElemKind) {\n  switch (ElemKind) {\n  default: llvm_unreachable(\"unexpected element type\");\n  case BuiltinType::Char_U:\n  case BuiltinType::UChar:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequb_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtub_p;\n  case BuiltinType::Char_S:\n  case BuiltinType::SChar:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequb_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsb_p;\n  case BuiltinType::UShort:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequh_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtuh_p;\n  case BuiltinType::Short:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequh_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsh_p;\n  case BuiltinType::UInt:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequw_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtuw_p;\n  case BuiltinType::Int:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequw_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsw_p;\n  case BuiltinType::ULong:\n  case BuiltinType::ULongLong:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequd_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtud_p;\n  case BuiltinType::Long:\n  case BuiltinType::LongLong:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequd_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtsd_p;\n  case BuiltinType::Float:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpeqfp_p :\n                            llvm::Intrinsic::ppc_altivec_vcmpgtfp_p;\n  case BuiltinType::Double:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_vsx_xvcmpeqdp_p :\n                            llvm::Intrinsic::ppc_vsx_xvcmpgtdp_p;\n  case BuiltinType::UInt128:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequq_p\n                          : llvm::Intrinsic::ppc_altivec_vcmpgtuq_p;\n  case BuiltinType::Int128:\n    return (IT == VCMPEQ) ? llvm::Intrinsic::ppc_altivec_vcmpequq_p\n                          : llvm::Intrinsic::ppc_altivec_vcmpgtsq_p;\n  }\n}\n\nValue *ScalarExprEmitter::EmitCompare(const BinaryOperator *E,\n                                      llvm::CmpInst::Predicate UICmpOpc,\n                                      llvm::CmpInst::Predicate SICmpOpc,\n                                      llvm::CmpInst::Predicate FCmpOpc,\n                                      bool IsSignaling) {\n  TestAndClearIgnoreResultAssign();\n  Value *Result;\n  QualType LHSTy = E->getLHS()->getType();\n  QualType RHSTy = E->getRHS()->getType();\n  if (const MemberPointerType *MPT = LHSTy->getAs<MemberPointerType>()) {\n    assert(E->getOpcode() == BO_EQ ||\n           E->getOpcode() == BO_NE);\n    Value *LHS = CGF.EmitScalarExpr(E->getLHS());\n    Value *RHS = CGF.EmitScalarExpr(E->getRHS());\n    Result = CGF.CGM.getCXXABI().EmitMemberPointerComparison(\n                   CGF, LHS, RHS, MPT, E->getOpcode() == BO_NE);\n  } else if (!LHSTy->isAnyComplexType() && !RHSTy->isAnyComplexType()) {\n    BinOpInfo BOInfo = EmitBinOps(E);\n    Value *LHS = BOInfo.LHS;\n    Value *RHS = BOInfo.RHS;\n\n    // If AltiVec, the comparison results in a numeric type, so we use\n    // intrinsics comparing vectors and giving 0 or 1 as a result\n    if (LHSTy->isVectorType() && !E->getType()->isVectorType()) {\n      // constants for mapping CR6 register bits to predicate result\n      enum { CR6_EQ=0, CR6_EQ_REV, CR6_LT, CR6_LT_REV } CR6;\n\n      llvm::Intrinsic::ID ID = llvm::Intrinsic::not_intrinsic;\n\n      // in several cases vector arguments order will be reversed\n      Value *FirstVecArg = LHS,\n            *SecondVecArg = RHS;\n\n      QualType ElTy = LHSTy->castAs<VectorType>()->getElementType();\n      BuiltinType::Kind ElementKind = ElTy->castAs<BuiltinType>()->getKind();\n\n      switch(E->getOpcode()) {\n      default: llvm_unreachable(\"is not a comparison operation\");\n      case BO_EQ:\n        CR6 = CR6_LT;\n        ID = GetIntrinsic(VCMPEQ, ElementKind);\n        break;\n      case BO_NE:\n        CR6 = CR6_EQ;\n        ID = GetIntrinsic(VCMPEQ, ElementKind);\n        break;\n      case BO_LT:\n        CR6 = CR6_LT;\n        ID = GetIntrinsic(VCMPGT, ElementKind);\n        std::swap(FirstVecArg, SecondVecArg);\n        break;\n      case BO_GT:\n        CR6 = CR6_LT;\n        ID = GetIntrinsic(VCMPGT, ElementKind);\n        break;\n      case BO_LE:\n        if (ElementKind == BuiltinType::Float) {\n          CR6 = CR6_LT;\n          ID = llvm::Intrinsic::ppc_altivec_vcmpgefp_p;\n          std::swap(FirstVecArg, SecondVecArg);\n        }\n        else {\n          CR6 = CR6_EQ;\n          ID = GetIntrinsic(VCMPGT, ElementKind);\n        }\n        break;\n      case BO_GE:\n        if (ElementKind == BuiltinType::Float) {\n          CR6 = CR6_LT;\n          ID = llvm::Intrinsic::ppc_altivec_vcmpgefp_p;\n        }\n        else {\n          CR6 = CR6_EQ;\n          ID = GetIntrinsic(VCMPGT, ElementKind);\n          std::swap(FirstVecArg, SecondVecArg);\n        }\n        break;\n      }\n\n      Value *CR6Param = Builder.getInt32(CR6);\n      llvm::Function *F = CGF.CGM.getIntrinsic(ID);\n      Result = Builder.CreateCall(F, {CR6Param, FirstVecArg, SecondVecArg});\n\n      // The result type of intrinsic may not be same as E->getType().\n      // If E->getType() is not BoolTy, EmitScalarConversion will do the\n      // conversion work. If E->getType() is BoolTy, EmitScalarConversion will\n      // do nothing, if ResultTy is not i1 at the same time, it will cause\n      // crash later.\n      llvm::IntegerType *ResultTy = cast<llvm::IntegerType>(Result->getType());\n      if (ResultTy->getBitWidth() > 1 &&\n          E->getType() == CGF.getContext().BoolTy)\n        Result = Builder.CreateTrunc(Result, Builder.getInt1Ty());\n      return EmitScalarConversion(Result, CGF.getContext().BoolTy, E->getType(),\n                                  E->getExprLoc());\n    }\n\n    if (BOInfo.isFixedPointOp()) {\n      Result = EmitFixedPointBinOp(BOInfo);\n    } else if (LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(CGF, BOInfo.FPFeatures);\n      if (!IsSignaling)\n        Result = Builder.CreateFCmp(FCmpOpc, LHS, RHS, \"cmp\");\n      else\n        Result = Builder.CreateFCmpS(FCmpOpc, LHS, RHS, \"cmp\");\n    } else if (LHSTy->hasSignedIntegerRepresentation()) {\n      Result = Builder.CreateICmp(SICmpOpc, LHS, RHS, \"cmp\");\n    } else {\n      // Unsigned integers and pointers.\n\n      if (CGF.CGM.getCodeGenOpts().StrictVTablePointers &&\n          !isa<llvm::ConstantPointerNull>(LHS) &&\n          !isa<llvm::ConstantPointerNull>(RHS)) {\n\n        // Dynamic information is required to be stripped for comparisons,\n        // because it could leak the dynamic information.  Based on comparisons\n        // of pointers to dynamic objects, the optimizer can replace one pointer\n        // with another, which might be incorrect in presence of invariant\n        // groups. Comparison with null is safe because null does not carry any\n        // dynamic information.\n        if (LHSTy.mayBeDynamicClass())\n          LHS = Builder.CreateStripInvariantGroup(LHS);\n        if (RHSTy.mayBeDynamicClass())\n          RHS = Builder.CreateStripInvariantGroup(RHS);\n      }\n\n      Result = Builder.CreateICmp(UICmpOpc, LHS, RHS, \"cmp\");\n    }\n\n    // If this is a vector comparison, sign extend the result to the appropriate\n    // vector integer type and return it (don't convert to bool).\n    if (LHSTy->isVectorType())\n      return Builder.CreateSExt(Result, ConvertType(E->getType()), \"sext\");\n\n  } else {\n    // Complex Comparison: can only be an equality comparison.\n    CodeGenFunction::ComplexPairTy LHS, RHS;\n    QualType CETy;\n    if (auto *CTy = LHSTy->getAs<ComplexType>()) {\n      LHS = CGF.EmitComplexExpr(E->getLHS());\n      CETy = CTy->getElementType();\n    } else {\n      LHS.first = Visit(E->getLHS());\n      LHS.second = llvm::Constant::getNullValue(LHS.first->getType());\n      CETy = LHSTy;\n    }\n    if (auto *CTy = RHSTy->getAs<ComplexType>()) {\n      RHS = CGF.EmitComplexExpr(E->getRHS());\n      assert(CGF.getContext().hasSameUnqualifiedType(CETy,\n                                                     CTy->getElementType()) &&\n             \"The element types must always match.\");\n      (void)CTy;\n    } else {\n      RHS.first = Visit(E->getRHS());\n      RHS.second = llvm::Constant::getNullValue(RHS.first->getType());\n      assert(CGF.getContext().hasSameUnqualifiedType(CETy, RHSTy) &&\n             \"The element types must always match.\");\n    }\n\n    Value *ResultR, *ResultI;\n    if (CETy->isRealFloatingType()) {\n      // As complex comparisons can only be equality comparisons, they\n      // are never signaling comparisons.\n      ResultR = Builder.CreateFCmp(FCmpOpc, LHS.first, RHS.first, \"cmp.r\");\n      ResultI = Builder.CreateFCmp(FCmpOpc, LHS.second, RHS.second, \"cmp.i\");\n    } else {\n      // Complex comparisons can only be equality comparisons.  As such, signed\n      // and unsigned opcodes are the same.\n      ResultR = Builder.CreateICmp(UICmpOpc, LHS.first, RHS.first, \"cmp.r\");\n      ResultI = Builder.CreateICmp(UICmpOpc, LHS.second, RHS.second, \"cmp.i\");\n    }\n\n    if (E->getOpcode() == BO_EQ) {\n      Result = Builder.CreateAnd(ResultR, ResultI, \"and.ri\");\n    } else {\n      assert(E->getOpcode() == BO_NE &&\n             \"Complex comparison other than == or != ?\");\n      Result = Builder.CreateOr(ResultR, ResultI, \"or.ri\");\n    }\n  }\n\n  return EmitScalarConversion(Result, CGF.getContext().BoolTy, E->getType(),\n                              E->getExprLoc());\n}\n\nValue *ScalarExprEmitter::VisitBinAssign(const BinaryOperator *E) {\n  bool Ignore = TestAndClearIgnoreResultAssign();\n\n  Value *RHS;\n  LValue LHS;\n\n  switch (E->getLHS()->getType().getObjCLifetime()) {\n  case Qualifiers::OCL_Strong:\n    std::tie(LHS, RHS) = CGF.EmitARCStoreStrong(E, Ignore);\n    break;\n\n  case Qualifiers::OCL_Autoreleasing:\n    std::tie(LHS, RHS) = CGF.EmitARCStoreAutoreleasing(E);\n    break;\n\n  case Qualifiers::OCL_ExplicitNone:\n    std::tie(LHS, RHS) = CGF.EmitARCStoreUnsafeUnretained(E, Ignore);\n    break;\n\n  case Qualifiers::OCL_Weak:\n    RHS = Visit(E->getRHS());\n    LHS = EmitCheckedLValue(E->getLHS(), CodeGenFunction::TCK_Store);\n    RHS = CGF.EmitARCStoreWeak(LHS.getAddress(CGF), RHS, Ignore);\n    break;\n\n  case Qualifiers::OCL_None:\n    // __block variables need to have the rhs evaluated first, plus\n    // this should improve codegen just a little.\n    RHS = Visit(E->getRHS());\n    LHS = EmitCheckedLValue(E->getLHS(), CodeGenFunction::TCK_Store);\n\n    // Store the value into the LHS.  Bit-fields are handled specially\n    // because the result is altered by the store, i.e., [C99 6.5.16p1]\n    // 'An assignment expression has the value of the left operand after\n    // the assignment...'.\n    if (LHS.isBitField()) {\n      CGF.EmitStoreThroughBitfieldLValue(RValue::get(RHS), LHS, &RHS);\n    } else {\n      CGF.EmitNullabilityCheck(LHS, RHS, E->getExprLoc());\n      CGF.EmitStoreThroughLValue(RValue::get(RHS), LHS);\n    }\n  }\n\n  // If the result is clearly ignored, return now.\n  if (Ignore)\n    return nullptr;\n\n  // The result of an assignment in C is the assigned r-value.\n  if (!CGF.getLangOpts().CPlusPlus)\n    return RHS;\n\n  // If the lvalue is non-volatile, return the computed value of the assignment.\n  if (!LHS.isVolatileQualified())\n    return RHS;\n\n  // Otherwise, reload the value.\n  return EmitLoadOfLValue(LHS, E->getExprLoc());\n}\n\nValue *ScalarExprEmitter::VisitBinLAnd(const BinaryOperator *E) {\n  // Perform vector logical and on comparisons with zero vectors.\n  if (E->getType()->isVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    Value *LHS = Visit(E->getLHS());\n    Value *RHS = Visit(E->getRHS());\n    Value *Zero = llvm::ConstantAggregateZero::get(LHS->getType());\n    if (LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(\n          CGF, E->getFPFeaturesInEffect(CGF.getLangOpts()));\n      LHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, RHS, Zero, \"cmp\");\n    } else {\n      LHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, RHS, Zero, \"cmp\");\n    }\n    Value *And = Builder.CreateAnd(LHS, RHS);\n    return Builder.CreateSExt(And, ConvertType(E->getType()), \"sext\");\n  }\n\n  bool InstrumentRegions = CGF.CGM.getCodeGenOpts().hasProfileClangInstr();\n  llvm::Type *ResTy = ConvertType(E->getType());\n\n  // If we have 0 && RHS, see if we can elide RHS, if so, just return 0.\n  // If we have 1 && X, just emit X without inserting the control flow.\n  bool LHSCondVal;\n  if (CGF.ConstantFoldsToSimpleInteger(E->getLHS(), LHSCondVal)) {\n    if (LHSCondVal) { // If we have 1 && X, just emit X.\n      CGF.incrementProfileCounter(E);\n\n      Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n\n      // If we're generating for profiling or coverage, generate a branch to a\n      // block that increments the RHS counter needed to track branch condition\n      // coverage. In this case, use \"FBlock\" as both the final \"TrueBlock\" and\n      // \"FalseBlock\" after the increment is done.\n      if (InstrumentRegions &&\n          CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n        llvm::BasicBlock *FBlock = CGF.createBasicBlock(\"land.end\");\n        llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"land.rhscnt\");\n        Builder.CreateCondBr(RHSCond, RHSBlockCnt, FBlock);\n        CGF.EmitBlock(RHSBlockCnt);\n        CGF.incrementProfileCounter(E->getRHS());\n        CGF.EmitBranch(FBlock);\n        CGF.EmitBlock(FBlock);\n      }\n\n      // ZExt result to int or bool.\n      return Builder.CreateZExtOrBitCast(RHSCond, ResTy, \"land.ext\");\n    }\n\n    // 0 && RHS: If it is safe, just elide the RHS, and return 0/false.\n    if (!CGF.ContainsLabel(E->getRHS()))\n      return llvm::Constant::getNullValue(ResTy);\n  }\n\n  llvm::BasicBlock *ContBlock = CGF.createBasicBlock(\"land.end\");\n  llvm::BasicBlock *RHSBlock  = CGF.createBasicBlock(\"land.rhs\");\n\n  CodeGenFunction::ConditionalEvaluation eval(CGF);\n\n  // Branch on the LHS first.  If it is false, go to the failure (cont) block.\n  CGF.EmitBranchOnBoolExpr(E->getLHS(), RHSBlock, ContBlock,\n                           CGF.getProfileCount(E->getRHS()));\n\n  // Any edges into the ContBlock are now from an (indeterminate number of)\n  // edges from this first condition.  All of these values will be false.  Start\n  // setting up the PHI node in the Cont Block for this.\n  llvm::PHINode *PN = llvm::PHINode::Create(llvm::Type::getInt1Ty(VMContext), 2,\n                                            \"\", ContBlock);\n  for (llvm::pred_iterator PI = pred_begin(ContBlock), PE = pred_end(ContBlock);\n       PI != PE; ++PI)\n    PN->addIncoming(llvm::ConstantInt::getFalse(VMContext), *PI);\n\n  eval.begin(CGF);\n  CGF.EmitBlock(RHSBlock);\n  CGF.incrementProfileCounter(E);\n  Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n  eval.end(CGF);\n\n  // Reaquire the RHS block, as there may be subblocks inserted.\n  RHSBlock = Builder.GetInsertBlock();\n\n  // If we're generating for profiling or coverage, generate a branch on the\n  // RHS to a block that increments the RHS true counter needed to track branch\n  // condition coverage.\n  if (InstrumentRegions &&\n      CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n    llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"land.rhscnt\");\n    Builder.CreateCondBr(RHSCond, RHSBlockCnt, ContBlock);\n    CGF.EmitBlock(RHSBlockCnt);\n    CGF.incrementProfileCounter(E->getRHS());\n    CGF.EmitBranch(ContBlock);\n    PN->addIncoming(RHSCond, RHSBlockCnt);\n  }\n\n  // Emit an unconditional branch from this block to ContBlock.\n  {\n    // There is no need to emit line number for unconditional branch.\n    auto NL = ApplyDebugLocation::CreateEmpty(CGF);\n    CGF.EmitBlock(ContBlock);\n  }\n  // Insert an entry into the phi node for the edge with the value of RHSCond.\n  PN->addIncoming(RHSCond, RHSBlock);\n\n  // Artificial location to preserve the scope information\n  {\n    auto NL = ApplyDebugLocation::CreateArtificial(CGF);\n    PN->setDebugLoc(Builder.getCurrentDebugLocation());\n  }\n\n  // ZExt result to int.\n  return Builder.CreateZExtOrBitCast(PN, ResTy, \"land.ext\");\n}\n\nValue *ScalarExprEmitter::VisitBinLOr(const BinaryOperator *E) {\n  // Perform vector logical or on comparisons with zero vectors.\n  if (E->getType()->isVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    Value *LHS = Visit(E->getLHS());\n    Value *RHS = Visit(E->getRHS());\n    Value *Zero = llvm::ConstantAggregateZero::get(LHS->getType());\n    if (LHS->getType()->isFPOrFPVectorTy()) {\n      CodeGenFunction::CGFPOptionsRAII FPOptsRAII(\n          CGF, E->getFPFeaturesInEffect(CGF.getLangOpts()));\n      LHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateFCmp(llvm::CmpInst::FCMP_UNE, RHS, Zero, \"cmp\");\n    } else {\n      LHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, LHS, Zero, \"cmp\");\n      RHS = Builder.CreateICmp(llvm::CmpInst::ICMP_NE, RHS, Zero, \"cmp\");\n    }\n    Value *Or = Builder.CreateOr(LHS, RHS);\n    return Builder.CreateSExt(Or, ConvertType(E->getType()), \"sext\");\n  }\n\n  bool InstrumentRegions = CGF.CGM.getCodeGenOpts().hasProfileClangInstr();\n  llvm::Type *ResTy = ConvertType(E->getType());\n\n  // If we have 1 || RHS, see if we can elide RHS, if so, just return 1.\n  // If we have 0 || X, just emit X without inserting the control flow.\n  bool LHSCondVal;\n  if (CGF.ConstantFoldsToSimpleInteger(E->getLHS(), LHSCondVal)) {\n    if (!LHSCondVal) { // If we have 0 || X, just emit X.\n      CGF.incrementProfileCounter(E);\n\n      Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n\n      // If we're generating for profiling or coverage, generate a branch to a\n      // block that increments the RHS counter need to track branch condition\n      // coverage. In this case, use \"FBlock\" as both the final \"TrueBlock\" and\n      // \"FalseBlock\" after the increment is done.\n      if (InstrumentRegions &&\n          CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n        llvm::BasicBlock *FBlock = CGF.createBasicBlock(\"lor.end\");\n        llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"lor.rhscnt\");\n        Builder.CreateCondBr(RHSCond, FBlock, RHSBlockCnt);\n        CGF.EmitBlock(RHSBlockCnt);\n        CGF.incrementProfileCounter(E->getRHS());\n        CGF.EmitBranch(FBlock);\n        CGF.EmitBlock(FBlock);\n      }\n\n      // ZExt result to int or bool.\n      return Builder.CreateZExtOrBitCast(RHSCond, ResTy, \"lor.ext\");\n    }\n\n    // 1 || RHS: If it is safe, just elide the RHS, and return 1/true.\n    if (!CGF.ContainsLabel(E->getRHS()))\n      return llvm::ConstantInt::get(ResTy, 1);\n  }\n\n  llvm::BasicBlock *ContBlock = CGF.createBasicBlock(\"lor.end\");\n  llvm::BasicBlock *RHSBlock = CGF.createBasicBlock(\"lor.rhs\");\n\n  CodeGenFunction::ConditionalEvaluation eval(CGF);\n\n  // Branch on the LHS first.  If it is true, go to the success (cont) block.\n  CGF.EmitBranchOnBoolExpr(E->getLHS(), ContBlock, RHSBlock,\n                           CGF.getCurrentProfileCount() -\n                               CGF.getProfileCount(E->getRHS()));\n\n  // Any edges into the ContBlock are now from an (indeterminate number of)\n  // edges from this first condition.  All of these values will be true.  Start\n  // setting up the PHI node in the Cont Block for this.\n  llvm::PHINode *PN = llvm::PHINode::Create(llvm::Type::getInt1Ty(VMContext), 2,\n                                            \"\", ContBlock);\n  for (llvm::pred_iterator PI = pred_begin(ContBlock), PE = pred_end(ContBlock);\n       PI != PE; ++PI)\n    PN->addIncoming(llvm::ConstantInt::getTrue(VMContext), *PI);\n\n  eval.begin(CGF);\n\n  // Emit the RHS condition as a bool value.\n  CGF.EmitBlock(RHSBlock);\n  CGF.incrementProfileCounter(E);\n  Value *RHSCond = CGF.EvaluateExprAsBool(E->getRHS());\n\n  eval.end(CGF);\n\n  // Reaquire the RHS block, as there may be subblocks inserted.\n  RHSBlock = Builder.GetInsertBlock();\n\n  // If we're generating for profiling or coverage, generate a branch on the\n  // RHS to a block that increments the RHS true counter needed to track branch\n  // condition coverage.\n  if (InstrumentRegions &&\n      CodeGenFunction::isInstrumentedCondition(E->getRHS())) {\n    llvm::BasicBlock *RHSBlockCnt = CGF.createBasicBlock(\"lor.rhscnt\");\n    Builder.CreateCondBr(RHSCond, ContBlock, RHSBlockCnt);\n    CGF.EmitBlock(RHSBlockCnt);\n    CGF.incrementProfileCounter(E->getRHS());\n    CGF.EmitBranch(ContBlock);\n    PN->addIncoming(RHSCond, RHSBlockCnt);\n  }\n\n  // Emit an unconditional branch from this block to ContBlock.  Insert an entry\n  // into the phi node for the edge with the value of RHSCond.\n  CGF.EmitBlock(ContBlock);\n  PN->addIncoming(RHSCond, RHSBlock);\n\n  // ZExt result to int.\n  return Builder.CreateZExtOrBitCast(PN, ResTy, \"lor.ext\");\n}\n\nValue *ScalarExprEmitter::VisitBinComma(const BinaryOperator *E) {\n  CGF.EmitIgnoredExpr(E->getLHS());\n  CGF.EnsureInsertPoint();\n  return Visit(E->getRHS());\n}\n\n//===----------------------------------------------------------------------===//\n//                             Other Operators\n//===----------------------------------------------------------------------===//\n\n/// isCheapEnoughToEvaluateUnconditionally - Return true if the specified\n/// expression is cheap enough and side-effect-free enough to evaluate\n/// unconditionally instead of conditionally.  This is used to convert control\n/// flow into selects in some cases.\nstatic bool isCheapEnoughToEvaluateUnconditionally(const Expr *E,\n                                                   CodeGenFunction &CGF) {\n  // Anything that is an integer or floating point constant is fine.\n  return E->IgnoreParens()->isEvaluatable(CGF.getContext());\n\n  // Even non-volatile automatic variables can't be evaluated unconditionally.\n  // Referencing a thread_local may cause non-trivial initialization work to\n  // occur. If we're inside a lambda and one of the variables is from the scope\n  // outside the lambda, that function may have returned already. Reading its\n  // locals is a bad idea. Also, these reads may introduce races there didn't\n  // exist in the source-level program.\n}\n\n\nValue *ScalarExprEmitter::\nVisitAbstractConditionalOperator(const AbstractConditionalOperator *E) {\n  TestAndClearIgnoreResultAssign();\n\n  // Bind the common expression if necessary.\n  CodeGenFunction::OpaqueValueMapping binding(CGF, E);\n\n  Expr *condExpr = E->getCond();\n  Expr *lhsExpr = E->getTrueExpr();\n  Expr *rhsExpr = E->getFalseExpr();\n\n  // If the condition constant folds and can be elided, try to avoid emitting\n  // the condition and the dead arm.\n  bool CondExprBool;\n  if (CGF.ConstantFoldsToSimpleInteger(condExpr, CondExprBool)) {\n    Expr *live = lhsExpr, *dead = rhsExpr;\n    if (!CondExprBool) std::swap(live, dead);\n\n    // If the dead side doesn't have labels we need, just emit the Live part.\n    if (!CGF.ContainsLabel(dead)) {\n      if (CondExprBool)\n        CGF.incrementProfileCounter(E);\n      Value *Result = Visit(live);\n\n      // If the live part is a throw expression, it acts like it has a void\n      // type, so evaluating it returns a null Value*.  However, a conditional\n      // with non-void type must return a non-null Value*.\n      if (!Result && !E->getType()->isVoidType())\n        Result = llvm::UndefValue::get(CGF.ConvertType(E->getType()));\n\n      return Result;\n    }\n  }\n\n  // OpenCL: If the condition is a vector, we can treat this condition like\n  // the select function.\n  if ((CGF.getLangOpts().OpenCL && condExpr->getType()->isVectorType()) ||\n      condExpr->getType()->isExtVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    llvm::Value *CondV = CGF.EmitScalarExpr(condExpr);\n    llvm::Value *LHS = Visit(lhsExpr);\n    llvm::Value *RHS = Visit(rhsExpr);\n\n    llvm::Type *condType = ConvertType(condExpr->getType());\n    auto *vecTy = cast<llvm::FixedVectorType>(condType);\n\n    unsigned numElem = vecTy->getNumElements();\n    llvm::Type *elemType = vecTy->getElementType();\n\n    llvm::Value *zeroVec = llvm::Constant::getNullValue(vecTy);\n    llvm::Value *TestMSB = Builder.CreateICmpSLT(CondV, zeroVec);\n    llvm::Value *tmp = Builder.CreateSExt(\n        TestMSB, llvm::FixedVectorType::get(elemType, numElem), \"sext\");\n    llvm::Value *tmp2 = Builder.CreateNot(tmp);\n\n    // Cast float to int to perform ANDs if necessary.\n    llvm::Value *RHSTmp = RHS;\n    llvm::Value *LHSTmp = LHS;\n    bool wasCast = false;\n    llvm::VectorType *rhsVTy = cast<llvm::VectorType>(RHS->getType());\n    if (rhsVTy->getElementType()->isFloatingPointTy()) {\n      RHSTmp = Builder.CreateBitCast(RHS, tmp2->getType());\n      LHSTmp = Builder.CreateBitCast(LHS, tmp->getType());\n      wasCast = true;\n    }\n\n    llvm::Value *tmp3 = Builder.CreateAnd(RHSTmp, tmp2);\n    llvm::Value *tmp4 = Builder.CreateAnd(LHSTmp, tmp);\n    llvm::Value *tmp5 = Builder.CreateOr(tmp3, tmp4, \"cond\");\n    if (wasCast)\n      tmp5 = Builder.CreateBitCast(tmp5, RHS->getType());\n\n    return tmp5;\n  }\n\n  if (condExpr->getType()->isVectorType()) {\n    CGF.incrementProfileCounter(E);\n\n    llvm::Value *CondV = CGF.EmitScalarExpr(condExpr);\n    llvm::Value *LHS = Visit(lhsExpr);\n    llvm::Value *RHS = Visit(rhsExpr);\n\n    llvm::Type *CondType = ConvertType(condExpr->getType());\n    auto *VecTy = cast<llvm::VectorType>(CondType);\n    llvm::Value *ZeroVec = llvm::Constant::getNullValue(VecTy);\n\n    CondV = Builder.CreateICmpNE(CondV, ZeroVec, \"vector_cond\");\n    return Builder.CreateSelect(CondV, LHS, RHS, \"vector_select\");\n  }\n\n  // If this is a really simple expression (like x ? 4 : 5), emit this as a\n  // select instead of as control flow.  We can only do this if it is cheap and\n  // safe to evaluate the LHS and RHS unconditionally.\n  if (isCheapEnoughToEvaluateUnconditionally(lhsExpr, CGF) &&\n      isCheapEnoughToEvaluateUnconditionally(rhsExpr, CGF)) {\n    llvm::Value *CondV = CGF.EvaluateExprAsBool(condExpr);\n    llvm::Value *StepV = Builder.CreateZExtOrBitCast(CondV, CGF.Int64Ty);\n\n    CGF.incrementProfileCounter(E, StepV);\n\n    llvm::Value *LHS = Visit(lhsExpr);\n    llvm::Value *RHS = Visit(rhsExpr);\n    if (!LHS) {\n      // If the conditional has void type, make sure we return a null Value*.\n      assert(!RHS && \"LHS and RHS types must match\");\n      return nullptr;\n    }\n    return Builder.CreateSelect(CondV, LHS, RHS, \"cond\");\n  }\n\n  llvm::BasicBlock *LHSBlock = CGF.createBasicBlock(\"cond.true\");\n  llvm::BasicBlock *RHSBlock = CGF.createBasicBlock(\"cond.false\");\n  llvm::BasicBlock *ContBlock = CGF.createBasicBlock(\"cond.end\");\n\n  CodeGenFunction::ConditionalEvaluation eval(CGF);\n  CGF.EmitBranchOnBoolExpr(condExpr, LHSBlock, RHSBlock,\n                           CGF.getProfileCount(lhsExpr));\n\n  CGF.EmitBlock(LHSBlock);\n  CGF.incrementProfileCounter(E);\n  eval.begin(CGF);\n  Value *LHS = Visit(lhsExpr);\n  eval.end(CGF);\n\n  LHSBlock = Builder.GetInsertBlock();\n  Builder.CreateBr(ContBlock);\n\n  CGF.EmitBlock(RHSBlock);\n  eval.begin(CGF);\n  Value *RHS = Visit(rhsExpr);\n  eval.end(CGF);\n\n  RHSBlock = Builder.GetInsertBlock();\n  CGF.EmitBlock(ContBlock);\n\n  // If the LHS or RHS is a throw expression, it will be legitimately null.\n  if (!LHS)\n    return RHS;\n  if (!RHS)\n    return LHS;\n\n  // Create a PHI node for the real part.\n  llvm::PHINode *PN = Builder.CreatePHI(LHS->getType(), 2, \"cond\");\n  PN->addIncoming(LHS, LHSBlock);\n  PN->addIncoming(RHS, RHSBlock);\n  return PN;\n}\n\nValue *ScalarExprEmitter::VisitChooseExpr(ChooseExpr *E) {\n  return Visit(E->getChosenSubExpr());\n}\n\nValue *ScalarExprEmitter::VisitVAArgExpr(VAArgExpr *VE) {\n  QualType Ty = VE->getType();\n\n  if (Ty->isVariablyModifiedType())\n    CGF.EmitVariablyModifiedType(Ty);\n\n  Address ArgValue = Address::invalid();\n  Address ArgPtr = CGF.EmitVAArg(VE, ArgValue);\n\n  llvm::Type *ArgTy = ConvertType(VE->getType());\n\n  // If EmitVAArg fails, emit an error.\n  if (!ArgPtr.isValid()) {\n    CGF.ErrorUnsupported(VE, \"va_arg expression\");\n    return llvm::UndefValue::get(ArgTy);\n  }\n\n  // FIXME Volatility.\n  llvm::Value *Val = Builder.CreateLoad(ArgPtr);\n\n  // If EmitVAArg promoted the type, we must truncate it.\n  if (ArgTy != Val->getType()) {\n    if (ArgTy->isPointerTy() && !Val->getType()->isPointerTy())\n      Val = Builder.CreateIntToPtr(Val, ArgTy);\n    else\n      Val = Builder.CreateTrunc(Val, ArgTy);\n  }\n\n  return Val;\n}\n\nValue *ScalarExprEmitter::VisitBlockExpr(const BlockExpr *block) {\n  return CGF.EmitBlockLiteral(block);\n}\n\n// Convert a vec3 to vec4, or vice versa.\nstatic Value *ConvertVec3AndVec4(CGBuilderTy &Builder, CodeGenFunction &CGF,\n                                 Value *Src, unsigned NumElementsDst) {\n  static constexpr int Mask[] = {0, 1, 2, -1};\n  return Builder.CreateShuffleVector(Src,\n                                     llvm::makeArrayRef(Mask, NumElementsDst));\n}\n\n// Create cast instructions for converting LLVM value \\p Src to LLVM type \\p\n// DstTy. \\p Src has the same size as \\p DstTy. Both are single value types\n// but could be scalar or vectors of different lengths, and either can be\n// pointer.\n// There are 4 cases:\n// 1. non-pointer -> non-pointer  : needs 1 bitcast\n// 2. pointer -> pointer          : needs 1 bitcast or addrspacecast\n// 3. pointer -> non-pointer\n//   a) pointer -> intptr_t       : needs 1 ptrtoint\n//   b) pointer -> non-intptr_t   : needs 1 ptrtoint then 1 bitcast\n// 4. non-pointer -> pointer\n//   a) intptr_t -> pointer       : needs 1 inttoptr\n//   b) non-intptr_t -> pointer   : needs 1 bitcast then 1 inttoptr\n// Note: for cases 3b and 4b two casts are required since LLVM casts do not\n// allow casting directly between pointer types and non-integer non-pointer\n// types.\nstatic Value *createCastsForTypeOfSameSize(CGBuilderTy &Builder,\n                                           const llvm::DataLayout &DL,\n                                           Value *Src, llvm::Type *DstTy,\n                                           StringRef Name = \"\") {\n  auto SrcTy = Src->getType();\n\n  // Case 1.\n  if (!SrcTy->isPointerTy() && !DstTy->isPointerTy())\n    return Builder.CreateBitCast(Src, DstTy, Name);\n\n  // Case 2.\n  if (SrcTy->isPointerTy() && DstTy->isPointerTy())\n    return Builder.CreatePointerBitCastOrAddrSpaceCast(Src, DstTy, Name);\n\n  // Case 3.\n  if (SrcTy->isPointerTy() && !DstTy->isPointerTy()) {\n    // Case 3b.\n    if (!DstTy->isIntegerTy())\n      Src = Builder.CreatePtrToInt(Src, DL.getIntPtrType(SrcTy));\n    // Cases 3a and 3b.\n    return Builder.CreateBitOrPointerCast(Src, DstTy, Name);\n  }\n\n  // Case 4b.\n  if (!SrcTy->isIntegerTy())\n    Src = Builder.CreateBitCast(Src, DL.getIntPtrType(DstTy));\n  // Cases 4a and 4b.\n  return Builder.CreateIntToPtr(Src, DstTy, Name);\n}\n\nValue *ScalarExprEmitter::VisitAsTypeExpr(AsTypeExpr *E) {\n  Value *Src  = CGF.EmitScalarExpr(E->getSrcExpr());\n  llvm::Type *DstTy = ConvertType(E->getType());\n\n  llvm::Type *SrcTy = Src->getType();\n  unsigned NumElementsSrc =\n      isa<llvm::VectorType>(SrcTy)\n          ? cast<llvm::FixedVectorType>(SrcTy)->getNumElements()\n          : 0;\n  unsigned NumElementsDst =\n      isa<llvm::VectorType>(DstTy)\n          ? cast<llvm::FixedVectorType>(DstTy)->getNumElements()\n          : 0;\n\n  // Going from vec3 to non-vec3 is a special case and requires a shuffle\n  // vector to get a vec4, then a bitcast if the target type is different.\n  if (NumElementsSrc == 3 && NumElementsDst != 3) {\n    Src = ConvertVec3AndVec4(Builder, CGF, Src, 4);\n\n    if (!CGF.CGM.getCodeGenOpts().PreserveVec3Type) {\n      Src = createCastsForTypeOfSameSize(Builder, CGF.CGM.getDataLayout(), Src,\n                                         DstTy);\n    }\n\n    Src->setName(\"astype\");\n    return Src;\n  }\n\n  // Going from non-vec3 to vec3 is a special case and requires a bitcast\n  // to vec4 if the original type is not vec4, then a shuffle vector to\n  // get a vec3.\n  if (NumElementsSrc != 3 && NumElementsDst == 3) {\n    if (!CGF.CGM.getCodeGenOpts().PreserveVec3Type) {\n      auto *Vec4Ty = llvm::FixedVectorType::get(\n          cast<llvm::VectorType>(DstTy)->getElementType(), 4);\n      Src = createCastsForTypeOfSameSize(Builder, CGF.CGM.getDataLayout(), Src,\n                                         Vec4Ty);\n    }\n\n    Src = ConvertVec3AndVec4(Builder, CGF, Src, 3);\n    Src->setName(\"astype\");\n    return Src;\n  }\n\n  return createCastsForTypeOfSameSize(Builder, CGF.CGM.getDataLayout(),\n                                      Src, DstTy, \"astype\");\n}\n\nValue *ScalarExprEmitter::VisitAtomicExpr(AtomicExpr *E) {\n  return CGF.EmitAtomicExpr(E).getScalarVal();\n}\n\n//===----------------------------------------------------------------------===//\n//                         Entry Point into this File\n//===----------------------------------------------------------------------===//\n\n/// Emit the computation of the specified expression of scalar type, ignoring\n/// the result.\nValue *CodeGenFunction::EmitScalarExpr(const Expr *E, bool IgnoreResultAssign) {\n  assert(E && hasScalarEvaluationKind(E->getType()) &&\n         \"Invalid scalar expression to emit\");\n\n  return ScalarExprEmitter(*this, IgnoreResultAssign)\n      .Visit(const_cast<Expr *>(E));\n}\n\n/// Emit a conversion from the specified type to the specified destination type,\n/// both of which are LLVM scalar types.\nValue *CodeGenFunction::EmitScalarConversion(Value *Src, QualType SrcTy,\n                                             QualType DstTy,\n                                             SourceLocation Loc) {\n  assert(hasScalarEvaluationKind(SrcTy) && hasScalarEvaluationKind(DstTy) &&\n         \"Invalid scalar expression to emit\");\n  return ScalarExprEmitter(*this).EmitScalarConversion(Src, SrcTy, DstTy, Loc);\n}\n\n/// Emit a conversion from the specified complex type to the specified\n/// destination type, where the destination type is an LLVM scalar type.\nValue *CodeGenFunction::EmitComplexToScalarConversion(ComplexPairTy Src,\n                                                      QualType SrcTy,\n                                                      QualType DstTy,\n                                                      SourceLocation Loc) {\n  assert(SrcTy->isAnyComplexType() && hasScalarEvaluationKind(DstTy) &&\n         \"Invalid complex -> scalar conversion\");\n  return ScalarExprEmitter(*this)\n      .EmitComplexToScalarConversion(Src, SrcTy, DstTy, Loc);\n}\n\n\nllvm::Value *CodeGenFunction::\nEmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                        bool isInc, bool isPre) {\n  return ScalarExprEmitter(*this).EmitScalarPrePostIncDec(E, LV, isInc, isPre);\n}\n\nLValue CodeGenFunction::EmitObjCIsaExpr(const ObjCIsaExpr *E) {\n  // object->isa or (*object).isa\n  // Generate code as for: *(Class*)object\n\n  Expr *BaseExpr = E->getBase();\n  Address Addr = Address::invalid();\n  if (BaseExpr->isRValue()) {\n    Addr = Address(EmitScalarExpr(BaseExpr), getPointerAlign());\n  } else {\n    Addr = EmitLValue(BaseExpr).getAddress(*this);\n  }\n\n  // Cast the address to Class*.\n  Addr = Builder.CreateElementBitCast(Addr, ConvertType(E->getType()));\n  return MakeAddrLValue(Addr, E->getType());\n}\n\n\nLValue CodeGenFunction::EmitCompoundAssignmentLValue(\n                                            const CompoundAssignOperator *E) {\n  ScalarExprEmitter Scalar(*this);\n  Value *Result = nullptr;\n  switch (E->getOpcode()) {\n#define COMPOUND_OP(Op)                                                       \\\n    case BO_##Op##Assign:                                                     \\\n      return Scalar.EmitCompoundAssignLValue(E, &ScalarExprEmitter::Emit##Op, \\\n                                             Result)\n  COMPOUND_OP(Mul);\n  COMPOUND_OP(Div);\n  COMPOUND_OP(Rem);\n  COMPOUND_OP(Add);\n  COMPOUND_OP(Sub);\n  COMPOUND_OP(Shl);\n  COMPOUND_OP(Shr);\n  COMPOUND_OP(And);\n  COMPOUND_OP(Xor);\n  COMPOUND_OP(Or);\n#undef COMPOUND_OP\n\n  case BO_PtrMemD:\n  case BO_PtrMemI:\n  case BO_Mul:\n  case BO_Div:\n  case BO_Rem:\n  case BO_Add:\n  case BO_Sub:\n  case BO_Shl:\n  case BO_Shr:\n  case BO_LT:\n  case BO_GT:\n  case BO_LE:\n  case BO_GE:\n  case BO_EQ:\n  case BO_NE:\n  case BO_Cmp:\n  case BO_And:\n  case BO_Xor:\n  case BO_Or:\n  case BO_LAnd:\n  case BO_LOr:\n  case BO_Assign:\n  case BO_Comma:\n    llvm_unreachable(\"Not valid compound assignment operators\");\n  }\n\n  llvm_unreachable(\"Unhandled compound assignment operator\");\n}\n\nstruct GEPOffsetAndOverflow {\n  // The total (signed) byte offset for the GEP.\n  llvm::Value *TotalOffset;\n  // The offset overflow flag - true if the total offset overflows.\n  llvm::Value *OffsetOverflows;\n};\n\n/// Evaluate given GEPVal, which is either an inbounds GEP, or a constant,\n/// and compute the total offset it applies from it's base pointer BasePtr.\n/// Returns offset in bytes and a boolean flag whether an overflow happened\n/// during evaluation.\nstatic GEPOffsetAndOverflow EmitGEPOffsetInBytes(Value *BasePtr, Value *GEPVal,\n                                                 llvm::LLVMContext &VMContext,\n                                                 CodeGenModule &CGM,\n                                                 CGBuilderTy &Builder) {\n  const auto &DL = CGM.getDataLayout();\n\n  // The total (signed) byte offset for the GEP.\n  llvm::Value *TotalOffset = nullptr;\n\n  // Was the GEP already reduced to a constant?\n  if (isa<llvm::Constant>(GEPVal)) {\n    // Compute the offset by casting both pointers to integers and subtracting:\n    // GEPVal = BasePtr + ptr(Offset) <--> Offset = int(GEPVal) - int(BasePtr)\n    Value *BasePtr_int =\n        Builder.CreatePtrToInt(BasePtr, DL.getIntPtrType(BasePtr->getType()));\n    Value *GEPVal_int =\n        Builder.CreatePtrToInt(GEPVal, DL.getIntPtrType(GEPVal->getType()));\n    TotalOffset = Builder.CreateSub(GEPVal_int, BasePtr_int);\n    return {TotalOffset, /*OffsetOverflows=*/Builder.getFalse()};\n  }\n\n  auto *GEP = cast<llvm::GEPOperator>(GEPVal);\n  assert(GEP->getPointerOperand() == BasePtr &&\n         \"BasePtr must be the the base of the GEP.\");\n  assert(GEP->isInBounds() && \"Expected inbounds GEP\");\n\n  auto *IntPtrTy = DL.getIntPtrType(GEP->getPointerOperandType());\n\n  // Grab references to the signed add/mul overflow intrinsics for intptr_t.\n  auto *Zero = llvm::ConstantInt::getNullValue(IntPtrTy);\n  auto *SAddIntrinsic =\n      CGM.getIntrinsic(llvm::Intrinsic::sadd_with_overflow, IntPtrTy);\n  auto *SMulIntrinsic =\n      CGM.getIntrinsic(llvm::Intrinsic::smul_with_overflow, IntPtrTy);\n\n  // The offset overflow flag - true if the total offset overflows.\n  llvm::Value *OffsetOverflows = Builder.getFalse();\n\n  /// Return the result of the given binary operation.\n  auto eval = [&](BinaryOperator::Opcode Opcode, llvm::Value *LHS,\n                  llvm::Value *RHS) -> llvm::Value * {\n    assert((Opcode == BO_Add || Opcode == BO_Mul) && \"Can't eval binop\");\n\n    // If the operands are constants, return a constant result.\n    if (auto *LHSCI = dyn_cast<llvm::ConstantInt>(LHS)) {\n      if (auto *RHSCI = dyn_cast<llvm::ConstantInt>(RHS)) {\n        llvm::APInt N;\n        bool HasOverflow = mayHaveIntegerOverflow(LHSCI, RHSCI, Opcode,\n                                                  /*Signed=*/true, N);\n        if (HasOverflow)\n          OffsetOverflows = Builder.getTrue();\n        return llvm::ConstantInt::get(VMContext, N);\n      }\n    }\n\n    // Otherwise, compute the result with checked arithmetic.\n    auto *ResultAndOverflow = Builder.CreateCall(\n        (Opcode == BO_Add) ? SAddIntrinsic : SMulIntrinsic, {LHS, RHS});\n    OffsetOverflows = Builder.CreateOr(\n        Builder.CreateExtractValue(ResultAndOverflow, 1), OffsetOverflows);\n    return Builder.CreateExtractValue(ResultAndOverflow, 0);\n  };\n\n  // Determine the total byte offset by looking at each GEP operand.\n  for (auto GTI = llvm::gep_type_begin(GEP), GTE = llvm::gep_type_end(GEP);\n       GTI != GTE; ++GTI) {\n    llvm::Value *LocalOffset;\n    auto *Index = GTI.getOperand();\n    // Compute the local offset contributed by this indexing step:\n    if (auto *STy = GTI.getStructTypeOrNull()) {\n      // For struct indexing, the local offset is the byte position of the\n      // specified field.\n      unsigned FieldNo = cast<llvm::ConstantInt>(Index)->getZExtValue();\n      LocalOffset = llvm::ConstantInt::get(\n          IntPtrTy, DL.getStructLayout(STy)->getElementOffset(FieldNo));\n    } else {\n      // Otherwise this is array-like indexing. The local offset is the index\n      // multiplied by the element size.\n      auto *ElementSize = llvm::ConstantInt::get(\n          IntPtrTy, DL.getTypeAllocSize(GTI.getIndexedType()));\n      auto *IndexS = Builder.CreateIntCast(Index, IntPtrTy, /*isSigned=*/true);\n      LocalOffset = eval(BO_Mul, ElementSize, IndexS);\n    }\n\n    // If this is the first offset, set it as the total offset. Otherwise, add\n    // the local offset into the running total.\n    if (!TotalOffset || TotalOffset == Zero)\n      TotalOffset = LocalOffset;\n    else\n      TotalOffset = eval(BO_Add, TotalOffset, LocalOffset);\n  }\n\n  return {TotalOffset, OffsetOverflows};\n}\n\nValue *\nCodeGenFunction::EmitCheckedInBoundsGEP(Value *Ptr, ArrayRef<Value *> IdxList,\n                                        bool SignedIndices, bool IsSubtraction,\n                                        SourceLocation Loc, const Twine &Name) {\n  Value *GEPVal = Builder.CreateInBoundsGEP(Ptr, IdxList, Name);\n\n  // If the pointer overflow sanitizer isn't enabled, do nothing.\n  if (!SanOpts.has(SanitizerKind::PointerOverflow))\n    return GEPVal;\n\n  llvm::Type *PtrTy = Ptr->getType();\n\n  // Perform nullptr-and-offset check unless the nullptr is defined.\n  bool PerformNullCheck = !NullPointerIsDefined(\n      Builder.GetInsertBlock()->getParent(), PtrTy->getPointerAddressSpace());\n  // Check for overflows unless the GEP got constant-folded,\n  // and only in the default address space\n  bool PerformOverflowCheck =\n      !isa<llvm::Constant>(GEPVal) && PtrTy->getPointerAddressSpace() == 0;\n\n  if (!(PerformNullCheck || PerformOverflowCheck))\n    return GEPVal;\n\n  const auto &DL = CGM.getDataLayout();\n\n  SanitizerScope SanScope(this);\n  llvm::Type *IntPtrTy = DL.getIntPtrType(PtrTy);\n\n  GEPOffsetAndOverflow EvaluatedGEP =\n      EmitGEPOffsetInBytes(Ptr, GEPVal, getLLVMContext(), CGM, Builder);\n\n  assert((!isa<llvm::Constant>(EvaluatedGEP.TotalOffset) ||\n          EvaluatedGEP.OffsetOverflows == Builder.getFalse()) &&\n         \"If the offset got constant-folded, we don't expect that there was an \"\n         \"overflow.\");\n\n  auto *Zero = llvm::ConstantInt::getNullValue(IntPtrTy);\n\n  // Common case: if the total offset is zero, and we are using C++ semantics,\n  // where nullptr+0 is defined, don't emit a check.\n  if (EvaluatedGEP.TotalOffset == Zero && CGM.getLangOpts().CPlusPlus)\n    return GEPVal;\n\n  // Now that we've computed the total offset, add it to the base pointer (with\n  // wrapping semantics).\n  auto *IntPtr = Builder.CreatePtrToInt(Ptr, IntPtrTy);\n  auto *ComputedGEP = Builder.CreateAdd(IntPtr, EvaluatedGEP.TotalOffset);\n\n  llvm::SmallVector<std::pair<llvm::Value *, SanitizerMask>, 2> Checks;\n\n  if (PerformNullCheck) {\n    // In C++, if the base pointer evaluates to a null pointer value,\n    // the only valid  pointer this inbounds GEP can produce is also\n    // a null pointer, so the offset must also evaluate to zero.\n    // Likewise, if we have non-zero base pointer, we can not get null pointer\n    // as a result, so the offset can not be -intptr_t(BasePtr).\n    // In other words, both pointers are either null, or both are non-null,\n    // or the behaviour is undefined.\n    //\n    // C, however, is more strict in this regard, and gives more\n    // optimization opportunities: in C, additionally, nullptr+0 is undefined.\n    // So both the input to the 'gep inbounds' AND the output must not be null.\n    auto *BaseIsNotNullptr = Builder.CreateIsNotNull(Ptr);\n    auto *ResultIsNotNullptr = Builder.CreateIsNotNull(ComputedGEP);\n    auto *Valid =\n        CGM.getLangOpts().CPlusPlus\n            ? Builder.CreateICmpEQ(BaseIsNotNullptr, ResultIsNotNullptr)\n            : Builder.CreateAnd(BaseIsNotNullptr, ResultIsNotNullptr);\n    Checks.emplace_back(Valid, SanitizerKind::PointerOverflow);\n  }\n\n  if (PerformOverflowCheck) {\n    // The GEP is valid if:\n    // 1) The total offset doesn't overflow, and\n    // 2) The sign of the difference between the computed address and the base\n    // pointer matches the sign of the total offset.\n    llvm::Value *ValidGEP;\n    auto *NoOffsetOverflow = Builder.CreateNot(EvaluatedGEP.OffsetOverflows);\n    if (SignedIndices) {\n      // GEP is computed as `unsigned base + signed offset`, therefore:\n      // * If offset was positive, then the computed pointer can not be\n      //   [unsigned] less than the base pointer, unless it overflowed.\n      // * If offset was negative, then the computed pointer can not be\n      //   [unsigned] greater than the bas pointere, unless it overflowed.\n      auto *PosOrZeroValid = Builder.CreateICmpUGE(ComputedGEP, IntPtr);\n      auto *PosOrZeroOffset =\n          Builder.CreateICmpSGE(EvaluatedGEP.TotalOffset, Zero);\n      llvm::Value *NegValid = Builder.CreateICmpULT(ComputedGEP, IntPtr);\n      ValidGEP =\n          Builder.CreateSelect(PosOrZeroOffset, PosOrZeroValid, NegValid);\n    } else if (!IsSubtraction) {\n      // GEP is computed as `unsigned base + unsigned offset`,  therefore the\n      // computed pointer can not be [unsigned] less than base pointer,\n      // unless there was an overflow.\n      // Equivalent to `@llvm.uadd.with.overflow(%base, %offset)`.\n      ValidGEP = Builder.CreateICmpUGE(ComputedGEP, IntPtr);\n    } else {\n      // GEP is computed as `unsigned base - unsigned offset`, therefore the\n      // computed pointer can not be [unsigned] greater than base pointer,\n      // unless there was an overflow.\n      // Equivalent to `@llvm.usub.with.overflow(%base, sub(0, %offset))`.\n      ValidGEP = Builder.CreateICmpULE(ComputedGEP, IntPtr);\n    }\n    ValidGEP = Builder.CreateAnd(ValidGEP, NoOffsetOverflow);\n    Checks.emplace_back(ValidGEP, SanitizerKind::PointerOverflow);\n  }\n\n  assert(!Checks.empty() && \"Should have produced some checks.\");\n\n  llvm::Constant *StaticArgs[] = {EmitCheckSourceLocation(Loc)};\n  // Pass the computed GEP to the runtime to avoid emitting poisoned arguments.\n  llvm::Value *DynamicArgs[] = {IntPtr, ComputedGEP};\n  EmitCheck(Checks, SanitizerHandler::PointerOverflow, StaticArgs, DynamicArgs);\n\n  return GEPVal;\n}\n"}, "38": {"id": 38, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGLoopInfo.h", "content": "//===---- CGLoopInfo.h - LLVM CodeGen for loop metadata -*- C++ -*---------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This is the internal state used for llvm translation for loop statement\n// metadata.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CGLOOPINFO_H\n#define LLVM_CLANG_LIB_CODEGEN_CGLOOPINFO_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/Compiler.h\"\n\nnamespace llvm {\nclass BasicBlock;\nclass Instruction;\nclass MDNode;\n} // end namespace llvm\n\nnamespace clang {\nclass Attr;\nclass ASTContext;\nclass CodeGenOptions;\nnamespace CodeGen {\n\n/// Attributes that may be specified on loops.\nstruct LoopAttributes {\n  explicit LoopAttributes(bool IsParallel = false);\n  void clear();\n\n  /// Generate llvm.loop.parallel metadata for loads and stores.\n  bool IsParallel;\n\n  /// State of loop vectorization or unrolling.\n  enum LVEnableState { Unspecified, Enable, Disable, Full };\n\n  /// Value for llvm.loop.vectorize.enable metadata.\n  LVEnableState VectorizeEnable;\n\n  /// Value for llvm.loop.unroll.* metadata (enable, disable, or full).\n  LVEnableState UnrollEnable;\n\n  /// Value for llvm.loop.unroll_and_jam.* metadata (enable, disable, or full).\n  LVEnableState UnrollAndJamEnable;\n\n  /// Value for llvm.loop.vectorize.predicate metadata\n  LVEnableState VectorizePredicateEnable;\n\n  /// Value for llvm.loop.vectorize.width metadata.\n  unsigned VectorizeWidth;\n\n  // Value for llvm.loop.vectorize.scalable.enable\n  LVEnableState VectorizeScalable;\n\n  /// Value for llvm.loop.interleave.count metadata.\n  unsigned InterleaveCount;\n\n  /// llvm.unroll.\n  unsigned UnrollCount;\n\n  /// llvm.unroll.\n  unsigned UnrollAndJamCount;\n\n  /// Value for llvm.loop.distribute.enable metadata.\n  LVEnableState DistributeEnable;\n\n  /// Value for llvm.loop.pipeline.disable metadata.\n  bool PipelineDisabled;\n\n  /// Value for llvm.loop.pipeline.iicount metadata.\n  unsigned PipelineInitiationInterval;\n\n  /// Value for whether the loop is required to make progress.\n  bool MustProgress;\n};\n\n/// Information used when generating a structured loop.\nclass LoopInfo {\npublic:\n  /// Construct a new LoopInfo for the loop with entry Header.\n  LoopInfo(llvm::BasicBlock *Header, const LoopAttributes &Attrs,\n           const llvm::DebugLoc &StartLoc, const llvm::DebugLoc &EndLoc,\n           LoopInfo *Parent);\n\n  /// Get the loop id metadata for this loop.\n  llvm::MDNode *getLoopID() const { return TempLoopID.get(); }\n\n  /// Get the header block of this loop.\n  llvm::BasicBlock *getHeader() const { return Header; }\n\n  /// Get the set of attributes active for this loop.\n  const LoopAttributes &getAttributes() const { return Attrs; }\n\n  /// Return this loop's access group or nullptr if it does not have one.\n  llvm::MDNode *getAccessGroup() const { return AccGroup; }\n\n  /// Create the loop's metadata. Must be called after its nested loops have\n  /// been processed.\n  void finish();\n\nprivate:\n  /// Loop ID metadata.\n  llvm::TempMDTuple TempLoopID;\n  /// Header block of this loop.\n  llvm::BasicBlock *Header;\n  /// The attributes for this loop.\n  LoopAttributes Attrs;\n  /// The access group for memory accesses parallel to this loop.\n  llvm::MDNode *AccGroup = nullptr;\n  /// Start location of this loop.\n  llvm::DebugLoc StartLoc;\n  /// End location of this loop.\n  llvm::DebugLoc EndLoc;\n  /// The next outer loop, or nullptr if this is the outermost loop.\n  LoopInfo *Parent;\n  /// If this loop has unroll-and-jam metadata, this can be set by the inner\n  /// loop's LoopInfo to set the llvm.loop.unroll_and_jam.followup_inner\n  /// metadata.\n  llvm::MDNode *UnrollAndJamInnerFollowup = nullptr;\n\n  /// Create a LoopID without any transformations.\n  llvm::MDNode *\n  createLoopPropertiesMetadata(llvm::ArrayRef<llvm::Metadata *> LoopProperties);\n\n  /// Create a LoopID for transformations.\n  ///\n  /// The methods call each other in case multiple transformations are applied\n  /// to a loop. The transformation first to be applied will use LoopID of the\n  /// next transformation in its followup attribute.\n  ///\n  /// @param Attrs             The loop's transformations.\n  /// @param LoopProperties    Non-transformation properties such as debug\n  ///                          location, parallel accesses and disabled\n  ///                          transformations. These are added to the returned\n  ///                          LoopID.\n  /// @param HasUserTransforms [out] Set to true if the returned MDNode encodes\n  ///                          at least one transformation.\n  ///\n  /// @return A LoopID (metadata node) that can be used for the llvm.loop\n  ///         annotation or followup-attribute.\n  /// @{\n  llvm::MDNode *\n  createPipeliningMetadata(const LoopAttributes &Attrs,\n                           llvm::ArrayRef<llvm::Metadata *> LoopProperties,\n                           bool &HasUserTransforms);\n  llvm::MDNode *\n  createPartialUnrollMetadata(const LoopAttributes &Attrs,\n                              llvm::ArrayRef<llvm::Metadata *> LoopProperties,\n                              bool &HasUserTransforms);\n  llvm::MDNode *\n  createUnrollAndJamMetadata(const LoopAttributes &Attrs,\n                             llvm::ArrayRef<llvm::Metadata *> LoopProperties,\n                             bool &HasUserTransforms);\n  llvm::MDNode *\n  createLoopVectorizeMetadata(const LoopAttributes &Attrs,\n                              llvm::ArrayRef<llvm::Metadata *> LoopProperties,\n                              bool &HasUserTransforms);\n  llvm::MDNode *\n  createLoopDistributeMetadata(const LoopAttributes &Attrs,\n                               llvm::ArrayRef<llvm::Metadata *> LoopProperties,\n                               bool &HasUserTransforms);\n  llvm::MDNode *\n  createFullUnrollMetadata(const LoopAttributes &Attrs,\n                           llvm::ArrayRef<llvm::Metadata *> LoopProperties,\n                           bool &HasUserTransforms);\n  /// @}\n\n  /// Create a LoopID for this loop, including transformation-unspecific\n  /// metadata such as debug location.\n  ///\n  /// @param Attrs             This loop's attributes and transformations.\n  /// @param LoopProperties    Additional non-transformation properties to add\n  ///                          to the LoopID, such as transformation-specific\n  ///                          metadata that are not covered by @p Attrs.\n  /// @param HasUserTransforms [out] Set to true if the returned MDNode encodes\n  ///                          at least one transformation.\n  ///\n  /// @return A LoopID (metadata node) that can be used for the llvm.loop\n  ///         annotation.\n  llvm::MDNode *createMetadata(const LoopAttributes &Attrs,\n                               llvm::ArrayRef<llvm::Metadata *> LoopProperties,\n                               bool &HasUserTransforms);\n};\n\n/// A stack of loop information corresponding to loop nesting levels.\n/// This stack can be used to prepare attributes which are applied when a loop\n/// is emitted.\nclass LoopInfoStack {\n  LoopInfoStack(const LoopInfoStack &) = delete;\n  void operator=(const LoopInfoStack &) = delete;\n\npublic:\n  LoopInfoStack() {}\n\n  /// Begin a new structured loop. The set of staged attributes will be\n  /// applied to the loop and then cleared.\n  void push(llvm::BasicBlock *Header, const llvm::DebugLoc &StartLoc,\n            const llvm::DebugLoc &EndLoc);\n\n  /// Begin a new structured loop. Stage attributes from the Attrs list.\n  /// The staged attributes are applied to the loop and then cleared.\n  void push(llvm::BasicBlock *Header, clang::ASTContext &Ctx,\n            const clang::CodeGenOptions &CGOpts,\n            llvm::ArrayRef<const Attr *> Attrs, const llvm::DebugLoc &StartLoc,\n            const llvm::DebugLoc &EndLoc, bool MustProgress = false);\n\n  /// End the current loop.\n  void pop();\n\n  /// Return the top loop id metadata.\n  llvm::MDNode *getCurLoopID() const { return getInfo().getLoopID(); }\n\n  /// Return true if the top loop is parallel.\n  bool getCurLoopParallel() const {\n    return hasInfo() ? getInfo().getAttributes().IsParallel : false;\n  }\n\n  /// Function called by the CodeGenFunction when an instruction is\n  /// created.\n  void InsertHelper(llvm::Instruction *I) const;\n\n  /// Set the next pushed loop as parallel.\n  void setParallel(bool Enable = true) { StagedAttrs.IsParallel = Enable; }\n\n  /// Set the next pushed loop 'vectorize.enable'\n  void setVectorizeEnable(bool Enable = true) {\n    StagedAttrs.VectorizeEnable =\n        Enable ? LoopAttributes::Enable : LoopAttributes::Disable;\n  }\n\n  /// Set the next pushed loop as a distribution candidate.\n  void setDistributeState(bool Enable = true) {\n    StagedAttrs.DistributeEnable =\n        Enable ? LoopAttributes::Enable : LoopAttributes::Disable;\n  }\n\n  /// Set the next pushed loop unroll state.\n  void setUnrollState(const LoopAttributes::LVEnableState &State) {\n    StagedAttrs.UnrollEnable = State;\n  }\n\n  /// Set the next pushed vectorize predicate state.\n  void setVectorizePredicateState(const LoopAttributes::LVEnableState &State) {\n    StagedAttrs.VectorizePredicateEnable = State;\n  }\n\n  /// Set the next pushed loop unroll_and_jam state.\n  void setUnrollAndJamState(const LoopAttributes::LVEnableState &State) {\n    StagedAttrs.UnrollAndJamEnable = State;\n  }\n\n  /// Set the vectorize width for the next loop pushed.\n  void setVectorizeWidth(unsigned W) { StagedAttrs.VectorizeWidth = W; }\n\n  void setVectorizeScalable(const LoopAttributes::LVEnableState &State) {\n    StagedAttrs.VectorizeScalable = State;\n  }\n\n  /// Set the interleave count for the next loop pushed.\n  void setInterleaveCount(unsigned C) { StagedAttrs.InterleaveCount = C; }\n\n  /// Set the unroll count for the next loop pushed.\n  void setUnrollCount(unsigned C) { StagedAttrs.UnrollCount = C; }\n\n  /// \\brief Set the unroll count for the next loop pushed.\n  void setUnrollAndJamCount(unsigned C) { StagedAttrs.UnrollAndJamCount = C; }\n\n  /// Set the pipeline disabled state.\n  void setPipelineDisabled(bool S) { StagedAttrs.PipelineDisabled = S; }\n\n  /// Set the pipeline initiation interval.\n  void setPipelineInitiationInterval(unsigned C) {\n    StagedAttrs.PipelineInitiationInterval = C;\n  }\n\n  /// Set no progress for the next loop pushed.\n  void setMustProgress(bool P) { StagedAttrs.MustProgress = P; }\n\nprivate:\n  /// Returns true if there is LoopInfo on the stack.\n  bool hasInfo() const { return !Active.empty(); }\n  /// Return the LoopInfo for the current loop. HasInfo should be called\n  /// first to ensure LoopInfo is present.\n  const LoopInfo &getInfo() const { return *Active.back(); }\n  /// The set of attributes that will be applied to the next pushed loop.\n  LoopAttributes StagedAttrs;\n  /// Stack of active loops.\n  llvm::SmallVector<std::unique_ptr<LoopInfo>, 4> Active;\n};\n\n} // end namespace CodeGen\n} // end namespace clang\n\n#endif\n"}, "40": {"id": 40, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "content": "//===-- CodeGenFunction.h - Per-Function state for LLVM CodeGen -*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This is the internal per-function state used for llvm translation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CODEGENFUNCTION_H\n#define LLVM_CLANG_LIB_CODEGEN_CODEGENFUNCTION_H\n\n#include \"CGBuilder.h\"\n#include \"CGDebugInfo.h\"\n#include \"CGLoopInfo.h\"\n#include \"CGValue.h\"\n#include \"CodeGenModule.h\"\n#include \"CodeGenPGO.h\"\n#include \"EHScopeStack.h\"\n#include \"VarBypassDetector.h\"\n#include \"clang/AST/CharUnits.h\"\n#include \"clang/AST/CurrentSourceLocExprScope.h\"\n#include \"clang/AST/ExprCXX.h\"\n#include \"clang/AST/ExprObjC.h\"\n#include \"clang/AST/ExprOpenMP.h\"\n#include \"clang/AST/StmtOpenMP.h\"\n#include \"clang/AST/Type.h\"\n#include \"clang/Basic/ABI.h\"\n#include \"clang/Basic/CapturedStmt.h\"\n#include \"clang/Basic/CodeGenOptions.h\"\n#include \"clang/Basic/OpenMPKinds.h\"\n#include \"clang/Basic/TargetInfo.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/MapVector.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/Frontend/OpenMP/OMPIRBuilder.h\"\n#include \"llvm/IR/ValueHandle.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Transforms/Utils/SanitizerStats.h\"\n\nnamespace llvm {\nclass BasicBlock;\nclass LLVMContext;\nclass MDNode;\nclass Module;\nclass SwitchInst;\nclass Twine;\nclass Value;\nclass CanonicalLoopInfo;\n}\n\nnamespace clang {\nclass ASTContext;\nclass BlockDecl;\nclass CXXDestructorDecl;\nclass CXXForRangeStmt;\nclass CXXTryStmt;\nclass Decl;\nclass LabelDecl;\nclass EnumConstantDecl;\nclass FunctionDecl;\nclass FunctionProtoType;\nclass LabelStmt;\nclass ObjCContainerDecl;\nclass ObjCInterfaceDecl;\nclass ObjCIvarDecl;\nclass ObjCMethodDecl;\nclass ObjCImplementationDecl;\nclass ObjCPropertyImplDecl;\nclass TargetInfo;\nclass VarDecl;\nclass ObjCForCollectionStmt;\nclass ObjCAtTryStmt;\nclass ObjCAtThrowStmt;\nclass ObjCAtSynchronizedStmt;\nclass ObjCAutoreleasePoolStmt;\nclass OMPUseDevicePtrClause;\nclass OMPUseDeviceAddrClause;\nclass ReturnsNonNullAttr;\nclass SVETypeFlags;\nclass OMPExecutableDirective;\n\nnamespace analyze_os_log {\nclass OSLogBufferLayout;\n}\n\nnamespace CodeGen {\nclass CodeGenTypes;\nclass CGCallee;\nclass CGFunctionInfo;\nclass CGRecordLayout;\nclass CGBlockInfo;\nclass CGCXXABI;\nclass BlockByrefHelpers;\nclass BlockByrefInfo;\nclass BlockFlags;\nclass BlockFieldFlags;\nclass RegionCodeGenTy;\nclass TargetCodeGenInfo;\nstruct OMPTaskDataTy;\nstruct CGCoroData;\n\n/// The kind of evaluation to perform on values of a particular\n/// type.  Basically, is the code in CGExprScalar, CGExprComplex, or\n/// CGExprAgg?\n///\n/// TODO: should vectors maybe be split out into their own thing?\nenum TypeEvaluationKind {\n  TEK_Scalar,\n  TEK_Complex,\n  TEK_Aggregate\n};\n\n#define LIST_SANITIZER_CHECKS                                                  \\\n  SANITIZER_CHECK(AddOverflow, add_overflow, 0)                                \\\n  SANITIZER_CHECK(BuiltinUnreachable, builtin_unreachable, 0)                  \\\n  SANITIZER_CHECK(CFICheckFail, cfi_check_fail, 0)                             \\\n  SANITIZER_CHECK(DivremOverflow, divrem_overflow, 0)                          \\\n  SANITIZER_CHECK(DynamicTypeCacheMiss, dynamic_type_cache_miss, 0)            \\\n  SANITIZER_CHECK(FloatCastOverflow, float_cast_overflow, 0)                   \\\n  SANITIZER_CHECK(FunctionTypeMismatch, function_type_mismatch, 1)             \\\n  SANITIZER_CHECK(ImplicitConversion, implicit_conversion, 0)                  \\\n  SANITIZER_CHECK(InvalidBuiltin, invalid_builtin, 0)                          \\\n  SANITIZER_CHECK(InvalidObjCCast, invalid_objc_cast, 0)                       \\\n  SANITIZER_CHECK(LoadInvalidValue, load_invalid_value, 0)                     \\\n  SANITIZER_CHECK(MissingReturn, missing_return, 0)                            \\\n  SANITIZER_CHECK(MulOverflow, mul_overflow, 0)                                \\\n  SANITIZER_CHECK(NegateOverflow, negate_overflow, 0)                          \\\n  SANITIZER_CHECK(NullabilityArg, nullability_arg, 0)                          \\\n  SANITIZER_CHECK(NullabilityReturn, nullability_return, 1)                    \\\n  SANITIZER_CHECK(NonnullArg, nonnull_arg, 0)                                  \\\n  SANITIZER_CHECK(NonnullReturn, nonnull_return, 1)                            \\\n  SANITIZER_CHECK(OutOfBounds, out_of_bounds, 0)                               \\\n  SANITIZER_CHECK(PointerOverflow, pointer_overflow, 0)                        \\\n  SANITIZER_CHECK(ShiftOutOfBounds, shift_out_of_bounds, 0)                    \\\n  SANITIZER_CHECK(SubOverflow, sub_overflow, 0)                                \\\n  SANITIZER_CHECK(TypeMismatch, type_mismatch, 1)                              \\\n  SANITIZER_CHECK(AlignmentAssumption, alignment_assumption, 0)                \\\n  SANITIZER_CHECK(VLABoundNotPositive, vla_bound_not_positive, 0)\n\nenum SanitizerHandler {\n#define SANITIZER_CHECK(Enum, Name, Version) Enum,\n  LIST_SANITIZER_CHECKS\n#undef SANITIZER_CHECK\n};\n\n/// Helper class with most of the code for saving a value for a\n/// conditional expression cleanup.\nstruct DominatingLLVMValue {\n  typedef llvm::PointerIntPair<llvm::Value*, 1, bool> saved_type;\n\n  /// Answer whether the given value needs extra work to be saved.\n  static bool needsSaving(llvm::Value *value) {\n    // If it's not an instruction, we don't need to save.\n    if (!isa<llvm::Instruction>(value)) return false;\n\n    // If it's an instruction in the entry block, we don't need to save.\n    llvm::BasicBlock *block = cast<llvm::Instruction>(value)->getParent();\n    return (block != &block->getParent()->getEntryBlock());\n  }\n\n  static saved_type save(CodeGenFunction &CGF, llvm::Value *value);\n  static llvm::Value *restore(CodeGenFunction &CGF, saved_type value);\n};\n\n/// A partial specialization of DominatingValue for llvm::Values that\n/// might be llvm::Instructions.\ntemplate <class T> struct DominatingPointer<T,true> : DominatingLLVMValue {\n  typedef T *type;\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return static_cast<T*>(DominatingLLVMValue::restore(CGF, value));\n  }\n};\n\n/// A specialization of DominatingValue for Address.\ntemplate <> struct DominatingValue<Address> {\n  typedef Address type;\n\n  struct saved_type {\n    DominatingLLVMValue::saved_type SavedValue;\n    CharUnits Alignment;\n  };\n\n  static bool needsSaving(type value) {\n    return DominatingLLVMValue::needsSaving(value.getPointer());\n  }\n  static saved_type save(CodeGenFunction &CGF, type value) {\n    return { DominatingLLVMValue::save(CGF, value.getPointer()),\n             value.getAlignment() };\n  }\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return Address(DominatingLLVMValue::restore(CGF, value.SavedValue),\n                   value.Alignment);\n  }\n};\n\n/// A specialization of DominatingValue for RValue.\ntemplate <> struct DominatingValue<RValue> {\n  typedef RValue type;\n  class saved_type {\n    enum Kind { ScalarLiteral, ScalarAddress, AggregateLiteral,\n                AggregateAddress, ComplexAddress };\n\n    llvm::Value *Value;\n    unsigned K : 3;\n    unsigned Align : 29;\n    saved_type(llvm::Value *v, Kind k, unsigned a = 0)\n      : Value(v), K(k), Align(a) {}\n\n  public:\n    static bool needsSaving(RValue value);\n    static saved_type save(CodeGenFunction &CGF, RValue value);\n    RValue restore(CodeGenFunction &CGF);\n\n    // implementations in CGCleanup.cpp\n  };\n\n  static bool needsSaving(type value) {\n    return saved_type::needsSaving(value);\n  }\n  static saved_type save(CodeGenFunction &CGF, type value) {\n    return saved_type::save(CGF, value);\n  }\n  static type restore(CodeGenFunction &CGF, saved_type value) {\n    return value.restore(CGF);\n  }\n};\n\n/// CodeGenFunction - This class organizes the per-function state that is used\n/// while generating LLVM code.\nclass CodeGenFunction : public CodeGenTypeCache {\n  CodeGenFunction(const CodeGenFunction &) = delete;\n  void operator=(const CodeGenFunction &) = delete;\n\n  friend class CGCXXABI;\npublic:\n  /// A jump destination is an abstract label, branching to which may\n  /// require a jump out through normal cleanups.\n  struct JumpDest {\n    JumpDest() : Block(nullptr), ScopeDepth(), Index(0) {}\n    JumpDest(llvm::BasicBlock *Block,\n             EHScopeStack::stable_iterator Depth,\n             unsigned Index)\n      : Block(Block), ScopeDepth(Depth), Index(Index) {}\n\n    bool isValid() const { return Block != nullptr; }\n    llvm::BasicBlock *getBlock() const { return Block; }\n    EHScopeStack::stable_iterator getScopeDepth() const { return ScopeDepth; }\n    unsigned getDestIndex() const { return Index; }\n\n    // This should be used cautiously.\n    void setScopeDepth(EHScopeStack::stable_iterator depth) {\n      ScopeDepth = depth;\n    }\n\n  private:\n    llvm::BasicBlock *Block;\n    EHScopeStack::stable_iterator ScopeDepth;\n    unsigned Index;\n  };\n\n  CodeGenModule &CGM;  // Per-module state.\n  const TargetInfo &Target;\n\n  // For EH/SEH outlined funclets, this field points to parent's CGF\n  CodeGenFunction *ParentCGF = nullptr;\n\n  typedef std::pair<llvm::Value *, llvm::Value *> ComplexPairTy;\n  LoopInfoStack LoopStack;\n  CGBuilderTy Builder;\n\n  // Stores variables for which we can't generate correct lifetime markers\n  // because of jumps.\n  VarBypassDetector Bypasses;\n\n  /// List of recently emitted OMPCanonicalLoops.\n  ///\n  /// Since OMPCanonicalLoops are nested inside other statements (in particular\n  /// CapturedStmt generated by OMPExecutableDirective and non-perfectly nested\n  /// loops), we cannot directly call OMPEmitOMPCanonicalLoop and receive its\n  /// llvm::CanonicalLoopInfo. Instead, we call EmitStmt and any\n  /// OMPEmitOMPCanonicalLoop called by it will add its CanonicalLoopInfo to\n  /// this stack when done. Entering a new loop requires clearing this list; it\n  /// either means we start parsing a new loop nest (in which case the previous\n  /// loop nest goes out of scope) or a second loop in the same level in which\n  /// case it would be ambiguous into which of the two (or more) loops the loop\n  /// nest would extend.\n  SmallVector<llvm::CanonicalLoopInfo *, 4> OMPLoopNestStack;\n\n  // CodeGen lambda for loops and support for ordered clause\n  typedef llvm::function_ref<void(CodeGenFunction &, const OMPLoopDirective &,\n                                  JumpDest)>\n      CodeGenLoopTy;\n  typedef llvm::function_ref<void(CodeGenFunction &, SourceLocation,\n                                  const unsigned, const bool)>\n      CodeGenOrderedTy;\n\n  // Codegen lambda for loop bounds in worksharing loop constructs\n  typedef llvm::function_ref<std::pair<LValue, LValue>(\n      CodeGenFunction &, const OMPExecutableDirective &S)>\n      CodeGenLoopBoundsTy;\n\n  // Codegen lambda for loop bounds in dispatch-based loop implementation\n  typedef llvm::function_ref<std::pair<llvm::Value *, llvm::Value *>(\n      CodeGenFunction &, const OMPExecutableDirective &S, Address LB,\n      Address UB)>\n      CodeGenDispatchBoundsTy;\n\n  /// CGBuilder insert helper. This function is called after an\n  /// instruction is created using Builder.\n  void InsertHelper(llvm::Instruction *I, const llvm::Twine &Name,\n                    llvm::BasicBlock *BB,\n                    llvm::BasicBlock::iterator InsertPt) const;\n\n  /// CurFuncDecl - Holds the Decl for the current outermost\n  /// non-closure context.\n  const Decl *CurFuncDecl;\n  /// CurCodeDecl - This is the inner-most code context, which includes blocks.\n  const Decl *CurCodeDecl;\n  const CGFunctionInfo *CurFnInfo;\n  QualType FnRetTy;\n  llvm::Function *CurFn = nullptr;\n\n  // Holds coroutine data if the current function is a coroutine. We use a\n  // wrapper to manage its lifetime, so that we don't have to define CGCoroData\n  // in this header.\n  struct CGCoroInfo {\n    std::unique_ptr<CGCoroData> Data;\n    CGCoroInfo();\n    ~CGCoroInfo();\n  };\n  CGCoroInfo CurCoro;\n\n  bool isCoroutine() const {\n    return CurCoro.Data != nullptr;\n  }\n\n  /// CurGD - The GlobalDecl for the current function being compiled.\n  GlobalDecl CurGD;\n\n  /// PrologueCleanupDepth - The cleanup depth enclosing all the\n  /// cleanups associated with the parameters.\n  EHScopeStack::stable_iterator PrologueCleanupDepth;\n\n  /// ReturnBlock - Unified return block.\n  JumpDest ReturnBlock;\n\n  /// ReturnValue - The temporary alloca to hold the return\n  /// value. This is invalid iff the function has no return value.\n  Address ReturnValue = Address::invalid();\n\n  /// ReturnValuePointer - The temporary alloca to hold a pointer to sret.\n  /// This is invalid if sret is not in use.\n  Address ReturnValuePointer = Address::invalid();\n\n  /// If a return statement is being visited, this holds the return statment's\n  /// result expression.\n  const Expr *RetExpr = nullptr;\n\n  /// Return true if a label was seen in the current scope.\n  bool hasLabelBeenSeenInCurrentScope() const {\n    if (CurLexicalScope)\n      return CurLexicalScope->hasLabels();\n    return !LabelMap.empty();\n  }\n\n  /// AllocaInsertPoint - This is an instruction in the entry block before which\n  /// we prefer to insert allocas.\n  llvm::AssertingVH<llvm::Instruction> AllocaInsertPt;\n\n  /// API for captured statement code generation.\n  class CGCapturedStmtInfo {\n  public:\n    explicit CGCapturedStmtInfo(CapturedRegionKind K = CR_Default)\n        : Kind(K), ThisValue(nullptr), CXXThisFieldDecl(nullptr) {}\n    explicit CGCapturedStmtInfo(const CapturedStmt &S,\n                                CapturedRegionKind K = CR_Default)\n      : Kind(K), ThisValue(nullptr), CXXThisFieldDecl(nullptr) {\n\n      RecordDecl::field_iterator Field =\n        S.getCapturedRecordDecl()->field_begin();\n      for (CapturedStmt::const_capture_iterator I = S.capture_begin(),\n                                                E = S.capture_end();\n           I != E; ++I, ++Field) {\n        if (I->capturesThis())\n          CXXThisFieldDecl = *Field;\n        else if (I->capturesVariable())\n          CaptureFields[I->getCapturedVar()->getCanonicalDecl()] = *Field;\n        else if (I->capturesVariableByCopy())\n          CaptureFields[I->getCapturedVar()->getCanonicalDecl()] = *Field;\n      }\n    }\n\n    virtual ~CGCapturedStmtInfo();\n\n    CapturedRegionKind getKind() const { return Kind; }\n\n    virtual void setContextValue(llvm::Value *V) { ThisValue = V; }\n    // Retrieve the value of the context parameter.\n    virtual llvm::Value *getContextValue() const { return ThisValue; }\n\n    /// Lookup the captured field decl for a variable.\n    virtual const FieldDecl *lookup(const VarDecl *VD) const {\n      return CaptureFields.lookup(VD->getCanonicalDecl());\n    }\n\n    bool isCXXThisExprCaptured() const { return getThisFieldDecl() != nullptr; }\n    virtual FieldDecl *getThisFieldDecl() const { return CXXThisFieldDecl; }\n\n    static bool classof(const CGCapturedStmtInfo *) {\n      return true;\n    }\n\n    /// Emit the captured statement body.\n    virtual void EmitBody(CodeGenFunction &CGF, const Stmt *S) {\n      CGF.incrementProfileCounter(S);\n      CGF.EmitStmt(S);\n    }\n\n    /// Get the name of the capture helper.\n    virtual StringRef getHelperName() const { return \"__captured_stmt\"; }\n\n  private:\n    /// The kind of captured statement being generated.\n    CapturedRegionKind Kind;\n\n    /// Keep the map between VarDecl and FieldDecl.\n    llvm::SmallDenseMap<const VarDecl *, FieldDecl *> CaptureFields;\n\n    /// The base address of the captured record, passed in as the first\n    /// argument of the parallel region function.\n    llvm::Value *ThisValue;\n\n    /// Captured 'this' type.\n    FieldDecl *CXXThisFieldDecl;\n  };\n  CGCapturedStmtInfo *CapturedStmtInfo = nullptr;\n\n  /// RAII for correct setting/restoring of CapturedStmtInfo.\n  class CGCapturedStmtRAII {\n  private:\n    CodeGenFunction &CGF;\n    CGCapturedStmtInfo *PrevCapturedStmtInfo;\n  public:\n    CGCapturedStmtRAII(CodeGenFunction &CGF,\n                       CGCapturedStmtInfo *NewCapturedStmtInfo)\n        : CGF(CGF), PrevCapturedStmtInfo(CGF.CapturedStmtInfo) {\n      CGF.CapturedStmtInfo = NewCapturedStmtInfo;\n    }\n    ~CGCapturedStmtRAII() { CGF.CapturedStmtInfo = PrevCapturedStmtInfo; }\n  };\n\n  /// An abstract representation of regular/ObjC call/message targets.\n  class AbstractCallee {\n    /// The function declaration of the callee.\n    const Decl *CalleeDecl;\n\n  public:\n    AbstractCallee() : CalleeDecl(nullptr) {}\n    AbstractCallee(const FunctionDecl *FD) : CalleeDecl(FD) {}\n    AbstractCallee(const ObjCMethodDecl *OMD) : CalleeDecl(OMD) {}\n    bool hasFunctionDecl() const {\n      return dyn_cast_or_null<FunctionDecl>(CalleeDecl);\n    }\n    const Decl *getDecl() const { return CalleeDecl; }\n    unsigned getNumParams() const {\n      if (const auto *FD = dyn_cast<FunctionDecl>(CalleeDecl))\n        return FD->getNumParams();\n      return cast<ObjCMethodDecl>(CalleeDecl)->param_size();\n    }\n    const ParmVarDecl *getParamDecl(unsigned I) const {\n      if (const auto *FD = dyn_cast<FunctionDecl>(CalleeDecl))\n        return FD->getParamDecl(I);\n      return *(cast<ObjCMethodDecl>(CalleeDecl)->param_begin() + I);\n    }\n  };\n\n  /// Sanitizers enabled for this function.\n  SanitizerSet SanOpts;\n\n  /// True if CodeGen currently emits code implementing sanitizer checks.\n  bool IsSanitizerScope = false;\n\n  /// RAII object to set/unset CodeGenFunction::IsSanitizerScope.\n  class SanitizerScope {\n    CodeGenFunction *CGF;\n  public:\n    SanitizerScope(CodeGenFunction *CGF);\n    ~SanitizerScope();\n  };\n\n  /// In C++, whether we are code generating a thunk.  This controls whether we\n  /// should emit cleanups.\n  bool CurFuncIsThunk = false;\n\n  /// In ARC, whether we should autorelease the return value.\n  bool AutoreleaseResult = false;\n\n  /// Whether we processed a Microsoft-style asm block during CodeGen. These can\n  /// potentially set the return value.\n  bool SawAsmBlock = false;\n\n  const NamedDecl *CurSEHParent = nullptr;\n\n  /// True if the current function is an outlined SEH helper. This can be a\n  /// finally block or filter expression.\n  bool IsOutlinedSEHHelper = false;\n\n  /// True if CodeGen currently emits code inside presereved access index\n  /// region.\n  bool IsInPreservedAIRegion = false;\n\n  /// True if the current statement has nomerge attribute.\n  bool InNoMergeAttributedStmt = false;\n\n  /// True if the current function should be marked mustprogress.\n  bool FnIsMustProgress = false;\n\n  /// True if the C++ Standard Requires Progress.\n  bool CPlusPlusWithProgress() {\n    if (CGM.getCodeGenOpts().getFiniteLoops() ==\n        CodeGenOptions::FiniteLoopsKind::Never)\n      return false;\n\n    return getLangOpts().CPlusPlus11 || getLangOpts().CPlusPlus14 ||\n           getLangOpts().CPlusPlus17 || getLangOpts().CPlusPlus20;\n  }\n\n  /// True if the C Standard Requires Progress.\n  bool CWithProgress() {\n    if (CGM.getCodeGenOpts().getFiniteLoops() ==\n        CodeGenOptions::FiniteLoopsKind::Always)\n      return true;\n    if (CGM.getCodeGenOpts().getFiniteLoops() ==\n        CodeGenOptions::FiniteLoopsKind::Never)\n      return false;\n\n    return getLangOpts().C11 || getLangOpts().C17 || getLangOpts().C2x;\n  }\n\n  /// True if the language standard requires progress in functions or\n  /// in infinite loops with non-constant conditionals.\n  bool LanguageRequiresProgress() {\n    return CWithProgress() || CPlusPlusWithProgress();\n  }\n\n  const CodeGen::CGBlockInfo *BlockInfo = nullptr;\n  llvm::Value *BlockPointer = nullptr;\n\n  llvm::DenseMap<const VarDecl *, FieldDecl *> LambdaCaptureFields;\n  FieldDecl *LambdaThisCaptureField = nullptr;\n\n  /// A mapping from NRVO variables to the flags used to indicate\n  /// when the NRVO has been applied to this variable.\n  llvm::DenseMap<const VarDecl *, llvm::Value *> NRVOFlags;\n\n  EHScopeStack EHStack;\n  llvm::SmallVector<char, 256> LifetimeExtendedCleanupStack;\n  llvm::SmallVector<const JumpDest *, 2> SEHTryEpilogueStack;\n\n  llvm::Instruction *CurrentFuncletPad = nullptr;\n\n  class CallLifetimeEnd final : public EHScopeStack::Cleanup {\n    llvm::Value *Addr;\n    llvm::Value *Size;\n\n  public:\n    CallLifetimeEnd(Address addr, llvm::Value *size)\n        : Addr(addr.getPointer()), Size(size) {}\n\n    void Emit(CodeGenFunction &CGF, Flags flags) override {\n      CGF.EmitLifetimeEnd(Size, Addr);\n    }\n  };\n\n  /// Header for data within LifetimeExtendedCleanupStack.\n  struct LifetimeExtendedCleanupHeader {\n    /// The size of the following cleanup object.\n    unsigned Size;\n    /// The kind of cleanup to push: a value from the CleanupKind enumeration.\n    unsigned Kind : 31;\n    /// Whether this is a conditional cleanup.\n    unsigned IsConditional : 1;\n\n    size_t getSize() const { return Size; }\n    CleanupKind getKind() const { return (CleanupKind)Kind; }\n    bool isConditional() const { return IsConditional; }\n  };\n\n  /// i32s containing the indexes of the cleanup destinations.\n  Address NormalCleanupDest = Address::invalid();\n\n  unsigned NextCleanupDestIndex = 1;\n\n  /// EHResumeBlock - Unified block containing a call to llvm.eh.resume.\n  llvm::BasicBlock *EHResumeBlock = nullptr;\n\n  /// The exception slot.  All landing pads write the current exception pointer\n  /// into this alloca.\n  llvm::Value *ExceptionSlot = nullptr;\n\n  /// The selector slot.  Under the MandatoryCleanup model, all landing pads\n  /// write the current selector value into this alloca.\n  llvm::AllocaInst *EHSelectorSlot = nullptr;\n\n  /// A stack of exception code slots. Entering an __except block pushes a slot\n  /// on the stack and leaving pops one. The __exception_code() intrinsic loads\n  /// a value from the top of the stack.\n  SmallVector<Address, 1> SEHCodeSlotStack;\n\n  /// Value returned by __exception_info intrinsic.\n  llvm::Value *SEHInfo = nullptr;\n\n  /// Emits a landing pad for the current EH stack.\n  llvm::BasicBlock *EmitLandingPad();\n\n  llvm::BasicBlock *getInvokeDestImpl();\n\n  /// Parent loop-based directive for scan directive.\n  const OMPExecutableDirective *OMPParentLoopDirectiveForScan = nullptr;\n  llvm::BasicBlock *OMPBeforeScanBlock = nullptr;\n  llvm::BasicBlock *OMPAfterScanBlock = nullptr;\n  llvm::BasicBlock *OMPScanExitBlock = nullptr;\n  llvm::BasicBlock *OMPScanDispatch = nullptr;\n  bool OMPFirstScanLoop = false;\n\n  /// Manages parent directive for scan directives.\n  class ParentLoopDirectiveForScanRegion {\n    CodeGenFunction &CGF;\n    const OMPExecutableDirective *ParentLoopDirectiveForScan;\n\n  public:\n    ParentLoopDirectiveForScanRegion(\n        CodeGenFunction &CGF,\n        const OMPExecutableDirective &ParentLoopDirectiveForScan)\n        : CGF(CGF),\n          ParentLoopDirectiveForScan(CGF.OMPParentLoopDirectiveForScan) {\n      CGF.OMPParentLoopDirectiveForScan = &ParentLoopDirectiveForScan;\n    }\n    ~ParentLoopDirectiveForScanRegion() {\n      CGF.OMPParentLoopDirectiveForScan = ParentLoopDirectiveForScan;\n    }\n  };\n\n  template <class T>\n  typename DominatingValue<T>::saved_type saveValueInCond(T value) {\n    return DominatingValue<T>::save(*this, value);\n  }\n\n  class CGFPOptionsRAII {\n  public:\n    CGFPOptionsRAII(CodeGenFunction &CGF, FPOptions FPFeatures);\n    CGFPOptionsRAII(CodeGenFunction &CGF, const Expr *E);\n    ~CGFPOptionsRAII();\n\n  private:\n    void ConstructorHelper(FPOptions FPFeatures);\n    CodeGenFunction &CGF;\n    FPOptions OldFPFeatures;\n    llvm::fp::ExceptionBehavior OldExcept;\n    llvm::RoundingMode OldRounding;\n    Optional<CGBuilderTy::FastMathFlagGuard> FMFGuard;\n  };\n  FPOptions CurFPFeatures;\n\npublic:\n  /// ObjCEHValueStack - Stack of Objective-C exception values, used for\n  /// rethrows.\n  SmallVector<llvm::Value*, 8> ObjCEHValueStack;\n\n  /// A class controlling the emission of a finally block.\n  class FinallyInfo {\n    /// Where the catchall's edge through the cleanup should go.\n    JumpDest RethrowDest;\n\n    /// A function to call to enter the catch.\n    llvm::FunctionCallee BeginCatchFn;\n\n    /// An i1 variable indicating whether or not the @finally is\n    /// running for an exception.\n    llvm::AllocaInst *ForEHVar;\n\n    /// An i8* variable into which the exception pointer to rethrow\n    /// has been saved.\n    llvm::AllocaInst *SavedExnVar;\n\n  public:\n    void enter(CodeGenFunction &CGF, const Stmt *Finally,\n               llvm::FunctionCallee beginCatchFn,\n               llvm::FunctionCallee endCatchFn, llvm::FunctionCallee rethrowFn);\n    void exit(CodeGenFunction &CGF);\n  };\n\n  /// Returns true inside SEH __try blocks.\n  bool isSEHTryScope() const { return !SEHTryEpilogueStack.empty(); }\n\n  /// Returns true while emitting a cleanuppad.\n  bool isCleanupPadScope() const {\n    return CurrentFuncletPad && isa<llvm::CleanupPadInst>(CurrentFuncletPad);\n  }\n\n  /// pushFullExprCleanup - Push a cleanup to be run at the end of the\n  /// current full-expression.  Safe against the possibility that\n  /// we're currently inside a conditionally-evaluated expression.\n  template <class T, class... As>\n  void pushFullExprCleanup(CleanupKind kind, As... A) {\n    // If we're not in a conditional branch, or if none of the\n    // arguments requires saving, then use the unconditional cleanup.\n    if (!isInConditionalBranch())\n      return EHStack.pushCleanup<T>(kind, A...);\n\n    // Stash values in a tuple so we can guarantee the order of saves.\n    typedef std::tuple<typename DominatingValue<As>::saved_type...> SavedTuple;\n    SavedTuple Saved{saveValueInCond(A)...};\n\n    typedef EHScopeStack::ConditionalCleanup<T, As...> CleanupType;\n    EHStack.pushCleanupTuple<CleanupType>(kind, Saved);\n    initFullExprCleanup();\n  }\n\n  /// Queue a cleanup to be pushed after finishing the current full-expression,\n  /// potentially with an active flag.\n  template <class T, class... As>\n  void pushCleanupAfterFullExpr(CleanupKind Kind, As... A) {\n    if (!isInConditionalBranch())\n      return pushCleanupAfterFullExprWithActiveFlag<T>(Kind, Address::invalid(),\n                                                       A...);\n\n    Address ActiveFlag = createCleanupActiveFlag();\n    assert(!DominatingValue<Address>::needsSaving(ActiveFlag) &&\n           \"cleanup active flag should never need saving\");\n\n    typedef std::tuple<typename DominatingValue<As>::saved_type...> SavedTuple;\n    SavedTuple Saved{saveValueInCond(A)...};\n\n    typedef EHScopeStack::ConditionalCleanup<T, As...> CleanupType;\n    pushCleanupAfterFullExprWithActiveFlag<CleanupType>(Kind, ActiveFlag, Saved);\n  }\n\n  template <class T, class... As>\n  void pushCleanupAfterFullExprWithActiveFlag(CleanupKind Kind,\n                                              Address ActiveFlag, As... A) {\n    LifetimeExtendedCleanupHeader Header = {sizeof(T), Kind,\n                                            ActiveFlag.isValid()};\n\n    size_t OldSize = LifetimeExtendedCleanupStack.size();\n    LifetimeExtendedCleanupStack.resize(\n        LifetimeExtendedCleanupStack.size() + sizeof(Header) + Header.Size +\n        (Header.IsConditional ? sizeof(ActiveFlag) : 0));\n\n    static_assert(sizeof(Header) % alignof(T) == 0,\n                  \"Cleanup will be allocated on misaligned address\");\n    char *Buffer = &LifetimeExtendedCleanupStack[OldSize];\n    new (Buffer) LifetimeExtendedCleanupHeader(Header);\n    new (Buffer + sizeof(Header)) T(A...);\n    if (Header.IsConditional)\n      new (Buffer + sizeof(Header) + sizeof(T)) Address(ActiveFlag);\n  }\n\n  /// Set up the last cleanup that was pushed as a conditional\n  /// full-expression cleanup.\n  void initFullExprCleanup() {\n    initFullExprCleanupWithFlag(createCleanupActiveFlag());\n  }\n\n  void initFullExprCleanupWithFlag(Address ActiveFlag);\n  Address createCleanupActiveFlag();\n\n  /// PushDestructorCleanup - Push a cleanup to call the\n  /// complete-object destructor of an object of the given type at the\n  /// given address.  Does nothing if T is not a C++ class type with a\n  /// non-trivial destructor.\n  void PushDestructorCleanup(QualType T, Address Addr);\n\n  /// PushDestructorCleanup - Push a cleanup to call the\n  /// complete-object variant of the given destructor on the object at\n  /// the given address.\n  void PushDestructorCleanup(const CXXDestructorDecl *Dtor, QualType T,\n                             Address Addr);\n\n  /// PopCleanupBlock - Will pop the cleanup entry on the stack and\n  /// process all branch fixups.\n  void PopCleanupBlock(bool FallThroughIsBranchThrough = false);\n\n  /// DeactivateCleanupBlock - Deactivates the given cleanup block.\n  /// The block cannot be reactivated.  Pops it if it's the top of the\n  /// stack.\n  ///\n  /// \\param DominatingIP - An instruction which is known to\n  ///   dominate the current IP (if set) and which lies along\n  ///   all paths of execution between the current IP and the\n  ///   the point at which the cleanup comes into scope.\n  void DeactivateCleanupBlock(EHScopeStack::stable_iterator Cleanup,\n                              llvm::Instruction *DominatingIP);\n\n  /// ActivateCleanupBlock - Activates an initially-inactive cleanup.\n  /// Cannot be used to resurrect a deactivated cleanup.\n  ///\n  /// \\param DominatingIP - An instruction which is known to\n  ///   dominate the current IP (if set) and which lies along\n  ///   all paths of execution between the current IP and the\n  ///   the point at which the cleanup comes into scope.\n  void ActivateCleanupBlock(EHScopeStack::stable_iterator Cleanup,\n                            llvm::Instruction *DominatingIP);\n\n  /// Enters a new scope for capturing cleanups, all of which\n  /// will be executed once the scope is exited.\n  class RunCleanupsScope {\n    EHScopeStack::stable_iterator CleanupStackDepth, OldCleanupScopeDepth;\n    size_t LifetimeExtendedCleanupStackSize;\n    bool OldDidCallStackSave;\n  protected:\n    bool PerformCleanup;\n  private:\n\n    RunCleanupsScope(const RunCleanupsScope &) = delete;\n    void operator=(const RunCleanupsScope &) = delete;\n\n  protected:\n    CodeGenFunction& CGF;\n\n  public:\n    /// Enter a new cleanup scope.\n    explicit RunCleanupsScope(CodeGenFunction &CGF)\n      : PerformCleanup(true), CGF(CGF)\n    {\n      CleanupStackDepth = CGF.EHStack.stable_begin();\n      LifetimeExtendedCleanupStackSize =\n          CGF.LifetimeExtendedCleanupStack.size();\n      OldDidCallStackSave = CGF.DidCallStackSave;\n      CGF.DidCallStackSave = false;\n      OldCleanupScopeDepth = CGF.CurrentCleanupScopeDepth;\n      CGF.CurrentCleanupScopeDepth = CleanupStackDepth;\n    }\n\n    /// Exit this cleanup scope, emitting any accumulated cleanups.\n    ~RunCleanupsScope() {\n      if (PerformCleanup)\n        ForceCleanup();\n    }\n\n    /// Determine whether this scope requires any cleanups.\n    bool requiresCleanups() const {\n      return CGF.EHStack.stable_begin() != CleanupStackDepth;\n    }\n\n    /// Force the emission of cleanups now, instead of waiting\n    /// until this object is destroyed.\n    /// \\param ValuesToReload - A list of values that need to be available at\n    /// the insertion point after cleanup emission. If cleanup emission created\n    /// a shared cleanup block, these value pointers will be rewritten.\n    /// Otherwise, they not will be modified.\n    void ForceCleanup(std::initializer_list<llvm::Value**> ValuesToReload = {}) {\n      assert(PerformCleanup && \"Already forced cleanup\");\n      CGF.DidCallStackSave = OldDidCallStackSave;\n      CGF.PopCleanupBlocks(CleanupStackDepth, LifetimeExtendedCleanupStackSize,\n                           ValuesToReload);\n      PerformCleanup = false;\n      CGF.CurrentCleanupScopeDepth = OldCleanupScopeDepth;\n    }\n  };\n\n  // Cleanup stack depth of the RunCleanupsScope that was pushed most recently.\n  EHScopeStack::stable_iterator CurrentCleanupScopeDepth =\n      EHScopeStack::stable_end();\n\n  class LexicalScope : public RunCleanupsScope {\n    SourceRange Range;\n    SmallVector<const LabelDecl*, 4> Labels;\n    LexicalScope *ParentScope;\n\n    LexicalScope(const LexicalScope &) = delete;\n    void operator=(const LexicalScope &) = delete;\n\n  public:\n    /// Enter a new cleanup scope.\n    explicit LexicalScope(CodeGenFunction &CGF, SourceRange Range)\n      : RunCleanupsScope(CGF), Range(Range), ParentScope(CGF.CurLexicalScope) {\n      CGF.CurLexicalScope = this;\n      if (CGDebugInfo *DI = CGF.getDebugInfo())\n        DI->EmitLexicalBlockStart(CGF.Builder, Range.getBegin());\n    }\n\n    void addLabel(const LabelDecl *label) {\n      assert(PerformCleanup && \"adding label to dead scope?\");\n      Labels.push_back(label);\n    }\n\n    /// Exit this cleanup scope, emitting any accumulated\n    /// cleanups.\n    ~LexicalScope() {\n      if (CGDebugInfo *DI = CGF.getDebugInfo())\n        DI->EmitLexicalBlockEnd(CGF.Builder, Range.getEnd());\n\n      // If we should perform a cleanup, force them now.  Note that\n      // this ends the cleanup scope before rescoping any labels.\n      if (PerformCleanup) {\n        ApplyDebugLocation DL(CGF, Range.getEnd());\n        ForceCleanup();\n      }\n    }\n\n    /// Force the emission of cleanups now, instead of waiting\n    /// until this object is destroyed.\n    void ForceCleanup() {\n      CGF.CurLexicalScope = ParentScope;\n      RunCleanupsScope::ForceCleanup();\n\n      if (!Labels.empty())\n        rescopeLabels();\n    }\n\n    bool hasLabels() const {\n      return !Labels.empty();\n    }\n\n    void rescopeLabels();\n  };\n\n  typedef llvm::DenseMap<const Decl *, Address> DeclMapTy;\n\n  /// The class used to assign some variables some temporarily addresses.\n  class OMPMapVars {\n    DeclMapTy SavedLocals;\n    DeclMapTy SavedTempAddresses;\n    OMPMapVars(const OMPMapVars &) = delete;\n    void operator=(const OMPMapVars &) = delete;\n\n  public:\n    explicit OMPMapVars() = default;\n    ~OMPMapVars() {\n      assert(SavedLocals.empty() && \"Did not restored original addresses.\");\n    };\n\n    /// Sets the address of the variable \\p LocalVD to be \\p TempAddr in\n    /// function \\p CGF.\n    /// \\return true if at least one variable was set already, false otherwise.\n    bool setVarAddr(CodeGenFunction &CGF, const VarDecl *LocalVD,\n                    Address TempAddr) {\n      LocalVD = LocalVD->getCanonicalDecl();\n      // Only save it once.\n      if (SavedLocals.count(LocalVD)) return false;\n\n      // Copy the existing local entry to SavedLocals.\n      auto it = CGF.LocalDeclMap.find(LocalVD);\n      if (it != CGF.LocalDeclMap.end())\n        SavedLocals.try_emplace(LocalVD, it->second);\n      else\n        SavedLocals.try_emplace(LocalVD, Address::invalid());\n\n      // Generate the private entry.\n      QualType VarTy = LocalVD->getType();\n      if (VarTy->isReferenceType()) {\n        Address Temp = CGF.CreateMemTemp(VarTy);\n        CGF.Builder.CreateStore(TempAddr.getPointer(), Temp);\n        TempAddr = Temp;\n      }\n      SavedTempAddresses.try_emplace(LocalVD, TempAddr);\n\n      return true;\n    }\n\n    /// Applies new addresses to the list of the variables.\n    /// \\return true if at least one variable is using new address, false\n    /// otherwise.\n    bool apply(CodeGenFunction &CGF) {\n      copyInto(SavedTempAddresses, CGF.LocalDeclMap);\n      SavedTempAddresses.clear();\n      return !SavedLocals.empty();\n    }\n\n    /// Restores original addresses of the variables.\n    void restore(CodeGenFunction &CGF) {\n      if (!SavedLocals.empty()) {\n        copyInto(SavedLocals, CGF.LocalDeclMap);\n        SavedLocals.clear();\n      }\n    }\n\n  private:\n    /// Copy all the entries in the source map over the corresponding\n    /// entries in the destination, which must exist.\n    static void copyInto(const DeclMapTy &Src, DeclMapTy &Dest) {\n      for (auto &Pair : Src) {\n        if (!Pair.second.isValid()) {\n          Dest.erase(Pair.first);\n          continue;\n        }\n\n        auto I = Dest.find(Pair.first);\n        if (I != Dest.end())\n          I->second = Pair.second;\n        else\n          Dest.insert(Pair);\n      }\n    }\n  };\n\n  /// The scope used to remap some variables as private in the OpenMP loop body\n  /// (or other captured region emitted without outlining), and to restore old\n  /// vars back on exit.\n  class OMPPrivateScope : public RunCleanupsScope {\n    OMPMapVars MappedVars;\n    OMPPrivateScope(const OMPPrivateScope &) = delete;\n    void operator=(const OMPPrivateScope &) = delete;\n\n  public:\n    /// Enter a new OpenMP private scope.\n    explicit OMPPrivateScope(CodeGenFunction &CGF) : RunCleanupsScope(CGF) {}\n\n    /// Registers \\p LocalVD variable as a private and apply \\p PrivateGen\n    /// function for it to generate corresponding private variable. \\p\n    /// PrivateGen returns an address of the generated private variable.\n    /// \\return true if the variable is registered as private, false if it has\n    /// been privatized already.\n    bool addPrivate(const VarDecl *LocalVD,\n                    const llvm::function_ref<Address()> PrivateGen) {\n      assert(PerformCleanup && \"adding private to dead scope\");\n      return MappedVars.setVarAddr(CGF, LocalVD, PrivateGen());\n    }\n\n    /// Privatizes local variables previously registered as private.\n    /// Registration is separate from the actual privatization to allow\n    /// initializers use values of the original variables, not the private one.\n    /// This is important, for example, if the private variable is a class\n    /// variable initialized by a constructor that references other private\n    /// variables. But at initialization original variables must be used, not\n    /// private copies.\n    /// \\return true if at least one variable was privatized, false otherwise.\n    bool Privatize() { return MappedVars.apply(CGF); }\n\n    void ForceCleanup() {\n      RunCleanupsScope::ForceCleanup();\n      MappedVars.restore(CGF);\n    }\n\n    /// Exit scope - all the mapped variables are restored.\n    ~OMPPrivateScope() {\n      if (PerformCleanup)\n        ForceCleanup();\n    }\n\n    /// Checks if the global variable is captured in current function.\n    bool isGlobalVarCaptured(const VarDecl *VD) const {\n      VD = VD->getCanonicalDecl();\n      return !VD->isLocalVarDeclOrParm() && CGF.LocalDeclMap.count(VD) > 0;\n    }\n  };\n\n  /// Save/restore original map of previously emitted local vars in case when we\n  /// need to duplicate emission of the same code several times in the same\n  /// function for OpenMP code.\n  class OMPLocalDeclMapRAII {\n    CodeGenFunction &CGF;\n    DeclMapTy SavedMap;\n\n  public:\n    OMPLocalDeclMapRAII(CodeGenFunction &CGF)\n        : CGF(CGF), SavedMap(CGF.LocalDeclMap) {}\n    ~OMPLocalDeclMapRAII() { SavedMap.swap(CGF.LocalDeclMap); }\n  };\n\n  /// Takes the old cleanup stack size and emits the cleanup blocks\n  /// that have been added.\n  void\n  PopCleanupBlocks(EHScopeStack::stable_iterator OldCleanupStackSize,\n                   std::initializer_list<llvm::Value **> ValuesToReload = {});\n\n  /// Takes the old cleanup stack size and emits the cleanup blocks\n  /// that have been added, then adds all lifetime-extended cleanups from\n  /// the given position to the stack.\n  void\n  PopCleanupBlocks(EHScopeStack::stable_iterator OldCleanupStackSize,\n                   size_t OldLifetimeExtendedStackSize,\n                   std::initializer_list<llvm::Value **> ValuesToReload = {});\n\n  void ResolveBranchFixups(llvm::BasicBlock *Target);\n\n  /// The given basic block lies in the current EH scope, but may be a\n  /// target of a potentially scope-crossing jump; get a stable handle\n  /// to which we can perform this jump later.\n  JumpDest getJumpDestInCurrentScope(llvm::BasicBlock *Target) {\n    return JumpDest(Target,\n                    EHStack.getInnermostNormalCleanup(),\n                    NextCleanupDestIndex++);\n  }\n\n  /// The given basic block lies in the current EH scope, but may be a\n  /// target of a potentially scope-crossing jump; get a stable handle\n  /// to which we can perform this jump later.\n  JumpDest getJumpDestInCurrentScope(StringRef Name = StringRef()) {\n    return getJumpDestInCurrentScope(createBasicBlock(Name));\n  }\n\n  /// EmitBranchThroughCleanup - Emit a branch from the current insert\n  /// block through the normal cleanup handling code (if any) and then\n  /// on to \\arg Dest.\n  void EmitBranchThroughCleanup(JumpDest Dest);\n\n  /// isObviouslyBranchWithoutCleanups - Return true if a branch to the\n  /// specified destination obviously has no cleanups to run.  'false' is always\n  /// a conservatively correct answer for this method.\n  bool isObviouslyBranchWithoutCleanups(JumpDest Dest) const;\n\n  /// popCatchScope - Pops the catch scope at the top of the EHScope\n  /// stack, emitting any required code (other than the catch handlers\n  /// themselves).\n  void popCatchScope();\n\n  llvm::BasicBlock *getEHResumeBlock(bool isCleanup);\n  llvm::BasicBlock *getEHDispatchBlock(EHScopeStack::stable_iterator scope);\n  llvm::BasicBlock *\n  getFuncletEHDispatchBlock(EHScopeStack::stable_iterator scope);\n\n  /// An object to manage conditionally-evaluated expressions.\n  class ConditionalEvaluation {\n    llvm::BasicBlock *StartBB;\n\n  public:\n    ConditionalEvaluation(CodeGenFunction &CGF)\n      : StartBB(CGF.Builder.GetInsertBlock()) {}\n\n    void begin(CodeGenFunction &CGF) {\n      assert(CGF.OutermostConditional != this);\n      if (!CGF.OutermostConditional)\n        CGF.OutermostConditional = this;\n    }\n\n    void end(CodeGenFunction &CGF) {\n      assert(CGF.OutermostConditional != nullptr);\n      if (CGF.OutermostConditional == this)\n        CGF.OutermostConditional = nullptr;\n    }\n\n    /// Returns a block which will be executed prior to each\n    /// evaluation of the conditional code.\n    llvm::BasicBlock *getStartingBlock() const {\n      return StartBB;\n    }\n  };\n\n  /// isInConditionalBranch - Return true if we're currently emitting\n  /// one branch or the other of a conditional expression.\n  bool isInConditionalBranch() const { return OutermostConditional != nullptr; }\n\n  void setBeforeOutermostConditional(llvm::Value *value, Address addr) {\n    assert(isInConditionalBranch());\n    llvm::BasicBlock *block = OutermostConditional->getStartingBlock();\n    auto store = new llvm::StoreInst(value, addr.getPointer(), &block->back());\n    store->setAlignment(addr.getAlignment().getAsAlign());\n  }\n\n  /// An RAII object to record that we're evaluating a statement\n  /// expression.\n  class StmtExprEvaluation {\n    CodeGenFunction &CGF;\n\n    /// We have to save the outermost conditional: cleanups in a\n    /// statement expression aren't conditional just because the\n    /// StmtExpr is.\n    ConditionalEvaluation *SavedOutermostConditional;\n\n  public:\n    StmtExprEvaluation(CodeGenFunction &CGF)\n      : CGF(CGF), SavedOutermostConditional(CGF.OutermostConditional) {\n      CGF.OutermostConditional = nullptr;\n    }\n\n    ~StmtExprEvaluation() {\n      CGF.OutermostConditional = SavedOutermostConditional;\n      CGF.EnsureInsertPoint();\n    }\n  };\n\n  /// An object which temporarily prevents a value from being\n  /// destroyed by aggressive peephole optimizations that assume that\n  /// all uses of a value have been realized in the IR.\n  class PeepholeProtection {\n    llvm::Instruction *Inst;\n    friend class CodeGenFunction;\n\n  public:\n    PeepholeProtection() : Inst(nullptr) {}\n  };\n\n  /// A non-RAII class containing all the information about a bound\n  /// opaque value.  OpaqueValueMapping, below, is a RAII wrapper for\n  /// this which makes individual mappings very simple; using this\n  /// class directly is useful when you have a variable number of\n  /// opaque values or don't want the RAII functionality for some\n  /// reason.\n  class OpaqueValueMappingData {\n    const OpaqueValueExpr *OpaqueValue;\n    bool BoundLValue;\n    CodeGenFunction::PeepholeProtection Protection;\n\n    OpaqueValueMappingData(const OpaqueValueExpr *ov,\n                           bool boundLValue)\n      : OpaqueValue(ov), BoundLValue(boundLValue) {}\n  public:\n    OpaqueValueMappingData() : OpaqueValue(nullptr) {}\n\n    static bool shouldBindAsLValue(const Expr *expr) {\n      // gl-values should be bound as l-values for obvious reasons.\n      // Records should be bound as l-values because IR generation\n      // always keeps them in memory.  Expressions of function type\n      // act exactly like l-values but are formally required to be\n      // r-values in C.\n      return expr->isGLValue() ||\n             expr->getType()->isFunctionType() ||\n             hasAggregateEvaluationKind(expr->getType());\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const Expr *e) {\n      if (shouldBindAsLValue(ov))\n        return bind(CGF, ov, CGF.EmitLValue(e));\n      return bind(CGF, ov, CGF.EmitAnyExpr(e));\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const LValue &lv) {\n      assert(shouldBindAsLValue(ov));\n      CGF.OpaqueLValues.insert(std::make_pair(ov, lv));\n      return OpaqueValueMappingData(ov, true);\n    }\n\n    static OpaqueValueMappingData bind(CodeGenFunction &CGF,\n                                       const OpaqueValueExpr *ov,\n                                       const RValue &rv) {\n      assert(!shouldBindAsLValue(ov));\n      CGF.OpaqueRValues.insert(std::make_pair(ov, rv));\n\n      OpaqueValueMappingData data(ov, false);\n\n      // Work around an extremely aggressive peephole optimization in\n      // EmitScalarConversion which assumes that all other uses of a\n      // value are extant.\n      data.Protection = CGF.protectFromPeepholes(rv);\n\n      return data;\n    }\n\n    bool isValid() const { return OpaqueValue != nullptr; }\n    void clear() { OpaqueValue = nullptr; }\n\n    void unbind(CodeGenFunction &CGF) {\n      assert(OpaqueValue && \"no data to unbind!\");\n\n      if (BoundLValue) {\n        CGF.OpaqueLValues.erase(OpaqueValue);\n      } else {\n        CGF.OpaqueRValues.erase(OpaqueValue);\n        CGF.unprotectFromPeepholes(Protection);\n      }\n    }\n  };\n\n  /// An RAII object to set (and then clear) a mapping for an OpaqueValueExpr.\n  class OpaqueValueMapping {\n    CodeGenFunction &CGF;\n    OpaqueValueMappingData Data;\n\n  public:\n    static bool shouldBindAsLValue(const Expr *expr) {\n      return OpaqueValueMappingData::shouldBindAsLValue(expr);\n    }\n\n    /// Build the opaque value mapping for the given conditional\n    /// operator if it's the GNU ?: extension.  This is a common\n    /// enough pattern that the convenience operator is really\n    /// helpful.\n    ///\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const AbstractConditionalOperator *op) : CGF(CGF) {\n      if (isa<ConditionalOperator>(op))\n        // Leave Data empty.\n        return;\n\n      const BinaryConditionalOperator *e = cast<BinaryConditionalOperator>(op);\n      Data = OpaqueValueMappingData::bind(CGF, e->getOpaqueValue(),\n                                          e->getCommon());\n    }\n\n    /// Build the opaque value mapping for an OpaqueValueExpr whose source\n    /// expression is set to the expression the OVE represents.\n    OpaqueValueMapping(CodeGenFunction &CGF, const OpaqueValueExpr *OV)\n        : CGF(CGF) {\n      if (OV) {\n        assert(OV->getSourceExpr() && \"wrong form of OpaqueValueMapping used \"\n                                      \"for OVE with no source expression\");\n        Data = OpaqueValueMappingData::bind(CGF, OV, OV->getSourceExpr());\n      }\n    }\n\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const OpaqueValueExpr *opaqueValue,\n                       LValue lvalue)\n      : CGF(CGF), Data(OpaqueValueMappingData::bind(CGF, opaqueValue, lvalue)) {\n    }\n\n    OpaqueValueMapping(CodeGenFunction &CGF,\n                       const OpaqueValueExpr *opaqueValue,\n                       RValue rvalue)\n      : CGF(CGF), Data(OpaqueValueMappingData::bind(CGF, opaqueValue, rvalue)) {\n    }\n\n    void pop() {\n      Data.unbind(CGF);\n      Data.clear();\n    }\n\n    ~OpaqueValueMapping() {\n      if (Data.isValid()) Data.unbind(CGF);\n    }\n  };\n\nprivate:\n  CGDebugInfo *DebugInfo;\n  /// Used to create unique names for artificial VLA size debug info variables.\n  unsigned VLAExprCounter = 0;\n  bool DisableDebugInfo = false;\n\n  /// DidCallStackSave - Whether llvm.stacksave has been called. Used to avoid\n  /// calling llvm.stacksave for multiple VLAs in the same scope.\n  bool DidCallStackSave = false;\n\n  /// IndirectBranch - The first time an indirect goto is seen we create a block\n  /// with an indirect branch.  Every time we see the address of a label taken,\n  /// we add the label to the indirect goto.  Every subsequent indirect goto is\n  /// codegen'd as a jump to the IndirectBranch's basic block.\n  llvm::IndirectBrInst *IndirectBranch = nullptr;\n\n  /// LocalDeclMap - This keeps track of the LLVM allocas or globals for local C\n  /// decls.\n  DeclMapTy LocalDeclMap;\n\n  // Keep track of the cleanups for callee-destructed parameters pushed to the\n  // cleanup stack so that they can be deactivated later.\n  llvm::DenseMap<const ParmVarDecl *, EHScopeStack::stable_iterator>\n      CalleeDestructedParamCleanups;\n\n  /// SizeArguments - If a ParmVarDecl had the pass_object_size attribute, this\n  /// will contain a mapping from said ParmVarDecl to its implicit \"object_size\"\n  /// parameter.\n  llvm::SmallDenseMap<const ParmVarDecl *, const ImplicitParamDecl *, 2>\n      SizeArguments;\n\n  /// Track escaped local variables with auto storage. Used during SEH\n  /// outlining to produce a call to llvm.localescape.\n  llvm::DenseMap<llvm::AllocaInst *, int> EscapedLocals;\n\n  /// LabelMap - This keeps track of the LLVM basic block for each C label.\n  llvm::DenseMap<const LabelDecl*, JumpDest> LabelMap;\n\n  // BreakContinueStack - This keeps track of where break and continue\n  // statements should jump to.\n  struct BreakContinue {\n    BreakContinue(JumpDest Break, JumpDest Continue)\n      : BreakBlock(Break), ContinueBlock(Continue) {}\n\n    JumpDest BreakBlock;\n    JumpDest ContinueBlock;\n  };\n  SmallVector<BreakContinue, 8> BreakContinueStack;\n\n  /// Handles cancellation exit points in OpenMP-related constructs.\n  class OpenMPCancelExitStack {\n    /// Tracks cancellation exit point and join point for cancel-related exit\n    /// and normal exit.\n    struct CancelExit {\n      CancelExit() = default;\n      CancelExit(OpenMPDirectiveKind Kind, JumpDest ExitBlock,\n                 JumpDest ContBlock)\n          : Kind(Kind), ExitBlock(ExitBlock), ContBlock(ContBlock) {}\n      OpenMPDirectiveKind Kind = llvm::omp::OMPD_unknown;\n      /// true if the exit block has been emitted already by the special\n      /// emitExit() call, false if the default codegen is used.\n      bool HasBeenEmitted = false;\n      JumpDest ExitBlock;\n      JumpDest ContBlock;\n    };\n\n    SmallVector<CancelExit, 8> Stack;\n\n  public:\n    OpenMPCancelExitStack() : Stack(1) {}\n    ~OpenMPCancelExitStack() = default;\n    /// Fetches the exit block for the current OpenMP construct.\n    JumpDest getExitBlock() const { return Stack.back().ExitBlock; }\n    /// Emits exit block with special codegen procedure specific for the related\n    /// OpenMP construct + emits code for normal construct cleanup.\n    void emitExit(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,\n                  const llvm::function_ref<void(CodeGenFunction &)> CodeGen) {\n      if (Stack.back().Kind == Kind && getExitBlock().isValid()) {\n        assert(CGF.getOMPCancelDestination(Kind).isValid());\n        assert(CGF.HaveInsertPoint());\n        assert(!Stack.back().HasBeenEmitted);\n        auto IP = CGF.Builder.saveAndClearIP();\n        CGF.EmitBlock(Stack.back().ExitBlock.getBlock());\n        CodeGen(CGF);\n        CGF.EmitBranch(Stack.back().ContBlock.getBlock());\n        CGF.Builder.restoreIP(IP);\n        Stack.back().HasBeenEmitted = true;\n      }\n      CodeGen(CGF);\n    }\n    /// Enter the cancel supporting \\a Kind construct.\n    /// \\param Kind OpenMP directive that supports cancel constructs.\n    /// \\param HasCancel true, if the construct has inner cancel directive,\n    /// false otherwise.\n    void enter(CodeGenFunction &CGF, OpenMPDirectiveKind Kind, bool HasCancel) {\n      Stack.push_back({Kind,\n                       HasCancel ? CGF.getJumpDestInCurrentScope(\"cancel.exit\")\n                                 : JumpDest(),\n                       HasCancel ? CGF.getJumpDestInCurrentScope(\"cancel.cont\")\n                                 : JumpDest()});\n    }\n    /// Emits default exit point for the cancel construct (if the special one\n    /// has not be used) + join point for cancel/normal exits.\n    void exit(CodeGenFunction &CGF) {\n      if (getExitBlock().isValid()) {\n        assert(CGF.getOMPCancelDestination(Stack.back().Kind).isValid());\n        bool HaveIP = CGF.HaveInsertPoint();\n        if (!Stack.back().HasBeenEmitted) {\n          if (HaveIP)\n            CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);\n          CGF.EmitBlock(Stack.back().ExitBlock.getBlock());\n          CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);\n        }\n        CGF.EmitBlock(Stack.back().ContBlock.getBlock());\n        if (!HaveIP) {\n          CGF.Builder.CreateUnreachable();\n          CGF.Builder.ClearInsertionPoint();\n        }\n      }\n      Stack.pop_back();\n    }\n  };\n  OpenMPCancelExitStack OMPCancelStack;\n\n  /// Calculate branch weights for the likelihood attribute\n  llvm::MDNode *createBranchWeights(Stmt::Likelihood LH) const;\n\n  CodeGenPGO PGO;\n\n  /// Calculate branch weights appropriate for PGO data\n  llvm::MDNode *createProfileWeights(uint64_t TrueCount,\n                                     uint64_t FalseCount) const;\n  llvm::MDNode *createProfileWeights(ArrayRef<uint64_t> Weights) const;\n  llvm::MDNode *createProfileWeightsForLoop(const Stmt *Cond,\n                                            uint64_t LoopCount) const;\n\n  /// Calculate the branch weight for PGO data or the likelihood attribute.\n  /// The function tries to get the weight of \\ref createProfileWeightsForLoop.\n  /// If that fails it gets the weight of \\ref createBranchWeights.\n  llvm::MDNode *createProfileOrBranchWeightsForLoop(const Stmt *Cond,\n                                                    uint64_t LoopCount,\n                                                    const Stmt *Body) const;\n\npublic:\n  /// Increment the profiler's counter for the given statement by \\p StepV.\n  /// If \\p StepV is null, the default increment is 1.\n  void incrementProfileCounter(const Stmt *S, llvm::Value *StepV = nullptr) {\n    if (CGM.getCodeGenOpts().hasProfileClangInstr() &&\n        !CurFn->hasFnAttribute(llvm::Attribute::NoProfile))\n      PGO.emitCounterIncrement(Builder, S, StepV);\n    PGO.setCurrentStmt(S);\n  }\n\n  /// Get the profiler's count for the given statement.\n  uint64_t getProfileCount(const Stmt *S) {\n    Optional<uint64_t> Count = PGO.getStmtCount(S);\n    if (!Count.hasValue())\n      return 0;\n    return *Count;\n  }\n\n  /// Set the profiler's current count.\n  void setCurrentProfileCount(uint64_t Count) {\n    PGO.setCurrentRegionCount(Count);\n  }\n\n  /// Get the profiler's current count. This is generally the count for the most\n  /// recently incremented counter.\n  uint64_t getCurrentProfileCount() {\n    return PGO.getCurrentRegionCount();\n  }\n\nprivate:\n\n  /// SwitchInsn - This is nearest current switch instruction. It is null if\n  /// current context is not in a switch.\n  llvm::SwitchInst *SwitchInsn = nullptr;\n  /// The branch weights of SwitchInsn when doing instrumentation based PGO.\n  SmallVector<uint64_t, 16> *SwitchWeights = nullptr;\n\n  /// The likelihood attributes of the SwitchCase.\n  SmallVector<Stmt::Likelihood, 16> *SwitchLikelihood = nullptr;\n\n  /// CaseRangeBlock - This block holds if condition check for last case\n  /// statement range in current switch instruction.\n  llvm::BasicBlock *CaseRangeBlock = nullptr;\n\n  /// OpaqueLValues - Keeps track of the current set of opaque value\n  /// expressions.\n  llvm::DenseMap<const OpaqueValueExpr *, LValue> OpaqueLValues;\n  llvm::DenseMap<const OpaqueValueExpr *, RValue> OpaqueRValues;\n\n  // VLASizeMap - This keeps track of the associated size for each VLA type.\n  // We track this by the size expression rather than the type itself because\n  // in certain situations, like a const qualifier applied to an VLA typedef,\n  // multiple VLA types can share the same size expression.\n  // FIXME: Maybe this could be a stack of maps that is pushed/popped as we\n  // enter/leave scopes.\n  llvm::DenseMap<const Expr*, llvm::Value*> VLASizeMap;\n\n  /// A block containing a single 'unreachable' instruction.  Created\n  /// lazily by getUnreachableBlock().\n  llvm::BasicBlock *UnreachableBlock = nullptr;\n\n  /// Counts of the number return expressions in the function.\n  unsigned NumReturnExprs = 0;\n\n  /// Count the number of simple (constant) return expressions in the function.\n  unsigned NumSimpleReturnExprs = 0;\n\n  /// The last regular (non-return) debug location (breakpoint) in the function.\n  SourceLocation LastStopPoint;\n\npublic:\n  /// Source location information about the default argument or member\n  /// initializer expression we're evaluating, if any.\n  CurrentSourceLocExprScope CurSourceLocExprScope;\n  using SourceLocExprScopeGuard =\n      CurrentSourceLocExprScope::SourceLocExprScopeGuard;\n\n  /// A scope within which we are constructing the fields of an object which\n  /// might use a CXXDefaultInitExpr. This stashes away a 'this' value to use\n  /// if we need to evaluate a CXXDefaultInitExpr within the evaluation.\n  class FieldConstructionScope {\n  public:\n    FieldConstructionScope(CodeGenFunction &CGF, Address This)\n        : CGF(CGF), OldCXXDefaultInitExprThis(CGF.CXXDefaultInitExprThis) {\n      CGF.CXXDefaultInitExprThis = This;\n    }\n    ~FieldConstructionScope() {\n      CGF.CXXDefaultInitExprThis = OldCXXDefaultInitExprThis;\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    Address OldCXXDefaultInitExprThis;\n  };\n\n  /// The scope of a CXXDefaultInitExpr. Within this scope, the value of 'this'\n  /// is overridden to be the object under construction.\n  class CXXDefaultInitExprScope  {\n  public:\n    CXXDefaultInitExprScope(CodeGenFunction &CGF, const CXXDefaultInitExpr *E)\n        : CGF(CGF), OldCXXThisValue(CGF.CXXThisValue),\n          OldCXXThisAlignment(CGF.CXXThisAlignment),\n          SourceLocScope(E, CGF.CurSourceLocExprScope) {\n      CGF.CXXThisValue = CGF.CXXDefaultInitExprThis.getPointer();\n      CGF.CXXThisAlignment = CGF.CXXDefaultInitExprThis.getAlignment();\n    }\n    ~CXXDefaultInitExprScope() {\n      CGF.CXXThisValue = OldCXXThisValue;\n      CGF.CXXThisAlignment = OldCXXThisAlignment;\n    }\n\n  public:\n    CodeGenFunction &CGF;\n    llvm::Value *OldCXXThisValue;\n    CharUnits OldCXXThisAlignment;\n    SourceLocExprScopeGuard SourceLocScope;\n  };\n\n  struct CXXDefaultArgExprScope : SourceLocExprScopeGuard {\n    CXXDefaultArgExprScope(CodeGenFunction &CGF, const CXXDefaultArgExpr *E)\n        : SourceLocExprScopeGuard(E, CGF.CurSourceLocExprScope) {}\n  };\n\n  /// The scope of an ArrayInitLoopExpr. Within this scope, the value of the\n  /// current loop index is overridden.\n  class ArrayInitLoopExprScope {\n  public:\n    ArrayInitLoopExprScope(CodeGenFunction &CGF, llvm::Value *Index)\n      : CGF(CGF), OldArrayInitIndex(CGF.ArrayInitIndex) {\n      CGF.ArrayInitIndex = Index;\n    }\n    ~ArrayInitLoopExprScope() {\n      CGF.ArrayInitIndex = OldArrayInitIndex;\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    llvm::Value *OldArrayInitIndex;\n  };\n\n  class InlinedInheritingConstructorScope {\n  public:\n    InlinedInheritingConstructorScope(CodeGenFunction &CGF, GlobalDecl GD)\n        : CGF(CGF), OldCurGD(CGF.CurGD), OldCurFuncDecl(CGF.CurFuncDecl),\n          OldCurCodeDecl(CGF.CurCodeDecl),\n          OldCXXABIThisDecl(CGF.CXXABIThisDecl),\n          OldCXXABIThisValue(CGF.CXXABIThisValue),\n          OldCXXThisValue(CGF.CXXThisValue),\n          OldCXXABIThisAlignment(CGF.CXXABIThisAlignment),\n          OldCXXThisAlignment(CGF.CXXThisAlignment),\n          OldReturnValue(CGF.ReturnValue), OldFnRetTy(CGF.FnRetTy),\n          OldCXXInheritedCtorInitExprArgs(\n              std::move(CGF.CXXInheritedCtorInitExprArgs)) {\n      CGF.CurGD = GD;\n      CGF.CurFuncDecl = CGF.CurCodeDecl =\n          cast<CXXConstructorDecl>(GD.getDecl());\n      CGF.CXXABIThisDecl = nullptr;\n      CGF.CXXABIThisValue = nullptr;\n      CGF.CXXThisValue = nullptr;\n      CGF.CXXABIThisAlignment = CharUnits();\n      CGF.CXXThisAlignment = CharUnits();\n      CGF.ReturnValue = Address::invalid();\n      CGF.FnRetTy = QualType();\n      CGF.CXXInheritedCtorInitExprArgs.clear();\n    }\n    ~InlinedInheritingConstructorScope() {\n      CGF.CurGD = OldCurGD;\n      CGF.CurFuncDecl = OldCurFuncDecl;\n      CGF.CurCodeDecl = OldCurCodeDecl;\n      CGF.CXXABIThisDecl = OldCXXABIThisDecl;\n      CGF.CXXABIThisValue = OldCXXABIThisValue;\n      CGF.CXXThisValue = OldCXXThisValue;\n      CGF.CXXABIThisAlignment = OldCXXABIThisAlignment;\n      CGF.CXXThisAlignment = OldCXXThisAlignment;\n      CGF.ReturnValue = OldReturnValue;\n      CGF.FnRetTy = OldFnRetTy;\n      CGF.CXXInheritedCtorInitExprArgs =\n          std::move(OldCXXInheritedCtorInitExprArgs);\n    }\n\n  private:\n    CodeGenFunction &CGF;\n    GlobalDecl OldCurGD;\n    const Decl *OldCurFuncDecl;\n    const Decl *OldCurCodeDecl;\n    ImplicitParamDecl *OldCXXABIThisDecl;\n    llvm::Value *OldCXXABIThisValue;\n    llvm::Value *OldCXXThisValue;\n    CharUnits OldCXXABIThisAlignment;\n    CharUnits OldCXXThisAlignment;\n    Address OldReturnValue;\n    QualType OldFnRetTy;\n    CallArgList OldCXXInheritedCtorInitExprArgs;\n  };\n\n  // Helper class for the OpenMP IR Builder. Allows reusability of code used for\n  // region body, and finalization codegen callbacks. This will class will also\n  // contain privatization functions used by the privatization call backs\n  //\n  // TODO: this is temporary class for things that are being moved out of\n  // CGOpenMPRuntime, new versions of current CodeGenFunction methods, or\n  // utility function for use with the OMPBuilder. Once that move to use the\n  // OMPBuilder is done, everything here will either become part of CodeGenFunc.\n  // directly, or a new helper class that will contain functions used by both\n  // this and the OMPBuilder\n\n  struct OMPBuilderCBHelpers {\n\n    OMPBuilderCBHelpers() = delete;\n    OMPBuilderCBHelpers(const OMPBuilderCBHelpers &) = delete;\n    OMPBuilderCBHelpers &operator=(const OMPBuilderCBHelpers &) = delete;\n\n    using InsertPointTy = llvm::OpenMPIRBuilder::InsertPointTy;\n\n    /// Cleanup action for allocate support.\n    class OMPAllocateCleanupTy final : public EHScopeStack::Cleanup {\n\n    private:\n      llvm::CallInst *RTLFnCI;\n\n    public:\n      OMPAllocateCleanupTy(llvm::CallInst *RLFnCI) : RTLFnCI(RLFnCI) {\n        RLFnCI->removeFromParent();\n      }\n\n      void Emit(CodeGenFunction &CGF, Flags /*flags*/) override {\n        if (!CGF.HaveInsertPoint())\n          return;\n        CGF.Builder.Insert(RTLFnCI);\n      }\n    };\n\n    /// Returns address of the threadprivate variable for the current\n    /// thread. This Also create any necessary OMP runtime calls.\n    ///\n    /// \\param VD VarDecl for Threadprivate variable.\n    /// \\param VDAddr Address of the Vardecl\n    /// \\param Loc  The location where the barrier directive was encountered\n    static Address getAddrOfThreadPrivate(CodeGenFunction &CGF,\n                                          const VarDecl *VD, Address VDAddr,\n                                          SourceLocation Loc);\n\n    /// Gets the OpenMP-specific address of the local variable /p VD.\n    static Address getAddressOfLocalVariable(CodeGenFunction &CGF,\n                                             const VarDecl *VD);\n    /// Get the platform-specific name separator.\n    /// \\param Parts different parts of the final name that needs separation\n    /// \\param FirstSeparator First separator used between the initial two\n    ///        parts of the name.\n    /// \\param Separator separator used between all of the rest consecutinve\n    ///        parts of the name\n    static std::string getNameWithSeparators(ArrayRef<StringRef> Parts,\n                                             StringRef FirstSeparator = \".\",\n                                             StringRef Separator = \".\");\n    /// Emit the Finalization for an OMP region\n    /// \\param CGF\tThe Codegen function this belongs to\n    /// \\param IP\tInsertion point for generating the finalization code.\n    static void FinalizeOMPRegion(CodeGenFunction &CGF, InsertPointTy IP) {\n      CGBuilderTy::InsertPointGuard IPG(CGF.Builder);\n      assert(IP.getBlock()->end() != IP.getPoint() &&\n             \"OpenMP IR Builder should cause terminated block!\");\n\n      llvm::BasicBlock *IPBB = IP.getBlock();\n      llvm::BasicBlock *DestBB = IPBB->getUniqueSuccessor();\n      assert(DestBB && \"Finalization block should have one successor!\");\n\n      // erase and replace with cleanup branch.\n      IPBB->getTerminator()->eraseFromParent();\n      CGF.Builder.SetInsertPoint(IPBB);\n      CodeGenFunction::JumpDest Dest = CGF.getJumpDestInCurrentScope(DestBB);\n      CGF.EmitBranchThroughCleanup(Dest);\n    }\n\n    /// Emit the body of an OMP region\n    /// \\param CGF\tThe Codegen function this belongs to\n    /// \\param RegionBodyStmt\tThe body statement for the OpenMP region being\n    /// \t\t\t generated\n    /// \\param CodeGenIP\tInsertion point for generating the body code.\n    /// \\param FiniBB\tThe finalization basic block\n    static void EmitOMPRegionBody(CodeGenFunction &CGF,\n                                  const Stmt *RegionBodyStmt,\n                                  InsertPointTy CodeGenIP,\n                                  llvm::BasicBlock &FiniBB) {\n      llvm::BasicBlock *CodeGenIPBB = CodeGenIP.getBlock();\n      if (llvm::Instruction *CodeGenIPBBTI = CodeGenIPBB->getTerminator())\n        CodeGenIPBBTI->eraseFromParent();\n\n      CGF.Builder.SetInsertPoint(CodeGenIPBB);\n\n      CGF.EmitStmt(RegionBodyStmt);\n\n      if (CGF.Builder.saveIP().isSet())\n        CGF.Builder.CreateBr(&FiniBB);\n    }\n\n    /// RAII for preserving necessary info during Outlined region body codegen.\n    class OutlinedRegionBodyRAII {\n\n      llvm::AssertingVH<llvm::Instruction> OldAllocaIP;\n      CodeGenFunction::JumpDest OldReturnBlock;\n      CGBuilderTy::InsertPoint IP;\n      CodeGenFunction &CGF;\n\n    public:\n      OutlinedRegionBodyRAII(CodeGenFunction &cgf, InsertPointTy &AllocaIP,\n                             llvm::BasicBlock &RetBB)\n          : CGF(cgf) {\n        assert(AllocaIP.isSet() &&\n               \"Must specify Insertion point for allocas of outlined function\");\n        OldAllocaIP = CGF.AllocaInsertPt;\n        CGF.AllocaInsertPt = &*AllocaIP.getPoint();\n        IP = CGF.Builder.saveIP();\n\n        OldReturnBlock = CGF.ReturnBlock;\n        CGF.ReturnBlock = CGF.getJumpDestInCurrentScope(&RetBB);\n      }\n\n      ~OutlinedRegionBodyRAII() {\n        CGF.AllocaInsertPt = OldAllocaIP;\n        CGF.ReturnBlock = OldReturnBlock;\n        CGF.Builder.restoreIP(IP);\n      }\n    };\n\n    /// RAII for preserving necessary info during inlined region body codegen.\n    class InlinedRegionBodyRAII {\n\n      llvm::AssertingVH<llvm::Instruction> OldAllocaIP;\n      CodeGenFunction &CGF;\n\n    public:\n      InlinedRegionBodyRAII(CodeGenFunction &cgf, InsertPointTy &AllocaIP,\n                            llvm::BasicBlock &FiniBB)\n          : CGF(cgf) {\n        // Alloca insertion block should be in the entry block of the containing\n        // function so it expects an empty AllocaIP in which case will reuse the\n        // old alloca insertion point, or a new AllocaIP in the same block as\n        // the old one\n        assert((!AllocaIP.isSet() ||\n                CGF.AllocaInsertPt->getParent() == AllocaIP.getBlock()) &&\n               \"Insertion point should be in the entry block of containing \"\n               \"function!\");\n        OldAllocaIP = CGF.AllocaInsertPt;\n        if (AllocaIP.isSet())\n          CGF.AllocaInsertPt = &*AllocaIP.getPoint();\n\n        // TODO: Remove the call, after making sure the counter is not used by\n        //       the EHStack.\n        // Since this is an inlined region, it should not modify the\n        // ReturnBlock, and should reuse the one for the enclosing outlined\n        // region. So, the JumpDest being return by the function is discarded\n        (void)CGF.getJumpDestInCurrentScope(&FiniBB);\n      }\n\n      ~InlinedRegionBodyRAII() { CGF.AllocaInsertPt = OldAllocaIP; }\n    };\n  };\n\nprivate:\n  /// CXXThisDecl - When generating code for a C++ member function,\n  /// this will hold the implicit 'this' declaration.\n  ImplicitParamDecl *CXXABIThisDecl = nullptr;\n  llvm::Value *CXXABIThisValue = nullptr;\n  llvm::Value *CXXThisValue = nullptr;\n  CharUnits CXXABIThisAlignment;\n  CharUnits CXXThisAlignment;\n\n  /// The value of 'this' to use when evaluating CXXDefaultInitExprs within\n  /// this expression.\n  Address CXXDefaultInitExprThis = Address::invalid();\n\n  /// The current array initialization index when evaluating an\n  /// ArrayInitIndexExpr within an ArrayInitLoopExpr.\n  llvm::Value *ArrayInitIndex = nullptr;\n\n  /// The values of function arguments to use when evaluating\n  /// CXXInheritedCtorInitExprs within this context.\n  CallArgList CXXInheritedCtorInitExprArgs;\n\n  /// CXXStructorImplicitParamDecl - When generating code for a constructor or\n  /// destructor, this will hold the implicit argument (e.g. VTT).\n  ImplicitParamDecl *CXXStructorImplicitParamDecl = nullptr;\n  llvm::Value *CXXStructorImplicitParamValue = nullptr;\n\n  /// OutermostConditional - Points to the outermost active\n  /// conditional control.  This is used so that we know if a\n  /// temporary should be destroyed conditionally.\n  ConditionalEvaluation *OutermostConditional = nullptr;\n\n  /// The current lexical scope.\n  LexicalScope *CurLexicalScope = nullptr;\n\n  /// The current source location that should be used for exception\n  /// handling code.\n  SourceLocation CurEHLocation;\n\n  /// BlockByrefInfos - For each __block variable, contains\n  /// information about the layout of the variable.\n  llvm::DenseMap<const ValueDecl *, BlockByrefInfo> BlockByrefInfos;\n\n  /// Used by -fsanitize=nullability-return to determine whether the return\n  /// value can be checked.\n  llvm::Value *RetValNullabilityPrecondition = nullptr;\n\n  /// Check if -fsanitize=nullability-return instrumentation is required for\n  /// this function.\n  bool requiresReturnValueNullabilityCheck() const {\n    return RetValNullabilityPrecondition;\n  }\n\n  /// Used to store precise source locations for return statements by the\n  /// runtime return value checks.\n  Address ReturnLocation = Address::invalid();\n\n  /// Check if the return value of this function requires sanitization.\n  bool requiresReturnValueCheck() const;\n\n  llvm::BasicBlock *TerminateLandingPad = nullptr;\n  llvm::BasicBlock *TerminateHandler = nullptr;\n  llvm::SmallVector<llvm::BasicBlock *, 2> TrapBBs;\n\n  /// Terminate funclets keyed by parent funclet pad.\n  llvm::MapVector<llvm::Value *, llvm::BasicBlock *> TerminateFunclets;\n\n  /// Largest vector width used in ths function. Will be used to create a\n  /// function attribute.\n  unsigned LargestVectorWidth = 0;\n\n  /// True if we need emit the life-time markers.\n  const bool ShouldEmitLifetimeMarkers;\n\n  /// Add OpenCL kernel arg metadata and the kernel attribute metadata to\n  /// the function metadata.\n  void EmitOpenCLKernelMetadata(const FunctionDecl *FD,\n                                llvm::Function *Fn);\n\npublic:\n  CodeGenFunction(CodeGenModule &cgm, bool suppressNewContext=false);\n  ~CodeGenFunction();\n\n  CodeGenTypes &getTypes() const { return CGM.getTypes(); }\n  ASTContext &getContext() const { return CGM.getContext(); }\n  CGDebugInfo *getDebugInfo() {\n    if (DisableDebugInfo)\n      return nullptr;\n    return DebugInfo;\n  }\n  void disableDebugInfo() { DisableDebugInfo = true; }\n  void enableDebugInfo() { DisableDebugInfo = false; }\n\n  bool shouldUseFusedARCCalls() {\n    return CGM.getCodeGenOpts().OptimizationLevel == 0;\n  }\n\n  const LangOptions &getLangOpts() const { return CGM.getLangOpts(); }\n\n  /// Returns a pointer to the function's exception object and selector slot,\n  /// which is assigned in every landing pad.\n  Address getExceptionSlot();\n  Address getEHSelectorSlot();\n\n  /// Returns the contents of the function's exception object and selector\n  /// slots.\n  llvm::Value *getExceptionFromSlot();\n  llvm::Value *getSelectorFromSlot();\n\n  Address getNormalCleanupDestSlot();\n\n  llvm::BasicBlock *getUnreachableBlock() {\n    if (!UnreachableBlock) {\n      UnreachableBlock = createBasicBlock(\"unreachable\");\n      new llvm::UnreachableInst(getLLVMContext(), UnreachableBlock);\n    }\n    return UnreachableBlock;\n  }\n\n  llvm::BasicBlock *getInvokeDest() {\n    if (!EHStack.requiresLandingPad()) return nullptr;\n    return getInvokeDestImpl();\n  }\n\n  bool currentFunctionUsesSEHTry() const { return CurSEHParent != nullptr; }\n\n  const TargetInfo &getTarget() const { return Target; }\n  llvm::LLVMContext &getLLVMContext() { return CGM.getLLVMContext(); }\n  const TargetCodeGenInfo &getTargetHooks() const {\n    return CGM.getTargetCodeGenInfo();\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                                  Cleanups\n  //===--------------------------------------------------------------------===//\n\n  typedef void Destroyer(CodeGenFunction &CGF, Address addr, QualType ty);\n\n  void pushIrregularPartialArrayCleanup(llvm::Value *arrayBegin,\n                                        Address arrayEndPointer,\n                                        QualType elementType,\n                                        CharUnits elementAlignment,\n                                        Destroyer *destroyer);\n  void pushRegularPartialArrayCleanup(llvm::Value *arrayBegin,\n                                      llvm::Value *arrayEnd,\n                                      QualType elementType,\n                                      CharUnits elementAlignment,\n                                      Destroyer *destroyer);\n\n  void pushDestroy(QualType::DestructionKind dtorKind,\n                   Address addr, QualType type);\n  void pushEHDestroy(QualType::DestructionKind dtorKind,\n                     Address addr, QualType type);\n  void pushDestroy(CleanupKind kind, Address addr, QualType type,\n                   Destroyer *destroyer, bool useEHCleanupForArray);\n  void pushLifetimeExtendedDestroy(CleanupKind kind, Address addr,\n                                   QualType type, Destroyer *destroyer,\n                                   bool useEHCleanupForArray);\n  void pushCallObjectDeleteCleanup(const FunctionDecl *OperatorDelete,\n                                   llvm::Value *CompletePtr,\n                                   QualType ElementType);\n  void pushStackRestore(CleanupKind kind, Address SPMem);\n  void emitDestroy(Address addr, QualType type, Destroyer *destroyer,\n                   bool useEHCleanupForArray);\n  llvm::Function *generateDestroyHelper(Address addr, QualType type,\n                                        Destroyer *destroyer,\n                                        bool useEHCleanupForArray,\n                                        const VarDecl *VD);\n  void emitArrayDestroy(llvm::Value *begin, llvm::Value *end,\n                        QualType elementType, CharUnits elementAlign,\n                        Destroyer *destroyer,\n                        bool checkZeroLength, bool useEHCleanup);\n\n  Destroyer *getDestroyer(QualType::DestructionKind destructionKind);\n\n  /// Determines whether an EH cleanup is required to destroy a type\n  /// with the given destruction kind.\n  bool needsEHCleanup(QualType::DestructionKind kind) {\n    switch (kind) {\n    case QualType::DK_none:\n      return false;\n    case QualType::DK_cxx_destructor:\n    case QualType::DK_objc_weak_lifetime:\n    case QualType::DK_nontrivial_c_struct:\n      return getLangOpts().Exceptions;\n    case QualType::DK_objc_strong_lifetime:\n      return getLangOpts().Exceptions &&\n             CGM.getCodeGenOpts().ObjCAutoRefCountExceptions;\n    }\n    llvm_unreachable(\"bad destruction kind\");\n  }\n\n  CleanupKind getCleanupKind(QualType::DestructionKind kind) {\n    return (needsEHCleanup(kind) ? NormalAndEHCleanup : NormalCleanup);\n  }\n\n  //===--------------------------------------------------------------------===//\n  //                                  Objective-C\n  //===--------------------------------------------------------------------===//\n\n  void GenerateObjCMethod(const ObjCMethodDecl *OMD);\n\n  void StartObjCMethod(const ObjCMethodDecl *MD, const ObjCContainerDecl *CD);\n\n  /// GenerateObjCGetter - Synthesize an Objective-C property getter function.\n  void GenerateObjCGetter(ObjCImplementationDecl *IMP,\n                          const ObjCPropertyImplDecl *PID);\n  void generateObjCGetterBody(const ObjCImplementationDecl *classImpl,\n                              const ObjCPropertyImplDecl *propImpl,\n                              const ObjCMethodDecl *GetterMothodDecl,\n                              llvm::Constant *AtomicHelperFn);\n\n  void GenerateObjCCtorDtorMethod(ObjCImplementationDecl *IMP,\n                                  ObjCMethodDecl *MD, bool ctor);\n\n  /// GenerateObjCSetter - Synthesize an Objective-C property setter function\n  /// for the given property.\n  void GenerateObjCSetter(ObjCImplementationDecl *IMP,\n                          const ObjCPropertyImplDecl *PID);\n  void generateObjCSetterBody(const ObjCImplementationDecl *classImpl,\n                              const ObjCPropertyImplDecl *propImpl,\n                              llvm::Constant *AtomicHelperFn);\n\n  //===--------------------------------------------------------------------===//\n  //                                  Block Bits\n  //===--------------------------------------------------------------------===//\n\n  /// Emit block literal.\n  /// \\return an LLVM value which is a pointer to a struct which contains\n  /// information about the block, including the block invoke function, the\n  /// captured variables, etc.\n  llvm::Value *EmitBlockLiteral(const BlockExpr *);\n\n  llvm::Function *GenerateBlockFunction(GlobalDecl GD,\n                                        const CGBlockInfo &Info,\n                                        const DeclMapTy &ldm,\n                                        bool IsLambdaConversionToBlock,\n                                        bool BuildGlobalBlock);\n\n  /// Check if \\p T is a C++ class that has a destructor that can throw.\n  static bool cxxDestructorCanThrow(QualType T);\n\n  llvm::Constant *GenerateCopyHelperFunction(const CGBlockInfo &blockInfo);\n  llvm::Constant *GenerateDestroyHelperFunction(const CGBlockInfo &blockInfo);\n  llvm::Constant *GenerateObjCAtomicSetterCopyHelperFunction(\n                                             const ObjCPropertyImplDecl *PID);\n  llvm::Constant *GenerateObjCAtomicGetterCopyHelperFunction(\n                                             const ObjCPropertyImplDecl *PID);\n  llvm::Value *EmitBlockCopyAndAutorelease(llvm::Value *Block, QualType Ty);\n\n  void BuildBlockRelease(llvm::Value *DeclPtr, BlockFieldFlags flags,\n                         bool CanThrow);\n\n  class AutoVarEmission;\n\n  void emitByrefStructureInit(const AutoVarEmission &emission);\n\n  /// Enter a cleanup to destroy a __block variable.  Note that this\n  /// cleanup should be a no-op if the variable hasn't left the stack\n  /// yet; if a cleanup is required for the variable itself, that needs\n  /// to be done externally.\n  ///\n  /// \\param Kind Cleanup kind.\n  ///\n  /// \\param Addr When \\p LoadBlockVarAddr is false, the address of the __block\n  /// structure that will be passed to _Block_object_dispose. When\n  /// \\p LoadBlockVarAddr is true, the address of the field of the block\n  /// structure that holds the address of the __block structure.\n  ///\n  /// \\param Flags The flag that will be passed to _Block_object_dispose.\n  ///\n  /// \\param LoadBlockVarAddr Indicates whether we need to emit a load from\n  /// \\p Addr to get the address of the __block structure.\n  void enterByrefCleanup(CleanupKind Kind, Address Addr, BlockFieldFlags Flags,\n                         bool LoadBlockVarAddr, bool CanThrow);\n\n  void setBlockContextParameter(const ImplicitParamDecl *D, unsigned argNum,\n                                llvm::Value *ptr);\n\n  Address LoadBlockStruct();\n  Address GetAddrOfBlockDecl(const VarDecl *var);\n\n  /// BuildBlockByrefAddress - Computes the location of the\n  /// data in a variable which is declared as __block.\n  Address emitBlockByrefAddress(Address baseAddr, const VarDecl *V,\n                                bool followForward = true);\n  Address emitBlockByrefAddress(Address baseAddr,\n                                const BlockByrefInfo &info,\n                                bool followForward,\n                                const llvm::Twine &name);\n\n  const BlockByrefInfo &getBlockByrefInfo(const VarDecl *var);\n\n  QualType BuildFunctionArgList(GlobalDecl GD, FunctionArgList &Args);\n\n  void GenerateCode(GlobalDecl GD, llvm::Function *Fn,\n                    const CGFunctionInfo &FnInfo);\n\n  /// Annotate the function with an attribute that disables TSan checking at\n  /// runtime.\n  void markAsIgnoreThreadCheckingAtRuntime(llvm::Function *Fn);\n\n  /// Emit code for the start of a function.\n  /// \\param Loc       The location to be associated with the function.\n  /// \\param StartLoc  The location of the function body.\n  void StartFunction(GlobalDecl GD,\n                     QualType RetTy,\n                     llvm::Function *Fn,\n                     const CGFunctionInfo &FnInfo,\n                     const FunctionArgList &Args,\n                     SourceLocation Loc = SourceLocation(),\n                     SourceLocation StartLoc = SourceLocation());\n\n  static bool IsConstructorDelegationValid(const CXXConstructorDecl *Ctor);\n\n  void EmitConstructorBody(FunctionArgList &Args);\n  void EmitDestructorBody(FunctionArgList &Args);\n  void emitImplicitAssignmentOperatorBody(FunctionArgList &Args);\n  void EmitFunctionBody(const Stmt *Body);\n  void EmitBlockWithFallThrough(llvm::BasicBlock *BB, const Stmt *S);\n\n  void EmitForwardingCallToLambda(const CXXMethodDecl *LambdaCallOperator,\n                                  CallArgList &CallArgs);\n  void EmitLambdaBlockInvokeBody();\n  void EmitLambdaDelegatingInvokeBody(const CXXMethodDecl *MD);\n  void EmitLambdaStaticInvokeBody(const CXXMethodDecl *MD);\n  void EmitLambdaVLACapture(const VariableArrayType *VAT, LValue LV) {\n    EmitStoreThroughLValue(RValue::get(VLASizeMap[VAT->getSizeExpr()]), LV);\n  }\n  void EmitAsanPrologueOrEpilogue(bool Prologue);\n\n  /// Emit the unified return block, trying to avoid its emission when\n  /// possible.\n  /// \\return The debug location of the user written return statement if the\n  /// return block is is avoided.\n  llvm::DebugLoc EmitReturnBlock();\n\n  /// FinishFunction - Complete IR generation of the current function. It is\n  /// legal to call this function even if there is no current insertion point.\n  void FinishFunction(SourceLocation EndLoc=SourceLocation());\n\n  void StartThunk(llvm::Function *Fn, GlobalDecl GD,\n                  const CGFunctionInfo &FnInfo, bool IsUnprototyped);\n\n  void EmitCallAndReturnForThunk(llvm::FunctionCallee Callee,\n                                 const ThunkInfo *Thunk, bool IsUnprototyped);\n\n  void FinishThunk();\n\n  /// Emit a musttail call for a thunk with a potentially adjusted this pointer.\n  void EmitMustTailThunk(GlobalDecl GD, llvm::Value *AdjustedThisPtr,\n                         llvm::FunctionCallee Callee);\n\n  /// Generate a thunk for the given method.\n  void generateThunk(llvm::Function *Fn, const CGFunctionInfo &FnInfo,\n                     GlobalDecl GD, const ThunkInfo &Thunk,\n                     bool IsUnprototyped);\n\n  llvm::Function *GenerateVarArgsThunk(llvm::Function *Fn,\n                                       const CGFunctionInfo &FnInfo,\n                                       GlobalDecl GD, const ThunkInfo &Thunk);\n\n  void EmitCtorPrologue(const CXXConstructorDecl *CD, CXXCtorType Type,\n                        FunctionArgList &Args);\n\n  void EmitInitializerForField(FieldDecl *Field, LValue LHS, Expr *Init);\n\n  /// Struct with all information about dynamic [sub]class needed to set vptr.\n  struct VPtr {\n    BaseSubobject Base;\n    const CXXRecordDecl *NearestVBase;\n    CharUnits OffsetFromNearestVBase;\n    const CXXRecordDecl *VTableClass;\n  };\n\n  /// Initialize the vtable pointer of the given subobject.\n  void InitializeVTablePointer(const VPtr &vptr);\n\n  typedef llvm::SmallVector<VPtr, 4> VPtrsVector;\n\n  typedef llvm::SmallPtrSet<const CXXRecordDecl *, 4> VisitedVirtualBasesSetTy;\n  VPtrsVector getVTablePointers(const CXXRecordDecl *VTableClass);\n\n  void getVTablePointers(BaseSubobject Base, const CXXRecordDecl *NearestVBase,\n                         CharUnits OffsetFromNearestVBase,\n                         bool BaseIsNonVirtualPrimaryBase,\n                         const CXXRecordDecl *VTableClass,\n                         VisitedVirtualBasesSetTy &VBases, VPtrsVector &vptrs);\n\n  void InitializeVTablePointers(const CXXRecordDecl *ClassDecl);\n\n  /// GetVTablePtr - Return the Value of the vtable pointer member pointed\n  /// to by This.\n  llvm::Value *GetVTablePtr(Address This, llvm::Type *VTableTy,\n                            const CXXRecordDecl *VTableClass);\n\n  enum CFITypeCheckKind {\n    CFITCK_VCall,\n    CFITCK_NVCall,\n    CFITCK_DerivedCast,\n    CFITCK_UnrelatedCast,\n    CFITCK_ICall,\n    CFITCK_NVMFCall,\n    CFITCK_VMFCall,\n  };\n\n  /// Derived is the presumed address of an object of type T after a\n  /// cast. If T is a polymorphic class type, emit a check that the virtual\n  /// table for Derived belongs to a class derived from T.\n  void EmitVTablePtrCheckForCast(QualType T, llvm::Value *Derived,\n                                 bool MayBeNull, CFITypeCheckKind TCK,\n                                 SourceLocation Loc);\n\n  /// EmitVTablePtrCheckForCall - Virtual method MD is being called via VTable.\n  /// If vptr CFI is enabled, emit a check that VTable is valid.\n  void EmitVTablePtrCheckForCall(const CXXRecordDecl *RD, llvm::Value *VTable,\n                                 CFITypeCheckKind TCK, SourceLocation Loc);\n\n  /// EmitVTablePtrCheck - Emit a check that VTable is a valid virtual table for\n  /// RD using llvm.type.test.\n  void EmitVTablePtrCheck(const CXXRecordDecl *RD, llvm::Value *VTable,\n                          CFITypeCheckKind TCK, SourceLocation Loc);\n\n  /// If whole-program virtual table optimization is enabled, emit an assumption\n  /// that VTable is a member of RD's type identifier. Or, if vptr CFI is\n  /// enabled, emit a check that VTable is a member of RD's type identifier.\n  void EmitTypeMetadataCodeForVCall(const CXXRecordDecl *RD,\n                                    llvm::Value *VTable, SourceLocation Loc);\n\n  /// Returns whether we should perform a type checked load when loading a\n  /// virtual function for virtual calls to members of RD. This is generally\n  /// true when both vcall CFI and whole-program-vtables are enabled.\n  bool ShouldEmitVTableTypeCheckedLoad(const CXXRecordDecl *RD);\n\n  /// Emit a type checked load from the given vtable.\n  llvm::Value *EmitVTableTypeCheckedLoad(const CXXRecordDecl *RD, llvm::Value *VTable,\n                                         uint64_t VTableByteOffset);\n\n  /// EnterDtorCleanups - Enter the cleanups necessary to complete the\n  /// given phase of destruction for a destructor.  The end result\n  /// should call destructors on members and base classes in reverse\n  /// order of their construction.\n  void EnterDtorCleanups(const CXXDestructorDecl *Dtor, CXXDtorType Type);\n\n  /// ShouldInstrumentFunction - Return true if the current function should be\n  /// instrumented with __cyg_profile_func_* calls\n  bool ShouldInstrumentFunction();\n\n  /// ShouldXRayInstrument - Return true if the current function should be\n  /// instrumented with XRay nop sleds.\n  bool ShouldXRayInstrumentFunction() const;\n\n  /// AlwaysEmitXRayCustomEvents - Return true if we must unconditionally emit\n  /// XRay custom event handling calls.\n  bool AlwaysEmitXRayCustomEvents() const;\n\n  /// AlwaysEmitXRayTypedEvents - Return true if clang must unconditionally emit\n  /// XRay typed event handling calls.\n  bool AlwaysEmitXRayTypedEvents() const;\n\n  /// Encode an address into a form suitable for use in a function prologue.\n  llvm::Constant *EncodeAddrForUseInPrologue(llvm::Function *F,\n                                             llvm::Constant *Addr);\n\n  /// Decode an address used in a function prologue, encoded by \\c\n  /// EncodeAddrForUseInPrologue.\n  llvm::Value *DecodeAddrUsedInPrologue(llvm::Value *F,\n                                        llvm::Value *EncodedAddr);\n\n  /// EmitFunctionProlog - Emit the target specific LLVM code to load the\n  /// arguments for the given function. This is also responsible for naming the\n  /// LLVM function arguments.\n  void EmitFunctionProlog(const CGFunctionInfo &FI,\n                          llvm::Function *Fn,\n                          const FunctionArgList &Args);\n\n  /// EmitFunctionEpilog - Emit the target specific LLVM code to return the\n  /// given temporary.\n  void EmitFunctionEpilog(const CGFunctionInfo &FI, bool EmitRetDbgLoc,\n                          SourceLocation EndLoc);\n\n  /// Emit a test that checks if the return value \\p RV is nonnull.\n  void EmitReturnValueCheck(llvm::Value *RV);\n\n  /// EmitStartEHSpec - Emit the start of the exception spec.\n  void EmitStartEHSpec(const Decl *D);\n\n  /// EmitEndEHSpec - Emit the end of the exception spec.\n  void EmitEndEHSpec(const Decl *D);\n\n  /// getTerminateLandingPad - Return a landing pad that just calls terminate.\n  llvm::BasicBlock *getTerminateLandingPad();\n\n  /// getTerminateLandingPad - Return a cleanup funclet that just calls\n  /// terminate.\n  llvm::BasicBlock *getTerminateFunclet();\n\n  /// getTerminateHandler - Return a handler (not a landing pad, just\n  /// a catch handler) that just calls terminate.  This is used when\n  /// a terminate scope encloses a try.\n  llvm::BasicBlock *getTerminateHandler();\n\n  llvm::Type *ConvertTypeForMem(QualType T);\n  llvm::Type *ConvertType(QualType T);\n  llvm::Type *ConvertType(const TypeDecl *T) {\n    return ConvertType(getContext().getTypeDeclType(T));\n  }\n\n  /// LoadObjCSelf - Load the value of self. This function is only valid while\n  /// generating code for an Objective-C method.\n  llvm::Value *LoadObjCSelf();\n\n  /// TypeOfSelfObject - Return type of object that this self represents.\n  QualType TypeOfSelfObject();\n\n  /// getEvaluationKind - Return the TypeEvaluationKind of QualType \\c T.\n  static TypeEvaluationKind getEvaluationKind(QualType T);\n\n  static bool hasScalarEvaluationKind(QualType T) {\n    return getEvaluationKind(T) == TEK_Scalar;\n  }\n\n  static bool hasAggregateEvaluationKind(QualType T) {\n    return getEvaluationKind(T) == TEK_Aggregate;\n  }\n\n  /// createBasicBlock - Create an LLVM basic block.\n  llvm::BasicBlock *createBasicBlock(const Twine &name = \"\",\n                                     llvm::Function *parent = nullptr,\n                                     llvm::BasicBlock *before = nullptr) {\n    return llvm::BasicBlock::Create(getLLVMContext(), name, parent, before);\n  }\n\n  /// getBasicBlockForLabel - Return the LLVM basicblock that the specified\n  /// label maps to.\n  JumpDest getJumpDestForLabel(const LabelDecl *S);\n\n  /// SimplifyForwardingBlocks - If the given basic block is only a branch to\n  /// another basic block, simplify it. This assumes that no other code could\n  /// potentially reference the basic block.\n  void SimplifyForwardingBlocks(llvm::BasicBlock *BB);\n\n  /// EmitBlock - Emit the given block \\arg BB and set it as the insert point,\n  /// adding a fall-through branch from the current insert block if\n  /// necessary. It is legal to call this function even if there is no current\n  /// insertion point.\n  ///\n  /// IsFinished - If true, indicates that the caller has finished emitting\n  /// branches to the given block and does not expect to emit code into it. This\n  /// means the block can be ignored if it is unreachable.\n  void EmitBlock(llvm::BasicBlock *BB, bool IsFinished=false);\n\n  /// EmitBlockAfterUses - Emit the given block somewhere hopefully\n  /// near its uses, and leave the insertion point in it.\n  void EmitBlockAfterUses(llvm::BasicBlock *BB);\n\n  /// EmitBranch - Emit a branch to the specified basic block from the current\n  /// insert block, taking care to avoid creation of branches from dummy\n  /// blocks. It is legal to call this function even if there is no current\n  /// insertion point.\n  ///\n  /// This function clears the current insertion point. The caller should follow\n  /// calls to this function with calls to Emit*Block prior to generation new\n  /// code.\n  void EmitBranch(llvm::BasicBlock *Block);\n\n  /// HaveInsertPoint - True if an insertion point is defined. If not, this\n  /// indicates that the current code being emitted is unreachable.\n  bool HaveInsertPoint() const {\n    return Builder.GetInsertBlock() != nullptr;\n  }\n\n  /// EnsureInsertPoint - Ensure that an insertion point is defined so that\n  /// emitted IR has a place to go. Note that by definition, if this function\n  /// creates a block then that block is unreachable; callers may do better to\n  /// detect when no insertion point is defined and simply skip IR generation.\n  void EnsureInsertPoint() {\n    if (!HaveInsertPoint())\n      EmitBlock(createBasicBlock());\n  }\n\n  /// ErrorUnsupported - Print out an error that codegen doesn't support the\n  /// specified stmt yet.\n  void ErrorUnsupported(const Stmt *S, const char *Type);\n\n  //===--------------------------------------------------------------------===//\n  //                                  Helpers\n  //===--------------------------------------------------------------------===//\n\n  LValue MakeAddrLValue(Address Addr, QualType T,\n                        AlignmentSource Source = AlignmentSource::Type) {\n    return LValue::MakeAddr(Addr, T, getContext(), LValueBaseInfo(Source),\n                            CGM.getTBAAAccessInfo(T));\n  }\n\n  LValue MakeAddrLValue(Address Addr, QualType T, LValueBaseInfo BaseInfo,\n                        TBAAAccessInfo TBAAInfo) {\n    return LValue::MakeAddr(Addr, T, getContext(), BaseInfo, TBAAInfo);\n  }\n\n  LValue MakeAddrLValue(llvm::Value *V, QualType T, CharUnits Alignment,\n                        AlignmentSource Source = AlignmentSource::Type) {\n    return LValue::MakeAddr(Address(V, Alignment), T, getContext(),\n                            LValueBaseInfo(Source), CGM.getTBAAAccessInfo(T));\n  }\n\n  LValue MakeAddrLValue(llvm::Value *V, QualType T, CharUnits Alignment,\n                        LValueBaseInfo BaseInfo, TBAAAccessInfo TBAAInfo) {\n    return LValue::MakeAddr(Address(V, Alignment), T, getContext(),\n                            BaseInfo, TBAAInfo);\n  }\n\n  LValue MakeNaturalAlignPointeeAddrLValue(llvm::Value *V, QualType T);\n  LValue MakeNaturalAlignAddrLValue(llvm::Value *V, QualType T);\n\n  Address EmitLoadOfReference(LValue RefLVal,\n                              LValueBaseInfo *PointeeBaseInfo = nullptr,\n                              TBAAAccessInfo *PointeeTBAAInfo = nullptr);\n  LValue EmitLoadOfReferenceLValue(LValue RefLVal);\n  LValue EmitLoadOfReferenceLValue(Address RefAddr, QualType RefTy,\n                                   AlignmentSource Source =\n                                       AlignmentSource::Type) {\n    LValue RefLVal = MakeAddrLValue(RefAddr, RefTy, LValueBaseInfo(Source),\n                                    CGM.getTBAAAccessInfo(RefTy));\n    return EmitLoadOfReferenceLValue(RefLVal);\n  }\n\n  Address EmitLoadOfPointer(Address Ptr, const PointerType *PtrTy,\n                            LValueBaseInfo *BaseInfo = nullptr,\n                            TBAAAccessInfo *TBAAInfo = nullptr);\n  LValue EmitLoadOfPointerLValue(Address Ptr, const PointerType *PtrTy);\n\n  /// CreateTempAlloca - This creates an alloca and inserts it into the entry\n  /// block if \\p ArraySize is nullptr, otherwise inserts it at the current\n  /// insertion point of the builder. The caller is responsible for setting an\n  /// appropriate alignment on\n  /// the alloca.\n  ///\n  /// \\p ArraySize is the number of array elements to be allocated if it\n  ///    is not nullptr.\n  ///\n  /// LangAS::Default is the address space of pointers to local variables and\n  /// temporaries, as exposed in the source language. In certain\n  /// configurations, this is not the same as the alloca address space, and a\n  /// cast is needed to lift the pointer from the alloca AS into\n  /// LangAS::Default. This can happen when the target uses a restricted\n  /// address space for the stack but the source language requires\n  /// LangAS::Default to be a generic address space. The latter condition is\n  /// common for most programming languages; OpenCL is an exception in that\n  /// LangAS::Default is the private address space, which naturally maps\n  /// to the stack.\n  ///\n  /// Because the address of a temporary is often exposed to the program in\n  /// various ways, this function will perform the cast. The original alloca\n  /// instruction is returned through \\p Alloca if it is not nullptr.\n  ///\n  /// The cast is not performaed in CreateTempAllocaWithoutCast. This is\n  /// more efficient if the caller knows that the address will not be exposed.\n  llvm::AllocaInst *CreateTempAlloca(llvm::Type *Ty, const Twine &Name = \"tmp\",\n                                     llvm::Value *ArraySize = nullptr);\n  Address CreateTempAlloca(llvm::Type *Ty, CharUnits align,\n                           const Twine &Name = \"tmp\",\n                           llvm::Value *ArraySize = nullptr,\n                           Address *Alloca = nullptr);\n  Address CreateTempAllocaWithoutCast(llvm::Type *Ty, CharUnits align,\n                                      const Twine &Name = \"tmp\",\n                                      llvm::Value *ArraySize = nullptr);\n\n  /// CreateDefaultAlignedTempAlloca - This creates an alloca with the\n  /// default ABI alignment of the given LLVM type.\n  ///\n  /// IMPORTANT NOTE: This is *not* generally the right alignment for\n  /// any given AST type that happens to have been lowered to the\n  /// given IR type.  This should only ever be used for function-local,\n  /// IR-driven manipulations like saving and restoring a value.  Do\n  /// not hand this address off to arbitrary IRGen routines, and especially\n  /// do not pass it as an argument to a function that might expect a\n  /// properly ABI-aligned value.\n  Address CreateDefaultAlignTempAlloca(llvm::Type *Ty,\n                                       const Twine &Name = \"tmp\");\n\n  /// InitTempAlloca - Provide an initial value for the given alloca which\n  /// will be observable at all locations in the function.\n  ///\n  /// The address should be something that was returned from one of\n  /// the CreateTempAlloca or CreateMemTemp routines, and the\n  /// initializer must be valid in the entry block (i.e. it must\n  /// either be a constant or an argument value).\n  void InitTempAlloca(Address Alloca, llvm::Value *Value);\n\n  /// CreateIRTemp - Create a temporary IR object of the given type, with\n  /// appropriate alignment. This routine should only be used when an temporary\n  /// value needs to be stored into an alloca (for example, to avoid explicit\n  /// PHI construction), but the type is the IR type, not the type appropriate\n  /// for storing in memory.\n  ///\n  /// That is, this is exactly equivalent to CreateMemTemp, but calling\n  /// ConvertType instead of ConvertTypeForMem.\n  Address CreateIRTemp(QualType T, const Twine &Name = \"tmp\");\n\n  /// CreateMemTemp - Create a temporary memory object of the given type, with\n  /// appropriate alignmen and cast it to the default address space. Returns\n  /// the original alloca instruction by \\p Alloca if it is not nullptr.\n  Address CreateMemTemp(QualType T, const Twine &Name = \"tmp\",\n                        Address *Alloca = nullptr);\n  Address CreateMemTemp(QualType T, CharUnits Align, const Twine &Name = \"tmp\",\n                        Address *Alloca = nullptr);\n\n  /// CreateMemTemp - Create a temporary memory object of the given type, with\n  /// appropriate alignmen without casting it to the default address space.\n  Address CreateMemTempWithoutCast(QualType T, const Twine &Name = \"tmp\");\n  Address CreateMemTempWithoutCast(QualType T, CharUnits Align,\n                                   const Twine &Name = \"tmp\");\n\n  /// CreateAggTemp - Create a temporary memory object for the given\n  /// aggregate type.\n  AggValueSlot CreateAggTemp(QualType T, const Twine &Name = \"tmp\",\n                             Address *Alloca = nullptr) {\n    return AggValueSlot::forAddr(CreateMemTemp(T, Name, Alloca),\n                                 T.getQualifiers(),\n                                 AggValueSlot::IsNotDestructed,\n                                 AggValueSlot::DoesNotNeedGCBarriers,\n                                 AggValueSlot::IsNotAliased,\n                                 AggValueSlot::DoesNotOverlap);\n  }\n\n  /// Emit a cast to void* in the appropriate address space.\n  llvm::Value *EmitCastToVoidPtr(llvm::Value *value);\n\n  /// EvaluateExprAsBool - Perform the usual unary conversions on the specified\n  /// expression and compare the result against zero, returning an Int1Ty value.\n  llvm::Value *EvaluateExprAsBool(const Expr *E);\n\n  /// EmitIgnoredExpr - Emit an expression in a context which ignores the result.\n  void EmitIgnoredExpr(const Expr *E);\n\n  /// EmitAnyExpr - Emit code to compute the specified expression which can have\n  /// any type.  The result is returned as an RValue struct.  If this is an\n  /// aggregate expression, the aggloc/agglocvolatile arguments indicate where\n  /// the result should be returned.\n  ///\n  /// \\param ignoreResult True if the resulting value isn't used.\n  RValue EmitAnyExpr(const Expr *E,\n                     AggValueSlot aggSlot = AggValueSlot::ignored(),\n                     bool ignoreResult = false);\n\n  // EmitVAListRef - Emit a \"reference\" to a va_list; this is either the address\n  // or the value of the expression, depending on how va_list is defined.\n  Address EmitVAListRef(const Expr *E);\n\n  /// Emit a \"reference\" to a __builtin_ms_va_list; this is\n  /// always the value of the expression, because a __builtin_ms_va_list is a\n  /// pointer to a char.\n  Address EmitMSVAListRef(const Expr *E);\n\n  /// EmitAnyExprToTemp - Similarly to EmitAnyExpr(), however, the result will\n  /// always be accessible even if no aggregate location is provided.\n  RValue EmitAnyExprToTemp(const Expr *E);\n\n  /// EmitAnyExprToMem - Emits the code necessary to evaluate an\n  /// arbitrary expression into the given memory location.\n  void EmitAnyExprToMem(const Expr *E, Address Location,\n                        Qualifiers Quals, bool IsInitializer);\n\n  void EmitAnyExprToExn(const Expr *E, Address Addr);\n\n  /// EmitExprAsInit - Emits the code necessary to initialize a\n  /// location in memory with the given initializer.\n  void EmitExprAsInit(const Expr *init, const ValueDecl *D, LValue lvalue,\n                      bool capturedByInit);\n\n  /// hasVolatileMember - returns true if aggregate type has a volatile\n  /// member.\n  bool hasVolatileMember(QualType T) {\n    if (const RecordType *RT = T->getAs<RecordType>()) {\n      const RecordDecl *RD = cast<RecordDecl>(RT->getDecl());\n      return RD->hasVolatileMember();\n    }\n    return false;\n  }\n\n  /// Determine whether a return value slot may overlap some other object.\n  AggValueSlot::Overlap_t getOverlapForReturnValue() {\n    // FIXME: Assuming no overlap here breaks guaranteed copy elision for base\n    // class subobjects. These cases may need to be revisited depending on the\n    // resolution of the relevant core issue.\n    return AggValueSlot::DoesNotOverlap;\n  }\n\n  /// Determine whether a field initialization may overlap some other object.\n  AggValueSlot::Overlap_t getOverlapForFieldInit(const FieldDecl *FD);\n\n  /// Determine whether a base class initialization may overlap some other\n  /// object.\n  AggValueSlot::Overlap_t getOverlapForBaseInit(const CXXRecordDecl *RD,\n                                                const CXXRecordDecl *BaseRD,\n                                                bool IsVirtual);\n\n  /// Emit an aggregate assignment.\n  void EmitAggregateAssign(LValue Dest, LValue Src, QualType EltTy) {\n    bool IsVolatile = hasVolatileMember(EltTy);\n    EmitAggregateCopy(Dest, Src, EltTy, AggValueSlot::MayOverlap, IsVolatile);\n  }\n\n  void EmitAggregateCopyCtor(LValue Dest, LValue Src,\n                             AggValueSlot::Overlap_t MayOverlap) {\n    EmitAggregateCopy(Dest, Src, Src.getType(), MayOverlap);\n  }\n\n  /// EmitAggregateCopy - Emit an aggregate copy.\n  ///\n  /// \\param isVolatile \\c true iff either the source or the destination is\n  ///        volatile.\n  /// \\param MayOverlap Whether the tail padding of the destination might be\n  ///        occupied by some other object. More efficient code can often be\n  ///        generated if not.\n  void EmitAggregateCopy(LValue Dest, LValue Src, QualType EltTy,\n                         AggValueSlot::Overlap_t MayOverlap,\n                         bool isVolatile = false);\n\n  /// GetAddrOfLocalVar - Return the address of a local variable.\n  Address GetAddrOfLocalVar(const VarDecl *VD) {\n    auto it = LocalDeclMap.find(VD);\n    assert(it != LocalDeclMap.end() &&\n           \"Invalid argument to GetAddrOfLocalVar(), no decl!\");\n    return it->second;\n  }\n\n  /// Given an opaque value expression, return its LValue mapping if it exists,\n  /// otherwise create one.\n  LValue getOrCreateOpaqueLValueMapping(const OpaqueValueExpr *e);\n\n  /// Given an opaque value expression, return its RValue mapping if it exists,\n  /// otherwise create one.\n  RValue getOrCreateOpaqueRValueMapping(const OpaqueValueExpr *e);\n\n  /// Get the index of the current ArrayInitLoopExpr, if any.\n  llvm::Value *getArrayInitIndex() { return ArrayInitIndex; }\n\n  /// getAccessedFieldNo - Given an encoded value and a result number, return\n  /// the input field number being accessed.\n  static unsigned getAccessedFieldNo(unsigned Idx, const llvm::Constant *Elts);\n\n  llvm::BlockAddress *GetAddrOfLabel(const LabelDecl *L);\n  llvm::BasicBlock *GetIndirectGotoBlock();\n\n  /// Check if \\p E is a C++ \"this\" pointer wrapped in value-preserving casts.\n  static bool IsWrappedCXXThis(const Expr *E);\n\n  /// EmitNullInitialization - Generate code to set a value of the given type to\n  /// null, If the type contains data member pointers, they will be initialized\n  /// to -1 in accordance with the Itanium C++ ABI.\n  void EmitNullInitialization(Address DestPtr, QualType Ty);\n\n  /// Emits a call to an LLVM variable-argument intrinsic, either\n  /// \\c llvm.va_start or \\c llvm.va_end.\n  /// \\param ArgValue A reference to the \\c va_list as emitted by either\n  /// \\c EmitVAListRef or \\c EmitMSVAListRef.\n  /// \\param IsStart If \\c true, emits a call to \\c llvm.va_start; otherwise,\n  /// calls \\c llvm.va_end.\n  llvm::Value *EmitVAStartEnd(llvm::Value *ArgValue, bool IsStart);\n\n  /// Generate code to get an argument from the passed in pointer\n  /// and update it accordingly.\n  /// \\param VE The \\c VAArgExpr for which to generate code.\n  /// \\param VAListAddr Receives a reference to the \\c va_list as emitted by\n  /// either \\c EmitVAListRef or \\c EmitMSVAListRef.\n  /// \\returns A pointer to the argument.\n  // FIXME: We should be able to get rid of this method and use the va_arg\n  // instruction in LLVM instead once it works well enough.\n  Address EmitVAArg(VAArgExpr *VE, Address &VAListAddr);\n\n  /// emitArrayLength - Compute the length of an array, even if it's a\n  /// VLA, and drill down to the base element type.\n  llvm::Value *emitArrayLength(const ArrayType *arrayType,\n                               QualType &baseType,\n                               Address &addr);\n\n  /// EmitVLASize - Capture all the sizes for the VLA expressions in\n  /// the given variably-modified type and store them in the VLASizeMap.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitVariablyModifiedType(QualType Ty);\n\n  struct VlaSizePair {\n    llvm::Value *NumElts;\n    QualType Type;\n\n    VlaSizePair(llvm::Value *NE, QualType T) : NumElts(NE), Type(T) {}\n  };\n\n  /// Return the number of elements for a single dimension\n  /// for the given array type.\n  VlaSizePair getVLAElements1D(const VariableArrayType *vla);\n  VlaSizePair getVLAElements1D(QualType vla);\n\n  /// Returns an LLVM value that corresponds to the size,\n  /// in non-variably-sized elements, of a variable length array type,\n  /// plus that largest non-variably-sized element type.  Assumes that\n  /// the type has already been emitted with EmitVariablyModifiedType.\n  VlaSizePair getVLASize(const VariableArrayType *vla);\n  VlaSizePair getVLASize(QualType vla);\n\n  /// LoadCXXThis - Load the value of 'this'. This function is only valid while\n  /// generating code for an C++ member function.\n  llvm::Value *LoadCXXThis() {\n    assert(CXXThisValue && \"no 'this' value for this function\");\n    return CXXThisValue;\n  }\n  Address LoadCXXThisAddress();\n\n  /// LoadCXXVTT - Load the VTT parameter to base constructors/destructors have\n  /// virtual bases.\n  // FIXME: Every place that calls LoadCXXVTT is something\n  // that needs to be abstracted properly.\n  llvm::Value *LoadCXXVTT() {\n    assert(CXXStructorImplicitParamValue && \"no VTT value for this function\");\n    return CXXStructorImplicitParamValue;\n  }\n\n  /// GetAddressOfBaseOfCompleteClass - Convert the given pointer to a\n  /// complete class to the given direct base.\n  Address\n  GetAddressOfDirectBaseInCompleteClass(Address Value,\n                                        const CXXRecordDecl *Derived,\n                                        const CXXRecordDecl *Base,\n                                        bool BaseIsVirtual);\n\n  static bool ShouldNullCheckClassCastValue(const CastExpr *Cast);\n\n  /// GetAddressOfBaseClass - This function will add the necessary delta to the\n  /// load of 'this' and returns address of the base class.\n  Address GetAddressOfBaseClass(Address Value,\n                                const CXXRecordDecl *Derived,\n                                CastExpr::path_const_iterator PathBegin,\n                                CastExpr::path_const_iterator PathEnd,\n                                bool NullCheckValue, SourceLocation Loc);\n\n  Address GetAddressOfDerivedClass(Address Value,\n                                   const CXXRecordDecl *Derived,\n                                   CastExpr::path_const_iterator PathBegin,\n                                   CastExpr::path_const_iterator PathEnd,\n                                   bool NullCheckValue);\n\n  /// GetVTTParameter - Return the VTT parameter that should be passed to a\n  /// base constructor/destructor with virtual bases.\n  /// FIXME: VTTs are Itanium ABI-specific, so the definition should move\n  /// to ItaniumCXXABI.cpp together with all the references to VTT.\n  llvm::Value *GetVTTParameter(GlobalDecl GD, bool ForVirtualBase,\n                               bool Delegating);\n\n  void EmitDelegateCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                      CXXCtorType CtorType,\n                                      const FunctionArgList &Args,\n                                      SourceLocation Loc);\n  // It's important not to confuse this and the previous function. Delegating\n  // constructors are the C++0x feature. The constructor delegate optimization\n  // is used to reduce duplication in the base and complete consturctors where\n  // they are substantially the same.\n  void EmitDelegatingCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                        const FunctionArgList &Args);\n\n  /// Emit a call to an inheriting constructor (that is, one that invokes a\n  /// constructor inherited from a base class) by inlining its definition. This\n  /// is necessary if the ABI does not support forwarding the arguments to the\n  /// base class constructor (because they're variadic or similar).\n  void EmitInlinedInheritingCXXConstructorCall(const CXXConstructorDecl *Ctor,\n                                               CXXCtorType CtorType,\n                                               bool ForVirtualBase,\n                                               bool Delegating,\n                                               CallArgList &Args);\n\n  /// Emit a call to a constructor inherited from a base class, passing the\n  /// current constructor's arguments along unmodified (without even making\n  /// a copy).\n  void EmitInheritedCXXConstructorCall(const CXXConstructorDecl *D,\n                                       bool ForVirtualBase, Address This,\n                                       bool InheritedFromVBase,\n                                       const CXXInheritedCtorInitExpr *E);\n\n  void EmitCXXConstructorCall(const CXXConstructorDecl *D, CXXCtorType Type,\n                              bool ForVirtualBase, bool Delegating,\n                              AggValueSlot ThisAVS, const CXXConstructExpr *E);\n\n  void EmitCXXConstructorCall(const CXXConstructorDecl *D, CXXCtorType Type,\n                              bool ForVirtualBase, bool Delegating,\n                              Address This, CallArgList &Args,\n                              AggValueSlot::Overlap_t Overlap,\n                              SourceLocation Loc, bool NewPointerIsChecked);\n\n  /// Emit assumption load for all bases. Requires to be be called only on\n  /// most-derived class and not under construction of the object.\n  void EmitVTableAssumptionLoads(const CXXRecordDecl *ClassDecl, Address This);\n\n  /// Emit assumption that vptr load == global vtable.\n  void EmitVTableAssumptionLoad(const VPtr &vptr, Address This);\n\n  void EmitSynthesizedCXXCopyCtorCall(const CXXConstructorDecl *D,\n                                      Address This, Address Src,\n                                      const CXXConstructExpr *E);\n\n  void EmitCXXAggrConstructorCall(const CXXConstructorDecl *D,\n                                  const ArrayType *ArrayTy,\n                                  Address ArrayPtr,\n                                  const CXXConstructExpr *E,\n                                  bool NewPointerIsChecked,\n                                  bool ZeroInitialization = false);\n\n  void EmitCXXAggrConstructorCall(const CXXConstructorDecl *D,\n                                  llvm::Value *NumElements,\n                                  Address ArrayPtr,\n                                  const CXXConstructExpr *E,\n                                  bool NewPointerIsChecked,\n                                  bool ZeroInitialization = false);\n\n  static Destroyer destroyCXXObject;\n\n  void EmitCXXDestructorCall(const CXXDestructorDecl *D, CXXDtorType Type,\n                             bool ForVirtualBase, bool Delegating, Address This,\n                             QualType ThisTy);\n\n  void EmitNewArrayInitializer(const CXXNewExpr *E, QualType elementType,\n                               llvm::Type *ElementTy, Address NewPtr,\n                               llvm::Value *NumElements,\n                               llvm::Value *AllocSizeWithoutCookie);\n\n  void EmitCXXTemporary(const CXXTemporary *Temporary, QualType TempType,\n                        Address Ptr);\n\n  llvm::Value *EmitLifetimeStart(uint64_t Size, llvm::Value *Addr);\n  void EmitLifetimeEnd(llvm::Value *Size, llvm::Value *Addr);\n\n  llvm::Value *EmitCXXNewExpr(const CXXNewExpr *E);\n  void EmitCXXDeleteExpr(const CXXDeleteExpr *E);\n\n  void EmitDeleteCall(const FunctionDecl *DeleteFD, llvm::Value *Ptr,\n                      QualType DeleteTy, llvm::Value *NumElements = nullptr,\n                      CharUnits CookieSize = CharUnits());\n\n  RValue EmitBuiltinNewDeleteCall(const FunctionProtoType *Type,\n                                  const CallExpr *TheCallExpr, bool IsDelete);\n\n  llvm::Value *EmitCXXTypeidExpr(const CXXTypeidExpr *E);\n  llvm::Value *EmitDynamicCast(Address V, const CXXDynamicCastExpr *DCE);\n  Address EmitCXXUuidofExpr(const CXXUuidofExpr *E);\n\n  /// Situations in which we might emit a check for the suitability of a\n  /// pointer or glvalue. Needs to be kept in sync with ubsan_handlers.cpp in\n  /// compiler-rt.\n  enum TypeCheckKind {\n    /// Checking the operand of a load. Must be suitably sized and aligned.\n    TCK_Load,\n    /// Checking the destination of a store. Must be suitably sized and aligned.\n    TCK_Store,\n    /// Checking the bound value in a reference binding. Must be suitably sized\n    /// and aligned, but is not required to refer to an object (until the\n    /// reference is used), per core issue 453.\n    TCK_ReferenceBinding,\n    /// Checking the object expression in a non-static data member access. Must\n    /// be an object within its lifetime.\n    TCK_MemberAccess,\n    /// Checking the 'this' pointer for a call to a non-static member function.\n    /// Must be an object within its lifetime.\n    TCK_MemberCall,\n    /// Checking the 'this' pointer for a constructor call.\n    TCK_ConstructorCall,\n    /// Checking the operand of a static_cast to a derived pointer type. Must be\n    /// null or an object within its lifetime.\n    TCK_DowncastPointer,\n    /// Checking the operand of a static_cast to a derived reference type. Must\n    /// be an object within its lifetime.\n    TCK_DowncastReference,\n    /// Checking the operand of a cast to a base object. Must be suitably sized\n    /// and aligned.\n    TCK_Upcast,\n    /// Checking the operand of a cast to a virtual base object. Must be an\n    /// object within its lifetime.\n    TCK_UpcastToVirtualBase,\n    /// Checking the value assigned to a _Nonnull pointer. Must not be null.\n    TCK_NonnullAssign,\n    /// Checking the operand of a dynamic_cast or a typeid expression.  Must be\n    /// null or an object within its lifetime.\n    TCK_DynamicOperation\n  };\n\n  /// Determine whether the pointer type check \\p TCK permits null pointers.\n  static bool isNullPointerAllowed(TypeCheckKind TCK);\n\n  /// Determine whether the pointer type check \\p TCK requires a vptr check.\n  static bool isVptrCheckRequired(TypeCheckKind TCK, QualType Ty);\n\n  /// Whether any type-checking sanitizers are enabled. If \\c false,\n  /// calls to EmitTypeCheck can be skipped.\n  bool sanitizePerformTypeCheck() const;\n\n  /// Emit a check that \\p V is the address of storage of the\n  /// appropriate size and alignment for an object of type \\p Type\n  /// (or if ArraySize is provided, for an array of that bound).\n  void EmitTypeCheck(TypeCheckKind TCK, SourceLocation Loc, llvm::Value *V,\n                     QualType Type, CharUnits Alignment = CharUnits::Zero(),\n                     SanitizerSet SkippedChecks = SanitizerSet(),\n                     llvm::Value *ArraySize = nullptr);\n\n  /// Emit a check that \\p Base points into an array object, which\n  /// we can access at index \\p Index. \\p Accessed should be \\c false if we\n  /// this expression is used as an lvalue, for instance in \"&Arr[Idx]\".\n  void EmitBoundsCheck(const Expr *E, const Expr *Base, llvm::Value *Index,\n                       QualType IndexType, bool Accessed);\n\n  llvm::Value *EmitScalarPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                       bool isInc, bool isPre);\n  ComplexPairTy EmitComplexPrePostIncDec(const UnaryOperator *E, LValue LV,\n                                         bool isInc, bool isPre);\n\n  /// Converts Location to a DebugLoc, if debug information is enabled.\n  llvm::DebugLoc SourceLocToDebugLoc(SourceLocation Location);\n\n  /// Get the record field index as represented in debug info.\n  unsigned getDebugInfoFIndex(const RecordDecl *Rec, unsigned FieldIndex);\n\n\n  //===--------------------------------------------------------------------===//\n  //                            Declaration Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitDecl - Emit a declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitDecl(const Decl &D);\n\n  /// EmitVarDecl - Emit a local variable declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitVarDecl(const VarDecl &D);\n\n  void EmitScalarInit(const Expr *init, const ValueDecl *D, LValue lvalue,\n                      bool capturedByInit);\n\n  typedef void SpecialInitFn(CodeGenFunction &Init, const VarDecl &D,\n                             llvm::Value *Address);\n\n  /// Determine whether the given initializer is trivial in the sense\n  /// that it requires no code to be generated.\n  bool isTrivialInitializer(const Expr *Init);\n\n  /// EmitAutoVarDecl - Emit an auto variable declaration.\n  ///\n  /// This function can be called with a null (unreachable) insert point.\n  void EmitAutoVarDecl(const VarDecl &D);\n\n  class AutoVarEmission {\n    friend class CodeGenFunction;\n\n    const VarDecl *Variable;\n\n    /// The address of the alloca for languages with explicit address space\n    /// (e.g. OpenCL) or alloca casted to generic pointer for address space\n    /// agnostic languages (e.g. C++). Invalid if the variable was emitted\n    /// as a global constant.\n    Address Addr;\n\n    llvm::Value *NRVOFlag;\n\n    /// True if the variable is a __block variable that is captured by an\n    /// escaping block.\n    bool IsEscapingByRef;\n\n    /// True if the variable is of aggregate type and has a constant\n    /// initializer.\n    bool IsConstantAggregate;\n\n    /// Non-null if we should use lifetime annotations.\n    llvm::Value *SizeForLifetimeMarkers;\n\n    /// Address with original alloca instruction. Invalid if the variable was\n    /// emitted as a global constant.\n    Address AllocaAddr;\n\n    struct Invalid {};\n    AutoVarEmission(Invalid)\n        : Variable(nullptr), Addr(Address::invalid()),\n          AllocaAddr(Address::invalid()) {}\n\n    AutoVarEmission(const VarDecl &variable)\n        : Variable(&variable), Addr(Address::invalid()), NRVOFlag(nullptr),\n          IsEscapingByRef(false), IsConstantAggregate(false),\n          SizeForLifetimeMarkers(nullptr), AllocaAddr(Address::invalid()) {}\n\n    bool wasEmittedAsGlobal() const { return !Addr.isValid(); }\n\n  public:\n    static AutoVarEmission invalid() { return AutoVarEmission(Invalid()); }\n\n    bool useLifetimeMarkers() const {\n      return SizeForLifetimeMarkers != nullptr;\n    }\n    llvm::Value *getSizeForLifetimeMarkers() const {\n      assert(useLifetimeMarkers());\n      return SizeForLifetimeMarkers;\n    }\n\n    /// Returns the raw, allocated address, which is not necessarily\n    /// the address of the object itself. It is casted to default\n    /// address space for address space agnostic languages.\n    Address getAllocatedAddress() const {\n      return Addr;\n    }\n\n    /// Returns the address for the original alloca instruction.\n    Address getOriginalAllocatedAddress() const { return AllocaAddr; }\n\n    /// Returns the address of the object within this declaration.\n    /// Note that this does not chase the forwarding pointer for\n    /// __block decls.\n    Address getObjectAddress(CodeGenFunction &CGF) const {\n      if (!IsEscapingByRef) return Addr;\n\n      return CGF.emitBlockByrefAddress(Addr, Variable, /*forward*/ false);\n    }\n  };\n  AutoVarEmission EmitAutoVarAlloca(const VarDecl &var);\n  void EmitAutoVarInit(const AutoVarEmission &emission);\n  void EmitAutoVarCleanups(const AutoVarEmission &emission);\n  void emitAutoVarTypeCleanup(const AutoVarEmission &emission,\n                              QualType::DestructionKind dtorKind);\n\n  /// Emits the alloca and debug information for the size expressions for each\n  /// dimension of an array. It registers the association of its (1-dimensional)\n  /// QualTypes and size expression's debug node, so that CGDebugInfo can\n  /// reference this node when creating the DISubrange object to describe the\n  /// array types.\n  void EmitAndRegisterVariableArrayDimensions(CGDebugInfo *DI,\n                                              const VarDecl &D,\n                                              bool EmitDebugInfo);\n\n  void EmitStaticVarDecl(const VarDecl &D,\n                         llvm::GlobalValue::LinkageTypes Linkage);\n\n  class ParamValue {\n    llvm::Value *Value;\n    unsigned Alignment;\n    ParamValue(llvm::Value *V, unsigned A) : Value(V), Alignment(A) {}\n  public:\n    static ParamValue forDirect(llvm::Value *value) {\n      return ParamValue(value, 0);\n    }\n    static ParamValue forIndirect(Address addr) {\n      assert(!addr.getAlignment().isZero());\n      return ParamValue(addr.getPointer(), addr.getAlignment().getQuantity());\n    }\n\n    bool isIndirect() const { return Alignment != 0; }\n    llvm::Value *getAnyValue() const { return Value; }\n\n    llvm::Value *getDirectValue() const {\n      assert(!isIndirect());\n      return Value;\n    }\n\n    Address getIndirectAddress() const {\n      assert(isIndirect());\n      return Address(Value, CharUnits::fromQuantity(Alignment));\n    }\n  };\n\n  /// EmitParmDecl - Emit a ParmVarDecl or an ImplicitParamDecl.\n  void EmitParmDecl(const VarDecl &D, ParamValue Arg, unsigned ArgNo);\n\n  /// protectFromPeepholes - Protect a value that we're intending to\n  /// store to the side, but which will probably be used later, from\n  /// aggressive peepholing optimizations that might delete it.\n  ///\n  /// Pass the result to unprotectFromPeepholes to declare that\n  /// protection is no longer required.\n  ///\n  /// There's no particular reason why this shouldn't apply to\n  /// l-values, it's just that no existing peepholes work on pointers.\n  PeepholeProtection protectFromPeepholes(RValue rvalue);\n  void unprotectFromPeepholes(PeepholeProtection protection);\n\n  void emitAlignmentAssumptionCheck(llvm::Value *Ptr, QualType Ty,\n                                    SourceLocation Loc,\n                                    SourceLocation AssumptionLoc,\n                                    llvm::Value *Alignment,\n                                    llvm::Value *OffsetValue,\n                                    llvm::Value *TheCheck,\n                                    llvm::Instruction *Assumption);\n\n  void emitAlignmentAssumption(llvm::Value *PtrValue, QualType Ty,\n                               SourceLocation Loc, SourceLocation AssumptionLoc,\n                               llvm::Value *Alignment,\n                               llvm::Value *OffsetValue = nullptr);\n\n  void emitAlignmentAssumption(llvm::Value *PtrValue, const Expr *E,\n                               SourceLocation AssumptionLoc,\n                               llvm::Value *Alignment,\n                               llvm::Value *OffsetValue = nullptr);\n\n  //===--------------------------------------------------------------------===//\n  //                             Statement Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitStopPoint - Emit a debug stoppoint if we are emitting debug info.\n  void EmitStopPoint(const Stmt *S);\n\n  /// EmitStmt - Emit the code for the statement \\arg S. It is legal to call\n  /// this function even if there is no current insertion point.\n  ///\n  /// This function may clear the current insertion point; callers should use\n  /// EnsureInsertPoint if they wish to subsequently generate code without first\n  /// calling EmitBlock, EmitBranch, or EmitStmt.\n  void EmitStmt(const Stmt *S, ArrayRef<const Attr *> Attrs = None);\n\n  /// EmitSimpleStmt - Try to emit a \"simple\" statement which does not\n  /// necessarily require an insertion point or debug information; typically\n  /// because the statement amounts to a jump or a container of other\n  /// statements.\n  ///\n  /// \\return True if the statement was handled.\n  bool EmitSimpleStmt(const Stmt *S, ArrayRef<const Attr *> Attrs);\n\n  Address EmitCompoundStmt(const CompoundStmt &S, bool GetLast = false,\n                           AggValueSlot AVS = AggValueSlot::ignored());\n  Address EmitCompoundStmtWithoutScope(const CompoundStmt &S,\n                                       bool GetLast = false,\n                                       AggValueSlot AVS =\n                                                AggValueSlot::ignored());\n\n  /// EmitLabel - Emit the block for the given label. It is legal to call this\n  /// function even if there is no current insertion point.\n  void EmitLabel(const LabelDecl *D); // helper for EmitLabelStmt.\n\n  void EmitLabelStmt(const LabelStmt &S);\n  void EmitAttributedStmt(const AttributedStmt &S);\n  void EmitGotoStmt(const GotoStmt &S);\n  void EmitIndirectGotoStmt(const IndirectGotoStmt &S);\n  void EmitIfStmt(const IfStmt &S);\n\n  void EmitWhileStmt(const WhileStmt &S,\n                     ArrayRef<const Attr *> Attrs = None);\n  void EmitDoStmt(const DoStmt &S, ArrayRef<const Attr *> Attrs = None);\n  void EmitForStmt(const ForStmt &S,\n                   ArrayRef<const Attr *> Attrs = None);\n  void EmitReturnStmt(const ReturnStmt &S);\n  void EmitDeclStmt(const DeclStmt &S);\n  void EmitBreakStmt(const BreakStmt &S);\n  void EmitContinueStmt(const ContinueStmt &S);\n  void EmitSwitchStmt(const SwitchStmt &S);\n  void EmitDefaultStmt(const DefaultStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitCaseStmt(const CaseStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitCaseStmtRange(const CaseStmt &S, ArrayRef<const Attr *> Attrs);\n  void EmitAsmStmt(const AsmStmt &S);\n\n  void EmitObjCForCollectionStmt(const ObjCForCollectionStmt &S);\n  void EmitObjCAtTryStmt(const ObjCAtTryStmt &S);\n  void EmitObjCAtThrowStmt(const ObjCAtThrowStmt &S);\n  void EmitObjCAtSynchronizedStmt(const ObjCAtSynchronizedStmt &S);\n  void EmitObjCAutoreleasePoolStmt(const ObjCAutoreleasePoolStmt &S);\n\n  void EmitCoroutineBody(const CoroutineBodyStmt &S);\n  void EmitCoreturnStmt(const CoreturnStmt &S);\n  RValue EmitCoawaitExpr(const CoawaitExpr &E,\n                         AggValueSlot aggSlot = AggValueSlot::ignored(),\n                         bool ignoreResult = false);\n  LValue EmitCoawaitLValue(const CoawaitExpr *E);\n  RValue EmitCoyieldExpr(const CoyieldExpr &E,\n                         AggValueSlot aggSlot = AggValueSlot::ignored(),\n                         bool ignoreResult = false);\n  LValue EmitCoyieldLValue(const CoyieldExpr *E);\n  RValue EmitCoroutineIntrinsic(const CallExpr *E, unsigned int IID);\n\n  void EnterCXXTryStmt(const CXXTryStmt &S, bool IsFnTryBlock = false);\n  void ExitCXXTryStmt(const CXXTryStmt &S, bool IsFnTryBlock = false);\n\n  void EmitCXXTryStmt(const CXXTryStmt &S);\n  void EmitSEHTryStmt(const SEHTryStmt &S);\n  void EmitSEHLeaveStmt(const SEHLeaveStmt &S);\n  void EnterSEHTryStmt(const SEHTryStmt &S);\n  void ExitSEHTryStmt(const SEHTryStmt &S);\n\n  void pushSEHCleanup(CleanupKind kind,\n                      llvm::Function *FinallyFunc);\n  void startOutlinedSEHHelper(CodeGenFunction &ParentCGF, bool IsFilter,\n                              const Stmt *OutlinedStmt);\n\n  llvm::Function *GenerateSEHFilterFunction(CodeGenFunction &ParentCGF,\n                                            const SEHExceptStmt &Except);\n\n  llvm::Function *GenerateSEHFinallyFunction(CodeGenFunction &ParentCGF,\n                                             const SEHFinallyStmt &Finally);\n\n  void EmitSEHExceptionCodeSave(CodeGenFunction &ParentCGF,\n                                llvm::Value *ParentFP,\n                                llvm::Value *EntryEBP);\n  llvm::Value *EmitSEHExceptionCode();\n  llvm::Value *EmitSEHExceptionInfo();\n  llvm::Value *EmitSEHAbnormalTermination();\n\n  /// Emit simple code for OpenMP directives in Simd-only mode.\n  void EmitSimpleOMPExecutableDirective(const OMPExecutableDirective &D);\n\n  /// Scan the outlined statement for captures from the parent function. For\n  /// each capture, mark the capture as escaped and emit a call to\n  /// llvm.localrecover. Insert the localrecover result into the LocalDeclMap.\n  void EmitCapturedLocals(CodeGenFunction &ParentCGF, const Stmt *OutlinedStmt,\n                          bool IsFilter);\n\n  /// Recovers the address of a local in a parent function. ParentVar is the\n  /// address of the variable used in the immediate parent function. It can\n  /// either be an alloca or a call to llvm.localrecover if there are nested\n  /// outlined functions. ParentFP is the frame pointer of the outermost parent\n  /// frame.\n  Address recoverAddrOfEscapedLocal(CodeGenFunction &ParentCGF,\n                                    Address ParentVar,\n                                    llvm::Value *ParentFP);\n\n  void EmitCXXForRangeStmt(const CXXForRangeStmt &S,\n                           ArrayRef<const Attr *> Attrs = None);\n\n  /// Controls insertion of cancellation exit blocks in worksharing constructs.\n  class OMPCancelStackRAII {\n    CodeGenFunction &CGF;\n\n  public:\n    OMPCancelStackRAII(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,\n                       bool HasCancel)\n        : CGF(CGF) {\n      CGF.OMPCancelStack.enter(CGF, Kind, HasCancel);\n    }\n    ~OMPCancelStackRAII() { CGF.OMPCancelStack.exit(CGF); }\n  };\n\n  /// Returns calculated size of the specified type.\n  llvm::Value *getTypeSize(QualType Ty);\n  LValue InitCapturedStruct(const CapturedStmt &S);\n  llvm::Function *EmitCapturedStmt(const CapturedStmt &S, CapturedRegionKind K);\n  llvm::Function *GenerateCapturedStmtFunction(const CapturedStmt &S);\n  Address GenerateCapturedStmtArgument(const CapturedStmt &S);\n  llvm::Function *GenerateOpenMPCapturedStmtFunction(const CapturedStmt &S,\n                                                     SourceLocation Loc);\n  void GenerateOpenMPCapturedVars(const CapturedStmt &S,\n                                  SmallVectorImpl<llvm::Value *> &CapturedVars);\n  void emitOMPSimpleStore(LValue LVal, RValue RVal, QualType RValTy,\n                          SourceLocation Loc);\n  /// Perform element by element copying of arrays with type \\a\n  /// OriginalType from \\a SrcAddr to \\a DestAddr using copying procedure\n  /// generated by \\a CopyGen.\n  ///\n  /// \\param DestAddr Address of the destination array.\n  /// \\param SrcAddr Address of the source array.\n  /// \\param OriginalType Type of destination and source arrays.\n  /// \\param CopyGen Copying procedure that copies value of single array element\n  /// to another single array element.\n  void EmitOMPAggregateAssign(\n      Address DestAddr, Address SrcAddr, QualType OriginalType,\n      const llvm::function_ref<void(Address, Address)> CopyGen);\n  /// Emit proper copying of data from one variable to another.\n  ///\n  /// \\param OriginalType Original type of the copied variables.\n  /// \\param DestAddr Destination address.\n  /// \\param SrcAddr Source address.\n  /// \\param DestVD Destination variable used in \\a CopyExpr (for arrays, has\n  /// type of the base array element).\n  /// \\param SrcVD Source variable used in \\a CopyExpr (for arrays, has type of\n  /// the base array element).\n  /// \\param Copy Actual copygin expression for copying data from \\a SrcVD to \\a\n  /// DestVD.\n  void EmitOMPCopy(QualType OriginalType,\n                   Address DestAddr, Address SrcAddr,\n                   const VarDecl *DestVD, const VarDecl *SrcVD,\n                   const Expr *Copy);\n  /// Emit atomic update code for constructs: \\a X = \\a X \\a BO \\a E or\n  /// \\a X = \\a E \\a BO \\a E.\n  ///\n  /// \\param X Value to be updated.\n  /// \\param E Update value.\n  /// \\param BO Binary operation for update operation.\n  /// \\param IsXLHSInRHSPart true if \\a X is LHS in RHS part of the update\n  /// expression, false otherwise.\n  /// \\param AO Atomic ordering of the generated atomic instructions.\n  /// \\param CommonGen Code generator for complex expressions that cannot be\n  /// expressed through atomicrmw instruction.\n  /// \\returns <true, OldAtomicValue> if simple 'atomicrmw' instruction was\n  /// generated, <false, RValue::get(nullptr)> otherwise.\n  std::pair<bool, RValue> EmitOMPAtomicSimpleUpdateExpr(\n      LValue X, RValue E, BinaryOperatorKind BO, bool IsXLHSInRHSPart,\n      llvm::AtomicOrdering AO, SourceLocation Loc,\n      const llvm::function_ref<RValue(RValue)> CommonGen);\n  bool EmitOMPFirstprivateClause(const OMPExecutableDirective &D,\n                                 OMPPrivateScope &PrivateScope);\n  void EmitOMPPrivateClause(const OMPExecutableDirective &D,\n                            OMPPrivateScope &PrivateScope);\n  void EmitOMPUseDevicePtrClause(\n      const OMPUseDevicePtrClause &C, OMPPrivateScope &PrivateScope,\n      const llvm::DenseMap<const ValueDecl *, Address> &CaptureDeviceAddrMap);\n  void EmitOMPUseDeviceAddrClause(\n      const OMPUseDeviceAddrClause &C, OMPPrivateScope &PrivateScope,\n      const llvm::DenseMap<const ValueDecl *, Address> &CaptureDeviceAddrMap);\n  /// Emit code for copyin clause in \\a D directive. The next code is\n  /// generated at the start of outlined functions for directives:\n  /// \\code\n  /// threadprivate_var1 = master_threadprivate_var1;\n  /// operator=(threadprivate_var2, master_threadprivate_var2);\n  /// ...\n  /// __kmpc_barrier(&loc, global_tid);\n  /// \\endcode\n  ///\n  /// \\param D OpenMP directive possibly with 'copyin' clause(s).\n  /// \\returns true if at least one copyin variable is found, false otherwise.\n  bool EmitOMPCopyinClause(const OMPExecutableDirective &D);\n  /// Emit initial code for lastprivate variables. If some variable is\n  /// not also firstprivate, then the default initialization is used. Otherwise\n  /// initialization of this variable is performed by EmitOMPFirstprivateClause\n  /// method.\n  ///\n  /// \\param D Directive that may have 'lastprivate' directives.\n  /// \\param PrivateScope Private scope for capturing lastprivate variables for\n  /// proper codegen in internal captured statement.\n  ///\n  /// \\returns true if there is at least one lastprivate variable, false\n  /// otherwise.\n  bool EmitOMPLastprivateClauseInit(const OMPExecutableDirective &D,\n                                    OMPPrivateScope &PrivateScope);\n  /// Emit final copying of lastprivate values to original variables at\n  /// the end of the worksharing or simd directive.\n  ///\n  /// \\param D Directive that has at least one 'lastprivate' directives.\n  /// \\param IsLastIterCond Boolean condition that must be set to 'i1 true' if\n  /// it is the last iteration of the loop code in associated directive, or to\n  /// 'i1 false' otherwise. If this item is nullptr, no final check is required.\n  void EmitOMPLastprivateClauseFinal(const OMPExecutableDirective &D,\n                                     bool NoFinals,\n                                     llvm::Value *IsLastIterCond = nullptr);\n  /// Emit initial code for linear clauses.\n  void EmitOMPLinearClause(const OMPLoopDirective &D,\n                           CodeGenFunction::OMPPrivateScope &PrivateScope);\n  /// Emit final code for linear clauses.\n  /// \\param CondGen Optional conditional code for final part of codegen for\n  /// linear clause.\n  void EmitOMPLinearClauseFinal(\n      const OMPLoopDirective &D,\n      const llvm::function_ref<llvm::Value *(CodeGenFunction &)> CondGen);\n  /// Emit initial code for reduction variables. Creates reduction copies\n  /// and initializes them with the values according to OpenMP standard.\n  ///\n  /// \\param D Directive (possibly) with the 'reduction' clause.\n  /// \\param PrivateScope Private scope for capturing reduction variables for\n  /// proper codegen in internal captured statement.\n  ///\n  void EmitOMPReductionClauseInit(const OMPExecutableDirective &D,\n                                  OMPPrivateScope &PrivateScope,\n                                  bool ForInscan = false);\n  /// Emit final update of reduction values to original variables at\n  /// the end of the directive.\n  ///\n  /// \\param D Directive that has at least one 'reduction' directives.\n  /// \\param ReductionKind The kind of reduction to perform.\n  void EmitOMPReductionClauseFinal(const OMPExecutableDirective &D,\n                                   const OpenMPDirectiveKind ReductionKind);\n  /// Emit initial code for linear variables. Creates private copies\n  /// and initializes them with the values according to OpenMP standard.\n  ///\n  /// \\param D Directive (possibly) with the 'linear' clause.\n  /// \\return true if at least one linear variable is found that should be\n  /// initialized with the value of the original variable, false otherwise.\n  bool EmitOMPLinearClauseInit(const OMPLoopDirective &D);\n\n  typedef const llvm::function_ref<void(CodeGenFunction & /*CGF*/,\n                                        llvm::Function * /*OutlinedFn*/,\n                                        const OMPTaskDataTy & /*Data*/)>\n      TaskGenTy;\n  void EmitOMPTaskBasedDirective(const OMPExecutableDirective &S,\n                                 const OpenMPDirectiveKind CapturedRegion,\n                                 const RegionCodeGenTy &BodyGen,\n                                 const TaskGenTy &TaskGen, OMPTaskDataTy &Data);\n  struct OMPTargetDataInfo {\n    Address BasePointersArray = Address::invalid();\n    Address PointersArray = Address::invalid();\n    Address SizesArray = Address::invalid();\n    Address MappersArray = Address::invalid();\n    unsigned NumberOfTargetItems = 0;\n    explicit OMPTargetDataInfo() = default;\n    OMPTargetDataInfo(Address BasePointersArray, Address PointersArray,\n                      Address SizesArray, Address MappersArray,\n                      unsigned NumberOfTargetItems)\n        : BasePointersArray(BasePointersArray), PointersArray(PointersArray),\n          SizesArray(SizesArray), MappersArray(MappersArray),\n          NumberOfTargetItems(NumberOfTargetItems) {}\n  };\n  void EmitOMPTargetTaskBasedDirective(const OMPExecutableDirective &S,\n                                       const RegionCodeGenTy &BodyGen,\n                                       OMPTargetDataInfo &InputInfo);\n\n  void EmitOMPParallelDirective(const OMPParallelDirective &S);\n  void EmitOMPSimdDirective(const OMPSimdDirective &S);\n  void EmitOMPTileDirective(const OMPTileDirective &S);\n  void EmitOMPForDirective(const OMPForDirective &S);\n  void EmitOMPForSimdDirective(const OMPForSimdDirective &S);\n  void EmitOMPSectionsDirective(const OMPSectionsDirective &S);\n  void EmitOMPSectionDirective(const OMPSectionDirective &S);\n  void EmitOMPSingleDirective(const OMPSingleDirective &S);\n  void EmitOMPMasterDirective(const OMPMasterDirective &S);\n  void EmitOMPCriticalDirective(const OMPCriticalDirective &S);\n  void EmitOMPParallelForDirective(const OMPParallelForDirective &S);\n  void EmitOMPParallelForSimdDirective(const OMPParallelForSimdDirective &S);\n  void EmitOMPParallelSectionsDirective(const OMPParallelSectionsDirective &S);\n  void EmitOMPParallelMasterDirective(const OMPParallelMasterDirective &S);\n  void EmitOMPTaskDirective(const OMPTaskDirective &S);\n  void EmitOMPTaskyieldDirective(const OMPTaskyieldDirective &S);\n  void EmitOMPBarrierDirective(const OMPBarrierDirective &S);\n  void EmitOMPTaskwaitDirective(const OMPTaskwaitDirective &S);\n  void EmitOMPTaskgroupDirective(const OMPTaskgroupDirective &S);\n  void EmitOMPFlushDirective(const OMPFlushDirective &S);\n  void EmitOMPDepobjDirective(const OMPDepobjDirective &S);\n  void EmitOMPScanDirective(const OMPScanDirective &S);\n  void EmitOMPOrderedDirective(const OMPOrderedDirective &S);\n  void EmitOMPAtomicDirective(const OMPAtomicDirective &S);\n  void EmitOMPTargetDirective(const OMPTargetDirective &S);\n  void EmitOMPTargetDataDirective(const OMPTargetDataDirective &S);\n  void EmitOMPTargetEnterDataDirective(const OMPTargetEnterDataDirective &S);\n  void EmitOMPTargetExitDataDirective(const OMPTargetExitDataDirective &S);\n  void EmitOMPTargetUpdateDirective(const OMPTargetUpdateDirective &S);\n  void EmitOMPTargetParallelDirective(const OMPTargetParallelDirective &S);\n  void\n  EmitOMPTargetParallelForDirective(const OMPTargetParallelForDirective &S);\n  void EmitOMPTeamsDirective(const OMPTeamsDirective &S);\n  void\n  EmitOMPCancellationPointDirective(const OMPCancellationPointDirective &S);\n  void EmitOMPCancelDirective(const OMPCancelDirective &S);\n  void EmitOMPTaskLoopBasedDirective(const OMPLoopDirective &S);\n  void EmitOMPTaskLoopDirective(const OMPTaskLoopDirective &S);\n  void EmitOMPTaskLoopSimdDirective(const OMPTaskLoopSimdDirective &S);\n  void EmitOMPMasterTaskLoopDirective(const OMPMasterTaskLoopDirective &S);\n  void\n  EmitOMPMasterTaskLoopSimdDirective(const OMPMasterTaskLoopSimdDirective &S);\n  void EmitOMPParallelMasterTaskLoopDirective(\n      const OMPParallelMasterTaskLoopDirective &S);\n  void EmitOMPParallelMasterTaskLoopSimdDirective(\n      const OMPParallelMasterTaskLoopSimdDirective &S);\n  void EmitOMPDistributeDirective(const OMPDistributeDirective &S);\n  void EmitOMPDistributeParallelForDirective(\n      const OMPDistributeParallelForDirective &S);\n  void EmitOMPDistributeParallelForSimdDirective(\n      const OMPDistributeParallelForSimdDirective &S);\n  void EmitOMPDistributeSimdDirective(const OMPDistributeSimdDirective &S);\n  void EmitOMPTargetParallelForSimdDirective(\n      const OMPTargetParallelForSimdDirective &S);\n  void EmitOMPTargetSimdDirective(const OMPTargetSimdDirective &S);\n  void EmitOMPTeamsDistributeDirective(const OMPTeamsDistributeDirective &S);\n  void\n  EmitOMPTeamsDistributeSimdDirective(const OMPTeamsDistributeSimdDirective &S);\n  void EmitOMPTeamsDistributeParallelForSimdDirective(\n      const OMPTeamsDistributeParallelForSimdDirective &S);\n  void EmitOMPTeamsDistributeParallelForDirective(\n      const OMPTeamsDistributeParallelForDirective &S);\n  void EmitOMPTargetTeamsDirective(const OMPTargetTeamsDirective &S);\n  void EmitOMPTargetTeamsDistributeDirective(\n      const OMPTargetTeamsDistributeDirective &S);\n  void EmitOMPTargetTeamsDistributeParallelForDirective(\n      const OMPTargetTeamsDistributeParallelForDirective &S);\n  void EmitOMPTargetTeamsDistributeParallelForSimdDirective(\n      const OMPTargetTeamsDistributeParallelForSimdDirective &S);\n  void EmitOMPTargetTeamsDistributeSimdDirective(\n      const OMPTargetTeamsDistributeSimdDirective &S);\n\n  /// Emit device code for the target directive.\n  static void EmitOMPTargetDeviceFunction(CodeGenModule &CGM,\n                                          StringRef ParentName,\n                                          const OMPTargetDirective &S);\n  static void\n  EmitOMPTargetParallelDeviceFunction(CodeGenModule &CGM, StringRef ParentName,\n                                      const OMPTargetParallelDirective &S);\n  /// Emit device code for the target parallel for directive.\n  static void EmitOMPTargetParallelForDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetParallelForDirective &S);\n  /// Emit device code for the target parallel for simd directive.\n  static void EmitOMPTargetParallelForSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetParallelForSimdDirective &S);\n  /// Emit device code for the target teams directive.\n  static void\n  EmitOMPTargetTeamsDeviceFunction(CodeGenModule &CGM, StringRef ParentName,\n                                   const OMPTargetTeamsDirective &S);\n  /// Emit device code for the target teams distribute directive.\n  static void EmitOMPTargetTeamsDistributeDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeDirective &S);\n  /// Emit device code for the target teams distribute simd directive.\n  static void EmitOMPTargetTeamsDistributeSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeSimdDirective &S);\n  /// Emit device code for the target simd directive.\n  static void EmitOMPTargetSimdDeviceFunction(CodeGenModule &CGM,\n                                              StringRef ParentName,\n                                              const OMPTargetSimdDirective &S);\n  /// Emit device code for the target teams distribute parallel for simd\n  /// directive.\n  static void EmitOMPTargetTeamsDistributeParallelForSimdDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeParallelForSimdDirective &S);\n\n  static void EmitOMPTargetTeamsDistributeParallelForDeviceFunction(\n      CodeGenModule &CGM, StringRef ParentName,\n      const OMPTargetTeamsDistributeParallelForDirective &S);\n\n  /// Emit the Stmt \\p S and return its topmost canonical loop, if any.\n  /// TODO: The \\p Depth paramter is not yet implemented and must be 1. In the\n  /// future it is meant to be the number of loops expected in the loop nests\n  /// (usually specified by the \"collapse\" clause) that are collapsed to a\n  /// single loop by this function.\n  llvm::CanonicalLoopInfo *EmitOMPCollapsedCanonicalLoopNest(const Stmt *S,\n                                                             int Depth);\n\n  /// Emit an OMPCanonicalLoop using the OpenMPIRBuilder.\n  void EmitOMPCanonicalLoop(const OMPCanonicalLoop *S);\n\n  /// Emit inner loop of the worksharing/simd construct.\n  ///\n  /// \\param S Directive, for which the inner loop must be emitted.\n  /// \\param RequiresCleanup true, if directive has some associated private\n  /// variables.\n  /// \\param LoopCond Bollean condition for loop continuation.\n  /// \\param IncExpr Increment expression for loop control variable.\n  /// \\param BodyGen Generator for the inner body of the inner loop.\n  /// \\param PostIncGen Genrator for post-increment code (required for ordered\n  /// loop directvies).\n  void EmitOMPInnerLoop(\n      const OMPExecutableDirective &S, bool RequiresCleanup,\n      const Expr *LoopCond, const Expr *IncExpr,\n      const llvm::function_ref<void(CodeGenFunction &)> BodyGen,\n      const llvm::function_ref<void(CodeGenFunction &)> PostIncGen);\n\n  JumpDest getOMPCancelDestination(OpenMPDirectiveKind Kind);\n  /// Emit initial code for loop counters of loop-based directives.\n  void EmitOMPPrivateLoopCounters(const OMPLoopDirective &S,\n                                  OMPPrivateScope &LoopScope);\n\n  /// Helper for the OpenMP loop directives.\n  void EmitOMPLoopBody(const OMPLoopDirective &D, JumpDest LoopExit);\n\n  /// Emit code for the worksharing loop-based directive.\n  /// \\return true, if this construct has any lastprivate clause, false -\n  /// otherwise.\n  bool EmitOMPWorksharingLoop(const OMPLoopDirective &S, Expr *EUB,\n                              const CodeGenLoopBoundsTy &CodeGenLoopBounds,\n                              const CodeGenDispatchBoundsTy &CGDispatchBounds);\n\n  /// Emit code for the distribute loop-based directive.\n  void EmitOMPDistributeLoop(const OMPLoopDirective &S,\n                             const CodeGenLoopTy &CodeGenLoop, Expr *IncExpr);\n\n  /// Helpers for the OpenMP loop directives.\n  void EmitOMPSimdInit(const OMPLoopDirective &D, bool IsMonotonic = false);\n  void EmitOMPSimdFinal(\n      const OMPLoopDirective &D,\n      const llvm::function_ref<llvm::Value *(CodeGenFunction &)> CondGen);\n\n  /// Emits the lvalue for the expression with possibly captured variable.\n  LValue EmitOMPSharedLValue(const Expr *E);\n\nprivate:\n  /// Helpers for blocks.\n  llvm::Value *EmitBlockLiteral(const CGBlockInfo &Info);\n\n  /// struct with the values to be passed to the OpenMP loop-related functions\n  struct OMPLoopArguments {\n    /// loop lower bound\n    Address LB = Address::invalid();\n    /// loop upper bound\n    Address UB = Address::invalid();\n    /// loop stride\n    Address ST = Address::invalid();\n    /// isLastIteration argument for runtime functions\n    Address IL = Address::invalid();\n    /// Chunk value generated by sema\n    llvm::Value *Chunk = nullptr;\n    /// EnsureUpperBound\n    Expr *EUB = nullptr;\n    /// IncrementExpression\n    Expr *IncExpr = nullptr;\n    /// Loop initialization\n    Expr *Init = nullptr;\n    /// Loop exit condition\n    Expr *Cond = nullptr;\n    /// Update of LB after a whole chunk has been executed\n    Expr *NextLB = nullptr;\n    /// Update of UB after a whole chunk has been executed\n    Expr *NextUB = nullptr;\n    OMPLoopArguments() = default;\n    OMPLoopArguments(Address LB, Address UB, Address ST, Address IL,\n                     llvm::Value *Chunk = nullptr, Expr *EUB = nullptr,\n                     Expr *IncExpr = nullptr, Expr *Init = nullptr,\n                     Expr *Cond = nullptr, Expr *NextLB = nullptr,\n                     Expr *NextUB = nullptr)\n        : LB(LB), UB(UB), ST(ST), IL(IL), Chunk(Chunk), EUB(EUB),\n          IncExpr(IncExpr), Init(Init), Cond(Cond), NextLB(NextLB),\n          NextUB(NextUB) {}\n  };\n  void EmitOMPOuterLoop(bool DynamicOrOrdered, bool IsMonotonic,\n                        const OMPLoopDirective &S, OMPPrivateScope &LoopScope,\n                        const OMPLoopArguments &LoopArgs,\n                        const CodeGenLoopTy &CodeGenLoop,\n                        const CodeGenOrderedTy &CodeGenOrdered);\n  void EmitOMPForOuterLoop(const OpenMPScheduleTy &ScheduleKind,\n                           bool IsMonotonic, const OMPLoopDirective &S,\n                           OMPPrivateScope &LoopScope, bool Ordered,\n                           const OMPLoopArguments &LoopArgs,\n                           const CodeGenDispatchBoundsTy &CGDispatchBounds);\n  void EmitOMPDistributeOuterLoop(OpenMPDistScheduleClauseKind ScheduleKind,\n                                  const OMPLoopDirective &S,\n                                  OMPPrivateScope &LoopScope,\n                                  const OMPLoopArguments &LoopArgs,\n                                  const CodeGenLoopTy &CodeGenLoopContent);\n  /// Emit code for sections directive.\n  void EmitSections(const OMPExecutableDirective &S);\n\npublic:\n\n  //===--------------------------------------------------------------------===//\n  //                         LValue Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  /// Create a check that a scalar RValue is non-null.\n  llvm::Value *EmitNonNullRValueCheck(RValue RV, QualType T);\n\n  /// GetUndefRValue - Get an appropriate 'undef' rvalue for the given type.\n  RValue GetUndefRValue(QualType Ty);\n\n  /// EmitUnsupportedRValue - Emit a dummy r-value using the type of E\n  /// and issue an ErrorUnsupported style diagnostic (using the\n  /// provided Name).\n  RValue EmitUnsupportedRValue(const Expr *E,\n                               const char *Name);\n\n  /// EmitUnsupportedLValue - Emit a dummy l-value using the type of E and issue\n  /// an ErrorUnsupported style diagnostic (using the provided Name).\n  LValue EmitUnsupportedLValue(const Expr *E,\n                               const char *Name);\n\n  /// EmitLValue - Emit code to compute a designator that specifies the location\n  /// of the expression.\n  ///\n  /// This can return one of two things: a simple address or a bitfield\n  /// reference.  In either case, the LLVM Value* in the LValue structure is\n  /// guaranteed to be an LLVM pointer type.\n  ///\n  /// If this returns a bitfield reference, nothing about the pointee type of\n  /// the LLVM value is known: For example, it may not be a pointer to an\n  /// integer.\n  ///\n  /// If this returns a normal address, and if the lvalue's C type is fixed\n  /// size, this method guarantees that the returned pointer type will point to\n  /// an LLVM type of the same size of the lvalue's type.  If the lvalue has a\n  /// variable length type, this is not possible.\n  ///\n  LValue EmitLValue(const Expr *E);\n\n  /// Same as EmitLValue but additionally we generate checking code to\n  /// guard against undefined behavior.  This is only suitable when we know\n  /// that the address will be used to access the object.\n  LValue EmitCheckedLValue(const Expr *E, TypeCheckKind TCK);\n\n  RValue convertTempToRValue(Address addr, QualType type,\n                             SourceLocation Loc);\n\n  void EmitAtomicInit(Expr *E, LValue lvalue);\n\n  bool LValueIsSuitableForInlineAtomic(LValue Src);\n\n  RValue EmitAtomicLoad(LValue LV, SourceLocation SL,\n                        AggValueSlot Slot = AggValueSlot::ignored());\n\n  RValue EmitAtomicLoad(LValue lvalue, SourceLocation loc,\n                        llvm::AtomicOrdering AO, bool IsVolatile = false,\n                        AggValueSlot slot = AggValueSlot::ignored());\n\n  void EmitAtomicStore(RValue rvalue, LValue lvalue, bool isInit);\n\n  void EmitAtomicStore(RValue rvalue, LValue lvalue, llvm::AtomicOrdering AO,\n                       bool IsVolatile, bool isInit);\n\n  std::pair<RValue, llvm::Value *> EmitAtomicCompareExchange(\n      LValue Obj, RValue Expected, RValue Desired, SourceLocation Loc,\n      llvm::AtomicOrdering Success =\n          llvm::AtomicOrdering::SequentiallyConsistent,\n      llvm::AtomicOrdering Failure =\n          llvm::AtomicOrdering::SequentiallyConsistent,\n      bool IsWeak = false, AggValueSlot Slot = AggValueSlot::ignored());\n\n  void EmitAtomicUpdate(LValue LVal, llvm::AtomicOrdering AO,\n                        const llvm::function_ref<RValue(RValue)> &UpdateOp,\n                        bool IsVolatile);\n\n  /// EmitToMemory - Change a scalar value from its value\n  /// representation to its in-memory representation.\n  llvm::Value *EmitToMemory(llvm::Value *Value, QualType Ty);\n\n  /// EmitFromMemory - Change a scalar value from its memory\n  /// representation to its value representation.\n  llvm::Value *EmitFromMemory(llvm::Value *Value, QualType Ty);\n\n  /// Check if the scalar \\p Value is within the valid range for the given\n  /// type \\p Ty.\n  ///\n  /// Returns true if a check is needed (even if the range is unknown).\n  bool EmitScalarRangeCheck(llvm::Value *Value, QualType Ty,\n                            SourceLocation Loc);\n\n  /// EmitLoadOfScalar - Load a scalar value from an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.\n  llvm::Value *EmitLoadOfScalar(Address Addr, bool Volatile, QualType Ty,\n                                SourceLocation Loc,\n                                AlignmentSource Source = AlignmentSource::Type,\n                                bool isNontemporal = false) {\n    return EmitLoadOfScalar(Addr, Volatile, Ty, Loc, LValueBaseInfo(Source),\n                            CGM.getTBAAAccessInfo(Ty), isNontemporal);\n  }\n\n  llvm::Value *EmitLoadOfScalar(Address Addr, bool Volatile, QualType Ty,\n                                SourceLocation Loc, LValueBaseInfo BaseInfo,\n                                TBAAAccessInfo TBAAInfo,\n                                bool isNontemporal = false);\n\n  /// EmitLoadOfScalar - Load a scalar value from an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.  The l-value must be a simple\n  /// l-value.\n  llvm::Value *EmitLoadOfScalar(LValue lvalue, SourceLocation Loc);\n\n  /// EmitStoreOfScalar - Store a scalar value to an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.\n  void EmitStoreOfScalar(llvm::Value *Value, Address Addr,\n                         bool Volatile, QualType Ty,\n                         AlignmentSource Source = AlignmentSource::Type,\n                         bool isInit = false, bool isNontemporal = false) {\n    EmitStoreOfScalar(Value, Addr, Volatile, Ty, LValueBaseInfo(Source),\n                      CGM.getTBAAAccessInfo(Ty), isInit, isNontemporal);\n  }\n\n  void EmitStoreOfScalar(llvm::Value *Value, Address Addr,\n                         bool Volatile, QualType Ty,\n                         LValueBaseInfo BaseInfo, TBAAAccessInfo TBAAInfo,\n                         bool isInit = false, bool isNontemporal = false);\n\n  /// EmitStoreOfScalar - Store a scalar value to an address, taking\n  /// care to appropriately convert from the memory representation to\n  /// the LLVM value representation.  The l-value must be a simple\n  /// l-value.  The isInit flag indicates whether this is an initialization.\n  /// If so, atomic qualifiers are ignored and the store is always non-atomic.\n  void EmitStoreOfScalar(llvm::Value *value, LValue lvalue, bool isInit=false);\n\n  /// EmitLoadOfLValue - Given an expression that represents a value lvalue,\n  /// this method emits the address of the lvalue, then loads the result as an\n  /// rvalue, returning the rvalue.\n  RValue EmitLoadOfLValue(LValue V, SourceLocation Loc);\n  RValue EmitLoadOfExtVectorElementLValue(LValue V);\n  RValue EmitLoadOfBitfieldLValue(LValue LV, SourceLocation Loc);\n  RValue EmitLoadOfGlobalRegLValue(LValue LV);\n\n  /// EmitStoreThroughLValue - Store the specified rvalue into the specified\n  /// lvalue, where both are guaranteed to the have the same type, and that type\n  /// is 'Ty'.\n  void EmitStoreThroughLValue(RValue Src, LValue Dst, bool isInit = false);\n  void EmitStoreThroughExtVectorComponentLValue(RValue Src, LValue Dst);\n  void EmitStoreThroughGlobalRegLValue(RValue Src, LValue Dst);\n\n  /// EmitStoreThroughBitfieldLValue - Store Src into Dst with same constraints\n  /// as EmitStoreThroughLValue.\n  ///\n  /// \\param Result [out] - If non-null, this will be set to a Value* for the\n  /// bit-field contents after the store, appropriate for use as the result of\n  /// an assignment to the bit-field.\n  void EmitStoreThroughBitfieldLValue(RValue Src, LValue Dst,\n                                      llvm::Value **Result=nullptr);\n\n  /// Emit an l-value for an assignment (simple or compound) of complex type.\n  LValue EmitComplexAssignmentLValue(const BinaryOperator *E);\n  LValue EmitComplexCompoundAssignmentLValue(const CompoundAssignOperator *E);\n  LValue EmitScalarCompoundAssignWithComplex(const CompoundAssignOperator *E,\n                                             llvm::Value *&Result);\n\n  // Note: only available for agg return types\n  LValue EmitBinaryOperatorLValue(const BinaryOperator *E);\n  LValue EmitCompoundAssignmentLValue(const CompoundAssignOperator *E);\n  // Note: only available for agg return types\n  LValue EmitCallExprLValue(const CallExpr *E);\n  // Note: only available for agg return types\n  LValue EmitVAArgExprLValue(const VAArgExpr *E);\n  LValue EmitDeclRefLValue(const DeclRefExpr *E);\n  LValue EmitStringLiteralLValue(const StringLiteral *E);\n  LValue EmitObjCEncodeExprLValue(const ObjCEncodeExpr *E);\n  LValue EmitPredefinedLValue(const PredefinedExpr *E);\n  LValue EmitUnaryOpLValue(const UnaryOperator *E);\n  LValue EmitArraySubscriptExpr(const ArraySubscriptExpr *E,\n                                bool Accessed = false);\n  LValue EmitMatrixSubscriptExpr(const MatrixSubscriptExpr *E);\n  LValue EmitOMPArraySectionExpr(const OMPArraySectionExpr *E,\n                                 bool IsLowerBound = true);\n  LValue EmitExtVectorElementExpr(const ExtVectorElementExpr *E);\n  LValue EmitMemberExpr(const MemberExpr *E);\n  LValue EmitObjCIsaExpr(const ObjCIsaExpr *E);\n  LValue EmitCompoundLiteralLValue(const CompoundLiteralExpr *E);\n  LValue EmitInitListLValue(const InitListExpr *E);\n  LValue EmitConditionalOperatorLValue(const AbstractConditionalOperator *E);\n  LValue EmitCastLValue(const CastExpr *E);\n  LValue EmitMaterializeTemporaryExpr(const MaterializeTemporaryExpr *E);\n  LValue EmitOpaqueValueLValue(const OpaqueValueExpr *e);\n\n  Address EmitExtVectorElementLValue(LValue V);\n\n  RValue EmitRValueForField(LValue LV, const FieldDecl *FD, SourceLocation Loc);\n\n  Address EmitArrayToPointerDecay(const Expr *Array,\n                                  LValueBaseInfo *BaseInfo = nullptr,\n                                  TBAAAccessInfo *TBAAInfo = nullptr);\n\n  class ConstantEmission {\n    llvm::PointerIntPair<llvm::Constant*, 1, bool> ValueAndIsReference;\n    ConstantEmission(llvm::Constant *C, bool isReference)\n      : ValueAndIsReference(C, isReference) {}\n  public:\n    ConstantEmission() {}\n    static ConstantEmission forReference(llvm::Constant *C) {\n      return ConstantEmission(C, true);\n    }\n    static ConstantEmission forValue(llvm::Constant *C) {\n      return ConstantEmission(C, false);\n    }\n\n    explicit operator bool() const {\n      return ValueAndIsReference.getOpaqueValue() != nullptr;\n    }\n\n    bool isReference() const { return ValueAndIsReference.getInt(); }\n    LValue getReferenceLValue(CodeGenFunction &CGF, Expr *refExpr) const {\n      assert(isReference());\n      return CGF.MakeNaturalAlignAddrLValue(ValueAndIsReference.getPointer(),\n                                            refExpr->getType());\n    }\n\n    llvm::Constant *getValue() const {\n      assert(!isReference());\n      return ValueAndIsReference.getPointer();\n    }\n  };\n\n  ConstantEmission tryEmitAsConstant(DeclRefExpr *refExpr);\n  ConstantEmission tryEmitAsConstant(const MemberExpr *ME);\n  llvm::Value *emitScalarConstant(const ConstantEmission &Constant, Expr *E);\n\n  RValue EmitPseudoObjectRValue(const PseudoObjectExpr *e,\n                                AggValueSlot slot = AggValueSlot::ignored());\n  LValue EmitPseudoObjectLValue(const PseudoObjectExpr *e);\n\n  llvm::Value *EmitIvarOffset(const ObjCInterfaceDecl *Interface,\n                              const ObjCIvarDecl *Ivar);\n  LValue EmitLValueForField(LValue Base, const FieldDecl* Field);\n  LValue EmitLValueForLambdaField(const FieldDecl *Field);\n\n  /// EmitLValueForFieldInitialization - Like EmitLValueForField, except that\n  /// if the Field is a reference, this will return the address of the reference\n  /// and not the address of the value stored in the reference.\n  LValue EmitLValueForFieldInitialization(LValue Base,\n                                          const FieldDecl* Field);\n\n  LValue EmitLValueForIvar(QualType ObjectTy,\n                           llvm::Value* Base, const ObjCIvarDecl *Ivar,\n                           unsigned CVRQualifiers);\n\n  LValue EmitCXXConstructLValue(const CXXConstructExpr *E);\n  LValue EmitCXXBindTemporaryLValue(const CXXBindTemporaryExpr *E);\n  LValue EmitCXXTypeidLValue(const CXXTypeidExpr *E);\n  LValue EmitCXXUuidofLValue(const CXXUuidofExpr *E);\n\n  LValue EmitObjCMessageExprLValue(const ObjCMessageExpr *E);\n  LValue EmitObjCIvarRefLValue(const ObjCIvarRefExpr *E);\n  LValue EmitStmtExprLValue(const StmtExpr *E);\n  LValue EmitPointerToDataMemberBinaryExpr(const BinaryOperator *E);\n  LValue EmitObjCSelectorLValue(const ObjCSelectorExpr *E);\n  void   EmitDeclRefExprDbgValue(const DeclRefExpr *E, const APValue &Init);\n\n  //===--------------------------------------------------------------------===//\n  //                         Scalar Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  /// EmitCall - Generate a call of the given function, expecting the given\n  /// result type, and using the given argument list which specifies both the\n  /// LLVM arguments and the types they were derived from.\n  RValue EmitCall(const CGFunctionInfo &CallInfo, const CGCallee &Callee,\n                  ReturnValueSlot ReturnValue, const CallArgList &Args,\n                  llvm::CallBase **callOrInvoke, SourceLocation Loc);\n  RValue EmitCall(const CGFunctionInfo &CallInfo, const CGCallee &Callee,\n                  ReturnValueSlot ReturnValue, const CallArgList &Args,\n                  llvm::CallBase **callOrInvoke = nullptr) {\n    return EmitCall(CallInfo, Callee, ReturnValue, Args, callOrInvoke,\n                    SourceLocation());\n  }\n  RValue EmitCall(QualType FnType, const CGCallee &Callee, const CallExpr *E,\n                  ReturnValueSlot ReturnValue, llvm::Value *Chain = nullptr);\n  RValue EmitCallExpr(const CallExpr *E,\n                      ReturnValueSlot ReturnValue = ReturnValueSlot());\n  RValue EmitSimpleCallExpr(const CallExpr *E, ReturnValueSlot ReturnValue);\n  CGCallee EmitCallee(const Expr *E);\n\n  void checkTargetFeatures(const CallExpr *E, const FunctionDecl *TargetDecl);\n  void checkTargetFeatures(SourceLocation Loc, const FunctionDecl *TargetDecl);\n\n  llvm::CallInst *EmitRuntimeCall(llvm::FunctionCallee callee,\n                                  const Twine &name = \"\");\n  llvm::CallInst *EmitRuntimeCall(llvm::FunctionCallee callee,\n                                  ArrayRef<llvm::Value *> args,\n                                  const Twine &name = \"\");\n  llvm::CallInst *EmitNounwindRuntimeCall(llvm::FunctionCallee callee,\n                                          const Twine &name = \"\");\n  llvm::CallInst *EmitNounwindRuntimeCall(llvm::FunctionCallee callee,\n                                          ArrayRef<llvm::Value *> args,\n                                          const Twine &name = \"\");\n\n  SmallVector<llvm::OperandBundleDef, 1>\n  getBundlesForFunclet(llvm::Value *Callee);\n\n  llvm::CallBase *EmitCallOrInvoke(llvm::FunctionCallee Callee,\n                                   ArrayRef<llvm::Value *> Args,\n                                   const Twine &Name = \"\");\n  llvm::CallBase *EmitRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                          ArrayRef<llvm::Value *> args,\n                                          const Twine &name = \"\");\n  llvm::CallBase *EmitRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                          const Twine &name = \"\");\n  void EmitNoreturnRuntimeCallOrInvoke(llvm::FunctionCallee callee,\n                                       ArrayRef<llvm::Value *> args);\n\n  CGCallee BuildAppleKextVirtualCall(const CXXMethodDecl *MD,\n                                     NestedNameSpecifier *Qual,\n                                     llvm::Type *Ty);\n\n  CGCallee BuildAppleKextVirtualDestructorCall(const CXXDestructorDecl *DD,\n                                               CXXDtorType Type,\n                                               const CXXRecordDecl *RD);\n\n  // Return the copy constructor name with the prefix \"__copy_constructor_\"\n  // removed.\n  static std::string getNonTrivialCopyConstructorStr(QualType QT,\n                                                     CharUnits Alignment,\n                                                     bool IsVolatile,\n                                                     ASTContext &Ctx);\n\n  // Return the destructor name with the prefix \"__destructor_\" removed.\n  static std::string getNonTrivialDestructorStr(QualType QT,\n                                                CharUnits Alignment,\n                                                bool IsVolatile,\n                                                ASTContext &Ctx);\n\n  // These functions emit calls to the special functions of non-trivial C\n  // structs.\n  void defaultInitNonTrivialCStructVar(LValue Dst);\n  void callCStructDefaultConstructor(LValue Dst);\n  void callCStructDestructor(LValue Dst);\n  void callCStructCopyConstructor(LValue Dst, LValue Src);\n  void callCStructMoveConstructor(LValue Dst, LValue Src);\n  void callCStructCopyAssignmentOperator(LValue Dst, LValue Src);\n  void callCStructMoveAssignmentOperator(LValue Dst, LValue Src);\n\n  RValue\n  EmitCXXMemberOrOperatorCall(const CXXMethodDecl *Method,\n                              const CGCallee &Callee,\n                              ReturnValueSlot ReturnValue, llvm::Value *This,\n                              llvm::Value *ImplicitParam,\n                              QualType ImplicitParamTy, const CallExpr *E,\n                              CallArgList *RtlArgs);\n  RValue EmitCXXDestructorCall(GlobalDecl Dtor, const CGCallee &Callee,\n                               llvm::Value *This, QualType ThisTy,\n                               llvm::Value *ImplicitParam,\n                               QualType ImplicitParamTy, const CallExpr *E);\n  RValue EmitCXXMemberCallExpr(const CXXMemberCallExpr *E,\n                               ReturnValueSlot ReturnValue);\n  RValue EmitCXXMemberOrOperatorMemberCallExpr(const CallExpr *CE,\n                                               const CXXMethodDecl *MD,\n                                               ReturnValueSlot ReturnValue,\n                                               bool HasQualifier,\n                                               NestedNameSpecifier *Qualifier,\n                                               bool IsArrow, const Expr *Base);\n  // Compute the object pointer.\n  Address EmitCXXMemberDataPointerAddress(const Expr *E, Address base,\n                                          llvm::Value *memberPtr,\n                                          const MemberPointerType *memberPtrType,\n                                          LValueBaseInfo *BaseInfo = nullptr,\n                                          TBAAAccessInfo *TBAAInfo = nullptr);\n  RValue EmitCXXMemberPointerCallExpr(const CXXMemberCallExpr *E,\n                                      ReturnValueSlot ReturnValue);\n\n  RValue EmitCXXOperatorMemberCallExpr(const CXXOperatorCallExpr *E,\n                                       const CXXMethodDecl *MD,\n                                       ReturnValueSlot ReturnValue);\n  RValue EmitCXXPseudoDestructorExpr(const CXXPseudoDestructorExpr *E);\n\n  RValue EmitCUDAKernelCallExpr(const CUDAKernelCallExpr *E,\n                                ReturnValueSlot ReturnValue);\n\n  RValue EmitNVPTXDevicePrintfCallExpr(const CallExpr *E,\n                                       ReturnValueSlot ReturnValue);\n  RValue EmitAMDGPUDevicePrintfCallExpr(const CallExpr *E,\n                                        ReturnValueSlot ReturnValue);\n\n  RValue EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,\n                         const CallExpr *E, ReturnValueSlot ReturnValue);\n\n  RValue emitRotate(const CallExpr *E, bool IsRotateRight);\n\n  /// Emit IR for __builtin_os_log_format.\n  RValue emitBuiltinOSLogFormat(const CallExpr &E);\n\n  /// Emit IR for __builtin_is_aligned.\n  RValue EmitBuiltinIsAligned(const CallExpr *E);\n  /// Emit IR for __builtin_align_up/__builtin_align_down.\n  RValue EmitBuiltinAlignTo(const CallExpr *E, bool AlignUp);\n\n  llvm::Function *generateBuiltinOSLogHelperFunction(\n      const analyze_os_log::OSLogBufferLayout &Layout,\n      CharUnits BufferAlignment);\n\n  RValue EmitBlockCallExpr(const CallExpr *E, ReturnValueSlot ReturnValue);\n\n  /// EmitTargetBuiltinExpr - Emit the given builtin call. Returns 0 if the call\n  /// is unhandled by the current target.\n  llvm::Value *EmitTargetBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue);\n\n  llvm::Value *EmitAArch64CompareBuiltinExpr(llvm::Value *Op, llvm::Type *Ty,\n                                             const llvm::CmpInst::Predicate Fp,\n                                             const llvm::CmpInst::Predicate Ip,\n                                             const llvm::Twine &Name = \"\");\n  llvm::Value *EmitARMBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                  ReturnValueSlot ReturnValue,\n                                  llvm::Triple::ArchType Arch);\n  llvm::Value *EmitARMMVEBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue,\n                                     llvm::Triple::ArchType Arch);\n  llvm::Value *EmitARMCDEBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                     ReturnValueSlot ReturnValue,\n                                     llvm::Triple::ArchType Arch);\n  llvm::Value *EmitCMSEClearRecord(llvm::Value *V, llvm::IntegerType *ITy,\n                                   QualType RTy);\n  llvm::Value *EmitCMSEClearRecord(llvm::Value *V, llvm::ArrayType *ATy,\n                                   QualType RTy);\n\n  llvm::Value *EmitCommonNeonBuiltinExpr(unsigned BuiltinID,\n                                         unsigned LLVMIntrinsic,\n                                         unsigned AltLLVMIntrinsic,\n                                         const char *NameHint,\n                                         unsigned Modifier,\n                                         const CallExpr *E,\n                                         SmallVectorImpl<llvm::Value *> &Ops,\n                                         Address PtrOp0, Address PtrOp1,\n                                         llvm::Triple::ArchType Arch);\n\n  llvm::Function *LookupNeonLLVMIntrinsic(unsigned IntrinsicID,\n                                          unsigned Modifier, llvm::Type *ArgTy,\n                                          const CallExpr *E);\n  llvm::Value *EmitNeonCall(llvm::Function *F,\n                            SmallVectorImpl<llvm::Value*> &O,\n                            const char *name,\n                            unsigned shift = 0, bool rightshift = false);\n  llvm::Value *EmitNeonSplat(llvm::Value *V, llvm::Constant *Idx,\n                             const llvm::ElementCount &Count);\n  llvm::Value *EmitNeonSplat(llvm::Value *V, llvm::Constant *Idx);\n  llvm::Value *EmitNeonShiftVector(llvm::Value *V, llvm::Type *Ty,\n                                   bool negateForRightShift);\n  llvm::Value *EmitNeonRShiftImm(llvm::Value *Vec, llvm::Value *Amt,\n                                 llvm::Type *Ty, bool usgn, const char *name);\n  llvm::Value *vectorWrapScalar16(llvm::Value *Op);\n  /// SVEBuiltinMemEltTy - Returns the memory element type for this memory\n  /// access builtin.  Only required if it can't be inferred from the base\n  /// pointer operand.\n  llvm::Type *SVEBuiltinMemEltTy(SVETypeFlags TypeFlags);\n\n  SmallVector<llvm::Type *, 2> getSVEOverloadTypes(SVETypeFlags TypeFlags,\n                                                   llvm::Type *ReturnType,\n                                                   ArrayRef<llvm::Value *> Ops);\n  llvm::Type *getEltType(SVETypeFlags TypeFlags);\n  llvm::ScalableVectorType *getSVEType(const SVETypeFlags &TypeFlags);\n  llvm::ScalableVectorType *getSVEPredType(SVETypeFlags TypeFlags);\n  llvm::Value *EmitSVEAllTruePred(SVETypeFlags TypeFlags);\n  llvm::Value *EmitSVEDupX(llvm::Value *Scalar);\n  llvm::Value *EmitSVEDupX(llvm::Value *Scalar, llvm::Type *Ty);\n  llvm::Value *EmitSVEReinterpret(llvm::Value *Val, llvm::Type *Ty);\n  llvm::Value *EmitSVEPMull(SVETypeFlags TypeFlags,\n                            llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                            unsigned BuiltinID);\n  llvm::Value *EmitSVEMovl(SVETypeFlags TypeFlags,\n                           llvm::ArrayRef<llvm::Value *> Ops,\n                           unsigned BuiltinID);\n  llvm::Value *EmitSVEPredicateCast(llvm::Value *Pred,\n                                    llvm::ScalableVectorType *VTy);\n  llvm::Value *EmitSVEGatherLoad(SVETypeFlags TypeFlags,\n                                 llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                                 unsigned IntID);\n  llvm::Value *EmitSVEScatterStore(SVETypeFlags TypeFlags,\n                                   llvm::SmallVectorImpl<llvm::Value *> &Ops,\n                                   unsigned IntID);\n  llvm::Value *EmitSVEMaskedLoad(const CallExpr *, llvm::Type *ReturnTy,\n                                 SmallVectorImpl<llvm::Value *> &Ops,\n                                 unsigned BuiltinID, bool IsZExtReturn);\n  llvm::Value *EmitSVEMaskedStore(const CallExpr *,\n                                  SmallVectorImpl<llvm::Value *> &Ops,\n                                  unsigned BuiltinID);\n  llvm::Value *EmitSVEPrefetchLoad(SVETypeFlags TypeFlags,\n                                   SmallVectorImpl<llvm::Value *> &Ops,\n                                   unsigned BuiltinID);\n  llvm::Value *EmitSVEGatherPrefetch(SVETypeFlags TypeFlags,\n                                     SmallVectorImpl<llvm::Value *> &Ops,\n                                     unsigned IntID);\n  llvm::Value *EmitSVEStructLoad(SVETypeFlags TypeFlags,\n                                 SmallVectorImpl<llvm::Value *> &Ops, unsigned IntID);\n  llvm::Value *EmitSVEStructStore(SVETypeFlags TypeFlags,\n                                  SmallVectorImpl<llvm::Value *> &Ops,\n                                  unsigned IntID);\n  llvm::Value *EmitAArch64SVEBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n\n  llvm::Value *EmitAArch64BuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                      llvm::Triple::ArchType Arch);\n  llvm::Value *EmitBPFBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n\n  llvm::Value *BuildVector(ArrayRef<llvm::Value*> Ops);\n  llvm::Value *EmitX86BuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitPPCBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitAMDGPUBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitSystemZBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitNVPTXBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitWebAssemblyBuiltinExpr(unsigned BuiltinID,\n                                          const CallExpr *E);\n  llvm::Value *EmitHexagonBuiltinExpr(unsigned BuiltinID, const CallExpr *E);\n  llvm::Value *EmitRISCVBuiltinExpr(unsigned BuiltinID, const CallExpr *E,\n                                    ReturnValueSlot ReturnValue);\n  bool ProcessOrderScopeAMDGCN(llvm::Value *Order, llvm::Value *Scope,\n                               llvm::AtomicOrdering &AO,\n                               llvm::SyncScope::ID &SSID);\n\n  enum class MSVCIntrin;\n  llvm::Value *EmitMSVCBuiltinExpr(MSVCIntrin BuiltinID, const CallExpr *E);\n\n  llvm::Value *EmitBuiltinAvailable(const VersionTuple &Version);\n\n  llvm::Value *EmitObjCProtocolExpr(const ObjCProtocolExpr *E);\n  llvm::Value *EmitObjCStringLiteral(const ObjCStringLiteral *E);\n  llvm::Value *EmitObjCBoxedExpr(const ObjCBoxedExpr *E);\n  llvm::Value *EmitObjCArrayLiteral(const ObjCArrayLiteral *E);\n  llvm::Value *EmitObjCDictionaryLiteral(const ObjCDictionaryLiteral *E);\n  llvm::Value *EmitObjCCollectionLiteral(const Expr *E,\n                                const ObjCMethodDecl *MethodWithObjects);\n  llvm::Value *EmitObjCSelectorExpr(const ObjCSelectorExpr *E);\n  RValue EmitObjCMessageExpr(const ObjCMessageExpr *E,\n                             ReturnValueSlot Return = ReturnValueSlot());\n\n  /// Retrieves the default cleanup kind for an ARC cleanup.\n  /// Except under -fobjc-arc-eh, ARC cleanups are normal-only.\n  CleanupKind getARCCleanupKind() {\n    return CGM.getCodeGenOpts().ObjCAutoRefCountExceptions\n             ? NormalAndEHCleanup : NormalCleanup;\n  }\n\n  // ARC primitives.\n  void EmitARCInitWeak(Address addr, llvm::Value *value);\n  void EmitARCDestroyWeak(Address addr);\n  llvm::Value *EmitARCLoadWeak(Address addr);\n  llvm::Value *EmitARCLoadWeakRetained(Address addr);\n  llvm::Value *EmitARCStoreWeak(Address addr, llvm::Value *value, bool ignored);\n  void emitARCCopyAssignWeak(QualType Ty, Address DstAddr, Address SrcAddr);\n  void emitARCMoveAssignWeak(QualType Ty, Address DstAddr, Address SrcAddr);\n  void EmitARCCopyWeak(Address dst, Address src);\n  void EmitARCMoveWeak(Address dst, Address src);\n  llvm::Value *EmitARCRetainAutorelease(QualType type, llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleaseNonBlock(llvm::Value *value);\n  llvm::Value *EmitARCStoreStrong(LValue lvalue, llvm::Value *value,\n                                  bool resultIgnored);\n  llvm::Value *EmitARCStoreStrongCall(Address addr, llvm::Value *value,\n                                      bool resultIgnored);\n  llvm::Value *EmitARCRetain(QualType type, llvm::Value *value);\n  llvm::Value *EmitARCRetainNonBlock(llvm::Value *value);\n  llvm::Value *EmitARCRetainBlock(llvm::Value *value, bool mandatory);\n  void EmitARCDestroyStrong(Address addr, ARCPreciseLifetime_t precise);\n  void EmitARCRelease(llvm::Value *value, ARCPreciseLifetime_t precise);\n  llvm::Value *EmitARCAutorelease(llvm::Value *value);\n  llvm::Value *EmitARCAutoreleaseReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleaseReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCRetainAutoreleasedReturnValue(llvm::Value *value);\n  llvm::Value *EmitARCUnsafeClaimAutoreleasedReturnValue(llvm::Value *value);\n\n  llvm::Value *EmitObjCAutorelease(llvm::Value *value, llvm::Type *returnType);\n  llvm::Value *EmitObjCRetainNonBlock(llvm::Value *value,\n                                      llvm::Type *returnType);\n  void EmitObjCRelease(llvm::Value *value, ARCPreciseLifetime_t precise);\n\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreAutoreleasing(const BinaryOperator *e);\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreStrong(const BinaryOperator *e, bool ignored);\n  std::pair<LValue,llvm::Value*>\n  EmitARCStoreUnsafeUnretained(const BinaryOperator *e, bool ignored);\n\n  llvm::Value *EmitObjCAlloc(llvm::Value *value,\n                             llvm::Type *returnType);\n  llvm::Value *EmitObjCAllocWithZone(llvm::Value *value,\n                                     llvm::Type *returnType);\n  llvm::Value *EmitObjCAllocInit(llvm::Value *value, llvm::Type *resultType);\n\n  llvm::Value *EmitObjCThrowOperand(const Expr *expr);\n  llvm::Value *EmitObjCConsumeObject(QualType T, llvm::Value *Ptr);\n  llvm::Value *EmitObjCExtendObjectLifetime(QualType T, llvm::Value *Ptr);\n\n  llvm::Value *EmitARCExtendBlockObject(const Expr *expr);\n  llvm::Value *EmitARCReclaimReturnedObject(const Expr *e,\n                                            bool allowUnsafeClaim);\n  llvm::Value *EmitARCRetainScalarExpr(const Expr *expr);\n  llvm::Value *EmitARCRetainAutoreleaseScalarExpr(const Expr *expr);\n  llvm::Value *EmitARCUnsafeUnretainedScalarExpr(const Expr *expr);\n\n  void EmitARCIntrinsicUse(ArrayRef<llvm::Value*> values);\n\n  void EmitARCNoopIntrinsicUse(ArrayRef<llvm::Value *> values);\n\n  static Destroyer destroyARCStrongImprecise;\n  static Destroyer destroyARCStrongPrecise;\n  static Destroyer destroyARCWeak;\n  static Destroyer emitARCIntrinsicUse;\n  static Destroyer destroyNonTrivialCStruct;\n\n  void EmitObjCAutoreleasePoolPop(llvm::Value *Ptr);\n  llvm::Value *EmitObjCAutoreleasePoolPush();\n  llvm::Value *EmitObjCMRRAutoreleasePoolPush();\n  void EmitObjCAutoreleasePoolCleanup(llvm::Value *Ptr);\n  void EmitObjCMRRAutoreleasePoolPop(llvm::Value *Ptr);\n\n  /// Emits a reference binding to the passed in expression.\n  RValue EmitReferenceBindingToExpr(const Expr *E);\n\n  //===--------------------------------------------------------------------===//\n  //                           Expression Emission\n  //===--------------------------------------------------------------------===//\n\n  // Expressions are broken into three classes: scalar, complex, aggregate.\n\n  /// EmitScalarExpr - Emit the computation of the specified expression of LLVM\n  /// scalar type, returning the result.\n  llvm::Value *EmitScalarExpr(const Expr *E , bool IgnoreResultAssign = false);\n\n  /// Emit a conversion from the specified type to the specified destination\n  /// type, both of which are LLVM scalar types.\n  llvm::Value *EmitScalarConversion(llvm::Value *Src, QualType SrcTy,\n                                    QualType DstTy, SourceLocation Loc);\n\n  /// Emit a conversion from the specified complex type to the specified\n  /// destination type, where the destination type is an LLVM scalar type.\n  llvm::Value *EmitComplexToScalarConversion(ComplexPairTy Src, QualType SrcTy,\n                                             QualType DstTy,\n                                             SourceLocation Loc);\n\n  /// EmitAggExpr - Emit the computation of the specified expression\n  /// of aggregate type.  The result is computed into the given slot,\n  /// which may be null to indicate that the value is not needed.\n  void EmitAggExpr(const Expr *E, AggValueSlot AS);\n\n  /// EmitAggExprToLValue - Emit the computation of the specified expression of\n  /// aggregate type into a temporary LValue.\n  LValue EmitAggExprToLValue(const Expr *E);\n\n  /// Build all the stores needed to initialize an aggregate at Dest with the\n  /// value Val.\n  void EmitAggregateStore(llvm::Value *Val, Address Dest, bool DestIsVolatile);\n\n  /// EmitExtendGCLifetime - Given a pointer to an Objective-C object,\n  /// make sure it survives garbage collection until this point.\n  void EmitExtendGCLifetime(llvm::Value *object);\n\n  /// EmitComplexExpr - Emit the computation of the specified expression of\n  /// complex type, returning the result.\n  ComplexPairTy EmitComplexExpr(const Expr *E,\n                                bool IgnoreReal = false,\n                                bool IgnoreImag = false);\n\n  /// EmitComplexExprIntoLValue - Emit the given expression of complex\n  /// type and place its result into the specified l-value.\n  void EmitComplexExprIntoLValue(const Expr *E, LValue dest, bool isInit);\n\n  /// EmitStoreOfComplex - Store a complex number into the specified l-value.\n  void EmitStoreOfComplex(ComplexPairTy V, LValue dest, bool isInit);\n\n  /// EmitLoadOfComplex - Load a complex number from the specified l-value.\n  ComplexPairTy EmitLoadOfComplex(LValue src, SourceLocation loc);\n\n  Address emitAddrOfRealComponent(Address complex, QualType complexType);\n  Address emitAddrOfImagComponent(Address complex, QualType complexType);\n\n  /// AddInitializerToStaticVarDecl - Add the initializer for 'D' to the\n  /// global variable that has already been created for it.  If the initializer\n  /// has a different type than GV does, this may free GV and return a different\n  /// one.  Otherwise it just returns GV.\n  llvm::GlobalVariable *\n  AddInitializerToStaticVarDecl(const VarDecl &D,\n                                llvm::GlobalVariable *GV);\n\n  // Emit an @llvm.invariant.start call for the given memory region.\n  void EmitInvariantStart(llvm::Constant *Addr, CharUnits Size);\n\n  /// EmitCXXGlobalVarDeclInit - Create the initializer for a C++\n  /// variable with global storage.\n  void EmitCXXGlobalVarDeclInit(const VarDecl &D, llvm::Constant *DeclPtr,\n                                bool PerformInit);\n\n  llvm::Function *createAtExitStub(const VarDecl &VD, llvm::FunctionCallee Dtor,\n                                   llvm::Constant *Addr);\n\n  /// Call atexit() with a function that passes the given argument to\n  /// the given function.\n  void registerGlobalDtorWithAtExit(const VarDecl &D, llvm::FunctionCallee fn,\n                                    llvm::Constant *addr);\n\n  /// Call atexit() with function dtorStub.\n  void registerGlobalDtorWithAtExit(llvm::Constant *dtorStub);\n\n  /// Call unatexit() with function dtorStub.\n  llvm::Value *unregisterGlobalDtorWithUnAtExit(llvm::Constant *dtorStub);\n\n  /// Emit code in this function to perform a guarded variable\n  /// initialization.  Guarded initializations are used when it's not\n  /// possible to prove that an initialization will be done exactly\n  /// once, e.g. with a static local variable or a static data member\n  /// of a class template.\n  void EmitCXXGuardedInit(const VarDecl &D, llvm::GlobalVariable *DeclPtr,\n                          bool PerformInit);\n\n  enum class GuardKind { VariableGuard, TlsGuard };\n\n  /// Emit a branch to select whether or not to perform guarded initialization.\n  void EmitCXXGuardedInitBranch(llvm::Value *NeedsInit,\n                                llvm::BasicBlock *InitBlock,\n                                llvm::BasicBlock *NoInitBlock,\n                                GuardKind Kind, const VarDecl *D);\n\n  /// GenerateCXXGlobalInitFunc - Generates code for initializing global\n  /// variables.\n  void\n  GenerateCXXGlobalInitFunc(llvm::Function *Fn,\n                            ArrayRef<llvm::Function *> CXXThreadLocals,\n                            ConstantAddress Guard = ConstantAddress::invalid());\n\n  /// GenerateCXXGlobalCleanUpFunc - Generates code for cleaning up global\n  /// variables.\n  void GenerateCXXGlobalCleanUpFunc(\n      llvm::Function *Fn,\n      const std::vector<std::tuple<llvm::FunctionType *, llvm::WeakTrackingVH,\n                                   llvm::Constant *>> &DtorsOrStermFinalizers);\n\n  void GenerateCXXGlobalVarDeclInitFunc(llvm::Function *Fn,\n                                        const VarDecl *D,\n                                        llvm::GlobalVariable *Addr,\n                                        bool PerformInit);\n\n  void EmitCXXConstructExpr(const CXXConstructExpr *E, AggValueSlot Dest);\n\n  void EmitSynthesizedCXXCopyCtor(Address Dest, Address Src, const Expr *Exp);\n\n  void EmitCXXThrowExpr(const CXXThrowExpr *E, bool KeepInsertionPoint = true);\n\n  RValue EmitAtomicExpr(AtomicExpr *E);\n\n  //===--------------------------------------------------------------------===//\n  //                         Annotations Emission\n  //===--------------------------------------------------------------------===//\n\n  /// Emit an annotation call (intrinsic).\n  llvm::Value *EmitAnnotationCall(llvm::Function *AnnotationFn,\n                                  llvm::Value *AnnotatedVal,\n                                  StringRef AnnotationStr,\n                                  SourceLocation Location,\n                                  const AnnotateAttr *Attr);\n\n  /// Emit local annotations for the local variable V, declared by D.\n  void EmitVarAnnotations(const VarDecl *D, llvm::Value *V);\n\n  /// Emit field annotations for the given field & value. Returns the\n  /// annotation result.\n  Address EmitFieldAnnotations(const FieldDecl *D, Address V);\n\n  //===--------------------------------------------------------------------===//\n  //                             Internal Helpers\n  //===--------------------------------------------------------------------===//\n\n  /// ContainsLabel - Return true if the statement contains a label in it.  If\n  /// this statement is not executed normally, it not containing a label means\n  /// that we can just remove the code.\n  static bool ContainsLabel(const Stmt *S, bool IgnoreCaseStmts = false);\n\n  /// containsBreak - Return true if the statement contains a break out of it.\n  /// If the statement (recursively) contains a switch or loop with a break\n  /// inside of it, this is fine.\n  static bool containsBreak(const Stmt *S);\n\n  /// Determine if the given statement might introduce a declaration into the\n  /// current scope, by being a (possibly-labelled) DeclStmt.\n  static bool mightAddDeclToScope(const Stmt *S);\n\n  /// ConstantFoldsToSimpleInteger - If the specified expression does not fold\n  /// to a constant, or if it does but contains a label, return false.  If it\n  /// constant folds return true and set the boolean result in Result.\n  bool ConstantFoldsToSimpleInteger(const Expr *Cond, bool &Result,\n                                    bool AllowLabels = false);\n\n  /// ConstantFoldsToSimpleInteger - If the specified expression does not fold\n  /// to a constant, or if it does but contains a label, return false.  If it\n  /// constant folds return true and set the folded value.\n  bool ConstantFoldsToSimpleInteger(const Expr *Cond, llvm::APSInt &Result,\n                                    bool AllowLabels = false);\n\n  /// isInstrumentedCondition - Determine whether the given condition is an\n  /// instrumentable condition (i.e. no \"&&\" or \"||\").\n  static bool isInstrumentedCondition(const Expr *C);\n\n  /// EmitBranchToCounterBlock - Emit a conditional branch to a new block that\n  /// increments a profile counter based on the semantics of the given logical\n  /// operator opcode.  This is used to instrument branch condition coverage\n  /// for logical operators.\n  void EmitBranchToCounterBlock(const Expr *Cond, BinaryOperator::Opcode LOp,\n                                llvm::BasicBlock *TrueBlock,\n                                llvm::BasicBlock *FalseBlock,\n                                uint64_t TrueCount = 0,\n                                Stmt::Likelihood LH = Stmt::LH_None,\n                                const Expr *CntrIdx = nullptr);\n\n  /// EmitBranchOnBoolExpr - Emit a branch on a boolean condition (e.g. for an\n  /// if statement) to the specified blocks.  Based on the condition, this might\n  /// try to simplify the codegen of the conditional based on the branch.\n  /// TrueCount should be the number of times we expect the condition to\n  /// evaluate to true based on PGO data.\n  void EmitBranchOnBoolExpr(const Expr *Cond, llvm::BasicBlock *TrueBlock,\n                            llvm::BasicBlock *FalseBlock, uint64_t TrueCount,\n                            Stmt::Likelihood LH = Stmt::LH_None);\n\n  /// Given an assignment `*LHS = RHS`, emit a test that checks if \\p RHS is\n  /// nonnull, if \\p LHS is marked _Nonnull.\n  void EmitNullabilityCheck(LValue LHS, llvm::Value *RHS, SourceLocation Loc);\n\n  /// An enumeration which makes it easier to specify whether or not an\n  /// operation is a subtraction.\n  enum { NotSubtraction = false, IsSubtraction = true };\n\n  /// Same as IRBuilder::CreateInBoundsGEP, but additionally emits a check to\n  /// detect undefined behavior when the pointer overflow sanitizer is enabled.\n  /// \\p SignedIndices indicates whether any of the GEP indices are signed.\n  /// \\p IsSubtraction indicates whether the expression used to form the GEP\n  /// is a subtraction.\n  llvm::Value *EmitCheckedInBoundsGEP(llvm::Value *Ptr,\n                                      ArrayRef<llvm::Value *> IdxList,\n                                      bool SignedIndices,\n                                      bool IsSubtraction,\n                                      SourceLocation Loc,\n                                      const Twine &Name = \"\");\n\n  /// Specifies which type of sanitizer check to apply when handling a\n  /// particular builtin.\n  enum BuiltinCheckKind {\n    BCK_CTZPassedZero,\n    BCK_CLZPassedZero,\n  };\n\n  /// Emits an argument for a call to a builtin. If the builtin sanitizer is\n  /// enabled, a runtime check specified by \\p Kind is also emitted.\n  llvm::Value *EmitCheckedArgForBuiltin(const Expr *E, BuiltinCheckKind Kind);\n\n  /// Emit a description of a type in a format suitable for passing to\n  /// a runtime sanitizer handler.\n  llvm::Constant *EmitCheckTypeDescriptor(QualType T);\n\n  /// Convert a value into a format suitable for passing to a runtime\n  /// sanitizer handler.\n  llvm::Value *EmitCheckValue(llvm::Value *V);\n\n  /// Emit a description of a source location in a format suitable for\n  /// passing to a runtime sanitizer handler.\n  llvm::Constant *EmitCheckSourceLocation(SourceLocation Loc);\n\n  /// Create a basic block that will either trap or call a handler function in\n  /// the UBSan runtime with the provided arguments, and create a conditional\n  /// branch to it.\n  void EmitCheck(ArrayRef<std::pair<llvm::Value *, SanitizerMask>> Checked,\n                 SanitizerHandler Check, ArrayRef<llvm::Constant *> StaticArgs,\n                 ArrayRef<llvm::Value *> DynamicArgs);\n\n  /// Emit a slow path cross-DSO CFI check which calls __cfi_slowpath\n  /// if Cond if false.\n  void EmitCfiSlowPathCheck(SanitizerMask Kind, llvm::Value *Cond,\n                            llvm::ConstantInt *TypeId, llvm::Value *Ptr,\n                            ArrayRef<llvm::Constant *> StaticArgs);\n\n  /// Emit a reached-unreachable diagnostic if \\p Loc is valid and runtime\n  /// checking is enabled. Otherwise, just emit an unreachable instruction.\n  void EmitUnreachable(SourceLocation Loc);\n\n  /// Create a basic block that will call the trap intrinsic, and emit a\n  /// conditional branch to it, for the -ftrapv checks.\n  void EmitTrapCheck(llvm::Value *Checked, SanitizerHandler CheckHandlerID);\n\n  /// Emit a call to trap or debugtrap and attach function attribute\n  /// \"trap-func-name\" if specified.\n  llvm::CallInst *EmitTrapCall(llvm::Intrinsic::ID IntrID);\n\n  /// Emit a stub for the cross-DSO CFI check function.\n  void EmitCfiCheckStub();\n\n  /// Emit a cross-DSO CFI failure handling function.\n  void EmitCfiCheckFail();\n\n  /// Create a check for a function parameter that may potentially be\n  /// declared as non-null.\n  void EmitNonNullArgCheck(RValue RV, QualType ArgType, SourceLocation ArgLoc,\n                           AbstractCallee AC, unsigned ParmNum);\n\n  /// EmitCallArg - Emit a single call argument.\n  void EmitCallArg(CallArgList &args, const Expr *E, QualType ArgType);\n\n  /// EmitDelegateCallArg - We are performing a delegate call; that\n  /// is, the current function is delegating to another one.  Produce\n  /// a r-value suitable for passing the given parameter.\n  void EmitDelegateCallArg(CallArgList &args, const VarDecl *param,\n                           SourceLocation loc);\n\n  /// SetFPAccuracy - Set the minimum required accuracy of the given floating\n  /// point operation, expressed as the maximum relative error in ulp.\n  void SetFPAccuracy(llvm::Value *Val, float Accuracy);\n\n  /// SetFPModel - Control floating point behavior via fp-model settings.\n  void SetFPModel();\n\n  /// Set the codegen fast-math flags.\n  void SetFastMathFlags(FPOptions FPFeatures);\n\nprivate:\n  llvm::MDNode *getRangeForLoadFromType(QualType Ty);\n  void EmitReturnOfRValue(RValue RV, QualType Ty);\n\n  void deferPlaceholderReplacement(llvm::Instruction *Old, llvm::Value *New);\n\n  llvm::SmallVector<std::pair<llvm::WeakTrackingVH, llvm::Value *>, 4>\n      DeferredReplacements;\n\n  /// Set the address of a local variable.\n  void setAddrOfLocalVar(const VarDecl *VD, Address Addr) {\n    assert(!LocalDeclMap.count(VD) && \"Decl already exists in LocalDeclMap!\");\n    LocalDeclMap.insert({VD, Addr});\n  }\n\n  /// ExpandTypeFromArgs - Reconstruct a structure of type \\arg Ty\n  /// from function arguments into \\arg Dst. See ABIArgInfo::Expand.\n  ///\n  /// \\param AI - The first function argument of the expansion.\n  void ExpandTypeFromArgs(QualType Ty, LValue Dst,\n                          llvm::Function::arg_iterator &AI);\n\n  /// ExpandTypeToArgs - Expand an CallArg \\arg Arg, with the LLVM type for \\arg\n  /// Ty, into individual arguments on the provided vector \\arg IRCallArgs,\n  /// starting at index \\arg IRCallArgPos. See ABIArgInfo::Expand.\n  void ExpandTypeToArgs(QualType Ty, CallArg Arg, llvm::FunctionType *IRFuncTy,\n                        SmallVectorImpl<llvm::Value *> &IRCallArgs,\n                        unsigned &IRCallArgPos);\n\n  llvm::Value* EmitAsmInput(const TargetInfo::ConstraintInfo &Info,\n                            const Expr *InputExpr, std::string &ConstraintStr);\n\n  llvm::Value* EmitAsmInputLValue(const TargetInfo::ConstraintInfo &Info,\n                                  LValue InputValue, QualType InputType,\n                                  std::string &ConstraintStr,\n                                  SourceLocation Loc);\n\n  /// Attempts to statically evaluate the object size of E. If that\n  /// fails, emits code to figure the size of E out for us. This is\n  /// pass_object_size aware.\n  ///\n  /// If EmittedExpr is non-null, this will use that instead of re-emitting E.\n  llvm::Value *evaluateOrEmitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                               llvm::IntegerType *ResType,\n                                               llvm::Value *EmittedE,\n                                               bool IsDynamic);\n\n  /// Emits the size of E, as required by __builtin_object_size. This\n  /// function is aware of pass_object_size parameters, and will act accordingly\n  /// if E is a parameter with the pass_object_size attribute.\n  llvm::Value *emitBuiltinObjectSize(const Expr *E, unsigned Type,\n                                     llvm::IntegerType *ResType,\n                                     llvm::Value *EmittedE,\n                                     bool IsDynamic);\n\n  void emitZeroOrPatternForAutoVarInit(QualType type, const VarDecl &D,\n                                       Address Loc);\n\npublic:\n  enum class EvaluationOrder {\n    ///! No language constraints on evaluation order.\n    Default,\n    ///! Language semantics require left-to-right evaluation.\n    ForceLeftToRight,\n    ///! Language semantics require right-to-left evaluation.\n    ForceRightToLeft\n  };\n\n  // Wrapper for function prototype sources. Wraps either a FunctionProtoType or\n  // an ObjCMethodDecl.\n  struct PrototypeWrapper {\n    llvm::PointerUnion<const FunctionProtoType *, const ObjCMethodDecl *> P;\n\n    PrototypeWrapper(const FunctionProtoType *FT) : P(FT) {}\n    PrototypeWrapper(const ObjCMethodDecl *MD) : P(MD) {}\n  };\n\n  void EmitCallArgs(CallArgList &Args, PrototypeWrapper Prototype,\n                    llvm::iterator_range<CallExpr::const_arg_iterator> ArgRange,\n                    AbstractCallee AC = AbstractCallee(),\n                    unsigned ParamsToSkip = 0,\n                    EvaluationOrder Order = EvaluationOrder::Default);\n\n  /// EmitPointerWithAlignment - Given an expression with a pointer type,\n  /// emit the value and compute our best estimate of the alignment of the\n  /// pointee.\n  ///\n  /// \\param BaseInfo - If non-null, this will be initialized with\n  /// information about the source of the alignment and the may-alias\n  /// attribute.  Note that this function will conservatively fall back on\n  /// the type when it doesn't recognize the expression and may-alias will\n  /// be set to false.\n  ///\n  /// One reasonable way to use this information is when there's a language\n  /// guarantee that the pointer must be aligned to some stricter value, and\n  /// we're simply trying to ensure that sufficiently obvious uses of under-\n  /// aligned objects don't get miscompiled; for example, a placement new\n  /// into the address of a local variable.  In such a case, it's quite\n  /// reasonable to just ignore the returned alignment when it isn't from an\n  /// explicit source.\n  Address EmitPointerWithAlignment(const Expr *Addr,\n                                   LValueBaseInfo *BaseInfo = nullptr,\n                                   TBAAAccessInfo *TBAAInfo = nullptr);\n\n  /// If \\p E references a parameter with pass_object_size info or a constant\n  /// array size modifier, emit the object size divided by the size of \\p EltTy.\n  /// Otherwise return null.\n  llvm::Value *LoadPassedObjectSize(const Expr *E, QualType EltTy);\n\n  void EmitSanitizerStatReport(llvm::SanitizerStatKind SSK);\n\n  struct MultiVersionResolverOption {\n    llvm::Function *Function;\n    FunctionDecl *FD;\n    struct Conds {\n      StringRef Architecture;\n      llvm::SmallVector<StringRef, 8> Features;\n\n      Conds(StringRef Arch, ArrayRef<StringRef> Feats)\n          : Architecture(Arch), Features(Feats.begin(), Feats.end()) {}\n    } Conditions;\n\n    MultiVersionResolverOption(llvm::Function *F, StringRef Arch,\n                               ArrayRef<StringRef> Feats)\n        : Function(F), Conditions(Arch, Feats) {}\n  };\n\n  // Emits the body of a multiversion function's resolver. Assumes that the\n  // options are already sorted in the proper order, with the 'default' option\n  // last (if it exists).\n  void EmitMultiVersionResolver(llvm::Function *Resolver,\n                                ArrayRef<MultiVersionResolverOption> Options);\n\n  static uint64_t GetX86CpuSupportsMask(ArrayRef<StringRef> FeatureStrs);\n\nprivate:\n  QualType getVarArgType(const Expr *Arg);\n\n  void EmitDeclMetadata();\n\n  BlockByrefHelpers *buildByrefHelpers(llvm::StructType &byrefType,\n                                  const AutoVarEmission &emission);\n\n  void AddObjCARCExceptionMetadata(llvm::Instruction *Inst);\n\n  llvm::Value *GetValueForARMHint(unsigned BuiltinID);\n  llvm::Value *EmitX86CpuIs(const CallExpr *E);\n  llvm::Value *EmitX86CpuIs(StringRef CPUStr);\n  llvm::Value *EmitX86CpuSupports(const CallExpr *E);\n  llvm::Value *EmitX86CpuSupports(ArrayRef<StringRef> FeatureStrs);\n  llvm::Value *EmitX86CpuSupports(uint64_t Mask);\n  llvm::Value *EmitX86CpuInit();\n  llvm::Value *FormResolverCondition(const MultiVersionResolverOption &RO);\n};\n\n/// TargetFeatures - This class is used to check whether the builtin function\n/// has the required tagert specific features. It is able to support the\n/// combination of ','(and), '|'(or), and '()'. By default, the priority of\n/// ',' is higher than that of '|' .\n/// E.g:\n/// A,B|C means the builtin function requires both A and B, or C.\n/// If we want the builtin function requires both A and B, or both A and C,\n/// there are two ways: A,B|A,C or A,(B|C).\n/// The FeaturesList should not contain spaces, and brackets must appear in\n/// pairs.\nclass TargetFeatures {\n  struct FeatureListStatus {\n    bool HasFeatures;\n    StringRef CurFeaturesList;\n  };\n\n  const llvm::StringMap<bool> &CallerFeatureMap;\n\n  FeatureListStatus getAndFeatures(StringRef FeatureList) {\n    int InParentheses = 0;\n    bool HasFeatures = true;\n    size_t SubexpressionStart = 0;\n    for (size_t i = 0, e = FeatureList.size(); i < e; ++i) {\n      char CurrentToken = FeatureList[i];\n      switch (CurrentToken) {\n      default:\n        break;\n      case '(':\n        if (InParentheses == 0)\n          SubexpressionStart = i + 1;\n        ++InParentheses;\n        break;\n      case ')':\n        --InParentheses;\n        assert(InParentheses >= 0 && \"Parentheses are not in pair\");\n        LLVM_FALLTHROUGH;\n      case '|':\n      case ',':\n        if (InParentheses == 0) {\n          if (HasFeatures && i != SubexpressionStart) {\n            StringRef F = FeatureList.slice(SubexpressionStart, i);\n            HasFeatures = CurrentToken == ')' ? hasRequiredFeatures(F)\n                                              : CallerFeatureMap.lookup(F);\n          }\n          SubexpressionStart = i + 1;\n          if (CurrentToken == '|') {\n            return {HasFeatures, FeatureList.substr(SubexpressionStart)};\n          }\n        }\n        break;\n      }\n    }\n    assert(InParentheses == 0 && \"Parentheses are not in pair\");\n    if (HasFeatures && SubexpressionStart != FeatureList.size())\n      HasFeatures =\n          CallerFeatureMap.lookup(FeatureList.substr(SubexpressionStart));\n    return {HasFeatures, StringRef()};\n  }\n\npublic:\n  bool hasRequiredFeatures(StringRef FeatureList) {\n    FeatureListStatus FS = {false, FeatureList};\n    while (!FS.HasFeatures && !FS.CurFeaturesList.empty())\n      FS = getAndFeatures(FS.CurFeaturesList);\n    return FS.HasFeatures;\n  }\n\n  TargetFeatures(const llvm::StringMap<bool> &CallerFeatureMap)\n      : CallerFeatureMap(CallerFeatureMap) {}\n};\n\ninline DominatingLLVMValue::saved_type\nDominatingLLVMValue::save(CodeGenFunction &CGF, llvm::Value *value) {\n  if (!needsSaving(value)) return saved_type(value, false);\n\n  // Otherwise, we need an alloca.\n  auto align = CharUnits::fromQuantity(\n            CGF.CGM.getDataLayout().getPrefTypeAlignment(value->getType()));\n  Address alloca =\n    CGF.CreateTempAlloca(value->getType(), align, \"cond-cleanup.save\");\n  CGF.Builder.CreateStore(value, alloca);\n\n  return saved_type(alloca.getPointer(), true);\n}\n\ninline llvm::Value *DominatingLLVMValue::restore(CodeGenFunction &CGF,\n                                                 saved_type value) {\n  // If the value says it wasn't saved, trust that it's still dominating.\n  if (!value.getInt()) return value.getPointer();\n\n  // Otherwise, it should be an alloca instruction, as set up in save().\n  auto alloca = cast<llvm::AllocaInst>(value.getPointer());\n  return CGF.Builder.CreateAlignedLoad(alloca, alloca->getAlign());\n}\n\n}  // end namespace CodeGen\n\n// Map the LangOption for floating point exception behavior into\n// the corresponding enum in the IR.\nllvm::fp::ExceptionBehavior\nToConstrainedExceptMD(LangOptions::FPExceptionModeKind Kind);\n}  // end namespace clang\n\n#endif\n"}, "41": {"id": 41, "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenPGO.h", "content": "//===--- CodeGenPGO.h - PGO Instrumentation for LLVM CodeGen ----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// Instrumentation-based profile-guided optimization\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_CLANG_LIB_CODEGEN_CODEGENPGO_H\n#define LLVM_CLANG_LIB_CODEGEN_CODEGENPGO_H\n\n#include \"CGBuilder.h\"\n#include \"CodeGenModule.h\"\n#include \"CodeGenTypes.h\"\n#include \"llvm/ProfileData/InstrProfReader.h\"\n#include <array>\n#include <memory>\n\nnamespace clang {\nnamespace CodeGen {\n\n/// Per-function PGO state.\nclass CodeGenPGO {\nprivate:\n  CodeGenModule &CGM;\n  std::string FuncName;\n  llvm::GlobalVariable *FuncNameVar;\n\n  std::array <unsigned, llvm::IPVK_Last + 1> NumValueSites;\n  unsigned NumRegionCounters;\n  uint64_t FunctionHash;\n  std::unique_ptr<llvm::DenseMap<const Stmt *, unsigned>> RegionCounterMap;\n  std::unique_ptr<llvm::DenseMap<const Stmt *, uint64_t>> StmtCountMap;\n  std::unique_ptr<llvm::InstrProfRecord> ProfRecord;\n  std::vector<uint64_t> RegionCounts;\n  uint64_t CurrentRegionCount;\n\npublic:\n  CodeGenPGO(CodeGenModule &CGModule)\n      : CGM(CGModule), FuncNameVar(nullptr), NumValueSites({{0}}),\n        NumRegionCounters(0), FunctionHash(0), CurrentRegionCount(0) {}\n\n  /// Whether or not we have PGO region data for the current function. This is\n  /// false both when we have no data at all and when our data has been\n  /// discarded.\n  bool haveRegionCounts() const { return !RegionCounts.empty(); }\n\n  /// Return the counter value of the current region.\n  uint64_t getCurrentRegionCount() const { return CurrentRegionCount; }\n\n  /// Set the counter value for the current region. This is used to keep track\n  /// of changes to the most recent counter from control flow and non-local\n  /// exits.\n  void setCurrentRegionCount(uint64_t Count) { CurrentRegionCount = Count; }\n\n  /// Check if an execution count is known for a given statement. If so, return\n  /// true and put the value in Count; else return false.\n  Optional<uint64_t> getStmtCount(const Stmt *S) const {\n    if (!StmtCountMap)\n      return None;\n    auto I = StmtCountMap->find(S);\n    if (I == StmtCountMap->end())\n      return None;\n    return I->second;\n  }\n\n  /// If the execution count for the current statement is known, record that\n  /// as the current count.\n  void setCurrentStmt(const Stmt *S) {\n    if (auto Count = getStmtCount(S))\n      setCurrentRegionCount(*Count);\n  }\n\n  /// Assign counters to regions and configure them for PGO of a given\n  /// function. Does nothing if instrumentation is not enabled and either\n  /// generates global variables or associates PGO data with each of the\n  /// counters depending on whether we are generating or using instrumentation.\n  void assignRegionCounters(GlobalDecl GD, llvm::Function *Fn);\n  /// Emit a coverage mapping range with a counter zero\n  /// for an unused declaration.\n  void emitEmptyCounterMapping(const Decl *D, StringRef FuncName,\n                               llvm::GlobalValue::LinkageTypes Linkage);\n  // Insert instrumentation or attach profile metadata at value sites\n  void valueProfile(CGBuilderTy &Builder, uint32_t ValueKind,\n                    llvm::Instruction *ValueSite, llvm::Value *ValuePtr);\nprivate:\n  void setFuncName(llvm::Function *Fn);\n  void setFuncName(StringRef Name, llvm::GlobalValue::LinkageTypes Linkage);\n  void mapRegionCounters(const Decl *D);\n  void computeRegionCounts(const Decl *D);\n  void applyFunctionAttributes(llvm::IndexedInstrProfReader *PGOReader,\n                               llvm::Function *Fn);\n  void loadRegionCounts(llvm::IndexedInstrProfReader *PGOReader,\n                        bool IsInMainFile);\n  bool skipRegionMappingForDecl(const Decl *D);\n  void emitCounterRegionMapping(const Decl *D);\n\npublic:\n  void emitCounterIncrement(CGBuilderTy &Builder, const Stmt *S,\n                            llvm::Value *StepV);\n\n  /// Return the region count for the counter at the given index.\n  uint64_t getRegionCount(const Stmt *S) {\n    if (!RegionCounterMap)\n      return 0;\n    if (!haveRegionCounts())\n      return 0;\n    return RegionCounts[(*RegionCounterMap)[S]];\n  }\n};\n\n}  // end namespace CodeGen\n}  // end namespace clang\n\n#endif\n"}, "57": {"id": 57, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/STLExtras.h", "content": "//===- llvm/ADT/STLExtras.h - Useful STL related functions ------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains some templates that are useful if you are working with the\n// STL at all.\n//\n// No library is required when using these functions.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_STLEXTRAS_H\n#define LLVM_ADT_STLEXTRAS_H\n\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Config/abi-breaking.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <cstdlib>\n#include <functional>\n#include <initializer_list>\n#include <iterator>\n#include <limits>\n#include <memory>\n#include <tuple>\n#include <type_traits>\n#include <utility>\n\n#ifdef EXPENSIVE_CHECKS\n#include <random> // for std::mt19937\n#endif\n\nnamespace llvm {\n\n// Only used by compiler if both template types are the same.  Useful when\n// using SFINAE to test for the existence of member functions.\ntemplate <typename T, T> struct SameType;\n\nnamespace detail {\n\ntemplate <typename RangeT>\nusing IterOfRange = decltype(std::begin(std::declval<RangeT &>()));\n\ntemplate <typename RangeT>\nusing ValueOfRange = typename std::remove_reference<decltype(\n    *std::begin(std::declval<RangeT &>()))>::type;\n\n} // end namespace detail\n\n//===----------------------------------------------------------------------===//\n//     Extra additions to <type_traits>\n//===----------------------------------------------------------------------===//\n\ntemplate <typename T>\nstruct negation : std::integral_constant<bool, !bool(T::value)> {};\n\ntemplate <typename...> struct conjunction : std::true_type {};\ntemplate <typename B1> struct conjunction<B1> : B1 {};\ntemplate <typename B1, typename... Bn>\nstruct conjunction<B1, Bn...>\n    : std::conditional<bool(B1::value), conjunction<Bn...>, B1>::type {};\n\ntemplate <typename T> struct make_const_ptr {\n  using type =\n      typename std::add_pointer<typename std::add_const<T>::type>::type;\n};\n\ntemplate <typename T> struct make_const_ref {\n  using type = typename std::add_lvalue_reference<\n      typename std::add_const<T>::type>::type;\n};\n\n/// Utilities for detecting if a given trait holds for some set of arguments\n/// 'Args'. For example, the given trait could be used to detect if a given type\n/// has a copy assignment operator:\n///   template<class T>\n///   using has_copy_assign_t = decltype(std::declval<T&>()\n///                                                 = std::declval<const T&>());\n///   bool fooHasCopyAssign = is_detected<has_copy_assign_t, FooClass>::value;\nnamespace detail {\ntemplate <typename...> using void_t = void;\ntemplate <class, template <class...> class Op, class... Args> struct detector {\n  using value_t = std::false_type;\n};\ntemplate <template <class...> class Op, class... Args>\nstruct detector<void_t<Op<Args...>>, Op, Args...> {\n  using value_t = std::true_type;\n};\n} // end namespace detail\n\ntemplate <template <class...> class Op, class... Args>\nusing is_detected = typename detail::detector<void, Op, Args...>::value_t;\n\n/// Check if a Callable type can be invoked with the given set of arg types.\nnamespace detail {\ntemplate <typename Callable, typename... Args>\nusing is_invocable =\n    decltype(std::declval<Callable &>()(std::declval<Args>()...));\n} // namespace detail\n\ntemplate <typename Callable, typename... Args>\nusing is_invocable = is_detected<detail::is_invocable, Callable, Args...>;\n\n/// This class provides various trait information about a callable object.\n///   * To access the number of arguments: Traits::num_args\n///   * To access the type of an argument: Traits::arg_t<Index>\n///   * To access the type of the result:  Traits::result_t\ntemplate <typename T, bool isClass = std::is_class<T>::value>\nstruct function_traits : public function_traits<decltype(&T::operator())> {};\n\n/// Overload for class function types.\ntemplate <typename ClassType, typename ReturnType, typename... Args>\nstruct function_traits<ReturnType (ClassType::*)(Args...) const, false> {\n  /// The number of arguments to this function.\n  enum { num_args = sizeof...(Args) };\n\n  /// The result type of this function.\n  using result_t = ReturnType;\n\n  /// The type of an argument to this function.\n  template <size_t Index>\n  using arg_t = typename std::tuple_element<Index, std::tuple<Args...>>::type;\n};\n/// Overload for class function types.\ntemplate <typename ClassType, typename ReturnType, typename... Args>\nstruct function_traits<ReturnType (ClassType::*)(Args...), false>\n    : function_traits<ReturnType (ClassType::*)(Args...) const> {};\n/// Overload for non-class function types.\ntemplate <typename ReturnType, typename... Args>\nstruct function_traits<ReturnType (*)(Args...), false> {\n  /// The number of arguments to this function.\n  enum { num_args = sizeof...(Args) };\n\n  /// The result type of this function.\n  using result_t = ReturnType;\n\n  /// The type of an argument to this function.\n  template <size_t i>\n  using arg_t = typename std::tuple_element<i, std::tuple<Args...>>::type;\n};\n/// Overload for non-class function type references.\ntemplate <typename ReturnType, typename... Args>\nstruct function_traits<ReturnType (&)(Args...), false>\n    : public function_traits<ReturnType (*)(Args...)> {};\n\n//===----------------------------------------------------------------------===//\n//     Extra additions to <functional>\n//===----------------------------------------------------------------------===//\n\ntemplate <class Ty> struct identity {\n  using argument_type = Ty;\n\n  Ty &operator()(Ty &self) const {\n    return self;\n  }\n  const Ty &operator()(const Ty &self) const {\n    return self;\n  }\n};\n\n/// An efficient, type-erasing, non-owning reference to a callable. This is\n/// intended for use as the type of a function parameter that is not used\n/// after the function in question returns.\n///\n/// This class does not own the callable, so it is not in general safe to store\n/// a function_ref.\ntemplate<typename Fn> class function_ref;\n\ntemplate<typename Ret, typename ...Params>\nclass function_ref<Ret(Params...)> {\n  Ret (*callback)(intptr_t callable, Params ...params) = nullptr;\n  intptr_t callable;\n\n  template<typename Callable>\n  static Ret callback_fn(intptr_t callable, Params ...params) {\n    return (*reinterpret_cast<Callable*>(callable))(\n        std::forward<Params>(params)...);\n  }\n\npublic:\n  function_ref() = default;\n  function_ref(std::nullptr_t) {}\n\n  template <typename Callable>\n  function_ref(\n      Callable &&callable,\n      // This is not the copy-constructor.\n      std::enable_if_t<\n          !std::is_same<std::remove_cv_t<std::remove_reference_t<Callable>>,\n                        function_ref>::value> * = nullptr,\n      // Functor must be callable and return a suitable type.\n      std::enable_if_t<std::is_void<Ret>::value ||\n                       std::is_convertible<decltype(std::declval<Callable>()(\n                                               std::declval<Params>()...)),\n                                           Ret>::value> * = nullptr)\n      : callback(callback_fn<typename std::remove_reference<Callable>::type>),\n        callable(reinterpret_cast<intptr_t>(&callable)) {}\n\n  Ret operator()(Params ...params) const {\n    return callback(callable, std::forward<Params>(params)...);\n  }\n\n  explicit operator bool() const { return callback; }\n};\n\n//===----------------------------------------------------------------------===//\n//     Extra additions to <iterator>\n//===----------------------------------------------------------------------===//\n\nnamespace adl_detail {\n\nusing std::begin;\n\ntemplate <typename ContainerTy>\ndecltype(auto) adl_begin(ContainerTy &&container) {\n  return begin(std::forward<ContainerTy>(container));\n}\n\nusing std::end;\n\ntemplate <typename ContainerTy>\ndecltype(auto) adl_end(ContainerTy &&container) {\n  return end(std::forward<ContainerTy>(container));\n}\n\nusing std::swap;\n\ntemplate <typename T>\nvoid adl_swap(T &&lhs, T &&rhs) noexcept(noexcept(swap(std::declval<T>(),\n                                                       std::declval<T>()))) {\n  swap(std::forward<T>(lhs), std::forward<T>(rhs));\n}\n\n} // end namespace adl_detail\n\ntemplate <typename ContainerTy>\ndecltype(auto) adl_begin(ContainerTy &&container) {\n  return adl_detail::adl_begin(std::forward<ContainerTy>(container));\n}\n\ntemplate <typename ContainerTy>\ndecltype(auto) adl_end(ContainerTy &&container) {\n  return adl_detail::adl_end(std::forward<ContainerTy>(container));\n}\n\ntemplate <typename T>\nvoid adl_swap(T &&lhs, T &&rhs) noexcept(\n    noexcept(adl_detail::adl_swap(std::declval<T>(), std::declval<T>()))) {\n  adl_detail::adl_swap(std::forward<T>(lhs), std::forward<T>(rhs));\n}\n\n/// Test whether \\p RangeOrContainer is empty. Similar to C++17 std::empty.\ntemplate <typename T>\nconstexpr bool empty(const T &RangeOrContainer) {\n  return adl_begin(RangeOrContainer) == adl_end(RangeOrContainer);\n}\n\n/// Returns true if the given container only contains a single element.\ntemplate <typename ContainerTy> bool hasSingleElement(ContainerTy &&C) {\n  auto B = std::begin(C), E = std::end(C);\n  return B != E && std::next(B) == E;\n}\n\n/// Return a range covering \\p RangeOrContainer with the first N elements\n/// excluded.\ntemplate <typename T> auto drop_begin(T &&RangeOrContainer, size_t N = 1) {\n  return make_range(std::next(adl_begin(RangeOrContainer), N),\n                    adl_end(RangeOrContainer));\n}\n\n// mapped_iterator - This is a simple iterator adapter that causes a function to\n// be applied whenever operator* is invoked on the iterator.\n\ntemplate <typename ItTy, typename FuncTy,\n          typename FuncReturnTy =\n            decltype(std::declval<FuncTy>()(*std::declval<ItTy>()))>\nclass mapped_iterator\n    : public iterator_adaptor_base<\n             mapped_iterator<ItTy, FuncTy>, ItTy,\n             typename std::iterator_traits<ItTy>::iterator_category,\n             typename std::remove_reference<FuncReturnTy>::type> {\npublic:\n  mapped_iterator(ItTy U, FuncTy F)\n    : mapped_iterator::iterator_adaptor_base(std::move(U)), F(std::move(F)) {}\n\n  ItTy getCurrent() { return this->I; }\n\n  FuncReturnTy operator*() const { return F(*this->I); }\n\nprivate:\n  FuncTy F;\n};\n\n// map_iterator - Provide a convenient way to create mapped_iterators, just like\n// make_pair is useful for creating pairs...\ntemplate <class ItTy, class FuncTy>\ninline mapped_iterator<ItTy, FuncTy> map_iterator(ItTy I, FuncTy F) {\n  return mapped_iterator<ItTy, FuncTy>(std::move(I), std::move(F));\n}\n\ntemplate <class ContainerTy, class FuncTy>\nauto map_range(ContainerTy &&C, FuncTy F) {\n  return make_range(map_iterator(C.begin(), F), map_iterator(C.end(), F));\n}\n\n/// Helper to determine if type T has a member called rbegin().\ntemplate <typename Ty> class has_rbegin_impl {\n  using yes = char[1];\n  using no = char[2];\n\n  template <typename Inner>\n  static yes& test(Inner *I, decltype(I->rbegin()) * = nullptr);\n\n  template <typename>\n  static no& test(...);\n\npublic:\n  static const bool value = sizeof(test<Ty>(nullptr)) == sizeof(yes);\n};\n\n/// Metafunction to determine if T& or T has a member called rbegin().\ntemplate <typename Ty>\nstruct has_rbegin : has_rbegin_impl<typename std::remove_reference<Ty>::type> {\n};\n\n// Returns an iterator_range over the given container which iterates in reverse.\n// Note that the container must have rbegin()/rend() methods for this to work.\ntemplate <typename ContainerTy>\nauto reverse(ContainerTy &&C,\n             std::enable_if_t<has_rbegin<ContainerTy>::value> * = nullptr) {\n  return make_range(C.rbegin(), C.rend());\n}\n\n// Returns a std::reverse_iterator wrapped around the given iterator.\ntemplate <typename IteratorTy>\nstd::reverse_iterator<IteratorTy> make_reverse_iterator(IteratorTy It) {\n  return std::reverse_iterator<IteratorTy>(It);\n}\n\n// Returns an iterator_range over the given container which iterates in reverse.\n// Note that the container must have begin()/end() methods which return\n// bidirectional iterators for this to work.\ntemplate <typename ContainerTy>\nauto reverse(ContainerTy &&C,\n             std::enable_if_t<!has_rbegin<ContainerTy>::value> * = nullptr) {\n  return make_range(llvm::make_reverse_iterator(std::end(C)),\n                    llvm::make_reverse_iterator(std::begin(C)));\n}\n\n/// An iterator adaptor that filters the elements of given inner iterators.\n///\n/// The predicate parameter should be a callable object that accepts the wrapped\n/// iterator's reference type and returns a bool. When incrementing or\n/// decrementing the iterator, it will call the predicate on each element and\n/// skip any where it returns false.\n///\n/// \\code\n///   int A[] = { 1, 2, 3, 4 };\n///   auto R = make_filter_range(A, [](int N) { return N % 2 == 1; });\n///   // R contains { 1, 3 }.\n/// \\endcode\n///\n/// Note: filter_iterator_base implements support for forward iteration.\n/// filter_iterator_impl exists to provide support for bidirectional iteration,\n/// conditional on whether the wrapped iterator supports it.\ntemplate <typename WrappedIteratorT, typename PredicateT, typename IterTag>\nclass filter_iterator_base\n    : public iterator_adaptor_base<\n          filter_iterator_base<WrappedIteratorT, PredicateT, IterTag>,\n          WrappedIteratorT,\n          typename std::common_type<\n              IterTag, typename std::iterator_traits<\n                           WrappedIteratorT>::iterator_category>::type> {\n  using BaseT = iterator_adaptor_base<\n      filter_iterator_base<WrappedIteratorT, PredicateT, IterTag>,\n      WrappedIteratorT,\n      typename std::common_type<\n          IterTag, typename std::iterator_traits<\n                       WrappedIteratorT>::iterator_category>::type>;\n\nprotected:\n  WrappedIteratorT End;\n  PredicateT Pred;\n\n  void findNextValid() {\n    while (this->I != End && !Pred(*this->I))\n      BaseT::operator++();\n  }\n\n  // Construct the iterator. The begin iterator needs to know where the end\n  // is, so that it can properly stop when it gets there. The end iterator only\n  // needs the predicate to support bidirectional iteration.\n  filter_iterator_base(WrappedIteratorT Begin, WrappedIteratorT End,\n                       PredicateT Pred)\n      : BaseT(Begin), End(End), Pred(Pred) {\n    findNextValid();\n  }\n\npublic:\n  using BaseT::operator++;\n\n  filter_iterator_base &operator++() {\n    BaseT::operator++();\n    findNextValid();\n    return *this;\n  }\n};\n\n/// Specialization of filter_iterator_base for forward iteration only.\ntemplate <typename WrappedIteratorT, typename PredicateT,\n          typename IterTag = std::forward_iterator_tag>\nclass filter_iterator_impl\n    : public filter_iterator_base<WrappedIteratorT, PredicateT, IterTag> {\n  using BaseT = filter_iterator_base<WrappedIteratorT, PredicateT, IterTag>;\n\npublic:\n  filter_iterator_impl(WrappedIteratorT Begin, WrappedIteratorT End,\n                       PredicateT Pred)\n      : BaseT(Begin, End, Pred) {}\n};\n\n/// Specialization of filter_iterator_base for bidirectional iteration.\ntemplate <typename WrappedIteratorT, typename PredicateT>\nclass filter_iterator_impl<WrappedIteratorT, PredicateT,\n                           std::bidirectional_iterator_tag>\n    : public filter_iterator_base<WrappedIteratorT, PredicateT,\n                                  std::bidirectional_iterator_tag> {\n  using BaseT = filter_iterator_base<WrappedIteratorT, PredicateT,\n                                     std::bidirectional_iterator_tag>;\n  void findPrevValid() {\n    while (!this->Pred(*this->I))\n      BaseT::operator--();\n  }\n\npublic:\n  using BaseT::operator--;\n\n  filter_iterator_impl(WrappedIteratorT Begin, WrappedIteratorT End,\n                       PredicateT Pred)\n      : BaseT(Begin, End, Pred) {}\n\n  filter_iterator_impl &operator--() {\n    BaseT::operator--();\n    findPrevValid();\n    return *this;\n  }\n};\n\nnamespace detail {\n\ntemplate <bool is_bidirectional> struct fwd_or_bidi_tag_impl {\n  using type = std::forward_iterator_tag;\n};\n\ntemplate <> struct fwd_or_bidi_tag_impl<true> {\n  using type = std::bidirectional_iterator_tag;\n};\n\n/// Helper which sets its type member to forward_iterator_tag if the category\n/// of \\p IterT does not derive from bidirectional_iterator_tag, and to\n/// bidirectional_iterator_tag otherwise.\ntemplate <typename IterT> struct fwd_or_bidi_tag {\n  using type = typename fwd_or_bidi_tag_impl<std::is_base_of<\n      std::bidirectional_iterator_tag,\n      typename std::iterator_traits<IterT>::iterator_category>::value>::type;\n};\n\n} // namespace detail\n\n/// Defines filter_iterator to a suitable specialization of\n/// filter_iterator_impl, based on the underlying iterator's category.\ntemplate <typename WrappedIteratorT, typename PredicateT>\nusing filter_iterator = filter_iterator_impl<\n    WrappedIteratorT, PredicateT,\n    typename detail::fwd_or_bidi_tag<WrappedIteratorT>::type>;\n\n/// Convenience function that takes a range of elements and a predicate,\n/// and return a new filter_iterator range.\n///\n/// FIXME: Currently if RangeT && is a rvalue reference to a temporary, the\n/// lifetime of that temporary is not kept by the returned range object, and the\n/// temporary is going to be dropped on the floor after the make_iterator_range\n/// full expression that contains this function call.\ntemplate <typename RangeT, typename PredicateT>\niterator_range<filter_iterator<detail::IterOfRange<RangeT>, PredicateT>>\nmake_filter_range(RangeT &&Range, PredicateT Pred) {\n  using FilterIteratorT =\n      filter_iterator<detail::IterOfRange<RangeT>, PredicateT>;\n  return make_range(\n      FilterIteratorT(std::begin(std::forward<RangeT>(Range)),\n                      std::end(std::forward<RangeT>(Range)), Pred),\n      FilterIteratorT(std::end(std::forward<RangeT>(Range)),\n                      std::end(std::forward<RangeT>(Range)), Pred));\n}\n\n/// A pseudo-iterator adaptor that is designed to implement \"early increment\"\n/// style loops.\n///\n/// This is *not a normal iterator* and should almost never be used directly. It\n/// is intended primarily to be used with range based for loops and some range\n/// algorithms.\n///\n/// The iterator isn't quite an `OutputIterator` or an `InputIterator` but\n/// somewhere between them. The constraints of these iterators are:\n///\n/// - On construction or after being incremented, it is comparable and\n///   dereferencable. It is *not* incrementable.\n/// - After being dereferenced, it is neither comparable nor dereferencable, it\n///   is only incrementable.\n///\n/// This means you can only dereference the iterator once, and you can only\n/// increment it once between dereferences.\ntemplate <typename WrappedIteratorT>\nclass early_inc_iterator_impl\n    : public iterator_adaptor_base<early_inc_iterator_impl<WrappedIteratorT>,\n                                   WrappedIteratorT, std::input_iterator_tag> {\n  using BaseT =\n      iterator_adaptor_base<early_inc_iterator_impl<WrappedIteratorT>,\n                            WrappedIteratorT, std::input_iterator_tag>;\n\n  using PointerT = typename std::iterator_traits<WrappedIteratorT>::pointer;\n\nprotected:\n#if LLVM_ENABLE_ABI_BREAKING_CHECKS\n  bool IsEarlyIncremented = false;\n#endif\n\npublic:\n  early_inc_iterator_impl(WrappedIteratorT I) : BaseT(I) {}\n\n  using BaseT::operator*;\n  decltype(*std::declval<WrappedIteratorT>()) operator*() {\n#if LLVM_ENABLE_ABI_BREAKING_CHECKS\n    assert(!IsEarlyIncremented && \"Cannot dereference twice!\");\n    IsEarlyIncremented = true;\n#endif\n    return *(this->I)++;\n  }\n\n  using BaseT::operator++;\n  early_inc_iterator_impl &operator++() {\n#if LLVM_ENABLE_ABI_BREAKING_CHECKS\n    assert(IsEarlyIncremented && \"Cannot increment before dereferencing!\");\n    IsEarlyIncremented = false;\n#endif\n    return *this;\n  }\n\n  friend bool operator==(const early_inc_iterator_impl &LHS,\n                         const early_inc_iterator_impl &RHS) {\n#if LLVM_ENABLE_ABI_BREAKING_CHECKS\n    assert(!LHS.IsEarlyIncremented && \"Cannot compare after dereferencing!\");\n#endif\n    return (const BaseT &)LHS == (const BaseT &)RHS;\n  }\n};\n\n/// Make a range that does early increment to allow mutation of the underlying\n/// range without disrupting iteration.\n///\n/// The underlying iterator will be incremented immediately after it is\n/// dereferenced, allowing deletion of the current node or insertion of nodes to\n/// not disrupt iteration provided they do not invalidate the *next* iterator --\n/// the current iterator can be invalidated.\n///\n/// This requires a very exact pattern of use that is only really suitable to\n/// range based for loops and other range algorithms that explicitly guarantee\n/// to dereference exactly once each element, and to increment exactly once each\n/// element.\ntemplate <typename RangeT>\niterator_range<early_inc_iterator_impl<detail::IterOfRange<RangeT>>>\nmake_early_inc_range(RangeT &&Range) {\n  using EarlyIncIteratorT =\n      early_inc_iterator_impl<detail::IterOfRange<RangeT>>;\n  return make_range(EarlyIncIteratorT(std::begin(std::forward<RangeT>(Range))),\n                    EarlyIncIteratorT(std::end(std::forward<RangeT>(Range))));\n}\n\n// forward declarations required by zip_shortest/zip_first/zip_longest\ntemplate <typename R, typename UnaryPredicate>\nbool all_of(R &&range, UnaryPredicate P);\ntemplate <typename R, typename UnaryPredicate>\nbool any_of(R &&range, UnaryPredicate P);\n\nnamespace detail {\n\nusing std::declval;\n\n// We have to alias this since inlining the actual type at the usage site\n// in the parameter list of iterator_facade_base<> below ICEs MSVC 2017.\ntemplate<typename... Iters> struct ZipTupleType {\n  using type = std::tuple<decltype(*declval<Iters>())...>;\n};\n\ntemplate <typename ZipType, typename... Iters>\nusing zip_traits = iterator_facade_base<\n    ZipType, typename std::common_type<std::bidirectional_iterator_tag,\n                                       typename std::iterator_traits<\n                                           Iters>::iterator_category...>::type,\n    // ^ TODO: Implement random access methods.\n    typename ZipTupleType<Iters...>::type,\n    typename std::iterator_traits<typename std::tuple_element<\n        0, std::tuple<Iters...>>::type>::difference_type,\n    // ^ FIXME: This follows boost::make_zip_iterator's assumption that all\n    // inner iterators have the same difference_type. It would fail if, for\n    // instance, the second field's difference_type were non-numeric while the\n    // first is.\n    typename ZipTupleType<Iters...>::type *,\n    typename ZipTupleType<Iters...>::type>;\n\ntemplate <typename ZipType, typename... Iters>\nstruct zip_common : public zip_traits<ZipType, Iters...> {\n  using Base = zip_traits<ZipType, Iters...>;\n  using value_type = typename Base::value_type;\n\n  std::tuple<Iters...> iterators;\n\nprotected:\n  template <size_t... Ns> value_type deref(std::index_sequence<Ns...>) const {\n    return value_type(*std::get<Ns>(iterators)...);\n  }\n\n  template <size_t... Ns>\n  decltype(iterators) tup_inc(std::index_sequence<Ns...>) const {\n    return std::tuple<Iters...>(std::next(std::get<Ns>(iterators))...);\n  }\n\n  template <size_t... Ns>\n  decltype(iterators) tup_dec(std::index_sequence<Ns...>) const {\n    return std::tuple<Iters...>(std::prev(std::get<Ns>(iterators))...);\n  }\n\npublic:\n  zip_common(Iters &&... ts) : iterators(std::forward<Iters>(ts)...) {}\n\n  value_type operator*() { return deref(std::index_sequence_for<Iters...>{}); }\n\n  const value_type operator*() const {\n    return deref(std::index_sequence_for<Iters...>{});\n  }\n\n  ZipType &operator++() {\n    iterators = tup_inc(std::index_sequence_for<Iters...>{});\n    return *reinterpret_cast<ZipType *>(this);\n  }\n\n  ZipType &operator--() {\n    static_assert(Base::IsBidirectional,\n                  \"All inner iterators must be at least bidirectional.\");\n    iterators = tup_dec(std::index_sequence_for<Iters...>{});\n    return *reinterpret_cast<ZipType *>(this);\n  }\n};\n\ntemplate <typename... Iters>\nstruct zip_first : public zip_common<zip_first<Iters...>, Iters...> {\n  using Base = zip_common<zip_first<Iters...>, Iters...>;\n\n  bool operator==(const zip_first<Iters...> &other) const {\n    return std::get<0>(this->iterators) == std::get<0>(other.iterators);\n  }\n\n  zip_first(Iters &&... ts) : Base(std::forward<Iters>(ts)...) {}\n};\n\ntemplate <typename... Iters>\nclass zip_shortest : public zip_common<zip_shortest<Iters...>, Iters...> {\n  template <size_t... Ns>\n  bool test(const zip_shortest<Iters...> &other,\n            std::index_sequence<Ns...>) const {\n    return all_of(std::initializer_list<bool>{std::get<Ns>(this->iterators) !=\n                                              std::get<Ns>(other.iterators)...},\n                  identity<bool>{});\n  }\n\npublic:\n  using Base = zip_common<zip_shortest<Iters...>, Iters...>;\n\n  zip_shortest(Iters &&... ts) : Base(std::forward<Iters>(ts)...) {}\n\n  bool operator==(const zip_shortest<Iters...> &other) const {\n    return !test(other, std::index_sequence_for<Iters...>{});\n  }\n};\n\ntemplate <template <typename...> class ItType, typename... Args> class zippy {\npublic:\n  using iterator = ItType<decltype(std::begin(std::declval<Args>()))...>;\n  using iterator_category = typename iterator::iterator_category;\n  using value_type = typename iterator::value_type;\n  using difference_type = typename iterator::difference_type;\n  using pointer = typename iterator::pointer;\n  using reference = typename iterator::reference;\n\nprivate:\n  std::tuple<Args...> ts;\n\n  template <size_t... Ns>\n  iterator begin_impl(std::index_sequence<Ns...>) const {\n    return iterator(std::begin(std::get<Ns>(ts))...);\n  }\n  template <size_t... Ns> iterator end_impl(std::index_sequence<Ns...>) const {\n    return iterator(std::end(std::get<Ns>(ts))...);\n  }\n\npublic:\n  zippy(Args &&... ts_) : ts(std::forward<Args>(ts_)...) {}\n\n  iterator begin() const {\n    return begin_impl(std::index_sequence_for<Args...>{});\n  }\n  iterator end() const { return end_impl(std::index_sequence_for<Args...>{}); }\n};\n\n} // end namespace detail\n\n/// zip iterator for two or more iteratable types.\ntemplate <typename T, typename U, typename... Args>\ndetail::zippy<detail::zip_shortest, T, U, Args...> zip(T &&t, U &&u,\n                                                       Args &&... args) {\n  return detail::zippy<detail::zip_shortest, T, U, Args...>(\n      std::forward<T>(t), std::forward<U>(u), std::forward<Args>(args)...);\n}\n\n/// zip iterator that, for the sake of efficiency, assumes the first iteratee to\n/// be the shortest.\ntemplate <typename T, typename U, typename... Args>\ndetail::zippy<detail::zip_first, T, U, Args...> zip_first(T &&t, U &&u,\n                                                          Args &&... args) {\n  return detail::zippy<detail::zip_first, T, U, Args...>(\n      std::forward<T>(t), std::forward<U>(u), std::forward<Args>(args)...);\n}\n\nnamespace detail {\ntemplate <typename Iter>\nIter next_or_end(const Iter &I, const Iter &End) {\n  if (I == End)\n    return End;\n  return std::next(I);\n}\n\ntemplate <typename Iter>\nauto deref_or_none(const Iter &I, const Iter &End) -> llvm::Optional<\n    std::remove_const_t<std::remove_reference_t<decltype(*I)>>> {\n  if (I == End)\n    return None;\n  return *I;\n}\n\ntemplate <typename Iter> struct ZipLongestItemType {\n  using type =\n      llvm::Optional<typename std::remove_const<typename std::remove_reference<\n          decltype(*std::declval<Iter>())>::type>::type>;\n};\n\ntemplate <typename... Iters> struct ZipLongestTupleType {\n  using type = std::tuple<typename ZipLongestItemType<Iters>::type...>;\n};\n\ntemplate <typename... Iters>\nclass zip_longest_iterator\n    : public iterator_facade_base<\n          zip_longest_iterator<Iters...>,\n          typename std::common_type<\n              std::forward_iterator_tag,\n              typename std::iterator_traits<Iters>::iterator_category...>::type,\n          typename ZipLongestTupleType<Iters...>::type,\n          typename std::iterator_traits<typename std::tuple_element<\n              0, std::tuple<Iters...>>::type>::difference_type,\n          typename ZipLongestTupleType<Iters...>::type *,\n          typename ZipLongestTupleType<Iters...>::type> {\npublic:\n  using value_type = typename ZipLongestTupleType<Iters...>::type;\n\nprivate:\n  std::tuple<Iters...> iterators;\n  std::tuple<Iters...> end_iterators;\n\n  template <size_t... Ns>\n  bool test(const zip_longest_iterator<Iters...> &other,\n            std::index_sequence<Ns...>) const {\n    return llvm::any_of(\n        std::initializer_list<bool>{std::get<Ns>(this->iterators) !=\n                                    std::get<Ns>(other.iterators)...},\n        identity<bool>{});\n  }\n\n  template <size_t... Ns> value_type deref(std::index_sequence<Ns...>) const {\n    return value_type(\n        deref_or_none(std::get<Ns>(iterators), std::get<Ns>(end_iterators))...);\n  }\n\n  template <size_t... Ns>\n  decltype(iterators) tup_inc(std::index_sequence<Ns...>) const {\n    return std::tuple<Iters...>(\n        next_or_end(std::get<Ns>(iterators), std::get<Ns>(end_iterators))...);\n  }\n\npublic:\n  zip_longest_iterator(std::pair<Iters &&, Iters &&>... ts)\n      : iterators(std::forward<Iters>(ts.first)...),\n        end_iterators(std::forward<Iters>(ts.second)...) {}\n\n  value_type operator*() { return deref(std::index_sequence_for<Iters...>{}); }\n\n  value_type operator*() const {\n    return deref(std::index_sequence_for<Iters...>{});\n  }\n\n  zip_longest_iterator<Iters...> &operator++() {\n    iterators = tup_inc(std::index_sequence_for<Iters...>{});\n    return *this;\n  }\n\n  bool operator==(const zip_longest_iterator<Iters...> &other) const {\n    return !test(other, std::index_sequence_for<Iters...>{});\n  }\n};\n\ntemplate <typename... Args> class zip_longest_range {\npublic:\n  using iterator =\n      zip_longest_iterator<decltype(adl_begin(std::declval<Args>()))...>;\n  using iterator_category = typename iterator::iterator_category;\n  using value_type = typename iterator::value_type;\n  using difference_type = typename iterator::difference_type;\n  using pointer = typename iterator::pointer;\n  using reference = typename iterator::reference;\n\nprivate:\n  std::tuple<Args...> ts;\n\n  template <size_t... Ns>\n  iterator begin_impl(std::index_sequence<Ns...>) const {\n    return iterator(std::make_pair(adl_begin(std::get<Ns>(ts)),\n                                   adl_end(std::get<Ns>(ts)))...);\n  }\n\n  template <size_t... Ns> iterator end_impl(std::index_sequence<Ns...>) const {\n    return iterator(std::make_pair(adl_end(std::get<Ns>(ts)),\n                                   adl_end(std::get<Ns>(ts)))...);\n  }\n\npublic:\n  zip_longest_range(Args &&... ts_) : ts(std::forward<Args>(ts_)...) {}\n\n  iterator begin() const {\n    return begin_impl(std::index_sequence_for<Args...>{});\n  }\n  iterator end() const { return end_impl(std::index_sequence_for<Args...>{}); }\n};\n} // namespace detail\n\n/// Iterate over two or more iterators at the same time. Iteration continues\n/// until all iterators reach the end. The llvm::Optional only contains a value\n/// if the iterator has not reached the end.\ntemplate <typename T, typename U, typename... Args>\ndetail::zip_longest_range<T, U, Args...> zip_longest(T &&t, U &&u,\n                                                     Args &&... args) {\n  return detail::zip_longest_range<T, U, Args...>(\n      std::forward<T>(t), std::forward<U>(u), std::forward<Args>(args)...);\n}\n\n/// Iterator wrapper that concatenates sequences together.\n///\n/// This can concatenate different iterators, even with different types, into\n/// a single iterator provided the value types of all the concatenated\n/// iterators expose `reference` and `pointer` types that can be converted to\n/// `ValueT &` and `ValueT *` respectively. It doesn't support more\n/// interesting/customized pointer or reference types.\n///\n/// Currently this only supports forward or higher iterator categories as\n/// inputs and always exposes a forward iterator interface.\ntemplate <typename ValueT, typename... IterTs>\nclass concat_iterator\n    : public iterator_facade_base<concat_iterator<ValueT, IterTs...>,\n                                  std::forward_iterator_tag, ValueT> {\n  using BaseT = typename concat_iterator::iterator_facade_base;\n\n  /// We store both the current and end iterators for each concatenated\n  /// sequence in a tuple of pairs.\n  ///\n  /// Note that something like iterator_range seems nice at first here, but the\n  /// range properties are of little benefit and end up getting in the way\n  /// because we need to do mutation on the current iterators.\n  std::tuple<IterTs...> Begins;\n  std::tuple<IterTs...> Ends;\n\n  /// Attempts to increment a specific iterator.\n  ///\n  /// Returns true if it was able to increment the iterator. Returns false if\n  /// the iterator is already at the end iterator.\n  template <size_t Index> bool incrementHelper() {\n    auto &Begin = std::get<Index>(Begins);\n    auto &End = std::get<Index>(Ends);\n    if (Begin == End)\n      return false;\n\n    ++Begin;\n    return true;\n  }\n\n  /// Increments the first non-end iterator.\n  ///\n  /// It is an error to call this with all iterators at the end.\n  template <size_t... Ns> void increment(std::index_sequence<Ns...>) {\n    // Build a sequence of functions to increment each iterator if possible.\n    bool (concat_iterator::*IncrementHelperFns[])() = {\n        &concat_iterator::incrementHelper<Ns>...};\n\n    // Loop over them, and stop as soon as we succeed at incrementing one.\n    for (auto &IncrementHelperFn : IncrementHelperFns)\n      if ((this->*IncrementHelperFn)())\n        return;\n\n    llvm_unreachable(\"Attempted to increment an end concat iterator!\");\n  }\n\n  /// Returns null if the specified iterator is at the end. Otherwise,\n  /// dereferences the iterator and returns the address of the resulting\n  /// reference.\n  template <size_t Index> ValueT *getHelper() const {\n    auto &Begin = std::get<Index>(Begins);\n    auto &End = std::get<Index>(Ends);\n    if (Begin == End)\n      return nullptr;\n\n    return &*Begin;\n  }\n\n  /// Finds the first non-end iterator, dereferences, and returns the resulting\n  /// reference.\n  ///\n  /// It is an error to call this with all iterators at the end.\n  template <size_t... Ns> ValueT &get(std::index_sequence<Ns...>) const {\n    // Build a sequence of functions to get from iterator if possible.\n    ValueT *(concat_iterator::*GetHelperFns[])() const = {\n        &concat_iterator::getHelper<Ns>...};\n\n    // Loop over them, and return the first result we find.\n    for (auto &GetHelperFn : GetHelperFns)\n      if (ValueT *P = (this->*GetHelperFn)())\n        return *P;\n\n    llvm_unreachable(\"Attempted to get a pointer from an end concat iterator!\");\n  }\n\npublic:\n  /// Constructs an iterator from a sequence of ranges.\n  ///\n  /// We need the full range to know how to switch between each of the\n  /// iterators.\n  template <typename... RangeTs>\n  explicit concat_iterator(RangeTs &&... Ranges)\n      : Begins(std::begin(Ranges)...), Ends(std::end(Ranges)...) {}\n\n  using BaseT::operator++;\n\n  concat_iterator &operator++() {\n    increment(std::index_sequence_for<IterTs...>());\n    return *this;\n  }\n\n  ValueT &operator*() const {\n    return get(std::index_sequence_for<IterTs...>());\n  }\n\n  bool operator==(const concat_iterator &RHS) const {\n    return Begins == RHS.Begins && Ends == RHS.Ends;\n  }\n};\n\nnamespace detail {\n\n/// Helper to store a sequence of ranges being concatenated and access them.\n///\n/// This is designed to facilitate providing actual storage when temporaries\n/// are passed into the constructor such that we can use it as part of range\n/// based for loops.\ntemplate <typename ValueT, typename... RangeTs> class concat_range {\npublic:\n  using iterator =\n      concat_iterator<ValueT,\n                      decltype(std::begin(std::declval<RangeTs &>()))...>;\n\nprivate:\n  std::tuple<RangeTs...> Ranges;\n\n  template <size_t... Ns> iterator begin_impl(std::index_sequence<Ns...>) {\n    return iterator(std::get<Ns>(Ranges)...);\n  }\n  template <size_t... Ns> iterator end_impl(std::index_sequence<Ns...>) {\n    return iterator(make_range(std::end(std::get<Ns>(Ranges)),\n                               std::end(std::get<Ns>(Ranges)))...);\n  }\n\npublic:\n  concat_range(RangeTs &&... Ranges)\n      : Ranges(std::forward<RangeTs>(Ranges)...) {}\n\n  iterator begin() { return begin_impl(std::index_sequence_for<RangeTs...>{}); }\n  iterator end() { return end_impl(std::index_sequence_for<RangeTs...>{}); }\n};\n\n} // end namespace detail\n\n/// Concatenated range across two or more ranges.\n///\n/// The desired value type must be explicitly specified.\ntemplate <typename ValueT, typename... RangeTs>\ndetail::concat_range<ValueT, RangeTs...> concat(RangeTs &&... Ranges) {\n  static_assert(sizeof...(RangeTs) > 1,\n                \"Need more than one range to concatenate!\");\n  return detail::concat_range<ValueT, RangeTs...>(\n      std::forward<RangeTs>(Ranges)...);\n}\n\n/// A utility class used to implement an iterator that contains some base object\n/// and an index. The iterator moves the index but keeps the base constant.\ntemplate <typename DerivedT, typename BaseT, typename T,\n          typename PointerT = T *, typename ReferenceT = T &>\nclass indexed_accessor_iterator\n    : public llvm::iterator_facade_base<DerivedT,\n                                        std::random_access_iterator_tag, T,\n                                        std::ptrdiff_t, PointerT, ReferenceT> {\npublic:\n  ptrdiff_t operator-(const indexed_accessor_iterator &rhs) const {\n    assert(base == rhs.base && \"incompatible iterators\");\n    return index - rhs.index;\n  }\n  bool operator==(const indexed_accessor_iterator &rhs) const {\n    return base == rhs.base && index == rhs.index;\n  }\n  bool operator<(const indexed_accessor_iterator &rhs) const {\n    assert(base == rhs.base && \"incompatible iterators\");\n    return index < rhs.index;\n  }\n\n  DerivedT &operator+=(ptrdiff_t offset) {\n    this->index += offset;\n    return static_cast<DerivedT &>(*this);\n  }\n  DerivedT &operator-=(ptrdiff_t offset) {\n    this->index -= offset;\n    return static_cast<DerivedT &>(*this);\n  }\n\n  /// Returns the current index of the iterator.\n  ptrdiff_t getIndex() const { return index; }\n\n  /// Returns the current base of the iterator.\n  const BaseT &getBase() const { return base; }\n\nprotected:\n  indexed_accessor_iterator(BaseT base, ptrdiff_t index)\n      : base(base), index(index) {}\n  BaseT base;\n  ptrdiff_t index;\n};\n\nnamespace detail {\n/// The class represents the base of a range of indexed_accessor_iterators. It\n/// provides support for many different range functionalities, e.g.\n/// drop_front/slice/etc.. Derived range classes must implement the following\n/// static methods:\n///   * ReferenceT dereference_iterator(const BaseT &base, ptrdiff_t index)\n///     - Dereference an iterator pointing to the base object at the given\n///       index.\n///   * BaseT offset_base(const BaseT &base, ptrdiff_t index)\n///     - Return a new base that is offset from the provide base by 'index'\n///       elements.\ntemplate <typename DerivedT, typename BaseT, typename T,\n          typename PointerT = T *, typename ReferenceT = T &>\nclass indexed_accessor_range_base {\npublic:\n  using RangeBaseT =\n      indexed_accessor_range_base<DerivedT, BaseT, T, PointerT, ReferenceT>;\n\n  /// An iterator element of this range.\n  class iterator : public indexed_accessor_iterator<iterator, BaseT, T,\n                                                    PointerT, ReferenceT> {\n  public:\n    // Index into this iterator, invoking a static method on the derived type.\n    ReferenceT operator*() const {\n      return DerivedT::dereference_iterator(this->getBase(), this->getIndex());\n    }\n\n  private:\n    iterator(BaseT owner, ptrdiff_t curIndex)\n        : indexed_accessor_iterator<iterator, BaseT, T, PointerT, ReferenceT>(\n              owner, curIndex) {}\n\n    /// Allow access to the constructor.\n    friend indexed_accessor_range_base<DerivedT, BaseT, T, PointerT,\n                                       ReferenceT>;\n  };\n\n  indexed_accessor_range_base(iterator begin, iterator end)\n      : base(offset_base(begin.getBase(), begin.getIndex())),\n        count(end.getIndex() - begin.getIndex()) {}\n  indexed_accessor_range_base(const iterator_range<iterator> &range)\n      : indexed_accessor_range_base(range.begin(), range.end()) {}\n  indexed_accessor_range_base(BaseT base, ptrdiff_t count)\n      : base(base), count(count) {}\n\n  iterator begin() const { return iterator(base, 0); }\n  iterator end() const { return iterator(base, count); }\n  ReferenceT operator[](size_t Index) const {\n    assert(Index < size() && \"invalid index for value range\");\n    return DerivedT::dereference_iterator(base, static_cast<ptrdiff_t>(Index));\n  }\n  ReferenceT front() const {\n    assert(!empty() && \"expected non-empty range\");\n    return (*this)[0];\n  }\n  ReferenceT back() const {\n    assert(!empty() && \"expected non-empty range\");\n    return (*this)[size() - 1];\n  }\n\n  /// Compare this range with another.\n  template <typename OtherT> bool operator==(const OtherT &other) const {\n    return size() ==\n               static_cast<size_t>(std::distance(other.begin(), other.end())) &&\n           std::equal(begin(), end(), other.begin());\n  }\n  template <typename OtherT> bool operator!=(const OtherT &other) const {\n    return !(*this == other);\n  }\n\n  /// Return the size of this range.\n  size_t size() const { return count; }\n\n  /// Return if the range is empty.\n  bool empty() const { return size() == 0; }\n\n  /// Drop the first N elements, and keep M elements.\n  DerivedT slice(size_t n, size_t m) const {\n    assert(n + m <= size() && \"invalid size specifiers\");\n    return DerivedT(offset_base(base, n), m);\n  }\n\n  /// Drop the first n elements.\n  DerivedT drop_front(size_t n = 1) const {\n    assert(size() >= n && \"Dropping more elements than exist\");\n    return slice(n, size() - n);\n  }\n  /// Drop the last n elements.\n  DerivedT drop_back(size_t n = 1) const {\n    assert(size() >= n && \"Dropping more elements than exist\");\n    return DerivedT(base, size() - n);\n  }\n\n  /// Take the first n elements.\n  DerivedT take_front(size_t n = 1) const {\n    return n < size() ? drop_back(size() - n)\n                      : static_cast<const DerivedT &>(*this);\n  }\n\n  /// Take the last n elements.\n  DerivedT take_back(size_t n = 1) const {\n    return n < size() ? drop_front(size() - n)\n                      : static_cast<const DerivedT &>(*this);\n  }\n\n  /// Allow conversion to any type accepting an iterator_range.\n  template <typename RangeT, typename = std::enable_if_t<std::is_constructible<\n                                 RangeT, iterator_range<iterator>>::value>>\n  operator RangeT() const {\n    return RangeT(iterator_range<iterator>(*this));\n  }\n\n  /// Returns the base of this range.\n  const BaseT &getBase() const { return base; }\n\nprivate:\n  /// Offset the given base by the given amount.\n  static BaseT offset_base(const BaseT &base, size_t n) {\n    return n == 0 ? base : DerivedT::offset_base(base, n);\n  }\n\nprotected:\n  indexed_accessor_range_base(const indexed_accessor_range_base &) = default;\n  indexed_accessor_range_base(indexed_accessor_range_base &&) = default;\n  indexed_accessor_range_base &\n  operator=(const indexed_accessor_range_base &) = default;\n\n  /// The base that owns the provided range of values.\n  BaseT base;\n  /// The size from the owning range.\n  ptrdiff_t count;\n};\n} // end namespace detail\n\n/// This class provides an implementation of a range of\n/// indexed_accessor_iterators where the base is not indexable. Ranges with\n/// bases that are offsetable should derive from indexed_accessor_range_base\n/// instead. Derived range classes are expected to implement the following\n/// static method:\n///   * ReferenceT dereference(const BaseT &base, ptrdiff_t index)\n///     - Dereference an iterator pointing to a parent base at the given index.\ntemplate <typename DerivedT, typename BaseT, typename T,\n          typename PointerT = T *, typename ReferenceT = T &>\nclass indexed_accessor_range\n    : public detail::indexed_accessor_range_base<\n          DerivedT, std::pair<BaseT, ptrdiff_t>, T, PointerT, ReferenceT> {\npublic:\n  indexed_accessor_range(BaseT base, ptrdiff_t startIndex, ptrdiff_t count)\n      : detail::indexed_accessor_range_base<\n            DerivedT, std::pair<BaseT, ptrdiff_t>, T, PointerT, ReferenceT>(\n            std::make_pair(base, startIndex), count) {}\n  using detail::indexed_accessor_range_base<\n      DerivedT, std::pair<BaseT, ptrdiff_t>, T, PointerT,\n      ReferenceT>::indexed_accessor_range_base;\n\n  /// Returns the current base of the range.\n  const BaseT &getBase() const { return this->base.first; }\n\n  /// Returns the current start index of the range.\n  ptrdiff_t getStartIndex() const { return this->base.second; }\n\n  /// See `detail::indexed_accessor_range_base` for details.\n  static std::pair<BaseT, ptrdiff_t>\n  offset_base(const std::pair<BaseT, ptrdiff_t> &base, ptrdiff_t index) {\n    // We encode the internal base as a pair of the derived base and a start\n    // index into the derived base.\n    return std::make_pair(base.first, base.second + index);\n  }\n  /// See `detail::indexed_accessor_range_base` for details.\n  static ReferenceT\n  dereference_iterator(const std::pair<BaseT, ptrdiff_t> &base,\n                       ptrdiff_t index) {\n    return DerivedT::dereference(base.first, base.second + index);\n  }\n};\n\n/// Given a container of pairs, return a range over the first elements.\ntemplate <typename ContainerTy> auto make_first_range(ContainerTy &&c) {\n  return llvm::map_range(\n      std::forward<ContainerTy>(c),\n      [](decltype((*std::begin(c))) elt) -> decltype((elt.first)) {\n        return elt.first;\n      });\n}\n\n/// Given a container of pairs, return a range over the second elements.\ntemplate <typename ContainerTy> auto make_second_range(ContainerTy &&c) {\n  return llvm::map_range(\n      std::forward<ContainerTy>(c),\n      [](decltype((*std::begin(c))) elt) -> decltype((elt.second)) {\n        return elt.second;\n      });\n}\n\n//===----------------------------------------------------------------------===//\n//     Extra additions to <utility>\n//===----------------------------------------------------------------------===//\n\n/// Function object to check whether the first component of a std::pair\n/// compares less than the first component of another std::pair.\nstruct less_first {\n  template <typename T> bool operator()(const T &lhs, const T &rhs) const {\n    return lhs.first < rhs.first;\n  }\n};\n\n/// Function object to check whether the second component of a std::pair\n/// compares less than the second component of another std::pair.\nstruct less_second {\n  template <typename T> bool operator()(const T &lhs, const T &rhs) const {\n    return lhs.second < rhs.second;\n  }\n};\n\n/// \\brief Function object to apply a binary function to the first component of\n/// a std::pair.\ntemplate<typename FuncTy>\nstruct on_first {\n  FuncTy func;\n\n  template <typename T>\n  decltype(auto) operator()(const T &lhs, const T &rhs) const {\n    return func(lhs.first, rhs.first);\n  }\n};\n\n/// Utility type to build an inheritance chain that makes it easy to rank\n/// overload candidates.\ntemplate <int N> struct rank : rank<N - 1> {};\ntemplate <> struct rank<0> {};\n\n/// traits class for checking whether type T is one of any of the given\n/// types in the variadic list.\ntemplate <typename T, typename... Ts> struct is_one_of {\n  static const bool value = false;\n};\n\ntemplate <typename T, typename U, typename... Ts>\nstruct is_one_of<T, U, Ts...> {\n  static const bool value =\n      std::is_same<T, U>::value || is_one_of<T, Ts...>::value;\n};\n\n/// traits class for checking whether type T is a base class for all\n///  the given types in the variadic list.\ntemplate <typename T, typename... Ts> struct are_base_of {\n  static const bool value = true;\n};\n\ntemplate <typename T, typename U, typename... Ts>\nstruct are_base_of<T, U, Ts...> {\n  static const bool value =\n      std::is_base_of<T, U>::value && are_base_of<T, Ts...>::value;\n};\n\n//===----------------------------------------------------------------------===//\n//     Extra additions for arrays\n//===----------------------------------------------------------------------===//\n\n// We have a copy here so that LLVM behaves the same when using different\n// standard libraries.\ntemplate <class Iterator, class RNG>\nvoid shuffle(Iterator first, Iterator last, RNG &&g) {\n  // It would be better to use a std::uniform_int_distribution,\n  // but that would be stdlib dependent.\n  typedef\n      typename std::iterator_traits<Iterator>::difference_type difference_type;\n  for (auto size = last - first; size > 1; ++first, (void)--size) {\n    difference_type offset = g() % size;\n    // Avoid self-assignment due to incorrect assertions in libstdc++\n    // containers (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85828).\n    if (offset != difference_type(0))\n      std::iter_swap(first, first + offset);\n  }\n}\n\n/// Find the length of an array.\ntemplate <class T, std::size_t N>\nconstexpr inline size_t array_lengthof(T (&)[N]) {\n  return N;\n}\n\n/// Adapt std::less<T> for array_pod_sort.\ntemplate<typename T>\ninline int array_pod_sort_comparator(const void *P1, const void *P2) {\n  if (std::less<T>()(*reinterpret_cast<const T*>(P1),\n                     *reinterpret_cast<const T*>(P2)))\n    return -1;\n  if (std::less<T>()(*reinterpret_cast<const T*>(P2),\n                     *reinterpret_cast<const T*>(P1)))\n    return 1;\n  return 0;\n}\n\n/// get_array_pod_sort_comparator - This is an internal helper function used to\n/// get type deduction of T right.\ntemplate<typename T>\ninline int (*get_array_pod_sort_comparator(const T &))\n             (const void*, const void*) {\n  return array_pod_sort_comparator<T>;\n}\n\n#ifdef EXPENSIVE_CHECKS\nnamespace detail {\n\ninline unsigned presortShuffleEntropy() {\n  static unsigned Result(std::random_device{}());\n  return Result;\n}\n\ntemplate <class IteratorTy>\ninline void presortShuffle(IteratorTy Start, IteratorTy End) {\n  std::mt19937 Generator(presortShuffleEntropy());\n  llvm::shuffle(Start, End, Generator);\n}\n\n} // end namespace detail\n#endif\n\n/// array_pod_sort - This sorts an array with the specified start and end\n/// extent.  This is just like std::sort, except that it calls qsort instead of\n/// using an inlined template.  qsort is slightly slower than std::sort, but\n/// most sorts are not performance critical in LLVM and std::sort has to be\n/// template instantiated for each type, leading to significant measured code\n/// bloat.  This function should generally be used instead of std::sort where\n/// possible.\n///\n/// This function assumes that you have simple POD-like types that can be\n/// compared with std::less and can be moved with memcpy.  If this isn't true,\n/// you should use std::sort.\n///\n/// NOTE: If qsort_r were portable, we could allow a custom comparator and\n/// default to std::less.\ntemplate<class IteratorTy>\ninline void array_pod_sort(IteratorTy Start, IteratorTy End) {\n  // Don't inefficiently call qsort with one element or trigger undefined\n  // behavior with an empty sequence.\n  auto NElts = End - Start;\n  if (NElts <= 1) return;\n#ifdef EXPENSIVE_CHECKS\n  detail::presortShuffle<IteratorTy>(Start, End);\n#endif\n  qsort(&*Start, NElts, sizeof(*Start), get_array_pod_sort_comparator(*Start));\n}\n\ntemplate <class IteratorTy>\ninline void array_pod_sort(\n    IteratorTy Start, IteratorTy End,\n    int (*Compare)(\n        const typename std::iterator_traits<IteratorTy>::value_type *,\n        const typename std::iterator_traits<IteratorTy>::value_type *)) {\n  // Don't inefficiently call qsort with one element or trigger undefined\n  // behavior with an empty sequence.\n  auto NElts = End - Start;\n  if (NElts <= 1) return;\n#ifdef EXPENSIVE_CHECKS\n  detail::presortShuffle<IteratorTy>(Start, End);\n#endif\n  qsort(&*Start, NElts, sizeof(*Start),\n        reinterpret_cast<int (*)(const void *, const void *)>(Compare));\n}\n\nnamespace detail {\ntemplate <typename T>\n// We can use qsort if the iterator type is a pointer and the underlying value\n// is trivially copyable.\nusing sort_trivially_copyable = conjunction<\n    std::is_pointer<T>,\n    std::is_trivially_copyable<typename std::iterator_traits<T>::value_type>>;\n} // namespace detail\n\n// Provide wrappers to std::sort which shuffle the elements before sorting\n// to help uncover non-deterministic behavior (PR35135).\ntemplate <typename IteratorTy,\n          std::enable_if_t<!detail::sort_trivially_copyable<IteratorTy>::value,\n                           int> = 0>\ninline void sort(IteratorTy Start, IteratorTy End) {\n#ifdef EXPENSIVE_CHECKS\n  detail::presortShuffle<IteratorTy>(Start, End);\n#endif\n  std::sort(Start, End);\n}\n\n// Forward trivially copyable types to array_pod_sort. This avoids a large\n// amount of code bloat for a minor performance hit.\ntemplate <typename IteratorTy,\n          std::enable_if_t<detail::sort_trivially_copyable<IteratorTy>::value,\n                           int> = 0>\ninline void sort(IteratorTy Start, IteratorTy End) {\n  array_pod_sort(Start, End);\n}\n\ntemplate <typename Container> inline void sort(Container &&C) {\n  llvm::sort(adl_begin(C), adl_end(C));\n}\n\ntemplate <typename IteratorTy, typename Compare>\ninline void sort(IteratorTy Start, IteratorTy End, Compare Comp) {\n#ifdef EXPENSIVE_CHECKS\n  detail::presortShuffle<IteratorTy>(Start, End);\n#endif\n  std::sort(Start, End, Comp);\n}\n\ntemplate <typename Container, typename Compare>\ninline void sort(Container &&C, Compare Comp) {\n  llvm::sort(adl_begin(C), adl_end(C), Comp);\n}\n\n//===----------------------------------------------------------------------===//\n//     Extra additions to <algorithm>\n//===----------------------------------------------------------------------===//\n\n/// Get the size of a range. This is a wrapper function around std::distance\n/// which is only enabled when the operation is O(1).\ntemplate <typename R>\nauto size(R &&Range,\n          std::enable_if_t<\n              std::is_base_of<std::random_access_iterator_tag,\n                              typename std::iterator_traits<decltype(\n                                  Range.begin())>::iterator_category>::value,\n              void> * = nullptr) {\n  return std::distance(Range.begin(), Range.end());\n}\n\n/// Provide wrappers to std::for_each which take ranges instead of having to\n/// pass begin/end explicitly.\ntemplate <typename R, typename UnaryFunction>\nUnaryFunction for_each(R &&Range, UnaryFunction F) {\n  return std::for_each(adl_begin(Range), adl_end(Range), F);\n}\n\n/// Provide wrappers to std::all_of which take ranges instead of having to pass\n/// begin/end explicitly.\ntemplate <typename R, typename UnaryPredicate>\nbool all_of(R &&Range, UnaryPredicate P) {\n  return std::all_of(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Provide wrappers to std::any_of which take ranges instead of having to pass\n/// begin/end explicitly.\ntemplate <typename R, typename UnaryPredicate>\nbool any_of(R &&Range, UnaryPredicate P) {\n  return std::any_of(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Provide wrappers to std::none_of which take ranges instead of having to pass\n/// begin/end explicitly.\ntemplate <typename R, typename UnaryPredicate>\nbool none_of(R &&Range, UnaryPredicate P) {\n  return std::none_of(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Provide wrappers to std::find which take ranges instead of having to pass\n/// begin/end explicitly.\ntemplate <typename R, typename T> auto find(R &&Range, const T &Val) {\n  return std::find(adl_begin(Range), adl_end(Range), Val);\n}\n\n/// Provide wrappers to std::find_if which take ranges instead of having to pass\n/// begin/end explicitly.\ntemplate <typename R, typename UnaryPredicate>\nauto find_if(R &&Range, UnaryPredicate P) {\n  return std::find_if(adl_begin(Range), adl_end(Range), P);\n}\n\ntemplate <typename R, typename UnaryPredicate>\nauto find_if_not(R &&Range, UnaryPredicate P) {\n  return std::find_if_not(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Provide wrappers to std::remove_if which take ranges instead of having to\n/// pass begin/end explicitly.\ntemplate <typename R, typename UnaryPredicate>\nauto remove_if(R &&Range, UnaryPredicate P) {\n  return std::remove_if(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Provide wrappers to std::copy_if which take ranges instead of having to\n/// pass begin/end explicitly.\ntemplate <typename R, typename OutputIt, typename UnaryPredicate>\nOutputIt copy_if(R &&Range, OutputIt Out, UnaryPredicate P) {\n  return std::copy_if(adl_begin(Range), adl_end(Range), Out, P);\n}\n\ntemplate <typename R, typename OutputIt>\nOutputIt copy(R &&Range, OutputIt Out) {\n  return std::copy(adl_begin(Range), adl_end(Range), Out);\n}\n\n/// Provide wrappers to std::move which take ranges instead of having to\n/// pass begin/end explicitly.\ntemplate <typename R, typename OutputIt>\nOutputIt move(R &&Range, OutputIt Out) {\n  return std::move(adl_begin(Range), adl_end(Range), Out);\n}\n\n/// Wrapper function around std::find to detect if an element exists\n/// in a container.\ntemplate <typename R, typename E>\nbool is_contained(R &&Range, const E &Element) {\n  return std::find(adl_begin(Range), adl_end(Range), Element) != adl_end(Range);\n}\n\n/// Wrapper function around std::is_sorted to check if elements in a range \\p R\n/// are sorted with respect to a comparator \\p C.\ntemplate <typename R, typename Compare> bool is_sorted(R &&Range, Compare C) {\n  return std::is_sorted(adl_begin(Range), adl_end(Range), C);\n}\n\n/// Wrapper function around std::is_sorted to check if elements in a range \\p R\n/// are sorted in non-descending order.\ntemplate <typename R> bool is_sorted(R &&Range) {\n  return std::is_sorted(adl_begin(Range), adl_end(Range));\n}\n\n/// Wrapper function around std::count to count the number of times an element\n/// \\p Element occurs in the given range \\p Range.\ntemplate <typename R, typename E> auto count(R &&Range, const E &Element) {\n  return std::count(adl_begin(Range), adl_end(Range), Element);\n}\n\n/// Wrapper function around std::count_if to count the number of times an\n/// element satisfying a given predicate occurs in a range.\ntemplate <typename R, typename UnaryPredicate>\nauto count_if(R &&Range, UnaryPredicate P) {\n  return std::count_if(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Wrapper function around std::transform to apply a function to a range and\n/// store the result elsewhere.\ntemplate <typename R, typename OutputIt, typename UnaryFunction>\nOutputIt transform(R &&Range, OutputIt d_first, UnaryFunction F) {\n  return std::transform(adl_begin(Range), adl_end(Range), d_first, F);\n}\n\n/// Provide wrappers to std::partition which take ranges instead of having to\n/// pass begin/end explicitly.\ntemplate <typename R, typename UnaryPredicate>\nauto partition(R &&Range, UnaryPredicate P) {\n  return std::partition(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Provide wrappers to std::lower_bound which take ranges instead of having to\n/// pass begin/end explicitly.\ntemplate <typename R, typename T> auto lower_bound(R &&Range, T &&Value) {\n  return std::lower_bound(adl_begin(Range), adl_end(Range),\n                          std::forward<T>(Value));\n}\n\ntemplate <typename R, typename T, typename Compare>\nauto lower_bound(R &&Range, T &&Value, Compare C) {\n  return std::lower_bound(adl_begin(Range), adl_end(Range),\n                          std::forward<T>(Value), C);\n}\n\n/// Provide wrappers to std::upper_bound which take ranges instead of having to\n/// pass begin/end explicitly.\ntemplate <typename R, typename T> auto upper_bound(R &&Range, T &&Value) {\n  return std::upper_bound(adl_begin(Range), adl_end(Range),\n                          std::forward<T>(Value));\n}\n\ntemplate <typename R, typename T, typename Compare>\nauto upper_bound(R &&Range, T &&Value, Compare C) {\n  return std::upper_bound(adl_begin(Range), adl_end(Range),\n                          std::forward<T>(Value), C);\n}\n\ntemplate <typename R>\nvoid stable_sort(R &&Range) {\n  std::stable_sort(adl_begin(Range), adl_end(Range));\n}\n\ntemplate <typename R, typename Compare>\nvoid stable_sort(R &&Range, Compare C) {\n  std::stable_sort(adl_begin(Range), adl_end(Range), C);\n}\n\n/// Binary search for the first iterator in a range where a predicate is false.\n/// Requires that C is always true below some limit, and always false above it.\ntemplate <typename R, typename Predicate,\n          typename Val = decltype(*adl_begin(std::declval<R>()))>\nauto partition_point(R &&Range, Predicate P) {\n  return std::partition_point(adl_begin(Range), adl_end(Range), P);\n}\n\n/// Wrapper function around std::equal to detect if all elements\n/// in a container are same.\ntemplate <typename R>\nbool is_splat(R &&Range) {\n  size_t range_size = size(Range);\n  return range_size != 0 && (range_size == 1 ||\n         std::equal(adl_begin(Range) + 1, adl_end(Range), adl_begin(Range)));\n}\n\n/// Provide a container algorithm similar to C++ Library Fundamentals v2's\n/// `erase_if` which is equivalent to:\n///\n///   C.erase(remove_if(C, pred), C.end());\n///\n/// This version works for any container with an erase method call accepting\n/// two iterators.\ntemplate <typename Container, typename UnaryPredicate>\nvoid erase_if(Container &C, UnaryPredicate P) {\n  C.erase(remove_if(C, P), C.end());\n}\n\n/// Wrapper function to remove a value from a container:\n///\n/// C.erase(remove(C.begin(), C.end(), V), C.end());\ntemplate <typename Container, typename ValueType>\nvoid erase_value(Container &C, ValueType V) {\n  C.erase(std::remove(C.begin(), C.end(), V), C.end());\n}\n\n/// Wrapper function to append a range to a container.\n///\n/// C.insert(C.end(), R.begin(), R.end());\ntemplate <typename Container, typename Range>\ninline void append_range(Container &C, Range &&R) {\n  C.insert(C.end(), R.begin(), R.end());\n}\n\n/// Given a sequence container Cont, replace the range [ContIt, ContEnd) with\n/// the range [ValIt, ValEnd) (which is not from the same container).\ntemplate<typename Container, typename RandomAccessIterator>\nvoid replace(Container &Cont, typename Container::iterator ContIt,\n             typename Container::iterator ContEnd, RandomAccessIterator ValIt,\n             RandomAccessIterator ValEnd) {\n  while (true) {\n    if (ValIt == ValEnd) {\n      Cont.erase(ContIt, ContEnd);\n      return;\n    } else if (ContIt == ContEnd) {\n      Cont.insert(ContIt, ValIt, ValEnd);\n      return;\n    }\n    *ContIt++ = *ValIt++;\n  }\n}\n\n/// Given a sequence container Cont, replace the range [ContIt, ContEnd) with\n/// the range R.\ntemplate<typename Container, typename Range = std::initializer_list<\n                                 typename Container::value_type>>\nvoid replace(Container &Cont, typename Container::iterator ContIt,\n             typename Container::iterator ContEnd, Range R) {\n  replace(Cont, ContIt, ContEnd, R.begin(), R.end());\n}\n\n/// An STL-style algorithm similar to std::for_each that applies a second\n/// functor between every pair of elements.\n///\n/// This provides the control flow logic to, for example, print a\n/// comma-separated list:\n/// \\code\n///   interleave(names.begin(), names.end(),\n///              [&](StringRef name) { os << name; },\n///              [&] { os << \", \"; });\n/// \\endcode\ntemplate <typename ForwardIterator, typename UnaryFunctor,\n          typename NullaryFunctor,\n          typename = typename std::enable_if<\n              !std::is_constructible<StringRef, UnaryFunctor>::value &&\n              !std::is_constructible<StringRef, NullaryFunctor>::value>::type>\ninline void interleave(ForwardIterator begin, ForwardIterator end,\n                       UnaryFunctor each_fn, NullaryFunctor between_fn) {\n  if (begin == end)\n    return;\n  each_fn(*begin);\n  ++begin;\n  for (; begin != end; ++begin) {\n    between_fn();\n    each_fn(*begin);\n  }\n}\n\ntemplate <typename Container, typename UnaryFunctor, typename NullaryFunctor,\n          typename = typename std::enable_if<\n              !std::is_constructible<StringRef, UnaryFunctor>::value &&\n              !std::is_constructible<StringRef, NullaryFunctor>::value>::type>\ninline void interleave(const Container &c, UnaryFunctor each_fn,\n                       NullaryFunctor between_fn) {\n  interleave(c.begin(), c.end(), each_fn, between_fn);\n}\n\n/// Overload of interleave for the common case of string separator.\ntemplate <typename Container, typename UnaryFunctor, typename StreamT,\n          typename T = detail::ValueOfRange<Container>>\ninline void interleave(const Container &c, StreamT &os, UnaryFunctor each_fn,\n                       const StringRef &separator) {\n  interleave(c.begin(), c.end(), each_fn, [&] { os << separator; });\n}\ntemplate <typename Container, typename StreamT,\n          typename T = detail::ValueOfRange<Container>>\ninline void interleave(const Container &c, StreamT &os,\n                       const StringRef &separator) {\n  interleave(\n      c, os, [&](const T &a) { os << a; }, separator);\n}\n\ntemplate <typename Container, typename UnaryFunctor, typename StreamT,\n          typename T = detail::ValueOfRange<Container>>\ninline void interleaveComma(const Container &c, StreamT &os,\n                            UnaryFunctor each_fn) {\n  interleave(c, os, each_fn, \", \");\n}\ntemplate <typename Container, typename StreamT,\n          typename T = detail::ValueOfRange<Container>>\ninline void interleaveComma(const Container &c, StreamT &os) {\n  interleaveComma(c, os, [&](const T &a) { os << a; });\n}\n\n//===----------------------------------------------------------------------===//\n//     Extra additions to <memory>\n//===----------------------------------------------------------------------===//\n\nstruct FreeDeleter {\n  void operator()(void* v) {\n    ::free(v);\n  }\n};\n\ntemplate<typename First, typename Second>\nstruct pair_hash {\n  size_t operator()(const std::pair<First, Second> &P) const {\n    return std::hash<First>()(P.first) * 31 + std::hash<Second>()(P.second);\n  }\n};\n\n/// Binary functor that adapts to any other binary functor after dereferencing\n/// operands.\ntemplate <typename T> struct deref {\n  T func;\n\n  // Could be further improved to cope with non-derivable functors and\n  // non-binary functors (should be a variadic template member function\n  // operator()).\n  template <typename A, typename B> auto operator()(A &lhs, B &rhs) const {\n    assert(lhs);\n    assert(rhs);\n    return func(*lhs, *rhs);\n  }\n};\n\nnamespace detail {\n\ntemplate <typename R> class enumerator_iter;\n\ntemplate <typename R> struct result_pair {\n  using value_reference =\n      typename std::iterator_traits<IterOfRange<R>>::reference;\n\n  friend class enumerator_iter<R>;\n\n  result_pair() = default;\n  result_pair(std::size_t Index, IterOfRange<R> Iter)\n      : Index(Index), Iter(Iter) {}\n\n  result_pair(const result_pair<R> &Other)\n      : Index(Other.Index), Iter(Other.Iter) {}\n  result_pair &operator=(const result_pair &Other) {\n    Index = Other.Index;\n    Iter = Other.Iter;\n    return *this;\n  }\n\n  std::size_t index() const { return Index; }\n  const value_reference value() const { return *Iter; }\n  value_reference value() { return *Iter; }\n\nprivate:\n  std::size_t Index = std::numeric_limits<std::size_t>::max();\n  IterOfRange<R> Iter;\n};\n\ntemplate <typename R>\nclass enumerator_iter\n    : public iterator_facade_base<\n          enumerator_iter<R>, std::forward_iterator_tag, result_pair<R>,\n          typename std::iterator_traits<IterOfRange<R>>::difference_type,\n          typename std::iterator_traits<IterOfRange<R>>::pointer,\n          typename std::iterator_traits<IterOfRange<R>>::reference> {\n  using result_type = result_pair<R>;\n\npublic:\n  explicit enumerator_iter(IterOfRange<R> EndIter)\n      : Result(std::numeric_limits<size_t>::max(), EndIter) {}\n\n  enumerator_iter(std::size_t Index, IterOfRange<R> Iter)\n      : Result(Index, Iter) {}\n\n  result_type &operator*() { return Result; }\n  const result_type &operator*() const { return Result; }\n\n  enumerator_iter &operator++() {\n    assert(Result.Index != std::numeric_limits<size_t>::max());\n    ++Result.Iter;\n    ++Result.Index;\n    return *this;\n  }\n\n  bool operator==(const enumerator_iter &RHS) const {\n    // Don't compare indices here, only iterators.  It's possible for an end\n    // iterator to have different indices depending on whether it was created\n    // by calling std::end() versus incrementing a valid iterator.\n    return Result.Iter == RHS.Result.Iter;\n  }\n\n  enumerator_iter(const enumerator_iter &Other) : Result(Other.Result) {}\n  enumerator_iter &operator=(const enumerator_iter &Other) {\n    Result = Other.Result;\n    return *this;\n  }\n\nprivate:\n  result_type Result;\n};\n\ntemplate <typename R> class enumerator {\npublic:\n  explicit enumerator(R &&Range) : TheRange(std::forward<R>(Range)) {}\n\n  enumerator_iter<R> begin() {\n    return enumerator_iter<R>(0, std::begin(TheRange));\n  }\n\n  enumerator_iter<R> end() {\n    return enumerator_iter<R>(std::end(TheRange));\n  }\n\nprivate:\n  R TheRange;\n};\n\n} // end namespace detail\n\n/// Given an input range, returns a new range whose values are are pair (A,B)\n/// such that A is the 0-based index of the item in the sequence, and B is\n/// the value from the original sequence.  Example:\n///\n/// std::vector<char> Items = {'A', 'B', 'C', 'D'};\n/// for (auto X : enumerate(Items)) {\n///   printf(\"Item %d - %c\\n\", X.index(), X.value());\n/// }\n///\n/// Output:\n///   Item 0 - A\n///   Item 1 - B\n///   Item 2 - C\n///   Item 3 - D\n///\ntemplate <typename R> detail::enumerator<R> enumerate(R &&TheRange) {\n  return detail::enumerator<R>(std::forward<R>(TheRange));\n}\n\nnamespace detail {\n\ntemplate <typename F, typename Tuple, std::size_t... I>\ndecltype(auto) apply_tuple_impl(F &&f, Tuple &&t, std::index_sequence<I...>) {\n  return std::forward<F>(f)(std::get<I>(std::forward<Tuple>(t))...);\n}\n\n} // end namespace detail\n\n/// Given an input tuple (a1, a2, ..., an), pass the arguments of the\n/// tuple variadically to f as if by calling f(a1, a2, ..., an) and\n/// return the result.\ntemplate <typename F, typename Tuple>\ndecltype(auto) apply_tuple(F &&f, Tuple &&t) {\n  using Indices = std::make_index_sequence<\n      std::tuple_size<typename std::decay<Tuple>::type>::value>;\n\n  return detail::apply_tuple_impl(std::forward<F>(f), std::forward<Tuple>(t),\n                                  Indices{});\n}\n\n/// Return true if the sequence [Begin, End) has exactly N items. Runs in O(N)\n/// time. Not meant for use with random-access iterators.\n/// Can optionally take a predicate to filter lazily some items.\ntemplate <typename IterTy,\n          typename Pred = bool (*)(const decltype(*std::declval<IterTy>()) &)>\nbool hasNItems(\n    IterTy &&Begin, IterTy &&End, unsigned N,\n    Pred &&ShouldBeCounted =\n        [](const decltype(*std::declval<IterTy>()) &) { return true; },\n    std::enable_if_t<\n        !std::is_base_of<std::random_access_iterator_tag,\n                         typename std::iterator_traits<std::remove_reference_t<\n                             decltype(Begin)>>::iterator_category>::value,\n        void> * = nullptr) {\n  for (; N; ++Begin) {\n    if (Begin == End)\n      return false; // Too few.\n    N -= ShouldBeCounted(*Begin);\n  }\n  for (; Begin != End; ++Begin)\n    if (ShouldBeCounted(*Begin))\n      return false; // Too many.\n  return true;\n}\n\n/// Return true if the sequence [Begin, End) has N or more items. Runs in O(N)\n/// time. Not meant for use with random-access iterators.\n/// Can optionally take a predicate to lazily filter some items.\ntemplate <typename IterTy,\n          typename Pred = bool (*)(const decltype(*std::declval<IterTy>()) &)>\nbool hasNItemsOrMore(\n    IterTy &&Begin, IterTy &&End, unsigned N,\n    Pred &&ShouldBeCounted =\n        [](const decltype(*std::declval<IterTy>()) &) { return true; },\n    std::enable_if_t<\n        !std::is_base_of<std::random_access_iterator_tag,\n                         typename std::iterator_traits<std::remove_reference_t<\n                             decltype(Begin)>>::iterator_category>::value,\n        void> * = nullptr) {\n  for (; N; ++Begin) {\n    if (Begin == End)\n      return false; // Too few.\n    N -= ShouldBeCounted(*Begin);\n  }\n  return true;\n}\n\n/// Returns true if the sequence [Begin, End) has N or less items. Can\n/// optionally take a predicate to lazily filter some items.\ntemplate <typename IterTy,\n          typename Pred = bool (*)(const decltype(*std::declval<IterTy>()) &)>\nbool hasNItemsOrLess(\n    IterTy &&Begin, IterTy &&End, unsigned N,\n    Pred &&ShouldBeCounted = [](const decltype(*std::declval<IterTy>()) &) {\n      return true;\n    }) {\n  assert(N != std::numeric_limits<unsigned>::max());\n  return !hasNItemsOrMore(Begin, End, N + 1, ShouldBeCounted);\n}\n\n/// Returns true if the given container has exactly N items\ntemplate <typename ContainerTy> bool hasNItems(ContainerTy &&C, unsigned N) {\n  return hasNItems(std::begin(C), std::end(C), N);\n}\n\n/// Returns true if the given container has N or more items\ntemplate <typename ContainerTy>\nbool hasNItemsOrMore(ContainerTy &&C, unsigned N) {\n  return hasNItemsOrMore(std::begin(C), std::end(C), N);\n}\n\n/// Returns true if the given container has N or less items\ntemplate <typename ContainerTy>\nbool hasNItemsOrLess(ContainerTy &&C, unsigned N) {\n  return hasNItemsOrLess(std::begin(C), std::end(C), N);\n}\n\n/// Returns a raw pointer that represents the same address as the argument.\n///\n/// This implementation can be removed once we move to C++20 where it's defined\n/// as std::to_address().\n///\n/// The std::pointer_traits<>::to_address(p) variations of these overloads has\n/// not been implemented.\ntemplate <class Ptr> auto to_address(const Ptr &P) { return P.operator->(); }\ntemplate <class T> constexpr T *to_address(T *P) { return P; }\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_STLEXTRAS_H\n"}, "66": {"id": 66, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist.h", "content": "//==-- llvm/ADT/ilist.h - Intrusive Linked List Template ---------*- C++ -*-==//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines classes to implement an intrusive doubly linked list class\n// (i.e. each node of the list must contain a next and previous field for the\n// list.\n//\n// The ilist class itself should be a plug in replacement for list.  This list\n// replacement does not provide a constant time size() method, so be careful to\n// use empty() when you really want to know if it's empty.\n//\n// The ilist class is implemented as a circular list.  The list itself contains\n// a sentinel node, whose Next points at begin() and whose Prev points at\n// rbegin().  The sentinel node itself serves as end() and rend().\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_ILIST_H\n#define LLVM_ADT_ILIST_H\n\n#include \"llvm/ADT/simple_ilist.h\"\n#include <cassert>\n#include <cstddef>\n#include <iterator>\n\nnamespace llvm {\n\n/// Use delete by default for iplist and ilist.\n///\n/// Specialize this to get different behaviour for ownership-related API.  (If\n/// you really want ownership semantics, consider using std::list or building\n/// something like \\a BumpPtrList.)\n///\n/// \\see ilist_noalloc_traits\ntemplate <typename NodeTy> struct ilist_alloc_traits {\n  static void deleteNode(NodeTy *V) { delete V; }\n};\n\n/// Custom traits to do nothing on deletion.\n///\n/// Specialize ilist_alloc_traits to inherit from this to disable the\n/// non-intrusive deletion in iplist (which implies ownership).\n///\n/// If you want purely intrusive semantics with no callbacks, consider using \\a\n/// simple_ilist instead.\n///\n/// \\code\n/// template <>\n/// struct ilist_alloc_traits<MyType> : ilist_noalloc_traits<MyType> {};\n/// \\endcode\ntemplate <typename NodeTy> struct ilist_noalloc_traits {\n  static void deleteNode(NodeTy *V) {}\n};\n\n/// Callbacks do nothing by default in iplist and ilist.\n///\n/// Specialize this for to use callbacks for when nodes change their list\n/// membership.\ntemplate <typename NodeTy> struct ilist_callback_traits {\n  void addNodeToList(NodeTy *) {}\n  void removeNodeFromList(NodeTy *) {}\n\n  /// Callback before transferring nodes to this list. The nodes may already be\n  /// in this same list.\n  template <class Iterator>\n  void transferNodesFromList(ilist_callback_traits &OldList, Iterator /*first*/,\n                             Iterator /*last*/) {\n    (void)OldList;\n  }\n};\n\n/// A fragment for template traits for intrusive list that provides default\n/// node related operations.\n///\n/// TODO: Remove this layer of indirection.  It's not necessary.\ntemplate <typename NodeTy>\nstruct ilist_node_traits : ilist_alloc_traits<NodeTy>,\n                           ilist_callback_traits<NodeTy> {};\n\n/// Template traits for intrusive list.\n///\n/// Customize callbacks and allocation semantics.\ntemplate <typename NodeTy>\nstruct ilist_traits : public ilist_node_traits<NodeTy> {};\n\n/// Const traits should never be instantiated.\ntemplate <typename Ty> struct ilist_traits<const Ty> {};\n\nnamespace ilist_detail {\n\ntemplate <class T> T &make();\n\n/// Type trait to check for a traits class that has a getNext member (as a\n/// canary for any of the ilist_nextprev_traits API).\ntemplate <class TraitsT, class NodeT> struct HasGetNext {\n  typedef char Yes[1];\n  typedef char No[2];\n  template <size_t N> struct SFINAE {};\n\n  template <class U>\n  static Yes &test(U *I, decltype(I->getNext(&make<NodeT>())) * = 0);\n  template <class> static No &test(...);\n\npublic:\n  static const bool value = sizeof(test<TraitsT>(nullptr)) == sizeof(Yes);\n};\n\n/// Type trait to check for a traits class that has a createSentinel member (as\n/// a canary for any of the ilist_sentinel_traits API).\ntemplate <class TraitsT> struct HasCreateSentinel {\n  typedef char Yes[1];\n  typedef char No[2];\n\n  template <class U>\n  static Yes &test(U *I, decltype(I->createSentinel()) * = 0);\n  template <class> static No &test(...);\n\npublic:\n  static const bool value = sizeof(test<TraitsT>(nullptr)) == sizeof(Yes);\n};\n\n/// Type trait to check for a traits class that has a createNode member.\n/// Allocation should be managed in a wrapper class, instead of in\n/// ilist_traits.\ntemplate <class TraitsT, class NodeT> struct HasCreateNode {\n  typedef char Yes[1];\n  typedef char No[2];\n  template <size_t N> struct SFINAE {};\n\n  template <class U>\n  static Yes &test(U *I, decltype(I->createNode(make<NodeT>())) * = 0);\n  template <class> static No &test(...);\n\npublic:\n  static const bool value = sizeof(test<TraitsT>(nullptr)) == sizeof(Yes);\n};\n\ntemplate <class TraitsT, class NodeT> struct HasObsoleteCustomization {\n  static const bool value = HasGetNext<TraitsT, NodeT>::value ||\n                            HasCreateSentinel<TraitsT>::value ||\n                            HasCreateNode<TraitsT, NodeT>::value;\n};\n\n} // end namespace ilist_detail\n\n//===----------------------------------------------------------------------===//\n//\n/// A wrapper around an intrusive list with callbacks and non-intrusive\n/// ownership.\n///\n/// This wraps a purely intrusive list (like simple_ilist) with a configurable\n/// traits class.  The traits can implement callbacks and customize the\n/// ownership semantics.\n///\n/// This is a subset of ilist functionality that can safely be used on nodes of\n/// polymorphic types, i.e. a heterogeneous list with a common base class that\n/// holds the next/prev pointers.  The only state of the list itself is an\n/// ilist_sentinel, which holds pointers to the first and last nodes in the\n/// list.\ntemplate <class IntrusiveListT, class TraitsT>\nclass iplist_impl : public TraitsT, IntrusiveListT {\n  typedef IntrusiveListT base_list_type;\n\npublic:\n  typedef typename base_list_type::pointer pointer;\n  typedef typename base_list_type::const_pointer const_pointer;\n  typedef typename base_list_type::reference reference;\n  typedef typename base_list_type::const_reference const_reference;\n  typedef typename base_list_type::value_type value_type;\n  typedef typename base_list_type::size_type size_type;\n  typedef typename base_list_type::difference_type difference_type;\n  typedef typename base_list_type::iterator iterator;\n  typedef typename base_list_type::const_iterator const_iterator;\n  typedef typename base_list_type::reverse_iterator reverse_iterator;\n  typedef\n      typename base_list_type::const_reverse_iterator const_reverse_iterator;\n\nprivate:\n  // TODO: Drop this assertion and the transitive type traits anytime after\n  // v4.0 is branched (i.e,. keep them for one release to help out-of-tree code\n  // update).\n  static_assert(\n      !ilist_detail::HasObsoleteCustomization<TraitsT, value_type>::value,\n      \"ilist customization points have changed!\");\n\n  static bool op_less(const_reference L, const_reference R) { return L < R; }\n  static bool op_equal(const_reference L, const_reference R) { return L == R; }\n\npublic:\n  iplist_impl() = default;\n\n  iplist_impl(const iplist_impl &) = delete;\n  iplist_impl &operator=(const iplist_impl &) = delete;\n\n  iplist_impl(iplist_impl &&X)\n      : TraitsT(std::move(static_cast<TraitsT &>(X))),\n        IntrusiveListT(std::move(static_cast<IntrusiveListT &>(X))) {}\n  iplist_impl &operator=(iplist_impl &&X) {\n    *static_cast<TraitsT *>(this) = std::move(static_cast<TraitsT &>(X));\n    *static_cast<IntrusiveListT *>(this) =\n        std::move(static_cast<IntrusiveListT &>(X));\n    return *this;\n  }\n\n  ~iplist_impl() { clear(); }\n\n  // Miscellaneous inspection routines.\n  size_type max_size() const { return size_type(-1); }\n\n  using base_list_type::begin;\n  using base_list_type::end;\n  using base_list_type::rbegin;\n  using base_list_type::rend;\n  using base_list_type::empty;\n  using base_list_type::front;\n  using base_list_type::back;\n\n  void swap(iplist_impl &RHS) {\n    assert(0 && \"Swap does not use list traits callback correctly yet!\");\n    base_list_type::swap(RHS);\n  }\n\n  iterator insert(iterator where, pointer New) {\n    this->addNodeToList(New); // Notify traits that we added a node...\n    return base_list_type::insert(where, *New);\n  }\n\n  iterator insert(iterator where, const_reference New) {\n    return this->insert(where, new value_type(New));\n  }\n\n  iterator insertAfter(iterator where, pointer New) {\n    if (empty())\n      return insert(begin(), New);\n    else\n      return insert(++where, New);\n  }\n\n  /// Clone another list.\n  template <class Cloner> void cloneFrom(const iplist_impl &L2, Cloner clone) {\n    clear();\n    for (const_reference V : L2)\n      push_back(clone(V));\n  }\n\n  pointer remove(iterator &IT) {\n    pointer Node = &*IT++;\n    this->removeNodeFromList(Node); // Notify traits that we removed a node...\n    base_list_type::remove(*Node);\n    return Node;\n  }\n\n  pointer remove(const iterator &IT) {\n    iterator MutIt = IT;\n    return remove(MutIt);\n  }\n\n  pointer remove(pointer IT) { return remove(iterator(IT)); }\n  pointer remove(reference IT) { return remove(iterator(IT)); }\n\n  // erase - remove a node from the controlled sequence... and delete it.\n  iterator erase(iterator where) {\n    this->deleteNode(remove(where));\n    return where;\n  }\n\n  iterator erase(pointer IT) { return erase(iterator(IT)); }\n  iterator erase(reference IT) { return erase(iterator(IT)); }\n\n  /// Remove all nodes from the list like clear(), but do not call\n  /// removeNodeFromList() or deleteNode().\n  ///\n  /// This should only be used immediately before freeing nodes in bulk to\n  /// avoid traversing the list and bringing all the nodes into cache.\n  void clearAndLeakNodesUnsafely() { base_list_type::clear(); }\n\nprivate:\n  // transfer - The heart of the splice function.  Move linked list nodes from\n  // [first, last) into position.\n  //\n  void transfer(iterator position, iplist_impl &L2, iterator first, iterator last) {\n    if (position == last)\n      return;\n\n    // Notify traits we moved the nodes...\n    this->transferNodesFromList(L2, first, last);\n\n    base_list_type::splice(position, L2, first, last);\n  }\n\npublic:\n  //===----------------------------------------------------------------------===\n  // Functionality derived from other functions defined above...\n  //\n\n  using base_list_type::size;\n\n  iterator erase(iterator first, iterator last) {\n    while (first != last)\n      first = erase(first);\n    return last;\n  }\n\n  void clear() { erase(begin(), end()); }\n\n  // Front and back inserters...\n  void push_front(pointer val) { insert(begin(), val); }\n  void push_back(pointer val) { insert(end(), val); }\n  void pop_front() {\n    assert(!empty() && \"pop_front() on empty list!\");\n    erase(begin());\n  }\n  void pop_back() {\n    assert(!empty() && \"pop_back() on empty list!\");\n    iterator t = end(); erase(--t);\n  }\n\n  // Special forms of insert...\n  template<class InIt> void insert(iterator where, InIt first, InIt last) {\n    for (; first != last; ++first) insert(where, *first);\n  }\n\n  // Splice members - defined in terms of transfer...\n  void splice(iterator where, iplist_impl &L2) {\n    if (!L2.empty())\n      transfer(where, L2, L2.begin(), L2.end());\n  }\n  void splice(iterator where, iplist_impl &L2, iterator first) {\n    iterator last = first; ++last;\n    if (where == first || where == last) return; // No change\n    transfer(where, L2, first, last);\n  }\n  void splice(iterator where, iplist_impl &L2, iterator first, iterator last) {\n    if (first != last) transfer(where, L2, first, last);\n  }\n  void splice(iterator where, iplist_impl &L2, reference N) {\n    splice(where, L2, iterator(N));\n  }\n  void splice(iterator where, iplist_impl &L2, pointer N) {\n    splice(where, L2, iterator(N));\n  }\n\n  template <class Compare>\n  void merge(iplist_impl &Right, Compare comp) {\n    if (this == &Right)\n      return;\n    this->transferNodesFromList(Right, Right.begin(), Right.end());\n    base_list_type::merge(Right, comp);\n  }\n  void merge(iplist_impl &Right) { return merge(Right, op_less); }\n\n  using base_list_type::sort;\n\n  /// Get the previous node, or \\c nullptr for the list head.\n  pointer getPrevNode(reference N) const {\n    auto I = N.getIterator();\n    if (I == begin())\n      return nullptr;\n    return &*std::prev(I);\n  }\n  /// Get the previous node, or \\c nullptr for the list head.\n  const_pointer getPrevNode(const_reference N) const {\n    return getPrevNode(const_cast<reference >(N));\n  }\n\n  /// Get the next node, or \\c nullptr for the list tail.\n  pointer getNextNode(reference N) const {\n    auto Next = std::next(N.getIterator());\n    if (Next == end())\n      return nullptr;\n    return &*Next;\n  }\n  /// Get the next node, or \\c nullptr for the list tail.\n  const_pointer getNextNode(const_reference N) const {\n    return getNextNode(const_cast<reference >(N));\n  }\n};\n\n/// An intrusive list with ownership and callbacks specified/controlled by\n/// ilist_traits, only with API safe for polymorphic types.\n///\n/// The \\p Options parameters are the same as those for \\a simple_ilist.  See\n/// there for a description of what's available.\ntemplate <class T, class... Options>\nclass iplist\n    : public iplist_impl<simple_ilist<T, Options...>, ilist_traits<T>> {\n  using iplist_impl_type = typename iplist::iplist_impl;\n\npublic:\n  iplist() = default;\n\n  iplist(const iplist &X) = delete;\n  iplist &operator=(const iplist &X) = delete;\n\n  iplist(iplist &&X) : iplist_impl_type(std::move(X)) {}\n  iplist &operator=(iplist &&X) {\n    *static_cast<iplist_impl_type *>(this) = std::move(X);\n    return *this;\n  }\n};\n\ntemplate <class T, class... Options> using ilist = iplist<T, Options...>;\n\n} // end namespace llvm\n\nnamespace std {\n\n  // Ensure that swap uses the fast list swap...\n  template<class Ty>\n  void swap(llvm::iplist<Ty> &Left, llvm::iplist<Ty> &Right) {\n    Left.swap(Right);\n  }\n\n} // end namespace std\n\n#endif // LLVM_ADT_ILIST_H\n"}, "67": {"id": 67, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_base.h", "content": "//===- llvm/ADT/ilist_base.h - Intrusive List Base --------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_ILIST_BASE_H\n#define LLVM_ADT_ILIST_BASE_H\n\n#include \"llvm/ADT/ilist_node_base.h\"\n#include <cassert>\n\nnamespace llvm {\n\n/// Implementations of list algorithms using ilist_node_base.\ntemplate <bool EnableSentinelTracking> class ilist_base {\npublic:\n  using node_base_type = ilist_node_base<EnableSentinelTracking>;\n\n  static void insertBeforeImpl(node_base_type &Next, node_base_type &N) {\n    node_base_type &Prev = *Next.getPrev();\n    N.setNext(&Next);\n    N.setPrev(&Prev);\n    Prev.setNext(&N);\n    Next.setPrev(&N);\n  }\n\n  static void removeImpl(node_base_type &N) {\n    node_base_type *Prev = N.getPrev();\n    node_base_type *Next = N.getNext();\n    Next->setPrev(Prev);\n    Prev->setNext(Next);\n\n    // Not strictly necessary, but helps catch a class of bugs.\n    N.setPrev(nullptr);\n    N.setNext(nullptr);\n  }\n\n  static void removeRangeImpl(node_base_type &First, node_base_type &Last) {\n    node_base_type *Prev = First.getPrev();\n    node_base_type *Final = Last.getPrev();\n    Last.setPrev(Prev);\n    Prev->setNext(&Last);\n\n    // Not strictly necessary, but helps catch a class of bugs.\n    First.setPrev(nullptr);\n    Final->setNext(nullptr);\n  }\n\n  static void transferBeforeImpl(node_base_type &Next, node_base_type &First,\n                                 node_base_type &Last) {\n    if (&Next == &Last || &First == &Last)\n      return;\n\n    // Position cannot be contained in the range to be transferred.\n    assert(&Next != &First &&\n           // Check for the most common mistake.\n           \"Insertion point can't be one of the transferred nodes\");\n\n    node_base_type &Final = *Last.getPrev();\n\n    // Detach from old list/position.\n    First.getPrev()->setNext(&Last);\n    Last.setPrev(First.getPrev());\n\n    // Splice [First, Final] into its new list/position.\n    node_base_type &Prev = *Next.getPrev();\n    Final.setNext(&Next);\n    First.setPrev(&Prev);\n    Prev.setNext(&First);\n    Next.setPrev(&Final);\n  }\n\n  template <class T> static void insertBefore(T &Next, T &N) {\n    insertBeforeImpl(Next, N);\n  }\n\n  template <class T> static void remove(T &N) { removeImpl(N); }\n  template <class T> static void removeRange(T &First, T &Last) {\n    removeRangeImpl(First, Last);\n  }\n\n  template <class T> static void transferBefore(T &Next, T &First, T &Last) {\n    transferBeforeImpl(Next, First, Last);\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_ILIST_BASE_H\n"}, "68": {"id": 68, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "content": "//===- llvm/ADT/ilist_node.h - Intrusive Linked List Helper -----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the ilist_node class template, which is a convenient\n// base class for creating classes that can be used with ilists.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_ILIST_NODE_H\n#define LLVM_ADT_ILIST_NODE_H\n\n#include \"llvm/ADT/ilist_node_base.h\"\n#include \"llvm/ADT/ilist_node_options.h\"\n\nnamespace llvm {\n\nnamespace ilist_detail {\n\nstruct NodeAccess;\n\n} // end namespace ilist_detail\n\ntemplate <class OptionsT, bool IsReverse, bool IsConst> class ilist_iterator;\ntemplate <class OptionsT> class ilist_sentinel;\n\n/// Implementation for an ilist node.\n///\n/// Templated on an appropriate \\a ilist_detail::node_options, usually computed\n/// by \\a ilist_detail::compute_node_options.\n///\n/// This is a wrapper around \\a ilist_node_base whose main purpose is to\n/// provide type safety: you can't insert nodes of \\a ilist_node_impl into the\n/// wrong \\a simple_ilist or \\a iplist.\ntemplate <class OptionsT> class ilist_node_impl : OptionsT::node_base_type {\n  using value_type = typename OptionsT::value_type;\n  using node_base_type = typename OptionsT::node_base_type;\n  using list_base_type = typename OptionsT::list_base_type;\n\n  friend typename OptionsT::list_base_type;\n  friend struct ilist_detail::NodeAccess;\n  friend class ilist_sentinel<OptionsT>;\n  friend class ilist_iterator<OptionsT, false, false>;\n  friend class ilist_iterator<OptionsT, false, true>;\n  friend class ilist_iterator<OptionsT, true, false>;\n  friend class ilist_iterator<OptionsT, true, true>;\n\nprotected:\n  using self_iterator = ilist_iterator<OptionsT, false, false>;\n  using const_self_iterator = ilist_iterator<OptionsT, false, true>;\n  using reverse_self_iterator = ilist_iterator<OptionsT, true, false>;\n  using const_reverse_self_iterator = ilist_iterator<OptionsT, true, true>;\n\n  ilist_node_impl() = default;\n\nprivate:\n  ilist_node_impl *getPrev() {\n    return static_cast<ilist_node_impl *>(node_base_type::getPrev());\n  }\n\n  ilist_node_impl *getNext() {\n    return static_cast<ilist_node_impl *>(node_base_type::getNext());\n  }\n\n  const ilist_node_impl *getPrev() const {\n    return static_cast<ilist_node_impl *>(node_base_type::getPrev());\n  }\n\n  const ilist_node_impl *getNext() const {\n    return static_cast<ilist_node_impl *>(node_base_type::getNext());\n  }\n\n  void setPrev(ilist_node_impl *N) { node_base_type::setPrev(N); }\n  void setNext(ilist_node_impl *N) { node_base_type::setNext(N); }\n\npublic:\n  self_iterator getIterator() { return self_iterator(*this); }\n  const_self_iterator getIterator() const { return const_self_iterator(*this); }\n\n  reverse_self_iterator getReverseIterator() {\n    return reverse_self_iterator(*this);\n  }\n\n  const_reverse_self_iterator getReverseIterator() const {\n    return const_reverse_self_iterator(*this);\n  }\n\n  // Under-approximation, but always available for assertions.\n  using node_base_type::isKnownSentinel;\n\n  /// Check whether this is the sentinel node.\n  ///\n  /// This requires sentinel tracking to be explicitly enabled.  Use the\n  /// ilist_sentinel_tracking<true> option to get this API.\n  bool isSentinel() const {\n    static_assert(OptionsT::is_sentinel_tracking_explicit,\n                  \"Use ilist_sentinel_tracking<true> to enable isSentinel()\");\n    return node_base_type::isSentinel();\n  }\n};\n\n/// An intrusive list node.\n///\n/// A base class to enable membership in intrusive lists, including \\a\n/// simple_ilist, \\a iplist, and \\a ilist.  The first template parameter is the\n/// \\a value_type for the list.\n///\n/// An ilist node can be configured with compile-time options to change\n/// behaviour and/or add API.\n///\n/// By default, an \\a ilist_node knows whether it is the list sentinel (an\n/// instance of \\a ilist_sentinel) if and only if\n/// LLVM_ENABLE_ABI_BREAKING_CHECKS.  The function \\a isKnownSentinel() always\n/// returns \\c false tracking is off.  Sentinel tracking steals a bit from the\n/// \"prev\" link, which adds a mask operation when decrementing an iterator, but\n/// enables bug-finding assertions in \\a ilist_iterator.\n///\n/// To turn sentinel tracking on all the time, pass in the\n/// ilist_sentinel_tracking<true> template parameter.  This also enables the \\a\n/// isSentinel() function.  The same option must be passed to the intrusive\n/// list.  (ilist_sentinel_tracking<false> turns sentinel tracking off all the\n/// time.)\n///\n/// A type can inherit from ilist_node multiple times by passing in different\n/// \\a ilist_tag options.  This allows a single instance to be inserted into\n/// multiple lists simultaneously, where each list is given the same tag.\n///\n/// \\example\n/// struct A {};\n/// struct B {};\n/// struct N : ilist_node<N, ilist_tag<A>>, ilist_node<N, ilist_tag<B>> {};\n///\n/// void foo() {\n///   simple_ilist<N, ilist_tag<A>> ListA;\n///   simple_ilist<N, ilist_tag<B>> ListB;\n///   N N1;\n///   ListA.push_back(N1);\n///   ListB.push_back(N1);\n/// }\n/// \\endexample\n///\n/// See \\a is_valid_option for steps on adding a new option.\ntemplate <class T, class... Options>\nclass ilist_node\n    : public ilist_node_impl<\n          typename ilist_detail::compute_node_options<T, Options...>::type> {\n  static_assert(ilist_detail::check_options<Options...>::value,\n                \"Unrecognized node option!\");\n};\n\nnamespace ilist_detail {\n\n/// An access class for ilist_node private API.\n///\n/// This gives access to the private parts of ilist nodes.  Nodes for an ilist\n/// should friend this class if they inherit privately from ilist_node.\n///\n/// Using this class outside of the ilist implementation is unsupported.\nstruct NodeAccess {\nprotected:\n  template <class OptionsT>\n  static ilist_node_impl<OptionsT> *getNodePtr(typename OptionsT::pointer N) {\n    return N;\n  }\n\n  template <class OptionsT>\n  static const ilist_node_impl<OptionsT> *\n  getNodePtr(typename OptionsT::const_pointer N) {\n    return N;\n  }\n\n  template <class OptionsT>\n  static typename OptionsT::pointer getValuePtr(ilist_node_impl<OptionsT> *N) {\n    return static_cast<typename OptionsT::pointer>(N);\n  }\n\n  template <class OptionsT>\n  static typename OptionsT::const_pointer\n  getValuePtr(const ilist_node_impl<OptionsT> *N) {\n    return static_cast<typename OptionsT::const_pointer>(N);\n  }\n\n  template <class OptionsT>\n  static ilist_node_impl<OptionsT> *getPrev(ilist_node_impl<OptionsT> &N) {\n    return N.getPrev();\n  }\n\n  template <class OptionsT>\n  static ilist_node_impl<OptionsT> *getNext(ilist_node_impl<OptionsT> &N) {\n    return N.getNext();\n  }\n\n  template <class OptionsT>\n  static const ilist_node_impl<OptionsT> *\n  getPrev(const ilist_node_impl<OptionsT> &N) {\n    return N.getPrev();\n  }\n\n  template <class OptionsT>\n  static const ilist_node_impl<OptionsT> *\n  getNext(const ilist_node_impl<OptionsT> &N) {\n    return N.getNext();\n  }\n};\n\ntemplate <class OptionsT> struct SpecificNodeAccess : NodeAccess {\nprotected:\n  using pointer = typename OptionsT::pointer;\n  using const_pointer = typename OptionsT::const_pointer;\n  using node_type = ilist_node_impl<OptionsT>;\n\n  static node_type *getNodePtr(pointer N) {\n    return NodeAccess::getNodePtr<OptionsT>(N);\n  }\n\n  static const node_type *getNodePtr(const_pointer N) {\n    return NodeAccess::getNodePtr<OptionsT>(N);\n  }\n\n  static pointer getValuePtr(node_type *N) {\n    return NodeAccess::getValuePtr<OptionsT>(N);\n  }\n\n  static const_pointer getValuePtr(const node_type *N) {\n    return NodeAccess::getValuePtr<OptionsT>(N);\n  }\n};\n\n} // end namespace ilist_detail\n\ntemplate <class OptionsT>\nclass ilist_sentinel : public ilist_node_impl<OptionsT> {\npublic:\n  ilist_sentinel() {\n    this->initializeSentinel();\n    reset();\n  }\n\n  void reset() {\n    this->setPrev(this);\n    this->setNext(this);\n  }\n\n  bool empty() const { return this == this->getPrev(); }\n};\n\n/// An ilist node that can access its parent list.\n///\n/// Requires \\c NodeTy to have \\a getParent() to find the parent node, and the\n/// \\c ParentTy to have \\a getSublistAccess() to get a reference to the list.\ntemplate <typename NodeTy, typename ParentTy, class... Options>\nclass ilist_node_with_parent : public ilist_node<NodeTy, Options...> {\nprotected:\n  ilist_node_with_parent() = default;\n\nprivate:\n  /// Forward to NodeTy::getParent().\n  ///\n  /// Note: do not use the name \"getParent()\".  We want a compile error\n  /// (instead of recursion) when the subclass fails to implement \\a\n  /// getParent().\n  const ParentTy *getNodeParent() const {\n    return static_cast<const NodeTy *>(this)->getParent();\n  }\n\npublic:\n  /// @name Adjacent Node Accessors\n  /// @{\n  /// Get the previous node, or \\c nullptr for the list head.\n  NodeTy *getPrevNode() {\n    // Should be separated to a reused function, but then we couldn't use auto\n    // (and would need the type of the list).\n    const auto &List =\n        getNodeParent()->*(ParentTy::getSublistAccess((NodeTy *)nullptr));\n    return List.getPrevNode(*static_cast<NodeTy *>(this));\n  }\n\n  /// Get the previous node, or \\c nullptr for the list head.\n  const NodeTy *getPrevNode() const {\n    return const_cast<ilist_node_with_parent *>(this)->getPrevNode();\n  }\n\n  /// Get the next node, or \\c nullptr for the list tail.\n  NodeTy *getNextNode() {\n    // Should be separated to a reused function, but then we couldn't use auto\n    // (and would need the type of the list).\n    const auto &List =\n        getNodeParent()->*(ParentTy::getSublistAccess((NodeTy *)nullptr));\n    return List.getNextNode(*static_cast<NodeTy *>(this));\n  }\n\n  /// Get the next node, or \\c nullptr for the list tail.\n  const NodeTy *getNextNode() const {\n    return const_cast<ilist_node_with_parent *>(this)->getNextNode();\n  }\n  /// @}\n};\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_ILIST_NODE_H\n"}, "70": {"id": 70, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/iterator.h", "content": "//===- iterator.h - Utilities for using and defining iterators --*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_ITERATOR_H\n#define LLVM_ADT_ITERATOR_H\n\n#include \"llvm/ADT/iterator_range.h\"\n#include <algorithm>\n#include <cstddef>\n#include <iterator>\n#include <type_traits>\n#include <utility>\n\nnamespace llvm {\n\n/// CRTP base class which implements the entire standard iterator facade\n/// in terms of a minimal subset of the interface.\n///\n/// Use this when it is reasonable to implement most of the iterator\n/// functionality in terms of a core subset. If you need special behavior or\n/// there are performance implications for this, you may want to override the\n/// relevant members instead.\n///\n/// Note, one abstraction that this does *not* provide is implementing\n/// subtraction in terms of addition by negating the difference. Negation isn't\n/// always information preserving, and I can see very reasonable iterator\n/// designs where this doesn't work well. It doesn't really force much added\n/// boilerplate anyways.\n///\n/// Another abstraction that this doesn't provide is implementing increment in\n/// terms of addition of one. These aren't equivalent for all iterator\n/// categories, and respecting that adds a lot of complexity for little gain.\n///\n/// Classes wishing to use `iterator_facade_base` should implement the following\n/// methods:\n///\n/// Forward Iterators:\n///   (All of the following methods)\n///   - DerivedT &operator=(const DerivedT &R);\n///   - bool operator==(const DerivedT &R) const;\n///   - const T &operator*() const;\n///   - T &operator*();\n///   - DerivedT &operator++();\n///\n/// Bidirectional Iterators:\n///   (All methods of forward iterators, plus the following)\n///   - DerivedT &operator--();\n///\n/// Random-access Iterators:\n///   (All methods of bidirectional iterators excluding the following)\n///   - DerivedT &operator++();\n///   - DerivedT &operator--();\n///   (and plus the following)\n///   - bool operator<(const DerivedT &RHS) const;\n///   - DifferenceTypeT operator-(const DerivedT &R) const;\n///   - DerivedT &operator+=(DifferenceTypeT N);\n///   - DerivedT &operator-=(DifferenceTypeT N);\n///\ntemplate <typename DerivedT, typename IteratorCategoryT, typename T,\n          typename DifferenceTypeT = std::ptrdiff_t, typename PointerT = T *,\n          typename ReferenceT = T &>\nclass iterator_facade_base\n    : public std::iterator<IteratorCategoryT, T, DifferenceTypeT, PointerT,\n                           ReferenceT> {\nprotected:\n  enum {\n    IsRandomAccess = std::is_base_of<std::random_access_iterator_tag,\n                                     IteratorCategoryT>::value,\n    IsBidirectional = std::is_base_of<std::bidirectional_iterator_tag,\n                                      IteratorCategoryT>::value,\n  };\n\n  /// A proxy object for computing a reference via indirecting a copy of an\n  /// iterator. This is used in APIs which need to produce a reference via\n  /// indirection but for which the iterator object might be a temporary. The\n  /// proxy preserves the iterator internally and exposes the indirected\n  /// reference via a conversion operator.\n  class ReferenceProxy {\n    friend iterator_facade_base;\n\n    DerivedT I;\n\n    ReferenceProxy(DerivedT I) : I(std::move(I)) {}\n\n  public:\n    operator ReferenceT() const { return *I; }\n  };\n\npublic:\n  DerivedT operator+(DifferenceTypeT n) const {\n    static_assert(std::is_base_of<iterator_facade_base, DerivedT>::value,\n                  \"Must pass the derived type to this template!\");\n    static_assert(\n        IsRandomAccess,\n        \"The '+' operator is only defined for random access iterators.\");\n    DerivedT tmp = *static_cast<const DerivedT *>(this);\n    tmp += n;\n    return tmp;\n  }\n  friend DerivedT operator+(DifferenceTypeT n, const DerivedT &i) {\n    static_assert(\n        IsRandomAccess,\n        \"The '+' operator is only defined for random access iterators.\");\n    return i + n;\n  }\n  DerivedT operator-(DifferenceTypeT n) const {\n    static_assert(\n        IsRandomAccess,\n        \"The '-' operator is only defined for random access iterators.\");\n    DerivedT tmp = *static_cast<const DerivedT *>(this);\n    tmp -= n;\n    return tmp;\n  }\n\n  DerivedT &operator++() {\n    static_assert(std::is_base_of<iterator_facade_base, DerivedT>::value,\n                  \"Must pass the derived type to this template!\");\n    return static_cast<DerivedT *>(this)->operator+=(1);\n  }\n  DerivedT operator++(int) {\n    DerivedT tmp = *static_cast<DerivedT *>(this);\n    ++*static_cast<DerivedT *>(this);\n    return tmp;\n  }\n  DerivedT &operator--() {\n    static_assert(\n        IsBidirectional,\n        \"The decrement operator is only defined for bidirectional iterators.\");\n    return static_cast<DerivedT *>(this)->operator-=(1);\n  }\n  DerivedT operator--(int) {\n    static_assert(\n        IsBidirectional,\n        \"The decrement operator is only defined for bidirectional iterators.\");\n    DerivedT tmp = *static_cast<DerivedT *>(this);\n    --*static_cast<DerivedT *>(this);\n    return tmp;\n  }\n\n#ifndef __cpp_impl_three_way_comparison\n  bool operator!=(const DerivedT &RHS) const {\n    return !(static_cast<const DerivedT &>(*this) == RHS);\n  }\n#endif\n\n  bool operator>(const DerivedT &RHS) const {\n    static_assert(\n        IsRandomAccess,\n        \"Relational operators are only defined for random access iterators.\");\n    return !(static_cast<const DerivedT &>(*this) < RHS) &&\n           !(static_cast<const DerivedT &>(*this) == RHS);\n  }\n  bool operator<=(const DerivedT &RHS) const {\n    static_assert(\n        IsRandomAccess,\n        \"Relational operators are only defined for random access iterators.\");\n    return !(static_cast<const DerivedT &>(*this) > RHS);\n  }\n  bool operator>=(const DerivedT &RHS) const {\n    static_assert(\n        IsRandomAccess,\n        \"Relational operators are only defined for random access iterators.\");\n    return !(static_cast<const DerivedT &>(*this) < RHS);\n  }\n\n  PointerT operator->() { return &static_cast<DerivedT *>(this)->operator*(); }\n  PointerT operator->() const {\n    return &static_cast<const DerivedT *>(this)->operator*();\n  }\n  ReferenceProxy operator[](DifferenceTypeT n) {\n    static_assert(IsRandomAccess,\n                  \"Subscripting is only defined for random access iterators.\");\n    return ReferenceProxy(static_cast<DerivedT *>(this)->operator+(n));\n  }\n  ReferenceProxy operator[](DifferenceTypeT n) const {\n    static_assert(IsRandomAccess,\n                  \"Subscripting is only defined for random access iterators.\");\n    return ReferenceProxy(static_cast<const DerivedT *>(this)->operator+(n));\n  }\n};\n\n/// CRTP base class for adapting an iterator to a different type.\n///\n/// This class can be used through CRTP to adapt one iterator into another.\n/// Typically this is done through providing in the derived class a custom \\c\n/// operator* implementation. Other methods can be overridden as well.\ntemplate <\n    typename DerivedT, typename WrappedIteratorT,\n    typename IteratorCategoryT =\n        typename std::iterator_traits<WrappedIteratorT>::iterator_category,\n    typename T = typename std::iterator_traits<WrappedIteratorT>::value_type,\n    typename DifferenceTypeT =\n        typename std::iterator_traits<WrappedIteratorT>::difference_type,\n    typename PointerT = std::conditional_t<\n        std::is_same<T, typename std::iterator_traits<\n                            WrappedIteratorT>::value_type>::value,\n        typename std::iterator_traits<WrappedIteratorT>::pointer, T *>,\n    typename ReferenceT = std::conditional_t<\n        std::is_same<T, typename std::iterator_traits<\n                            WrappedIteratorT>::value_type>::value,\n        typename std::iterator_traits<WrappedIteratorT>::reference, T &>>\nclass iterator_adaptor_base\n    : public iterator_facade_base<DerivedT, IteratorCategoryT, T,\n                                  DifferenceTypeT, PointerT, ReferenceT> {\n  using BaseT = typename iterator_adaptor_base::iterator_facade_base;\n\nprotected:\n  WrappedIteratorT I;\n\n  iterator_adaptor_base() = default;\n\n  explicit iterator_adaptor_base(WrappedIteratorT u) : I(std::move(u)) {\n    static_assert(std::is_base_of<iterator_adaptor_base, DerivedT>::value,\n                  \"Must pass the derived type to this template!\");\n  }\n\n  const WrappedIteratorT &wrapped() const { return I; }\n\npublic:\n  using difference_type = DifferenceTypeT;\n\n  DerivedT &operator+=(difference_type n) {\n    static_assert(\n        BaseT::IsRandomAccess,\n        \"The '+=' operator is only defined for random access iterators.\");\n    I += n;\n    return *static_cast<DerivedT *>(this);\n  }\n  DerivedT &operator-=(difference_type n) {\n    static_assert(\n        BaseT::IsRandomAccess,\n        \"The '-=' operator is only defined for random access iterators.\");\n    I -= n;\n    return *static_cast<DerivedT *>(this);\n  }\n  using BaseT::operator-;\n  difference_type operator-(const DerivedT &RHS) const {\n    static_assert(\n        BaseT::IsRandomAccess,\n        \"The '-' operator is only defined for random access iterators.\");\n    return I - RHS.I;\n  }\n\n  // We have to explicitly provide ++ and -- rather than letting the facade\n  // forward to += because WrappedIteratorT might not support +=.\n  using BaseT::operator++;\n  DerivedT &operator++() {\n    ++I;\n    return *static_cast<DerivedT *>(this);\n  }\n  using BaseT::operator--;\n  DerivedT &operator--() {\n    static_assert(\n        BaseT::IsBidirectional,\n        \"The decrement operator is only defined for bidirectional iterators.\");\n    --I;\n    return *static_cast<DerivedT *>(this);\n  }\n\n  friend bool operator==(const iterator_adaptor_base &LHS,\n                         const iterator_adaptor_base &RHS) {\n    return LHS.I == RHS.I;\n  }\n  friend bool operator<(const iterator_adaptor_base &LHS,\n                        const iterator_adaptor_base &RHS) {\n    static_assert(\n        BaseT::IsRandomAccess,\n        \"Relational operators are only defined for random access iterators.\");\n    return LHS.I < RHS.I;\n  }\n\n  ReferenceT operator*() const { return *I; }\n};\n\n/// An iterator type that allows iterating over the pointees via some\n/// other iterator.\n///\n/// The typical usage of this is to expose a type that iterates over Ts, but\n/// which is implemented with some iterator over T*s:\n///\n/// \\code\n///   using iterator = pointee_iterator<SmallVectorImpl<T *>::iterator>;\n/// \\endcode\ntemplate <typename WrappedIteratorT,\n          typename T = std::remove_reference_t<decltype(\n              **std::declval<WrappedIteratorT>())>>\nstruct pointee_iterator\n    : iterator_adaptor_base<\n          pointee_iterator<WrappedIteratorT, T>, WrappedIteratorT,\n          typename std::iterator_traits<WrappedIteratorT>::iterator_category,\n          T> {\n  pointee_iterator() = default;\n  template <typename U>\n  pointee_iterator(U &&u)\n      : pointee_iterator::iterator_adaptor_base(std::forward<U &&>(u)) {}\n\n  T &operator*() const { return **this->I; }\n};\n\ntemplate <typename RangeT, typename WrappedIteratorT =\n                               decltype(std::begin(std::declval<RangeT>()))>\niterator_range<pointee_iterator<WrappedIteratorT>>\nmake_pointee_range(RangeT &&Range) {\n  using PointeeIteratorT = pointee_iterator<WrappedIteratorT>;\n  return make_range(PointeeIteratorT(std::begin(std::forward<RangeT>(Range))),\n                    PointeeIteratorT(std::end(std::forward<RangeT>(Range))));\n}\n\ntemplate <typename WrappedIteratorT,\n          typename T = decltype(&*std::declval<WrappedIteratorT>())>\nclass pointer_iterator\n    : public iterator_adaptor_base<\n          pointer_iterator<WrappedIteratorT, T>, WrappedIteratorT,\n          typename std::iterator_traits<WrappedIteratorT>::iterator_category,\n          T> {\n  mutable T Ptr;\n\npublic:\n  pointer_iterator() = default;\n\n  explicit pointer_iterator(WrappedIteratorT u)\n      : pointer_iterator::iterator_adaptor_base(std::move(u)) {}\n\n  T &operator*() { return Ptr = &*this->I; }\n  const T &operator*() const { return Ptr = &*this->I; }\n};\n\ntemplate <typename RangeT, typename WrappedIteratorT =\n                               decltype(std::begin(std::declval<RangeT>()))>\niterator_range<pointer_iterator<WrappedIteratorT>>\nmake_pointer_range(RangeT &&Range) {\n  using PointerIteratorT = pointer_iterator<WrappedIteratorT>;\n  return make_range(PointerIteratorT(std::begin(std::forward<RangeT>(Range))),\n                    PointerIteratorT(std::end(std::forward<RangeT>(Range))));\n}\n\ntemplate <typename WrappedIteratorT,\n          typename T1 = std::remove_reference_t<decltype(\n              **std::declval<WrappedIteratorT>())>,\n          typename T2 = std::add_pointer_t<T1>>\nusing raw_pointer_iterator =\n    pointer_iterator<pointee_iterator<WrappedIteratorT, T1>, T2>;\n\n// Wrapper iterator over iterator ItType, adding DataRef to the type of ItType,\n// to create NodeRef = std::pair<InnerTypeOfItType, DataRef>.\ntemplate <typename ItType, typename NodeRef, typename DataRef>\nclass WrappedPairNodeDataIterator\n    : public iterator_adaptor_base<\n          WrappedPairNodeDataIterator<ItType, NodeRef, DataRef>, ItType,\n          typename std::iterator_traits<ItType>::iterator_category, NodeRef,\n          std::ptrdiff_t, NodeRef *, NodeRef &> {\n  using BaseT = iterator_adaptor_base<\n      WrappedPairNodeDataIterator, ItType,\n      typename std::iterator_traits<ItType>::iterator_category, NodeRef,\n      std::ptrdiff_t, NodeRef *, NodeRef &>;\n\n  const DataRef DR;\n  mutable NodeRef NR;\n\npublic:\n  WrappedPairNodeDataIterator(ItType Begin, const DataRef DR)\n      : BaseT(Begin), DR(DR) {\n    NR.first = DR;\n  }\n\n  NodeRef &operator*() const {\n    NR.second = *this->I;\n    return NR;\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_ITERATOR_H\n"}, "71": {"id": 71, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/simple_ilist.h", "content": "//===- llvm/ADT/simple_ilist.h - Simple Intrusive List ----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ADT_SIMPLE_ILIST_H\n#define LLVM_ADT_SIMPLE_ILIST_H\n\n#include \"llvm/ADT/ilist_base.h\"\n#include \"llvm/ADT/ilist_iterator.h\"\n#include \"llvm/ADT/ilist_node.h\"\n#include \"llvm/ADT/ilist_node_options.h\"\n#include \"llvm/Support/Compiler.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <functional>\n#include <iterator>\n#include <utility>\n\nnamespace llvm {\n\n/// A simple intrusive list implementation.\n///\n/// This is a simple intrusive list for a \\c T that inherits from \\c\n/// ilist_node<T>.  The list never takes ownership of anything inserted in it.\n///\n/// Unlike \\a iplist<T> and \\a ilist<T>, \\a simple_ilist<T> never deletes\n/// values, and has no callback traits.\n///\n/// The API for adding nodes include \\a push_front(), \\a push_back(), and \\a\n/// insert().  These all take values by reference (not by pointer), except for\n/// the range version of \\a insert().\n///\n/// There are three sets of API for discarding nodes from the list: \\a\n/// remove(), which takes a reference to the node to remove, \\a erase(), which\n/// takes an iterator or iterator range and returns the next one, and \\a\n/// clear(), which empties out the container.  All three are constant time\n/// operations.  None of these deletes any nodes; in particular, if there is a\n/// single node in the list, then these have identical semantics:\n/// \\li \\c L.remove(L.front());\n/// \\li \\c L.erase(L.begin());\n/// \\li \\c L.clear();\n///\n/// As a convenience for callers, there are parallel APIs that take a \\c\n/// Disposer (such as \\c std::default_delete<T>): \\a removeAndDispose(), \\a\n/// eraseAndDispose(), and \\a clearAndDispose().  These have different names\n/// because the extra semantic is otherwise non-obvious.  They are equivalent\n/// to calling \\a std::for_each() on the range to be discarded.\n///\n/// The currently available \\p Options customize the nodes in the list.  The\n/// same options must be specified in the \\a ilist_node instantiation for\n/// compatibility (although the order is irrelevant).\n/// \\li Use \\a ilist_tag to designate which ilist_node for a given \\p T this\n/// list should use.  This is useful if a type \\p T is part of multiple,\n/// independent lists simultaneously.\n/// \\li Use \\a ilist_sentinel_tracking to always (or never) track whether a\n/// node is a sentinel.  Specifying \\c true enables the \\a\n/// ilist_node::isSentinel() API.  Unlike \\a ilist_node::isKnownSentinel(),\n/// which is only appropriate for assertions, \\a ilist_node::isSentinel() is\n/// appropriate for real logic.\n///\n/// Here are examples of \\p Options usage:\n/// \\li \\c simple_ilist<T> gives the defaults.  \\li \\c\n/// simple_ilist<T,ilist_sentinel_tracking<true>> enables the \\a\n/// ilist_node::isSentinel() API.\n/// \\li \\c simple_ilist<T,ilist_tag<A>,ilist_sentinel_tracking<false>>\n/// specifies a tag of A and that tracking should be off (even when\n/// LLVM_ENABLE_ABI_BREAKING_CHECKS are enabled).\n/// \\li \\c simple_ilist<T,ilist_sentinel_tracking<false>,ilist_tag<A>> is\n/// equivalent to the last.\n///\n/// See \\a is_valid_option for steps on adding a new option.\ntemplate <typename T, class... Options>\nclass simple_ilist\n    : ilist_detail::compute_node_options<T, Options...>::type::list_base_type,\n      ilist_detail::SpecificNodeAccess<\n          typename ilist_detail::compute_node_options<T, Options...>::type> {\n  static_assert(ilist_detail::check_options<Options...>::value,\n                \"Unrecognized node option!\");\n  using OptionsT =\n      typename ilist_detail::compute_node_options<T, Options...>::type;\n  using list_base_type = typename OptionsT::list_base_type;\n  ilist_sentinel<OptionsT> Sentinel;\n\npublic:\n  using value_type = typename OptionsT::value_type;\n  using pointer = typename OptionsT::pointer;\n  using reference = typename OptionsT::reference;\n  using const_pointer = typename OptionsT::const_pointer;\n  using const_reference = typename OptionsT::const_reference;\n  using iterator = ilist_iterator<OptionsT, false, false>;\n  using const_iterator = ilist_iterator<OptionsT, false, true>;\n  using reverse_iterator = ilist_iterator<OptionsT, true, false>;\n  using const_reverse_iterator = ilist_iterator<OptionsT, true, true>;\n  using size_type = size_t;\n  using difference_type = ptrdiff_t;\n\n  simple_ilist() = default;\n  ~simple_ilist() = default;\n\n  // No copy constructors.\n  simple_ilist(const simple_ilist &) = delete;\n  simple_ilist &operator=(const simple_ilist &) = delete;\n\n  // Move constructors.\n  simple_ilist(simple_ilist &&X) { splice(end(), X); }\n  simple_ilist &operator=(simple_ilist &&X) {\n    clear();\n    splice(end(), X);\n    return *this;\n  }\n\n  iterator begin() { return ++iterator(Sentinel); }\n  const_iterator begin() const { return ++const_iterator(Sentinel); }\n  iterator end() { return iterator(Sentinel); }\n  const_iterator end() const { return const_iterator(Sentinel); }\n  reverse_iterator rbegin() { return ++reverse_iterator(Sentinel); }\n  const_reverse_iterator rbegin() const {\n    return ++const_reverse_iterator(Sentinel);\n  }\n  reverse_iterator rend() { return reverse_iterator(Sentinel); }\n  const_reverse_iterator rend() const {\n    return const_reverse_iterator(Sentinel);\n  }\n\n  /// Check if the list is empty in constant time.\n  LLVM_NODISCARD bool empty() const { return Sentinel.empty(); }\n\n  /// Calculate the size of the list in linear time.\n  LLVM_NODISCARD size_type size() const {\n    return std::distance(begin(), end());\n  }\n\n  reference front() { return *begin(); }\n  const_reference front() const { return *begin(); }\n  reference back() { return *rbegin(); }\n  const_reference back() const { return *rbegin(); }\n\n  /// Insert a node at the front; never copies.\n  void push_front(reference Node) { insert(begin(), Node); }\n\n  /// Insert a node at the back; never copies.\n  void push_back(reference Node) { insert(end(), Node); }\n\n  /// Remove the node at the front; never deletes.\n  void pop_front() { erase(begin()); }\n\n  /// Remove the node at the back; never deletes.\n  void pop_back() { erase(--end()); }\n\n  /// Swap with another list in place using std::swap.\n  void swap(simple_ilist &X) { std::swap(*this, X); }\n\n  /// Insert a node by reference; never copies.\n  iterator insert(iterator I, reference Node) {\n    list_base_type::insertBefore(*I.getNodePtr(), *this->getNodePtr(&Node));\n    return iterator(&Node);\n  }\n\n  /// Insert a range of nodes; never copies.\n  template <class Iterator>\n  void insert(iterator I, Iterator First, Iterator Last) {\n    for (; First != Last; ++First)\n      insert(I, *First);\n  }\n\n  /// Clone another list.\n  template <class Cloner, class Disposer>\n  void cloneFrom(const simple_ilist &L2, Cloner clone, Disposer dispose) {\n    clearAndDispose(dispose);\n    for (const_reference V : L2)\n      push_back(*clone(V));\n  }\n\n  /// Remove a node by reference; never deletes.\n  ///\n  /// \\see \\a erase() for removing by iterator.\n  /// \\see \\a removeAndDispose() if the node should be deleted.\n  void remove(reference N) { list_base_type::remove(*this->getNodePtr(&N)); }\n\n  /// Remove a node by reference and dispose of it.\n  template <class Disposer>\n  void removeAndDispose(reference N, Disposer dispose) {\n    remove(N);\n    dispose(&N);\n  }\n\n  /// Remove a node by iterator; never deletes.\n  ///\n  /// \\see \\a remove() for removing by reference.\n  /// \\see \\a eraseAndDispose() it the node should be deleted.\n  iterator erase(iterator I) {\n    assert(I != end() && \"Cannot remove end of list!\");\n    remove(*I++);\n    return I;\n  }\n\n  /// Remove a range of nodes; never deletes.\n  ///\n  /// \\see \\a eraseAndDispose() if the nodes should be deleted.\n  iterator erase(iterator First, iterator Last) {\n    list_base_type::removeRange(*First.getNodePtr(), *Last.getNodePtr());\n    return Last;\n  }\n\n  /// Remove a node by iterator and dispose of it.\n  template <class Disposer>\n  iterator eraseAndDispose(iterator I, Disposer dispose) {\n    auto Next = std::next(I);\n    erase(I);\n    dispose(&*I);\n    return Next;\n  }\n\n  /// Remove a range of nodes and dispose of them.\n  template <class Disposer>\n  iterator eraseAndDispose(iterator First, iterator Last, Disposer dispose) {\n    while (First != Last)\n      First = eraseAndDispose(First, dispose);\n    return Last;\n  }\n\n  /// Clear the list; never deletes.\n  ///\n  /// \\see \\a clearAndDispose() if the nodes should be deleted.\n  void clear() { Sentinel.reset(); }\n\n  /// Clear the list and dispose of the nodes.\n  template <class Disposer> void clearAndDispose(Disposer dispose) {\n    eraseAndDispose(begin(), end(), dispose);\n  }\n\n  /// Splice in another list.\n  void splice(iterator I, simple_ilist &L2) {\n    splice(I, L2, L2.begin(), L2.end());\n  }\n\n  /// Splice in a node from another list.\n  void splice(iterator I, simple_ilist &L2, iterator Node) {\n    splice(I, L2, Node, std::next(Node));\n  }\n\n  /// Splice in a range of nodes from another list.\n  void splice(iterator I, simple_ilist &, iterator First, iterator Last) {\n    list_base_type::transferBefore(*I.getNodePtr(), *First.getNodePtr(),\n                                   *Last.getNodePtr());\n  }\n\n  /// Merge in another list.\n  ///\n  /// \\pre \\c this and \\p RHS are sorted.\n  ///@{\n  void merge(simple_ilist &RHS) { merge(RHS, std::less<T>()); }\n  template <class Compare> void merge(simple_ilist &RHS, Compare comp);\n  ///@}\n\n  /// Sort the list.\n  ///@{\n  void sort() { sort(std::less<T>()); }\n  template <class Compare> void sort(Compare comp);\n  ///@}\n};\n\ntemplate <class T, class... Options>\ntemplate <class Compare>\nvoid simple_ilist<T, Options...>::merge(simple_ilist &RHS, Compare comp) {\n  if (this == &RHS || RHS.empty())\n    return;\n  iterator LI = begin(), LE = end();\n  iterator RI = RHS.begin(), RE = RHS.end();\n  while (LI != LE) {\n    if (comp(*RI, *LI)) {\n      // Transfer a run of at least size 1 from RHS to LHS.\n      iterator RunStart = RI++;\n      RI = std::find_if(RI, RE, [&](reference RV) { return !comp(RV, *LI); });\n      splice(LI, RHS, RunStart, RI);\n      if (RI == RE)\n        return;\n    }\n    ++LI;\n  }\n  // Transfer the remaining RHS nodes once LHS is finished.\n  splice(LE, RHS, RI, RE);\n}\n\ntemplate <class T, class... Options>\ntemplate <class Compare>\nvoid simple_ilist<T, Options...>::sort(Compare comp) {\n  // Vacuously sorted.\n  if (empty() || std::next(begin()) == end())\n    return;\n\n  // Split the list in the middle.\n  iterator Center = begin(), End = begin();\n  while (End != end() && ++End != end()) {\n    ++Center;\n    ++End;\n  }\n  simple_ilist RHS;\n  RHS.splice(RHS.end(), *this, Center, end());\n\n  // Sort the sublists and merge back together.\n  sort(comp);\n  RHS.sort(comp);\n  merge(RHS, comp);\n}\n\n} // end namespace llvm\n\n#endif // LLVM_ADT_SIMPLE_ILIST_H\n"}, "81": {"id": 81, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InstrTypes.h", "content": "//===- llvm/InstrTypes.h - Important Instruction subclasses -----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines various meta classes of instructions that exist in the VM\n// representation.  Specific concrete subclasses of these may be found in the\n// i*.h files...\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_INSTRTYPES_H\n#define LLVM_IR_INSTRTYPES_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/StringMap.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/LLVMContext.h\"\n#include \"llvm/IR/OperandTraits.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <iterator>\n#include <string>\n#include <vector>\n\nnamespace llvm {\n\nnamespace Intrinsic {\ntypedef unsigned ID;\n}\n\n//===----------------------------------------------------------------------===//\n//                          UnaryInstruction Class\n//===----------------------------------------------------------------------===//\n\nclass UnaryInstruction : public Instruction {\nprotected:\n  UnaryInstruction(Type *Ty, unsigned iType, Value *V,\n                   Instruction *IB = nullptr)\n    : Instruction(Ty, iType, &Op<0>(), 1, IB) {\n    Op<0>() = V;\n  }\n  UnaryInstruction(Type *Ty, unsigned iType, Value *V, BasicBlock *IAE)\n    : Instruction(Ty, iType, &Op<0>(), 1, IAE) {\n    Op<0>() = V;\n  }\n\npublic:\n  // allocate space for exactly one operand\n  void *operator new(size_t s) {\n    return User::operator new(s, 1);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isUnaryOp() ||\n           I->getOpcode() == Instruction::Alloca ||\n           I->getOpcode() == Instruction::Load ||\n           I->getOpcode() == Instruction::VAArg ||\n           I->getOpcode() == Instruction::ExtractValue ||\n           (I->getOpcode() >= CastOpsBegin && I->getOpcode() < CastOpsEnd);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<UnaryInstruction> :\n  public FixedNumOperandTraits<UnaryInstruction, 1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(UnaryInstruction, Value)\n\n//===----------------------------------------------------------------------===//\n//                                UnaryOperator Class\n//===----------------------------------------------------------------------===//\n\nclass UnaryOperator : public UnaryInstruction {\n  void AssertOK();\n\nprotected:\n  UnaryOperator(UnaryOps iType, Value *S, Type *Ty,\n                const Twine &Name, Instruction *InsertBefore);\n  UnaryOperator(UnaryOps iType, Value *S, Type *Ty,\n                const Twine &Name, BasicBlock *InsertAtEnd);\n\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  UnaryOperator *cloneImpl() const;\n\npublic:\n\n  /// Construct a unary instruction, given the opcode and an operand.\n  /// Optionally (if InstBefore is specified) insert the instruction\n  /// into a BasicBlock right before the specified instruction.  The specified\n  /// Instruction is allowed to be a dereferenced end iterator.\n  ///\n  static UnaryOperator *Create(UnaryOps Op, Value *S,\n                               const Twine &Name = Twine(),\n                               Instruction *InsertBefore = nullptr);\n\n  /// Construct a unary instruction, given the opcode and an operand.\n  /// Also automatically insert this instruction to the end of the\n  /// BasicBlock specified.\n  ///\n  static UnaryOperator *Create(UnaryOps Op, Value *S,\n                               const Twine &Name,\n                               BasicBlock *InsertAtEnd);\n\n  /// These methods just forward to Create, and are useful when you\n  /// statically know what type of instruction you're going to create.  These\n  /// helpers just save some typing.\n#define HANDLE_UNARY_INST(N, OPC, CLASS) \\\n  static UnaryOperator *Create##OPC(Value *V, const Twine &Name = \"\") {\\\n    return Create(Instruction::OPC, V, Name);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_UNARY_INST(N, OPC, CLASS) \\\n  static UnaryOperator *Create##OPC(Value *V, const Twine &Name, \\\n                                    BasicBlock *BB) {\\\n    return Create(Instruction::OPC, V, Name, BB);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_UNARY_INST(N, OPC, CLASS) \\\n  static UnaryOperator *Create##OPC(Value *V, const Twine &Name, \\\n                                    Instruction *I) {\\\n    return Create(Instruction::OPC, V, Name, I);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n\n  static UnaryOperator *\n  CreateWithCopiedFlags(UnaryOps Opc, Value *V, Instruction *CopyO,\n                        const Twine &Name = \"\",\n                        Instruction *InsertBefore = nullptr) {\n    UnaryOperator *UO = Create(Opc, V, Name, InsertBefore);\n    UO->copyIRFlags(CopyO);\n    return UO;\n  }\n\n  static UnaryOperator *CreateFNegFMF(Value *Op, Instruction *FMFSource,\n                                      const Twine &Name = \"\",\n                                      Instruction *InsertBefore = nullptr) {\n    return CreateWithCopiedFlags(Instruction::FNeg, Op, FMFSource, Name,\n                                 InsertBefore);\n  }\n\n  UnaryOps getOpcode() const {\n    return static_cast<UnaryOps>(Instruction::getOpcode());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isUnaryOp();\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                           BinaryOperator Class\n//===----------------------------------------------------------------------===//\n\nclass BinaryOperator : public Instruction {\n  void AssertOK();\n\nprotected:\n  BinaryOperator(BinaryOps iType, Value *S1, Value *S2, Type *Ty,\n                 const Twine &Name, Instruction *InsertBefore);\n  BinaryOperator(BinaryOps iType, Value *S1, Value *S2, Type *Ty,\n                 const Twine &Name, BasicBlock *InsertAtEnd);\n\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  BinaryOperator *cloneImpl() const;\n\npublic:\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Construct a binary instruction, given the opcode and the two\n  /// operands.  Optionally (if InstBefore is specified) insert the instruction\n  /// into a BasicBlock right before the specified instruction.  The specified\n  /// Instruction is allowed to be a dereferenced end iterator.\n  ///\n  static BinaryOperator *Create(BinaryOps Op, Value *S1, Value *S2,\n                                const Twine &Name = Twine(),\n                                Instruction *InsertBefore = nullptr);\n\n  /// Construct a binary instruction, given the opcode and the two\n  /// operands.  Also automatically insert this instruction to the end of the\n  /// BasicBlock specified.\n  ///\n  static BinaryOperator *Create(BinaryOps Op, Value *S1, Value *S2,\n                                const Twine &Name, BasicBlock *InsertAtEnd);\n\n  /// These methods just forward to Create, and are useful when you\n  /// statically know what type of instruction you're going to create.  These\n  /// helpers just save some typing.\n#define HANDLE_BINARY_INST(N, OPC, CLASS) \\\n  static BinaryOperator *Create##OPC(Value *V1, Value *V2, \\\n                                     const Twine &Name = \"\") {\\\n    return Create(Instruction::OPC, V1, V2, Name);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_BINARY_INST(N, OPC, CLASS) \\\n  static BinaryOperator *Create##OPC(Value *V1, Value *V2, \\\n                                     const Twine &Name, BasicBlock *BB) {\\\n    return Create(Instruction::OPC, V1, V2, Name, BB);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n#define HANDLE_BINARY_INST(N, OPC, CLASS) \\\n  static BinaryOperator *Create##OPC(Value *V1, Value *V2, \\\n                                     const Twine &Name, Instruction *I) {\\\n    return Create(Instruction::OPC, V1, V2, Name, I);\\\n  }\n#include \"llvm/IR/Instruction.def\"\n\n  static BinaryOperator *CreateWithCopiedFlags(BinaryOps Opc,\n                                               Value *V1, Value *V2,\n                                               Instruction *CopyO,\n                                               const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->copyIRFlags(CopyO);\n    return BO;\n  }\n\n  static BinaryOperator *CreateFAddFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FAdd, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFSubFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FSub, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFMulFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FMul, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFDivFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FDiv, V1, V2, FMFSource, Name);\n  }\n  static BinaryOperator *CreateFRemFMF(Value *V1, Value *V2,\n                                       Instruction *FMFSource,\n                                       const Twine &Name = \"\") {\n    return CreateWithCopiedFlags(Instruction::FRem, V1, V2, FMFSource, Name);\n  }\n\n  static BinaryOperator *CreateNSW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->setHasNoSignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNSW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, BasicBlock *BB) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, BB);\n    BO->setHasNoSignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNSW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, Instruction *I) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, I);\n    BO->setHasNoSignedWrap(true);\n    return BO;\n  }\n\n  static BinaryOperator *CreateNUW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->setHasNoUnsignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNUW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, BasicBlock *BB) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, BB);\n    BO->setHasNoUnsignedWrap(true);\n    return BO;\n  }\n  static BinaryOperator *CreateNUW(BinaryOps Opc, Value *V1, Value *V2,\n                                   const Twine &Name, Instruction *I) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, I);\n    BO->setHasNoUnsignedWrap(true);\n    return BO;\n  }\n\n  static BinaryOperator *CreateExact(BinaryOps Opc, Value *V1, Value *V2,\n                                     const Twine &Name = \"\") {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name);\n    BO->setIsExact(true);\n    return BO;\n  }\n  static BinaryOperator *CreateExact(BinaryOps Opc, Value *V1, Value *V2,\n                                     const Twine &Name, BasicBlock *BB) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, BB);\n    BO->setIsExact(true);\n    return BO;\n  }\n  static BinaryOperator *CreateExact(BinaryOps Opc, Value *V1, Value *V2,\n                                     const Twine &Name, Instruction *I) {\n    BinaryOperator *BO = Create(Opc, V1, V2, Name, I);\n    BO->setIsExact(true);\n    return BO;\n  }\n\n#define DEFINE_HELPERS(OPC, NUWNSWEXACT)                                       \\\n  static BinaryOperator *Create##NUWNSWEXACT##OPC(Value *V1, Value *V2,        \\\n                                                  const Twine &Name = \"\") {    \\\n    return Create##NUWNSWEXACT(Instruction::OPC, V1, V2, Name);                \\\n  }                                                                            \\\n  static BinaryOperator *Create##NUWNSWEXACT##OPC(                             \\\n      Value *V1, Value *V2, const Twine &Name, BasicBlock *BB) {               \\\n    return Create##NUWNSWEXACT(Instruction::OPC, V1, V2, Name, BB);            \\\n  }                                                                            \\\n  static BinaryOperator *Create##NUWNSWEXACT##OPC(                             \\\n      Value *V1, Value *V2, const Twine &Name, Instruction *I) {               \\\n    return Create##NUWNSWEXACT(Instruction::OPC, V1, V2, Name, I);             \\\n  }\n\n  DEFINE_HELPERS(Add, NSW) // CreateNSWAdd\n  DEFINE_HELPERS(Add, NUW) // CreateNUWAdd\n  DEFINE_HELPERS(Sub, NSW) // CreateNSWSub\n  DEFINE_HELPERS(Sub, NUW) // CreateNUWSub\n  DEFINE_HELPERS(Mul, NSW) // CreateNSWMul\n  DEFINE_HELPERS(Mul, NUW) // CreateNUWMul\n  DEFINE_HELPERS(Shl, NSW) // CreateNSWShl\n  DEFINE_HELPERS(Shl, NUW) // CreateNUWShl\n\n  DEFINE_HELPERS(SDiv, Exact)  // CreateExactSDiv\n  DEFINE_HELPERS(UDiv, Exact)  // CreateExactUDiv\n  DEFINE_HELPERS(AShr, Exact)  // CreateExactAShr\n  DEFINE_HELPERS(LShr, Exact)  // CreateExactLShr\n\n#undef DEFINE_HELPERS\n\n  /// Helper functions to construct and inspect unary operations (NEG and NOT)\n  /// via binary operators SUB and XOR:\n  ///\n  /// Create the NEG and NOT instructions out of SUB and XOR instructions.\n  ///\n  static BinaryOperator *CreateNeg(Value *Op, const Twine &Name = \"\",\n                                   Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNeg(Value *Op, const Twine &Name,\n                                   BasicBlock *InsertAtEnd);\n  static BinaryOperator *CreateNSWNeg(Value *Op, const Twine &Name = \"\",\n                                      Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNSWNeg(Value *Op, const Twine &Name,\n                                      BasicBlock *InsertAtEnd);\n  static BinaryOperator *CreateNUWNeg(Value *Op, const Twine &Name = \"\",\n                                      Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNUWNeg(Value *Op, const Twine &Name,\n                                      BasicBlock *InsertAtEnd);\n  static BinaryOperator *CreateNot(Value *Op, const Twine &Name = \"\",\n                                   Instruction *InsertBefore = nullptr);\n  static BinaryOperator *CreateNot(Value *Op, const Twine &Name,\n                                   BasicBlock *InsertAtEnd);\n\n  BinaryOps getOpcode() const {\n    return static_cast<BinaryOps>(Instruction::getOpcode());\n  }\n\n  /// Exchange the two operands to this instruction.\n  /// This instruction is safe to use on any binary instruction and\n  /// does not modify the semantics of the instruction.  If the instruction\n  /// cannot be reversed (ie, it's a Div), then return true.\n  ///\n  bool swapOperands();\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isBinaryOp();\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<BinaryOperator> :\n  public FixedNumOperandTraits<BinaryOperator, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(BinaryOperator, Value)\n\n//===----------------------------------------------------------------------===//\n//                               CastInst Class\n//===----------------------------------------------------------------------===//\n\n/// This is the base class for all instructions that perform data\n/// casts. It is simply provided so that instruction category testing\n/// can be performed with code like:\n///\n/// if (isa<CastInst>(Instr)) { ... }\n/// Base class of casting instructions.\nclass CastInst : public UnaryInstruction {\nprotected:\n  /// Constructor with insert-before-instruction semantics for subclasses\n  CastInst(Type *Ty, unsigned iType, Value *S,\n           const Twine &NameStr = \"\", Instruction *InsertBefore = nullptr)\n    : UnaryInstruction(Ty, iType, S, InsertBefore) {\n    setName(NameStr);\n  }\n  /// Constructor with insert-at-end-of-block semantics for subclasses\n  CastInst(Type *Ty, unsigned iType, Value *S,\n           const Twine &NameStr, BasicBlock *InsertAtEnd)\n    : UnaryInstruction(Ty, iType, S, InsertAtEnd) {\n    setName(NameStr);\n  }\n\npublic:\n  /// Provides a way to construct any of the CastInst subclasses using an\n  /// opcode instead of the subclass's constructor. The opcode must be in the\n  /// CastOps category (Instruction::isCast(opcode) returns true). This\n  /// constructor has insert-before-instruction semantics to automatically\n  /// insert the new CastInst before InsertBefore (if it is non-null).\n  /// Construct any of the CastInst subclasses\n  static CastInst *Create(\n    Instruction::CastOps,    ///< The opcode of the cast instruction\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n  /// Provides a way to construct any of the CastInst subclasses using an\n  /// opcode instead of the subclass's constructor. The opcode must be in the\n  /// CastOps category. This constructor has insert-at-end-of-block semantics\n  /// to automatically insert the new CastInst at the end of InsertAtEnd (if\n  /// its non-null).\n  /// Construct any of the CastInst subclasses\n  static CastInst *Create(\n    Instruction::CastOps,    ///< The opcode for the cast instruction\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a ZExt or BitCast cast instruction\n  static CastInst *CreateZExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a ZExt or BitCast cast instruction\n  static CastInst *CreateZExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a SExt or BitCast cast instruction\n  static CastInst *CreateSExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a SExt or BitCast cast instruction\n  static CastInst *CreateSExtOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a BitCast AddrSpaceCast, or a PtrToInt cast instruction.\n  static CastInst *CreatePointerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a BitCast, AddrSpaceCast or a PtrToInt cast instruction.\n  static CastInst *CreatePointerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a BitCast or an AddrSpaceCast cast instruction.\n  static CastInst *CreatePointerBitCastOrAddrSpaceCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a BitCast or an AddrSpaceCast cast instruction.\n  static CastInst *CreatePointerBitCastOrAddrSpaceCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a BitCast, a PtrToInt, or an IntToPTr cast instruction.\n  ///\n  /// If the value is a pointer type and the destination an integer type,\n  /// creates a PtrToInt cast. If the value is an integer type and the\n  /// destination a pointer type, creates an IntToPtr cast. Otherwise, creates\n  /// a bitcast.\n  static CastInst *CreateBitOrPointerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a ZExt, BitCast, or Trunc for int -> int casts.\n  static CastInst *CreateIntegerCast(\n    Value *S,                ///< The pointer value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    bool isSigned,           ///< Whether to regard S as signed or not\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a ZExt, BitCast, or Trunc for int -> int casts.\n  static CastInst *CreateIntegerCast(\n    Value *S,                ///< The integer value to be casted (operand 0)\n    Type *Ty,          ///< The integer type to which operand is casted\n    bool isSigned,           ///< Whether to regard S as signed or not\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create an FPExt, BitCast, or FPTrunc for fp -> fp casts\n  static CastInst *CreateFPCast(\n    Value *S,                ///< The floating point value to be casted\n    Type *Ty,          ///< The floating point type to cast to\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create an FPExt, BitCast, or FPTrunc for fp -> fp casts\n  static CastInst *CreateFPCast(\n    Value *S,                ///< The floating point value to be casted\n    Type *Ty,          ///< The floating point type to cast to\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Create a Trunc or BitCast cast instruction\n  static CastInst *CreateTruncOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which cast should be made\n    const Twine &Name = \"\", ///< Name for the instruction\n    Instruction *InsertBefore = nullptr ///< Place to insert the instruction\n  );\n\n  /// Create a Trunc or BitCast cast instruction\n  static CastInst *CreateTruncOrBitCast(\n    Value *S,                ///< The value to be casted (operand 0)\n    Type *Ty,          ///< The type to which operand is casted\n    const Twine &Name, ///< The name for the instruction\n    BasicBlock *InsertAtEnd  ///< The block to insert the instruction into\n  );\n\n  /// Check whether a bitcast between these types is valid\n  static bool isBitCastable(\n    Type *SrcTy, ///< The Type from which the value should be cast.\n    Type *DestTy ///< The Type to which the value should be cast.\n  );\n\n  /// Check whether a bitcast, inttoptr, or ptrtoint cast between these\n  /// types is valid and a no-op.\n  ///\n  /// This ensures that any pointer<->integer cast has enough bits in the\n  /// integer and any other cast is a bitcast.\n  static bool isBitOrNoopPointerCastable(\n      Type *SrcTy,  ///< The Type from which the value should be cast.\n      Type *DestTy, ///< The Type to which the value should be cast.\n      const DataLayout &DL);\n\n  /// Returns the opcode necessary to cast Val into Ty using usual casting\n  /// rules.\n  /// Infer the opcode for cast operand and type\n  static Instruction::CastOps getCastOpcode(\n    const Value *Val, ///< The value to cast\n    bool SrcIsSigned, ///< Whether to treat the source as signed\n    Type *Ty,   ///< The Type to which the value should be casted\n    bool DstIsSigned  ///< Whether to treate the dest. as signed\n  );\n\n  /// There are several places where we need to know if a cast instruction\n  /// only deals with integer source and destination types. To simplify that\n  /// logic, this method is provided.\n  /// @returns true iff the cast has only integral typed operand and dest type.\n  /// Determine if this is an integer-only cast.\n  bool isIntegerCast() const;\n\n  /// A lossless cast is one that does not alter the basic value. It implies\n  /// a no-op cast but is more stringent, preventing things like int->float,\n  /// long->double, or int->ptr.\n  /// @returns true iff the cast is lossless.\n  /// Determine if this is a lossless cast.\n  bool isLosslessCast() const;\n\n  /// A no-op cast is one that can be effected without changing any bits.\n  /// It implies that the source and destination types are the same size. The\n  /// DataLayout argument is to determine the pointer size when examining casts\n  /// involving Integer and Pointer types. They are no-op casts if the integer\n  /// is the same size as the pointer. However, pointer size varies with\n  /// platform.  Note that a precondition of this method is that the cast is\n  /// legal - i.e. the instruction formed with these operands would verify.\n  static bool isNoopCast(\n    Instruction::CastOps Opcode, ///< Opcode of cast\n    Type *SrcTy,         ///< SrcTy of cast\n    Type *DstTy,         ///< DstTy of cast\n    const DataLayout &DL ///< DataLayout to get the Int Ptr type from.\n  );\n\n  /// Determine if this cast is a no-op cast.\n  ///\n  /// \\param DL is the DataLayout to determine pointer size.\n  bool isNoopCast(const DataLayout &DL) const;\n\n  /// Determine how a pair of casts can be eliminated, if they can be at all.\n  /// This is a helper function for both CastInst and ConstantExpr.\n  /// @returns 0 if the CastInst pair can't be eliminated, otherwise\n  /// returns Instruction::CastOps value for a cast that can replace\n  /// the pair, casting SrcTy to DstTy.\n  /// Determine if a cast pair is eliminable\n  static unsigned isEliminableCastPair(\n    Instruction::CastOps firstOpcode,  ///< Opcode of first cast\n    Instruction::CastOps secondOpcode, ///< Opcode of second cast\n    Type *SrcTy, ///< SrcTy of 1st cast\n    Type *MidTy, ///< DstTy of 1st cast & SrcTy of 2nd cast\n    Type *DstTy, ///< DstTy of 2nd cast\n    Type *SrcIntPtrTy, ///< Integer type corresponding to Ptr SrcTy, or null\n    Type *MidIntPtrTy, ///< Integer type corresponding to Ptr MidTy, or null\n    Type *DstIntPtrTy  ///< Integer type corresponding to Ptr DstTy, or null\n  );\n\n  /// Return the opcode of this CastInst\n  Instruction::CastOps getOpcode() const {\n    return Instruction::CastOps(Instruction::getOpcode());\n  }\n\n  /// Return the source type, as a convenience\n  Type* getSrcTy() const { return getOperand(0)->getType(); }\n  /// Return the destination type, as a convenience\n  Type* getDestTy() const { return getType(); }\n\n  /// This method can be used to determine if a cast from SrcTy to DstTy using\n  /// Opcode op is valid or not.\n  /// @returns true iff the proposed cast is valid.\n  /// Determine if a cast is valid without creating one.\n  static bool castIsValid(Instruction::CastOps op, Type *SrcTy, Type *DstTy);\n  static bool castIsValid(Instruction::CastOps op, Value *S, Type *DstTy) {\n    return castIsValid(op, S->getType(), DstTy);\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->isCast();\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               CmpInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class is the base class for the comparison instructions.\n/// Abstract base class of comparison instructions.\nclass CmpInst : public Instruction {\npublic:\n  /// This enumeration lists the possible predicates for CmpInst subclasses.\n  /// Values in the range 0-31 are reserved for FCmpInst, while values in the\n  /// range 32-64 are reserved for ICmpInst. This is necessary to ensure the\n  /// predicate values are not overlapping between the classes.\n  ///\n  /// Some passes (e.g. InstCombine) depend on the bit-wise characteristics of\n  /// FCMP_* values. Changing the bit patterns requires a potential change to\n  /// those passes.\n  enum Predicate : unsigned {\n    // Opcode            U L G E    Intuitive operation\n    FCMP_FALSE = 0, ///< 0 0 0 0    Always false (always folded)\n    FCMP_OEQ = 1,   ///< 0 0 0 1    True if ordered and equal\n    FCMP_OGT = 2,   ///< 0 0 1 0    True if ordered and greater than\n    FCMP_OGE = 3,   ///< 0 0 1 1    True if ordered and greater than or equal\n    FCMP_OLT = 4,   ///< 0 1 0 0    True if ordered and less than\n    FCMP_OLE = 5,   ///< 0 1 0 1    True if ordered and less than or equal\n    FCMP_ONE = 6,   ///< 0 1 1 0    True if ordered and operands are unequal\n    FCMP_ORD = 7,   ///< 0 1 1 1    True if ordered (no nans)\n    FCMP_UNO = 8,   ///< 1 0 0 0    True if unordered: isnan(X) | isnan(Y)\n    FCMP_UEQ = 9,   ///< 1 0 0 1    True if unordered or equal\n    FCMP_UGT = 10,  ///< 1 0 1 0    True if unordered or greater than\n    FCMP_UGE = 11,  ///< 1 0 1 1    True if unordered, greater than, or equal\n    FCMP_ULT = 12,  ///< 1 1 0 0    True if unordered or less than\n    FCMP_ULE = 13,  ///< 1 1 0 1    True if unordered, less than, or equal\n    FCMP_UNE = 14,  ///< 1 1 1 0    True if unordered or not equal\n    FCMP_TRUE = 15, ///< 1 1 1 1    Always true (always folded)\n    FIRST_FCMP_PREDICATE = FCMP_FALSE,\n    LAST_FCMP_PREDICATE = FCMP_TRUE,\n    BAD_FCMP_PREDICATE = FCMP_TRUE + 1,\n    ICMP_EQ = 32,  ///< equal\n    ICMP_NE = 33,  ///< not equal\n    ICMP_UGT = 34, ///< unsigned greater than\n    ICMP_UGE = 35, ///< unsigned greater or equal\n    ICMP_ULT = 36, ///< unsigned less than\n    ICMP_ULE = 37, ///< unsigned less or equal\n    ICMP_SGT = 38, ///< signed greater than\n    ICMP_SGE = 39, ///< signed greater or equal\n    ICMP_SLT = 40, ///< signed less than\n    ICMP_SLE = 41, ///< signed less or equal\n    FIRST_ICMP_PREDICATE = ICMP_EQ,\n    LAST_ICMP_PREDICATE = ICMP_SLE,\n    BAD_ICMP_PREDICATE = ICMP_SLE + 1\n  };\n  using PredicateField =\n      Bitfield::Element<Predicate, 0, 6, LAST_ICMP_PREDICATE>;\n\nprotected:\n  CmpInst(Type *ty, Instruction::OtherOps op, Predicate pred,\n          Value *LHS, Value *RHS, const Twine &Name = \"\",\n          Instruction *InsertBefore = nullptr,\n          Instruction *FlagsSource = nullptr);\n\n  CmpInst(Type *ty, Instruction::OtherOps op, Predicate pred,\n          Value *LHS, Value *RHS, const Twine &Name,\n          BasicBlock *InsertAtEnd);\n\npublic:\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  /// Construct a compare instruction, given the opcode, the predicate and\n  /// the two operands.  Optionally (if InstBefore is specified) insert the\n  /// instruction into a BasicBlock right before the specified instruction.\n  /// The specified Instruction is allowed to be a dereferenced end iterator.\n  /// Create a CmpInst\n  static CmpInst *Create(OtherOps Op,\n                         Predicate predicate, Value *S1,\n                         Value *S2, const Twine &Name = \"\",\n                         Instruction *InsertBefore = nullptr);\n\n  /// Construct a compare instruction, given the opcode, the predicate and the\n  /// two operands.  Also automatically insert this instruction to the end of\n  /// the BasicBlock specified.\n  /// Create a CmpInst\n  static CmpInst *Create(OtherOps Op, Predicate predicate, Value *S1,\n                         Value *S2, const Twine &Name, BasicBlock *InsertAtEnd);\n\n  /// Get the opcode casted to the right type\n  OtherOps getOpcode() const {\n    return static_cast<OtherOps>(Instruction::getOpcode());\n  }\n\n  /// Return the predicate for this instruction.\n  Predicate getPredicate() const { return getSubclassData<PredicateField>(); }\n\n  /// Set the predicate for this instruction to the specified value.\n  void setPredicate(Predicate P) { setSubclassData<PredicateField>(P); }\n\n  static bool isFPPredicate(Predicate P) {\n    static_assert(FIRST_FCMP_PREDICATE == 0,\n                  \"FIRST_FCMP_PREDICATE is required to be 0\");\n    return P <= LAST_FCMP_PREDICATE;\n  }\n\n  static bool isIntPredicate(Predicate P) {\n    return P >= FIRST_ICMP_PREDICATE && P <= LAST_ICMP_PREDICATE;\n  }\n\n  static StringRef getPredicateName(Predicate P);\n\n  bool isFPPredicate() const { return isFPPredicate(getPredicate()); }\n  bool isIntPredicate() const { return isIntPredicate(getPredicate()); }\n\n  /// For example, EQ -> NE, UGT -> ULE, SLT -> SGE,\n  ///              OEQ -> UNE, UGT -> OLE, OLT -> UGE, etc.\n  /// @returns the inverse predicate for the instruction's current predicate.\n  /// Return the inverse of the instruction's predicate.\n  Predicate getInversePredicate() const {\n    return getInversePredicate(getPredicate());\n  }\n\n  /// For example, EQ -> NE, UGT -> ULE, SLT -> SGE,\n  ///              OEQ -> UNE, UGT -> OLE, OLT -> UGE, etc.\n  /// @returns the inverse predicate for predicate provided in \\p pred.\n  /// Return the inverse of a given predicate\n  static Predicate getInversePredicate(Predicate pred);\n\n  /// For example, EQ->EQ, SLE->SGE, ULT->UGT,\n  ///              OEQ->OEQ, ULE->UGE, OLT->OGT, etc.\n  /// @returns the predicate that would be the result of exchanging the two\n  /// operands of the CmpInst instruction without changing the result\n  /// produced.\n  /// Return the predicate as if the operands were swapped\n  Predicate getSwappedPredicate() const {\n    return getSwappedPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// Return the predicate as if the operands were swapped.\n  static Predicate getSwappedPredicate(Predicate pred);\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns true if the comparison predicate is strict, false otherwise.\n  static bool isStrictPredicate(Predicate predicate);\n\n  /// @returns true if the comparison predicate is strict, false otherwise.\n  /// Determine if this instruction is using an strict comparison predicate.\n  bool isStrictPredicate() const { return isStrictPredicate(getPredicate()); }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns true if the comparison predicate is non-strict, false otherwise.\n  static bool isNonStrictPredicate(Predicate predicate);\n\n  /// @returns true if the comparison predicate is non-strict, false otherwise.\n  /// Determine if this instruction is using an non-strict comparison predicate.\n  bool isNonStrictPredicate() const {\n    return isNonStrictPredicate(getPredicate());\n  }\n\n  /// For example, SGE -> SGT, SLE -> SLT, ULE -> ULT, UGE -> UGT.\n  /// Returns the strict version of non-strict comparisons.\n  Predicate getStrictPredicate() const {\n    return getStrictPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns the strict version of comparison provided in \\p pred.\n  /// If \\p pred is not a strict comparison predicate, returns \\p pred.\n  /// Returns the strict version of non-strict comparisons.\n  static Predicate getStrictPredicate(Predicate pred);\n\n  /// For example, SGT -> SGE, SLT -> SLE, ULT -> ULE, UGT -> UGE.\n  /// Returns the non-strict version of strict comparisons.\n  Predicate getNonStrictPredicate() const {\n    return getNonStrictPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// @returns the non-strict version of comparison provided in \\p pred.\n  /// If \\p pred is not a strict comparison predicate, returns \\p pred.\n  /// Returns the non-strict version of strict comparisons.\n  static Predicate getNonStrictPredicate(Predicate pred);\n\n  /// This is a static version that you can use without an instruction\n  /// available.\n  /// Return the flipped strictness of predicate\n  static Predicate getFlippedStrictnessPredicate(Predicate pred);\n\n  /// For predicate of kind \"is X or equal to 0\" returns the predicate \"is X\".\n  /// For predicate of kind \"is X\" returns the predicate \"is X or equal to 0\".\n  /// does not support other kind of predicates.\n  /// @returns the predicate that does not contains is equal to zero if\n  /// it had and vice versa.\n  /// Return the flipped strictness of predicate\n  Predicate getFlippedStrictnessPredicate() const {\n    return getFlippedStrictnessPredicate(getPredicate());\n  }\n\n  /// Provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// This is just a convenience that dispatches to the subclasses.\n  /// Swap the operands and adjust predicate accordingly to retain\n  /// the same comparison.\n  void swapOperands();\n\n  /// This is just a convenience that dispatches to the subclasses.\n  /// Determine if this CmpInst is commutative.\n  bool isCommutative() const;\n\n  /// Determine if this is an equals/not equals predicate.\n  /// This is a static version that you can use without an instruction\n  /// available.\n  static bool isEquality(Predicate pred);\n\n  /// Determine if this is an equals/not equals predicate.\n  bool isEquality() const { return isEquality(getPredicate()); }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  static bool isRelational(Predicate P) { return !isEquality(P); }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  bool isRelational() const { return !isEquality(); }\n\n  /// @returns true if the comparison is signed, false otherwise.\n  /// Determine if this instruction is using a signed comparison.\n  bool isSigned() const {\n    return isSigned(getPredicate());\n  }\n\n  /// @returns true if the comparison is unsigned, false otherwise.\n  /// Determine if this instruction is using an unsigned comparison.\n  bool isUnsigned() const {\n    return isUnsigned(getPredicate());\n  }\n\n  /// For example, ULT->SLT, ULE->SLE, UGT->SGT, UGE->SGE, SLT->Failed assert\n  /// @returns the signed version of the unsigned predicate pred.\n  /// return the signed version of a predicate\n  static Predicate getSignedPredicate(Predicate pred);\n\n  /// For example, ULT->SLT, ULE->SLE, UGT->SGT, UGE->SGE, SLT->Failed assert\n  /// @returns the signed version of the predicate for this instruction (which\n  /// has to be an unsigned predicate).\n  /// return the signed version of a predicate\n  Predicate getSignedPredicate() {\n    return getSignedPredicate(getPredicate());\n  }\n\n  /// For example, SLT->ULT, SLE->ULE, SGT->UGT, SGE->UGE, ULT->Failed assert\n  /// @returns the unsigned version of the signed predicate pred.\n  static Predicate getUnsignedPredicate(Predicate pred);\n\n  /// For example, SLT->ULT, SLE->ULE, SGT->UGT, SGE->UGE, ULT->Failed assert\n  /// @returns the unsigned version of the predicate for this instruction (which\n  /// has to be an signed predicate).\n  /// return the unsigned version of a predicate\n  Predicate getUnsignedPredicate() {\n    return getUnsignedPredicate(getPredicate());\n  }\n\n  /// For example, SLT->ULT, ULT->SLT, SLE->ULE, ULE->SLE, EQ->Failed assert\n  /// @returns the unsigned version of the signed predicate pred or\n  ///          the signed version of the signed predicate pred.\n  static Predicate getFlippedSignednessPredicate(Predicate pred);\n\n  /// For example, SLT->ULT, ULT->SLT, SLE->ULE, ULE->SLE, EQ->Failed assert\n  /// @returns the unsigned version of the signed predicate pred or\n  ///          the signed version of the signed predicate pred.\n  Predicate getFlippedSignednessPredicate() {\n    return getFlippedSignednessPredicate(getPredicate());\n  }\n\n  /// This is just a convenience.\n  /// Determine if this is true when both operands are the same.\n  bool isTrueWhenEqual() const {\n    return isTrueWhenEqual(getPredicate());\n  }\n\n  /// This is just a convenience.\n  /// Determine if this is false when both operands are the same.\n  bool isFalseWhenEqual() const {\n    return isFalseWhenEqual(getPredicate());\n  }\n\n  /// @returns true if the predicate is unsigned, false otherwise.\n  /// Determine if the predicate is an unsigned operation.\n  static bool isUnsigned(Predicate predicate);\n\n  /// @returns true if the predicate is signed, false otherwise.\n  /// Determine if the predicate is an signed operation.\n  static bool isSigned(Predicate predicate);\n\n  /// Determine if the predicate is an ordered operation.\n  static bool isOrdered(Predicate predicate);\n\n  /// Determine if the predicate is an unordered operation.\n  static bool isUnordered(Predicate predicate);\n\n  /// Determine if the predicate is true when comparing a value with itself.\n  static bool isTrueWhenEqual(Predicate predicate);\n\n  /// Determine if the predicate is false when comparing a value with itself.\n  static bool isFalseWhenEqual(Predicate predicate);\n\n  /// Determine if Pred1 implies Pred2 is true when two compares have matching\n  /// operands.\n  static bool isImpliedTrueByMatchingCmp(Predicate Pred1, Predicate Pred2);\n\n  /// Determine if Pred1 implies Pred2 is false when two compares have matching\n  /// operands.\n  static bool isImpliedFalseByMatchingCmp(Predicate Pred1, Predicate Pred2);\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ICmp ||\n           I->getOpcode() == Instruction::FCmp;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  /// Create a result type for fcmp/icmp\n  static Type* makeCmpResultType(Type* opnd_type) {\n    if (VectorType* vt = dyn_cast<VectorType>(opnd_type)) {\n      return VectorType::get(Type::getInt1Ty(opnd_type->getContext()),\n                             vt->getElementCount());\n    }\n    return Type::getInt1Ty(opnd_type->getContext());\n  }\n\nprivate:\n  // Shadow Value::setValueSubclassData with a private forwarding method so that\n  // subclasses cannot accidentally use it.\n  void setValueSubclassData(unsigned short D) {\n    Value::setValueSubclassData(D);\n  }\n};\n\n// FIXME: these are redundant if CmpInst < BinaryOperator\ntemplate <>\nstruct OperandTraits<CmpInst> : public FixedNumOperandTraits<CmpInst, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CmpInst, Value)\n\n/// A lightweight accessor for an operand bundle meant to be passed\n/// around by value.\nstruct OperandBundleUse {\n  ArrayRef<Use> Inputs;\n\n  OperandBundleUse() = default;\n  explicit OperandBundleUse(StringMapEntry<uint32_t> *Tag, ArrayRef<Use> Inputs)\n      : Inputs(Inputs), Tag(Tag) {}\n\n  /// Return true if the operand at index \\p Idx in this operand bundle\n  /// has the attribute A.\n  bool operandHasAttr(unsigned Idx, Attribute::AttrKind A) const {\n    if (isDeoptOperandBundle())\n      if (A == Attribute::ReadOnly || A == Attribute::NoCapture)\n        return Inputs[Idx]->getType()->isPointerTy();\n\n    // Conservative answer:  no operands have any attributes.\n    return false;\n  }\n\n  /// Return the tag of this operand bundle as a string.\n  StringRef getTagName() const {\n    return Tag->getKey();\n  }\n\n  /// Return the tag of this operand bundle as an integer.\n  ///\n  /// Operand bundle tags are interned by LLVMContextImpl::getOrInsertBundleTag,\n  /// and this function returns the unique integer getOrInsertBundleTag\n  /// associated the tag of this operand bundle to.\n  uint32_t getTagID() const {\n    return Tag->getValue();\n  }\n\n  /// Return true if this is a \"deopt\" operand bundle.\n  bool isDeoptOperandBundle() const {\n    return getTagID() == LLVMContext::OB_deopt;\n  }\n\n  /// Return true if this is a \"funclet\" operand bundle.\n  bool isFuncletOperandBundle() const {\n    return getTagID() == LLVMContext::OB_funclet;\n  }\n\n  /// Return true if this is a \"cfguardtarget\" operand bundle.\n  bool isCFGuardTargetOperandBundle() const {\n    return getTagID() == LLVMContext::OB_cfguardtarget;\n  }\n\nprivate:\n  /// Pointer to an entry in LLVMContextImpl::getOrInsertBundleTag.\n  StringMapEntry<uint32_t> *Tag;\n};\n\n/// A container for an operand bundle being viewed as a set of values\n/// rather than a set of uses.\n///\n/// Unlike OperandBundleUse, OperandBundleDefT owns the memory it carries, and\n/// so it is possible to create and pass around \"self-contained\" instances of\n/// OperandBundleDef and ConstOperandBundleDef.\ntemplate <typename InputTy> class OperandBundleDefT {\n  std::string Tag;\n  std::vector<InputTy> Inputs;\n\npublic:\n  explicit OperandBundleDefT(std::string Tag, std::vector<InputTy> Inputs)\n      : Tag(std::move(Tag)), Inputs(std::move(Inputs)) {}\n  explicit OperandBundleDefT(std::string Tag, ArrayRef<InputTy> Inputs)\n      : Tag(std::move(Tag)), Inputs(Inputs) {}\n\n  explicit OperandBundleDefT(const OperandBundleUse &OBU) {\n    Tag = std::string(OBU.getTagName());\n    llvm::append_range(Inputs, OBU.Inputs);\n  }\n\n  ArrayRef<InputTy> inputs() const { return Inputs; }\n\n  using input_iterator = typename std::vector<InputTy>::const_iterator;\n\n  size_t input_size() const { return Inputs.size(); }\n  input_iterator input_begin() const { return Inputs.begin(); }\n  input_iterator input_end() const { return Inputs.end(); }\n\n  StringRef getTag() const { return Tag; }\n};\n\nusing OperandBundleDef = OperandBundleDefT<Value *>;\nusing ConstOperandBundleDef = OperandBundleDefT<const Value *>;\n\n//===----------------------------------------------------------------------===//\n//                               CallBase Class\n//===----------------------------------------------------------------------===//\n\n/// Base class for all callable instructions (InvokeInst and CallInst)\n/// Holds everything related to calling a function.\n///\n/// All call-like instructions are required to use a common operand layout:\n/// - Zero or more arguments to the call,\n/// - Zero or more operand bundles with zero or more operand inputs each\n///   bundle,\n/// - Zero or more subclass controlled operands\n/// - The called function.\n///\n/// This allows this base class to easily access the called function and the\n/// start of the arguments without knowing how many other operands a particular\n/// subclass requires. Note that accessing the end of the argument list isn't\n/// as cheap as most other operations on the base class.\nclass CallBase : public Instruction {\nprotected:\n  // The first two bits are reserved by CallInst for fast retrieval,\n  using CallInstReservedField = Bitfield::Element<unsigned, 0, 2>;\n  using CallingConvField =\n      Bitfield::Element<CallingConv::ID, CallInstReservedField::NextBit, 10,\n                        CallingConv::MaxID>;\n  static_assert(\n      Bitfield::areContiguous<CallInstReservedField, CallingConvField>(),\n      \"Bitfields must be contiguous\");\n\n  /// The last operand is the called operand.\n  static constexpr int CalledOperandOpEndIdx = -1;\n\n  AttributeList Attrs; ///< parameter attributes for callable\n  FunctionType *FTy;\n\n  template <class... ArgsTy>\n  CallBase(AttributeList const &A, FunctionType *FT, ArgsTy &&... Args)\n      : Instruction(std::forward<ArgsTy>(Args)...), Attrs(A), FTy(FT) {}\n\n  using Instruction::Instruction;\n\n  bool hasDescriptor() const { return Value::HasDescriptor; }\n\n  unsigned getNumSubclassExtraOperands() const {\n    switch (getOpcode()) {\n    case Instruction::Call:\n      return 0;\n    case Instruction::Invoke:\n      return 2;\n    case Instruction::CallBr:\n      return getNumSubclassExtraOperandsDynamic();\n    }\n    llvm_unreachable(\"Invalid opcode!\");\n  }\n\n  /// Get the number of extra operands for instructions that don't have a fixed\n  /// number of extra operands.\n  unsigned getNumSubclassExtraOperandsDynamic() const;\n\npublic:\n  using Instruction::getContext;\n\n  /// Create a clone of \\p CB with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned call instruction is identical \\p CB in every way except that\n  /// the operand bundles for the new instruction are set to the operand bundles\n  /// in \\p Bundles.\n  static CallBase *Create(CallBase *CB, ArrayRef<OperandBundleDef> Bundles,\n                          Instruction *InsertPt = nullptr);\n\n  /// Create a clone of \\p CB with the operand bundle with the tag matching\n  /// \\p Bundle's tag replaced with Bundle, and insert it before \\p InsertPt.\n  ///\n  /// The returned call instruction is identical \\p CI in every way except that\n  /// the specified operand bundle has been replaced.\n  static CallBase *Create(CallBase *CB,\n                          OperandBundleDef Bundle,\n                          Instruction *InsertPt = nullptr);\n\n  /// Create a clone of \\p CB with operand bundle \\p OB added.\n  static CallBase *addOperandBundle(CallBase *CB, uint32_t ID,\n                                    OperandBundleDef OB,\n                                    Instruction *InsertPt = nullptr);\n\n  /// Create a clone of \\p CB with operand bundle \\p ID removed.\n  static CallBase *removeOperandBundle(CallBase *CB, uint32_t ID,\n                                       Instruction *InsertPt = nullptr);\n\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Call ||\n           I->getOpcode() == Instruction::Invoke ||\n           I->getOpcode() == Instruction::CallBr;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  FunctionType *getFunctionType() const { return FTy; }\n\n  void mutateFunctionType(FunctionType *FTy) {\n    Value::mutateType(FTy->getReturnType());\n    this->FTy = FTy;\n  }\n\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// data_operands_begin/data_operands_end - Return iterators iterating over\n  /// the call / invoke argument list and bundle operands.  For invokes, this is\n  /// the set of instruction operands except the invoke target and the two\n  /// successor blocks; and for calls this is the set of instruction operands\n  /// except the call target.\n  User::op_iterator data_operands_begin() { return op_begin(); }\n  User::const_op_iterator data_operands_begin() const {\n    return const_cast<CallBase *>(this)->data_operands_begin();\n  }\n  User::op_iterator data_operands_end() {\n    // Walk from the end of the operands over the called operand and any\n    // subclass operands.\n    return op_end() - getNumSubclassExtraOperands() - 1;\n  }\n  User::const_op_iterator data_operands_end() const {\n    return const_cast<CallBase *>(this)->data_operands_end();\n  }\n  iterator_range<User::op_iterator> data_ops() {\n    return make_range(data_operands_begin(), data_operands_end());\n  }\n  iterator_range<User::const_op_iterator> data_ops() const {\n    return make_range(data_operands_begin(), data_operands_end());\n  }\n  bool data_operands_empty() const {\n    return data_operands_end() == data_operands_begin();\n  }\n  unsigned data_operands_size() const {\n    return std::distance(data_operands_begin(), data_operands_end());\n  }\n\n  bool isDataOperand(const Use *U) const {\n    assert(this == U->getUser() &&\n           \"Only valid to query with a use of this instruction!\");\n    return data_operands_begin() <= U && U < data_operands_end();\n  }\n  bool isDataOperand(Value::const_user_iterator UI) const {\n    return isDataOperand(&UI.getUse());\n  }\n\n  /// Given a value use iterator, return the data operand corresponding to it.\n  /// Iterator must actually correspond to a data operand.\n  unsigned getDataOperandNo(Value::const_user_iterator UI) const {\n    return getDataOperandNo(&UI.getUse());\n  }\n\n  /// Given a use for a data operand, get the data operand number that\n  /// corresponds to it.\n  unsigned getDataOperandNo(const Use *U) const {\n    assert(isDataOperand(U) && \"Data operand # out of range!\");\n    return U - data_operands_begin();\n  }\n\n  /// Return the iterator pointing to the beginning of the argument list.\n  User::op_iterator arg_begin() { return op_begin(); }\n  User::const_op_iterator arg_begin() const {\n    return const_cast<CallBase *>(this)->arg_begin();\n  }\n\n  /// Return the iterator pointing to the end of the argument list.\n  User::op_iterator arg_end() {\n    // From the end of the data operands, walk backwards past the bundle\n    // operands.\n    return data_operands_end() - getNumTotalBundleOperands();\n  }\n  User::const_op_iterator arg_end() const {\n    return const_cast<CallBase *>(this)->arg_end();\n  }\n\n  /// Iteration adapter for range-for loops.\n  iterator_range<User::op_iterator> args() {\n    return make_range(arg_begin(), arg_end());\n  }\n  iterator_range<User::const_op_iterator> args() const {\n    return make_range(arg_begin(), arg_end());\n  }\n  bool arg_empty() const { return arg_end() == arg_begin(); }\n  unsigned arg_size() const { return arg_end() - arg_begin(); }\n\n  // Legacy API names that duplicate the above and will be removed once users\n  // are migrated.\n  iterator_range<User::op_iterator> arg_operands() {\n    return make_range(arg_begin(), arg_end());\n  }\n  iterator_range<User::const_op_iterator> arg_operands() const {\n    return make_range(arg_begin(), arg_end());\n  }\n  unsigned getNumArgOperands() const { return arg_size(); }\n\n  Value *getArgOperand(unsigned i) const {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    return getOperand(i);\n  }\n\n  void setArgOperand(unsigned i, Value *v) {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    setOperand(i, v);\n  }\n\n  /// Wrappers for getting the \\c Use of a call argument.\n  const Use &getArgOperandUse(unsigned i) const {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    return User::getOperandUse(i);\n  }\n  Use &getArgOperandUse(unsigned i) {\n    assert(i < getNumArgOperands() && \"Out of bounds!\");\n    return User::getOperandUse(i);\n  }\n\n  bool isArgOperand(const Use *U) const {\n    assert(this == U->getUser() &&\n           \"Only valid to query with a use of this instruction!\");\n    return arg_begin() <= U && U < arg_end();\n  }\n  bool isArgOperand(Value::const_user_iterator UI) const {\n    return isArgOperand(&UI.getUse());\n  }\n\n  /// Given a use for a arg operand, get the arg operand number that\n  /// corresponds to it.\n  unsigned getArgOperandNo(const Use *U) const {\n    assert(isArgOperand(U) && \"Arg operand # out of range!\");\n    return U - arg_begin();\n  }\n\n  /// Given a value use iterator, return the arg operand number corresponding to\n  /// it. Iterator must actually correspond to a data operand.\n  unsigned getArgOperandNo(Value::const_user_iterator UI) const {\n    return getArgOperandNo(&UI.getUse());\n  }\n\n  /// Returns true if this CallSite passes the given Value* as an argument to\n  /// the called function.\n  bool hasArgument(const Value *V) const {\n    return llvm::is_contained(args(), V);\n  }\n\n  Value *getCalledOperand() const { return Op<CalledOperandOpEndIdx>(); }\n\n  const Use &getCalledOperandUse() const { return Op<CalledOperandOpEndIdx>(); }\n  Use &getCalledOperandUse() { return Op<CalledOperandOpEndIdx>(); }\n\n  /// Returns the function called, or null if this is an\n  /// indirect function invocation.\n  Function *getCalledFunction() const {\n    return dyn_cast_or_null<Function>(getCalledOperand());\n  }\n\n  /// Return true if the callsite is an indirect call.\n  bool isIndirectCall() const;\n\n  /// Determine whether the passed iterator points to the callee operand's Use.\n  bool isCallee(Value::const_user_iterator UI) const {\n    return isCallee(&UI.getUse());\n  }\n\n  /// Determine whether this Use is the callee operand's Use.\n  bool isCallee(const Use *U) const { return &getCalledOperandUse() == U; }\n\n  /// Helper to get the caller (the parent function).\n  Function *getCaller();\n  const Function *getCaller() const {\n    return const_cast<CallBase *>(this)->getCaller();\n  }\n\n  /// Tests if this call site must be tail call optimized. Only a CallInst can\n  /// be tail call optimized.\n  bool isMustTailCall() const;\n\n  /// Tests if this call site is marked as a tail call.\n  bool isTailCall() const;\n\n  /// Returns the intrinsic ID of the intrinsic called or\n  /// Intrinsic::not_intrinsic if the called function is not an intrinsic, or if\n  /// this is an indirect call.\n  Intrinsic::ID getIntrinsicID() const;\n\n  void setCalledOperand(Value *V) { Op<CalledOperandOpEndIdx>() = V; }\n\n  /// Sets the function called, including updating the function type.\n  void setCalledFunction(Function *Fn) {\n    setCalledFunction(Fn->getFunctionType(), Fn);\n  }\n\n  /// Sets the function called, including updating the function type.\n  void setCalledFunction(FunctionCallee Fn) {\n    setCalledFunction(Fn.getFunctionType(), Fn.getCallee());\n  }\n\n  /// Sets the function called, including updating to the specified function\n  /// type.\n  void setCalledFunction(FunctionType *FTy, Value *Fn) {\n    this->FTy = FTy;\n    assert(FTy == cast<FunctionType>(\n                      cast<PointerType>(Fn->getType())->getElementType()));\n    // This function doesn't mutate the return type, only the function\n    // type. Seems broken, but I'm just gonna stick an assert in for now.\n    assert(getType() == FTy->getReturnType());\n    setCalledOperand(Fn);\n  }\n\n  CallingConv::ID getCallingConv() const {\n    return getSubclassData<CallingConvField>();\n  }\n\n  void setCallingConv(CallingConv::ID CC) {\n    setSubclassData<CallingConvField>(CC);\n  }\n\n  /// Check if this call is an inline asm statement.\n  bool isInlineAsm() const { return isa<InlineAsm>(getCalledOperand()); }\n\n  /// \\name Attribute API\n  ///\n  /// These methods access and modify attributes on this call (including\n  /// looking through to the attributes on the called function when necessary).\n  ///@{\n\n  /// Return the parameter attributes for this call.\n  ///\n  AttributeList getAttributes() const { return Attrs; }\n\n  /// Set the parameter attributes for this call.\n  ///\n  void setAttributes(AttributeList A) { Attrs = A; }\n\n  /// Determine whether this call has the given attribute. If it does not\n  /// then determine if the called function has the attribute, but only if\n  /// the attribute is allowed for the call.\n  bool hasFnAttr(Attribute::AttrKind Kind) const {\n    assert(Kind != Attribute::NoBuiltin &&\n           \"Use CallBase::isNoBuiltin() to check for Attribute::NoBuiltin\");\n    return hasFnAttrImpl(Kind);\n  }\n\n  /// Determine whether this call has the given attribute. If it does not\n  /// then determine if the called function has the attribute, but only if\n  /// the attribute is allowed for the call.\n  bool hasFnAttr(StringRef Kind) const { return hasFnAttrImpl(Kind); }\n\n  /// adds the attribute to the list of attributes.\n  void addAttribute(unsigned i, Attribute::AttrKind Kind) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addAttribute(getContext(), i, Kind);\n    setAttributes(PAL);\n  }\n\n  /// adds the attribute to the list of attributes.\n  void addAttribute(unsigned i, Attribute Attr) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addAttribute(getContext(), i, Attr);\n    setAttributes(PAL);\n  }\n\n  /// Adds the attribute to the indicated argument\n  void addParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addParamAttribute(getContext(), ArgNo, Kind);\n    setAttributes(PAL);\n  }\n\n  /// Adds the attribute to the indicated argument\n  void addParamAttr(unsigned ArgNo, Attribute Attr) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addParamAttribute(getContext(), ArgNo, Attr);\n    setAttributes(PAL);\n  }\n\n  /// removes the attribute from the list of attributes.\n  void removeAttribute(unsigned i, Attribute::AttrKind Kind) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeAttribute(getContext(), i, Kind);\n    setAttributes(PAL);\n  }\n\n  /// removes the attribute from the list of attributes.\n  void removeAttribute(unsigned i, StringRef Kind) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeAttribute(getContext(), i, Kind);\n    setAttributes(PAL);\n  }\n\n  void removeAttributes(unsigned i, const AttrBuilder &Attrs) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeAttributes(getContext(), i, Attrs);\n    setAttributes(PAL);\n  }\n\n  /// Removes the attribute from the given argument\n  void removeParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeParamAttribute(getContext(), ArgNo, Kind);\n    setAttributes(PAL);\n  }\n\n  /// Removes the attribute from the given argument\n  void removeParamAttr(unsigned ArgNo, StringRef Kind) {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    AttributeList PAL = getAttributes();\n    PAL = PAL.removeParamAttribute(getContext(), ArgNo, Kind);\n    setAttributes(PAL);\n  }\n\n  /// adds the dereferenceable attribute to the list of attributes.\n  void addDereferenceableAttr(unsigned i, uint64_t Bytes) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addDereferenceableAttr(getContext(), i, Bytes);\n    setAttributes(PAL);\n  }\n\n  /// adds the dereferenceable_or_null attribute to the list of\n  /// attributes.\n  void addDereferenceableOrNullAttr(unsigned i, uint64_t Bytes) {\n    AttributeList PAL = getAttributes();\n    PAL = PAL.addDereferenceableOrNullAttr(getContext(), i, Bytes);\n    setAttributes(PAL);\n  }\n\n  /// Determine whether the return value has the given attribute.\n  bool hasRetAttr(Attribute::AttrKind Kind) const {\n    return hasRetAttrImpl(Kind);\n  }\n  /// Determine whether the return value has the given attribute.\n  bool hasRetAttr(StringRef Kind) const { return hasRetAttrImpl(Kind); }\n\n  /// Determine whether the argument or parameter has the given attribute.\n  bool paramHasAttr(unsigned ArgNo, Attribute::AttrKind Kind) const;\n\n  /// Get the attribute of a given kind at a position.\n  Attribute getAttribute(unsigned i, Attribute::AttrKind Kind) const {\n    return getAttributes().getAttribute(i, Kind);\n  }\n\n  /// Get the attribute of a given kind at a position.\n  Attribute getAttribute(unsigned i, StringRef Kind) const {\n    return getAttributes().getAttribute(i, Kind);\n  }\n\n  /// Get the attribute of a given kind from a given arg\n  Attribute getParamAttr(unsigned ArgNo, Attribute::AttrKind Kind) const {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    return getAttributes().getParamAttr(ArgNo, Kind);\n  }\n\n  /// Get the attribute of a given kind from a given arg\n  Attribute getParamAttr(unsigned ArgNo, StringRef Kind) const {\n    assert(ArgNo < getNumArgOperands() && \"Out of bounds\");\n    return getAttributes().getParamAttr(ArgNo, Kind);\n  }\n\n  /// Return true if the data operand at index \\p i has the attribute \\p\n  /// A.\n  ///\n  /// Data operands include call arguments and values used in operand bundles,\n  /// but does not include the callee operand.  This routine dispatches to the\n  /// underlying AttributeList or the OperandBundleUser as appropriate.\n  ///\n  /// The index \\p i is interpreted as\n  ///\n  ///  \\p i == Attribute::ReturnIndex  -> the return value\n  ///  \\p i in [1, arg_size + 1)  -> argument number (\\p i - 1)\n  ///  \\p i in [arg_size + 1, data_operand_size + 1) -> bundle operand at index\n  ///     (\\p i - 1) in the operand list.\n  bool dataOperandHasImpliedAttr(unsigned i, Attribute::AttrKind Kind) const {\n    // Note that we have to add one because `i` isn't zero-indexed.\n    assert(i < (getNumArgOperands() + getNumTotalBundleOperands() + 1) &&\n           \"Data operand index out of bounds!\");\n\n    // The attribute A can either be directly specified, if the operand in\n    // question is a call argument; or be indirectly implied by the kind of its\n    // containing operand bundle, if the operand is a bundle operand.\n\n    if (i == AttributeList::ReturnIndex)\n      return hasRetAttr(Kind);\n\n    // FIXME: Avoid these i - 1 calculations and update the API to use\n    // zero-based indices.\n    if (i < (getNumArgOperands() + 1))\n      return paramHasAttr(i - 1, Kind);\n\n    assert(hasOperandBundles() && i >= (getBundleOperandsStartIndex() + 1) &&\n           \"Must be either a call argument or an operand bundle!\");\n    return bundleOperandHasAttr(i - 1, Kind);\n  }\n\n  /// Determine whether this data operand is not captured.\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool doesNotCapture(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::NoCapture);\n  }\n\n  /// Determine whether this argument is passed by value.\n  bool isByValArgument(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::ByVal);\n  }\n\n  /// Determine whether this argument is passed in an alloca.\n  bool isInAllocaArgument(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::InAlloca);\n  }\n\n  /// Determine whether this argument is passed by value, in an alloca, or is\n  /// preallocated.\n  bool isPassPointeeByValueArgument(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::ByVal) ||\n           paramHasAttr(ArgNo, Attribute::InAlloca) ||\n           paramHasAttr(ArgNo, Attribute::Preallocated);\n  }\n\n  /// Determine whether passing undef to this argument is undefined behavior.\n  /// If passing undef to this argument is UB, passing poison is UB as well\n  /// because poison is more undefined than undef.\n  bool isPassingUndefUB(unsigned ArgNo) const {\n    return paramHasAttr(ArgNo, Attribute::NoUndef) ||\n           // dereferenceable implies noundef.\n           paramHasAttr(ArgNo, Attribute::Dereferenceable) ||\n           // dereferenceable implies noundef, and null is a well-defined value.\n           paramHasAttr(ArgNo, Attribute::DereferenceableOrNull);\n  }\n\n  /// Determine if there are is an inalloca argument. Only the last argument can\n  /// have the inalloca attribute.\n  bool hasInAllocaArgument() const {\n    return !arg_empty() && paramHasAttr(arg_size() - 1, Attribute::InAlloca);\n  }\n\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool doesNotAccessMemory(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadNone);\n  }\n\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool onlyReadsMemory(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadOnly) ||\n           dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadNone);\n  }\n\n  // FIXME: Once this API is no longer duplicated in `CallSite`, rename this to\n  // better indicate that this may return a conservative answer.\n  bool doesNotReadMemory(unsigned OpNo) const {\n    return dataOperandHasImpliedAttr(OpNo + 1, Attribute::WriteOnly) ||\n           dataOperandHasImpliedAttr(OpNo + 1, Attribute::ReadNone);\n  }\n\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned getRetAlignment() const,\n                            \"Use getRetAlign() instead\") {\n    if (const auto MA = Attrs.getRetAlignment())\n      return MA->value();\n    return 0;\n  }\n\n  /// Extract the alignment of the return value.\n  MaybeAlign getRetAlign() const { return Attrs.getRetAlignment(); }\n\n  /// Extract the alignment for a call or parameter (0=unknown).\n  LLVM_ATTRIBUTE_DEPRECATED(unsigned getParamAlignment(unsigned ArgNo) const,\n                            \"Use getParamAlign() instead\") {\n    if (const auto MA = Attrs.getParamAlignment(ArgNo))\n      return MA->value();\n    return 0;\n  }\n\n  /// Extract the alignment for a call or parameter (0=unknown).\n  MaybeAlign getParamAlign(unsigned ArgNo) const {\n    return Attrs.getParamAlignment(ArgNo);\n  }\n\n  /// Extract the byval type for a call or parameter.\n  Type *getParamByValType(unsigned ArgNo) const {\n    Type *Ty = Attrs.getParamByValType(ArgNo);\n    return Ty ? Ty : getArgOperand(ArgNo)->getType()->getPointerElementType();\n  }\n\n  /// Extract the preallocated type for a call or parameter.\n  Type *getParamPreallocatedType(unsigned ArgNo) const {\n    Type *Ty = Attrs.getParamPreallocatedType(ArgNo);\n    return Ty ? Ty : getArgOperand(ArgNo)->getType()->getPointerElementType();\n  }\n\n  /// Extract the number of dereferenceable bytes for a call or\n  /// parameter (0=unknown).\n  uint64_t getDereferenceableBytes(unsigned i) const {\n    return Attrs.getDereferenceableBytes(i);\n  }\n\n  /// Extract the number of dereferenceable_or_null bytes for a call or\n  /// parameter (0=unknown).\n  uint64_t getDereferenceableOrNullBytes(unsigned i) const {\n    return Attrs.getDereferenceableOrNullBytes(i);\n  }\n\n  /// Return true if the return value is known to be not null.\n  /// This may be because it has the nonnull attribute, or because at least\n  /// one byte is dereferenceable and the pointer is in addrspace(0).\n  bool isReturnNonNull() const;\n\n  /// Determine if the return value is marked with NoAlias attribute.\n  bool returnDoesNotAlias() const {\n    return Attrs.hasAttribute(AttributeList::ReturnIndex, Attribute::NoAlias);\n  }\n\n  /// If one of the arguments has the 'returned' attribute, returns its\n  /// operand value. Otherwise, return nullptr.\n  Value *getReturnedArgOperand() const;\n\n  /// Return true if the call should not be treated as a call to a\n  /// builtin.\n  bool isNoBuiltin() const {\n    return hasFnAttrImpl(Attribute::NoBuiltin) &&\n           !hasFnAttrImpl(Attribute::Builtin);\n  }\n\n  /// Determine if the call requires strict floating point semantics.\n  bool isStrictFP() const { return hasFnAttr(Attribute::StrictFP); }\n\n  /// Return true if the call should not be inlined.\n  bool isNoInline() const { return hasFnAttr(Attribute::NoInline); }\n  void setIsNoInline() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoInline);\n  }\n  /// Determine if the call does not access memory.\n  bool doesNotAccessMemory() const { return hasFnAttr(Attribute::ReadNone); }\n  void setDoesNotAccessMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ReadNone);\n  }\n\n  /// Determine if the call does not access or only reads memory.\n  bool onlyReadsMemory() const {\n    return doesNotAccessMemory() || hasFnAttr(Attribute::ReadOnly);\n  }\n\n  void setOnlyReadsMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ReadOnly);\n  }\n\n  /// Determine if the call does not access or only writes memory.\n  bool doesNotReadMemory() const {\n    return doesNotAccessMemory() || hasFnAttr(Attribute::WriteOnly);\n  }\n  void setDoesNotReadMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::WriteOnly);\n  }\n\n  /// Determine if the call can access memmory only using pointers based\n  /// on its arguments.\n  bool onlyAccessesArgMemory() const {\n    return hasFnAttr(Attribute::ArgMemOnly);\n  }\n  void setOnlyAccessesArgMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ArgMemOnly);\n  }\n\n  /// Determine if the function may only access memory that is\n  /// inaccessible from the IR.\n  bool onlyAccessesInaccessibleMemory() const {\n    return hasFnAttr(Attribute::InaccessibleMemOnly);\n  }\n  void setOnlyAccessesInaccessibleMemory() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::InaccessibleMemOnly);\n  }\n\n  /// Determine if the function may only access memory that is\n  /// either inaccessible from the IR or pointed to by its arguments.\n  bool onlyAccessesInaccessibleMemOrArgMem() const {\n    return hasFnAttr(Attribute::InaccessibleMemOrArgMemOnly);\n  }\n  void setOnlyAccessesInaccessibleMemOrArgMem() {\n    addAttribute(AttributeList::FunctionIndex,\n                 Attribute::InaccessibleMemOrArgMemOnly);\n  }\n  /// Determine if the call cannot return.\n  bool doesNotReturn() const { return hasFnAttr(Attribute::NoReturn); }\n  void setDoesNotReturn() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoReturn);\n  }\n\n  /// Determine if the call should not perform indirect branch tracking.\n  bool doesNoCfCheck() const { return hasFnAttr(Attribute::NoCfCheck); }\n\n  /// Determine if the call cannot unwind.\n  bool doesNotThrow() const { return hasFnAttr(Attribute::NoUnwind); }\n  void setDoesNotThrow() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoUnwind);\n  }\n\n  /// Determine if the invoke cannot be duplicated.\n  bool cannotDuplicate() const { return hasFnAttr(Attribute::NoDuplicate); }\n  void setCannotDuplicate() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoDuplicate);\n  }\n\n  /// Determine if the call cannot be tail merged.\n  bool cannotMerge() const { return hasFnAttr(Attribute::NoMerge); }\n  void setCannotMerge() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::NoMerge);\n  }\n\n  /// Determine if the invoke is convergent\n  bool isConvergent() const { return hasFnAttr(Attribute::Convergent); }\n  void setConvergent() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::Convergent);\n  }\n  void setNotConvergent() {\n    removeAttribute(AttributeList::FunctionIndex, Attribute::Convergent);\n  }\n\n  /// Determine if the call returns a structure through first\n  /// pointer argument.\n  bool hasStructRetAttr() const {\n    if (getNumArgOperands() == 0)\n      return false;\n\n    // Be friendly and also check the callee.\n    return paramHasAttr(0, Attribute::StructRet);\n  }\n\n  /// Determine if any call argument is an aggregate passed by value.\n  bool hasByValArgument() const {\n    return Attrs.hasAttrSomewhere(Attribute::ByVal);\n  }\n\n  ///@{\n  // End of attribute API.\n\n  /// \\name Operand Bundle API\n  ///\n  /// This group of methods provides the API to access and manipulate operand\n  /// bundles on this call.\n  /// @{\n\n  /// Return the number of operand bundles associated with this User.\n  unsigned getNumOperandBundles() const {\n    return std::distance(bundle_op_info_begin(), bundle_op_info_end());\n  }\n\n  /// Return true if this User has any operand bundles.\n  bool hasOperandBundles() const { return getNumOperandBundles() != 0; }\n\n  /// Return the index of the first bundle operand in the Use array.\n  unsigned getBundleOperandsStartIndex() const {\n    assert(hasOperandBundles() && \"Don't call otherwise!\");\n    return bundle_op_info_begin()->Begin;\n  }\n\n  /// Return the index of the last bundle operand in the Use array.\n  unsigned getBundleOperandsEndIndex() const {\n    assert(hasOperandBundles() && \"Don't call otherwise!\");\n    return bundle_op_info_end()[-1].End;\n  }\n\n  /// Return true if the operand at index \\p Idx is a bundle operand.\n  bool isBundleOperand(unsigned Idx) const {\n    return hasOperandBundles() && Idx >= getBundleOperandsStartIndex() &&\n           Idx < getBundleOperandsEndIndex();\n  }\n\n  /// Returns true if the use is a bundle operand.\n  bool isBundleOperand(const Use *U) const {\n    assert(this == U->getUser() &&\n           \"Only valid to query with a use of this instruction!\");\n    return hasOperandBundles() && isBundleOperand(U - op_begin());\n  }\n  bool isBundleOperand(Value::const_user_iterator UI) const {\n    return isBundleOperand(&UI.getUse());\n  }\n\n  /// Return the total number operands (not operand bundles) used by\n  /// every operand bundle in this OperandBundleUser.\n  unsigned getNumTotalBundleOperands() const {\n    if (!hasOperandBundles())\n      return 0;\n\n    unsigned Begin = getBundleOperandsStartIndex();\n    unsigned End = getBundleOperandsEndIndex();\n\n    assert(Begin <= End && \"Should be!\");\n    return End - Begin;\n  }\n\n  /// Return the operand bundle at a specific index.\n  OperandBundleUse getOperandBundleAt(unsigned Index) const {\n    assert(Index < getNumOperandBundles() && \"Index out of bounds!\");\n    return operandBundleFromBundleOpInfo(*(bundle_op_info_begin() + Index));\n  }\n\n  /// Return the number of operand bundles with the tag Name attached to\n  /// this instruction.\n  unsigned countOperandBundlesOfType(StringRef Name) const {\n    unsigned Count = 0;\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i)\n      if (getOperandBundleAt(i).getTagName() == Name)\n        Count++;\n\n    return Count;\n  }\n\n  /// Return the number of operand bundles with the tag ID attached to\n  /// this instruction.\n  unsigned countOperandBundlesOfType(uint32_t ID) const {\n    unsigned Count = 0;\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i)\n      if (getOperandBundleAt(i).getTagID() == ID)\n        Count++;\n\n    return Count;\n  }\n\n  /// Return an operand bundle by name, if present.\n  ///\n  /// It is an error to call this for operand bundle types that may have\n  /// multiple instances of them on the same instruction.\n  Optional<OperandBundleUse> getOperandBundle(StringRef Name) const {\n    assert(countOperandBundlesOfType(Name) < 2 && \"Precondition violated!\");\n\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i) {\n      OperandBundleUse U = getOperandBundleAt(i);\n      if (U.getTagName() == Name)\n        return U;\n    }\n\n    return None;\n  }\n\n  /// Return an operand bundle by tag ID, if present.\n  ///\n  /// It is an error to call this for operand bundle types that may have\n  /// multiple instances of them on the same instruction.\n  Optional<OperandBundleUse> getOperandBundle(uint32_t ID) const {\n    assert(countOperandBundlesOfType(ID) < 2 && \"Precondition violated!\");\n\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i) {\n      OperandBundleUse U = getOperandBundleAt(i);\n      if (U.getTagID() == ID)\n        return U;\n    }\n\n    return None;\n  }\n\n  /// Return the list of operand bundles attached to this instruction as\n  /// a vector of OperandBundleDefs.\n  ///\n  /// This function copies the OperandBundeUse instances associated with this\n  /// OperandBundleUser to a vector of OperandBundleDefs.  Note:\n  /// OperandBundeUses and OperandBundleDefs are non-trivially *different*\n  /// representations of operand bundles (see documentation above).\n  void getOperandBundlesAsDefs(SmallVectorImpl<OperandBundleDef> &Defs) const;\n\n  /// Return the operand bundle for the operand at index OpIdx.\n  ///\n  /// It is an error to call this with an OpIdx that does not correspond to an\n  /// bundle operand.\n  OperandBundleUse getOperandBundleForOperand(unsigned OpIdx) const {\n    return operandBundleFromBundleOpInfo(getBundleOpInfoForOperand(OpIdx));\n  }\n\n  /// Return true if this operand bundle user has operand bundles that\n  /// may read from the heap.\n  bool hasReadingOperandBundles() const {\n    // Implementation note: this is a conservative implementation of operand\n    // bundle semantics, where *any* operand bundle forces a callsite to be at\n    // least readonly.\n    return hasOperandBundles();\n  }\n\n  /// Return true if this operand bundle user has operand bundles that\n  /// may write to the heap.\n  bool hasClobberingOperandBundles() const {\n    for (auto &BOI : bundle_op_infos()) {\n      if (BOI.Tag->second == LLVMContext::OB_deopt ||\n          BOI.Tag->second == LLVMContext::OB_funclet)\n        continue;\n\n      // This instruction has an operand bundle that is not known to us.\n      // Assume the worst.\n      return true;\n    }\n\n    return false;\n  }\n\n  /// Return true if the bundle operand at index \\p OpIdx has the\n  /// attribute \\p A.\n  bool bundleOperandHasAttr(unsigned OpIdx,  Attribute::AttrKind A) const {\n    auto &BOI = getBundleOpInfoForOperand(OpIdx);\n    auto OBU = operandBundleFromBundleOpInfo(BOI);\n    return OBU.operandHasAttr(OpIdx - BOI.Begin, A);\n  }\n\n  /// Return true if \\p Other has the same sequence of operand bundle\n  /// tags with the same number of operands on each one of them as this\n  /// OperandBundleUser.\n  bool hasIdenticalOperandBundleSchema(const CallBase &Other) const {\n    if (getNumOperandBundles() != Other.getNumOperandBundles())\n      return false;\n\n    return std::equal(bundle_op_info_begin(), bundle_op_info_end(),\n                      Other.bundle_op_info_begin());\n  }\n\n  /// Return true if this operand bundle user contains operand bundles\n  /// with tags other than those specified in \\p IDs.\n  bool hasOperandBundlesOtherThan(ArrayRef<uint32_t> IDs) const {\n    for (unsigned i = 0, e = getNumOperandBundles(); i != e; ++i) {\n      uint32_t ID = getOperandBundleAt(i).getTagID();\n      if (!is_contained(IDs, ID))\n        return true;\n    }\n    return false;\n  }\n\n  /// Is the function attribute S disallowed by some operand bundle on\n  /// this operand bundle user?\n  bool isFnAttrDisallowedByOpBundle(StringRef S) const {\n    // Operand bundles only possibly disallow readnone, readonly and argmemonly\n    // attributes.  All String attributes are fine.\n    return false;\n  }\n\n  /// Is the function attribute A disallowed by some operand bundle on\n  /// this operand bundle user?\n  bool isFnAttrDisallowedByOpBundle(Attribute::AttrKind A) const {\n    switch (A) {\n    default:\n      return false;\n\n    case Attribute::InaccessibleMemOrArgMemOnly:\n      return hasReadingOperandBundles();\n\n    case Attribute::InaccessibleMemOnly:\n      return hasReadingOperandBundles();\n\n    case Attribute::ArgMemOnly:\n      return hasReadingOperandBundles();\n\n    case Attribute::ReadNone:\n      return hasReadingOperandBundles();\n\n    case Attribute::ReadOnly:\n      return hasClobberingOperandBundles();\n    }\n\n    llvm_unreachable(\"switch has a default case!\");\n  }\n\n  /// Used to keep track of an operand bundle.  See the main comment on\n  /// OperandBundleUser above.\n  struct BundleOpInfo {\n    /// The operand bundle tag, interned by\n    /// LLVMContextImpl::getOrInsertBundleTag.\n    StringMapEntry<uint32_t> *Tag;\n\n    /// The index in the Use& vector where operands for this operand\n    /// bundle starts.\n    uint32_t Begin;\n\n    /// The index in the Use& vector where operands for this operand\n    /// bundle ends.\n    uint32_t End;\n\n    bool operator==(const BundleOpInfo &Other) const {\n      return Tag == Other.Tag && Begin == Other.Begin && End == Other.End;\n    }\n  };\n\n  /// Simple helper function to map a BundleOpInfo to an\n  /// OperandBundleUse.\n  OperandBundleUse\n  operandBundleFromBundleOpInfo(const BundleOpInfo &BOI) const {\n    auto begin = op_begin();\n    ArrayRef<Use> Inputs(begin + BOI.Begin, begin + BOI.End);\n    return OperandBundleUse(BOI.Tag, Inputs);\n  }\n\n  using bundle_op_iterator = BundleOpInfo *;\n  using const_bundle_op_iterator = const BundleOpInfo *;\n\n  /// Return the start of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  ///\n  /// OperandBundleUser uses the descriptor area co-allocated with the host User\n  /// to store some meta information about which operands are \"normal\" operands,\n  /// and which ones belong to some operand bundle.\n  ///\n  /// The layout of an operand bundle user is\n  ///\n  ///          +-----------uint32_t End-------------------------------------+\n  ///          |                                                            |\n  ///          |  +--------uint32_t Begin--------------------+              |\n  ///          |  |                                          |              |\n  ///          ^  ^                                          v              v\n  ///  |------|------|----|----|----|----|----|---------|----|---------|----|-----\n  ///  | BOI0 | BOI1 | .. | DU | U0 | U1 | .. | BOI0_U0 | .. | BOI1_U0 | .. | Un\n  ///  |------|------|----|----|----|----|----|---------|----|---------|----|-----\n  ///   v  v                                  ^              ^\n  ///   |  |                                  |              |\n  ///   |  +--------uint32_t Begin------------+              |\n  ///   |                                                    |\n  ///   +-----------uint32_t End-----------------------------+\n  ///\n  ///\n  /// BOI0, BOI1 ... are descriptions of operand bundles in this User's use\n  /// list. These descriptions are installed and managed by this class, and\n  /// they're all instances of OperandBundleUser<T>::BundleOpInfo.\n  ///\n  /// DU is an additional descriptor installed by User's 'operator new' to keep\n  /// track of the 'BOI0 ... BOIN' co-allocation.  OperandBundleUser does not\n  /// access or modify DU in any way, it's an implementation detail private to\n  /// User.\n  ///\n  /// The regular Use& vector for the User starts at U0.  The operand bundle\n  /// uses are part of the Use& vector, just like normal uses.  In the diagram\n  /// above, the operand bundle uses start at BOI0_U0.  Each instance of\n  /// BundleOpInfo has information about a contiguous set of uses constituting\n  /// an operand bundle, and the total set of operand bundle uses themselves\n  /// form a contiguous set of uses (i.e. there are no gaps between uses\n  /// corresponding to individual operand bundles).\n  ///\n  /// This class does not know the location of the set of operand bundle uses\n  /// within the use list -- that is decided by the User using this class via\n  /// the BeginIdx argument in populateBundleOperandInfos.\n  ///\n  /// Currently operand bundle users with hung-off operands are not supported.\n  bundle_op_iterator bundle_op_info_begin() {\n    if (!hasDescriptor())\n      return nullptr;\n\n    uint8_t *BytesBegin = getDescriptor().begin();\n    return reinterpret_cast<bundle_op_iterator>(BytesBegin);\n  }\n\n  /// Return the start of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  const_bundle_op_iterator bundle_op_info_begin() const {\n    auto *NonConstThis = const_cast<CallBase *>(this);\n    return NonConstThis->bundle_op_info_begin();\n  }\n\n  /// Return the end of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  bundle_op_iterator bundle_op_info_end() {\n    if (!hasDescriptor())\n      return nullptr;\n\n    uint8_t *BytesEnd = getDescriptor().end();\n    return reinterpret_cast<bundle_op_iterator>(BytesEnd);\n  }\n\n  /// Return the end of the list of BundleOpInfo instances associated\n  /// with this OperandBundleUser.\n  const_bundle_op_iterator bundle_op_info_end() const {\n    auto *NonConstThis = const_cast<CallBase *>(this);\n    return NonConstThis->bundle_op_info_end();\n  }\n\n  /// Return the range [\\p bundle_op_info_begin, \\p bundle_op_info_end).\n  iterator_range<bundle_op_iterator> bundle_op_infos() {\n    return make_range(bundle_op_info_begin(), bundle_op_info_end());\n  }\n\n  /// Return the range [\\p bundle_op_info_begin, \\p bundle_op_info_end).\n  iterator_range<const_bundle_op_iterator> bundle_op_infos() const {\n    return make_range(bundle_op_info_begin(), bundle_op_info_end());\n  }\n\n  /// Populate the BundleOpInfo instances and the Use& vector from \\p\n  /// Bundles.  Return the op_iterator pointing to the Use& one past the last\n  /// last bundle operand use.\n  ///\n  /// Each \\p OperandBundleDef instance is tracked by a OperandBundleInfo\n  /// instance allocated in this User's descriptor.\n  op_iterator populateBundleOperandInfos(ArrayRef<OperandBundleDef> Bundles,\n                                         const unsigned BeginIndex);\n\npublic:\n  /// Return the BundleOpInfo for the operand at index OpIdx.\n  ///\n  /// It is an error to call this with an OpIdx that does not correspond to an\n  /// bundle operand.\n  BundleOpInfo &getBundleOpInfoForOperand(unsigned OpIdx);\n  const BundleOpInfo &getBundleOpInfoForOperand(unsigned OpIdx) const {\n    return const_cast<CallBase *>(this)->getBundleOpInfoForOperand(OpIdx);\n  }\n\nprotected:\n  /// Return the total number of values used in \\p Bundles.\n  static unsigned CountBundleInputs(ArrayRef<OperandBundleDef> Bundles) {\n    unsigned Total = 0;\n    for (auto &B : Bundles)\n      Total += B.input_size();\n    return Total;\n  }\n\n  /// @}\n  // End of operand bundle API.\n\nprivate:\n  bool hasFnAttrOnCalledFunction(Attribute::AttrKind Kind) const;\n  bool hasFnAttrOnCalledFunction(StringRef Kind) const;\n\n  template <typename AttrKind> bool hasFnAttrImpl(AttrKind Kind) const {\n    if (Attrs.hasFnAttribute(Kind))\n      return true;\n\n    // Operand bundles override attributes on the called function, but don't\n    // override attributes directly present on the call instruction.\n    if (isFnAttrDisallowedByOpBundle(Kind))\n      return false;\n\n    return hasFnAttrOnCalledFunction(Kind);\n  }\n\n  /// Determine whether the return value has the given attribute. Supports\n  /// Attribute::AttrKind and StringRef as \\p AttrKind types.\n  template <typename AttrKind> bool hasRetAttrImpl(AttrKind Kind) const {\n    if (Attrs.hasAttribute(AttributeList::ReturnIndex, Kind))\n      return true;\n\n    // Look at the callee, if available.\n    if (const Function *F = getCalledFunction())\n      return F->getAttributes().hasAttribute(AttributeList::ReturnIndex, Kind);\n    return false;\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CallBase> : public VariadicOperandTraits<CallBase, 1> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CallBase, Value)\n\n//===----------------------------------------------------------------------===//\n//                           FuncletPadInst Class\n//===----------------------------------------------------------------------===//\nclass FuncletPadInst : public Instruction {\nprivate:\n  FuncletPadInst(const FuncletPadInst &CPI);\n\n  explicit FuncletPadInst(Instruction::FuncletPadOps Op, Value *ParentPad,\n                          ArrayRef<Value *> Args, unsigned Values,\n                          const Twine &NameStr, Instruction *InsertBefore);\n  explicit FuncletPadInst(Instruction::FuncletPadOps Op, Value *ParentPad,\n                          ArrayRef<Value *> Args, unsigned Values,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(Value *ParentPad, ArrayRef<Value *> Args, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n  friend class CatchPadInst;\n  friend class CleanupPadInst;\n\n  FuncletPadInst *cloneImpl() const;\n\npublic:\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// getNumArgOperands - Return the number of funcletpad arguments.\n  ///\n  unsigned getNumArgOperands() const { return getNumOperands() - 1; }\n\n  /// Convenience accessors\n\n  /// Return the outer EH-pad this funclet is nested within.\n  ///\n  /// Note: This returns the associated CatchSwitchInst if this FuncletPadInst\n  /// is a CatchPadInst.\n  Value *getParentPad() const { return Op<-1>(); }\n  void setParentPad(Value *ParentPad) {\n    assert(ParentPad);\n    Op<-1>() = ParentPad;\n  }\n\n  /// getArgOperand/setArgOperand - Return/set the i-th funcletpad argument.\n  ///\n  Value *getArgOperand(unsigned i) const { return getOperand(i); }\n  void setArgOperand(unsigned i, Value *v) { setOperand(i, v); }\n\n  /// arg_operands - iteration adapter for range-for loops.\n  op_range arg_operands() { return op_range(op_begin(), op_end() - 1); }\n\n  /// arg_operands - iteration adapter for range-for loops.\n  const_op_range arg_operands() const {\n    return const_op_range(op_begin(), op_end() - 1);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) { return I->isFuncletPad(); }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<FuncletPadInst>\n    : public VariadicOperandTraits<FuncletPadInst, /*MINARITY=*/1> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(FuncletPadInst, Value)\n\n} // end namespace llvm\n\n#endif // LLVM_IR_INSTRTYPES_H\n"}, "82": {"id": 82, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instruction.h", "content": "//===-- llvm/Instruction.h - Instruction class definition -------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains the declaration of the Instruction class, which is the\n// base class for all of the LLVM instructions.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_INSTRUCTION_H\n#define LLVM_IR_INSTRUCTION_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/Bitfields.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/ilist_node.h\"\n#include \"llvm/IR/DebugLoc.h\"\n#include \"llvm/IR/SymbolTableListTraits.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstdint>\n#include <utility>\n\nnamespace llvm {\n\nclass BasicBlock;\nclass FastMathFlags;\nclass MDNode;\nclass Module;\nstruct AAMDNodes;\n\ntemplate <> struct ilist_alloc_traits<Instruction> {\n  static inline void deleteNode(Instruction *V);\n};\n\nclass Instruction : public User,\n                    public ilist_node_with_parent<Instruction, BasicBlock> {\n  BasicBlock *Parent;\n  DebugLoc DbgLoc;                         // 'dbg' Metadata cache.\n\n  /// Relative order of this instruction in its parent basic block. Used for\n  /// O(1) local dominance checks between instructions.\n  mutable unsigned Order = 0;\n\nprotected:\n  // The 15 first bits of `Value::SubclassData` are available for subclasses of\n  // `Instruction` to use.\n  using OpaqueField = Bitfield::Element<uint16_t, 0, 15>;\n\n  // Template alias so that all Instruction storing alignment use the same\n  // definiton.\n  // Valid alignments are powers of two from 2^0 to 2^MaxAlignmentExponent =\n  // 2^29. We store them as Log2(Alignment), so we need 5 bits to encode the 30\n  // possible values.\n  template <unsigned Offset>\n  using AlignmentBitfieldElementT =\n      typename Bitfield::Element<unsigned, Offset, 5,\n                                 Value::MaxAlignmentExponent>;\n\n  template <unsigned Offset>\n  using BoolBitfieldElementT = typename Bitfield::Element<bool, Offset, 1>;\n\n  template <unsigned Offset>\n  using AtomicOrderingBitfieldElementT =\n      typename Bitfield::Element<AtomicOrdering, Offset, 3,\n                                 AtomicOrdering::LAST>;\n\nprivate:\n  // The last bit is used to store whether the instruction has metadata attached\n  // or not.\n  using HasMetadataField = Bitfield::Element<bool, 15, 1>;\n\nprotected:\n  ~Instruction(); // Use deleteValue() to delete a generic Instruction.\n\npublic:\n  Instruction(const Instruction &) = delete;\n  Instruction &operator=(const Instruction &) = delete;\n\n  /// Specialize the methods defined in Value, as we know that an instruction\n  /// can only be used by other instructions.\n  Instruction       *user_back()       { return cast<Instruction>(*user_begin());}\n  const Instruction *user_back() const { return cast<Instruction>(*user_begin());}\n\n  inline const BasicBlock *getParent() const { return Parent; }\n  inline       BasicBlock *getParent()       { return Parent; }\n\n  /// Return the module owning the function this instruction belongs to\n  /// or nullptr it the function does not have a module.\n  ///\n  /// Note: this is undefined behavior if the instruction does not have a\n  /// parent, or the parent basic block does not have a parent function.\n  const Module *getModule() const;\n  Module *getModule() {\n    return const_cast<Module *>(\n                           static_cast<const Instruction *>(this)->getModule());\n  }\n\n  /// Return the function this instruction belongs to.\n  ///\n  /// Note: it is undefined behavior to call this on an instruction not\n  /// currently inserted into a function.\n  const Function *getFunction() const;\n  Function *getFunction() {\n    return const_cast<Function *>(\n                         static_cast<const Instruction *>(this)->getFunction());\n  }\n\n  /// This method unlinks 'this' from the containing basic block, but does not\n  /// delete it.\n  void removeFromParent();\n\n  /// This method unlinks 'this' from the containing basic block and deletes it.\n  ///\n  /// \\returns an iterator pointing to the element after the erased one\n  SymbolTableList<Instruction>::iterator eraseFromParent();\n\n  /// Insert an unlinked instruction into a basic block immediately before\n  /// the specified instruction.\n  void insertBefore(Instruction *InsertPos);\n\n  /// Insert an unlinked instruction into a basic block immediately after the\n  /// specified instruction.\n  void insertAfter(Instruction *InsertPos);\n\n  /// Unlink this instruction from its current basic block and insert it into\n  /// the basic block that MovePos lives in, right before MovePos.\n  void moveBefore(Instruction *MovePos);\n\n  /// Unlink this instruction and insert into BB before I.\n  ///\n  /// \\pre I is a valid iterator into BB.\n  void moveBefore(BasicBlock &BB, SymbolTableList<Instruction>::iterator I);\n\n  /// Unlink this instruction from its current basic block and insert it into\n  /// the basic block that MovePos lives in, right after MovePos.\n  void moveAfter(Instruction *MovePos);\n\n  /// Given an instruction Other in the same basic block as this instruction,\n  /// return true if this instruction comes before Other. In this worst case,\n  /// this takes linear time in the number of instructions in the block. The\n  /// results are cached, so in common cases when the block remains unmodified,\n  /// it takes constant time.\n  bool comesBefore(const Instruction *Other) const;\n\n  //===--------------------------------------------------------------------===//\n  // Subclass classification.\n  //===--------------------------------------------------------------------===//\n\n  /// Returns a member of one of the enums like Instruction::Add.\n  unsigned getOpcode() const { return getValueID() - InstructionVal; }\n\n  const char *getOpcodeName() const { return getOpcodeName(getOpcode()); }\n  bool isTerminator() const { return isTerminator(getOpcode()); }\n  bool isUnaryOp() const { return isUnaryOp(getOpcode()); }\n  bool isBinaryOp() const { return isBinaryOp(getOpcode()); }\n  bool isIntDivRem() const { return isIntDivRem(getOpcode()); }\n  bool isShift() const { return isShift(getOpcode()); }\n  bool isCast() const { return isCast(getOpcode()); }\n  bool isFuncletPad() const { return isFuncletPad(getOpcode()); }\n  bool isExceptionalTerminator() const {\n    return isExceptionalTerminator(getOpcode());\n  }\n  bool isIndirectTerminator() const {\n    return isIndirectTerminator(getOpcode());\n  }\n\n  static const char* getOpcodeName(unsigned OpCode);\n\n  static inline bool isTerminator(unsigned OpCode) {\n    return OpCode >= TermOpsBegin && OpCode < TermOpsEnd;\n  }\n\n  static inline bool isUnaryOp(unsigned Opcode) {\n    return Opcode >= UnaryOpsBegin && Opcode < UnaryOpsEnd;\n  }\n  static inline bool isBinaryOp(unsigned Opcode) {\n    return Opcode >= BinaryOpsBegin && Opcode < BinaryOpsEnd;\n  }\n\n  static inline bool isIntDivRem(unsigned Opcode) {\n    return Opcode == UDiv || Opcode == SDiv || Opcode == URem || Opcode == SRem;\n  }\n\n  /// Determine if the Opcode is one of the shift instructions.\n  static inline bool isShift(unsigned Opcode) {\n    return Opcode >= Shl && Opcode <= AShr;\n  }\n\n  /// Return true if this is a logical shift left or a logical shift right.\n  inline bool isLogicalShift() const {\n    return getOpcode() == Shl || getOpcode() == LShr;\n  }\n\n  /// Return true if this is an arithmetic shift right.\n  inline bool isArithmeticShift() const {\n    return getOpcode() == AShr;\n  }\n\n  /// Determine if the Opcode is and/or/xor.\n  static inline bool isBitwiseLogicOp(unsigned Opcode) {\n    return Opcode == And || Opcode == Or || Opcode == Xor;\n  }\n\n  /// Return true if this is and/or/xor.\n  inline bool isBitwiseLogicOp() const {\n    return isBitwiseLogicOp(getOpcode());\n  }\n\n  /// Determine if the OpCode is one of the CastInst instructions.\n  static inline bool isCast(unsigned OpCode) {\n    return OpCode >= CastOpsBegin && OpCode < CastOpsEnd;\n  }\n\n  /// Determine if the OpCode is one of the FuncletPadInst instructions.\n  static inline bool isFuncletPad(unsigned OpCode) {\n    return OpCode >= FuncletPadOpsBegin && OpCode < FuncletPadOpsEnd;\n  }\n\n  /// Returns true if the OpCode is a terminator related to exception handling.\n  static inline bool isExceptionalTerminator(unsigned OpCode) {\n    switch (OpCode) {\n    case Instruction::CatchSwitch:\n    case Instruction::CatchRet:\n    case Instruction::CleanupRet:\n    case Instruction::Invoke:\n    case Instruction::Resume:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  /// Returns true if the OpCode is a terminator with indirect targets.\n  static inline bool isIndirectTerminator(unsigned OpCode) {\n    switch (OpCode) {\n    case Instruction::IndirectBr:\n    case Instruction::CallBr:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  //===--------------------------------------------------------------------===//\n  // Metadata manipulation.\n  //===--------------------------------------------------------------------===//\n\n  /// Return true if this instruction has any metadata attached to it.\n  bool hasMetadata() const { return DbgLoc || Value::hasMetadata(); }\n\n  /// Return true if this instruction has metadata attached to it other than a\n  /// debug location.\n  bool hasMetadataOtherThanDebugLoc() const { return Value::hasMetadata(); }\n\n  /// Return true if this instruction has the given type of metadata attached.\n  bool hasMetadata(unsigned KindID) const {\n    return getMetadata(KindID) != nullptr;\n  }\n\n  /// Return true if this instruction has the given type of metadata attached.\n  bool hasMetadata(StringRef Kind) const {\n    return getMetadata(Kind) != nullptr;\n  }\n\n  /// Get the metadata of given kind attached to this Instruction.\n  /// If the metadata is not found then return null.\n  MDNode *getMetadata(unsigned KindID) const {\n    if (!hasMetadata()) return nullptr;\n    return getMetadataImpl(KindID);\n  }\n\n  /// Get the metadata of given kind attached to this Instruction.\n  /// If the metadata is not found then return null.\n  MDNode *getMetadata(StringRef Kind) const {\n    if (!hasMetadata()) return nullptr;\n    return getMetadataImpl(Kind);\n  }\n\n  /// Get all metadata attached to this Instruction. The first element of each\n  /// pair returned is the KindID, the second element is the metadata value.\n  /// This list is returned sorted by the KindID.\n  void\n  getAllMetadata(SmallVectorImpl<std::pair<unsigned, MDNode *>> &MDs) const {\n    if (hasMetadata())\n      getAllMetadataImpl(MDs);\n  }\n\n  /// This does the same thing as getAllMetadata, except that it filters out the\n  /// debug location.\n  void getAllMetadataOtherThanDebugLoc(\n      SmallVectorImpl<std::pair<unsigned, MDNode *>> &MDs) const {\n    Value::getAllMetadata(MDs);\n  }\n\n  /// Fills the AAMDNodes structure with AA metadata from this instruction.\n  /// When Merge is true, the existing AA metadata is merged with that from this\n  /// instruction providing the most-general result.\n  void getAAMetadata(AAMDNodes &N, bool Merge = false) const;\n\n  /// Set the metadata of the specified kind to the specified node. This updates\n  /// or replaces metadata if already present, or removes it if Node is null.\n  void setMetadata(unsigned KindID, MDNode *Node);\n  void setMetadata(StringRef Kind, MDNode *Node);\n\n  /// Copy metadata from \\p SrcInst to this instruction. \\p WL, if not empty,\n  /// specifies the list of meta data that needs to be copied. If \\p WL is\n  /// empty, all meta data will be copied.\n  void copyMetadata(const Instruction &SrcInst,\n                    ArrayRef<unsigned> WL = ArrayRef<unsigned>());\n\n  /// If the instruction has \"branch_weights\" MD_prof metadata and the MDNode\n  /// has three operands (including name string), swap the order of the\n  /// metadata.\n  void swapProfMetadata();\n\n  /// Drop all unknown metadata except for debug locations.\n  /// @{\n  /// Passes are required to drop metadata they don't understand. This is a\n  /// convenience method for passes to do so.\n  void dropUnknownNonDebugMetadata(ArrayRef<unsigned> KnownIDs);\n  void dropUnknownNonDebugMetadata() {\n    return dropUnknownNonDebugMetadata(None);\n  }\n  void dropUnknownNonDebugMetadata(unsigned ID1) {\n    return dropUnknownNonDebugMetadata(makeArrayRef(ID1));\n  }\n  void dropUnknownNonDebugMetadata(unsigned ID1, unsigned ID2) {\n    unsigned IDs[] = {ID1, ID2};\n    return dropUnknownNonDebugMetadata(IDs);\n  }\n  /// @}\n\n  /// Adds an !annotation metadata node with \\p Annotation to this instruction.\n  /// If this instruction already has !annotation metadata, append \\p Annotation\n  /// to the existing node.\n  void addAnnotationMetadata(StringRef Annotation);\n\n  /// Sets the metadata on this instruction from the AAMDNodes structure.\n  void setAAMetadata(const AAMDNodes &N);\n\n  /// Retrieve the raw weight values of a conditional branch or select.\n  /// Returns true on success with profile weights filled in.\n  /// Returns false if no metadata or invalid metadata was found.\n  bool extractProfMetadata(uint64_t &TrueVal, uint64_t &FalseVal) const;\n\n  /// Retrieve total raw weight values of a branch.\n  /// Returns true on success with profile total weights filled in.\n  /// Returns false if no metadata was found.\n  bool extractProfTotalWeight(uint64_t &TotalVal) const;\n\n  /// Set the debug location information for this instruction.\n  void setDebugLoc(DebugLoc Loc) { DbgLoc = std::move(Loc); }\n\n  /// Return the debug location for this node as a DebugLoc.\n  const DebugLoc &getDebugLoc() const { return DbgLoc; }\n\n  /// Set or clear the nuw flag on this instruction, which must be an operator\n  /// which supports this flag. See LangRef.html for the meaning of this flag.\n  void setHasNoUnsignedWrap(bool b = true);\n\n  /// Set or clear the nsw flag on this instruction, which must be an operator\n  /// which supports this flag. See LangRef.html for the meaning of this flag.\n  void setHasNoSignedWrap(bool b = true);\n\n  /// Set or clear the exact flag on this instruction, which must be an operator\n  /// which supports this flag. See LangRef.html for the meaning of this flag.\n  void setIsExact(bool b = true);\n\n  /// Determine whether the no unsigned wrap flag is set.\n  bool hasNoUnsignedWrap() const;\n\n  /// Determine whether the no signed wrap flag is set.\n  bool hasNoSignedWrap() const;\n\n  /// Drops flags that may cause this instruction to evaluate to poison despite\n  /// having non-poison inputs.\n  void dropPoisonGeneratingFlags();\n\n  /// Determine whether the exact flag is set.\n  bool isExact() const;\n\n  /// Set or clear all fast-math-flags on this instruction, which must be an\n  /// operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setFast(bool B);\n\n  /// Set or clear the reassociation flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasAllowReassoc(bool B);\n\n  /// Set or clear the no-nans flag on this instruction, which must be an\n  /// operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasNoNaNs(bool B);\n\n  /// Set or clear the no-infs flag on this instruction, which must be an\n  /// operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasNoInfs(bool B);\n\n  /// Set or clear the no-signed-zeros flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasNoSignedZeros(bool B);\n\n  /// Set or clear the allow-reciprocal flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasAllowReciprocal(bool B);\n\n  /// Set or clear the allow-contract flag on this instruction, which must be\n  /// an operator which supports this flag. See LangRef.html for the meaning of\n  /// this flag.\n  void setHasAllowContract(bool B);\n\n  /// Set or clear the approximate-math-functions flag on this instruction,\n  /// which must be an operator which supports this flag. See LangRef.html for\n  /// the meaning of this flag.\n  void setHasApproxFunc(bool B);\n\n  /// Convenience function for setting multiple fast-math flags on this\n  /// instruction, which must be an operator which supports these flags. See\n  /// LangRef.html for the meaning of these flags.\n  void setFastMathFlags(FastMathFlags FMF);\n\n  /// Convenience function for transferring all fast-math flag values to this\n  /// instruction, which must be an operator which supports these flags. See\n  /// LangRef.html for the meaning of these flags.\n  void copyFastMathFlags(FastMathFlags FMF);\n\n  /// Determine whether all fast-math-flags are set.\n  bool isFast() const;\n\n  /// Determine whether the allow-reassociation flag is set.\n  bool hasAllowReassoc() const;\n\n  /// Determine whether the no-NaNs flag is set.\n  bool hasNoNaNs() const;\n\n  /// Determine whether the no-infs flag is set.\n  bool hasNoInfs() const;\n\n  /// Determine whether the no-signed-zeros flag is set.\n  bool hasNoSignedZeros() const;\n\n  /// Determine whether the allow-reciprocal flag is set.\n  bool hasAllowReciprocal() const;\n\n  /// Determine whether the allow-contract flag is set.\n  bool hasAllowContract() const;\n\n  /// Determine whether the approximate-math-functions flag is set.\n  bool hasApproxFunc() const;\n\n  /// Convenience function for getting all the fast-math flags, which must be an\n  /// operator which supports these flags. See LangRef.html for the meaning of\n  /// these flags.\n  FastMathFlags getFastMathFlags() const;\n\n  /// Copy I's fast-math flags\n  void copyFastMathFlags(const Instruction *I);\n\n  /// Convenience method to copy supported exact, fast-math, and (optionally)\n  /// wrapping flags from V to this instruction.\n  void copyIRFlags(const Value *V, bool IncludeWrapFlags = true);\n\n  /// Logical 'and' of any supported wrapping, exact, and fast-math flags of\n  /// V and this instruction.\n  void andIRFlags(const Value *V);\n\n  /// Merge 2 debug locations and apply it to the Instruction. If the\n  /// instruction is a CallIns, we need to traverse the inline chain to find\n  /// the common scope. This is not efficient for N-way merging as each time\n  /// you merge 2 iterations, you need to rebuild the hashmap to find the\n  /// common scope. However, we still choose this API because:\n  ///  1) Simplicity: it takes 2 locations instead of a list of locations.\n  ///  2) In worst case, it increases the complexity from O(N*I) to\n  ///     O(2*N*I), where N is # of Instructions to merge, and I is the\n  ///     maximum level of inline stack. So it is still linear.\n  ///  3) Merging of call instructions should be extremely rare in real\n  ///     applications, thus the N-way merging should be in code path.\n  /// The DebugLoc attached to this instruction will be overwritten by the\n  /// merged DebugLoc.\n  void applyMergedLocation(const DILocation *LocA, const DILocation *LocB);\n\n  /// Updates the debug location given that the instruction has been hoisted\n  /// from a block to a predecessor of that block.\n  /// Note: it is undefined behavior to call this on an instruction not\n  /// currently inserted into a function.\n  void updateLocationAfterHoist();\n\n  /// Drop the instruction's debug location. This does not guarantee removal\n  /// of the !dbg source location attachment, as it must set a line 0 location\n  /// with scope information attached on call instructions. To guarantee\n  /// removal of the !dbg attachment, use the \\ref setDebugLoc() API.\n  /// Note: it is undefined behavior to call this on an instruction not\n  /// currently inserted into a function.\n  void dropLocation();\n\nprivate:\n  // These are all implemented in Metadata.cpp.\n  MDNode *getMetadataImpl(unsigned KindID) const;\n  MDNode *getMetadataImpl(StringRef Kind) const;\n  void\n  getAllMetadataImpl(SmallVectorImpl<std::pair<unsigned, MDNode *>> &) const;\n\npublic:\n  //===--------------------------------------------------------------------===//\n  // Predicates and helper methods.\n  //===--------------------------------------------------------------------===//\n\n  /// Return true if the instruction is associative:\n  ///\n  ///   Associative operators satisfy:  x op (y op z) === (x op y) op z\n  ///\n  /// In LLVM, the Add, Mul, And, Or, and Xor operators are associative.\n  ///\n  bool isAssociative() const LLVM_READONLY;\n  static bool isAssociative(unsigned Opcode) {\n    return Opcode == And || Opcode == Or || Opcode == Xor ||\n           Opcode == Add || Opcode == Mul;\n  }\n\n  /// Return true if the instruction is commutative:\n  ///\n  ///   Commutative operators satisfy: (x op y) === (y op x)\n  ///\n  /// In LLVM, these are the commutative operators, plus SetEQ and SetNE, when\n  /// applied to any type.\n  ///\n  bool isCommutative() const LLVM_READONLY;\n  static bool isCommutative(unsigned Opcode) {\n    switch (Opcode) {\n    case Add: case FAdd:\n    case Mul: case FMul:\n    case And: case Or: case Xor:\n      return true;\n    default:\n      return false;\n  }\n  }\n\n  /// Return true if the instruction is idempotent:\n  ///\n  ///   Idempotent operators satisfy:  x op x === x\n  ///\n  /// In LLVM, the And and Or operators are idempotent.\n  ///\n  bool isIdempotent() const { return isIdempotent(getOpcode()); }\n  static bool isIdempotent(unsigned Opcode) {\n    return Opcode == And || Opcode == Or;\n  }\n\n  /// Return true if the instruction is nilpotent:\n  ///\n  ///   Nilpotent operators satisfy:  x op x === Id,\n  ///\n  ///   where Id is the identity for the operator, i.e. a constant such that\n  ///     x op Id === x and Id op x === x for all x.\n  ///\n  /// In LLVM, the Xor operator is nilpotent.\n  ///\n  bool isNilpotent() const { return isNilpotent(getOpcode()); }\n  static bool isNilpotent(unsigned Opcode) {\n    return Opcode == Xor;\n  }\n\n  /// Return true if this instruction may modify memory.\n  bool mayWriteToMemory() const;\n\n  /// Return true if this instruction may read memory.\n  bool mayReadFromMemory() const;\n\n  /// Return true if this instruction may read or write memory.\n  bool mayReadOrWriteMemory() const {\n    return mayReadFromMemory() || mayWriteToMemory();\n  }\n\n  /// Return true if this instruction has an AtomicOrdering of unordered or\n  /// higher.\n  bool isAtomic() const;\n\n  /// Return true if this atomic instruction loads from memory.\n  bool hasAtomicLoad() const;\n\n  /// Return true if this atomic instruction stores to memory.\n  bool hasAtomicStore() const;\n\n  /// Return true if this instruction may throw an exception.\n  bool mayThrow() const;\n\n  /// Return true if this instruction behaves like a memory fence: it can load\n  /// or store to memory location without being given a memory location.\n  bool isFenceLike() const {\n    switch (getOpcode()) {\n    default:\n      return false;\n    // This list should be kept in sync with the list in mayWriteToMemory for\n    // all opcodes which don't have a memory location.\n    case Instruction::Fence:\n    case Instruction::CatchPad:\n    case Instruction::CatchRet:\n    case Instruction::Call:\n    case Instruction::Invoke:\n      return true;\n    }\n  }\n\n  /// Return true if the instruction may have side effects.\n  ///\n  /// Note that this does not consider malloc and alloca to have side\n  /// effects because the newly allocated memory is completely invisible to\n  /// instructions which don't use the returned value.  For cases where this\n  /// matters, isSafeToSpeculativelyExecute may be more appropriate.\n  bool mayHaveSideEffects() const { return mayWriteToMemory() || mayThrow(); }\n\n  /// Return true if the instruction can be removed if the result is unused.\n  ///\n  /// When constant folding some instructions cannot be removed even if their\n  /// results are unused. Specifically terminator instructions and calls that\n  /// may have side effects cannot be removed without semantically changing the\n  /// generated program.\n  bool isSafeToRemove() const;\n\n  /// Return true if the instruction will return (unwinding is considered as\n  /// a form of returning control flow here).\n  bool willReturn() const;\n\n  /// Return true if the instruction is a variety of EH-block.\n  bool isEHPad() const {\n    switch (getOpcode()) {\n    case Instruction::CatchSwitch:\n    case Instruction::CatchPad:\n    case Instruction::CleanupPad:\n    case Instruction::LandingPad:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  /// Return true if the instruction is a llvm.lifetime.start or\n  /// llvm.lifetime.end marker.\n  bool isLifetimeStartOrEnd() const;\n\n  /// Return true if the instruction is a DbgInfoIntrinsic or PseudoProbeInst.\n  bool isDebugOrPseudoInst() const;\n\n  /// Return a pointer to the next non-debug instruction in the same basic\n  /// block as 'this', or nullptr if no such instruction exists. Skip any pseudo\n  /// operations if \\c SkipPseudoOp is true.\n  const Instruction *\n  getNextNonDebugInstruction(bool SkipPseudoOp = false) const;\n  Instruction *getNextNonDebugInstruction(bool SkipPseudoOp = false) {\n    return const_cast<Instruction *>(\n        static_cast<const Instruction *>(this)->getNextNonDebugInstruction(\n            SkipPseudoOp));\n  }\n\n  /// Return a pointer to the previous non-debug instruction in the same basic\n  /// block as 'this', or nullptr if no such instruction exists. Skip any pseudo\n  /// operations if \\c SkipPseudoOp is true.\n  const Instruction *\n  getPrevNonDebugInstruction(bool SkipPseudoOp = false) const;\n  Instruction *getPrevNonDebugInstruction(bool SkipPseudoOp = false) {\n    return const_cast<Instruction *>(\n        static_cast<const Instruction *>(this)->getPrevNonDebugInstruction(\n            SkipPseudoOp));\n  }\n\n  /// Create a copy of 'this' instruction that is identical in all ways except\n  /// the following:\n  ///   * The instruction has no parent\n  ///   * The instruction has no name\n  ///\n  Instruction *clone() const;\n\n  /// Return true if the specified instruction is exactly identical to the\n  /// current one. This means that all operands match and any extra information\n  /// (e.g. load is volatile) agree.\n  bool isIdenticalTo(const Instruction *I) const;\n\n  /// This is like isIdenticalTo, except that it ignores the\n  /// SubclassOptionalData flags, which may specify conditions under which the\n  /// instruction's result is undefined.\n  bool isIdenticalToWhenDefined(const Instruction *I) const;\n\n  /// When checking for operation equivalence (using isSameOperationAs) it is\n  /// sometimes useful to ignore certain attributes.\n  enum OperationEquivalenceFlags {\n    /// Check for equivalence ignoring load/store alignment.\n    CompareIgnoringAlignment = 1<<0,\n    /// Check for equivalence treating a type and a vector of that type\n    /// as equivalent.\n    CompareUsingScalarTypes = 1<<1\n  };\n\n  /// This function determines if the specified instruction executes the same\n  /// operation as the current one. This means that the opcodes, type, operand\n  /// types and any other factors affecting the operation must be the same. This\n  /// is similar to isIdenticalTo except the operands themselves don't have to\n  /// be identical.\n  /// @returns true if the specified instruction is the same operation as\n  /// the current one.\n  /// Determine if one instruction is the same operation as another.\n  bool isSameOperationAs(const Instruction *I, unsigned flags = 0) const;\n\n  /// Return true if there are any uses of this instruction in blocks other than\n  /// the specified block. Note that PHI nodes are considered to evaluate their\n  /// operands in the corresponding predecessor block.\n  bool isUsedOutsideOfBlock(const BasicBlock *BB) const;\n\n  /// Return the number of successors that this instruction has. The instruction\n  /// must be a terminator.\n  unsigned getNumSuccessors() const;\n\n  /// Return the specified successor. This instruction must be a terminator.\n  BasicBlock *getSuccessor(unsigned Idx) const;\n\n  /// Update the specified successor to point at the provided block. This\n  /// instruction must be a terminator.\n  void setSuccessor(unsigned Idx, BasicBlock *BB);\n\n  /// Replace specified successor OldBB to point at the provided block.\n  /// This instruction must be a terminator.\n  void replaceSuccessorWith(BasicBlock *OldBB, BasicBlock *NewBB);\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Value *V) {\n    return V->getValueID() >= Value::InstructionVal;\n  }\n\n  //----------------------------------------------------------------------\n  // Exported enumerations.\n  //\n  enum TermOps {       // These terminate basic blocks\n#define  FIRST_TERM_INST(N)             TermOpsBegin = N,\n#define HANDLE_TERM_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_TERM_INST(N)             TermOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum UnaryOps {\n#define  FIRST_UNARY_INST(N)             UnaryOpsBegin = N,\n#define HANDLE_UNARY_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_UNARY_INST(N)             UnaryOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum BinaryOps {\n#define  FIRST_BINARY_INST(N)             BinaryOpsBegin = N,\n#define HANDLE_BINARY_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_BINARY_INST(N)             BinaryOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum MemoryOps {\n#define  FIRST_MEMORY_INST(N)             MemoryOpsBegin = N,\n#define HANDLE_MEMORY_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_MEMORY_INST(N)             MemoryOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum CastOps {\n#define  FIRST_CAST_INST(N)             CastOpsBegin = N,\n#define HANDLE_CAST_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_CAST_INST(N)             CastOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum FuncletPadOps {\n#define  FIRST_FUNCLETPAD_INST(N)             FuncletPadOpsBegin = N,\n#define HANDLE_FUNCLETPAD_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_FUNCLETPAD_INST(N)             FuncletPadOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\n  enum OtherOps {\n#define  FIRST_OTHER_INST(N)             OtherOpsBegin = N,\n#define HANDLE_OTHER_INST(N, OPC, CLASS) OPC = N,\n#define   LAST_OTHER_INST(N)             OtherOpsEnd = N+1\n#include \"llvm/IR/Instruction.def\"\n  };\n\nprivate:\n  friend class SymbolTableListTraits<Instruction>;\n  friend class BasicBlock; // For renumbering.\n\n  // Shadow Value::setValueSubclassData with a private forwarding method so that\n  // subclasses cannot accidentally use it.\n  void setValueSubclassData(unsigned short D) {\n    Value::setValueSubclassData(D);\n  }\n\n  unsigned short getSubclassDataFromValue() const {\n    return Value::getSubclassDataFromValue();\n  }\n\n  void setParent(BasicBlock *P);\n\nprotected:\n  // Instruction subclasses can stick up to 15 bits of stuff into the\n  // SubclassData field of instruction with these members.\n\n  template <typename BitfieldElement>\n  typename BitfieldElement::Type getSubclassData() const {\n    static_assert(\n        std::is_same<BitfieldElement, HasMetadataField>::value ||\n            !Bitfield::isOverlapping<BitfieldElement, HasMetadataField>(),\n        \"Must not overlap with the metadata bit\");\n    return Bitfield::get<BitfieldElement>(getSubclassDataFromValue());\n  }\n\n  template <typename BitfieldElement>\n  void setSubclassData(typename BitfieldElement::Type Value) {\n    static_assert(\n        std::is_same<BitfieldElement, HasMetadataField>::value ||\n            !Bitfield::isOverlapping<BitfieldElement, HasMetadataField>(),\n        \"Must not overlap with the metadata bit\");\n    auto Storage = getSubclassDataFromValue();\n    Bitfield::set<BitfieldElement>(Storage, Value);\n    setValueSubclassData(Storage);\n  }\n\n  Instruction(Type *Ty, unsigned iType, Use *Ops, unsigned NumOps,\n              Instruction *InsertBefore = nullptr);\n  Instruction(Type *Ty, unsigned iType, Use *Ops, unsigned NumOps,\n              BasicBlock *InsertAtEnd);\n\nprivate:\n  /// Create a copy of this instruction.\n  Instruction *cloneImpl() const;\n};\n\ninline void ilist_alloc_traits<Instruction>::deleteNode(Instruction *V) {\n  V->deleteValue();\n}\n\n} // end namespace llvm\n\n#endif // LLVM_IR_INSTRUCTION_H\n"}, "83": {"id": 83, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "content": "//===- llvm/Instructions.h - Instruction subclass definitions ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file exposes the class definitions of all of the subclasses of the\n// Instruction class.  This is meant to be an easy way to get access to all\n// instruction subclasses.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_INSTRUCTIONS_H\n#define LLVM_IR_INSTRUCTIONS_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/Bitfields.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/Twine.h\"\n#include \"llvm/ADT/iterator.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/IR/Attributes.h\"\n#include \"llvm/IR/BasicBlock.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/CFG.h\"\n#include \"llvm/IR/Constant.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/OperandTraits.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/IR/Use.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/Support/AtomicOrdering.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <iterator>\n\nnamespace llvm {\n\nclass APInt;\nclass ConstantInt;\nclass DataLayout;\nclass LLVMContext;\n\n//===----------------------------------------------------------------------===//\n//                                AllocaInst Class\n//===----------------------------------------------------------------------===//\n\n/// an instruction to allocate memory on the stack\nclass AllocaInst : public UnaryInstruction {\n  Type *AllocatedType;\n\n  using AlignmentField = AlignmentBitfieldElementT<0>;\n  using UsedWithInAllocaField = BoolBitfieldElementT<AlignmentField::NextBit>;\n  using SwiftErrorField = BoolBitfieldElementT<UsedWithInAllocaField::NextBit>;\n  static_assert(Bitfield::areContiguous<AlignmentField, UsedWithInAllocaField,\n                                        SwiftErrorField>(),\n                \"Bitfields must be contiguous\");\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  AllocaInst *cloneImpl() const;\n\npublic:\n  explicit AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize,\n                      const Twine &Name, Instruction *InsertBefore);\n  AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize,\n             const Twine &Name, BasicBlock *InsertAtEnd);\n\n  AllocaInst(Type *Ty, unsigned AddrSpace, const Twine &Name,\n             Instruction *InsertBefore);\n  AllocaInst(Type *Ty, unsigned AddrSpace,\n             const Twine &Name, BasicBlock *InsertAtEnd);\n\n  AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize, Align Align,\n             const Twine &Name = \"\", Instruction *InsertBefore = nullptr);\n  AllocaInst(Type *Ty, unsigned AddrSpace, Value *ArraySize, Align Align,\n             const Twine &Name, BasicBlock *InsertAtEnd);\n\n  /// Return true if there is an allocation size parameter to the allocation\n  /// instruction that is not 1.\n  bool isArrayAllocation() const;\n\n  /// Get the number of elements allocated. For a simple allocation of a single\n  /// element, this will return a constant 1 value.\n  const Value *getArraySize() const { return getOperand(0); }\n  Value *getArraySize() { return getOperand(0); }\n\n  /// Overload to return most specific pointer type.\n  PointerType *getType() const {\n    return cast<PointerType>(Instruction::getType());\n  }\n\n  /// Get allocation size in bits. Returns None if size can't be determined,\n  /// e.g. in case of a VLA.\n  Optional<TypeSize> getAllocationSizeInBits(const DataLayout &DL) const;\n\n  /// Return the type that is being allocated by the instruction.\n  Type *getAllocatedType() const { return AllocatedType; }\n  /// for use only in special circumstances that need to generically\n  /// transform a whole instruction (eg: IR linking and vectorization).\n  void setAllocatedType(Type *Ty) { AllocatedType = Ty; }\n\n  /// Return the alignment of the memory that is being allocated by the\n  /// instruction.\n  Align getAlign() const {\n    return Align(1ULL << getSubclassData<AlignmentField>());\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  // FIXME: Remove this one transition to Align is over.\n  unsigned getAlignment() const { return getAlign().value(); }\n\n  /// Return true if this alloca is in the entry block of the function and is a\n  /// constant size. If so, the code generator will fold it into the\n  /// prolog/epilog code, so it is basically free.\n  bool isStaticAlloca() const;\n\n  /// Return true if this alloca is used as an inalloca argument to a call. Such\n  /// allocas are never considered static even if they are in the entry block.\n  bool isUsedWithInAlloca() const {\n    return getSubclassData<UsedWithInAllocaField>();\n  }\n\n  /// Specify whether this alloca is used to represent the arguments to a call.\n  void setUsedWithInAlloca(bool V) {\n    setSubclassData<UsedWithInAllocaField>(V);\n  }\n\n  /// Return true if this alloca is used as a swifterror argument to a call.\n  bool isSwiftError() const { return getSubclassData<SwiftErrorField>(); }\n  /// Specify whether this alloca is used to represent a swifterror.\n  void setSwiftError(bool V) { setSubclassData<SwiftErrorField>(V); }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Alloca);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                LoadInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction for reading from memory. This uses the SubclassData field in\n/// Value to store whether or not the load is volatile.\nclass LoadInst : public UnaryInstruction {\n  using VolatileField = BoolBitfieldElementT<0>;\n  using AlignmentField = AlignmentBitfieldElementT<VolatileField::NextBit>;\n  using OrderingField = AtomicOrderingBitfieldElementT<AlignmentField::NextBit>;\n  static_assert(\n      Bitfield::areContiguous<VolatileField, AlignmentField, OrderingField>(),\n      \"Bitfields must be contiguous\");\n\n  void AssertOK();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  LoadInst *cloneImpl() const;\n\npublic:\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr,\n           Instruction *InsertBefore);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, BasicBlock *InsertAtEnd);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Instruction *InsertBefore);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           BasicBlock *InsertAtEnd);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, Instruction *InsertBefore = nullptr);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, BasicBlock *InsertAtEnd);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, AtomicOrdering Order,\n           SyncScope::ID SSID = SyncScope::System,\n           Instruction *InsertBefore = nullptr);\n  LoadInst(Type *Ty, Value *Ptr, const Twine &NameStr, bool isVolatile,\n           Align Align, AtomicOrdering Order, SyncScope::ID SSID,\n           BasicBlock *InsertAtEnd);\n\n  /// Return true if this is a load from a volatile memory location.\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile load or not.\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Return the alignment of the access that is being performed.\n  /// FIXME: Remove this function once transition to Align is over.\n  /// Use getAlign() instead.\n  unsigned getAlignment() const { return getAlign().value(); }\n\n  /// Return the alignment of the access that is being performed.\n  Align getAlign() const {\n    return Align(1ULL << (getSubclassData<AlignmentField>()));\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Returns the ordering constraint of this load instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<OrderingField>();\n  }\n  /// Sets the ordering constraint of this load instruction.  May not be Release\n  /// or AcquireRelease.\n  void setOrdering(AtomicOrdering Ordering) {\n    setSubclassData<OrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this load instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this load instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  /// Sets the ordering constraint and the synchronization scope ID of this load\n  /// instruction.\n  void setAtomic(AtomicOrdering Ordering,\n                 SyncScope::ID SSID = SyncScope::System) {\n    setOrdering(Ordering);\n    setSyncScopeID(SSID);\n  }\n\n  bool isSimple() const { return !isAtomic() && !isVolatile(); }\n\n  bool isUnordered() const {\n    return (getOrdering() == AtomicOrdering::NotAtomic ||\n            getOrdering() == AtomicOrdering::Unordered) &&\n           !isVolatile();\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n  Type *getPointerOperandType() const { return getPointerOperand()->getType(); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperandType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Load;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this load instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\n//===----------------------------------------------------------------------===//\n//                                StoreInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction for storing to memory.\nclass StoreInst : public Instruction {\n  using VolatileField = BoolBitfieldElementT<0>;\n  using AlignmentField = AlignmentBitfieldElementT<VolatileField::NextBit>;\n  using OrderingField = AtomicOrderingBitfieldElementT<AlignmentField::NextBit>;\n  static_assert(\n      Bitfield::areContiguous<VolatileField, AlignmentField, OrderingField>(),\n      \"Bitfields must be contiguous\");\n\n  void AssertOK();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  StoreInst *cloneImpl() const;\n\npublic:\n  StoreInst(Value *Val, Value *Ptr, Instruction *InsertBefore);\n  StoreInst(Value *Val, Value *Ptr, BasicBlock *InsertAtEnd);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Instruction *InsertBefore);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, BasicBlock *InsertAtEnd);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            Instruction *InsertBefore = nullptr);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            BasicBlock *InsertAtEnd);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            AtomicOrdering Order, SyncScope::ID SSID = SyncScope::System,\n            Instruction *InsertBefore = nullptr);\n  StoreInst(Value *Val, Value *Ptr, bool isVolatile, Align Align,\n            AtomicOrdering Order, SyncScope::ID SSID, BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  /// Return true if this is a store to a volatile memory location.\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile store or not.\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Return the alignment of the access that is being performed\n  /// FIXME: Remove this function once transition to Align is over.\n  /// Use getAlign() instead.\n  unsigned getAlignment() const { return getAlign().value(); }\n\n  Align getAlign() const {\n    return Align(1ULL << (getSubclassData<AlignmentField>()));\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Returns the ordering constraint of this store instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<OrderingField>();\n  }\n\n  /// Sets the ordering constraint of this store instruction.  May not be\n  /// Acquire or AcquireRelease.\n  void setOrdering(AtomicOrdering Ordering) {\n    setSubclassData<OrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this store instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this store instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  /// Sets the ordering constraint and the synchronization scope ID of this\n  /// store instruction.\n  void setAtomic(AtomicOrdering Ordering,\n                 SyncScope::ID SSID = SyncScope::System) {\n    setOrdering(Ordering);\n    setSyncScopeID(SSID);\n  }\n\n  bool isSimple() const { return !isAtomic() && !isVolatile(); }\n\n  bool isUnordered() const {\n    return (getOrdering() == AtomicOrdering::NotAtomic ||\n            getOrdering() == AtomicOrdering::Unordered) &&\n           !isVolatile();\n  }\n\n  Value *getValueOperand() { return getOperand(0); }\n  const Value *getValueOperand() const { return getOperand(0); }\n\n  Value *getPointerOperand() { return getOperand(1); }\n  const Value *getPointerOperand() const { return getOperand(1); }\n  static unsigned getPointerOperandIndex() { return 1U; }\n  Type *getPointerOperandType() const { return getPointerOperand()->getType(); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperandType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Store;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this store instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\ntemplate <>\nstruct OperandTraits<StoreInst> : public FixedNumOperandTraits<StoreInst, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(StoreInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                FenceInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction for ordering other memory operations.\nclass FenceInst : public Instruction {\n  using OrderingField = AtomicOrderingBitfieldElementT<0>;\n\n  void Init(AtomicOrdering Ordering, SyncScope::ID SSID);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  FenceInst *cloneImpl() const;\n\npublic:\n  // Ordering may only be Acquire, Release, AcquireRelease, or\n  // SequentiallyConsistent.\n  FenceInst(LLVMContext &C, AtomicOrdering Ordering,\n            SyncScope::ID SSID = SyncScope::System,\n            Instruction *InsertBefore = nullptr);\n  FenceInst(LLVMContext &C, AtomicOrdering Ordering, SyncScope::ID SSID,\n            BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 0);\n  }\n\n  /// Returns the ordering constraint of this fence instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<OrderingField>();\n  }\n\n  /// Sets the ordering constraint of this fence instruction.  May only be\n  /// Acquire, Release, AcquireRelease, or SequentiallyConsistent.\n  void setOrdering(AtomicOrdering Ordering) {\n    setSubclassData<OrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this fence instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this fence instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Fence;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this fence instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\n//===----------------------------------------------------------------------===//\n//                                AtomicCmpXchgInst Class\n//===----------------------------------------------------------------------===//\n\n/// An instruction that atomically checks whether a\n/// specified value is in a memory location, and, if it is, stores a new value\n/// there. The value returned by this instruction is a pair containing the\n/// original value as first element, and an i1 indicating success (true) or\n/// failure (false) as second element.\n///\nclass AtomicCmpXchgInst : public Instruction {\n  void Init(Value *Ptr, Value *Cmp, Value *NewVal, Align Align,\n            AtomicOrdering SuccessOrdering, AtomicOrdering FailureOrdering,\n            SyncScope::ID SSID);\n\n  template <unsigned Offset>\n  using AtomicOrderingBitfieldElement =\n      typename Bitfield::Element<AtomicOrdering, Offset, 3,\n                                 AtomicOrdering::LAST>;\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  AtomicCmpXchgInst *cloneImpl() const;\n\npublic:\n  AtomicCmpXchgInst(Value *Ptr, Value *Cmp, Value *NewVal, Align Alignment,\n                    AtomicOrdering SuccessOrdering,\n                    AtomicOrdering FailureOrdering, SyncScope::ID SSID,\n                    Instruction *InsertBefore = nullptr);\n  AtomicCmpXchgInst(Value *Ptr, Value *Cmp, Value *NewVal, Align Alignment,\n                    AtomicOrdering SuccessOrdering,\n                    AtomicOrdering FailureOrdering, SyncScope::ID SSID,\n                    BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly three operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 3);\n  }\n\n  using VolatileField = BoolBitfieldElementT<0>;\n  using WeakField = BoolBitfieldElementT<VolatileField::NextBit>;\n  using SuccessOrderingField =\n      AtomicOrderingBitfieldElementT<WeakField::NextBit>;\n  using FailureOrderingField =\n      AtomicOrderingBitfieldElementT<SuccessOrderingField::NextBit>;\n  using AlignmentField =\n      AlignmentBitfieldElementT<FailureOrderingField::NextBit>;\n  static_assert(\n      Bitfield::areContiguous<VolatileField, WeakField, SuccessOrderingField,\n                              FailureOrderingField, AlignmentField>(),\n      \"Bitfields must be contiguous\");\n\n  /// Return the alignment of the memory that is being allocated by the\n  /// instruction.\n  Align getAlign() const {\n    return Align(1ULL << getSubclassData<AlignmentField>());\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Return true if this is a cmpxchg from a volatile memory\n  /// location.\n  ///\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile cmpxchg.\n  ///\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Return true if this cmpxchg may spuriously fail.\n  bool isWeak() const { return getSubclassData<WeakField>(); }\n\n  void setWeak(bool IsWeak) { setSubclassData<WeakField>(IsWeak); }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Returns the success ordering constraint of this cmpxchg instruction.\n  AtomicOrdering getSuccessOrdering() const {\n    return getSubclassData<SuccessOrderingField>();\n  }\n\n  /// Sets the success ordering constraint of this cmpxchg instruction.\n  void setSuccessOrdering(AtomicOrdering Ordering) {\n    assert(Ordering != AtomicOrdering::NotAtomic &&\n           \"CmpXchg instructions can only be atomic.\");\n    setSubclassData<SuccessOrderingField>(Ordering);\n  }\n\n  /// Returns the failure ordering constraint of this cmpxchg instruction.\n  AtomicOrdering getFailureOrdering() const {\n    return getSubclassData<FailureOrderingField>();\n  }\n\n  /// Sets the failure ordering constraint of this cmpxchg instruction.\n  void setFailureOrdering(AtomicOrdering Ordering) {\n    assert(Ordering != AtomicOrdering::NotAtomic &&\n           \"CmpXchg instructions can only be atomic.\");\n    setSubclassData<FailureOrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this cmpxchg instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this cmpxchg instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  Value *getCompareOperand() { return getOperand(1); }\n  const Value *getCompareOperand() const { return getOperand(1); }\n\n  Value *getNewValOperand() { return getOperand(2); }\n  const Value *getNewValOperand() const { return getOperand(2); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  /// Returns the strongest permitted ordering on failure, given the\n  /// desired ordering on success.\n  ///\n  /// If the comparison in a cmpxchg operation fails, there is no atomic store\n  /// so release semantics cannot be provided. So this function drops explicit\n  /// Release requests from the AtomicOrdering. A SequentiallyConsistent\n  /// operation would remain SequentiallyConsistent.\n  static AtomicOrdering\n  getStrongestFailureOrdering(AtomicOrdering SuccessOrdering) {\n    switch (SuccessOrdering) {\n    default:\n      llvm_unreachable(\"invalid cmpxchg success ordering\");\n    case AtomicOrdering::Release:\n    case AtomicOrdering::Monotonic:\n      return AtomicOrdering::Monotonic;\n    case AtomicOrdering::AcquireRelease:\n    case AtomicOrdering::Acquire:\n      return AtomicOrdering::Acquire;\n    case AtomicOrdering::SequentiallyConsistent:\n      return AtomicOrdering::SequentiallyConsistent;\n    }\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::AtomicCmpXchg;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this cmpxchg instruction.  Not quite\n  /// enough room in SubClassData for everything, so synchronization scope ID\n  /// gets its own field.\n  SyncScope::ID SSID;\n};\n\ntemplate <>\nstruct OperandTraits<AtomicCmpXchgInst> :\n    public FixedNumOperandTraits<AtomicCmpXchgInst, 3> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(AtomicCmpXchgInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                AtomicRMWInst Class\n//===----------------------------------------------------------------------===//\n\n/// an instruction that atomically reads a memory location,\n/// combines it with another value, and then stores the result back.  Returns\n/// the old value.\n///\nclass AtomicRMWInst : public Instruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  AtomicRMWInst *cloneImpl() const;\n\npublic:\n  /// This enumeration lists the possible modifications atomicrmw can make.  In\n  /// the descriptions, 'p' is the pointer to the instruction's memory location,\n  /// 'old' is the initial value of *p, and 'v' is the other value passed to the\n  /// instruction.  These instructions always return 'old'.\n  enum BinOp : unsigned {\n    /// *p = v\n    Xchg,\n    /// *p = old + v\n    Add,\n    /// *p = old - v\n    Sub,\n    /// *p = old & v\n    And,\n    /// *p = ~(old & v)\n    Nand,\n    /// *p = old | v\n    Or,\n    /// *p = old ^ v\n    Xor,\n    /// *p = old >signed v ? old : v\n    Max,\n    /// *p = old <signed v ? old : v\n    Min,\n    /// *p = old >unsigned v ? old : v\n    UMax,\n    /// *p = old <unsigned v ? old : v\n    UMin,\n\n    /// *p = old + v\n    FAdd,\n\n    /// *p = old - v\n    FSub,\n\n    FIRST_BINOP = Xchg,\n    LAST_BINOP = FSub,\n    BAD_BINOP\n  };\n\nprivate:\n  template <unsigned Offset>\n  using AtomicOrderingBitfieldElement =\n      typename Bitfield::Element<AtomicOrdering, Offset, 3,\n                                 AtomicOrdering::LAST>;\n\n  template <unsigned Offset>\n  using BinOpBitfieldElement =\n      typename Bitfield::Element<BinOp, Offset, 4, BinOp::LAST_BINOP>;\n\npublic:\n  AtomicRMWInst(BinOp Operation, Value *Ptr, Value *Val, Align Alignment,\n                AtomicOrdering Ordering, SyncScope::ID SSID,\n                Instruction *InsertBefore = nullptr);\n  AtomicRMWInst(BinOp Operation, Value *Ptr, Value *Val, Align Alignment,\n                AtomicOrdering Ordering, SyncScope::ID SSID,\n                BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  using VolatileField = BoolBitfieldElementT<0>;\n  using AtomicOrderingField =\n      AtomicOrderingBitfieldElementT<VolatileField::NextBit>;\n  using OperationField = BinOpBitfieldElement<AtomicOrderingField::NextBit>;\n  using AlignmentField = AlignmentBitfieldElementT<OperationField::NextBit>;\n  static_assert(Bitfield::areContiguous<VolatileField, AtomicOrderingField,\n                                        OperationField, AlignmentField>(),\n                \"Bitfields must be contiguous\");\n\n  BinOp getOperation() const { return getSubclassData<OperationField>(); }\n\n  static StringRef getOperationName(BinOp Op);\n\n  static bool isFPOperation(BinOp Op) {\n    switch (Op) {\n    case AtomicRMWInst::FAdd:\n    case AtomicRMWInst::FSub:\n      return true;\n    default:\n      return false;\n    }\n  }\n\n  void setOperation(BinOp Operation) {\n    setSubclassData<OperationField>(Operation);\n  }\n\n  /// Return the alignment of the memory that is being allocated by the\n  /// instruction.\n  Align getAlign() const {\n    return Align(1ULL << getSubclassData<AlignmentField>());\n  }\n\n  void setAlignment(Align Align) {\n    setSubclassData<AlignmentField>(Log2(Align));\n  }\n\n  /// Return true if this is a RMW on a volatile memory location.\n  ///\n  bool isVolatile() const { return getSubclassData<VolatileField>(); }\n\n  /// Specify whether this is a volatile RMW or not.\n  ///\n  void setVolatile(bool V) { setSubclassData<VolatileField>(V); }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Returns the ordering constraint of this rmw instruction.\n  AtomicOrdering getOrdering() const {\n    return getSubclassData<AtomicOrderingField>();\n  }\n\n  /// Sets the ordering constraint of this rmw instruction.\n  void setOrdering(AtomicOrdering Ordering) {\n    assert(Ordering != AtomicOrdering::NotAtomic &&\n           \"atomicrmw instructions can only be atomic.\");\n    setSubclassData<AtomicOrderingField>(Ordering);\n  }\n\n  /// Returns the synchronization scope ID of this rmw instruction.\n  SyncScope::ID getSyncScopeID() const {\n    return SSID;\n  }\n\n  /// Sets the synchronization scope ID of this rmw instruction.\n  void setSyncScopeID(SyncScope::ID SSID) {\n    this->SSID = SSID;\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  Value *getValOperand() { return getOperand(1); }\n  const Value *getValOperand() const { return getOperand(1); }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  bool isFloatingPointOperation() const {\n    return isFPOperation(getOperation());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::AtomicRMW;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  void Init(BinOp Operation, Value *Ptr, Value *Val, Align Align,\n            AtomicOrdering Ordering, SyncScope::ID SSID);\n\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n\n  /// The synchronization scope ID of this rmw instruction.  Not quite enough\n  /// room in SubClassData for everything, so synchronization scope ID gets its\n  /// own field.\n  SyncScope::ID SSID;\n};\n\ntemplate <>\nstruct OperandTraits<AtomicRMWInst>\n    : public FixedNumOperandTraits<AtomicRMWInst,2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(AtomicRMWInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                             GetElementPtrInst Class\n//===----------------------------------------------------------------------===//\n\n// checkGEPType - Simple wrapper function to give a better assertion failure\n// message on bad indexes for a gep instruction.\n//\ninline Type *checkGEPType(Type *Ty) {\n  assert(Ty && \"Invalid GetElementPtrInst indices for type!\");\n  return Ty;\n}\n\n/// an instruction for type-safe pointer arithmetic to\n/// access elements of arrays and structs\n///\nclass GetElementPtrInst : public Instruction {\n  Type *SourceElementType;\n  Type *ResultElementType;\n\n  GetElementPtrInst(const GetElementPtrInst &GEPI);\n\n  /// Constructors - Create a getelementptr instruction with a base pointer an\n  /// list of indices. The first ctor can optionally insert before an existing\n  /// instruction, the second appends the new instruction to the specified\n  /// BasicBlock.\n  inline GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                           ArrayRef<Value *> IdxList, unsigned Values,\n                           const Twine &NameStr, Instruction *InsertBefore);\n  inline GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                           ArrayRef<Value *> IdxList, unsigned Values,\n                           const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(Value *Ptr, ArrayRef<Value *> IdxList, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  GetElementPtrInst *cloneImpl() const;\n\npublic:\n  static GetElementPtrInst *Create(Type *PointeeType, Value *Ptr,\n                                   ArrayRef<Value *> IdxList,\n                                   const Twine &NameStr = \"\",\n                                   Instruction *InsertBefore = nullptr) {\n    unsigned Values = 1 + unsigned(IdxList.size());\n    if (!PointeeType)\n      PointeeType =\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType();\n    else\n      assert(\n          PointeeType ==\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType());\n    return new (Values) GetElementPtrInst(PointeeType, Ptr, IdxList, Values,\n                                          NameStr, InsertBefore);\n  }\n\n  static GetElementPtrInst *Create(Type *PointeeType, Value *Ptr,\n                                   ArrayRef<Value *> IdxList,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd) {\n    unsigned Values = 1 + unsigned(IdxList.size());\n    if (!PointeeType)\n      PointeeType =\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType();\n    else\n      assert(\n          PointeeType ==\n          cast<PointerType>(Ptr->getType()->getScalarType())->getElementType());\n    return new (Values) GetElementPtrInst(PointeeType, Ptr, IdxList, Values,\n                                          NameStr, InsertAtEnd);\n  }\n\n  /// Create an \"inbounds\" getelementptr. See the documentation for the\n  /// \"inbounds\" flag in LangRef.html for details.\n  static GetElementPtrInst *CreateInBounds(Value *Ptr,\n                                           ArrayRef<Value *> IdxList,\n                                           const Twine &NameStr = \"\",\n                                           Instruction *InsertBefore = nullptr){\n    return CreateInBounds(nullptr, Ptr, IdxList, NameStr, InsertBefore);\n  }\n\n  static GetElementPtrInst *\n  CreateInBounds(Type *PointeeType, Value *Ptr, ArrayRef<Value *> IdxList,\n                 const Twine &NameStr = \"\",\n                 Instruction *InsertBefore = nullptr) {\n    GetElementPtrInst *GEP =\n        Create(PointeeType, Ptr, IdxList, NameStr, InsertBefore);\n    GEP->setIsInBounds(true);\n    return GEP;\n  }\n\n  static GetElementPtrInst *CreateInBounds(Value *Ptr,\n                                           ArrayRef<Value *> IdxList,\n                                           const Twine &NameStr,\n                                           BasicBlock *InsertAtEnd) {\n    return CreateInBounds(nullptr, Ptr, IdxList, NameStr, InsertAtEnd);\n  }\n\n  static GetElementPtrInst *CreateInBounds(Type *PointeeType, Value *Ptr,\n                                           ArrayRef<Value *> IdxList,\n                                           const Twine &NameStr,\n                                           BasicBlock *InsertAtEnd) {\n    GetElementPtrInst *GEP =\n        Create(PointeeType, Ptr, IdxList, NameStr, InsertAtEnd);\n    GEP->setIsInBounds(true);\n    return GEP;\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  Type *getSourceElementType() const { return SourceElementType; }\n\n  void setSourceElementType(Type *Ty) { SourceElementType = Ty; }\n  void setResultElementType(Type *Ty) { ResultElementType = Ty; }\n\n  Type *getResultElementType() const {\n    assert(ResultElementType ==\n           cast<PointerType>(getType()->getScalarType())->getElementType());\n    return ResultElementType;\n  }\n\n  /// Returns the address space of this instruction's pointer type.\n  unsigned getAddressSpace() const {\n    // Note that this is always the same as the pointer operand's address space\n    // and that is cheaper to compute, so cheat here.\n    return getPointerAddressSpace();\n  }\n\n  /// Returns the result type of a getelementptr with the given source\n  /// element type and indexes.\n  ///\n  /// Null is returned if the indices are invalid for the specified\n  /// source element type.\n  static Type *getIndexedType(Type *Ty, ArrayRef<Value *> IdxList);\n  static Type *getIndexedType(Type *Ty, ArrayRef<Constant *> IdxList);\n  static Type *getIndexedType(Type *Ty, ArrayRef<uint64_t> IdxList);\n\n  /// Return the type of the element at the given index of an indexable\n  /// type.  This is equivalent to \"getIndexedType(Agg, {Zero, Idx})\".\n  ///\n  /// Returns null if the type can't be indexed, or the given index is not\n  /// legal for the given type.\n  static Type *getTypeAtIndex(Type *Ty, Value *Idx);\n  static Type *getTypeAtIndex(Type *Ty, uint64_t Idx);\n\n  inline op_iterator       idx_begin()       { return op_begin()+1; }\n  inline const_op_iterator idx_begin() const { return op_begin()+1; }\n  inline op_iterator       idx_end()         { return op_end(); }\n  inline const_op_iterator idx_end()   const { return op_end(); }\n\n  inline iterator_range<op_iterator> indices() {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  inline iterator_range<const_op_iterator> indices() const {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  Value *getPointerOperand() {\n    return getOperand(0);\n  }\n  const Value *getPointerOperand() const {\n    return getOperand(0);\n  }\n  static unsigned getPointerOperandIndex() {\n    return 0U;    // get index for modifying correct operand.\n  }\n\n  /// Method to return the pointer operand as a\n  /// PointerType.\n  Type *getPointerOperandType() const {\n    return getPointerOperand()->getType();\n  }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperandType()->getPointerAddressSpace();\n  }\n\n  /// Returns the pointer type returned by the GEP\n  /// instruction, which may be a vector of pointers.\n  static Type *getGEPReturnType(Type *ElTy, Value *Ptr,\n                                ArrayRef<Value *> IdxList) {\n    Type *PtrTy = PointerType::get(checkGEPType(getIndexedType(ElTy, IdxList)),\n                                   Ptr->getType()->getPointerAddressSpace());\n    // Vector GEP\n    if (auto *PtrVTy = dyn_cast<VectorType>(Ptr->getType())) {\n      ElementCount EltCount = PtrVTy->getElementCount();\n      return VectorType::get(PtrTy, EltCount);\n    }\n    for (Value *Index : IdxList)\n      if (auto *IndexVTy = dyn_cast<VectorType>(Index->getType())) {\n        ElementCount EltCount = IndexVTy->getElementCount();\n        return VectorType::get(PtrTy, EltCount);\n      }\n    // Scalar GEP\n    return PtrTy;\n  }\n\n  unsigned getNumIndices() const {  // Note: always non-negative\n    return getNumOperands() - 1;\n  }\n\n  bool hasIndices() const {\n    return getNumOperands() > 1;\n  }\n\n  /// Return true if all of the indices of this GEP are\n  /// zeros.  If so, the result pointer and the first operand have the same\n  /// value, just potentially different types.\n  bool hasAllZeroIndices() const;\n\n  /// Return true if all of the indices of this GEP are\n  /// constant integers.  If so, the result pointer and the first operand have\n  /// a constant offset between them.\n  bool hasAllConstantIndices() const;\n\n  /// Set or clear the inbounds flag on this GEP instruction.\n  /// See LangRef.html for the meaning of inbounds on a getelementptr.\n  void setIsInBounds(bool b = true);\n\n  /// Determine whether the GEP has the inbounds flag.\n  bool isInBounds() const;\n\n  /// Accumulate the constant address offset of this GEP if possible.\n  ///\n  /// This routine accepts an APInt into which it will accumulate the constant\n  /// offset of this GEP if the GEP is in fact constant. If the GEP is not\n  /// all-constant, it returns false and the value of the offset APInt is\n  /// undefined (it is *not* preserved!). The APInt passed into this routine\n  /// must be at least as wide as the IntPtr type for the address space of\n  /// the base GEP pointer.\n  bool accumulateConstantOffset(const DataLayout &DL, APInt &Offset) const;\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::GetElementPtr);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<GetElementPtrInst> :\n  public VariadicOperandTraits<GetElementPtrInst, 1> {\n};\n\nGetElementPtrInst::GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                                     ArrayRef<Value *> IdxList, unsigned Values,\n                                     const Twine &NameStr,\n                                     Instruction *InsertBefore)\n    : Instruction(getGEPReturnType(PointeeType, Ptr, IdxList), GetElementPtr,\n                  OperandTraits<GetElementPtrInst>::op_end(this) - Values,\n                  Values, InsertBefore),\n      SourceElementType(PointeeType),\n      ResultElementType(getIndexedType(PointeeType, IdxList)) {\n  assert(ResultElementType ==\n         cast<PointerType>(getType()->getScalarType())->getElementType());\n  init(Ptr, IdxList, NameStr);\n}\n\nGetElementPtrInst::GetElementPtrInst(Type *PointeeType, Value *Ptr,\n                                     ArrayRef<Value *> IdxList, unsigned Values,\n                                     const Twine &NameStr,\n                                     BasicBlock *InsertAtEnd)\n    : Instruction(getGEPReturnType(PointeeType, Ptr, IdxList), GetElementPtr,\n                  OperandTraits<GetElementPtrInst>::op_end(this) - Values,\n                  Values, InsertAtEnd),\n      SourceElementType(PointeeType),\n      ResultElementType(getIndexedType(PointeeType, IdxList)) {\n  assert(ResultElementType ==\n         cast<PointerType>(getType()->getScalarType())->getElementType());\n  init(Ptr, IdxList, NameStr);\n}\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(GetElementPtrInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               ICmpInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction compares its operands according to the predicate given\n/// to the constructor. It only operates on integers or pointers. The operands\n/// must be identical types.\n/// Represent an integer comparison operator.\nclass ICmpInst: public CmpInst {\n  void AssertOK() {\n    assert(isIntPredicate() &&\n           \"Invalid ICmp predicate value\");\n    assert(getOperand(0)->getType() == getOperand(1)->getType() &&\n          \"Both operands to ICmp instruction are not of the same type!\");\n    // Check that the operands are the right type\n    assert((getOperand(0)->getType()->isIntOrIntVectorTy() ||\n            getOperand(0)->getType()->isPtrOrPtrVectorTy()) &&\n           \"Invalid operand types for ICmp instruction\");\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical ICmpInst\n  ICmpInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics.\n  ICmpInst(\n    Instruction *InsertBefore,  ///< Where to insert\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::ICmp, pred, LHS, RHS, NameStr,\n              InsertBefore) {\n#ifndef NDEBUG\n  AssertOK();\n#endif\n  }\n\n  /// Constructor with insert-at-end semantics.\n  ICmpInst(\n    BasicBlock &InsertAtEnd, ///< Block to insert into.\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::ICmp, pred, LHS, RHS, NameStr,\n              &InsertAtEnd) {\n#ifndef NDEBUG\n  AssertOK();\n#endif\n  }\n\n  /// Constructor with no-insertion semantics\n  ICmpInst(\n    Predicate pred, ///< The predicate to use for the comparison\n    Value *LHS,     ///< The left-hand-side of the expression\n    Value *RHS,     ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\" ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::ICmp, pred, LHS, RHS, NameStr) {\n#ifndef NDEBUG\n  AssertOK();\n#endif\n  }\n\n  /// For example, EQ->EQ, SLE->SLE, UGT->SGT, etc.\n  /// @returns the predicate that would be the result if the operand were\n  /// regarded as signed.\n  /// Return the signed version of the predicate\n  Predicate getSignedPredicate() const {\n    return getSignedPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction.\n  /// Return the signed version of the predicate.\n  static Predicate getSignedPredicate(Predicate pred);\n\n  /// For example, EQ->EQ, SLE->ULE, UGT->UGT, etc.\n  /// @returns the predicate that would be the result if the operand were\n  /// regarded as unsigned.\n  /// Return the unsigned version of the predicate\n  Predicate getUnsignedPredicate() const {\n    return getUnsignedPredicate(getPredicate());\n  }\n\n  /// This is a static version that you can use without an instruction.\n  /// Return the unsigned version of the predicate.\n  static Predicate getUnsignedPredicate(Predicate pred);\n\n  /// Return true if this predicate is either EQ or NE.  This also\n  /// tests for commutativity.\n  static bool isEquality(Predicate P) {\n    return P == ICMP_EQ || P == ICMP_NE;\n  }\n\n  /// Return true if this predicate is either EQ or NE.  This also\n  /// tests for commutativity.\n  bool isEquality() const {\n    return isEquality(getPredicate());\n  }\n\n  /// @returns true if the predicate of this ICmpInst is commutative\n  /// Determine if this relation is commutative.\n  bool isCommutative() const { return isEquality(); }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  ///\n  bool isRelational() const {\n    return !isEquality();\n  }\n\n  /// Return true if the predicate is relational (not EQ or NE).\n  ///\n  static bool isRelational(Predicate P) {\n    return !isEquality(P);\n  }\n\n  /// Return true if the predicate is SGT or UGT.\n  ///\n  static bool isGT(Predicate P) {\n    return P == ICMP_SGT || P == ICMP_UGT;\n  }\n\n  /// Return true if the predicate is SLT or ULT.\n  ///\n  static bool isLT(Predicate P) {\n    return P == ICMP_SLT || P == ICMP_ULT;\n  }\n\n  /// Return true if the predicate is SGE or UGE.\n  ///\n  static bool isGE(Predicate P) {\n    return P == ICMP_SGE || P == ICMP_UGE;\n  }\n\n  /// Return true if the predicate is SLE or ULE.\n  ///\n  static bool isLE(Predicate P) {\n    return P == ICMP_SLE || P == ICMP_ULE;\n  }\n\n  /// Exchange the two operands to this instruction in such a way that it does\n  /// not modify the semantics of the instruction. The predicate value may be\n  /// changed to retain the same result if the predicate is order dependent\n  /// (e.g. ult).\n  /// Swap operands and adjust predicate.\n  void swapOperands() {\n    setPredicate(getSwappedPredicate());\n    Op<0>().swap(Op<1>());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ICmp;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               FCmpInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction compares its operands according to the predicate given\n/// to the constructor. It only operates on floating point values or packed\n/// vectors of floating point values. The operands must be identical types.\n/// Represents a floating point comparison operator.\nclass FCmpInst: public CmpInst {\n  void AssertOK() {\n    assert(isFPPredicate() && \"Invalid FCmp predicate value\");\n    assert(getOperand(0)->getType() == getOperand(1)->getType() &&\n           \"Both operands to FCmp instruction are not of the same type!\");\n    // Check that the operands are the right type\n    assert(getOperand(0)->getType()->isFPOrFPVectorTy() &&\n           \"Invalid operand types for FCmp instruction\");\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FCmpInst\n  FCmpInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics.\n  FCmpInst(\n    Instruction *InsertBefore, ///< Where to insert\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::FCmp, pred, LHS, RHS, NameStr,\n              InsertBefore) {\n    AssertOK();\n  }\n\n  /// Constructor with insert-at-end semantics.\n  FCmpInst(\n    BasicBlock &InsertAtEnd, ///< Block to insert into.\n    Predicate pred,  ///< The predicate to use for the comparison\n    Value *LHS,      ///< The left-hand-side of the expression\n    Value *RHS,      ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\"  ///< Name of the instruction\n  ) : CmpInst(makeCmpResultType(LHS->getType()),\n              Instruction::FCmp, pred, LHS, RHS, NameStr,\n              &InsertAtEnd) {\n    AssertOK();\n  }\n\n  /// Constructor with no-insertion semantics\n  FCmpInst(\n    Predicate Pred, ///< The predicate to use for the comparison\n    Value *LHS,     ///< The left-hand-side of the expression\n    Value *RHS,     ///< The right-hand-side of the expression\n    const Twine &NameStr = \"\", ///< Name of the instruction\n    Instruction *FlagsSource = nullptr\n  ) : CmpInst(makeCmpResultType(LHS->getType()), Instruction::FCmp, Pred, LHS,\n              RHS, NameStr, nullptr, FlagsSource) {\n    AssertOK();\n  }\n\n  /// @returns true if the predicate of this instruction is EQ or NE.\n  /// Determine if this is an equality predicate.\n  static bool isEquality(Predicate Pred) {\n    return Pred == FCMP_OEQ || Pred == FCMP_ONE || Pred == FCMP_UEQ ||\n           Pred == FCMP_UNE;\n  }\n\n  /// @returns true if the predicate of this instruction is EQ or NE.\n  /// Determine if this is an equality predicate.\n  bool isEquality() const { return isEquality(getPredicate()); }\n\n  /// @returns true if the predicate of this instruction is commutative.\n  /// Determine if this is a commutative predicate.\n  bool isCommutative() const {\n    return isEquality() ||\n           getPredicate() == FCMP_FALSE ||\n           getPredicate() == FCMP_TRUE ||\n           getPredicate() == FCMP_ORD ||\n           getPredicate() == FCMP_UNO;\n  }\n\n  /// @returns true if the predicate is relational (not EQ or NE).\n  /// Determine if this a relational predicate.\n  bool isRelational() const { return !isEquality(); }\n\n  /// Exchange the two operands to this instruction in such a way that it does\n  /// not modify the semantics of the instruction. The predicate value may be\n  /// changed to retain the same result if the predicate is order dependent\n  /// (e.g. ult).\n  /// Swap operands and adjust predicate.\n  void swapOperands() {\n    setPredicate(getSwappedPredicate());\n    Op<0>().swap(Op<1>());\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::FCmp;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n/// This class represents a function call, abstracting a target\n/// machine's calling convention.  This class uses low bit of the SubClassData\n/// field to indicate whether or not this is a tail call.  The rest of the bits\n/// hold the calling convention of the call.\n///\nclass CallInst : public CallBase {\n  CallInst(const CallInst &CI);\n\n  /// Construct a CallInst given a range of arguments.\n  /// Construct a CallInst from a range of arguments\n  inline CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                  ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                  Instruction *InsertBefore);\n\n  inline CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                  const Twine &NameStr, Instruction *InsertBefore)\n      : CallInst(Ty, Func, Args, None, NameStr, InsertBefore) {}\n\n  /// Construct a CallInst given a range of arguments.\n  /// Construct a CallInst from a range of arguments\n  inline CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                  ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                  BasicBlock *InsertAtEnd);\n\n  explicit CallInst(FunctionType *Ty, Value *F, const Twine &NameStr,\n                    Instruction *InsertBefore);\n\n  CallInst(FunctionType *ty, Value *F, const Twine &NameStr,\n           BasicBlock *InsertAtEnd);\n\n  void init(FunctionType *FTy, Value *Func, ArrayRef<Value *> Args,\n            ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr);\n  void init(FunctionType *FTy, Value *Func, const Twine &NameStr);\n\n  /// Compute the number of operands to allocate.\n  static int ComputeNumOperands(int NumArgs, int NumBundleInputs = 0) {\n    // We need one operand for the called function, plus the input operand\n    // counts provided.\n    return 1 + NumArgs + NumBundleInputs;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CallInst *cloneImpl() const;\n\npublic:\n  static CallInst *Create(FunctionType *Ty, Value *F, const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    return new (ComputeNumOperands(0)) CallInst(Ty, F, NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr,\n                          Instruction *InsertBefore = nullptr) {\n    return new (ComputeNumOperands(Args.size()))\n        CallInst(Ty, Func, Args, None, NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles = None,\n                          const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    const int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    const unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallInst(Ty, Func, Args, Bundles, NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *F, const Twine &NameStr,\n                          BasicBlock *InsertAtEnd) {\n    return new (ComputeNumOperands(0)) CallInst(Ty, F, NameStr, InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return new (ComputeNumOperands(Args.size()))\n        CallInst(Ty, Func, Args, None, NameStr, InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    const int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    const unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallInst(Ty, Func, Args, Bundles, NameStr, InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionCallee Func, const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), NameStr,\n                  InsertBefore);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles = None,\n                          const Twine &NameStr = \"\",\n                          Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, Bundles,\n                  NameStr, InsertBefore);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr,\n                          Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, NameStr,\n                  InsertBefore);\n  }\n\n  static CallInst *Create(FunctionCallee Func, const Twine &NameStr,\n                          BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), NameStr,\n                  InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, NameStr,\n                  InsertAtEnd);\n  }\n\n  static CallInst *Create(FunctionCallee Func, ArrayRef<Value *> Args,\n                          ArrayRef<OperandBundleDef> Bundles,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), Args, Bundles,\n                  NameStr, InsertAtEnd);\n  }\n\n  /// Create a clone of \\p CI with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned call instruction is identical \\p CI in every way except that\n  /// the operand bundles for the new instruction are set to the operand bundles\n  /// in \\p Bundles.\n  static CallInst *Create(CallInst *CI, ArrayRef<OperandBundleDef> Bundles,\n                          Instruction *InsertPt = nullptr);\n\n  /// Generate the IR for a call to malloc:\n  /// 1. Compute the malloc call's argument as the specified type's size,\n  ///    possibly multiplied by the array size if the array size is not\n  ///    constant 1.\n  /// 2. Call malloc with that argument.\n  /// 3. Bitcast the result of the malloc call to the specified type.\n  static Instruction *CreateMalloc(Instruction *InsertBefore, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  static Instruction *CreateMalloc(BasicBlock *InsertAtEnd, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  static Instruction *CreateMalloc(Instruction *InsertBefore, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   ArrayRef<OperandBundleDef> Bundles = None,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  static Instruction *CreateMalloc(BasicBlock *InsertAtEnd, Type *IntPtrTy,\n                                   Type *AllocTy, Value *AllocSize,\n                                   Value *ArraySize = nullptr,\n                                   ArrayRef<OperandBundleDef> Bundles = None,\n                                   Function *MallocF = nullptr,\n                                   const Twine &Name = \"\");\n  /// Generate the IR for a call to the builtin free function.\n  static Instruction *CreateFree(Value *Source, Instruction *InsertBefore);\n  static Instruction *CreateFree(Value *Source, BasicBlock *InsertAtEnd);\n  static Instruction *CreateFree(Value *Source,\n                                 ArrayRef<OperandBundleDef> Bundles,\n                                 Instruction *InsertBefore);\n  static Instruction *CreateFree(Value *Source,\n                                 ArrayRef<OperandBundleDef> Bundles,\n                                 BasicBlock *InsertAtEnd);\n\n  // Note that 'musttail' implies 'tail'.\n  enum TailCallKind : unsigned {\n    TCK_None = 0,\n    TCK_Tail = 1,\n    TCK_MustTail = 2,\n    TCK_NoTail = 3,\n    TCK_LAST = TCK_NoTail\n  };\n\n  using TailCallKindField = Bitfield::Element<TailCallKind, 0, 2, TCK_LAST>;\n  static_assert(\n      Bitfield::areContiguous<TailCallKindField, CallBase::CallingConvField>(),\n      \"Bitfields must be contiguous\");\n\n  TailCallKind getTailCallKind() const {\n    return getSubclassData<TailCallKindField>();\n  }\n\n  bool isTailCall() const {\n    TailCallKind Kind = getTailCallKind();\n    return Kind == TCK_Tail || Kind == TCK_MustTail;\n  }\n\n  bool isMustTailCall() const { return getTailCallKind() == TCK_MustTail; }\n\n  bool isNoTailCall() const { return getTailCallKind() == TCK_NoTail; }\n\n  void setTailCallKind(TailCallKind TCK) {\n    setSubclassData<TailCallKindField>(TCK);\n  }\n\n  void setTailCall(bool IsTc = true) {\n    setTailCallKind(IsTc ? TCK_Tail : TCK_None);\n  }\n\n  /// Return true if the call can return twice\n  bool canReturnTwice() const { return hasFnAttr(Attribute::ReturnsTwice); }\n  void setCanReturnTwice() {\n    addAttribute(AttributeList::FunctionIndex, Attribute::ReturnsTwice);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Call;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  /// Updates profile metadata by scaling it by \\p S / \\p T.\n  void updateProfWeight(uint64_t S, uint64_t T);\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\nCallInst::CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                   ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                   BasicBlock *InsertAtEnd)\n    : CallBase(Ty->getReturnType(), Instruction::Call,\n               OperandTraits<CallBase>::op_end(this) -\n                   (Args.size() + CountBundleInputs(Bundles) + 1),\n               unsigned(Args.size() + CountBundleInputs(Bundles) + 1),\n               InsertAtEnd) {\n  init(Ty, Func, Args, Bundles, NameStr);\n}\n\nCallInst::CallInst(FunctionType *Ty, Value *Func, ArrayRef<Value *> Args,\n                   ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr,\n                   Instruction *InsertBefore)\n    : CallBase(Ty->getReturnType(), Instruction::Call,\n               OperandTraits<CallBase>::op_end(this) -\n                   (Args.size() + CountBundleInputs(Bundles) + 1),\n               unsigned(Args.size() + CountBundleInputs(Bundles) + 1),\n               InsertBefore) {\n  init(Ty, Func, Args, Bundles, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                               SelectInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents the LLVM 'select' instruction.\n///\nclass SelectInst : public Instruction {\n  SelectInst(Value *C, Value *S1, Value *S2, const Twine &NameStr,\n             Instruction *InsertBefore)\n    : Instruction(S1->getType(), Instruction::Select,\n                  &Op<0>(), 3, InsertBefore) {\n    init(C, S1, S2);\n    setName(NameStr);\n  }\n\n  SelectInst(Value *C, Value *S1, Value *S2, const Twine &NameStr,\n             BasicBlock *InsertAtEnd)\n    : Instruction(S1->getType(), Instruction::Select,\n                  &Op<0>(), 3, InsertAtEnd) {\n    init(C, S1, S2);\n    setName(NameStr);\n  }\n\n  void init(Value *C, Value *S1, Value *S2) {\n    assert(!areInvalidOperands(C, S1, S2) && \"Invalid operands for select\");\n    Op<0>() = C;\n    Op<1>() = S1;\n    Op<2>() = S2;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  SelectInst *cloneImpl() const;\n\npublic:\n  static SelectInst *Create(Value *C, Value *S1, Value *S2,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr,\n                            Instruction *MDFrom = nullptr) {\n    SelectInst *Sel = new(3) SelectInst(C, S1, S2, NameStr, InsertBefore);\n    if (MDFrom)\n      Sel->copyMetadata(*MDFrom);\n    return Sel;\n  }\n\n  static SelectInst *Create(Value *C, Value *S1, Value *S2,\n                            const Twine &NameStr,\n                            BasicBlock *InsertAtEnd) {\n    return new(3) SelectInst(C, S1, S2, NameStr, InsertAtEnd);\n  }\n\n  const Value *getCondition() const { return Op<0>(); }\n  const Value *getTrueValue() const { return Op<1>(); }\n  const Value *getFalseValue() const { return Op<2>(); }\n  Value *getCondition() { return Op<0>(); }\n  Value *getTrueValue() { return Op<1>(); }\n  Value *getFalseValue() { return Op<2>(); }\n\n  void setCondition(Value *V) { Op<0>() = V; }\n  void setTrueValue(Value *V) { Op<1>() = V; }\n  void setFalseValue(Value *V) { Op<2>() = V; }\n\n  /// Swap the true and false values of the select instruction.\n  /// This doesn't swap prof metadata.\n  void swapValues() { Op<1>().swap(Op<2>()); }\n\n  /// Return a string if the specified operands are invalid\n  /// for a select operation, otherwise return null.\n  static const char *areInvalidOperands(Value *Cond, Value *True, Value *False);\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  OtherOps getOpcode() const {\n    return static_cast<OtherOps>(Instruction::getOpcode());\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Select;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<SelectInst> : public FixedNumOperandTraits<SelectInst, 3> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(SelectInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                VAArgInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents the va_arg llvm instruction, which returns\n/// an argument of the specified type given a va_list and increments that list\n///\nclass VAArgInst : public UnaryInstruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  VAArgInst *cloneImpl() const;\n\npublic:\n  VAArgInst(Value *List, Type *Ty, const Twine &NameStr = \"\",\n             Instruction *InsertBefore = nullptr)\n    : UnaryInstruction(Ty, VAArg, List, InsertBefore) {\n    setName(NameStr);\n  }\n\n  VAArgInst(Value *List, Type *Ty, const Twine &NameStr,\n            BasicBlock *InsertAtEnd)\n    : UnaryInstruction(Ty, VAArg, List, InsertAtEnd) {\n    setName(NameStr);\n  }\n\n  Value *getPointerOperand() { return getOperand(0); }\n  const Value *getPointerOperand() const { return getOperand(0); }\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == VAArg;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                ExtractElementInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction extracts a single (scalar)\n/// element from a VectorType value\n///\nclass ExtractElementInst : public Instruction {\n  ExtractElementInst(Value *Vec, Value *Idx, const Twine &NameStr = \"\",\n                     Instruction *InsertBefore = nullptr);\n  ExtractElementInst(Value *Vec, Value *Idx, const Twine &NameStr,\n                     BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ExtractElementInst *cloneImpl() const;\n\npublic:\n  static ExtractElementInst *Create(Value *Vec, Value *Idx,\n                                   const Twine &NameStr = \"\",\n                                   Instruction *InsertBefore = nullptr) {\n    return new(2) ExtractElementInst(Vec, Idx, NameStr, InsertBefore);\n  }\n\n  static ExtractElementInst *Create(Value *Vec, Value *Idx,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd) {\n    return new(2) ExtractElementInst(Vec, Idx, NameStr, InsertAtEnd);\n  }\n\n  /// Return true if an extractelement instruction can be\n  /// formed with the specified operands.\n  static bool isValidOperands(const Value *Vec, const Value *Idx);\n\n  Value *getVectorOperand() { return Op<0>(); }\n  Value *getIndexOperand() { return Op<1>(); }\n  const Value *getVectorOperand() const { return Op<0>(); }\n  const Value *getIndexOperand() const { return Op<1>(); }\n\n  VectorType *getVectorOperandType() const {\n    return cast<VectorType>(getVectorOperand()->getType());\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ExtractElement;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ExtractElementInst> :\n  public FixedNumOperandTraits<ExtractElementInst, 2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ExtractElementInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                InsertElementInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction inserts a single (scalar)\n/// element into a VectorType value\n///\nclass InsertElementInst : public Instruction {\n  InsertElementInst(Value *Vec, Value *NewElt, Value *Idx,\n                    const Twine &NameStr = \"\",\n                    Instruction *InsertBefore = nullptr);\n  InsertElementInst(Value *Vec, Value *NewElt, Value *Idx, const Twine &NameStr,\n                    BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  InsertElementInst *cloneImpl() const;\n\npublic:\n  static InsertElementInst *Create(Value *Vec, Value *NewElt, Value *Idx,\n                                   const Twine &NameStr = \"\",\n                                   Instruction *InsertBefore = nullptr) {\n    return new(3) InsertElementInst(Vec, NewElt, Idx, NameStr, InsertBefore);\n  }\n\n  static InsertElementInst *Create(Value *Vec, Value *NewElt, Value *Idx,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd) {\n    return new(3) InsertElementInst(Vec, NewElt, Idx, NameStr, InsertAtEnd);\n  }\n\n  /// Return true if an insertelement instruction can be\n  /// formed with the specified operands.\n  static bool isValidOperands(const Value *Vec, const Value *NewElt,\n                              const Value *Idx);\n\n  /// Overload to return most specific vector type.\n  ///\n  VectorType *getType() const {\n    return cast<VectorType>(Instruction::getType());\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::InsertElement;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<InsertElementInst> :\n  public FixedNumOperandTraits<InsertElementInst, 3> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(InsertElementInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                           ShuffleVectorInst Class\n//===----------------------------------------------------------------------===//\n\nconstexpr int UndefMaskElem = -1;\n\n/// This instruction constructs a fixed permutation of two\n/// input vectors.\n///\n/// For each element of the result vector, the shuffle mask selects an element\n/// from one of the input vectors to copy to the result. Non-negative elements\n/// in the mask represent an index into the concatenated pair of input vectors.\n/// UndefMaskElem (-1) specifies that the result element is undefined.\n///\n/// For scalable vectors, all the elements of the mask must be 0 or -1. This\n/// requirement may be relaxed in the future.\nclass ShuffleVectorInst : public Instruction {\n  SmallVector<int, 4> ShuffleMask;\n  Constant *ShuffleMaskForBitcode;\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ShuffleVectorInst *cloneImpl() const;\n\npublic:\n  ShuffleVectorInst(Value *V1, Value *V2, Value *Mask,\n                    const Twine &NameStr = \"\",\n                    Instruction *InsertBefor = nullptr);\n  ShuffleVectorInst(Value *V1, Value *V2, Value *Mask,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n  ShuffleVectorInst(Value *V1, Value *V2, ArrayRef<int> Mask,\n                    const Twine &NameStr = \"\",\n                    Instruction *InsertBefor = nullptr);\n  ShuffleVectorInst(Value *V1, Value *V2, ArrayRef<int> Mask,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void *operator new(size_t s) { return User::operator new(s, 2); }\n\n  /// Swap the operands and adjust the mask to preserve the semantics\n  /// of the instruction.\n  void commute();\n\n  /// Return true if a shufflevector instruction can be\n  /// formed with the specified operands.\n  static bool isValidOperands(const Value *V1, const Value *V2,\n                              const Value *Mask);\n  static bool isValidOperands(const Value *V1, const Value *V2,\n                              ArrayRef<int> Mask);\n\n  /// Overload to return most specific vector type.\n  ///\n  VectorType *getType() const {\n    return cast<VectorType>(Instruction::getType());\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Return the shuffle mask value of this instruction for the given element\n  /// index. Return UndefMaskElem if the element is undef.\n  int getMaskValue(unsigned Elt) const { return ShuffleMask[Elt]; }\n\n  /// Convert the input shuffle mask operand to a vector of integers. Undefined\n  /// elements of the mask are returned as UndefMaskElem.\n  static void getShuffleMask(const Constant *Mask,\n                             SmallVectorImpl<int> &Result);\n\n  /// Return the mask for this instruction as a vector of integers. Undefined\n  /// elements of the mask are returned as UndefMaskElem.\n  void getShuffleMask(SmallVectorImpl<int> &Result) const {\n    Result.assign(ShuffleMask.begin(), ShuffleMask.end());\n  }\n\n  /// Return the mask for this instruction, for use in bitcode.\n  ///\n  /// TODO: This is temporary until we decide a new bitcode encoding for\n  /// shufflevector.\n  Constant *getShuffleMaskForBitcode() const { return ShuffleMaskForBitcode; }\n\n  static Constant *convertShuffleMaskForBitcode(ArrayRef<int> Mask,\n                                                Type *ResultTy);\n\n  void setShuffleMask(ArrayRef<int> Mask);\n\n  ArrayRef<int> getShuffleMask() const { return ShuffleMask; }\n\n  /// Return true if this shuffle returns a vector with a different number of\n  /// elements than its source vectors.\n  /// Examples: shufflevector <4 x n> A, <4 x n> B, <1,2,3>\n  ///           shufflevector <4 x n> A, <4 x n> B, <1,2,3,4,5>\n  bool changesLength() const {\n    unsigned NumSourceElts = cast<VectorType>(Op<0>()->getType())\n                                 ->getElementCount()\n                                 .getKnownMinValue();\n    unsigned NumMaskElts = ShuffleMask.size();\n    return NumSourceElts != NumMaskElts;\n  }\n\n  /// Return true if this shuffle returns a vector with a greater number of\n  /// elements than its source vectors.\n  /// Example: shufflevector <2 x n> A, <2 x n> B, <1,2,3>\n  bool increasesLength() const {\n    unsigned NumSourceElts = cast<VectorType>(Op<0>()->getType())\n                                 ->getElementCount()\n                                 .getKnownMinValue();\n    unsigned NumMaskElts = ShuffleMask.size();\n    return NumSourceElts < NumMaskElts;\n  }\n\n  /// Return true if this shuffle mask chooses elements from exactly one source\n  /// vector.\n  /// Example: <7,5,undef,7>\n  /// This assumes that vector operands are the same length as the mask.\n  static bool isSingleSourceMask(ArrayRef<int> Mask);\n  static bool isSingleSourceMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isSingleSourceMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle chooses elements from exactly one source\n  /// vector without changing the length of that vector.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <3,0,undef,3>\n  /// TODO: Optionally allow length-changing shuffles.\n  bool isSingleSource() const {\n    return !changesLength() && isSingleSourceMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask chooses elements from exactly one source\n  /// vector without lane crossings. A shuffle using this mask is not\n  /// necessarily a no-op because it may change the number of elements from its\n  /// input vectors or it may provide demanded bits knowledge via undef lanes.\n  /// Example: <undef,undef,2,3>\n  static bool isIdentityMask(ArrayRef<int> Mask);\n  static bool isIdentityMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isIdentityMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle chooses elements from exactly one source\n  /// vector without lane crossings and does not change the number of elements\n  /// from its input vectors.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <4,undef,6,undef>\n  bool isIdentity() const {\n    return !changesLength() && isIdentityMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle lengthens exactly one source vector with\n  /// undefs in the high elements.\n  bool isIdentityWithPadding() const;\n\n  /// Return true if this shuffle extracts the first N elements of exactly one\n  /// source vector.\n  bool isIdentityWithExtract() const;\n\n  /// Return true if this shuffle concatenates its 2 source vectors. This\n  /// returns false if either input is undefined. In that case, the shuffle is\n  /// is better classified as an identity with padding operation.\n  bool isConcat() const;\n\n  /// Return true if this shuffle mask chooses elements from its source vectors\n  /// without lane crossings. A shuffle using this mask would be\n  /// equivalent to a vector select with a constant condition operand.\n  /// Example: <4,1,6,undef>\n  /// This returns false if the mask does not choose from both input vectors.\n  /// In that case, the shuffle is better classified as an identity shuffle.\n  /// This assumes that vector operands are the same length as the mask\n  /// (a length-changing shuffle can never be equivalent to a vector select).\n  static bool isSelectMask(ArrayRef<int> Mask);\n  static bool isSelectMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isSelectMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle chooses elements from its source vectors\n  /// without lane crossings and all operands have the same number of elements.\n  /// In other words, this shuffle is equivalent to a vector select with a\n  /// constant condition operand.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <undef,1,6,3>\n  /// This returns false if the mask does not choose from both input vectors.\n  /// In that case, the shuffle is better classified as an identity shuffle.\n  /// TODO: Optionally allow length-changing shuffles.\n  bool isSelect() const {\n    return !changesLength() && isSelectMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask swaps the order of elements from exactly\n  /// one source vector.\n  /// Example: <7,6,undef,4>\n  /// This assumes that vector operands are the same length as the mask.\n  static bool isReverseMask(ArrayRef<int> Mask);\n  static bool isReverseMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isReverseMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle swaps the order of elements from exactly\n  /// one source vector.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <3,undef,1,undef>\n  /// TODO: Optionally allow length-changing shuffles.\n  bool isReverse() const {\n    return !changesLength() && isReverseMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask chooses all elements with the same value\n  /// as the first element of exactly one source vector.\n  /// Example: <4,undef,undef,4>\n  /// This assumes that vector operands are the same length as the mask.\n  static bool isZeroEltSplatMask(ArrayRef<int> Mask);\n  static bool isZeroEltSplatMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isZeroEltSplatMask(MaskAsInts);\n  }\n\n  /// Return true if all elements of this shuffle are the same value as the\n  /// first element of exactly one source vector without changing the length\n  /// of that vector.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <undef,0,undef,0>\n  /// TODO: Optionally allow length-changing shuffles.\n  /// TODO: Optionally allow splats from other elements.\n  bool isZeroEltSplat() const {\n    return !changesLength() && isZeroEltSplatMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask is a transpose mask.\n  /// Transpose vector masks transpose a 2xn matrix. They read corresponding\n  /// even- or odd-numbered vector elements from two n-dimensional source\n  /// vectors and write each result into consecutive elements of an\n  /// n-dimensional destination vector. Two shuffles are necessary to complete\n  /// the transpose, one for the even elements and another for the odd elements.\n  /// This description closely follows how the TRN1 and TRN2 AArch64\n  /// instructions operate.\n  ///\n  /// For example, a simple 2x2 matrix can be transposed with:\n  ///\n  ///   ; Original matrix\n  ///   m0 = < a, b >\n  ///   m1 = < c, d >\n  ///\n  ///   ; Transposed matrix\n  ///   t0 = < a, c > = shufflevector m0, m1, < 0, 2 >\n  ///   t1 = < b, d > = shufflevector m0, m1, < 1, 3 >\n  ///\n  /// For matrices having greater than n columns, the resulting nx2 transposed\n  /// matrix is stored in two result vectors such that one vector contains\n  /// interleaved elements from all the even-numbered rows and the other vector\n  /// contains interleaved elements from all the odd-numbered rows. For example,\n  /// a 2x4 matrix can be transposed with:\n  ///\n  ///   ; Original matrix\n  ///   m0 = < a, b, c, d >\n  ///   m1 = < e, f, g, h >\n  ///\n  ///   ; Transposed matrix\n  ///   t0 = < a, e, c, g > = shufflevector m0, m1 < 0, 4, 2, 6 >\n  ///   t1 = < b, f, d, h > = shufflevector m0, m1 < 1, 5, 3, 7 >\n  static bool isTransposeMask(ArrayRef<int> Mask);\n  static bool isTransposeMask(const Constant *Mask) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isTransposeMask(MaskAsInts);\n  }\n\n  /// Return true if this shuffle transposes the elements of its inputs without\n  /// changing the length of the vectors. This operation may also be known as a\n  /// merge or interleave. See the description for isTransposeMask() for the\n  /// exact specification.\n  /// Example: shufflevector <4 x n> A, <4 x n> B, <0,4,2,6>\n  bool isTranspose() const {\n    return !changesLength() && isTransposeMask(ShuffleMask);\n  }\n\n  /// Return true if this shuffle mask is an extract subvector mask.\n  /// A valid extract subvector mask returns a smaller vector from a single\n  /// source operand. The base extraction index is returned as well.\n  static bool isExtractSubvectorMask(ArrayRef<int> Mask, int NumSrcElts,\n                                     int &Index);\n  static bool isExtractSubvectorMask(const Constant *Mask, int NumSrcElts,\n                                     int &Index) {\n    assert(Mask->getType()->isVectorTy() && \"Shuffle needs vector constant.\");\n    // Not possible to express a shuffle mask for a scalable vector for this\n    // case.\n    if (isa<ScalableVectorType>(Mask->getType()))\n      return false;\n    SmallVector<int, 16> MaskAsInts;\n    getShuffleMask(Mask, MaskAsInts);\n    return isExtractSubvectorMask(MaskAsInts, NumSrcElts, Index);\n  }\n\n  /// Return true if this shuffle mask is an extract subvector mask.\n  bool isExtractSubvectorMask(int &Index) const {\n    // Not possible to express a shuffle mask for a scalable vector for this\n    // case.\n    if (isa<ScalableVectorType>(getType()))\n      return false;\n\n    int NumSrcElts =\n        cast<FixedVectorType>(Op<0>()->getType())->getNumElements();\n    return isExtractSubvectorMask(ShuffleMask, NumSrcElts, Index);\n  }\n\n  /// Change values in a shuffle permute mask assuming the two vector operands\n  /// of length InVecNumElts have swapped position.\n  static void commuteShuffleMask(MutableArrayRef<int> Mask,\n                                 unsigned InVecNumElts) {\n    for (int &Idx : Mask) {\n      if (Idx == -1)\n        continue;\n      Idx = Idx < (int)InVecNumElts ? Idx + InVecNumElts : Idx - InVecNumElts;\n      assert(Idx >= 0 && Idx < (int)InVecNumElts * 2 &&\n             \"shufflevector mask index out of range\");\n    }\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ShuffleVector;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ShuffleVectorInst>\n    : public FixedNumOperandTraits<ShuffleVectorInst, 2> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ShuffleVectorInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                                ExtractValueInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction extracts a struct member or array\n/// element value from an aggregate value.\n///\nclass ExtractValueInst : public UnaryInstruction {\n  SmallVector<unsigned, 4> Indices;\n\n  ExtractValueInst(const ExtractValueInst &EVI);\n\n  /// Constructors - Create a extractvalue instruction with a base aggregate\n  /// value and a list of indices.  The first ctor can optionally insert before\n  /// an existing instruction, the second appends the new instruction to the\n  /// specified BasicBlock.\n  inline ExtractValueInst(Value *Agg,\n                          ArrayRef<unsigned> Idxs,\n                          const Twine &NameStr,\n                          Instruction *InsertBefore);\n  inline ExtractValueInst(Value *Agg,\n                          ArrayRef<unsigned> Idxs,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(ArrayRef<unsigned> Idxs, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ExtractValueInst *cloneImpl() const;\n\npublic:\n  static ExtractValueInst *Create(Value *Agg,\n                                  ArrayRef<unsigned> Idxs,\n                                  const Twine &NameStr = \"\",\n                                  Instruction *InsertBefore = nullptr) {\n    return new\n      ExtractValueInst(Agg, Idxs, NameStr, InsertBefore);\n  }\n\n  static ExtractValueInst *Create(Value *Agg,\n                                  ArrayRef<unsigned> Idxs,\n                                  const Twine &NameStr,\n                                  BasicBlock *InsertAtEnd) {\n    return new ExtractValueInst(Agg, Idxs, NameStr, InsertAtEnd);\n  }\n\n  /// Returns the type of the element that would be extracted\n  /// with an extractvalue instruction with the specified parameters.\n  ///\n  /// Null is returned if the indices are invalid for the specified type.\n  static Type *getIndexedType(Type *Agg, ArrayRef<unsigned> Idxs);\n\n  using idx_iterator = const unsigned*;\n\n  inline idx_iterator idx_begin() const { return Indices.begin(); }\n  inline idx_iterator idx_end()   const { return Indices.end(); }\n  inline iterator_range<idx_iterator> indices() const {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  Value *getAggregateOperand() {\n    return getOperand(0);\n  }\n  const Value *getAggregateOperand() const {\n    return getOperand(0);\n  }\n  static unsigned getAggregateOperandIndex() {\n    return 0U;                      // get index for modifying correct operand\n  }\n\n  ArrayRef<unsigned> getIndices() const {\n    return Indices;\n  }\n\n  unsigned getNumIndices() const {\n    return (unsigned)Indices.size();\n  }\n\n  bool hasIndices() const {\n    return true;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::ExtractValue;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\nExtractValueInst::ExtractValueInst(Value *Agg,\n                                   ArrayRef<unsigned> Idxs,\n                                   const Twine &NameStr,\n                                   Instruction *InsertBefore)\n  : UnaryInstruction(checkGEPType(getIndexedType(Agg->getType(), Idxs)),\n                     ExtractValue, Agg, InsertBefore) {\n  init(Idxs, NameStr);\n}\n\nExtractValueInst::ExtractValueInst(Value *Agg,\n                                   ArrayRef<unsigned> Idxs,\n                                   const Twine &NameStr,\n                                   BasicBlock *InsertAtEnd)\n  : UnaryInstruction(checkGEPType(getIndexedType(Agg->getType(), Idxs)),\n                     ExtractValue, Agg, InsertAtEnd) {\n  init(Idxs, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                                InsertValueInst Class\n//===----------------------------------------------------------------------===//\n\n/// This instruction inserts a struct field of array element\n/// value into an aggregate value.\n///\nclass InsertValueInst : public Instruction {\n  SmallVector<unsigned, 4> Indices;\n\n  InsertValueInst(const InsertValueInst &IVI);\n\n  /// Constructors - Create a insertvalue instruction with a base aggregate\n  /// value, a value to insert, and a list of indices.  The first ctor can\n  /// optionally insert before an existing instruction, the second appends\n  /// the new instruction to the specified BasicBlock.\n  inline InsertValueInst(Value *Agg, Value *Val,\n                         ArrayRef<unsigned> Idxs,\n                         const Twine &NameStr,\n                         Instruction *InsertBefore);\n  inline InsertValueInst(Value *Agg, Value *Val,\n                         ArrayRef<unsigned> Idxs,\n                         const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  /// Constructors - These two constructors are convenience methods because one\n  /// and two index insertvalue instructions are so common.\n  InsertValueInst(Value *Agg, Value *Val, unsigned Idx,\n                  const Twine &NameStr = \"\",\n                  Instruction *InsertBefore = nullptr);\n  InsertValueInst(Value *Agg, Value *Val, unsigned Idx, const Twine &NameStr,\n                  BasicBlock *InsertAtEnd);\n\n  void init(Value *Agg, Value *Val, ArrayRef<unsigned> Idxs,\n            const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  InsertValueInst *cloneImpl() const;\n\npublic:\n  // allocate space for exactly two operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 2);\n  }\n\n  static InsertValueInst *Create(Value *Agg, Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr = \"\",\n                                 Instruction *InsertBefore = nullptr) {\n    return new InsertValueInst(Agg, Val, Idxs, NameStr, InsertBefore);\n  }\n\n  static InsertValueInst *Create(Value *Agg, Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr,\n                                 BasicBlock *InsertAtEnd) {\n    return new InsertValueInst(Agg, Val, Idxs, NameStr, InsertAtEnd);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  using idx_iterator = const unsigned*;\n\n  inline idx_iterator idx_begin() const { return Indices.begin(); }\n  inline idx_iterator idx_end()   const { return Indices.end(); }\n  inline iterator_range<idx_iterator> indices() const {\n    return make_range(idx_begin(), idx_end());\n  }\n\n  Value *getAggregateOperand() {\n    return getOperand(0);\n  }\n  const Value *getAggregateOperand() const {\n    return getOperand(0);\n  }\n  static unsigned getAggregateOperandIndex() {\n    return 0U;                      // get index for modifying correct operand\n  }\n\n  Value *getInsertedValueOperand() {\n    return getOperand(1);\n  }\n  const Value *getInsertedValueOperand() const {\n    return getOperand(1);\n  }\n  static unsigned getInsertedValueOperandIndex() {\n    return 1U;                      // get index for modifying correct operand\n  }\n\n  ArrayRef<unsigned> getIndices() const {\n    return Indices;\n  }\n\n  unsigned getNumIndices() const {\n    return (unsigned)Indices.size();\n  }\n\n  bool hasIndices() const {\n    return true;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::InsertValue;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<InsertValueInst> :\n  public FixedNumOperandTraits<InsertValueInst, 2> {\n};\n\nInsertValueInst::InsertValueInst(Value *Agg,\n                                 Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr,\n                                 Instruction *InsertBefore)\n  : Instruction(Agg->getType(), InsertValue,\n                OperandTraits<InsertValueInst>::op_begin(this),\n                2, InsertBefore) {\n  init(Agg, Val, Idxs, NameStr);\n}\n\nInsertValueInst::InsertValueInst(Value *Agg,\n                                 Value *Val,\n                                 ArrayRef<unsigned> Idxs,\n                                 const Twine &NameStr,\n                                 BasicBlock *InsertAtEnd)\n  : Instruction(Agg->getType(), InsertValue,\n                OperandTraits<InsertValueInst>::op_begin(this),\n                2, InsertAtEnd) {\n  init(Agg, Val, Idxs, NameStr);\n}\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(InsertValueInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               PHINode Class\n//===----------------------------------------------------------------------===//\n\n// PHINode - The PHINode class is used to represent the magical mystical PHI\n// node, that can not exist in nature, but can be synthesized in a computer\n// scientist's overactive imagination.\n//\nclass PHINode : public Instruction {\n  /// The number of operands actually allocated.  NumOperands is\n  /// the number actually in use.\n  unsigned ReservedSpace;\n\n  PHINode(const PHINode &PN);\n\n  explicit PHINode(Type *Ty, unsigned NumReservedValues,\n                   const Twine &NameStr = \"\",\n                   Instruction *InsertBefore = nullptr)\n    : Instruction(Ty, Instruction::PHI, nullptr, 0, InsertBefore),\n      ReservedSpace(NumReservedValues) {\n    setName(NameStr);\n    allocHungoffUses(ReservedSpace);\n  }\n\n  PHINode(Type *Ty, unsigned NumReservedValues, const Twine &NameStr,\n          BasicBlock *InsertAtEnd)\n    : Instruction(Ty, Instruction::PHI, nullptr, 0, InsertAtEnd),\n      ReservedSpace(NumReservedValues) {\n    setName(NameStr);\n    allocHungoffUses(ReservedSpace);\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  PHINode *cloneImpl() const;\n\n  // allocHungoffUses - this is more complicated than the generic\n  // User::allocHungoffUses, because we have to allocate Uses for the incoming\n  // values and pointers to the incoming blocks, all in one allocation.\n  void allocHungoffUses(unsigned N) {\n    User::allocHungoffUses(N, /* IsPhi */ true);\n  }\n\npublic:\n  /// Constructors - NumReservedValues is a hint for the number of incoming\n  /// edges that this phi node will have (use 0 if you really have no idea).\n  static PHINode *Create(Type *Ty, unsigned NumReservedValues,\n                         const Twine &NameStr = \"\",\n                         Instruction *InsertBefore = nullptr) {\n    return new PHINode(Ty, NumReservedValues, NameStr, InsertBefore);\n  }\n\n  static PHINode *Create(Type *Ty, unsigned NumReservedValues,\n                         const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return new PHINode(Ty, NumReservedValues, NameStr, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Block iterator interface. This provides access to the list of incoming\n  // basic blocks, which parallels the list of incoming values.\n\n  using block_iterator = BasicBlock **;\n  using const_block_iterator = BasicBlock * const *;\n\n  block_iterator block_begin() {\n    return reinterpret_cast<block_iterator>(op_begin() + ReservedSpace);\n  }\n\n  const_block_iterator block_begin() const {\n    return reinterpret_cast<const_block_iterator>(op_begin() + ReservedSpace);\n  }\n\n  block_iterator block_end() {\n    return block_begin() + getNumOperands();\n  }\n\n  const_block_iterator block_end() const {\n    return block_begin() + getNumOperands();\n  }\n\n  iterator_range<block_iterator> blocks() {\n    return make_range(block_begin(), block_end());\n  }\n\n  iterator_range<const_block_iterator> blocks() const {\n    return make_range(block_begin(), block_end());\n  }\n\n  op_range incoming_values() { return operands(); }\n\n  const_op_range incoming_values() const { return operands(); }\n\n  /// Return the number of incoming edges\n  ///\n  unsigned getNumIncomingValues() const { return getNumOperands(); }\n\n  /// Return incoming value number x\n  ///\n  Value *getIncomingValue(unsigned i) const {\n    return getOperand(i);\n  }\n  void setIncomingValue(unsigned i, Value *V) {\n    assert(V && \"PHI node got a null value!\");\n    assert(getType() == V->getType() &&\n           \"All operands to PHI node must be the same type as the PHI node!\");\n    setOperand(i, V);\n  }\n\n  static unsigned getOperandNumForIncomingValue(unsigned i) {\n    return i;\n  }\n\n  static unsigned getIncomingValueNumForOperand(unsigned i) {\n    return i;\n  }\n\n  /// Return incoming basic block number @p i.\n  ///\n  BasicBlock *getIncomingBlock(unsigned i) const {\n    return block_begin()[i];\n  }\n\n  /// Return incoming basic block corresponding\n  /// to an operand of the PHI.\n  ///\n  BasicBlock *getIncomingBlock(const Use &U) const {\n    assert(this == U.getUser() && \"Iterator doesn't point to PHI's Uses?\");\n    return getIncomingBlock(unsigned(&U - op_begin()));\n  }\n\n  /// Return incoming basic block corresponding\n  /// to value use iterator.\n  ///\n  BasicBlock *getIncomingBlock(Value::const_user_iterator I) const {\n    return getIncomingBlock(I.getUse());\n  }\n\n  void setIncomingBlock(unsigned i, BasicBlock *BB) {\n    assert(BB && \"PHI node got a null basic block!\");\n    block_begin()[i] = BB;\n  }\n\n  /// Replace every incoming basic block \\p Old to basic block \\p New.\n  void replaceIncomingBlockWith(const BasicBlock *Old, BasicBlock *New) {\n    assert(New && Old && \"PHI node got a null basic block!\");\n    for (unsigned Op = 0, NumOps = getNumOperands(); Op != NumOps; ++Op)\n      if (getIncomingBlock(Op) == Old)\n        setIncomingBlock(Op, New);\n  }\n\n  /// Add an incoming value to the end of the PHI list\n  ///\n  void addIncoming(Value *V, BasicBlock *BB) {\n    if (getNumOperands() == ReservedSpace)\n      growOperands();  // Get more space!\n    // Initialize some new operands.\n    setNumHungOffUseOperands(getNumOperands() + 1);\n    setIncomingValue(getNumOperands() - 1, V);\n    setIncomingBlock(getNumOperands() - 1, BB);\n  }\n\n  /// Remove an incoming value.  This is useful if a\n  /// predecessor basic block is deleted.  The value removed is returned.\n  ///\n  /// If the last incoming value for a PHI node is removed (and DeletePHIIfEmpty\n  /// is true), the PHI node is destroyed and any uses of it are replaced with\n  /// dummy values.  The only time there should be zero incoming values to a PHI\n  /// node is when the block is dead, so this strategy is sound.\n  ///\n  Value *removeIncomingValue(unsigned Idx, bool DeletePHIIfEmpty = true);\n\n  Value *removeIncomingValue(const BasicBlock *BB, bool DeletePHIIfEmpty=true) {\n    int Idx = getBasicBlockIndex(BB);\n    assert(Idx >= 0 && \"Invalid basic block argument to remove!\");\n    return removeIncomingValue(Idx, DeletePHIIfEmpty);\n  }\n\n  /// Return the first index of the specified basic\n  /// block in the value list for this PHI.  Returns -1 if no instance.\n  ///\n  int getBasicBlockIndex(const BasicBlock *BB) const {\n    for (unsigned i = 0, e = getNumOperands(); i != e; ++i)\n      if (block_begin()[i] == BB)\n        return i;\n    return -1;\n  }\n\n  Value *getIncomingValueForBlock(const BasicBlock *BB) const {\n    int Idx = getBasicBlockIndex(BB);\n    assert(Idx >= 0 && \"Invalid basic block argument!\");\n    return getIncomingValue(Idx);\n  }\n\n  /// Set every incoming value(s) for block \\p BB to \\p V.\n  void setIncomingValueForBlock(const BasicBlock *BB, Value *V) {\n    assert(BB && \"PHI node got a null basic block!\");\n    bool Found = false;\n    for (unsigned Op = 0, NumOps = getNumOperands(); Op != NumOps; ++Op)\n      if (getIncomingBlock(Op) == BB) {\n        Found = true;\n        setIncomingValue(Op, V);\n      }\n    (void)Found;\n    assert(Found && \"Invalid basic block argument to set!\");\n  }\n\n  /// If the specified PHI node always merges together the\n  /// same value, return the value, otherwise return null.\n  Value *hasConstantValue() const;\n\n  /// Whether the specified PHI node always merges\n  /// together the same value, assuming undefs are equal to a unique\n  /// non-undef value.\n  bool hasConstantOrUndefValue() const;\n\n  /// If the PHI node is complete which means all of its parent's predecessors\n  /// have incoming value in this PHI, return true, otherwise return false.\n  bool isComplete() const {\n    return llvm::all_of(predecessors(getParent()),\n                        [this](const BasicBlock *Pred) {\n                          return getBasicBlockIndex(Pred) >= 0;\n                        });\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::PHI;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  void growOperands();\n};\n\ntemplate <>\nstruct OperandTraits<PHINode> : public HungoffOperandTraits<2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(PHINode, Value)\n\n//===----------------------------------------------------------------------===//\n//                           LandingPadInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// The landingpad instruction holds all of the information\n/// necessary to generate correct exception handling. The landingpad instruction\n/// cannot be moved from the top of a landing pad block, which itself is\n/// accessible only from the 'unwind' edge of an invoke. This uses the\n/// SubclassData field in Value to store whether or not the landingpad is a\n/// cleanup.\n///\nclass LandingPadInst : public Instruction {\n  using CleanupField = BoolBitfieldElementT<0>;\n\n  /// The number of operands actually allocated.  NumOperands is\n  /// the number actually in use.\n  unsigned ReservedSpace;\n\n  LandingPadInst(const LandingPadInst &LP);\n\npublic:\n  enum ClauseType { Catch, Filter };\n\nprivate:\n  explicit LandingPadInst(Type *RetTy, unsigned NumReservedValues,\n                          const Twine &NameStr, Instruction *InsertBefore);\n  explicit LandingPadInst(Type *RetTy, unsigned NumReservedValues,\n                          const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  // Allocate space for exactly zero operands.\n  void *operator new(size_t s) {\n    return User::operator new(s);\n  }\n\n  void growOperands(unsigned Size);\n  void init(unsigned NumReservedValues, const Twine &NameStr);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  LandingPadInst *cloneImpl() const;\n\npublic:\n  /// Constructors - NumReservedClauses is a hint for the number of incoming\n  /// clauses that this landingpad will have (use 0 if you really have no idea).\n  static LandingPadInst *Create(Type *RetTy, unsigned NumReservedClauses,\n                                const Twine &NameStr = \"\",\n                                Instruction *InsertBefore = nullptr);\n  static LandingPadInst *Create(Type *RetTy, unsigned NumReservedClauses,\n                                const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Return 'true' if this landingpad instruction is a\n  /// cleanup. I.e., it should be run when unwinding even if its landing pad\n  /// doesn't catch the exception.\n  bool isCleanup() const { return getSubclassData<CleanupField>(); }\n\n  /// Indicate that this landingpad instruction is a cleanup.\n  void setCleanup(bool V) { setSubclassData<CleanupField>(V); }\n\n  /// Add a catch or filter clause to the landing pad.\n  void addClause(Constant *ClauseVal);\n\n  /// Get the value of the clause at index Idx. Use isCatch/isFilter to\n  /// determine what type of clause this is.\n  Constant *getClause(unsigned Idx) const {\n    return cast<Constant>(getOperandList()[Idx]);\n  }\n\n  /// Return 'true' if the clause and index Idx is a catch clause.\n  bool isCatch(unsigned Idx) const {\n    return !isa<ArrayType>(getOperandList()[Idx]->getType());\n  }\n\n  /// Return 'true' if the clause and index Idx is a filter clause.\n  bool isFilter(unsigned Idx) const {\n    return isa<ArrayType>(getOperandList()[Idx]->getType());\n  }\n\n  /// Get the number of clauses for this landing pad.\n  unsigned getNumClauses() const { return getNumOperands(); }\n\n  /// Grow the size of the operand list to accommodate the new\n  /// number of clauses.\n  void reserveClauses(unsigned Size) { growOperands(Size); }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::LandingPad;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<LandingPadInst> : public HungoffOperandTraits<1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(LandingPadInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               ReturnInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Return a value (possibly void), from a function.  Execution\n/// does not continue in this function any longer.\n///\nclass ReturnInst : public Instruction {\n  ReturnInst(const ReturnInst &RI);\n\nprivate:\n  // ReturnInst constructors:\n  // ReturnInst()                  - 'ret void' instruction\n  // ReturnInst(    null)          - 'ret void' instruction\n  // ReturnInst(Value* X)          - 'ret X'    instruction\n  // ReturnInst(    null, Inst *I) - 'ret void' instruction, insert before I\n  // ReturnInst(Value* X, Inst *I) - 'ret X'    instruction, insert before I\n  // ReturnInst(    null, BB *B)   - 'ret void' instruction, insert @ end of B\n  // ReturnInst(Value* X, BB *B)   - 'ret X'    instruction, insert @ end of B\n  //\n  // NOTE: If the Value* passed is of type void then the constructor behaves as\n  // if it was passed NULL.\n  explicit ReturnInst(LLVMContext &C, Value *retVal = nullptr,\n                      Instruction *InsertBefore = nullptr);\n  ReturnInst(LLVMContext &C, Value *retVal, BasicBlock *InsertAtEnd);\n  explicit ReturnInst(LLVMContext &C, BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ReturnInst *cloneImpl() const;\n\npublic:\n  static ReturnInst* Create(LLVMContext &C, Value *retVal = nullptr,\n                            Instruction *InsertBefore = nullptr) {\n    return new(!!retVal) ReturnInst(C, retVal, InsertBefore);\n  }\n\n  static ReturnInst* Create(LLVMContext &C, Value *retVal,\n                            BasicBlock *InsertAtEnd) {\n    return new(!!retVal) ReturnInst(C, retVal, InsertAtEnd);\n  }\n\n  static ReturnInst* Create(LLVMContext &C, BasicBlock *InsertAtEnd) {\n    return new(0) ReturnInst(C, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Convenience accessor. Returns null if there is no return value.\n  Value *getReturnValue() const {\n    return getNumOperands() != 0 ? getOperand(0) : nullptr;\n  }\n\n  unsigned getNumSuccessors() const { return 0; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Ret);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned idx) const {\n    llvm_unreachable(\"ReturnInst has no successors!\");\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *B) {\n    llvm_unreachable(\"ReturnInst has no successors!\");\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ReturnInst> : public VariadicOperandTraits<ReturnInst> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ReturnInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               BranchInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Conditional or Unconditional Branch instruction.\n///\nclass BranchInst : public Instruction {\n  /// Ops list - Branches are strange.  The operands are ordered:\n  ///  [Cond, FalseDest,] TrueDest.  This makes some accessors faster because\n  /// they don't have to check for cond/uncond branchness. These are mostly\n  /// accessed relative from op_end().\n  BranchInst(const BranchInst &BI);\n  // BranchInst constructors (where {B, T, F} are blocks, and C is a condition):\n  // BranchInst(BB *B)                           - 'br B'\n  // BranchInst(BB* T, BB *F, Value *C)          - 'br C, T, F'\n  // BranchInst(BB* B, Inst *I)                  - 'br B'        insert before I\n  // BranchInst(BB* T, BB *F, Value *C, Inst *I) - 'br C, T, F', insert before I\n  // BranchInst(BB* B, BB *I)                    - 'br B'        insert at end\n  // BranchInst(BB* T, BB *F, Value *C, BB *I)   - 'br C, T, F', insert at end\n  explicit BranchInst(BasicBlock *IfTrue, Instruction *InsertBefore = nullptr);\n  BranchInst(BasicBlock *IfTrue, BasicBlock *IfFalse, Value *Cond,\n             Instruction *InsertBefore = nullptr);\n  BranchInst(BasicBlock *IfTrue, BasicBlock *InsertAtEnd);\n  BranchInst(BasicBlock *IfTrue, BasicBlock *IfFalse, Value *Cond,\n             BasicBlock *InsertAtEnd);\n\n  void AssertOK();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  BranchInst *cloneImpl() const;\n\npublic:\n  /// Iterator type that casts an operand to a basic block.\n  ///\n  /// This only makes sense because the successors are stored as adjacent\n  /// operands for branch instructions.\n  struct succ_op_iterator\n      : iterator_adaptor_base<succ_op_iterator, value_op_iterator,\n                              std::random_access_iterator_tag, BasicBlock *,\n                              ptrdiff_t, BasicBlock *, BasicBlock *> {\n    explicit succ_op_iterator(value_op_iterator I) : iterator_adaptor_base(I) {}\n\n    BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    BasicBlock *operator->() const { return operator*(); }\n  };\n\n  /// The const version of `succ_op_iterator`.\n  struct const_succ_op_iterator\n      : iterator_adaptor_base<const_succ_op_iterator, const_value_op_iterator,\n                              std::random_access_iterator_tag,\n                              const BasicBlock *, ptrdiff_t, const BasicBlock *,\n                              const BasicBlock *> {\n    explicit const_succ_op_iterator(const_value_op_iterator I)\n        : iterator_adaptor_base(I) {}\n\n    const BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    const BasicBlock *operator->() const { return operator*(); }\n  };\n\n  static BranchInst *Create(BasicBlock *IfTrue,\n                            Instruction *InsertBefore = nullptr) {\n    return new(1) BranchInst(IfTrue, InsertBefore);\n  }\n\n  static BranchInst *Create(BasicBlock *IfTrue, BasicBlock *IfFalse,\n                            Value *Cond, Instruction *InsertBefore = nullptr) {\n    return new(3) BranchInst(IfTrue, IfFalse, Cond, InsertBefore);\n  }\n\n  static BranchInst *Create(BasicBlock *IfTrue, BasicBlock *InsertAtEnd) {\n    return new(1) BranchInst(IfTrue, InsertAtEnd);\n  }\n\n  static BranchInst *Create(BasicBlock *IfTrue, BasicBlock *IfFalse,\n                            Value *Cond, BasicBlock *InsertAtEnd) {\n    return new(3) BranchInst(IfTrue, IfFalse, Cond, InsertAtEnd);\n  }\n\n  /// Transparently provide more efficient getOperand methods.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  bool isUnconditional() const { return getNumOperands() == 1; }\n  bool isConditional()   const { return getNumOperands() == 3; }\n\n  Value *getCondition() const {\n    assert(isConditional() && \"Cannot get condition of an uncond branch!\");\n    return Op<-3>();\n  }\n\n  void setCondition(Value *V) {\n    assert(isConditional() && \"Cannot set condition of unconditional branch!\");\n    Op<-3>() = V;\n  }\n\n  unsigned getNumSuccessors() const { return 1+isConditional(); }\n\n  BasicBlock *getSuccessor(unsigned i) const {\n    assert(i < getNumSuccessors() && \"Successor # out of range for Branch!\");\n    return cast_or_null<BasicBlock>((&Op<-1>() - i)->get());\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *NewSucc) {\n    assert(idx < getNumSuccessors() && \"Successor # out of range for Branch!\");\n    *(&Op<-1>() - idx) = NewSucc;\n  }\n\n  /// Swap the successors of this branch instruction.\n  ///\n  /// Swaps the successors of the branch instruction. This also swaps any\n  /// branch weight metadata associated with the instruction so that it\n  /// continues to map correctly to each operand.\n  void swapSuccessors();\n\n  iterator_range<succ_op_iterator> successors() {\n    return make_range(\n        succ_op_iterator(std::next(value_op_begin(), isConditional() ? 1 : 0)),\n        succ_op_iterator(value_op_end()));\n  }\n\n  iterator_range<const_succ_op_iterator> successors() const {\n    return make_range(const_succ_op_iterator(\n                          std::next(value_op_begin(), isConditional() ? 1 : 0)),\n                      const_succ_op_iterator(value_op_end()));\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Br);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<BranchInst> : public VariadicOperandTraits<BranchInst, 1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(BranchInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               SwitchInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Multiway switch\n///\nclass SwitchInst : public Instruction {\n  unsigned ReservedSpace;\n\n  // Operand[0]    = Value to switch on\n  // Operand[1]    = Default basic block destination\n  // Operand[2n  ] = Value to match\n  // Operand[2n+1] = BasicBlock to go to on match\n  SwitchInst(const SwitchInst &SI);\n\n  /// Create a new switch instruction, specifying a value to switch on and a\n  /// default destination. The number of additional cases can be specified here\n  /// to make memory allocation more efficient. This constructor can also\n  /// auto-insert before another instruction.\n  SwitchInst(Value *Value, BasicBlock *Default, unsigned NumCases,\n             Instruction *InsertBefore);\n\n  /// Create a new switch instruction, specifying a value to switch on and a\n  /// default destination. The number of additional cases can be specified here\n  /// to make memory allocation more efficient. This constructor also\n  /// auto-inserts at the end of the specified BasicBlock.\n  SwitchInst(Value *Value, BasicBlock *Default, unsigned NumCases,\n             BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s);\n  }\n\n  void init(Value *Value, BasicBlock *Default, unsigned NumReserved);\n  void growOperands();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  SwitchInst *cloneImpl() const;\n\npublic:\n  // -2\n  static const unsigned DefaultPseudoIndex = static_cast<unsigned>(~0L-1);\n\n  template <typename CaseHandleT> class CaseIteratorImpl;\n\n  /// A handle to a particular switch case. It exposes a convenient interface\n  /// to both the case value and the successor block.\n  ///\n  /// We define this as a template and instantiate it to form both a const and\n  /// non-const handle.\n  template <typename SwitchInstT, typename ConstantIntT, typename BasicBlockT>\n  class CaseHandleImpl {\n    // Directly befriend both const and non-const iterators.\n    friend class SwitchInst::CaseIteratorImpl<\n        CaseHandleImpl<SwitchInstT, ConstantIntT, BasicBlockT>>;\n\n  protected:\n    // Expose the switch type we're parameterized with to the iterator.\n    using SwitchInstType = SwitchInstT;\n\n    SwitchInstT *SI;\n    ptrdiff_t Index;\n\n    CaseHandleImpl() = default;\n    CaseHandleImpl(SwitchInstT *SI, ptrdiff_t Index) : SI(SI), Index(Index) {}\n\n  public:\n    /// Resolves case value for current case.\n    ConstantIntT *getCaseValue() const {\n      assert((unsigned)Index < SI->getNumCases() &&\n             \"Index out the number of cases.\");\n      return reinterpret_cast<ConstantIntT *>(SI->getOperand(2 + Index * 2));\n    }\n\n    /// Resolves successor for current case.\n    BasicBlockT *getCaseSuccessor() const {\n      assert(((unsigned)Index < SI->getNumCases() ||\n              (unsigned)Index == DefaultPseudoIndex) &&\n             \"Index out the number of cases.\");\n      return SI->getSuccessor(getSuccessorIndex());\n    }\n\n    /// Returns number of current case.\n    unsigned getCaseIndex() const { return Index; }\n\n    /// Returns successor index for current case successor.\n    unsigned getSuccessorIndex() const {\n      assert(((unsigned)Index == DefaultPseudoIndex ||\n              (unsigned)Index < SI->getNumCases()) &&\n             \"Index out the number of cases.\");\n      return (unsigned)Index != DefaultPseudoIndex ? Index + 1 : 0;\n    }\n\n    bool operator==(const CaseHandleImpl &RHS) const {\n      assert(SI == RHS.SI && \"Incompatible operators.\");\n      return Index == RHS.Index;\n    }\n  };\n\n  using ConstCaseHandle =\n      CaseHandleImpl<const SwitchInst, const ConstantInt, const BasicBlock>;\n\n  class CaseHandle\n      : public CaseHandleImpl<SwitchInst, ConstantInt, BasicBlock> {\n    friend class SwitchInst::CaseIteratorImpl<CaseHandle>;\n\n  public:\n    CaseHandle(SwitchInst *SI, ptrdiff_t Index) : CaseHandleImpl(SI, Index) {}\n\n    /// Sets the new value for current case.\n    void setValue(ConstantInt *V) {\n      assert((unsigned)Index < SI->getNumCases() &&\n             \"Index out the number of cases.\");\n      SI->setOperand(2 + Index*2, reinterpret_cast<Value*>(V));\n    }\n\n    /// Sets the new successor for current case.\n    void setSuccessor(BasicBlock *S) {\n      SI->setSuccessor(getSuccessorIndex(), S);\n    }\n  };\n\n  template <typename CaseHandleT>\n  class CaseIteratorImpl\n      : public iterator_facade_base<CaseIteratorImpl<CaseHandleT>,\n                                    std::random_access_iterator_tag,\n                                    CaseHandleT> {\n    using SwitchInstT = typename CaseHandleT::SwitchInstType;\n\n    CaseHandleT Case;\n\n  public:\n    /// Default constructed iterator is in an invalid state until assigned to\n    /// a case for a particular switch.\n    CaseIteratorImpl() = default;\n\n    /// Initializes case iterator for given SwitchInst and for given\n    /// case number.\n    CaseIteratorImpl(SwitchInstT *SI, unsigned CaseNum) : Case(SI, CaseNum) {}\n\n    /// Initializes case iterator for given SwitchInst and for given\n    /// successor index.\n    static CaseIteratorImpl fromSuccessorIndex(SwitchInstT *SI,\n                                               unsigned SuccessorIndex) {\n      assert(SuccessorIndex < SI->getNumSuccessors() &&\n             \"Successor index # out of range!\");\n      return SuccessorIndex != 0 ? CaseIteratorImpl(SI, SuccessorIndex - 1)\n                                 : CaseIteratorImpl(SI, DefaultPseudoIndex);\n    }\n\n    /// Support converting to the const variant. This will be a no-op for const\n    /// variant.\n    operator CaseIteratorImpl<ConstCaseHandle>() const {\n      return CaseIteratorImpl<ConstCaseHandle>(Case.SI, Case.Index);\n    }\n\n    CaseIteratorImpl &operator+=(ptrdiff_t N) {\n      // Check index correctness after addition.\n      // Note: Index == getNumCases() means end().\n      assert(Case.Index + N >= 0 &&\n             (unsigned)(Case.Index + N) <= Case.SI->getNumCases() &&\n             \"Case.Index out the number of cases.\");\n      Case.Index += N;\n      return *this;\n    }\n    CaseIteratorImpl &operator-=(ptrdiff_t N) {\n      // Check index correctness after subtraction.\n      // Note: Case.Index == getNumCases() means end().\n      assert(Case.Index - N >= 0 &&\n             (unsigned)(Case.Index - N) <= Case.SI->getNumCases() &&\n             \"Case.Index out the number of cases.\");\n      Case.Index -= N;\n      return *this;\n    }\n    ptrdiff_t operator-(const CaseIteratorImpl &RHS) const {\n      assert(Case.SI == RHS.Case.SI && \"Incompatible operators.\");\n      return Case.Index - RHS.Case.Index;\n    }\n    bool operator==(const CaseIteratorImpl &RHS) const {\n      return Case == RHS.Case;\n    }\n    bool operator<(const CaseIteratorImpl &RHS) const {\n      assert(Case.SI == RHS.Case.SI && \"Incompatible operators.\");\n      return Case.Index < RHS.Case.Index;\n    }\n    CaseHandleT &operator*() { return Case; }\n    const CaseHandleT &operator*() const { return Case; }\n  };\n\n  using CaseIt = CaseIteratorImpl<CaseHandle>;\n  using ConstCaseIt = CaseIteratorImpl<ConstCaseHandle>;\n\n  static SwitchInst *Create(Value *Value, BasicBlock *Default,\n                            unsigned NumCases,\n                            Instruction *InsertBefore = nullptr) {\n    return new SwitchInst(Value, Default, NumCases, InsertBefore);\n  }\n\n  static SwitchInst *Create(Value *Value, BasicBlock *Default,\n                            unsigned NumCases, BasicBlock *InsertAtEnd) {\n    return new SwitchInst(Value, Default, NumCases, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Accessor Methods for Switch stmt\n  Value *getCondition() const { return getOperand(0); }\n  void setCondition(Value *V) { setOperand(0, V); }\n\n  BasicBlock *getDefaultDest() const {\n    return cast<BasicBlock>(getOperand(1));\n  }\n\n  void setDefaultDest(BasicBlock *DefaultCase) {\n    setOperand(1, reinterpret_cast<Value*>(DefaultCase));\n  }\n\n  /// Return the number of 'cases' in this switch instruction, excluding the\n  /// default case.\n  unsigned getNumCases() const {\n    return getNumOperands()/2 - 1;\n  }\n\n  /// Returns a read/write iterator that points to the first case in the\n  /// SwitchInst.\n  CaseIt case_begin() {\n    return CaseIt(this, 0);\n  }\n\n  /// Returns a read-only iterator that points to the first case in the\n  /// SwitchInst.\n  ConstCaseIt case_begin() const {\n    return ConstCaseIt(this, 0);\n  }\n\n  /// Returns a read/write iterator that points one past the last in the\n  /// SwitchInst.\n  CaseIt case_end() {\n    return CaseIt(this, getNumCases());\n  }\n\n  /// Returns a read-only iterator that points one past the last in the\n  /// SwitchInst.\n  ConstCaseIt case_end() const {\n    return ConstCaseIt(this, getNumCases());\n  }\n\n  /// Iteration adapter for range-for loops.\n  iterator_range<CaseIt> cases() {\n    return make_range(case_begin(), case_end());\n  }\n\n  /// Constant iteration adapter for range-for loops.\n  iterator_range<ConstCaseIt> cases() const {\n    return make_range(case_begin(), case_end());\n  }\n\n  /// Returns an iterator that points to the default case.\n  /// Note: this iterator allows to resolve successor only. Attempt\n  /// to resolve case value causes an assertion.\n  /// Also note, that increment and decrement also causes an assertion and\n  /// makes iterator invalid.\n  CaseIt case_default() {\n    return CaseIt(this, DefaultPseudoIndex);\n  }\n  ConstCaseIt case_default() const {\n    return ConstCaseIt(this, DefaultPseudoIndex);\n  }\n\n  /// Search all of the case values for the specified constant. If it is\n  /// explicitly handled, return the case iterator of it, otherwise return\n  /// default case iterator to indicate that it is handled by the default\n  /// handler.\n  CaseIt findCaseValue(const ConstantInt *C) {\n    CaseIt I = llvm::find_if(\n        cases(), [C](CaseHandle &Case) { return Case.getCaseValue() == C; });\n    if (I != case_end())\n      return I;\n\n    return case_default();\n  }\n  ConstCaseIt findCaseValue(const ConstantInt *C) const {\n    ConstCaseIt I = llvm::find_if(cases(), [C](ConstCaseHandle &Case) {\n      return Case.getCaseValue() == C;\n    });\n    if (I != case_end())\n      return I;\n\n    return case_default();\n  }\n\n  /// Finds the unique case value for a given successor. Returns null if the\n  /// successor is not found, not unique, or is the default case.\n  ConstantInt *findCaseDest(BasicBlock *BB) {\n    if (BB == getDefaultDest())\n      return nullptr;\n\n    ConstantInt *CI = nullptr;\n    for (auto Case : cases()) {\n      if (Case.getCaseSuccessor() != BB)\n        continue;\n\n      if (CI)\n        return nullptr; // Multiple cases lead to BB.\n\n      CI = Case.getCaseValue();\n    }\n\n    return CI;\n  }\n\n  /// Add an entry to the switch instruction.\n  /// Note:\n  /// This action invalidates case_end(). Old case_end() iterator will\n  /// point to the added case.\n  void addCase(ConstantInt *OnVal, BasicBlock *Dest);\n\n  /// This method removes the specified case and its successor from the switch\n  /// instruction. Note that this operation may reorder the remaining cases at\n  /// index idx and above.\n  /// Note:\n  /// This action invalidates iterators for all cases following the one removed,\n  /// including the case_end() iterator. It returns an iterator for the next\n  /// case.\n  CaseIt removeCase(CaseIt I);\n\n  unsigned getNumSuccessors() const { return getNumOperands()/2; }\n  BasicBlock *getSuccessor(unsigned idx) const {\n    assert(idx < getNumSuccessors() &&\"Successor idx out of range for switch!\");\n    return cast<BasicBlock>(getOperand(idx*2+1));\n  }\n  void setSuccessor(unsigned idx, BasicBlock *NewSucc) {\n    assert(idx < getNumSuccessors() && \"Successor # out of range for switch!\");\n    setOperand(idx * 2 + 1, NewSucc);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Switch;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n/// A wrapper class to simplify modification of SwitchInst cases along with\n/// their prof branch_weights metadata.\nclass SwitchInstProfUpdateWrapper {\n  SwitchInst &SI;\n  Optional<SmallVector<uint32_t, 8> > Weights = None;\n  bool Changed = false;\n\nprotected:\n  static MDNode *getProfBranchWeightsMD(const SwitchInst &SI);\n\n  MDNode *buildProfBranchWeightsMD();\n\n  void init();\n\npublic:\n  using CaseWeightOpt = Optional<uint32_t>;\n  SwitchInst *operator->() { return &SI; }\n  SwitchInst &operator*() { return SI; }\n  operator SwitchInst *() { return &SI; }\n\n  SwitchInstProfUpdateWrapper(SwitchInst &SI) : SI(SI) { init(); }\n\n  ~SwitchInstProfUpdateWrapper() {\n    if (Changed)\n      SI.setMetadata(LLVMContext::MD_prof, buildProfBranchWeightsMD());\n  }\n\n  /// Delegate the call to the underlying SwitchInst::removeCase() and remove\n  /// correspondent branch weight.\n  SwitchInst::CaseIt removeCase(SwitchInst::CaseIt I);\n\n  /// Delegate the call to the underlying SwitchInst::addCase() and set the\n  /// specified branch weight for the added case.\n  void addCase(ConstantInt *OnVal, BasicBlock *Dest, CaseWeightOpt W);\n\n  /// Delegate the call to the underlying SwitchInst::eraseFromParent() and mark\n  /// this object to not touch the underlying SwitchInst in destructor.\n  SymbolTableList<Instruction>::iterator eraseFromParent();\n\n  void setSuccessorWeight(unsigned idx, CaseWeightOpt W);\n  CaseWeightOpt getSuccessorWeight(unsigned idx);\n\n  static CaseWeightOpt getSuccessorWeight(const SwitchInst &SI, unsigned idx);\n};\n\ntemplate <>\nstruct OperandTraits<SwitchInst> : public HungoffOperandTraits<2> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(SwitchInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                             IndirectBrInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Indirect Branch Instruction.\n///\nclass IndirectBrInst : public Instruction {\n  unsigned ReservedSpace;\n\n  // Operand[0]   = Address to jump to\n  // Operand[n+1] = n-th destination\n  IndirectBrInst(const IndirectBrInst &IBI);\n\n  /// Create a new indirectbr instruction, specifying an\n  /// Address to jump to.  The number of expected destinations can be specified\n  /// here to make memory allocation more efficient.  This constructor can also\n  /// autoinsert before another instruction.\n  IndirectBrInst(Value *Address, unsigned NumDests, Instruction *InsertBefore);\n\n  /// Create a new indirectbr instruction, specifying an\n  /// Address to jump to.  The number of expected destinations can be specified\n  /// here to make memory allocation more efficient.  This constructor also\n  /// autoinserts at the end of the specified BasicBlock.\n  IndirectBrInst(Value *Address, unsigned NumDests, BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s);\n  }\n\n  void init(Value *Address, unsigned NumDests);\n  void growOperands();\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  IndirectBrInst *cloneImpl() const;\n\npublic:\n  /// Iterator type that casts an operand to a basic block.\n  ///\n  /// This only makes sense because the successors are stored as adjacent\n  /// operands for indirectbr instructions.\n  struct succ_op_iterator\n      : iterator_adaptor_base<succ_op_iterator, value_op_iterator,\n                              std::random_access_iterator_tag, BasicBlock *,\n                              ptrdiff_t, BasicBlock *, BasicBlock *> {\n    explicit succ_op_iterator(value_op_iterator I) : iterator_adaptor_base(I) {}\n\n    BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    BasicBlock *operator->() const { return operator*(); }\n  };\n\n  /// The const version of `succ_op_iterator`.\n  struct const_succ_op_iterator\n      : iterator_adaptor_base<const_succ_op_iterator, const_value_op_iterator,\n                              std::random_access_iterator_tag,\n                              const BasicBlock *, ptrdiff_t, const BasicBlock *,\n                              const BasicBlock *> {\n    explicit const_succ_op_iterator(const_value_op_iterator I)\n        : iterator_adaptor_base(I) {}\n\n    const BasicBlock *operator*() const { return cast<BasicBlock>(*I); }\n    const BasicBlock *operator->() const { return operator*(); }\n  };\n\n  static IndirectBrInst *Create(Value *Address, unsigned NumDests,\n                                Instruction *InsertBefore = nullptr) {\n    return new IndirectBrInst(Address, NumDests, InsertBefore);\n  }\n\n  static IndirectBrInst *Create(Value *Address, unsigned NumDests,\n                                BasicBlock *InsertAtEnd) {\n    return new IndirectBrInst(Address, NumDests, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors.\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Accessor Methods for IndirectBrInst instruction.\n  Value *getAddress() { return getOperand(0); }\n  const Value *getAddress() const { return getOperand(0); }\n  void setAddress(Value *V) { setOperand(0, V); }\n\n  /// return the number of possible destinations in this\n  /// indirectbr instruction.\n  unsigned getNumDestinations() const { return getNumOperands()-1; }\n\n  /// Return the specified destination.\n  BasicBlock *getDestination(unsigned i) { return getSuccessor(i); }\n  const BasicBlock *getDestination(unsigned i) const { return getSuccessor(i); }\n\n  /// Add a destination.\n  ///\n  void addDestination(BasicBlock *Dest);\n\n  /// This method removes the specified successor from the\n  /// indirectbr instruction.\n  void removeDestination(unsigned i);\n\n  unsigned getNumSuccessors() const { return getNumOperands()-1; }\n  BasicBlock *getSuccessor(unsigned i) const {\n    return cast<BasicBlock>(getOperand(i+1));\n  }\n  void setSuccessor(unsigned i, BasicBlock *NewSucc) {\n    setOperand(i + 1, NewSucc);\n  }\n\n  iterator_range<succ_op_iterator> successors() {\n    return make_range(succ_op_iterator(std::next(value_op_begin())),\n                      succ_op_iterator(value_op_end()));\n  }\n\n  iterator_range<const_succ_op_iterator> successors() const {\n    return make_range(const_succ_op_iterator(std::next(value_op_begin())),\n                      const_succ_op_iterator(value_op_end()));\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::IndirectBr;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<IndirectBrInst> : public HungoffOperandTraits<1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(IndirectBrInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               InvokeInst Class\n//===----------------------------------------------------------------------===//\n\n/// Invoke instruction.  The SubclassData field is used to hold the\n/// calling convention of the call.\n///\nclass InvokeInst : public CallBase {\n  /// The number of operands for this call beyond the called function,\n  /// arguments, and operand bundles.\n  static constexpr int NumExtraOperands = 2;\n\n  /// The index from the end of the operand array to the normal destination.\n  static constexpr int NormalDestOpEndIdx = -3;\n\n  /// The index from the end of the operand array to the unwind destination.\n  static constexpr int UnwindDestOpEndIdx = -2;\n\n  InvokeInst(const InvokeInst &BI);\n\n  /// Construct an InvokeInst given a range of arguments.\n  ///\n  /// Construct an InvokeInst from a range of arguments\n  inline InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                    BasicBlock *IfException, ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, Instruction *InsertBefore);\n\n  inline InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                    BasicBlock *IfException, ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n            BasicBlock *IfException, ArrayRef<Value *> Args,\n            ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr);\n\n  /// Compute the number of operands to allocate.\n  static int ComputeNumOperands(int NumArgs, int NumBundleInputs = 0) {\n    // We need one operand for the called function, plus our extra operands and\n    // the input operand counts provided.\n    return 1 + NumExtraOperands + NumArgs + NumBundleInputs;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  InvokeInst *cloneImpl() const;\n\npublic:\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands = ComputeNumOperands(Args.size());\n    return new (NumOperands)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, None, NumOperands,\n                   NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, Bundles, NumOperands,\n                   NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    int NumOperands = ComputeNumOperands(Args.size());\n    return new (NumOperands)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, None, NumOperands,\n                   NameStr, InsertAtEnd);\n  }\n\n  static InvokeInst *Create(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    int NumOperands =\n        ComputeNumOperands(Args.size(), CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        InvokeInst(Ty, Func, IfNormal, IfException, Args, Bundles, NumOperands,\n                   NameStr, InsertAtEnd);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, None, NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, Bundles, NameStr, InsertBefore);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, NameStr, InsertAtEnd);\n  }\n\n  static InvokeInst *Create(FunctionCallee Func, BasicBlock *IfNormal,\n                            BasicBlock *IfException, ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), IfNormal,\n                  IfException, Args, Bundles, NameStr, InsertAtEnd);\n  }\n\n  /// Create a clone of \\p II with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned invoke instruction is identical to \\p II in every way except\n  /// that the operand bundles for the new instruction are set to the operand\n  /// bundles in \\p Bundles.\n  static InvokeInst *Create(InvokeInst *II, ArrayRef<OperandBundleDef> Bundles,\n                            Instruction *InsertPt = nullptr);\n\n  // get*Dest - Return the destination basic blocks...\n  BasicBlock *getNormalDest() const {\n    return cast<BasicBlock>(Op<NormalDestOpEndIdx>());\n  }\n  BasicBlock *getUnwindDest() const {\n    return cast<BasicBlock>(Op<UnwindDestOpEndIdx>());\n  }\n  void setNormalDest(BasicBlock *B) {\n    Op<NormalDestOpEndIdx>() = reinterpret_cast<Value *>(B);\n  }\n  void setUnwindDest(BasicBlock *B) {\n    Op<UnwindDestOpEndIdx>() = reinterpret_cast<Value *>(B);\n  }\n\n  /// Get the landingpad instruction from the landing pad\n  /// block (the unwind destination).\n  LandingPadInst *getLandingPadInst() const;\n\n  BasicBlock *getSuccessor(unsigned i) const {\n    assert(i < 2 && \"Successor # out of range for invoke!\");\n    return i == 0 ? getNormalDest() : getUnwindDest();\n  }\n\n  void setSuccessor(unsigned i, BasicBlock *NewSucc) {\n    assert(i < 2 && \"Successor # out of range for invoke!\");\n    if (i == 0)\n      setNormalDest(NewSucc);\n    else\n      setUnwindDest(NewSucc);\n  }\n\n  unsigned getNumSuccessors() const { return 2; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::Invoke);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\nInvokeInst::InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                       BasicBlock *IfException, ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, Instruction *InsertBefore)\n    : CallBase(Ty->getReturnType(), Instruction::Invoke,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertBefore) {\n  init(Ty, Func, IfNormal, IfException, Args, Bundles, NameStr);\n}\n\nInvokeInst::InvokeInst(FunctionType *Ty, Value *Func, BasicBlock *IfNormal,\n                       BasicBlock *IfException, ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, BasicBlock *InsertAtEnd)\n    : CallBase(Ty->getReturnType(), Instruction::Invoke,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertAtEnd) {\n  init(Ty, Func, IfNormal, IfException, Args, Bundles, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                              CallBrInst Class\n//===----------------------------------------------------------------------===//\n\n/// CallBr instruction, tracking function calls that may not return control but\n/// instead transfer it to a third location. The SubclassData field is used to\n/// hold the calling convention of the call.\n///\nclass CallBrInst : public CallBase {\n\n  unsigned NumIndirectDests;\n\n  CallBrInst(const CallBrInst &BI);\n\n  /// Construct a CallBrInst given a range of arguments.\n  ///\n  /// Construct a CallBrInst from a range of arguments\n  inline CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                    ArrayRef<BasicBlock *> IndirectDests,\n                    ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, Instruction *InsertBefore);\n\n  inline CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                    ArrayRef<BasicBlock *> IndirectDests,\n                    ArrayRef<Value *> Args,\n                    ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                    const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  void init(FunctionType *FTy, Value *Func, BasicBlock *DefaultDest,\n            ArrayRef<BasicBlock *> IndirectDests, ArrayRef<Value *> Args,\n            ArrayRef<OperandBundleDef> Bundles, const Twine &NameStr);\n\n  /// Should the Indirect Destinations change, scan + update the Arg list.\n  void updateArgBlockAddresses(unsigned i, BasicBlock *B);\n\n  /// Compute the number of operands to allocate.\n  static int ComputeNumOperands(int NumArgs, int NumIndirectDests,\n                                int NumBundleInputs = 0) {\n    // We need one operand for the called function, plus our extra operands and\n    // the input operand counts provided.\n    return 2 + NumIndirectDests + NumArgs + NumBundleInputs;\n  }\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CallBrInst *cloneImpl() const;\n\npublic:\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size());\n    return new (NumOperands)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, None,\n                   NumOperands, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size(),\n                                         CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, Bundles,\n                   NumOperands, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            BasicBlock *InsertAtEnd) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size());\n    return new (NumOperands)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, None,\n                   NumOperands, NameStr, InsertAtEnd);\n  }\n\n  static CallBrInst *Create(FunctionType *Ty, Value *Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    int NumOperands = ComputeNumOperands(Args.size(), IndirectDests.size(),\n                                         CountBundleInputs(Bundles));\n    unsigned DescriptorBytes = Bundles.size() * sizeof(BundleOpInfo);\n\n    return new (NumOperands, DescriptorBytes)\n        CallBrInst(Ty, Func, DefaultDest, IndirectDests, Args, Bundles,\n                   NumOperands, NameStr, InsertAtEnd);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func, BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func, BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles = None,\n                            const Twine &NameStr = \"\",\n                            Instruction *InsertBefore = nullptr) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, Bundles, NameStr, InsertBefore);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func, BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args, const Twine &NameStr,\n                            BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, NameStr, InsertAtEnd);\n  }\n\n  static CallBrInst *Create(FunctionCallee Func,\n                            BasicBlock *DefaultDest,\n                            ArrayRef<BasicBlock *> IndirectDests,\n                            ArrayRef<Value *> Args,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    return Create(Func.getFunctionType(), Func.getCallee(), DefaultDest,\n                  IndirectDests, Args, Bundles, NameStr, InsertAtEnd);\n  }\n\n  /// Create a clone of \\p CBI with a different set of operand bundles and\n  /// insert it before \\p InsertPt.\n  ///\n  /// The returned callbr instruction is identical to \\p CBI in every way\n  /// except that the operand bundles for the new instruction are set to the\n  /// operand bundles in \\p Bundles.\n  static CallBrInst *Create(CallBrInst *CBI,\n                            ArrayRef<OperandBundleDef> Bundles,\n                            Instruction *InsertPt = nullptr);\n\n  /// Return the number of callbr indirect dest labels.\n  ///\n  unsigned getNumIndirectDests() const { return NumIndirectDests; }\n\n  /// getIndirectDestLabel - Return the i-th indirect dest label.\n  ///\n  Value *getIndirectDestLabel(unsigned i) const {\n    assert(i < getNumIndirectDests() && \"Out of bounds!\");\n    return getOperand(i + getNumArgOperands() + getNumTotalBundleOperands() +\n                      1);\n  }\n\n  Value *getIndirectDestLabelUse(unsigned i) const {\n    assert(i < getNumIndirectDests() && \"Out of bounds!\");\n    return getOperandUse(i + getNumArgOperands() + getNumTotalBundleOperands() +\n                         1);\n  }\n\n  // Return the destination basic blocks...\n  BasicBlock *getDefaultDest() const {\n    return cast<BasicBlock>(*(&Op<-1>() - getNumIndirectDests() - 1));\n  }\n  BasicBlock *getIndirectDest(unsigned i) const {\n    return cast_or_null<BasicBlock>(*(&Op<-1>() - getNumIndirectDests() + i));\n  }\n  SmallVector<BasicBlock *, 16> getIndirectDests() const {\n    SmallVector<BasicBlock *, 16> IndirectDests;\n    for (unsigned i = 0, e = getNumIndirectDests(); i < e; ++i)\n      IndirectDests.push_back(getIndirectDest(i));\n    return IndirectDests;\n  }\n  void setDefaultDest(BasicBlock *B) {\n    *(&Op<-1>() - getNumIndirectDests() - 1) = reinterpret_cast<Value *>(B);\n  }\n  void setIndirectDest(unsigned i, BasicBlock *B) {\n    updateArgBlockAddresses(i, B);\n    *(&Op<-1>() - getNumIndirectDests() + i) = reinterpret_cast<Value *>(B);\n  }\n\n  BasicBlock *getSuccessor(unsigned i) const {\n    assert(i < getNumSuccessors() + 1 &&\n           \"Successor # out of range for callbr!\");\n    return i == 0 ? getDefaultDest() : getIndirectDest(i - 1);\n  }\n\n  void setSuccessor(unsigned i, BasicBlock *NewSucc) {\n    assert(i < getNumIndirectDests() + 1 &&\n           \"Successor # out of range for callbr!\");\n    return i == 0 ? setDefaultDest(NewSucc) : setIndirectDest(i - 1, NewSucc);\n  }\n\n  unsigned getNumSuccessors() const { return getNumIndirectDests() + 1; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::CallBr);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\nCallBrInst::CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                       ArrayRef<BasicBlock *> IndirectDests,\n                       ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, Instruction *InsertBefore)\n    : CallBase(Ty->getReturnType(), Instruction::CallBr,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertBefore) {\n  init(Ty, Func, DefaultDest, IndirectDests, Args, Bundles, NameStr);\n}\n\nCallBrInst::CallBrInst(FunctionType *Ty, Value *Func, BasicBlock *DefaultDest,\n                       ArrayRef<BasicBlock *> IndirectDests,\n                       ArrayRef<Value *> Args,\n                       ArrayRef<OperandBundleDef> Bundles, int NumOperands,\n                       const Twine &NameStr, BasicBlock *InsertAtEnd)\n    : CallBase(Ty->getReturnType(), Instruction::CallBr,\n               OperandTraits<CallBase>::op_end(this) - NumOperands, NumOperands,\n               InsertAtEnd) {\n  init(Ty, Func, DefaultDest, IndirectDests, Args, Bundles, NameStr);\n}\n\n//===----------------------------------------------------------------------===//\n//                              ResumeInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// Resume the propagation of an exception.\n///\nclass ResumeInst : public Instruction {\n  ResumeInst(const ResumeInst &RI);\n\n  explicit ResumeInst(Value *Exn, Instruction *InsertBefore=nullptr);\n  ResumeInst(Value *Exn, BasicBlock *InsertAtEnd);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  ResumeInst *cloneImpl() const;\n\npublic:\n  static ResumeInst *Create(Value *Exn, Instruction *InsertBefore = nullptr) {\n    return new(1) ResumeInst(Exn, InsertBefore);\n  }\n\n  static ResumeInst *Create(Value *Exn, BasicBlock *InsertAtEnd) {\n    return new(1) ResumeInst(Exn, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Convenience accessor.\n  Value *getValue() const { return Op<0>(); }\n\n  unsigned getNumSuccessors() const { return 0; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Resume;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned idx) const {\n    llvm_unreachable(\"ResumeInst has no successors!\");\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *NewSucc) {\n    llvm_unreachable(\"ResumeInst has no successors!\");\n  }\n};\n\ntemplate <>\nstruct OperandTraits<ResumeInst> :\n    public FixedNumOperandTraits<ResumeInst, 1> {\n};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(ResumeInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                         CatchSwitchInst Class\n//===----------------------------------------------------------------------===//\nclass CatchSwitchInst : public Instruction {\n  using UnwindDestField = BoolBitfieldElementT<0>;\n\n  /// The number of operands actually allocated.  NumOperands is\n  /// the number actually in use.\n  unsigned ReservedSpace;\n\n  // Operand[0] = Outer scope\n  // Operand[1] = Unwind block destination\n  // Operand[n] = BasicBlock to go to on match\n  CatchSwitchInst(const CatchSwitchInst &CSI);\n\n  /// Create a new switch instruction, specifying a\n  /// default destination.  The number of additional handlers can be specified\n  /// here to make memory allocation more efficient.\n  /// This constructor can also autoinsert before another instruction.\n  CatchSwitchInst(Value *ParentPad, BasicBlock *UnwindDest,\n                  unsigned NumHandlers, const Twine &NameStr,\n                  Instruction *InsertBefore);\n\n  /// Create a new switch instruction, specifying a\n  /// default destination.  The number of additional handlers can be specified\n  /// here to make memory allocation more efficient.\n  /// This constructor also autoinserts at the end of the specified BasicBlock.\n  CatchSwitchInst(Value *ParentPad, BasicBlock *UnwindDest,\n                  unsigned NumHandlers, const Twine &NameStr,\n                  BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) { return User::operator new(s); }\n\n  void init(Value *ParentPad, BasicBlock *UnwindDest, unsigned NumReserved);\n  void growOperands(unsigned Size);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CatchSwitchInst *cloneImpl() const;\n\npublic:\n  static CatchSwitchInst *Create(Value *ParentPad, BasicBlock *UnwindDest,\n                                 unsigned NumHandlers,\n                                 const Twine &NameStr = \"\",\n                                 Instruction *InsertBefore = nullptr) {\n    return new CatchSwitchInst(ParentPad, UnwindDest, NumHandlers, NameStr,\n                               InsertBefore);\n  }\n\n  static CatchSwitchInst *Create(Value *ParentPad, BasicBlock *UnwindDest,\n                                 unsigned NumHandlers, const Twine &NameStr,\n                                 BasicBlock *InsertAtEnd) {\n    return new CatchSwitchInst(ParentPad, UnwindDest, NumHandlers, NameStr,\n                               InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  // Accessor Methods for CatchSwitch stmt\n  Value *getParentPad() const { return getOperand(0); }\n  void setParentPad(Value *ParentPad) { setOperand(0, ParentPad); }\n\n  // Accessor Methods for CatchSwitch stmt\n  bool hasUnwindDest() const { return getSubclassData<UnwindDestField>(); }\n  bool unwindsToCaller() const { return !hasUnwindDest(); }\n  BasicBlock *getUnwindDest() const {\n    if (hasUnwindDest())\n      return cast<BasicBlock>(getOperand(1));\n    return nullptr;\n  }\n  void setUnwindDest(BasicBlock *UnwindDest) {\n    assert(UnwindDest);\n    assert(hasUnwindDest());\n    setOperand(1, UnwindDest);\n  }\n\n  /// return the number of 'handlers' in this catchswitch\n  /// instruction, except the default handler\n  unsigned getNumHandlers() const {\n    if (hasUnwindDest())\n      return getNumOperands() - 2;\n    return getNumOperands() - 1;\n  }\n\nprivate:\n  static BasicBlock *handler_helper(Value *V) { return cast<BasicBlock>(V); }\n  static const BasicBlock *handler_helper(const Value *V) {\n    return cast<BasicBlock>(V);\n  }\n\npublic:\n  using DerefFnTy = BasicBlock *(*)(Value *);\n  using handler_iterator = mapped_iterator<op_iterator, DerefFnTy>;\n  using handler_range = iterator_range<handler_iterator>;\n  using ConstDerefFnTy = const BasicBlock *(*)(const Value *);\n  using const_handler_iterator =\n      mapped_iterator<const_op_iterator, ConstDerefFnTy>;\n  using const_handler_range = iterator_range<const_handler_iterator>;\n\n  /// Returns an iterator that points to the first handler in CatchSwitchInst.\n  handler_iterator handler_begin() {\n    op_iterator It = op_begin() + 1;\n    if (hasUnwindDest())\n      ++It;\n    return handler_iterator(It, DerefFnTy(handler_helper));\n  }\n\n  /// Returns an iterator that points to the first handler in the\n  /// CatchSwitchInst.\n  const_handler_iterator handler_begin() const {\n    const_op_iterator It = op_begin() + 1;\n    if (hasUnwindDest())\n      ++It;\n    return const_handler_iterator(It, ConstDerefFnTy(handler_helper));\n  }\n\n  /// Returns a read-only iterator that points one past the last\n  /// handler in the CatchSwitchInst.\n  handler_iterator handler_end() {\n    return handler_iterator(op_end(), DerefFnTy(handler_helper));\n  }\n\n  /// Returns an iterator that points one past the last handler in the\n  /// CatchSwitchInst.\n  const_handler_iterator handler_end() const {\n    return const_handler_iterator(op_end(), ConstDerefFnTy(handler_helper));\n  }\n\n  /// iteration adapter for range-for loops.\n  handler_range handlers() {\n    return make_range(handler_begin(), handler_end());\n  }\n\n  /// iteration adapter for range-for loops.\n  const_handler_range handlers() const {\n    return make_range(handler_begin(), handler_end());\n  }\n\n  /// Add an entry to the switch instruction...\n  /// Note:\n  /// This action invalidates handler_end(). Old handler_end() iterator will\n  /// point to the added handler.\n  void addHandler(BasicBlock *Dest);\n\n  void removeHandler(handler_iterator HI);\n\n  unsigned getNumSuccessors() const { return getNumOperands() - 1; }\n  BasicBlock *getSuccessor(unsigned Idx) const {\n    assert(Idx < getNumSuccessors() &&\n           \"Successor # out of range for catchswitch!\");\n    return cast<BasicBlock>(getOperand(Idx + 1));\n  }\n  void setSuccessor(unsigned Idx, BasicBlock *NewSucc) {\n    assert(Idx < getNumSuccessors() &&\n           \"Successor # out of range for catchswitch!\");\n    setOperand(Idx + 1, NewSucc);\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::CatchSwitch;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CatchSwitchInst> : public HungoffOperandTraits<2> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CatchSwitchInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               CleanupPadInst Class\n//===----------------------------------------------------------------------===//\nclass CleanupPadInst : public FuncletPadInst {\nprivate:\n  explicit CleanupPadInst(Value *ParentPad, ArrayRef<Value *> Args,\n                          unsigned Values, const Twine &NameStr,\n                          Instruction *InsertBefore)\n      : FuncletPadInst(Instruction::CleanupPad, ParentPad, Args, Values,\n                       NameStr, InsertBefore) {}\n  explicit CleanupPadInst(Value *ParentPad, ArrayRef<Value *> Args,\n                          unsigned Values, const Twine &NameStr,\n                          BasicBlock *InsertAtEnd)\n      : FuncletPadInst(Instruction::CleanupPad, ParentPad, Args, Values,\n                       NameStr, InsertAtEnd) {}\n\npublic:\n  static CleanupPadInst *Create(Value *ParentPad, ArrayRef<Value *> Args = None,\n                                const Twine &NameStr = \"\",\n                                Instruction *InsertBefore = nullptr) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CleanupPadInst(ParentPad, Args, Values, NameStr, InsertBefore);\n  }\n\n  static CleanupPadInst *Create(Value *ParentPad, ArrayRef<Value *> Args,\n                                const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CleanupPadInst(ParentPad, Args, Values, NameStr, InsertAtEnd);\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::CleanupPad;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               CatchPadInst Class\n//===----------------------------------------------------------------------===//\nclass CatchPadInst : public FuncletPadInst {\nprivate:\n  explicit CatchPadInst(Value *CatchSwitch, ArrayRef<Value *> Args,\n                        unsigned Values, const Twine &NameStr,\n                        Instruction *InsertBefore)\n      : FuncletPadInst(Instruction::CatchPad, CatchSwitch, Args, Values,\n                       NameStr, InsertBefore) {}\n  explicit CatchPadInst(Value *CatchSwitch, ArrayRef<Value *> Args,\n                        unsigned Values, const Twine &NameStr,\n                        BasicBlock *InsertAtEnd)\n      : FuncletPadInst(Instruction::CatchPad, CatchSwitch, Args, Values,\n                       NameStr, InsertAtEnd) {}\n\npublic:\n  static CatchPadInst *Create(Value *CatchSwitch, ArrayRef<Value *> Args,\n                              const Twine &NameStr = \"\",\n                              Instruction *InsertBefore = nullptr) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CatchPadInst(CatchSwitch, Args, Values, NameStr, InsertBefore);\n  }\n\n  static CatchPadInst *Create(Value *CatchSwitch, ArrayRef<Value *> Args,\n                              const Twine &NameStr, BasicBlock *InsertAtEnd) {\n    unsigned Values = 1 + Args.size();\n    return new (Values)\n        CatchPadInst(CatchSwitch, Args, Values, NameStr, InsertAtEnd);\n  }\n\n  /// Convenience accessors\n  CatchSwitchInst *getCatchSwitch() const {\n    return cast<CatchSwitchInst>(Op<-1>());\n  }\n  void setCatchSwitch(Value *CatchSwitch) {\n    assert(CatchSwitch);\n    Op<-1>() = CatchSwitch;\n  }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::CatchPad;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                               CatchReturnInst Class\n//===----------------------------------------------------------------------===//\n\nclass CatchReturnInst : public Instruction {\n  CatchReturnInst(const CatchReturnInst &RI);\n  CatchReturnInst(Value *CatchPad, BasicBlock *BB, Instruction *InsertBefore);\n  CatchReturnInst(Value *CatchPad, BasicBlock *BB, BasicBlock *InsertAtEnd);\n\n  void init(Value *CatchPad, BasicBlock *BB);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CatchReturnInst *cloneImpl() const;\n\npublic:\n  static CatchReturnInst *Create(Value *CatchPad, BasicBlock *BB,\n                                 Instruction *InsertBefore = nullptr) {\n    assert(CatchPad);\n    assert(BB);\n    return new (2) CatchReturnInst(CatchPad, BB, InsertBefore);\n  }\n\n  static CatchReturnInst *Create(Value *CatchPad, BasicBlock *BB,\n                                 BasicBlock *InsertAtEnd) {\n    assert(CatchPad);\n    assert(BB);\n    return new (2) CatchReturnInst(CatchPad, BB, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  /// Convenience accessors.\n  CatchPadInst *getCatchPad() const { return cast<CatchPadInst>(Op<0>()); }\n  void setCatchPad(CatchPadInst *CatchPad) {\n    assert(CatchPad);\n    Op<0>() = CatchPad;\n  }\n\n  BasicBlock *getSuccessor() const { return cast<BasicBlock>(Op<1>()); }\n  void setSuccessor(BasicBlock *NewSucc) {\n    assert(NewSucc);\n    Op<1>() = NewSucc;\n  }\n  unsigned getNumSuccessors() const { return 1; }\n\n  /// Get the parentPad of this catchret's catchpad's catchswitch.\n  /// The successor block is implicitly a member of this funclet.\n  Value *getCatchSwitchParentPad() const {\n    return getCatchPad()->getCatchSwitch()->getParentPad();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::CatchRet);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned Idx) const {\n    assert(Idx < getNumSuccessors() && \"Successor # out of range for catchret!\");\n    return getSuccessor();\n  }\n\n  void setSuccessor(unsigned Idx, BasicBlock *B) {\n    assert(Idx < getNumSuccessors() && \"Successor # out of range for catchret!\");\n    setSuccessor(B);\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CatchReturnInst>\n    : public FixedNumOperandTraits<CatchReturnInst, 2> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CatchReturnInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                               CleanupReturnInst Class\n//===----------------------------------------------------------------------===//\n\nclass CleanupReturnInst : public Instruction {\n  using UnwindDestField = BoolBitfieldElementT<0>;\n\nprivate:\n  CleanupReturnInst(const CleanupReturnInst &RI);\n  CleanupReturnInst(Value *CleanupPad, BasicBlock *UnwindBB, unsigned Values,\n                    Instruction *InsertBefore = nullptr);\n  CleanupReturnInst(Value *CleanupPad, BasicBlock *UnwindBB, unsigned Values,\n                    BasicBlock *InsertAtEnd);\n\n  void init(Value *CleanupPad, BasicBlock *UnwindBB);\n\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  CleanupReturnInst *cloneImpl() const;\n\npublic:\n  static CleanupReturnInst *Create(Value *CleanupPad,\n                                   BasicBlock *UnwindBB = nullptr,\n                                   Instruction *InsertBefore = nullptr) {\n    assert(CleanupPad);\n    unsigned Values = 1;\n    if (UnwindBB)\n      ++Values;\n    return new (Values)\n        CleanupReturnInst(CleanupPad, UnwindBB, Values, InsertBefore);\n  }\n\n  static CleanupReturnInst *Create(Value *CleanupPad, BasicBlock *UnwindBB,\n                                   BasicBlock *InsertAtEnd) {\n    assert(CleanupPad);\n    unsigned Values = 1;\n    if (UnwindBB)\n      ++Values;\n    return new (Values)\n        CleanupReturnInst(CleanupPad, UnwindBB, Values, InsertAtEnd);\n  }\n\n  /// Provide fast operand accessors\n  DECLARE_TRANSPARENT_OPERAND_ACCESSORS(Value);\n\n  bool hasUnwindDest() const { return getSubclassData<UnwindDestField>(); }\n  bool unwindsToCaller() const { return !hasUnwindDest(); }\n\n  /// Convenience accessor.\n  CleanupPadInst *getCleanupPad() const {\n    return cast<CleanupPadInst>(Op<0>());\n  }\n  void setCleanupPad(CleanupPadInst *CleanupPad) {\n    assert(CleanupPad);\n    Op<0>() = CleanupPad;\n  }\n\n  unsigned getNumSuccessors() const { return hasUnwindDest() ? 1 : 0; }\n\n  BasicBlock *getUnwindDest() const {\n    return hasUnwindDest() ? cast<BasicBlock>(Op<1>()) : nullptr;\n  }\n  void setUnwindDest(BasicBlock *NewDest) {\n    assert(NewDest);\n    assert(hasUnwindDest());\n    Op<1>() = NewDest;\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return (I->getOpcode() == Instruction::CleanupRet);\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned Idx) const {\n    assert(Idx == 0);\n    return getUnwindDest();\n  }\n\n  void setSuccessor(unsigned Idx, BasicBlock *B) {\n    assert(Idx == 0);\n    setUnwindDest(B);\n  }\n\n  // Shadow Instruction::setInstructionSubclassData with a private forwarding\n  // method so that subclasses cannot accidentally use it.\n  template <typename Bitfield>\n  void setSubclassData(typename Bitfield::Type Value) {\n    Instruction::setSubclassData<Bitfield>(Value);\n  }\n};\n\ntemplate <>\nstruct OperandTraits<CleanupReturnInst>\n    : public VariadicOperandTraits<CleanupReturnInst, /*MINARITY=*/1> {};\n\nDEFINE_TRANSPARENT_OPERAND_ACCESSORS(CleanupReturnInst, Value)\n\n//===----------------------------------------------------------------------===//\n//                           UnreachableInst Class\n//===----------------------------------------------------------------------===//\n\n//===---------------------------------------------------------------------------\n/// This function has undefined behavior.  In particular, the\n/// presence of this instruction indicates some higher level knowledge that the\n/// end of the block cannot be reached.\n///\nclass UnreachableInst : public Instruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  UnreachableInst *cloneImpl() const;\n\npublic:\n  explicit UnreachableInst(LLVMContext &C, Instruction *InsertBefore = nullptr);\n  explicit UnreachableInst(LLVMContext &C, BasicBlock *InsertAtEnd);\n\n  // allocate space for exactly zero operands\n  void *operator new(size_t s) {\n    return User::operator new(s, 0);\n  }\n\n  unsigned getNumSuccessors() const { return 0; }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Instruction::Unreachable;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\nprivate:\n  BasicBlock *getSuccessor(unsigned idx) const {\n    llvm_unreachable(\"UnreachableInst has no successors!\");\n  }\n\n  void setSuccessor(unsigned idx, BasicBlock *B) {\n    llvm_unreachable(\"UnreachableInst has no successors!\");\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 TruncInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a truncation of integer types.\nclass TruncInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical TruncInst\n  TruncInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  TruncInst(\n    Value *S,                           ///< The value to be truncated\n    Type *Ty,                           ///< The (smaller) type to truncate to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  TruncInst(\n    Value *S,                     ///< The value to be truncated\n    Type *Ty,                     ///< The (smaller) type to truncate to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == Trunc;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 ZExtInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents zero extension of integer types.\nclass ZExtInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical ZExtInst\n  ZExtInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  ZExtInst(\n    Value *S,                           ///< The value to be zero extended\n    Type *Ty,                           ///< The type to zero extend to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end semantics.\n  ZExtInst(\n    Value *S,                     ///< The value to be zero extended\n    Type *Ty,                     ///< The type to zero extend to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == ZExt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 SExtInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a sign extension of integer types.\nclass SExtInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical SExtInst\n  SExtInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  SExtInst(\n    Value *S,                           ///< The value to be sign extended\n    Type *Ty,                           ///< The type to sign extend to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  SExtInst(\n    Value *S,                     ///< The value to be sign extended\n    Type *Ty,                     ///< The type to sign extend to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == SExt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPTruncInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a truncation of floating point types.\nclass FPTruncInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPTruncInst\n  FPTruncInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPTruncInst(\n    Value *S,                           ///< The value to be truncated\n    Type *Ty,                           ///< The type to truncate to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-before-instruction semantics\n  FPTruncInst(\n    Value *S,                     ///< The value to be truncated\n    Type *Ty,                     ///< The type to truncate to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPTrunc;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPExtInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents an extension of floating point types.\nclass FPExtInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPExtInst\n  FPExtInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPExtInst(\n    Value *S,                           ///< The value to be extended\n    Type *Ty,                           ///< The type to extend to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  FPExtInst(\n    Value *S,                     ///< The value to be extended\n    Type *Ty,                     ///< The type to extend to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPExt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 UIToFPInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast unsigned integer to floating point.\nclass UIToFPInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical UIToFPInst\n  UIToFPInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  UIToFPInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  UIToFPInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == UIToFP;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 SIToFPInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from signed integer to floating point.\nclass SIToFPInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical SIToFPInst\n  SIToFPInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  SIToFPInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  SIToFPInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == SIToFP;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPToUIInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from floating point to unsigned integer\nclass FPToUIInst  : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPToUIInst\n  FPToUIInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPToUIInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  FPToUIInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< Where to insert the new instruction\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPToUI;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 FPToSIInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from floating point to signed integer.\nclass FPToSIInst  : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FPToSIInst\n  FPToSIInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  FPToSIInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  FPToSIInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == FPToSI;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 IntToPtrInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from an integer to a pointer.\nclass IntToPtrInst : public CastInst {\npublic:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Constructor with insert-before-instruction semantics\n  IntToPtrInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  IntToPtrInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Clone an identical IntToPtrInst.\n  IntToPtrInst *cloneImpl() const;\n\n  /// Returns the address space of this instruction's pointer type.\n  unsigned getAddressSpace() const {\n    return getType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == IntToPtr;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                                 PtrToIntInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a cast from a pointer to an integer.\nclass PtrToIntInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical PtrToIntInst.\n  PtrToIntInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  PtrToIntInst(\n    Value *S,                           ///< The value to be converted\n    Type *Ty,                           ///< The type to convert to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  PtrToIntInst(\n    Value *S,                     ///< The value to be converted\n    Type *Ty,                     ///< The type to convert to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  /// Gets the pointer operand.\n  Value *getPointerOperand() { return getOperand(0); }\n  /// Gets the pointer operand.\n  const Value *getPointerOperand() const { return getOperand(0); }\n  /// Gets the operand index of the pointer operand.\n  static unsigned getPointerOperandIndex() { return 0U; }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getPointerAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == PtrToInt;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                             BitCastInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a no-op cast from one type to another.\nclass BitCastInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical BitCastInst.\n  BitCastInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  BitCastInst(\n    Value *S,                           ///< The value to be casted\n    Type *Ty,                           ///< The type to casted to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  BitCastInst(\n    Value *S,                     ///< The value to be casted\n    Type *Ty,                     ///< The type to casted to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == BitCast;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n//===----------------------------------------------------------------------===//\n//                          AddrSpaceCastInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a conversion between pointers from one address space\n/// to another.\nclass AddrSpaceCastInst : public CastInst {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical AddrSpaceCastInst.\n  AddrSpaceCastInst *cloneImpl() const;\n\npublic:\n  /// Constructor with insert-before-instruction semantics\n  AddrSpaceCastInst(\n    Value *S,                           ///< The value to be casted\n    Type *Ty,                           ///< The type to casted to\n    const Twine &NameStr = \"\",          ///< A name for the new instruction\n    Instruction *InsertBefore = nullptr ///< Where to insert the new instruction\n  );\n\n  /// Constructor with insert-at-end-of-block semantics\n  AddrSpaceCastInst(\n    Value *S,                     ///< The value to be casted\n    Type *Ty,                     ///< The type to casted to\n    const Twine &NameStr,         ///< A name for the new instruction\n    BasicBlock *InsertAtEnd       ///< The block to insert the instruction into\n  );\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const Instruction *I) {\n    return I->getOpcode() == AddrSpaceCast;\n  }\n  static bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n\n  /// Gets the pointer operand.\n  Value *getPointerOperand() {\n    return getOperand(0);\n  }\n\n  /// Gets the pointer operand.\n  const Value *getPointerOperand() const {\n    return getOperand(0);\n  }\n\n  /// Gets the operand index of the pointer operand.\n  static unsigned getPointerOperandIndex() {\n    return 0U;\n  }\n\n  /// Returns the address space of the pointer operand.\n  unsigned getSrcAddressSpace() const {\n    return getPointerOperand()->getType()->getPointerAddressSpace();\n  }\n\n  /// Returns the address space of the result.\n  unsigned getDestAddressSpace() const {\n    return getType()->getPointerAddressSpace();\n  }\n};\n\n/// A helper function that returns the pointer operand of a load or store\n/// instruction. Returns nullptr if not load or store.\ninline const Value *getLoadStorePointerOperand(const Value *V) {\n  if (auto *Load = dyn_cast<LoadInst>(V))\n    return Load->getPointerOperand();\n  if (auto *Store = dyn_cast<StoreInst>(V))\n    return Store->getPointerOperand();\n  return nullptr;\n}\ninline Value *getLoadStorePointerOperand(Value *V) {\n  return const_cast<Value *>(\n      getLoadStorePointerOperand(static_cast<const Value *>(V)));\n}\n\n/// A helper function that returns the pointer operand of a load, store\n/// or GEP instruction. Returns nullptr if not load, store, or GEP.\ninline const Value *getPointerOperand(const Value *V) {\n  if (auto *Ptr = getLoadStorePointerOperand(V))\n    return Ptr;\n  if (auto *Gep = dyn_cast<GetElementPtrInst>(V))\n    return Gep->getPointerOperand();\n  return nullptr;\n}\ninline Value *getPointerOperand(Value *V) {\n  return const_cast<Value *>(getPointerOperand(static_cast<const Value *>(V)));\n}\n\n/// A helper function that returns the alignment of load or store instruction.\ninline Align getLoadStoreAlignment(Value *I) {\n  assert((isa<LoadInst>(I) || isa<StoreInst>(I)) &&\n         \"Expected Load or Store instruction\");\n  if (auto *LI = dyn_cast<LoadInst>(I))\n    return LI->getAlign();\n  return cast<StoreInst>(I)->getAlign();\n}\n\n/// A helper function that returns the address space of the pointer operand of\n/// load or store instruction.\ninline unsigned getLoadStoreAddressSpace(Value *I) {\n  assert((isa<LoadInst>(I) || isa<StoreInst>(I)) &&\n         \"Expected Load or Store instruction\");\n  if (auto *LI = dyn_cast<LoadInst>(I))\n    return LI->getPointerAddressSpace();\n  return cast<StoreInst>(I)->getPointerAddressSpace();\n}\n\n//===----------------------------------------------------------------------===//\n//                              FreezeInst Class\n//===----------------------------------------------------------------------===//\n\n/// This class represents a freeze function that returns random concrete\n/// value if an operand is either a poison value or an undef value\nclass FreezeInst : public UnaryInstruction {\nprotected:\n  // Note: Instruction needs to be a friend here to call cloneImpl.\n  friend class Instruction;\n\n  /// Clone an identical FreezeInst\n  FreezeInst *cloneImpl() const;\n\npublic:\n  explicit FreezeInst(Value *S,\n                      const Twine &NameStr = \"\",\n                      Instruction *InsertBefore = nullptr);\n  FreezeInst(Value *S, const Twine &NameStr, BasicBlock *InsertAtEnd);\n\n  // Methods for support type inquiry through isa, cast, and dyn_cast:\n  static inline bool classof(const Instruction *I) {\n    return I->getOpcode() == Freeze;\n  }\n  static inline bool classof(const Value *V) {\n    return isa<Instruction>(V) && classof(cast<Instruction>(V));\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_INSTRUCTIONS_H\n"}, "87": {"id": 87, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/SymbolTableListTraits.h", "content": "//===- llvm/SymbolTableListTraits.h - Traits for iplist ---------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines a generic class that is used to implement the automatic\n// symbol table manipulation that occurs when you put (for example) a named\n// instruction into a basic block.\n//\n// The way that this is implemented is by using a special traits class with the\n// intrusive list that makes up the list of instructions in a basic block.  When\n// a new element is added to the list of instructions, the traits class is\n// notified, allowing the symbol table to be updated.\n//\n// This generic class implements the traits class.  It must be generic so that\n// it can work for all uses it, which include lists of instructions, basic\n// blocks, arguments, functions, global variables, etc...\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_SYMBOLTABLELISTTRAITS_H\n#define LLVM_IR_SYMBOLTABLELISTTRAITS_H\n\n#include \"llvm/ADT/ilist.h\"\n#include \"llvm/ADT/simple_ilist.h\"\n#include <cstddef>\n\nnamespace llvm {\n\nclass Argument;\nclass BasicBlock;\nclass Function;\nclass GlobalAlias;\nclass GlobalIFunc;\nclass GlobalVariable;\nclass Instruction;\nclass Module;\nclass ValueSymbolTable;\n\n/// Template metafunction to get the parent type for a symbol table list.\n///\n/// Implementations create a typedef called \\c type so that we only need a\n/// single template parameter for the list and traits.\ntemplate <typename NodeTy> struct SymbolTableListParentType {};\n\n#define DEFINE_SYMBOL_TABLE_PARENT_TYPE(NODE, PARENT)                          \\\n  template <> struct SymbolTableListParentType<NODE> { using type = PARENT; };\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(Instruction, BasicBlock)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(BasicBlock, Function)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(Argument, Function)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(Function, Module)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(GlobalVariable, Module)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(GlobalAlias, Module)\nDEFINE_SYMBOL_TABLE_PARENT_TYPE(GlobalIFunc, Module)\n#undef DEFINE_SYMBOL_TABLE_PARENT_TYPE\n\ntemplate <typename NodeTy> class SymbolTableList;\n\n// ValueSubClass   - The type of objects that I hold, e.g. Instruction.\n// ItemParentClass - The type of object that owns the list, e.g. BasicBlock.\n//\ntemplate <typename ValueSubClass>\nclass SymbolTableListTraits : public ilist_alloc_traits<ValueSubClass> {\n  using ListTy = SymbolTableList<ValueSubClass>;\n  using iterator = typename simple_ilist<ValueSubClass>::iterator;\n  using ItemParentClass =\n      typename SymbolTableListParentType<ValueSubClass>::type;\n\npublic:\n  SymbolTableListTraits() = default;\n\nprivate:\n  /// getListOwner - Return the object that owns this list.  If this is a list\n  /// of instructions, it returns the BasicBlock that owns them.\n  ItemParentClass *getListOwner() {\n    size_t Offset = reinterpret_cast<size_t>(\n        &((ItemParentClass *)nullptr->*ItemParentClass::getSublistAccess(\n                                           static_cast<ValueSubClass *>(\n                                               nullptr))));\n    ListTy *Anchor = static_cast<ListTy *>(this);\n    return reinterpret_cast<ItemParentClass*>(reinterpret_cast<char*>(Anchor)-\n                                              Offset);\n  }\n\n  static ListTy &getList(ItemParentClass *Par) {\n    return Par->*(Par->getSublistAccess((ValueSubClass*)nullptr));\n  }\n\n  static ValueSymbolTable *getSymTab(ItemParentClass *Par) {\n    return Par ? toPtr(Par->getValueSymbolTable()) : nullptr;\n  }\n\npublic:\n  void addNodeToList(ValueSubClass *V);\n  void removeNodeFromList(ValueSubClass *V);\n  void transferNodesFromList(SymbolTableListTraits &L2, iterator first,\n                             iterator last);\n  // private:\n  template<typename TPtr>\n  void setSymTabObject(TPtr *, TPtr);\n  static ValueSymbolTable *toPtr(ValueSymbolTable *P) { return P; }\n  static ValueSymbolTable *toPtr(ValueSymbolTable &R) { return &R; }\n};\n\n/// List that automatically updates parent links and symbol tables.\n///\n/// When nodes are inserted into and removed from this list, the associated\n/// symbol table will be automatically updated.  Similarly, parent links get\n/// updated automatically.\ntemplate <class T>\nclass SymbolTableList\n    : public iplist_impl<simple_ilist<T>, SymbolTableListTraits<T>> {};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_SYMBOLTABLELISTTRAITS_H\n"}, "88": {"id": 88, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/TrackingMDRef.h", "content": "//===- llvm/IR/TrackingMDRef.h - Tracking Metadata references ---*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// References to metadata that track RAUW.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_IR_TRACKINGMDREF_H\n#define LLVM_IR_TRACKINGMDREF_H\n\n#include \"llvm/IR/Metadata.h\"\n#include <algorithm>\n#include <cassert>\n\nnamespace llvm {\n\n/// Tracking metadata reference.\n///\n/// This class behaves like \\a TrackingVH, but for metadata.\nclass TrackingMDRef {\n  Metadata *MD = nullptr;\n\npublic:\n  TrackingMDRef() = default;\n  explicit TrackingMDRef(Metadata *MD) : MD(MD) { track(); }\n\n  TrackingMDRef(TrackingMDRef &&X) : MD(X.MD) { retrack(X); }\n  TrackingMDRef(const TrackingMDRef &X) : MD(X.MD) { track(); }\n\n  TrackingMDRef &operator=(TrackingMDRef &&X) {\n    if (&X == this)\n      return *this;\n\n    untrack();\n    MD = X.MD;\n    retrack(X);\n    return *this;\n  }\n\n  TrackingMDRef &operator=(const TrackingMDRef &X) {\n    if (&X == this)\n      return *this;\n\n    untrack();\n    MD = X.MD;\n    track();\n    return *this;\n  }\n\n  ~TrackingMDRef() { untrack(); }\n\n  Metadata *get() const { return MD; }\n  operator Metadata *() const { return get(); }\n  Metadata *operator->() const { return get(); }\n  Metadata &operator*() const { return *get(); }\n\n  void reset() {\n    untrack();\n    MD = nullptr;\n  }\n  void reset(Metadata *MD) {\n    untrack();\n    this->MD = MD;\n    track();\n  }\n\n  /// Check whether this has a trivial destructor.\n  ///\n  /// If \\c MD isn't replaceable, the destructor will be a no-op.\n  bool hasTrivialDestructor() const {\n    return !MD || !MetadataTracking::isReplaceable(*MD);\n  }\n\n  bool operator==(const TrackingMDRef &X) const { return MD == X.MD; }\n  bool operator!=(const TrackingMDRef &X) const { return MD != X.MD; }\n\nprivate:\n  void track() {\n    if (MD)\n      MetadataTracking::track(MD);\n  }\n\n  void untrack() {\n    if (MD)\n      MetadataTracking::untrack(MD);\n  }\n\n  void retrack(TrackingMDRef &X) {\n    assert(MD == X.MD && \"Expected values to match\");\n    if (X.MD) {\n      MetadataTracking::retrack(X.MD, MD);\n      X.MD = nullptr;\n    }\n  }\n};\n\n/// Typed tracking ref.\n///\n/// Track refererences of a particular type.  It's useful to use this for \\a\n/// MDNode and \\a ValueAsMetadata.\ntemplate <class T> class TypedTrackingMDRef {\n  TrackingMDRef Ref;\n\npublic:\n  TypedTrackingMDRef() = default;\n  explicit TypedTrackingMDRef(T *MD) : Ref(static_cast<Metadata *>(MD)) {}\n\n  TypedTrackingMDRef(TypedTrackingMDRef &&X) : Ref(std::move(X.Ref)) {}\n  TypedTrackingMDRef(const TypedTrackingMDRef &X) : Ref(X.Ref) {}\n\n  TypedTrackingMDRef &operator=(TypedTrackingMDRef &&X) {\n    Ref = std::move(X.Ref);\n    return *this;\n  }\n\n  TypedTrackingMDRef &operator=(const TypedTrackingMDRef &X) {\n    Ref = X.Ref;\n    return *this;\n  }\n\n  T *get() const { return (T *)Ref.get(); }\n  operator T *() const { return get(); }\n  T *operator->() const { return get(); }\n  T &operator*() const { return *get(); }\n\n  bool operator==(const TypedTrackingMDRef &X) const { return Ref == X.Ref; }\n  bool operator!=(const TypedTrackingMDRef &X) const { return Ref != X.Ref; }\n\n  void reset() { Ref.reset(); }\n  void reset(T *MD) { Ref.reset(static_cast<Metadata *>(MD)); }\n\n  /// Check whether this has a trivial destructor.\n  bool hasTrivialDestructor() const { return Ref.hasTrivialDestructor(); }\n};\n\nusing TrackingMDNodeRef = TypedTrackingMDRef<MDNode>;\nusing TrackingValueAsMetadataRef = TypedTrackingMDRef<ValueAsMetadata>;\n\n// Expose the underlying metadata to casting.\ntemplate <> struct simplify_type<TrackingMDRef> {\n  using SimpleType = Metadata *;\n\n  static SimpleType getSimplifiedValue(TrackingMDRef &MD) { return MD.get(); }\n};\n\ntemplate <> struct simplify_type<const TrackingMDRef> {\n  using SimpleType = Metadata *;\n\n  static SimpleType getSimplifiedValue(const TrackingMDRef &MD) {\n    return MD.get();\n  }\n};\n\ntemplate <class T> struct simplify_type<TypedTrackingMDRef<T>> {\n  using SimpleType = T *;\n\n  static SimpleType getSimplifiedValue(TypedTrackingMDRef<T> &MD) {\n    return MD.get();\n  }\n};\n\ntemplate <class T> struct simplify_type<const TypedTrackingMDRef<T>> {\n  using SimpleType = T *;\n\n  static SimpleType getSimplifiedValue(const TypedTrackingMDRef<T> &MD) {\n    return MD.get();\n  }\n};\n\n} // end namespace llvm\n\n#endif // LLVM_IR_TRACKINGMDREF_H\n"}, "90": {"id": 90, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ProfileData/InstrProf.h", "content": "//===- InstrProf.h - Instrumented profiling format support ------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// Instrumentation-based profiling data is generated by instrumented\n// binaries through library functions in compiler-rt, and read by the clang\n// frontend to feed PGO.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_PROFILEDATA_INSTRPROF_H\n#define LLVM_PROFILEDATA_INSTRPROF_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/StringSet.h\"\n#include \"llvm/ADT/Triple.h\"\n#include \"llvm/IR/GlobalValue.h\"\n#include \"llvm/IR/ProfileSummary.h\"\n#include \"llvm/ProfileData/InstrProfData.inc\"\n#include \"llvm/Support/CommandLine.h\"\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/Endian.h\"\n#include \"llvm/Support/Error.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/Host.h\"\n#include \"llvm/Support/MD5.h\"\n#include \"llvm/Support/MathExtras.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstddef>\n#include <cstdint>\n#include <cstring>\n#include <list>\n#include <memory>\n#include <string>\n#include <system_error>\n#include <utility>\n#include <vector>\n\nnamespace llvm {\n\nclass Function;\nclass GlobalVariable;\nstruct InstrProfRecord;\nclass InstrProfSymtab;\nclass Instruction;\nclass MDNode;\nclass Module;\n\nenum InstrProfSectKind {\n#define INSTR_PROF_SECT_ENTRY(Kind, SectNameCommon, SectNameCoff, Prefix) Kind,\n#include \"llvm/ProfileData/InstrProfData.inc\"\n};\n\n/// Return the name of the profile section corresponding to \\p IPSK.\n///\n/// The name of the section depends on the object format type \\p OF. If\n/// \\p AddSegmentInfo is true, a segment prefix and additional linker hints may\n/// be added to the section name (this is the default).\nstd::string getInstrProfSectionName(InstrProfSectKind IPSK,\n                                    Triple::ObjectFormatType OF,\n                                    bool AddSegmentInfo = true);\n\n/// Return the name profile runtime entry point to do value profiling\n/// for a given site.\ninline StringRef getInstrProfValueProfFuncName() {\n  return INSTR_PROF_VALUE_PROF_FUNC_STR;\n}\n\n/// Return the name profile runtime entry point to do memop size value\n/// profiling.\ninline StringRef getInstrProfValueProfMemOpFuncName() {\n  return INSTR_PROF_VALUE_PROF_MEMOP_FUNC_STR;\n}\n\n/// Return the name prefix of variables containing instrumented function names.\ninline StringRef getInstrProfNameVarPrefix() { return \"__profn_\"; }\n\n/// Return the name prefix of variables containing per-function control data.\ninline StringRef getInstrProfDataVarPrefix() { return \"__profd_\"; }\n\n/// Return the name prefix of profile counter variables.\ninline StringRef getInstrProfCountersVarPrefix() { return \"__profc_\"; }\n\n/// Return the name prefix of value profile variables.\ninline StringRef getInstrProfValuesVarPrefix() { return \"__profvp_\"; }\n\n/// Return the name of value profile node array variables:\ninline StringRef getInstrProfVNodesVarName() { return \"__llvm_prf_vnodes\"; }\n\n/// Return the name of the variable holding the strings (possibly compressed)\n/// of all function's PGO names.\ninline StringRef getInstrProfNamesVarName() {\n  return \"__llvm_prf_nm\";\n}\n\n/// Return the name of a covarage mapping variable (internal linkage)\n/// for each instrumented source module. Such variables are allocated\n/// in the __llvm_covmap section.\ninline StringRef getCoverageMappingVarName() {\n  return \"__llvm_coverage_mapping\";\n}\n\n/// Return the name of the internal variable recording the array\n/// of PGO name vars referenced by the coverage mapping. The owning\n/// functions of those names are not emitted by FE (e.g, unused inline\n/// functions.)\ninline StringRef getCoverageUnusedNamesVarName() {\n  return \"__llvm_coverage_names\";\n}\n\n/// Return the name of function that registers all the per-function control\n/// data at program startup time by calling __llvm_register_function. This\n/// function has internal linkage and is called by  __llvm_profile_init\n/// runtime method. This function is not generated for these platforms:\n/// Darwin, Linux, and FreeBSD.\ninline StringRef getInstrProfRegFuncsName() {\n  return \"__llvm_profile_register_functions\";\n}\n\n/// Return the name of the runtime interface that registers per-function control\n/// data for one instrumented function.\ninline StringRef getInstrProfRegFuncName() {\n  return \"__llvm_profile_register_function\";\n}\n\n/// Return the name of the runtime interface that registers the PGO name strings.\ninline StringRef getInstrProfNamesRegFuncName() {\n  return \"__llvm_profile_register_names_function\";\n}\n\n/// Return the name of the runtime initialization method that is generated by\n/// the compiler. The function calls __llvm_profile_register_functions and\n/// __llvm_profile_override_default_filename functions if needed. This function\n/// has internal linkage and invoked at startup time via init_array.\ninline StringRef getInstrProfInitFuncName() { return \"__llvm_profile_init\"; }\n\n/// Return the name of the hook variable defined in profile runtime library.\n/// A reference to the variable causes the linker to link in the runtime\n/// initialization module (which defines the hook variable).\ninline StringRef getInstrProfRuntimeHookVarName() {\n  return INSTR_PROF_QUOTE(INSTR_PROF_PROFILE_RUNTIME_VAR);\n}\n\n/// Return the name of the compiler generated function that references the\n/// runtime hook variable. The function is a weak global.\ninline StringRef getInstrProfRuntimeHookVarUseFuncName() {\n  return \"__llvm_profile_runtime_user\";\n}\n\ninline StringRef getInstrProfCounterBiasVarName() {\n  return \"__llvm_profile_counter_bias\";\n}\n\n/// Return the marker used to separate PGO names during serialization.\ninline StringRef getInstrProfNameSeparator() { return \"\\01\"; }\n\n/// Return the modified name for function \\c F suitable to be\n/// used the key for profile lookup. Variable \\c InLTO indicates if this\n/// is called in LTO optimization passes.\nstd::string getPGOFuncName(const Function &F, bool InLTO = false,\n                           uint64_t Version = INSTR_PROF_INDEX_VERSION);\n\n/// Return the modified name for a function suitable to be\n/// used the key for profile lookup. The function's original\n/// name is \\c RawFuncName and has linkage of type \\c Linkage.\n/// The function is defined in module \\c FileName.\nstd::string getPGOFuncName(StringRef RawFuncName,\n                           GlobalValue::LinkageTypes Linkage,\n                           StringRef FileName,\n                           uint64_t Version = INSTR_PROF_INDEX_VERSION);\n\n/// Return the name of the global variable used to store a function\n/// name in PGO instrumentation. \\c FuncName is the name of the function\n/// returned by the \\c getPGOFuncName call.\nstd::string getPGOFuncNameVarName(StringRef FuncName,\n                                  GlobalValue::LinkageTypes Linkage);\n\n/// Create and return the global variable for function name used in PGO\n/// instrumentation. \\c FuncName is the name of the function returned\n/// by \\c getPGOFuncName call.\nGlobalVariable *createPGOFuncNameVar(Function &F, StringRef PGOFuncName);\n\n/// Create and return the global variable for function name used in PGO\n/// instrumentation.  /// \\c FuncName is the name of the function\n/// returned by \\c getPGOFuncName call, \\c M is the owning module,\n/// and \\c Linkage is the linkage of the instrumented function.\nGlobalVariable *createPGOFuncNameVar(Module &M,\n                                     GlobalValue::LinkageTypes Linkage,\n                                     StringRef PGOFuncName);\n\n/// Return the initializer in string of the PGO name var \\c NameVar.\nStringRef getPGOFuncNameVarInitializer(GlobalVariable *NameVar);\n\n/// Given a PGO function name, remove the filename prefix and return\n/// the original (static) function name.\nStringRef getFuncNameWithoutPrefix(StringRef PGOFuncName,\n                                   StringRef FileName = \"<unknown>\");\n\n/// Given a vector of strings (function PGO names) \\c NameStrs, the\n/// method generates a combined string \\c Result thatis ready to be\n/// serialized.  The \\c Result string is comprised of three fields:\n/// The first field is the legnth of the uncompressed strings, and the\n/// the second field is the length of the zlib-compressed string.\n/// Both fields are encoded in ULEB128.  If \\c doCompress is false, the\n///  third field is the uncompressed strings; otherwise it is the\n/// compressed string. When the string compression is off, the\n/// second field will have value zero.\nError collectPGOFuncNameStrings(ArrayRef<std::string> NameStrs,\n                                bool doCompression, std::string &Result);\n\n/// Produce \\c Result string with the same format described above. The input\n/// is vector of PGO function name variables that are referenced.\nError collectPGOFuncNameStrings(ArrayRef<GlobalVariable *> NameVars,\n                                std::string &Result, bool doCompression = true);\n\n/// \\c NameStrings is a string composed of one of more sub-strings encoded in\n/// the format described above. The substrings are separated by 0 or more zero\n/// bytes. This method decodes the string and populates the \\c Symtab.\nError readPGOFuncNameStrings(StringRef NameStrings, InstrProfSymtab &Symtab);\n\n/// Check if INSTR_PROF_RAW_VERSION_VAR is defined. This global is only being\n/// set in IR PGO compilation.\nbool isIRPGOFlagSet(const Module *M);\n\n/// Check if we can safely rename this Comdat function. Instances of the same\n/// comdat function may have different control flows thus can not share the\n/// same counter variable.\nbool canRenameComdatFunc(const Function &F, bool CheckAddressTaken = false);\n\nenum InstrProfValueKind : uint32_t {\n#define VALUE_PROF_KIND(Enumerator, Value, Descr) Enumerator = Value,\n#include \"llvm/ProfileData/InstrProfData.inc\"\n};\n\n/// Get the value profile data for value site \\p SiteIdx from \\p InstrProfR\n/// and annotate the instruction \\p Inst with the value profile meta data.\n/// Annotate up to \\p MaxMDCount (default 3) number of records per value site.\nvoid annotateValueSite(Module &M, Instruction &Inst,\n                       const InstrProfRecord &InstrProfR,\n                       InstrProfValueKind ValueKind, uint32_t SiteIndx,\n                       uint32_t MaxMDCount = 3);\n\n/// Same as the above interface but using an ArrayRef, as well as \\p Sum.\nvoid annotateValueSite(Module &M, Instruction &Inst,\n                       ArrayRef<InstrProfValueData> VDs, uint64_t Sum,\n                       InstrProfValueKind ValueKind, uint32_t MaxMDCount);\n\n/// Magic number in the value profile data showing a target has been\n/// promoted for the instruction and shouldn't be promoted again.\nconst uint64_t NOMORE_ICP_MAGICNUM = -1;\n\n/// Extract the value profile data from \\p Inst which is annotated with\n/// value profile meta data. Return false if there is no value data annotated,\n/// otherwise  return true.\nbool getValueProfDataFromInst(const Instruction &Inst,\n                              InstrProfValueKind ValueKind,\n                              uint32_t MaxNumValueData,\n                              InstrProfValueData ValueData[],\n                              uint32_t &ActualNumValueData, uint64_t &TotalC,\n                              bool GetNoICPValue = false);\n\ninline StringRef getPGOFuncNameMetadataName() { return \"PGOFuncName\"; }\n\n/// Return the PGOFuncName meta data associated with a function.\nMDNode *getPGOFuncNameMetadata(const Function &F);\n\n/// Create the PGOFuncName meta data if PGOFuncName is different from\n/// function's raw name. This should only apply to internal linkage functions\n/// declared by users only.\nvoid createPGOFuncNameMetadata(Function &F, StringRef PGOFuncName);\n\n/// Check if we can use Comdat for profile variables. This will eliminate\n/// the duplicated profile variables for Comdat functions.\nbool needsComdatForCounter(const Function &F, const Module &M);\n\nconst std::error_category &instrprof_category();\n\nenum class instrprof_error {\n  success = 0,\n  eof,\n  unrecognized_format,\n  bad_magic,\n  bad_header,\n  unsupported_version,\n  unsupported_hash_type,\n  too_large,\n  truncated,\n  malformed,\n  unknown_function,\n  invalid_prof,\n  hash_mismatch,\n  count_mismatch,\n  counter_overflow,\n  value_site_count_mismatch,\n  compress_failed,\n  uncompress_failed,\n  empty_raw_profile,\n  zlib_unavailable\n};\n\ninline std::error_code make_error_code(instrprof_error E) {\n  return std::error_code(static_cast<int>(E), instrprof_category());\n}\n\nclass InstrProfError : public ErrorInfo<InstrProfError> {\npublic:\n  InstrProfError(instrprof_error Err) : Err(Err) {\n    assert(Err != instrprof_error::success && \"Not an error\");\n  }\n\n  std::string message() const override;\n\n  void log(raw_ostream &OS) const override { OS << message(); }\n\n  std::error_code convertToErrorCode() const override {\n    return make_error_code(Err);\n  }\n\n  instrprof_error get() const { return Err; }\n\n  /// Consume an Error and return the raw enum value contained within it. The\n  /// Error must either be a success value, or contain a single InstrProfError.\n  static instrprof_error take(Error E) {\n    auto Err = instrprof_error::success;\n    handleAllErrors(std::move(E), [&Err](const InstrProfError &IPE) {\n      assert(Err == instrprof_error::success && \"Multiple errors encountered\");\n      Err = IPE.get();\n    });\n    return Err;\n  }\n\n  static char ID;\n\nprivate:\n  instrprof_error Err;\n};\n\nclass SoftInstrProfErrors {\n  /// Count the number of soft instrprof_errors encountered and keep track of\n  /// the first such error for reporting purposes.\n\n  /// The first soft error encountered.\n  instrprof_error FirstError = instrprof_error::success;\n\n  /// The number of hash mismatches.\n  unsigned NumHashMismatches = 0;\n\n  /// The number of count mismatches.\n  unsigned NumCountMismatches = 0;\n\n  /// The number of counter overflows.\n  unsigned NumCounterOverflows = 0;\n\n  /// The number of value site count mismatches.\n  unsigned NumValueSiteCountMismatches = 0;\n\npublic:\n  SoftInstrProfErrors() = default;\n\n  ~SoftInstrProfErrors() {\n    assert(FirstError == instrprof_error::success &&\n           \"Unchecked soft error encountered\");\n  }\n\n  /// Track a soft error (\\p IE) and increment its associated counter.\n  void addError(instrprof_error IE);\n\n  /// Get the number of hash mismatches.\n  unsigned getNumHashMismatches() const { return NumHashMismatches; }\n\n  /// Get the number of count mismatches.\n  unsigned getNumCountMismatches() const { return NumCountMismatches; }\n\n  /// Get the number of counter overflows.\n  unsigned getNumCounterOverflows() const { return NumCounterOverflows; }\n\n  /// Get the number of value site count mismatches.\n  unsigned getNumValueSiteCountMismatches() const {\n    return NumValueSiteCountMismatches;\n  }\n\n  /// Return the first encountered error and reset FirstError to a success\n  /// value.\n  Error takeError() {\n    if (FirstError == instrprof_error::success)\n      return Error::success();\n    auto E = make_error<InstrProfError>(FirstError);\n    FirstError = instrprof_error::success;\n    return E;\n  }\n};\n\nnamespace object {\n\nclass SectionRef;\n\n} // end namespace object\n\nnamespace IndexedInstrProf {\n\nuint64_t ComputeHash(StringRef K);\n\n} // end namespace IndexedInstrProf\n\n/// A symbol table used for function PGO name look-up with keys\n/// (such as pointers, md5hash values) to the function. A function's\n/// PGO name or name's md5hash are used in retrieving the profile\n/// data of the function. See \\c getPGOFuncName() method for details\n/// on how PGO name is formed.\nclass InstrProfSymtab {\npublic:\n  using AddrHashMap = std::vector<std::pair<uint64_t, uint64_t>>;\n\nprivate:\n  StringRef Data;\n  uint64_t Address = 0;\n  // Unique name strings.\n  StringSet<> NameTab;\n  // A map from MD5 keys to function name strings.\n  std::vector<std::pair<uint64_t, StringRef>> MD5NameMap;\n  // A map from MD5 keys to function define. We only populate this map\n  // when build the Symtab from a Module.\n  std::vector<std::pair<uint64_t, Function *>> MD5FuncMap;\n  // A map from function runtime address to function name MD5 hash.\n  // This map is only populated and used by raw instr profile reader.\n  AddrHashMap AddrToMD5Map;\n  bool Sorted = false;\n\n  static StringRef getExternalSymbol() {\n    return \"** External Symbol **\";\n  }\n\n  // If the symtab is created by a series of calls to \\c addFuncName, \\c\n  // finalizeSymtab needs to be called before looking up function names.\n  // This is required because the underlying map is a vector (for space\n  // efficiency) which needs to be sorted.\n  inline void finalizeSymtab();\n\npublic:\n  InstrProfSymtab() = default;\n\n  /// Create InstrProfSymtab from an object file section which\n  /// contains function PGO names. When section may contain raw\n  /// string data or string data in compressed form. This method\n  /// only initialize the symtab with reference to the data and\n  /// the section base address. The decompression will be delayed\n  /// until before it is used. See also \\c create(StringRef) method.\n  Error create(object::SectionRef &Section);\n\n  /// This interface is used by reader of CoverageMapping test\n  /// format.\n  inline Error create(StringRef D, uint64_t BaseAddr);\n\n  /// \\c NameStrings is a string composed of one of more sub-strings\n  ///  encoded in the format described in \\c collectPGOFuncNameStrings.\n  /// This method is a wrapper to \\c readPGOFuncNameStrings method.\n  inline Error create(StringRef NameStrings);\n\n  /// A wrapper interface to populate the PGO symtab with functions\n  /// decls from module \\c M. This interface is used by transformation\n  /// passes such as indirect function call promotion. Variable \\c InLTO\n  /// indicates if this is called from LTO optimization passes.\n  Error create(Module &M, bool InLTO = false);\n\n  /// Create InstrProfSymtab from a set of names iteratable from\n  /// \\p IterRange. This interface is used by IndexedProfReader.\n  template <typename NameIterRange> Error create(const NameIterRange &IterRange);\n\n  /// Update the symtab by adding \\p FuncName to the table. This interface\n  /// is used by the raw and text profile readers.\n  Error addFuncName(StringRef FuncName) {\n    if (FuncName.empty())\n      return make_error<InstrProfError>(instrprof_error::malformed);\n    auto Ins = NameTab.insert(FuncName);\n    if (Ins.second) {\n      MD5NameMap.push_back(std::make_pair(\n          IndexedInstrProf::ComputeHash(FuncName), Ins.first->getKey()));\n      Sorted = false;\n    }\n    return Error::success();\n  }\n\n  /// Map a function address to its name's MD5 hash. This interface\n  /// is only used by the raw profiler reader.\n  void mapAddress(uint64_t Addr, uint64_t MD5Val) {\n    AddrToMD5Map.push_back(std::make_pair(Addr, MD5Val));\n  }\n\n  /// Return a function's hash, or 0, if the function isn't in this SymTab.\n  uint64_t getFunctionHashFromAddress(uint64_t Address);\n\n  /// Return function's PGO name from the function name's symbol\n  /// address in the object file. If an error occurs, return\n  /// an empty string.\n  StringRef getFuncName(uint64_t FuncNameAddress, size_t NameSize);\n\n  /// Return function's PGO name from the name's md5 hash value.\n  /// If not found, return an empty string.\n  inline StringRef getFuncName(uint64_t FuncMD5Hash);\n\n  /// Just like getFuncName, except that it will return a non-empty StringRef\n  /// if the function is external to this symbol table. All such cases\n  /// will be represented using the same StringRef value.\n  inline StringRef getFuncNameOrExternalSymbol(uint64_t FuncMD5Hash);\n\n  /// True if Symbol is the value used to represent external symbols.\n  static bool isExternalSymbol(const StringRef &Symbol) {\n    return Symbol == InstrProfSymtab::getExternalSymbol();\n  }\n\n  /// Return function from the name's md5 hash. Return nullptr if not found.\n  inline Function *getFunction(uint64_t FuncMD5Hash);\n\n  /// Return the function's original assembly name by stripping off\n  /// the prefix attached (to symbols with priviate linkage). For\n  /// global functions, it returns the same string as getFuncName.\n  inline StringRef getOrigFuncName(uint64_t FuncMD5Hash);\n\n  /// Return the name section data.\n  inline StringRef getNameData() const { return Data; }\n};\n\nError InstrProfSymtab::create(StringRef D, uint64_t BaseAddr) {\n  Data = D;\n  Address = BaseAddr;\n  return Error::success();\n}\n\nError InstrProfSymtab::create(StringRef NameStrings) {\n  return readPGOFuncNameStrings(NameStrings, *this);\n}\n\ntemplate <typename NameIterRange>\nError InstrProfSymtab::create(const NameIterRange &IterRange) {\n  for (auto Name : IterRange)\n    if (Error E = addFuncName(Name))\n      return E;\n\n  finalizeSymtab();\n  return Error::success();\n}\n\nvoid InstrProfSymtab::finalizeSymtab() {\n  if (Sorted)\n    return;\n  llvm::sort(MD5NameMap, less_first());\n  llvm::sort(MD5FuncMap, less_first());\n  llvm::sort(AddrToMD5Map, less_first());\n  AddrToMD5Map.erase(std::unique(AddrToMD5Map.begin(), AddrToMD5Map.end()),\n                     AddrToMD5Map.end());\n  Sorted = true;\n}\n\nStringRef InstrProfSymtab::getFuncNameOrExternalSymbol(uint64_t FuncMD5Hash) {\n  StringRef ret = getFuncName(FuncMD5Hash);\n  if (ret.empty())\n    return InstrProfSymtab::getExternalSymbol();\n  return ret;\n}\n\nStringRef InstrProfSymtab::getFuncName(uint64_t FuncMD5Hash) {\n  finalizeSymtab();\n  auto Result = llvm::lower_bound(MD5NameMap, FuncMD5Hash,\n                                  [](const std::pair<uint64_t, StringRef> &LHS,\n                                     uint64_t RHS) { return LHS.first < RHS; });\n  if (Result != MD5NameMap.end() && Result->first == FuncMD5Hash)\n    return Result->second;\n  return StringRef();\n}\n\nFunction* InstrProfSymtab::getFunction(uint64_t FuncMD5Hash) {\n  finalizeSymtab();\n  auto Result = llvm::lower_bound(MD5FuncMap, FuncMD5Hash,\n                                  [](const std::pair<uint64_t, Function *> &LHS,\n                                     uint64_t RHS) { return LHS.first < RHS; });\n  if (Result != MD5FuncMap.end() && Result->first == FuncMD5Hash)\n    return Result->second;\n  return nullptr;\n}\n\n// See also getPGOFuncName implementation. These two need to be\n// matched.\nStringRef InstrProfSymtab::getOrigFuncName(uint64_t FuncMD5Hash) {\n  StringRef PGOName = getFuncName(FuncMD5Hash);\n  size_t S = PGOName.find_first_of(':');\n  if (S == StringRef::npos)\n    return PGOName;\n  return PGOName.drop_front(S + 1);\n}\n\n// To store the sums of profile count values, or the percentage of\n// the sums of the total count values.\nstruct CountSumOrPercent {\n  uint64_t NumEntries;\n  double CountSum;\n  double ValueCounts[IPVK_Last - IPVK_First + 1];\n  CountSumOrPercent() : NumEntries(0), CountSum(0.0f), ValueCounts() {}\n  void reset() {\n    NumEntries = 0;\n    CountSum = 0.0f;\n    for (unsigned I = 0; I < IPVK_Last - IPVK_First + 1; I++)\n      ValueCounts[I] = 0.0f;\n  }\n};\n\n// Function level or program level overlap information.\nstruct OverlapStats {\n  enum OverlapStatsLevel { ProgramLevel, FunctionLevel };\n  // Sum of the total count values for the base profile.\n  CountSumOrPercent Base;\n  // Sum of the total count values for the test profile.\n  CountSumOrPercent Test;\n  // Overlap lap score. Should be in range of [0.0f to 1.0f].\n  CountSumOrPercent Overlap;\n  CountSumOrPercent Mismatch;\n  CountSumOrPercent Unique;\n  OverlapStatsLevel Level;\n  const std::string *BaseFilename;\n  const std::string *TestFilename;\n  StringRef FuncName;\n  uint64_t FuncHash;\n  bool Valid;\n\n  OverlapStats(OverlapStatsLevel L = ProgramLevel)\n      : Level(L), BaseFilename(nullptr), TestFilename(nullptr), FuncHash(0),\n        Valid(false) {}\n\n  void dump(raw_fd_ostream &OS) const;\n\n  void setFuncInfo(StringRef Name, uint64_t Hash) {\n    FuncName = Name;\n    FuncHash = Hash;\n  }\n\n  Error accumulateCounts(const std::string &BaseFilename,\n                         const std::string &TestFilename, bool IsCS);\n  void addOneMismatch(const CountSumOrPercent &MismatchFunc);\n  void addOneUnique(const CountSumOrPercent &UniqueFunc);\n\n  static inline double score(uint64_t Val1, uint64_t Val2, double Sum1,\n                             double Sum2) {\n    if (Sum1 < 1.0f || Sum2 < 1.0f)\n      return 0.0f;\n    return std::min(Val1 / Sum1, Val2 / Sum2);\n  }\n};\n\n// This is used to filter the functions whose overlap information\n// to be output.\nstruct OverlapFuncFilters {\n  uint64_t ValueCutoff;\n  const std::string NameFilter;\n};\n\nstruct InstrProfValueSiteRecord {\n  /// Value profiling data pairs at a given value site.\n  std::list<InstrProfValueData> ValueData;\n\n  InstrProfValueSiteRecord() { ValueData.clear(); }\n  template <class InputIterator>\n  InstrProfValueSiteRecord(InputIterator F, InputIterator L)\n      : ValueData(F, L) {}\n\n  /// Sort ValueData ascending by Value\n  void sortByTargetValues() {\n    ValueData.sort(\n        [](const InstrProfValueData &left, const InstrProfValueData &right) {\n          return left.Value < right.Value;\n        });\n  }\n  /// Sort ValueData Descending by Count\n  inline void sortByCount();\n\n  /// Merge data from another InstrProfValueSiteRecord\n  /// Optionally scale merged counts by \\p Weight.\n  void merge(InstrProfValueSiteRecord &Input, uint64_t Weight,\n             function_ref<void(instrprof_error)> Warn);\n  /// Scale up value profile data counts by N (Numerator) / D (Denominator).\n  void scale(uint64_t N, uint64_t D, function_ref<void(instrprof_error)> Warn);\n\n  /// Compute the overlap b/w this record and Input record.\n  void overlap(InstrProfValueSiteRecord &Input, uint32_t ValueKind,\n               OverlapStats &Overlap, OverlapStats &FuncLevelOverlap);\n};\n\n/// Profiling information for a single function.\nstruct InstrProfRecord {\n  std::vector<uint64_t> Counts;\n\n  InstrProfRecord() = default;\n  InstrProfRecord(std::vector<uint64_t> Counts) : Counts(std::move(Counts)) {}\n  InstrProfRecord(InstrProfRecord &&) = default;\n  InstrProfRecord(const InstrProfRecord &RHS)\n      : Counts(RHS.Counts),\n        ValueData(RHS.ValueData\n                      ? std::make_unique<ValueProfData>(*RHS.ValueData)\n                      : nullptr) {}\n  InstrProfRecord &operator=(InstrProfRecord &&) = default;\n  InstrProfRecord &operator=(const InstrProfRecord &RHS) {\n    Counts = RHS.Counts;\n    if (!RHS.ValueData) {\n      ValueData = nullptr;\n      return *this;\n    }\n    if (!ValueData)\n      ValueData = std::make_unique<ValueProfData>(*RHS.ValueData);\n    else\n      *ValueData = *RHS.ValueData;\n    return *this;\n  }\n\n  /// Return the number of value profile kinds with non-zero number\n  /// of profile sites.\n  inline uint32_t getNumValueKinds() const;\n  /// Return the number of instrumented sites for ValueKind.\n  inline uint32_t getNumValueSites(uint32_t ValueKind) const;\n\n  /// Return the total number of ValueData for ValueKind.\n  inline uint32_t getNumValueData(uint32_t ValueKind) const;\n\n  /// Return the number of value data collected for ValueKind at profiling\n  /// site: Site.\n  inline uint32_t getNumValueDataForSite(uint32_t ValueKind,\n                                         uint32_t Site) const;\n\n  /// Return the array of profiled values at \\p Site. If \\p TotalC\n  /// is not null, the total count of all target values at this site\n  /// will be stored in \\c *TotalC.\n  inline std::unique_ptr<InstrProfValueData[]>\n  getValueForSite(uint32_t ValueKind, uint32_t Site,\n                  uint64_t *TotalC = nullptr) const;\n\n  /// Get the target value/counts of kind \\p ValueKind collected at site\n  /// \\p Site and store the result in array \\p Dest. Return the total\n  /// counts of all target values at this site.\n  inline uint64_t getValueForSite(InstrProfValueData Dest[], uint32_t ValueKind,\n                                  uint32_t Site) const;\n\n  /// Reserve space for NumValueSites sites.\n  inline void reserveSites(uint32_t ValueKind, uint32_t NumValueSites);\n\n  /// Add ValueData for ValueKind at value Site.\n  void addValueData(uint32_t ValueKind, uint32_t Site,\n                    InstrProfValueData *VData, uint32_t N,\n                    InstrProfSymtab *SymTab);\n\n  /// Merge the counts in \\p Other into this one.\n  /// Optionally scale merged counts by \\p Weight.\n  void merge(InstrProfRecord &Other, uint64_t Weight,\n             function_ref<void(instrprof_error)> Warn);\n\n  /// Scale up profile counts (including value profile data) by\n  /// a factor of (N / D).\n  void scale(uint64_t N, uint64_t D, function_ref<void(instrprof_error)> Warn);\n\n  /// Sort value profile data (per site) by count.\n  void sortValueData() {\n    for (uint32_t Kind = IPVK_First; Kind <= IPVK_Last; ++Kind)\n      for (auto &SR : getValueSitesForKind(Kind))\n        SR.sortByCount();\n  }\n\n  /// Clear value data entries and edge counters.\n  void Clear() {\n    Counts.clear();\n    clearValueData();\n  }\n\n  /// Clear value data entries\n  void clearValueData() { ValueData = nullptr; }\n\n  /// Compute the sums of all counts and store in Sum.\n  void accumulateCounts(CountSumOrPercent &Sum) const;\n\n  /// Compute the overlap b/w this IntrprofRecord and Other.\n  void overlap(InstrProfRecord &Other, OverlapStats &Overlap,\n               OverlapStats &FuncLevelOverlap, uint64_t ValueCutoff);\n\n  /// Compute the overlap of value profile counts.\n  void overlapValueProfData(uint32_t ValueKind, InstrProfRecord &Src,\n                            OverlapStats &Overlap,\n                            OverlapStats &FuncLevelOverlap);\n\nprivate:\n  struct ValueProfData {\n    std::vector<InstrProfValueSiteRecord> IndirectCallSites;\n    std::vector<InstrProfValueSiteRecord> MemOPSizes;\n  };\n  std::unique_ptr<ValueProfData> ValueData;\n\n  MutableArrayRef<InstrProfValueSiteRecord>\n  getValueSitesForKind(uint32_t ValueKind) {\n    // Cast to /add/ const (should be an implicit_cast, ideally, if that's ever\n    // implemented in LLVM) to call the const overload of this function, then\n    // cast away the constness from the result.\n    auto AR = const_cast<const InstrProfRecord *>(this)->getValueSitesForKind(\n        ValueKind);\n    return makeMutableArrayRef(\n        const_cast<InstrProfValueSiteRecord *>(AR.data()), AR.size());\n  }\n  ArrayRef<InstrProfValueSiteRecord>\n  getValueSitesForKind(uint32_t ValueKind) const {\n    if (!ValueData)\n      return None;\n    switch (ValueKind) {\n    case IPVK_IndirectCallTarget:\n      return ValueData->IndirectCallSites;\n    case IPVK_MemOPSize:\n      return ValueData->MemOPSizes;\n    default:\n      llvm_unreachable(\"Unknown value kind!\");\n    }\n  }\n\n  std::vector<InstrProfValueSiteRecord> &\n  getOrCreateValueSitesForKind(uint32_t ValueKind) {\n    if (!ValueData)\n      ValueData = std::make_unique<ValueProfData>();\n    switch (ValueKind) {\n    case IPVK_IndirectCallTarget:\n      return ValueData->IndirectCallSites;\n    case IPVK_MemOPSize:\n      return ValueData->MemOPSizes;\n    default:\n      llvm_unreachable(\"Unknown value kind!\");\n    }\n  }\n\n  // Map indirect call target name hash to name string.\n  uint64_t remapValue(uint64_t Value, uint32_t ValueKind,\n                      InstrProfSymtab *SymTab);\n\n  // Merge Value Profile data from Src record to this record for ValueKind.\n  // Scale merged value counts by \\p Weight.\n  void mergeValueProfData(uint32_t ValkeKind, InstrProfRecord &Src,\n                          uint64_t Weight,\n                          function_ref<void(instrprof_error)> Warn);\n\n  // Scale up value profile data count by N (Numerator) / D (Denominator).\n  void scaleValueProfData(uint32_t ValueKind, uint64_t N, uint64_t D,\n                          function_ref<void(instrprof_error)> Warn);\n};\n\nstruct NamedInstrProfRecord : InstrProfRecord {\n  StringRef Name;\n  uint64_t Hash;\n\n  // We reserve this bit as the flag for context sensitive profile record.\n  static const int CS_FLAG_IN_FUNC_HASH = 60;\n\n  NamedInstrProfRecord() = default;\n  NamedInstrProfRecord(StringRef Name, uint64_t Hash,\n                       std::vector<uint64_t> Counts)\n      : InstrProfRecord(std::move(Counts)), Name(Name), Hash(Hash) {}\n\n  static bool hasCSFlagInHash(uint64_t FuncHash) {\n    return ((FuncHash >> CS_FLAG_IN_FUNC_HASH) & 1);\n  }\n  static void setCSFlagInHash(uint64_t &FuncHash) {\n    FuncHash |= ((uint64_t)1 << CS_FLAG_IN_FUNC_HASH);\n  }\n};\n\nuint32_t InstrProfRecord::getNumValueKinds() const {\n  uint32_t NumValueKinds = 0;\n  for (uint32_t Kind = IPVK_First; Kind <= IPVK_Last; ++Kind)\n    NumValueKinds += !(getValueSitesForKind(Kind).empty());\n  return NumValueKinds;\n}\n\nuint32_t InstrProfRecord::getNumValueData(uint32_t ValueKind) const {\n  uint32_t N = 0;\n  for (auto &SR : getValueSitesForKind(ValueKind))\n    N += SR.ValueData.size();\n  return N;\n}\n\nuint32_t InstrProfRecord::getNumValueSites(uint32_t ValueKind) const {\n  return getValueSitesForKind(ValueKind).size();\n}\n\nuint32_t InstrProfRecord::getNumValueDataForSite(uint32_t ValueKind,\n                                                 uint32_t Site) const {\n  return getValueSitesForKind(ValueKind)[Site].ValueData.size();\n}\n\nstd::unique_ptr<InstrProfValueData[]>\nInstrProfRecord::getValueForSite(uint32_t ValueKind, uint32_t Site,\n                                 uint64_t *TotalC) const {\n  uint64_t Dummy = 0;\n  uint64_t &TotalCount = (TotalC == nullptr ? Dummy : *TotalC);\n  uint32_t N = getNumValueDataForSite(ValueKind, Site);\n  if (N == 0) {\n    TotalCount = 0;\n    return std::unique_ptr<InstrProfValueData[]>(nullptr);\n  }\n\n  auto VD = std::make_unique<InstrProfValueData[]>(N);\n  TotalCount = getValueForSite(VD.get(), ValueKind, Site);\n\n  return VD;\n}\n\nuint64_t InstrProfRecord::getValueForSite(InstrProfValueData Dest[],\n                                          uint32_t ValueKind,\n                                          uint32_t Site) const {\n  uint32_t I = 0;\n  uint64_t TotalCount = 0;\n  for (auto V : getValueSitesForKind(ValueKind)[Site].ValueData) {\n    Dest[I].Value = V.Value;\n    Dest[I].Count = V.Count;\n    TotalCount = SaturatingAdd(TotalCount, V.Count);\n    I++;\n  }\n  return TotalCount;\n}\n\nvoid InstrProfRecord::reserveSites(uint32_t ValueKind, uint32_t NumValueSites) {\n  if (!NumValueSites)\n    return;\n  getOrCreateValueSitesForKind(ValueKind).reserve(NumValueSites);\n}\n\ninline support::endianness getHostEndianness() {\n  return sys::IsLittleEndianHost ? support::little : support::big;\n}\n\n// Include definitions for value profile data\n#define INSTR_PROF_VALUE_PROF_DATA\n#include \"llvm/ProfileData/InstrProfData.inc\"\n\nvoid InstrProfValueSiteRecord::sortByCount() {\n  ValueData.sort(\n      [](const InstrProfValueData &left, const InstrProfValueData &right) {\n        return left.Count > right.Count;\n      });\n  // Now truncate\n  size_t max_s = INSTR_PROF_MAX_NUM_VAL_PER_SITE;\n  if (ValueData.size() > max_s)\n    ValueData.resize(max_s);\n}\n\nnamespace IndexedInstrProf {\n\nenum class HashT : uint32_t {\n  MD5,\n  Last = MD5\n};\n\ninline uint64_t ComputeHash(HashT Type, StringRef K) {\n  switch (Type) {\n  case HashT::MD5:\n    return MD5Hash(K);\n  }\n  llvm_unreachable(\"Unhandled hash type\");\n}\n\nconst uint64_t Magic = 0x8169666f72706cff; // \"\\xfflprofi\\x81\"\n\nenum ProfVersion {\n  // Version 1 is the first version. In this version, the value of\n  // a key/value pair can only include profile data of a single function.\n  // Due to this restriction, the number of block counters for a given\n  // function is not recorded but derived from the length of the value.\n  Version1 = 1,\n  // The version 2 format supports recording profile data of multiple\n  // functions which share the same key in one value field. To support this,\n  // the number block counters is recorded as an uint64_t field right after the\n  // function structural hash.\n  Version2 = 2,\n  // Version 3 supports value profile data. The value profile data is expected\n  // to follow the block counter profile data.\n  Version3 = 3,\n  // In this version, profile summary data \\c IndexedInstrProf::Summary is\n  // stored after the profile header.\n  Version4 = 4,\n  // In this version, the frontend PGO stable hash algorithm defaults to V2.\n  Version5 = 5,\n  // In this version, the frontend PGO stable hash algorithm got fixed and\n  // may produce hashes different from Version5.\n  Version6 = 6,\n  // An additional counter is added around logical operators.\n  Version7 = 7,\n  // The current version is 7.\n  CurrentVersion = INSTR_PROF_INDEX_VERSION\n};\nconst uint64_t Version = ProfVersion::CurrentVersion;\n\nconst HashT HashType = HashT::MD5;\n\ninline uint64_t ComputeHash(StringRef K) { return ComputeHash(HashType, K); }\n\n// This structure defines the file header of the LLVM profile\n// data file in indexed-format.\nstruct Header {\n  uint64_t Magic;\n  uint64_t Version;\n  uint64_t Unused; // Becomes unused since version 4\n  uint64_t HashType;\n  uint64_t HashOffset;\n};\n\n// Profile summary data recorded in the profile data file in indexed\n// format. It is introduced in version 4. The summary data follows\n// right after the profile file header.\nstruct Summary {\n  struct Entry {\n    uint64_t Cutoff; ///< The required percentile of total execution count.\n    uint64_t\n        MinBlockCount;  ///< The minimum execution count for this percentile.\n    uint64_t NumBlocks; ///< Number of blocks >= the minumum execution count.\n  };\n  // The field kind enumerator to assigned value mapping should remain\n  // unchanged  when a new kind is added or an old kind gets deleted in\n  // the future.\n  enum SummaryFieldKind {\n    /// The total number of functions instrumented.\n    TotalNumFunctions = 0,\n    /// Total number of instrumented blocks/edges.\n    TotalNumBlocks = 1,\n    /// The maximal execution count among all functions.\n    /// This field does not exist for profile data from IR based\n    /// instrumentation.\n    MaxFunctionCount = 2,\n    /// Max block count of the program.\n    MaxBlockCount = 3,\n    /// Max internal block count of the program (excluding entry blocks).\n    MaxInternalBlockCount = 4,\n    /// The sum of all instrumented block counts.\n    TotalBlockCount = 5,\n    NumKinds = TotalBlockCount + 1\n  };\n\n  // The number of summmary fields following the summary header.\n  uint64_t NumSummaryFields;\n  // The number of Cutoff Entries (Summary::Entry) following summary fields.\n  uint64_t NumCutoffEntries;\n\n  Summary() = delete;\n  Summary(uint32_t Size) { memset(this, 0, Size); }\n\n  void operator delete(void *ptr) { ::operator delete(ptr); }\n\n  static uint32_t getSize(uint32_t NumSumFields, uint32_t NumCutoffEntries) {\n    return sizeof(Summary) + NumCutoffEntries * sizeof(Entry) +\n           NumSumFields * sizeof(uint64_t);\n  }\n\n  const uint64_t *getSummaryDataBase() const {\n    return reinterpret_cast<const uint64_t *>(this + 1);\n  }\n\n  uint64_t *getSummaryDataBase() {\n    return reinterpret_cast<uint64_t *>(this + 1);\n  }\n\n  const Entry *getCutoffEntryBase() const {\n    return reinterpret_cast<const Entry *>(\n        &getSummaryDataBase()[NumSummaryFields]);\n  }\n\n  Entry *getCutoffEntryBase() {\n    return reinterpret_cast<Entry *>(&getSummaryDataBase()[NumSummaryFields]);\n  }\n\n  uint64_t get(SummaryFieldKind K) const {\n    return getSummaryDataBase()[K];\n  }\n\n  void set(SummaryFieldKind K, uint64_t V) {\n    getSummaryDataBase()[K] = V;\n  }\n\n  const Entry &getEntry(uint32_t I) const { return getCutoffEntryBase()[I]; }\n\n  void setEntry(uint32_t I, const ProfileSummaryEntry &E) {\n    Entry &ER = getCutoffEntryBase()[I];\n    ER.Cutoff = E.Cutoff;\n    ER.MinBlockCount = E.MinCount;\n    ER.NumBlocks = E.NumCounts;\n  }\n};\n\ninline std::unique_ptr<Summary> allocSummary(uint32_t TotalSize) {\n  return std::unique_ptr<Summary>(new (::operator new(TotalSize))\n                                      Summary(TotalSize));\n}\n\n} // end namespace IndexedInstrProf\n\nnamespace RawInstrProf {\n\n// Version 1: First version\n// Version 2: Added value profile data section. Per-function control data\n// struct has more fields to describe value profile information.\n// Version 3: Compressed name section support. Function PGO name reference\n// from control data struct is changed from raw pointer to Name's MD5 value.\n// Version 4: ValueDataBegin and ValueDataSizes fields are removed from the\n// raw header.\n// Version 5: Bit 60 of FuncHash is reserved for the flag for the context\n// sensitive records.\nconst uint64_t Version = INSTR_PROF_RAW_VERSION;\n\ntemplate <class IntPtrT> inline uint64_t getMagic();\ntemplate <> inline uint64_t getMagic<uint64_t>() {\n  return INSTR_PROF_RAW_MAGIC_64;\n}\n\ntemplate <> inline uint64_t getMagic<uint32_t>() {\n  return INSTR_PROF_RAW_MAGIC_32;\n}\n\n// Per-function profile data header/control structure.\n// The definition should match the structure defined in\n// compiler-rt/lib/profile/InstrProfiling.h.\n// It should also match the synthesized type in\n// Transforms/Instrumentation/InstrProfiling.cpp:getOrCreateRegionCounters.\ntemplate <class IntPtrT> struct alignas(8) ProfileData {\n  #define INSTR_PROF_DATA(Type, LLVMType, Name, Init) Type Name;\n  #include \"llvm/ProfileData/InstrProfData.inc\"\n};\n\n// File header structure of the LLVM profile data in raw format.\n// The definition should match the header referenced in\n// compiler-rt/lib/profile/InstrProfilingFile.c  and\n// InstrProfilingBuffer.c.\nstruct Header {\n#define INSTR_PROF_RAW_HEADER(Type, Name, Init) const Type Name;\n#include \"llvm/ProfileData/InstrProfData.inc\"\n};\n\n} // end namespace RawInstrProf\n\n// Parse MemOP Size range option.\nvoid getMemOPSizeRangeFromOption(StringRef Str, int64_t &RangeStart,\n                                 int64_t &RangeLast);\n\n// Create a COMDAT variable INSTR_PROF_RAW_VERSION_VAR to make the runtime\n// aware this is an ir_level profile so it can set the version flag.\nvoid createIRLevelProfileFlagVar(Module &M, bool IsCS,\n                                 bool InstrEntryBBEnabled);\n\n// Create the variable for the profile file name.\nvoid createProfileFileNameVar(Module &M, StringRef InstrProfileOutput);\n\n// Whether to compress function names in profile records, and filenames in\n// code coverage mappings. Used by the Instrumentation library and unit tests.\nextern cl::opt<bool> DoInstrProfNameCompression;\n\n} // end namespace llvm\n#endif // LLVM_PROFILEDATA_INSTRPROF_H\n"}, "94": {"id": 94, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/EndianStream.h", "content": "//===- EndianStream.h - Stream ops with endian specific data ----*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines utilities for operating on streams that have endian\n// specific data.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_SUPPORT_ENDIANSTREAM_H\n#define LLVM_SUPPORT_ENDIANSTREAM_H\n\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/Support/Endian.h\"\n#include \"llvm/Support/raw_ostream.h\"\n\nnamespace llvm {\nnamespace support {\n\nnamespace endian {\n\ntemplate <typename value_type>\ninline void write(raw_ostream &os, value_type value, endianness endian) {\n  value = byte_swap<value_type>(value, endian);\n  os.write((const char *)&value, sizeof(value_type));\n}\n\ntemplate <>\ninline void write<float>(raw_ostream &os, float value, endianness endian) {\n  write(os, FloatToBits(value), endian);\n}\n\ntemplate <>\ninline void write<double>(raw_ostream &os, double value,\n                          endianness endian) {\n  write(os, DoubleToBits(value), endian);\n}\n\ntemplate <typename value_type>\ninline void write(raw_ostream &os, ArrayRef<value_type> vals,\n                  endianness endian) {\n  for (value_type v : vals)\n    write(os, v, endian);\n}\n\n/// Adapter to write values to a stream in a particular byte order.\nstruct Writer {\n  raw_ostream &OS;\n  endianness Endian;\n  Writer(raw_ostream &OS, endianness Endian) : OS(OS), Endian(Endian) {}\n  template <typename value_type> void write(ArrayRef<value_type> Val) {\n    endian::write(OS, Val, Endian);\n  }\n  template <typename value_type> void write(value_type Val) {\n    endian::write(OS, Val, Endian);\n  }\n};\n\n} // end namespace endian\n\n} // end namespace support\n} // end namespace llvm\n\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 7, "file": 8, "line": 2270}, "message": "'OffsetOfNode' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/include/clang/AST/Expr.h", "reportHash": "3ad6950a4d694cb9a4117bfa4f728c3f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 34, "line": 319}, "message": "'AddedStructorArgCounts' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCXXABI.h", "reportHash": "045cd0713ba8d79ac496e84a42dc8ca4", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 35, "line": 251}, "message": "'ExtInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGCleanup.h", "reportHash": "f13056bd13deaa725e1ae6df6113f0a2", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 37, "line": 219}, "message": "'ScalarExprEmitter' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp", "reportHash": "a74fae814cedb7b91bc05c32f3b72cdc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 38, "line": 87}, "message": "'LoopInfo' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CGLoopInfo.h", "reportHash": "3791b022240d5c8a779a7d6e970aae7c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 40, "line": 1120}, "message": "'ConditionalEvaluation' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenFunction.h", "reportHash": "b9cd091bcb85921bfd9feb57db7cca49", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 41, "line": 27}, "message": "'CodeGenPGO' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/clang/lib/CodeGen/CodeGenPGO.h", "reportHash": "3a069e074668d002e6448bdd963a0ffb", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 57, "line": 376}, "message": "'filter_iterator_base' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/STLExtras.h", "reportHash": "d8674425c513512df32b54902d9fa4c0", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 35, "file": 66, "line": 40}, "message": "'ilist_alloc_traits' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist.h", "reportHash": "30f3c3bcec2ccd39f3be3f0b6336bb38", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 66, "line": 200}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 66, "line": 200}, "message": "'iplist_impl' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist.h", "reportHash": "10ace1c87795ed4f65bbeeff820b9135", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 46, "file": 67, "line": 18}, "message": "'ilist_base' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_base.h", "reportHash": "f8c83a09061a742e172576136108b459", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 34, "file": 68, "line": 210}, "message": "'SpecificNodeAccess' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "b77cdda7498be5aa610ea970cca02232", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 68, "line": 236}, "message": "'ilist_sentinel' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/ilist_node.h", "reportHash": "fe8a303c7912579f5fe9d0b9d4d064fd", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 70, "line": 207}, "message": "'iterator_adaptor_base' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/iterator.h", "reportHash": "90594fb5d6ba58891e2f30699c637659", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 71, "line": 110}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 71, "line": 110}, "message": "'simple_ilist' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ADT/simple_ilist.h", "reportHash": "0034a08a49a925611171d82e7b81d887", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 81, "line": 103}, "message": "'UnaryOperator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InstrTypes.h", "reportHash": "26f7789fa3331ded74c2c2e13f36af71", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 81, "line": 432}, "message": "'CastInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/InstrTypes.h", "reportHash": "7c979294a48f52ce83300538490ff195", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 20, "file": 82, "line": 41}, "message": "'ilist_alloc_traits' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instruction.h", "reportHash": "e17eec91ae4076869361326a47fc7262", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 1344}, "message": "'FCmpInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "079a51fdcbb7a6515e791906a10f7af9", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 83, "line": 3038}, "message": "'succ_op_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "363b90fc3106b48a6fab2ca9414d23bc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 83, "line": 3049}, "message": "'const_succ_op_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "261e1b9cc59e68472f79c781da2c9065", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 83, "line": 3589}, "message": "'succ_op_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "363b90fc3106b48a6fab2ca9414d23bc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 10, "file": 83, "line": 3600}, "message": "'const_succ_op_iterator' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "261e1b9cc59e68472f79c781da2c9065", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4691}, "message": "'TruncInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "8994a29918a8861e051a41e939ceaa64", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4730}, "message": "'ZExtInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "540ea28f9f5b4d1355cd77af1041b9fc", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4769}, "message": "'SExtInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "28ef556fd3d85d79d2ec210e738ce930", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4808}, "message": "'FPTruncInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "a35bd6974a8f87bbcd08cf2f7c34bd08", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4847}, "message": "'FPExtInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "6e799c5bd10188d2a944efd73c0f937c", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4886}, "message": "'UIToFPInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "50a368f03aa53175ccb1f54f7fe15c05", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4925}, "message": "'SIToFPInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "a8094beb07b3a4bf97a2f7596e8d67b4", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 4964}, "message": "'FPToUIInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "da032c43eeeedf50609772bc45aa4c55", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 5003}, "message": "'FPToSIInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "3b7545e842c6c4e675937489a372f305", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 5042}, "message": "'IntToPtrInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "1d2a51abafc5a7302962f65a9fc3e5cf", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 5085}, "message": "'PtrToIntInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "9dd0fed391fa10b93412a53a3790fb56", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 5136}, "message": "'BitCastInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "480385c57c5bf0af1ab6bcc0aab65d5a", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 83, "line": 5176}, "message": "'AddrSpaceCastInst' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/Instructions.h", "reportHash": "06597a4a7b258e995e86c64f182273db", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 87, "line": 66}, "message": "'SymbolTableListTraits' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/SymbolTableListTraits.h", "reportHash": "b8f9524fae8a70e00c1545e0963eeef1", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 87, "line": 114}, "message": "'SymbolTableList' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/SymbolTableListTraits.h", "reportHash": "b95630d10ae986fad73436c7726c09f9", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 88, "line": 113}, "message": "mark 'noexcept'"}, {"location": {"col": 3, "file": 88, "line": 113}, "message": "'TypedTrackingMDRef' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/IR/TrackingMDRef.h", "reportHash": "de08ad07a88b196e7ccf52948306ad11", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 90, "line": 658}, "message": "'OverlapFuncFilters' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/ProfileData/InstrProf.h", "reportHash": "cf40d55b99f1c5e4627db379066beeba", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 94, "line": 51}, "message": "'Writer' should be marked noexcept"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Support/EndianStream.h", "reportHash": "e31fed8ba94da24719cea2c6bccbbd1f", "checkerName": "cppcoreguidelines-noexcept", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
